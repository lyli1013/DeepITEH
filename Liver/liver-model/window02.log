/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd66cf69d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd66cf69d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd6db1871d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd6db1871d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2c91a5b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2c91a5b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd6db187950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd6db187950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd6d2e8be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd6d2e8be10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66cf7ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66cf7ac50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd6d2ec5490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd6d2ec5490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd6d2e02410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd6d2e02410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd6d2e8be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd6d2e8be10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd66ccbd6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd66ccbd6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66cf13750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66cf13750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd66cccc5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd66cccc5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66cc33b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66cc33b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd66ca509d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd66ca509d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd66c8d9410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd66c8d9410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66c8ca5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66c8ca5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd6fd405890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd6fd405890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd664696750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd664696750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd664610910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd664610910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd6645225d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd6645225d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd664696450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd664696450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd664610790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd664610790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66440eb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66440eb10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd66c8e8450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd66c8e8450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd664249d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd664249d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66447ea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd66447ea10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd6645fcb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd6645fcb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd664303050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd664303050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd664068c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd664068c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd663eea850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd663eea850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd663fa1f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd663fa1f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd664068c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd664068c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd663fa4250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd663fa4250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd663d285d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd663d285d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd663bfef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd663bfef90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd663ca3f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd663ca3f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd663dd62d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd663dd62d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd663ae6450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd663ae6450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd663bcd090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd663bcd090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd66389c990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd66389c990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd6639e82d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd6639e82d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd66cdaa310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd66cdaa310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd663a186d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd663a186d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd65375ead0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd65375ead0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd6535b6690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd6535b6690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd653698dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd653698dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd6639ea810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd6639ea810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd653475b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd653475b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd6533a1e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd6533a1e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd653254fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd653254fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd6533dbfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd6533dbfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd6537398d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd6537398d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd653294290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd653294290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd64b052150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd64b052150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd64af6bc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd64af6bc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd6530e7ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd6530e7ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd64b052650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd64b052650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd6530b4090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd6530b4090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd64ad78dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd64ad78dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd64acdf410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd64acdf410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd64ad37890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd64ad37890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd64ad78750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd64ad78750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd64aad6d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd64aad6d50>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-17 10:34:18.535619: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-17 10:34:18.581196: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-17 10:34:18.617439: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ae439f3770 executing computations on platform Host. Devices:
2022-11-17 10:34:18.617524: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-17 10:34:19.223917: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window02.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 24:12 - loss: 0.7946 - acc: 0.3906
 128/4566 [..............................] - ETA: 18:00 - loss: 0.9123 - acc: 0.4375
 192/4566 [>.............................] - ETA: 19:29 - loss: 0.8755 - acc: 0.4115
 256/4566 [>.............................] - ETA: 19:52 - loss: 0.8323 - acc: 0.4336
 320/4566 [=>............................] - ETA: 19:20 - loss: 0.8242 - acc: 0.4562
 384/4566 [=>............................] - ETA: 18:43 - loss: 0.8099 - acc: 0.4714
 448/4566 [=>............................] - ETA: 18:03 - loss: 0.7990 - acc: 0.4754
 512/4566 [==>...........................] - ETA: 17:24 - loss: 0.7880 - acc: 0.4766
 576/4566 [==>...........................] - ETA: 16:18 - loss: 0.7799 - acc: 0.4774
 640/4566 [===>..........................] - ETA: 15:11 - loss: 0.7702 - acc: 0.4813
 704/4566 [===>..........................] - ETA: 14:17 - loss: 0.7700 - acc: 0.4801
 768/4566 [====>.........................] - ETA: 13:31 - loss: 0.7621 - acc: 0.4896
 832/4566 [====>.........................] - ETA: 12:52 - loss: 0.7656 - acc: 0.4832
 896/4566 [====>.........................] - ETA: 12:15 - loss: 0.7650 - acc: 0.4866
 960/4566 [=====>........................] - ETA: 11:44 - loss: 0.7644 - acc: 0.4833
1024/4566 [=====>........................] - ETA: 11:14 - loss: 0.7584 - acc: 0.4893
1088/4566 [======>.......................] - ETA: 10:48 - loss: 0.7541 - acc: 0.4926
1152/4566 [======>.......................] - ETA: 10:24 - loss: 0.7502 - acc: 0.4957
1216/4566 [======>.......................] - ETA: 10:01 - loss: 0.7489 - acc: 0.4984
1280/4566 [=======>......................] - ETA: 9:39 - loss: 0.7521 - acc: 0.4977 
1344/4566 [=======>......................] - ETA: 9:17 - loss: 0.7506 - acc: 0.4970
1408/4566 [========>.....................] - ETA: 8:59 - loss: 0.7495 - acc: 0.4972
1472/4566 [========>.....................] - ETA: 8:54 - loss: 0.7502 - acc: 0.4925
1536/4566 [=========>....................] - ETA: 8:50 - loss: 0.7484 - acc: 0.4928
1600/4566 [=========>....................] - ETA: 8:46 - loss: 0.7483 - acc: 0.4900
1664/4566 [=========>....................] - ETA: 8:40 - loss: 0.7485 - acc: 0.4892
1728/4566 [==========>...................] - ETA: 8:33 - loss: 0.7463 - acc: 0.4907
1792/4566 [==========>...................] - ETA: 8:25 - loss: 0.7457 - acc: 0.4905
1856/4566 [===========>..................] - ETA: 8:15 - loss: 0.7444 - acc: 0.4903
1920/4566 [===========>..................] - ETA: 7:57 - loss: 0.7429 - acc: 0.4906
1984/4566 [============>.................] - ETA: 7:40 - loss: 0.7423 - acc: 0.4904
2048/4566 [============>.................] - ETA: 7:24 - loss: 0.7431 - acc: 0.4863
2112/4566 [============>.................] - ETA: 7:08 - loss: 0.7407 - acc: 0.4896
2176/4566 [=============>................] - ETA: 6:52 - loss: 0.7409 - acc: 0.4885
2240/4566 [=============>................] - ETA: 6:37 - loss: 0.7394 - acc: 0.4902
2304/4566 [==============>...............] - ETA: 6:23 - loss: 0.7383 - acc: 0.4905
2368/4566 [==============>...............] - ETA: 6:09 - loss: 0.7375 - acc: 0.4903
2432/4566 [==============>...............] - ETA: 5:56 - loss: 0.7356 - acc: 0.4910
2496/4566 [===============>..............] - ETA: 5:43 - loss: 0.7355 - acc: 0.4908
2560/4566 [===============>..............] - ETA: 5:30 - loss: 0.7344 - acc: 0.4922
2624/4566 [================>.............] - ETA: 5:17 - loss: 0.7348 - acc: 0.4909
2688/4566 [================>.............] - ETA: 5:05 - loss: 0.7341 - acc: 0.4926
2752/4566 [=================>............] - ETA: 4:53 - loss: 0.7347 - acc: 0.4913
2816/4566 [=================>............] - ETA: 4:41 - loss: 0.7332 - acc: 0.4936
2880/4566 [=================>............] - ETA: 4:33 - loss: 0.7321 - acc: 0.4944
2944/4566 [==================>...........] - ETA: 4:25 - loss: 0.7306 - acc: 0.4959
3008/4566 [==================>...........] - ETA: 4:27 - loss: 0.7309 - acc: 0.4957
3072/4566 [===================>..........] - ETA: 4:19 - loss: 0.7309 - acc: 0.4958
3136/4566 [===================>..........] - ETA: 4:12 - loss: 0.7316 - acc: 0.4943
3200/4566 [====================>.........] - ETA: 4:00 - loss: 0.7325 - acc: 0.4925
3264/4566 [====================>.........] - ETA: 3:48 - loss: 0.7323 - acc: 0.4917
3328/4566 [====================>.........] - ETA: 3:35 - loss: 0.7321 - acc: 0.4922
3392/4566 [=====================>........] - ETA: 3:23 - loss: 0.7315 - acc: 0.4920
3456/4566 [=====================>........] - ETA: 3:11 - loss: 0.7308 - acc: 0.4931
3520/4566 [======================>.......] - ETA: 2:59 - loss: 0.7308 - acc: 0.4920
3584/4566 [======================>.......] - ETA: 2:47 - loss: 0.7298 - acc: 0.4933
3648/4566 [======================>.......] - ETA: 2:36 - loss: 0.7293 - acc: 0.4934
3712/4566 [=======================>......] - ETA: 2:24 - loss: 0.7297 - acc: 0.4927
3776/4566 [=======================>......] - ETA: 2:13 - loss: 0.7293 - acc: 0.4944
3840/4566 [========================>.....] - ETA: 2:01 - loss: 0.7285 - acc: 0.4964
3904/4566 [========================>.....] - ETA: 1:50 - loss: 0.7271 - acc: 0.4987
3968/4566 [=========================>....] - ETA: 1:39 - loss: 0.7271 - acc: 0.4982
4032/4566 [=========================>....] - ETA: 1:29 - loss: 0.7272 - acc: 0.4980
4096/4566 [=========================>....] - ETA: 1:19 - loss: 0.7258 - acc: 0.5007
4160/4566 [==========================>...] - ETA: 1:08 - loss: 0.7253 - acc: 0.5022
4224/4566 [==========================>...] - ETA: 58s - loss: 0.7251 - acc: 0.5017 
4288/4566 [===========================>..] - ETA: 47s - loss: 0.7255 - acc: 0.5005
4352/4566 [===========================>..] - ETA: 36s - loss: 0.7255 - acc: 0.5009
4416/4566 [============================>.] - ETA: 25s - loss: 0.7249 - acc: 0.5011
4480/4566 [============================>.] - ETA: 14s - loss: 0.7249 - acc: 0.5013
4544/4566 [============================>.] - ETA: 3s - loss: 0.7247 - acc: 0.5011 
4566/4566 [==============================] - 796s 174ms/step - loss: 0.7246 - acc: 0.5011 - val_loss: 0.6784 - val_acc: 0.5610

Epoch 00001: val_acc improved from -inf to 0.56102, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window02/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 8:51 - loss: 0.6568 - acc: 0.6094
 128/4566 [..............................] - ETA: 8:40 - loss: 0.6726 - acc: 0.6016
 192/4566 [>.............................] - ETA: 8:37 - loss: 0.6834 - acc: 0.5781
 256/4566 [>.............................] - ETA: 8:34 - loss: 0.6819 - acc: 0.5781
 320/4566 [=>............................] - ETA: 8:22 - loss: 0.6822 - acc: 0.5750
 384/4566 [=>............................] - ETA: 8:13 - loss: 0.6876 - acc: 0.5625
 448/4566 [=>............................] - ETA: 8:07 - loss: 0.6837 - acc: 0.5737
 512/4566 [==>...........................] - ETA: 7:56 - loss: 0.6833 - acc: 0.5762
 576/4566 [==>...........................] - ETA: 7:47 - loss: 0.6801 - acc: 0.5799
 640/4566 [===>..........................] - ETA: 7:56 - loss: 0.6792 - acc: 0.5844
 704/4566 [===>..........................] - ETA: 8:29 - loss: 0.6820 - acc: 0.5781
 768/4566 [====>.........................] - ETA: 8:51 - loss: 0.6846 - acc: 0.5742
 832/4566 [====>.........................] - ETA: 9:07 - loss: 0.6848 - acc: 0.5709
 896/4566 [====>.........................] - ETA: 9:15 - loss: 0.6855 - acc: 0.5725
 960/4566 [=====>........................] - ETA: 9:26 - loss: 0.6879 - acc: 0.5646
1024/4566 [=====>........................] - ETA: 9:28 - loss: 0.6918 - acc: 0.5566
1088/4566 [======>.......................] - ETA: 9:15 - loss: 0.6923 - acc: 0.5524
1152/4566 [======>.......................] - ETA: 8:58 - loss: 0.6944 - acc: 0.5469
1216/4566 [======>.......................] - ETA: 8:42 - loss: 0.6931 - acc: 0.5502
1280/4566 [=======>......................] - ETA: 8:25 - loss: 0.6952 - acc: 0.5492
1344/4566 [=======>......................] - ETA: 8:10 - loss: 0.6959 - acc: 0.5499
1408/4566 [========>.....................] - ETA: 7:56 - loss: 0.6959 - acc: 0.5483
1472/4566 [========>.....................] - ETA: 7:41 - loss: 0.6963 - acc: 0.5469
1536/4566 [=========>....................] - ETA: 7:28 - loss: 0.6955 - acc: 0.5449
1600/4566 [=========>....................] - ETA: 7:15 - loss: 0.6966 - acc: 0.5406
1664/4566 [=========>....................] - ETA: 7:02 - loss: 0.6978 - acc: 0.5391
1728/4566 [==========>...................] - ETA: 6:50 - loss: 0.6986 - acc: 0.5359
1792/4566 [==========>...................] - ETA: 6:38 - loss: 0.6990 - acc: 0.5346
1856/4566 [===========>..................] - ETA: 6:27 - loss: 0.6992 - acc: 0.5361
1920/4566 [===========>..................] - ETA: 6:15 - loss: 0.6999 - acc: 0.5370
1984/4566 [============>.................] - ETA: 6:04 - loss: 0.6998 - acc: 0.5363
2048/4566 [============>.................] - ETA: 5:58 - loss: 0.6993 - acc: 0.5381
2112/4566 [============>.................] - ETA: 5:54 - loss: 0.6988 - acc: 0.5407
2176/4566 [=============>................] - ETA: 5:50 - loss: 0.6992 - acc: 0.5414
2240/4566 [=============>................] - ETA: 5:44 - loss: 0.6986 - acc: 0.5446
2304/4566 [==============>...............] - ETA: 5:39 - loss: 0.6982 - acc: 0.5464
2368/4566 [==============>...............] - ETA: 5:32 - loss: 0.6977 - acc: 0.5473
2432/4566 [==============>...............] - ETA: 5:26 - loss: 0.6979 - acc: 0.5448
2496/4566 [===============>..............] - ETA: 5:16 - loss: 0.6983 - acc: 0.5433
2560/4566 [===============>..............] - ETA: 5:05 - loss: 0.6972 - acc: 0.5441
2624/4566 [================>.............] - ETA: 4:54 - loss: 0.6960 - acc: 0.5457
2688/4566 [================>.............] - ETA: 4:42 - loss: 0.6953 - acc: 0.5461
2752/4566 [=================>............] - ETA: 4:31 - loss: 0.6942 - acc: 0.5494
2816/4566 [=================>............] - ETA: 4:21 - loss: 0.6938 - acc: 0.5494
2880/4566 [=================>............] - ETA: 4:10 - loss: 0.6941 - acc: 0.5486
2944/4566 [==================>...........] - ETA: 3:59 - loss: 0.6947 - acc: 0.5489
3008/4566 [==================>...........] - ETA: 3:49 - loss: 0.6971 - acc: 0.5449
3072/4566 [===================>..........] - ETA: 3:39 - loss: 0.6975 - acc: 0.5426
3136/4566 [===================>..........] - ETA: 3:29 - loss: 0.6983 - acc: 0.5415
3200/4566 [====================>.........] - ETA: 3:18 - loss: 0.6990 - acc: 0.5397
3264/4566 [====================>.........] - ETA: 3:08 - loss: 0.6988 - acc: 0.5401
3328/4566 [====================>.........] - ETA: 2:58 - loss: 0.6983 - acc: 0.5406
3392/4566 [=====================>........] - ETA: 2:48 - loss: 0.6975 - acc: 0.5410
3456/4566 [=====================>........] - ETA: 2:40 - loss: 0.6977 - acc: 0.5414
3520/4566 [======================>.......] - ETA: 2:32 - loss: 0.6983 - acc: 0.5398
3584/4566 [======================>.......] - ETA: 2:23 - loss: 0.6982 - acc: 0.5391
3648/4566 [======================>.......] - ETA: 2:15 - loss: 0.6979 - acc: 0.5403
3712/4566 [=======================>......] - ETA: 2:07 - loss: 0.6981 - acc: 0.5388
3776/4566 [=======================>......] - ETA: 1:58 - loss: 0.6982 - acc: 0.5384
3840/4566 [========================>.....] - ETA: 1:49 - loss: 0.6982 - acc: 0.5385
3904/4566 [========================>.....] - ETA: 1:40 - loss: 0.6979 - acc: 0.5394
3968/4566 [=========================>....] - ETA: 1:30 - loss: 0.6979 - acc: 0.5393
4032/4566 [=========================>....] - ETA: 1:20 - loss: 0.6984 - acc: 0.5384
4096/4566 [=========================>....] - ETA: 1:10 - loss: 0.6990 - acc: 0.5369
4160/4566 [==========================>...] - ETA: 1:00 - loss: 0.6988 - acc: 0.5365
4224/4566 [==========================>...] - ETA: 50s - loss: 0.6986 - acc: 0.5372 
4288/4566 [===========================>..] - ETA: 41s - loss: 0.6986 - acc: 0.5366
4352/4566 [===========================>..] - ETA: 31s - loss: 0.6987 - acc: 0.5354
4416/4566 [============================>.] - ETA: 22s - loss: 0.6987 - acc: 0.5349
4480/4566 [============================>.] - ETA: 12s - loss: 0.6990 - acc: 0.5353
4544/4566 [============================>.] - ETA: 3s - loss: 0.6991 - acc: 0.5348 
4566/4566 [==============================] - 690s 151ms/step - loss: 0.6991 - acc: 0.5342 - val_loss: 0.6833 - val_acc: 0.5472

Epoch 00002: val_acc did not improve from 0.56102
Epoch 3/10

  64/4566 [..............................] - ETA: 9:29 - loss: 0.7462 - acc: 0.4531
 128/4566 [..............................] - ETA: 9:42 - loss: 0.7121 - acc: 0.5156
 192/4566 [>.............................] - ETA: 11:33 - loss: 0.6991 - acc: 0.5260
 256/4566 [>.............................] - ETA: 12:26 - loss: 0.6929 - acc: 0.5273
 320/4566 [=>............................] - ETA: 12:58 - loss: 0.6885 - acc: 0.5469
 384/4566 [=>............................] - ETA: 13:19 - loss: 0.6857 - acc: 0.5547
 448/4566 [=>............................] - ETA: 13:35 - loss: 0.6811 - acc: 0.5625
 512/4566 [==>...........................] - ETA: 13:34 - loss: 0.6873 - acc: 0.5508
 576/4566 [==>...........................] - ETA: 13:12 - loss: 0.6896 - acc: 0.5399
 640/4566 [===>..........................] - ETA: 12:29 - loss: 0.6976 - acc: 0.5281
 704/4566 [===>..........................] - ETA: 11:50 - loss: 0.6997 - acc: 0.5298
 768/4566 [====>.........................] - ETA: 11:19 - loss: 0.6991 - acc: 0.5326
 832/4566 [====>.........................] - ETA: 10:52 - loss: 0.6978 - acc: 0.5373
 896/4566 [====>.........................] - ETA: 10:28 - loss: 0.6961 - acc: 0.5402
 960/4566 [=====>........................] - ETA: 10:04 - loss: 0.6954 - acc: 0.5448
1024/4566 [=====>........................] - ETA: 9:44 - loss: 0.6971 - acc: 0.5420 
1088/4566 [======>.......................] - ETA: 9:27 - loss: 0.6950 - acc: 0.5460
1152/4566 [======>.......................] - ETA: 9:07 - loss: 0.6944 - acc: 0.5495
1216/4566 [======>.......................] - ETA: 8:50 - loss: 0.6947 - acc: 0.5452
1280/4566 [=======>......................] - ETA: 8:34 - loss: 0.6932 - acc: 0.5477
1344/4566 [=======>......................] - ETA: 8:18 - loss: 0.6930 - acc: 0.5454
1408/4566 [========>.....................] - ETA: 8:04 - loss: 0.6911 - acc: 0.5504
1472/4566 [========>.....................] - ETA: 7:50 - loss: 0.6914 - acc: 0.5503
1536/4566 [=========>....................] - ETA: 7:43 - loss: 0.6918 - acc: 0.5488
1600/4566 [=========>....................] - ETA: 7:43 - loss: 0.6905 - acc: 0.5531
1664/4566 [=========>....................] - ETA: 7:41 - loss: 0.6904 - acc: 0.5505
1728/4566 [==========>...................] - ETA: 7:36 - loss: 0.6905 - acc: 0.5509
1792/4566 [==========>...................] - ETA: 7:33 - loss: 0.6912 - acc: 0.5491
1856/4566 [===========>..................] - ETA: 7:27 - loss: 0.6921 - acc: 0.5458
1920/4566 [===========>..................] - ETA: 7:19 - loss: 0.6931 - acc: 0.5427
1984/4566 [============>.................] - ETA: 7:05 - loss: 0.6932 - acc: 0.5413
2048/4566 [============>.................] - ETA: 6:51 - loss: 0.6924 - acc: 0.5405
2112/4566 [============>.................] - ETA: 6:36 - loss: 0.6938 - acc: 0.5360
2176/4566 [=============>................] - ETA: 6:23 - loss: 0.6939 - acc: 0.5358
2240/4566 [=============>................] - ETA: 6:10 - loss: 0.6937 - acc: 0.5362
2304/4566 [==============>...............] - ETA: 5:57 - loss: 0.6944 - acc: 0.5347
2368/4566 [==============>...............] - ETA: 5:44 - loss: 0.6938 - acc: 0.5359
2432/4566 [==============>...............] - ETA: 5:31 - loss: 0.6942 - acc: 0.5341
2496/4566 [===============>..............] - ETA: 5:19 - loss: 0.6943 - acc: 0.5341
2560/4566 [===============>..............] - ETA: 5:07 - loss: 0.6939 - acc: 0.5359
2624/4566 [================>.............] - ETA: 4:56 - loss: 0.6933 - acc: 0.5381
2688/4566 [================>.............] - ETA: 4:45 - loss: 0.6930 - acc: 0.5387
2752/4566 [=================>............] - ETA: 4:33 - loss: 0.6929 - acc: 0.5382
2816/4566 [=================>............] - ETA: 4:22 - loss: 0.6926 - acc: 0.5387
2880/4566 [=================>............] - ETA: 4:12 - loss: 0.6919 - acc: 0.5399
2944/4566 [==================>...........] - ETA: 4:03 - loss: 0.6922 - acc: 0.5380
3008/4566 [==================>...........] - ETA: 3:57 - loss: 0.6921 - acc: 0.5372
3072/4566 [===================>..........] - ETA: 3:49 - loss: 0.6929 - acc: 0.5348
3136/4566 [===================>..........] - ETA: 3:41 - loss: 0.6928 - acc: 0.5367
3200/4566 [====================>.........] - ETA: 3:33 - loss: 0.6924 - acc: 0.5369
3264/4566 [====================>.........] - ETA: 3:24 - loss: 0.6925 - acc: 0.5355
3328/4566 [====================>.........] - ETA: 3:15 - loss: 0.6923 - acc: 0.5367
3392/4566 [=====================>........] - ETA: 3:04 - loss: 0.6916 - acc: 0.5386
3456/4566 [=====================>........] - ETA: 2:53 - loss: 0.6912 - acc: 0.5376
3520/4566 [======================>.......] - ETA: 2:42 - loss: 0.6910 - acc: 0.5381
3584/4566 [======================>.......] - ETA: 2:31 - loss: 0.6908 - acc: 0.5385
3648/4566 [======================>.......] - ETA: 2:21 - loss: 0.6915 - acc: 0.5376
3712/4566 [=======================>......] - ETA: 2:10 - loss: 0.6911 - acc: 0.5383
3776/4566 [=======================>......] - ETA: 2:00 - loss: 0.6913 - acc: 0.5376
3840/4566 [========================>.....] - ETA: 1:50 - loss: 0.6910 - acc: 0.5393
3904/4566 [========================>.....] - ETA: 1:40 - loss: 0.6910 - acc: 0.5387
3968/4566 [=========================>....] - ETA: 1:30 - loss: 0.6911 - acc: 0.5398
4032/4566 [=========================>....] - ETA: 1:20 - loss: 0.6909 - acc: 0.5399
4096/4566 [=========================>....] - ETA: 1:10 - loss: 0.6901 - acc: 0.5420
4160/4566 [==========================>...] - ETA: 1:00 - loss: 0.6899 - acc: 0.5423
4224/4566 [==========================>...] - ETA: 50s - loss: 0.6897 - acc: 0.5421 
4288/4566 [===========================>..] - ETA: 41s - loss: 0.6899 - acc: 0.5420
4352/4566 [===========================>..] - ETA: 31s - loss: 0.6902 - acc: 0.5423
4416/4566 [============================>.] - ETA: 22s - loss: 0.6904 - acc: 0.5410
4480/4566 [============================>.] - ETA: 12s - loss: 0.6904 - acc: 0.5408
4544/4566 [============================>.] - ETA: 3s - loss: 0.6902 - acc: 0.5416 
4566/4566 [==============================] - 728s 159ms/step - loss: 0.6902 - acc: 0.5416 - val_loss: 0.6760 - val_acc: 0.5748

Epoch 00003: val_acc improved from 0.56102 to 0.57480, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window02/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 4/10

  64/4566 [..............................] - ETA: 8:08 - loss: 0.7244 - acc: 0.5156
 128/4566 [..............................] - ETA: 7:57 - loss: 0.7061 - acc: 0.5078
 192/4566 [>.............................] - ETA: 7:39 - loss: 0.6962 - acc: 0.5156
 256/4566 [>.............................] - ETA: 7:33 - loss: 0.6903 - acc: 0.5312
 320/4566 [=>............................] - ETA: 7:30 - loss: 0.6933 - acc: 0.5312
 384/4566 [=>............................] - ETA: 7:23 - loss: 0.6946 - acc: 0.5182
 448/4566 [=>............................] - ETA: 7:15 - loss: 0.6950 - acc: 0.5223
 512/4566 [==>...........................] - ETA: 7:08 - loss: 0.6941 - acc: 0.5273
 576/4566 [==>...........................] - ETA: 7:03 - loss: 0.6927 - acc: 0.5365
 640/4566 [===>..........................] - ETA: 6:56 - loss: 0.6904 - acc: 0.5344
 704/4566 [===>..........................] - ETA: 6:48 - loss: 0.6882 - acc: 0.5440
 768/4566 [====>.........................] - ETA: 6:38 - loss: 0.6877 - acc: 0.5482
 832/4566 [====>.........................] - ETA: 6:30 - loss: 0.6876 - acc: 0.5517
 896/4566 [====>.........................] - ETA: 6:21 - loss: 0.6857 - acc: 0.5603
 960/4566 [=====>........................] - ETA: 6:14 - loss: 0.6842 - acc: 0.5625
1024/4566 [=====>........................] - ETA: 6:06 - loss: 0.6850 - acc: 0.5625
1088/4566 [======>.......................] - ETA: 6:20 - loss: 0.6834 - acc: 0.5689
1152/4566 [======>.......................] - ETA: 6:33 - loss: 0.6835 - acc: 0.5677
1216/4566 [======>.......................] - ETA: 6:43 - loss: 0.6842 - acc: 0.5650
1280/4566 [=======>......................] - ETA: 6:48 - loss: 0.6859 - acc: 0.5602
1344/4566 [=======>......................] - ETA: 6:54 - loss: 0.6865 - acc: 0.5618
1408/4566 [========>.....................] - ETA: 6:56 - loss: 0.6854 - acc: 0.5604
1472/4566 [========>.....................] - ETA: 6:54 - loss: 0.6844 - acc: 0.5645
1536/4566 [=========>....................] - ETA: 6:40 - loss: 0.6843 - acc: 0.5638
1600/4566 [=========>....................] - ETA: 6:27 - loss: 0.6835 - acc: 0.5669
1664/4566 [=========>....................] - ETA: 6:15 - loss: 0.6822 - acc: 0.5697
1728/4566 [==========>...................] - ETA: 6:04 - loss: 0.6821 - acc: 0.5712
1792/4566 [==========>...................] - ETA: 5:52 - loss: 0.6805 - acc: 0.5753
1856/4566 [===========>..................] - ETA: 5:40 - loss: 0.6810 - acc: 0.5749
1920/4566 [===========>..................] - ETA: 5:29 - loss: 0.6799 - acc: 0.5750
1984/4566 [============>.................] - ETA: 5:19 - loss: 0.6809 - acc: 0.5736
2048/4566 [============>.................] - ETA: 5:08 - loss: 0.6831 - acc: 0.5708
2112/4566 [============>.................] - ETA: 4:58 - loss: 0.6841 - acc: 0.5687
2176/4566 [=============>................] - ETA: 4:48 - loss: 0.6845 - acc: 0.5685
2240/4566 [=============>................] - ETA: 4:39 - loss: 0.6864 - acc: 0.5638
2304/4566 [==============>...............] - ETA: 4:30 - loss: 0.6862 - acc: 0.5642
2368/4566 [==============>...............] - ETA: 4:21 - loss: 0.6861 - acc: 0.5633
2432/4566 [==============>...............] - ETA: 4:12 - loss: 0.6864 - acc: 0.5613
2496/4566 [===============>..............] - ETA: 4:03 - loss: 0.6860 - acc: 0.5629
2560/4566 [===============>..............] - ETA: 3:56 - loss: 0.6859 - acc: 0.5621
2624/4566 [================>.............] - ETA: 3:53 - loss: 0.6860 - acc: 0.5610
2688/4566 [================>.............] - ETA: 3:48 - loss: 0.6853 - acc: 0.5629
2752/4566 [=================>............] - ETA: 3:44 - loss: 0.6853 - acc: 0.5643
2816/4566 [=================>............] - ETA: 3:39 - loss: 0.6851 - acc: 0.5639
2880/4566 [=================>............] - ETA: 3:34 - loss: 0.6861 - acc: 0.5604
2944/4566 [==================>...........] - ETA: 3:28 - loss: 0.6860 - acc: 0.5608
3008/4566 [==================>...........] - ETA: 3:20 - loss: 0.6855 - acc: 0.5618
3072/4566 [===================>..........] - ETA: 3:11 - loss: 0.6851 - acc: 0.5628
3136/4566 [===================>..........] - ETA: 3:01 - loss: 0.6862 - acc: 0.5609
3200/4566 [====================>.........] - ETA: 2:52 - loss: 0.6871 - acc: 0.5587
3264/4566 [====================>.........] - ETA: 2:43 - loss: 0.6865 - acc: 0.5594
3328/4566 [====================>.........] - ETA: 2:34 - loss: 0.6874 - acc: 0.5574
3392/4566 [=====================>........] - ETA: 2:25 - loss: 0.6866 - acc: 0.5590
3456/4566 [=====================>........] - ETA: 2:16 - loss: 0.6867 - acc: 0.5582
3520/4566 [======================>.......] - ETA: 2:08 - loss: 0.6872 - acc: 0.5571
3584/4566 [======================>.......] - ETA: 1:59 - loss: 0.6872 - acc: 0.5569
3648/4566 [======================>.......] - ETA: 1:51 - loss: 0.6868 - acc: 0.5567
3712/4566 [=======================>......] - ETA: 1:42 - loss: 0.6867 - acc: 0.5563
3776/4566 [=======================>......] - ETA: 1:34 - loss: 0.6858 - acc: 0.5588
3840/4566 [========================>.....] - ETA: 1:26 - loss: 0.6862 - acc: 0.5570
3904/4566 [========================>.....] - ETA: 1:18 - loss: 0.6863 - acc: 0.5571
3968/4566 [=========================>....] - ETA: 1:10 - loss: 0.6867 - acc: 0.5567
4032/4566 [=========================>....] - ETA: 1:02 - loss: 0.6863 - acc: 0.5570
4096/4566 [=========================>....] - ETA: 55s - loss: 0.6856 - acc: 0.5596 
4160/4566 [==========================>...] - ETA: 48s - loss: 0.6852 - acc: 0.5599
4224/4566 [==========================>...] - ETA: 41s - loss: 0.6849 - acc: 0.5608
4288/4566 [===========================>..] - ETA: 33s - loss: 0.6843 - acc: 0.5634
4352/4566 [===========================>..] - ETA: 26s - loss: 0.6842 - acc: 0.5634
4416/4566 [============================>.] - ETA: 18s - loss: 0.6842 - acc: 0.5639
4480/4566 [============================>.] - ETA: 10s - loss: 0.6844 - acc: 0.5634
4544/4566 [============================>.] - ETA: 2s - loss: 0.6839 - acc: 0.5645 
4566/4566 [==============================] - 583s 128ms/step - loss: 0.6840 - acc: 0.5642 - val_loss: 0.6799 - val_acc: 0.5669

Epoch 00004: val_acc did not improve from 0.57480
Epoch 5/10

  64/4566 [..............................] - ETA: 6:30 - loss: 0.6255 - acc: 0.6562
 128/4566 [..............................] - ETA: 6:05 - loss: 0.6305 - acc: 0.6484
 192/4566 [>.............................] - ETA: 6:00 - loss: 0.6407 - acc: 0.6354
 256/4566 [>.............................] - ETA: 5:53 - loss: 0.6435 - acc: 0.6328
 320/4566 [=>............................] - ETA: 5:50 - loss: 0.6505 - acc: 0.6219
 384/4566 [=>............................] - ETA: 5:46 - loss: 0.6609 - acc: 0.5964
 448/4566 [=>............................] - ETA: 5:44 - loss: 0.6649 - acc: 0.5982
 512/4566 [==>...........................] - ETA: 5:37 - loss: 0.6642 - acc: 0.5996
 576/4566 [==>...........................] - ETA: 5:31 - loss: 0.6663 - acc: 0.5868
 640/4566 [===>..........................] - ETA: 5:24 - loss: 0.6659 - acc: 0.5891
 704/4566 [===>..........................] - ETA: 5:19 - loss: 0.6671 - acc: 0.5838
 768/4566 [====>.........................] - ETA: 5:14 - loss: 0.6676 - acc: 0.5872
 832/4566 [====>.........................] - ETA: 5:10 - loss: 0.6681 - acc: 0.5901
 896/4566 [====>.........................] - ETA: 5:31 - loss: 0.6686 - acc: 0.5848
 960/4566 [=====>........................] - ETA: 5:53 - loss: 0.6674 - acc: 0.5927
1024/4566 [=====>........................] - ETA: 6:10 - loss: 0.6705 - acc: 0.5908
1088/4566 [======>.......................] - ETA: 6:24 - loss: 0.6703 - acc: 0.5892
1152/4566 [======>.......................] - ETA: 6:34 - loss: 0.6713 - acc: 0.5877
1216/4566 [======>.......................] - ETA: 6:42 - loss: 0.6709 - acc: 0.5896
1280/4566 [=======>......................] - ETA: 6:43 - loss: 0.6713 - acc: 0.5875
1344/4566 [=======>......................] - ETA: 6:29 - loss: 0.6729 - acc: 0.5878
1408/4566 [========>.....................] - ETA: 6:18 - loss: 0.6735 - acc: 0.5845
1472/4566 [========>.....................] - ETA: 6:06 - loss: 0.6760 - acc: 0.5808
1536/4566 [=========>....................] - ETA: 5:54 - loss: 0.6780 - acc: 0.5755
1600/4566 [=========>....................] - ETA: 5:43 - loss: 0.6779 - acc: 0.5775
1664/4566 [=========>....................] - ETA: 5:32 - loss: 0.6797 - acc: 0.5739
1728/4566 [==========>...................] - ETA: 5:23 - loss: 0.6783 - acc: 0.5793
1792/4566 [==========>...................] - ETA: 5:12 - loss: 0.6788 - acc: 0.5781
1856/4566 [===========>..................] - ETA: 5:02 - loss: 0.6798 - acc: 0.5733
1920/4566 [===========>..................] - ETA: 4:53 - loss: 0.6798 - acc: 0.5734
1984/4566 [============>.................] - ETA: 4:44 - loss: 0.6802 - acc: 0.5731
2048/4566 [============>.................] - ETA: 4:34 - loss: 0.6799 - acc: 0.5718
2112/4566 [============>.................] - ETA: 4:25 - loss: 0.6809 - acc: 0.5710
2176/4566 [=============>................] - ETA: 4:16 - loss: 0.6806 - acc: 0.5703
2240/4566 [=============>................] - ETA: 4:07 - loss: 0.6796 - acc: 0.5723
2304/4566 [==============>...............] - ETA: 3:59 - loss: 0.6804 - acc: 0.5686
2368/4566 [==============>...............] - ETA: 3:53 - loss: 0.6817 - acc: 0.5663
2432/4566 [==============>...............] - ETA: 3:52 - loss: 0.6818 - acc: 0.5666
2496/4566 [===============>..............] - ETA: 3:50 - loss: 0.6816 - acc: 0.5673
2560/4566 [===============>..............] - ETA: 3:47 - loss: 0.6816 - acc: 0.5664
2624/4566 [================>.............] - ETA: 3:44 - loss: 0.6814 - acc: 0.5667
2688/4566 [================>.............] - ETA: 3:40 - loss: 0.6804 - acc: 0.5681
2752/4566 [=================>............] - ETA: 3:36 - loss: 0.6803 - acc: 0.5705
2816/4566 [=================>............] - ETA: 3:27 - loss: 0.6805 - acc: 0.5692
2880/4566 [=================>............] - ETA: 3:19 - loss: 0.6803 - acc: 0.5701
2944/4566 [==================>...........] - ETA: 3:10 - loss: 0.6810 - acc: 0.5679
3008/4566 [==================>...........] - ETA: 3:01 - loss: 0.6803 - acc: 0.5688
3072/4566 [===================>..........] - ETA: 2:52 - loss: 0.6796 - acc: 0.5697
3136/4566 [===================>..........] - ETA: 2:44 - loss: 0.6797 - acc: 0.5702
3200/4566 [====================>.........] - ETA: 2:36 - loss: 0.6805 - acc: 0.5703
3264/4566 [====================>.........] - ETA: 2:27 - loss: 0.6804 - acc: 0.5714
3328/4566 [====================>.........] - ETA: 2:19 - loss: 0.6803 - acc: 0.5706
3392/4566 [=====================>........] - ETA: 2:11 - loss: 0.6798 - acc: 0.5719
3456/4566 [=====================>........] - ETA: 2:03 - loss: 0.6801 - acc: 0.5720
3520/4566 [======================>.......] - ETA: 1:56 - loss: 0.6796 - acc: 0.5736
3584/4566 [======================>.......] - ETA: 1:48 - loss: 0.6799 - acc: 0.5725
3648/4566 [======================>.......] - ETA: 1:41 - loss: 0.6799 - acc: 0.5726
3712/4566 [=======================>......] - ETA: 1:33 - loss: 0.6797 - acc: 0.5735
3776/4566 [=======================>......] - ETA: 1:26 - loss: 0.6805 - acc: 0.5723
3840/4566 [========================>.....] - ETA: 1:19 - loss: 0.6801 - acc: 0.5729
3904/4566 [========================>.....] - ETA: 1:12 - loss: 0.6807 - acc: 0.5720
3968/4566 [=========================>....] - ETA: 1:06 - loss: 0.6813 - acc: 0.5711
4032/4566 [=========================>....] - ETA: 1:00 - loss: 0.6817 - acc: 0.5709
4096/4566 [=========================>....] - ETA: 53s - loss: 0.6831 - acc: 0.5688 
4160/4566 [==========================>...] - ETA: 46s - loss: 0.6831 - acc: 0.5697
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6841 - acc: 0.5665
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6840 - acc: 0.5672
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6836 - acc: 0.5680
4416/4566 [============================>.] - ETA: 17s - loss: 0.6832 - acc: 0.5693
4480/4566 [============================>.] - ETA: 10s - loss: 0.6828 - acc: 0.5692
4544/4566 [============================>.] - ETA: 2s - loss: 0.6832 - acc: 0.5678 
4566/4566 [==============================] - 545s 119ms/step - loss: 0.6831 - acc: 0.5681 - val_loss: 0.6713 - val_acc: 0.5886

Epoch 00005: val_acc improved from 0.57480 to 0.58858, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window02/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 6/10

  64/4566 [..............................] - ETA: 6:27 - loss: 0.6911 - acc: 0.5312
 128/4566 [..............................] - ETA: 5:56 - loss: 0.6891 - acc: 0.5312
 192/4566 [>.............................] - ETA: 5:55 - loss: 0.6976 - acc: 0.5156
 256/4566 [>.............................] - ETA: 5:51 - loss: 0.7026 - acc: 0.5000
 320/4566 [=>............................] - ETA: 5:50 - loss: 0.7005 - acc: 0.5031
 384/4566 [=>............................] - ETA: 5:45 - loss: 0.6955 - acc: 0.5156
 448/4566 [=>............................] - ETA: 5:36 - loss: 0.6955 - acc: 0.5201
 512/4566 [==>...........................] - ETA: 5:31 - loss: 0.6956 - acc: 0.5215
 576/4566 [==>...........................] - ETA: 5:27 - loss: 0.6995 - acc: 0.5139
 640/4566 [===>..........................] - ETA: 5:37 - loss: 0.6991 - acc: 0.5078
 704/4566 [===>..........................] - ETA: 6:11 - loss: 0.6980 - acc: 0.5128
 768/4566 [====>.........................] - ETA: 6:41 - loss: 0.7010 - acc: 0.5052
 832/4566 [====>.........................] - ETA: 7:04 - loss: 0.6982 - acc: 0.5084
 896/4566 [====>.........................] - ETA: 7:22 - loss: 0.6962 - acc: 0.5100
 960/4566 [=====>........................] - ETA: 7:33 - loss: 0.6949 - acc: 0.5125
1024/4566 [=====>........................] - ETA: 7:41 - loss: 0.6932 - acc: 0.5215
1088/4566 [======>.......................] - ETA: 7:29 - loss: 0.6935 - acc: 0.5184
1152/4566 [======>.......................] - ETA: 7:12 - loss: 0.6953 - acc: 0.5156
1216/4566 [======>.......................] - ETA: 6:56 - loss: 0.6935 - acc: 0.5206
1280/4566 [=======>......................] - ETA: 6:41 - loss: 0.6941 - acc: 0.5203
1344/4566 [=======>......................] - ETA: 6:27 - loss: 0.6919 - acc: 0.5275
1408/4566 [========>.....................] - ETA: 6:15 - loss: 0.6910 - acc: 0.5312
1472/4566 [========>.....................] - ETA: 6:02 - loss: 0.6901 - acc: 0.5326
1536/4566 [=========>....................] - ETA: 5:50 - loss: 0.6884 - acc: 0.5326
1600/4566 [=========>....................] - ETA: 5:39 - loss: 0.6877 - acc: 0.5337
1664/4566 [=========>....................] - ETA: 5:29 - loss: 0.6868 - acc: 0.5379
1728/4566 [==========>...................] - ETA: 5:18 - loss: 0.6858 - acc: 0.5388
1792/4566 [==========>...................] - ETA: 5:09 - loss: 0.6850 - acc: 0.5407
1856/4566 [===========>..................] - ETA: 4:59 - loss: 0.6826 - acc: 0.5458
1920/4566 [===========>..................] - ETA: 4:49 - loss: 0.6836 - acc: 0.5469
1984/4566 [============>.................] - ETA: 4:39 - loss: 0.6841 - acc: 0.5489
2048/4566 [============>.................] - ETA: 4:30 - loss: 0.6848 - acc: 0.5474
2112/4566 [============>.................] - ETA: 4:22 - loss: 0.6845 - acc: 0.5488
2176/4566 [=============>................] - ETA: 4:21 - loss: 0.6839 - acc: 0.5487
2240/4566 [=============>................] - ETA: 4:21 - loss: 0.6841 - acc: 0.5482
2304/4566 [==============>...............] - ETA: 4:18 - loss: 0.6835 - acc: 0.5495
2368/4566 [==============>...............] - ETA: 4:16 - loss: 0.6838 - acc: 0.5481
2432/4566 [==============>...............] - ETA: 4:14 - loss: 0.6835 - acc: 0.5498
2496/4566 [===============>..............] - ETA: 4:10 - loss: 0.6836 - acc: 0.5505
2560/4566 [===============>..............] - ETA: 4:04 - loss: 0.6832 - acc: 0.5512
2624/4566 [================>.............] - ETA: 3:54 - loss: 0.6835 - acc: 0.5511
2688/4566 [================>.............] - ETA: 3:44 - loss: 0.6832 - acc: 0.5513
2752/4566 [=================>............] - ETA: 3:35 - loss: 0.6828 - acc: 0.5512
2816/4566 [=================>............] - ETA: 3:26 - loss: 0.6833 - acc: 0.5504
2880/4566 [=================>............] - ETA: 3:17 - loss: 0.6845 - acc: 0.5490
2944/4566 [==================>...........] - ETA: 3:08 - loss: 0.6841 - acc: 0.5499
3008/4566 [==================>...........] - ETA: 2:59 - loss: 0.6840 - acc: 0.5509
3072/4566 [===================>..........] - ETA: 2:51 - loss: 0.6845 - acc: 0.5505
3136/4566 [===================>..........] - ETA: 2:43 - loss: 0.6851 - acc: 0.5497
3200/4566 [====================>.........] - ETA: 2:34 - loss: 0.6849 - acc: 0.5494
3264/4566 [====================>.........] - ETA: 2:26 - loss: 0.6851 - acc: 0.5493
3328/4566 [====================>.........] - ETA: 2:18 - loss: 0.6843 - acc: 0.5490
3392/4566 [=====================>........] - ETA: 2:11 - loss: 0.6846 - acc: 0.5469
3456/4566 [=====================>........] - ETA: 2:03 - loss: 0.6845 - acc: 0.5469
3520/4566 [======================>.......] - ETA: 1:55 - loss: 0.6840 - acc: 0.5480
3584/4566 [======================>.......] - ETA: 1:48 - loss: 0.6845 - acc: 0.5474
3648/4566 [======================>.......] - ETA: 1:42 - loss: 0.6842 - acc: 0.5482
3712/4566 [=======================>......] - ETA: 1:36 - loss: 0.6845 - acc: 0.5480
3776/4566 [=======================>......] - ETA: 1:31 - loss: 0.6839 - acc: 0.5490
3840/4566 [========================>.....] - ETA: 1:24 - loss: 0.6841 - acc: 0.5497
3904/4566 [========================>.....] - ETA: 1:18 - loss: 0.6846 - acc: 0.5482
3968/4566 [=========================>....] - ETA: 1:11 - loss: 0.6843 - acc: 0.5481
4032/4566 [=========================>....] - ETA: 1:03 - loss: 0.6837 - acc: 0.5491
4096/4566 [=========================>....] - ETA: 56s - loss: 0.6841 - acc: 0.5476 
4160/4566 [==========================>...] - ETA: 48s - loss: 0.6836 - acc: 0.5481
4224/4566 [==========================>...] - ETA: 40s - loss: 0.6839 - acc: 0.5473
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6836 - acc: 0.5483
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6838 - acc: 0.5483
4416/4566 [============================>.] - ETA: 17s - loss: 0.6834 - acc: 0.5482
4480/4566 [============================>.] - ETA: 9s - loss: 0.6837 - acc: 0.5471 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6841 - acc: 0.5469
4566/4566 [==============================] - 541s 119ms/step - loss: 0.6840 - acc: 0.5469 - val_loss: 0.6689 - val_acc: 0.5846

Epoch 00006: val_acc did not improve from 0.58858
Epoch 7/10

  64/4566 [..............................] - ETA: 6:00 - loss: 0.6492 - acc: 0.6719
 128/4566 [..............................] - ETA: 5:39 - loss: 0.6539 - acc: 0.6328
 192/4566 [>.............................] - ETA: 5:45 - loss: 0.6594 - acc: 0.5938
 256/4566 [>.............................] - ETA: 5:41 - loss: 0.6671 - acc: 0.5820
 320/4566 [=>............................] - ETA: 5:36 - loss: 0.6705 - acc: 0.5719
 384/4566 [=>............................] - ETA: 5:38 - loss: 0.6699 - acc: 0.5729
 448/4566 [=>............................] - ETA: 6:32 - loss: 0.6678 - acc: 0.5826
 512/4566 [==>...........................] - ETA: 7:23 - loss: 0.6648 - acc: 0.5938
 576/4566 [==>...........................] - ETA: 7:57 - loss: 0.6596 - acc: 0.6042
 640/4566 [===>..........................] - ETA: 8:19 - loss: 0.6571 - acc: 0.6141
 704/4566 [===>..........................] - ETA: 8:37 - loss: 0.6569 - acc: 0.6179
 768/4566 [====>.........................] - ETA: 8:52 - loss: 0.6571 - acc: 0.6211
 832/4566 [====>.........................] - ETA: 8:47 - loss: 0.6552 - acc: 0.6286
 896/4566 [====>.........................] - ETA: 8:22 - loss: 0.6595 - acc: 0.6228
 960/4566 [=====>........................] - ETA: 8:00 - loss: 0.6623 - acc: 0.6229
1024/4566 [=====>........................] - ETA: 7:40 - loss: 0.6645 - acc: 0.6191
1088/4566 [======>.......................] - ETA: 7:21 - loss: 0.6658 - acc: 0.6158
1152/4566 [======>.......................] - ETA: 7:04 - loss: 0.6675 - acc: 0.6111
1216/4566 [======>.......................] - ETA: 6:47 - loss: 0.6661 - acc: 0.6118
1280/4566 [=======>......................] - ETA: 6:33 - loss: 0.6667 - acc: 0.6094
1344/4566 [=======>......................] - ETA: 6:20 - loss: 0.6677 - acc: 0.6094
1408/4566 [========>.....................] - ETA: 6:07 - loss: 0.6677 - acc: 0.6101
1472/4566 [========>.....................] - ETA: 5:54 - loss: 0.6656 - acc: 0.6101
1536/4566 [=========>....................] - ETA: 5:42 - loss: 0.6658 - acc: 0.6087
1600/4566 [=========>....................] - ETA: 5:32 - loss: 0.6659 - acc: 0.6056
1664/4566 [=========>....................] - ETA: 5:22 - loss: 0.6665 - acc: 0.6034
1728/4566 [==========>...................] - ETA: 5:12 - loss: 0.6666 - acc: 0.6030
1792/4566 [==========>...................] - ETA: 5:02 - loss: 0.6675 - acc: 0.6004
1856/4566 [===========>..................] - ETA: 4:52 - loss: 0.6680 - acc: 0.5986
1920/4566 [===========>..................] - ETA: 4:44 - loss: 0.6679 - acc: 0.5979
1984/4566 [============>.................] - ETA: 4:45 - loss: 0.6674 - acc: 0.5973
2048/4566 [============>.................] - ETA: 4:45 - loss: 0.6699 - acc: 0.5918
2112/4566 [============>.................] - ETA: 4:44 - loss: 0.6701 - acc: 0.5904
2176/4566 [=============>................] - ETA: 4:43 - loss: 0.6722 - acc: 0.5878
2240/4566 [=============>................] - ETA: 4:40 - loss: 0.6725 - acc: 0.5884
2304/4566 [==============>...............] - ETA: 4:37 - loss: 0.6740 - acc: 0.5868
2368/4566 [==============>...............] - ETA: 4:31 - loss: 0.6736 - acc: 0.5866
2432/4566 [==============>...............] - ETA: 4:20 - loss: 0.6729 - acc: 0.5868
2496/4566 [===============>..............] - ETA: 4:10 - loss: 0.6714 - acc: 0.5893
2560/4566 [===============>..............] - ETA: 4:01 - loss: 0.6721 - acc: 0.5879
2624/4566 [================>.............] - ETA: 3:51 - loss: 0.6721 - acc: 0.5869
2688/4566 [================>.............] - ETA: 3:42 - loss: 0.6715 - acc: 0.5874
2752/4566 [=================>............] - ETA: 3:32 - loss: 0.6709 - acc: 0.5887
2816/4566 [=================>............] - ETA: 3:23 - loss: 0.6711 - acc: 0.5884
2880/4566 [=================>............] - ETA: 3:14 - loss: 0.6718 - acc: 0.5882
2944/4566 [==================>...........] - ETA: 3:06 - loss: 0.6735 - acc: 0.5856
3008/4566 [==================>...........] - ETA: 2:57 - loss: 0.6734 - acc: 0.5861
3072/4566 [===================>..........] - ETA: 2:49 - loss: 0.6739 - acc: 0.5840
3136/4566 [===================>..........] - ETA: 2:40 - loss: 0.6743 - acc: 0.5832
3200/4566 [====================>.........] - ETA: 2:32 - loss: 0.6741 - acc: 0.5837
3264/4566 [====================>.........] - ETA: 2:25 - loss: 0.6745 - acc: 0.5824
3328/4566 [====================>.........] - ETA: 2:17 - loss: 0.6741 - acc: 0.5826
3392/4566 [=====================>........] - ETA: 2:09 - loss: 0.6750 - acc: 0.5805
3456/4566 [=====================>........] - ETA: 2:04 - loss: 0.6758 - acc: 0.5796
3520/4566 [======================>.......] - ETA: 1:58 - loss: 0.6763 - acc: 0.5778
3584/4566 [======================>.......] - ETA: 1:53 - loss: 0.6763 - acc: 0.5784
3648/4566 [======================>.......] - ETA: 1:47 - loss: 0.6763 - acc: 0.5789
3712/4566 [=======================>......] - ETA: 1:40 - loss: 0.6765 - acc: 0.5787
3776/4566 [=======================>......] - ETA: 1:34 - loss: 0.6768 - acc: 0.5779
3840/4566 [========================>.....] - ETA: 1:27 - loss: 0.6770 - acc: 0.5779
3904/4566 [========================>.....] - ETA: 1:19 - loss: 0.6770 - acc: 0.5781
3968/4566 [=========================>....] - ETA: 1:11 - loss: 0.6775 - acc: 0.5769
4032/4566 [=========================>....] - ETA: 1:03 - loss: 0.6767 - acc: 0.5779
4096/4566 [=========================>....] - ETA: 55s - loss: 0.6769 - acc: 0.5774 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6771 - acc: 0.5774
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6770 - acc: 0.5779
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6779 - acc: 0.5751
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6786 - acc: 0.5735
4416/4566 [============================>.] - ETA: 17s - loss: 0.6796 - acc: 0.5713
4480/4566 [============================>.] - ETA: 9s - loss: 0.6801 - acc: 0.5690 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6802 - acc: 0.5684
4566/4566 [==============================] - 534s 117ms/step - loss: 0.6801 - acc: 0.5696 - val_loss: 0.7105 - val_acc: 0.5039

Epoch 00007: val_acc did not improve from 0.58858
Epoch 8/10

  64/4566 [..............................] - ETA: 6:05 - loss: 0.6423 - acc: 0.6875
 128/4566 [..............................] - ETA: 5:58 - loss: 0.6543 - acc: 0.6328
 192/4566 [>.............................] - ETA: 6:29 - loss: 0.6665 - acc: 0.5781
 256/4566 [>.............................] - ETA: 8:05 - loss: 0.6723 - acc: 0.5820
 320/4566 [=>............................] - ETA: 9:08 - loss: 0.6751 - acc: 0.5938
 384/4566 [=>............................] - ETA: 9:49 - loss: 0.6733 - acc: 0.5938
 448/4566 [=>............................] - ETA: 10:09 - loss: 0.6743 - acc: 0.5826
 512/4566 [==>...........................] - ETA: 10:24 - loss: 0.6734 - acc: 0.5859
 576/4566 [==>...........................] - ETA: 10:37 - loss: 0.6729 - acc: 0.5833
 640/4566 [===>..........................] - ETA: 10:18 - loss: 0.6723 - acc: 0.5766
 704/4566 [===>..........................] - ETA: 9:46 - loss: 0.6726 - acc: 0.5767 
 768/4566 [====>.........................] - ETA: 9:13 - loss: 0.6728 - acc: 0.5781
 832/4566 [====>.........................] - ETA: 8:44 - loss: 0.6745 - acc: 0.5757
 896/4566 [====>.........................] - ETA: 8:20 - loss: 0.6747 - acc: 0.5703
 960/4566 [=====>........................] - ETA: 7:58 - loss: 0.6744 - acc: 0.5719
1024/4566 [=====>........................] - ETA: 7:37 - loss: 0.6742 - acc: 0.5703
1088/4566 [======>.......................] - ETA: 7:20 - loss: 0.6759 - acc: 0.5662
1152/4566 [======>.......................] - ETA: 7:03 - loss: 0.6755 - acc: 0.5660
1216/4566 [======>.......................] - ETA: 6:46 - loss: 0.6755 - acc: 0.5658
1280/4566 [=======>......................] - ETA: 6:31 - loss: 0.6750 - acc: 0.5719
1344/4566 [=======>......................] - ETA: 6:17 - loss: 0.6735 - acc: 0.5759
1408/4566 [========>.....................] - ETA: 6:06 - loss: 0.6730 - acc: 0.5746
1472/4566 [========>.....................] - ETA: 5:53 - loss: 0.6733 - acc: 0.5747
1536/4566 [=========>....................] - ETA: 5:42 - loss: 0.6738 - acc: 0.5749
1600/4566 [=========>....................] - ETA: 5:31 - loss: 0.6733 - acc: 0.5775
1664/4566 [=========>....................] - ETA: 5:21 - loss: 0.6740 - acc: 0.5769
1728/4566 [==========>...................] - ETA: 5:11 - loss: 0.6750 - acc: 0.5747
1792/4566 [==========>...................] - ETA: 5:12 - loss: 0.6758 - acc: 0.5725
1856/4566 [===========>..................] - ETA: 5:14 - loss: 0.6756 - acc: 0.5760
1920/4566 [===========>..................] - ETA: 5:14 - loss: 0.6756 - acc: 0.5760
1984/4566 [============>.................] - ETA: 5:14 - loss: 0.6771 - acc: 0.5721
2048/4566 [============>.................] - ETA: 5:12 - loss: 0.6774 - acc: 0.5708
2112/4566 [============>.................] - ETA: 5:10 - loss: 0.6778 - acc: 0.5696
2176/4566 [=============>................] - ETA: 5:03 - loss: 0.6773 - acc: 0.5708
2240/4566 [=============>................] - ETA: 4:51 - loss: 0.6773 - acc: 0.5705
2304/4566 [==============>...............] - ETA: 4:41 - loss: 0.6770 - acc: 0.5712
2368/4566 [==============>...............] - ETA: 4:30 - loss: 0.6780 - acc: 0.5684
2432/4566 [==============>...............] - ETA: 4:19 - loss: 0.6774 - acc: 0.5707
2496/4566 [===============>..............] - ETA: 4:10 - loss: 0.6789 - acc: 0.5685
2560/4566 [===============>..............] - ETA: 4:00 - loss: 0.6788 - acc: 0.5684
2624/4566 [================>.............] - ETA: 3:50 - loss: 0.6780 - acc: 0.5709
2688/4566 [================>.............] - ETA: 3:41 - loss: 0.6781 - acc: 0.5725
2752/4566 [=================>............] - ETA: 3:32 - loss: 0.6776 - acc: 0.5745
2816/4566 [=================>............] - ETA: 3:23 - loss: 0.6768 - acc: 0.5749
2880/4566 [=================>............] - ETA: 3:14 - loss: 0.6777 - acc: 0.5733
2944/4566 [==================>...........] - ETA: 3:05 - loss: 0.6770 - acc: 0.5754
3008/4566 [==================>...........] - ETA: 2:57 - loss: 0.6783 - acc: 0.5738
3072/4566 [===================>..........] - ETA: 2:48 - loss: 0.6778 - acc: 0.5739
3136/4566 [===================>..........] - ETA: 2:40 - loss: 0.6777 - acc: 0.5746
3200/4566 [====================>.........] - ETA: 2:32 - loss: 0.6778 - acc: 0.5737
3264/4566 [====================>.........] - ETA: 2:24 - loss: 0.6776 - acc: 0.5735
3328/4566 [====================>.........] - ETA: 2:19 - loss: 0.6777 - acc: 0.5727
3392/4566 [=====================>........] - ETA: 2:14 - loss: 0.6775 - acc: 0.5728
3456/4566 [=====================>........] - ETA: 2:08 - loss: 0.6773 - acc: 0.5732
3520/4566 [======================>.......] - ETA: 2:02 - loss: 0.6778 - acc: 0.5727
3584/4566 [======================>.......] - ETA: 1:56 - loss: 0.6790 - acc: 0.5700
3648/4566 [======================>.......] - ETA: 1:50 - loss: 0.6793 - acc: 0.5710
3712/4566 [=======================>......] - ETA: 1:43 - loss: 0.6794 - acc: 0.5714
3776/4566 [=======================>......] - ETA: 1:35 - loss: 0.6789 - acc: 0.5726
3840/4566 [========================>.....] - ETA: 1:26 - loss: 0.6787 - acc: 0.5719
3904/4566 [========================>.....] - ETA: 1:18 - loss: 0.6794 - acc: 0.5697
3968/4566 [=========================>....] - ETA: 1:10 - loss: 0.6793 - acc: 0.5706
4032/4566 [=========================>....] - ETA: 1:02 - loss: 0.6791 - acc: 0.5699
4096/4566 [=========================>....] - ETA: 54s - loss: 0.6789 - acc: 0.5696 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6784 - acc: 0.5714
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6782 - acc: 0.5720
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6773 - acc: 0.5735
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6772 - acc: 0.5747
4416/4566 [============================>.] - ETA: 17s - loss: 0.6772 - acc: 0.5747
4480/4566 [============================>.] - ETA: 9s - loss: 0.6774 - acc: 0.5746 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6773 - acc: 0.5755
4566/4566 [==============================] - 531s 116ms/step - loss: 0.6769 - acc: 0.5760 - val_loss: 0.6675 - val_acc: 0.5984

Epoch 00008: val_acc improved from 0.58858 to 0.59843, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window02/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 9/10

  64/4566 [..............................] - ETA: 7:31 - loss: 0.6746 - acc: 0.6094
 128/4566 [..............................] - ETA: 11:10 - loss: 0.6694 - acc: 0.5938
 192/4566 [>.............................] - ETA: 12:13 - loss: 0.6833 - acc: 0.5729
 256/4566 [>.............................] - ETA: 12:29 - loss: 0.6781 - acc: 0.5781
 320/4566 [=>............................] - ETA: 12:51 - loss: 0.6855 - acc: 0.5687
 384/4566 [=>............................] - ETA: 12:52 - loss: 0.6858 - acc: 0.5703
 448/4566 [=>............................] - ETA: 12:45 - loss: 0.6883 - acc: 0.5670
 512/4566 [==>...........................] - ETA: 12:07 - loss: 0.6854 - acc: 0.5801
 576/4566 [==>...........................] - ETA: 11:13 - loss: 0.6859 - acc: 0.5799
 640/4566 [===>..........................] - ETA: 10:26 - loss: 0.6833 - acc: 0.5766
 704/4566 [===>..........................] - ETA: 9:49 - loss: 0.6874 - acc: 0.5682 
 768/4566 [====>.........................] - ETA: 9:17 - loss: 0.6853 - acc: 0.5716
 832/4566 [====>.........................] - ETA: 8:48 - loss: 0.6880 - acc: 0.5601
 896/4566 [====>.........................] - ETA: 8:22 - loss: 0.6848 - acc: 0.5647
 960/4566 [=====>........................] - ETA: 8:02 - loss: 0.6823 - acc: 0.5687
1024/4566 [=====>........................] - ETA: 7:42 - loss: 0.6795 - acc: 0.5713
1088/4566 [======>.......................] - ETA: 7:23 - loss: 0.6793 - acc: 0.5680
1152/4566 [======>.......................] - ETA: 7:07 - loss: 0.6784 - acc: 0.5729
1216/4566 [======>.......................] - ETA: 6:52 - loss: 0.6806 - acc: 0.5707
1280/4566 [=======>......................] - ETA: 6:36 - loss: 0.6797 - acc: 0.5734
1344/4566 [=======>......................] - ETA: 6:21 - loss: 0.6817 - acc: 0.5707
1408/4566 [========>.....................] - ETA: 6:09 - loss: 0.6812 - acc: 0.5668
1472/4566 [========>.....................] - ETA: 5:57 - loss: 0.6816 - acc: 0.5673
1536/4566 [=========>....................] - ETA: 5:46 - loss: 0.6822 - acc: 0.5671
1600/4566 [=========>....................] - ETA: 5:37 - loss: 0.6821 - acc: 0.5669
1664/4566 [=========>....................] - ETA: 5:39 - loss: 0.6837 - acc: 0.5631
1728/4566 [==========>...................] - ETA: 5:40 - loss: 0.6824 - acc: 0.5648
1792/4566 [==========>...................] - ETA: 5:40 - loss: 0.6830 - acc: 0.5642
1856/4566 [===========>..................] - ETA: 5:39 - loss: 0.6832 - acc: 0.5641
1920/4566 [===========>..................] - ETA: 5:36 - loss: 0.6838 - acc: 0.5625
1984/4566 [============>.................] - ETA: 5:35 - loss: 0.6849 - acc: 0.5610
2048/4566 [============>.................] - ETA: 5:27 - loss: 0.6857 - acc: 0.5581
2112/4566 [============>.................] - ETA: 5:15 - loss: 0.6859 - acc: 0.5568
2176/4566 [=============>................] - ETA: 5:03 - loss: 0.6850 - acc: 0.5584
2240/4566 [=============>................] - ETA: 4:51 - loss: 0.6849 - acc: 0.5585
2304/4566 [==============>...............] - ETA: 4:40 - loss: 0.6848 - acc: 0.5577
2368/4566 [==============>...............] - ETA: 4:29 - loss: 0.6841 - acc: 0.5587
2432/4566 [==============>...............] - ETA: 4:19 - loss: 0.6833 - acc: 0.5600
2496/4566 [===============>..............] - ETA: 4:09 - loss: 0.6837 - acc: 0.5605
2560/4566 [===============>..............] - ETA: 3:59 - loss: 0.6828 - acc: 0.5629
2624/4566 [================>.............] - ETA: 3:50 - loss: 0.6828 - acc: 0.5644
2688/4566 [================>.............] - ETA: 3:40 - loss: 0.6835 - acc: 0.5632
2752/4566 [=================>............] - ETA: 3:31 - loss: 0.6834 - acc: 0.5610
2816/4566 [=================>............] - ETA: 3:22 - loss: 0.6825 - acc: 0.5629
2880/4566 [=================>............] - ETA: 3:14 - loss: 0.6828 - acc: 0.5622
2944/4566 [==================>...........] - ETA: 3:05 - loss: 0.6814 - acc: 0.5652
3008/4566 [==================>...........] - ETA: 2:56 - loss: 0.6811 - acc: 0.5665
3072/4566 [===================>..........] - ETA: 2:48 - loss: 0.6814 - acc: 0.5661
3136/4566 [===================>..........] - ETA: 2:40 - loss: 0.6820 - acc: 0.5647
3200/4566 [====================>.........] - ETA: 2:33 - loss: 0.6814 - acc: 0.5663
3264/4566 [====================>.........] - ETA: 2:29 - loss: 0.6817 - acc: 0.5671
3328/4566 [====================>.........] - ETA: 2:23 - loss: 0.6821 - acc: 0.5667
3392/4566 [=====================>........] - ETA: 2:17 - loss: 0.6809 - acc: 0.5681
3456/4566 [=====================>........] - ETA: 2:11 - loss: 0.6809 - acc: 0.5674
3520/4566 [======================>.......] - ETA: 2:05 - loss: 0.6806 - acc: 0.5673
3584/4566 [======================>.......] - ETA: 1:59 - loss: 0.6805 - acc: 0.5672
3648/4566 [======================>.......] - ETA: 1:51 - loss: 0.6800 - acc: 0.5683
3712/4566 [=======================>......] - ETA: 1:42 - loss: 0.6797 - acc: 0.5684
3776/4566 [=======================>......] - ETA: 1:34 - loss: 0.6796 - acc: 0.5691
3840/4566 [========================>.....] - ETA: 1:26 - loss: 0.6797 - acc: 0.5677
3904/4566 [========================>.....] - ETA: 1:18 - loss: 0.6788 - acc: 0.5686
3968/4566 [=========================>....] - ETA: 1:10 - loss: 0.6787 - acc: 0.5693
4032/4566 [=========================>....] - ETA: 1:02 - loss: 0.6783 - acc: 0.5704
4096/4566 [=========================>....] - ETA: 54s - loss: 0.6780 - acc: 0.5713 
4160/4566 [==========================>...] - ETA: 46s - loss: 0.6775 - acc: 0.5716
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6776 - acc: 0.5708
4288/4566 [===========================>..] - ETA: 31s - loss: 0.6779 - acc: 0.5700
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6774 - acc: 0.5705
4416/4566 [============================>.] - ETA: 17s - loss: 0.6778 - acc: 0.5711
4480/4566 [============================>.] - ETA: 9s - loss: 0.6785 - acc: 0.5694 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6782 - acc: 0.5702
4566/4566 [==============================] - 530s 116ms/step - loss: 0.6781 - acc: 0.5699 - val_loss: 0.6720 - val_acc: 0.5728

Epoch 00009: val_acc did not improve from 0.59843
Epoch 10/10

  64/4566 [..............................] - ETA: 14:37 - loss: 0.7059 - acc: 0.5000
 128/4566 [..............................] - ETA: 14:54 - loss: 0.6802 - acc: 0.5469
 192/4566 [>.............................] - ETA: 14:28 - loss: 0.6755 - acc: 0.5521
 256/4566 [>.............................] - ETA: 14:16 - loss: 0.6725 - acc: 0.5703
 320/4566 [=>............................] - ETA: 14:02 - loss: 0.6796 - acc: 0.5625
 384/4566 [=>............................] - ETA: 13:48 - loss: 0.6734 - acc: 0.5755
 448/4566 [=>............................] - ETA: 12:52 - loss: 0.6720 - acc: 0.5737
 512/4566 [==>...........................] - ETA: 11:46 - loss: 0.6761 - acc: 0.5605
 576/4566 [==>...........................] - ETA: 10:52 - loss: 0.6765 - acc: 0.5660
 640/4566 [===>..........................] - ETA: 10:08 - loss: 0.6745 - acc: 0.5719
 704/4566 [===>..........................] - ETA: 9:30 - loss: 0.6728 - acc: 0.5767 
 768/4566 [====>.........................] - ETA: 8:57 - loss: 0.6711 - acc: 0.5833
 832/4566 [====>.........................] - ETA: 8:28 - loss: 0.6715 - acc: 0.5817
 896/4566 [====>.........................] - ETA: 8:03 - loss: 0.6755 - acc: 0.5759
 960/4566 [=====>........................] - ETA: 7:41 - loss: 0.6739 - acc: 0.5781
1024/4566 [=====>........................] - ETA: 7:23 - loss: 0.6706 - acc: 0.5869
1088/4566 [======>.......................] - ETA: 7:04 - loss: 0.6709 - acc: 0.5901
1152/4566 [======>.......................] - ETA: 6:48 - loss: 0.6738 - acc: 0.5885
1216/4566 [======>.......................] - ETA: 6:34 - loss: 0.6749 - acc: 0.5872
1280/4566 [=======>......................] - ETA: 6:20 - loss: 0.6751 - acc: 0.5844
1344/4566 [=======>......................] - ETA: 6:06 - loss: 0.6762 - acc: 0.5804
1408/4566 [========>.....................] - ETA: 5:53 - loss: 0.6756 - acc: 0.5817
1472/4566 [========>.....................] - ETA: 5:41 - loss: 0.6750 - acc: 0.5842
1536/4566 [=========>....................] - ETA: 5:30 - loss: 0.6764 - acc: 0.5827
1600/4566 [=========>....................] - ETA: 5:29 - loss: 0.6750 - acc: 0.5856
1664/4566 [=========>....................] - ETA: 5:33 - loss: 0.6744 - acc: 0.5853
1728/4566 [==========>...................] - ETA: 5:35 - loss: 0.6743 - acc: 0.5839
1792/4566 [==========>...................] - ETA: 5:35 - loss: 0.6738 - acc: 0.5815
1856/4566 [===========>..................] - ETA: 5:35 - loss: 0.6735 - acc: 0.5792
1920/4566 [===========>..................] - ETA: 5:34 - loss: 0.6732 - acc: 0.5781
1984/4566 [============>.................] - ETA: 5:31 - loss: 0.6732 - acc: 0.5776
2048/4566 [============>.................] - ETA: 5:19 - loss: 0.6731 - acc: 0.5767
2112/4566 [============>.................] - ETA: 5:09 - loss: 0.6734 - acc: 0.5762
2176/4566 [=============>................] - ETA: 4:57 - loss: 0.6723 - acc: 0.5781
2240/4566 [=============>................] - ETA: 4:46 - loss: 0.6713 - acc: 0.5808
2304/4566 [==============>...............] - ETA: 4:36 - loss: 0.6721 - acc: 0.5786
2368/4566 [==============>...............] - ETA: 4:27 - loss: 0.6720 - acc: 0.5773
2432/4566 [==============>...............] - ETA: 4:17 - loss: 0.6718 - acc: 0.5777
2496/4566 [===============>..............] - ETA: 4:07 - loss: 0.6710 - acc: 0.5801
2560/4566 [===============>..............] - ETA: 3:58 - loss: 0.6716 - acc: 0.5805
2624/4566 [================>.............] - ETA: 3:48 - loss: 0.6720 - acc: 0.5804
2688/4566 [================>.............] - ETA: 3:39 - loss: 0.6733 - acc: 0.5785
2752/4566 [=================>............] - ETA: 3:30 - loss: 0.6728 - acc: 0.5799
2816/4566 [=================>............] - ETA: 3:21 - loss: 0.6713 - acc: 0.5824
2880/4566 [=================>............] - ETA: 3:12 - loss: 0.6714 - acc: 0.5830
2944/4566 [==================>...........] - ETA: 3:03 - loss: 0.6711 - acc: 0.5839
3008/4566 [==================>...........] - ETA: 2:55 - loss: 0.6710 - acc: 0.5834
3072/4566 [===================>..........] - ETA: 2:47 - loss: 0.6721 - acc: 0.5820
3136/4566 [===================>..........] - ETA: 2:41 - loss: 0.6740 - acc: 0.5788
3200/4566 [====================>.........] - ETA: 2:36 - loss: 0.6752 - acc: 0.5772
3264/4566 [====================>.........] - ETA: 2:31 - loss: 0.6760 - acc: 0.5754
3328/4566 [====================>.........] - ETA: 2:25 - loss: 0.6770 - acc: 0.5733
3392/4566 [=====================>........] - ETA: 2:19 - loss: 0.6773 - acc: 0.5743
3456/4566 [=====================>........] - ETA: 2:13 - loss: 0.6772 - acc: 0.5758
3520/4566 [======================>.......] - ETA: 2:07 - loss: 0.6775 - acc: 0.5747
3584/4566 [======================>.......] - ETA: 1:59 - loss: 0.6777 - acc: 0.5753
3648/4566 [======================>.......] - ETA: 1:50 - loss: 0.6776 - acc: 0.5743
3712/4566 [=======================>......] - ETA: 1:42 - loss: 0.6769 - acc: 0.5760
3776/4566 [=======================>......] - ETA: 1:34 - loss: 0.6768 - acc: 0.5771
3840/4566 [========================>.....] - ETA: 1:26 - loss: 0.6764 - acc: 0.5773
3904/4566 [========================>.....] - ETA: 1:18 - loss: 0.6768 - acc: 0.5753
3968/4566 [=========================>....] - ETA: 1:10 - loss: 0.6770 - acc: 0.5743
4032/4566 [=========================>....] - ETA: 1:02 - loss: 0.6779 - acc: 0.5722
4096/4566 [=========================>....] - ETA: 54s - loss: 0.6778 - acc: 0.5718 
4160/4566 [==========================>...] - ETA: 46s - loss: 0.6774 - acc: 0.5726
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6774 - acc: 0.5729
4288/4566 [===========================>..] - ETA: 31s - loss: 0.6777 - acc: 0.5723
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6778 - acc: 0.5715
4416/4566 [============================>.] - ETA: 17s - loss: 0.6776 - acc: 0.5720
4480/4566 [============================>.] - ETA: 9s - loss: 0.6779 - acc: 0.5717 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6775 - acc: 0.5726
4566/4566 [==============================] - 541s 118ms/step - loss: 0.6775 - acc: 0.5723 - val_loss: 0.6649 - val_acc: 0.5965

Epoch 00010: val_acc did not improve from 0.59843
Saved model to disk
