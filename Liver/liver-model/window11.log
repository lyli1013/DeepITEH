nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd15de58490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd15de58490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd1cbc90990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd1cbc90990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbdb23d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbdb23d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1cbd802d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1cbd802d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15dd467d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15dd467d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbda4450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbda4450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15dd7aad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15dd7aad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15ddb8ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15ddb8ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd15dcaf090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd15dcaf090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15da9d990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15da9d990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15dbb22d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15dbb22d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15dcaf350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15dcaf350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15db327d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15db327d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1cbd836d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1cbd836d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15d8a2490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15d8a2490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15da93bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15da93bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15db17050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15db17050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d635bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d635bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd15d584e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd15d584e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15d45fe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15d45fe50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d4c0810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d4c0810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15d584a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15d584a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d461250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d461250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd15ddb8b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd15ddb8b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15d0fdd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15d0fdd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d4e76d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d4e76d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15db43310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15db43310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d100590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d100590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd15d219790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd15d219790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd154e19a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd154e19a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d017210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd15d017210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15d17c3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15d17c3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd154e02810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd154e02810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd154bcc650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd154bcc650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd154aba650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd154aba650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd154e97d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd154e97d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15d0fd3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd15d0fd3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1549e7d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1549e7d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd15492ea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd15492ea90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15478ff50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd15478ff50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1548e9350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1548e9350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1549e1950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1549e1950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1547d5490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1547d5490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd14c626f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd14c626f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd14c4eaed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd14c4eaed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd14c5e4ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd14c5e4ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd14c6260d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd14c6260d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd14c4f3b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd14c4f3b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd14c2baad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd14c2baad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd14c2922d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd14c2922d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd14c4d8490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd14c4d8490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd14c4ef2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd14c4ef2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd14c17a610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd14c17a610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd14c0401d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd14c0401d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd143e58e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd143e58e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd143f41210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd143f41210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd14c2242d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd14c2242d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd14c193190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd14c193190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd143d59250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd143d59250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd143b09a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd143b09a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd143b41410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd143b41410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd143d59090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd143d59090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd143c4c590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd143c4c590>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-18 22:14:34.996804: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-18 22:14:35.080726: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-18 22:14:35.135503: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e5e49c5b90 executing computations on platform Host. Devices:
2022-11-18 22:14:35.135724: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-18 22:14:35.696657: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window11.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 55:53 - loss: 0.7336 - acc: 0.5156
 128/4566 [..............................] - ETA: 43:57 - loss: 0.9054 - acc: 0.4844
 192/4566 [>.............................] - ETA: 42:53 - loss: 0.8946 - acc: 0.4792
 256/4566 [>.............................] - ETA: 37:50 - loss: 0.8457 - acc: 0.5000
 320/4566 [=>............................] - ETA: 33:14 - loss: 0.8255 - acc: 0.5000
 384/4566 [=>............................] - ETA: 29:34 - loss: 0.8188 - acc: 0.4922
 448/4566 [=>............................] - ETA: 26:36 - loss: 0.8168 - acc: 0.4844
 512/4566 [==>...........................] - ETA: 24:27 - loss: 0.8036 - acc: 0.4941
 576/4566 [==>...........................] - ETA: 22:51 - loss: 0.8044 - acc: 0.4809
 640/4566 [===>..........................] - ETA: 21:33 - loss: 0.7938 - acc: 0.4859
 704/4566 [===>..........................] - ETA: 20:16 - loss: 0.7881 - acc: 0.4915
 768/4566 [====>.........................] - ETA: 19:18 - loss: 0.7836 - acc: 0.4935
 832/4566 [====>.........................] - ETA: 18:36 - loss: 0.7841 - acc: 0.4904
 896/4566 [====>.........................] - ETA: 17:44 - loss: 0.7785 - acc: 0.4978
 960/4566 [=====>........................] - ETA: 16:58 - loss: 0.7740 - acc: 0.5010
1024/4566 [=====>........................] - ETA: 16:20 - loss: 0.7686 - acc: 0.5039
1088/4566 [======>.......................] - ETA: 15:57 - loss: 0.7675 - acc: 0.5046
1152/4566 [======>.......................] - ETA: 15:35 - loss: 0.7629 - acc: 0.5122
1216/4566 [======>.......................] - ETA: 15:16 - loss: 0.7585 - acc: 0.5156
1280/4566 [=======>......................] - ETA: 14:54 - loss: 0.7552 - acc: 0.5180
1344/4566 [=======>......................] - ETA: 14:32 - loss: 0.7529 - acc: 0.5164
1408/4566 [========>.....................] - ETA: 14:08 - loss: 0.7494 - acc: 0.5206
1472/4566 [========>.....................] - ETA: 13:37 - loss: 0.7466 - acc: 0.5238
1536/4566 [=========>....................] - ETA: 13:07 - loss: 0.7445 - acc: 0.5247
1600/4566 [=========>....................] - ETA: 12:38 - loss: 0.7442 - acc: 0.5225
1664/4566 [=========>....................] - ETA: 12:11 - loss: 0.7436 - acc: 0.5210
1728/4566 [==========>...................] - ETA: 11:44 - loss: 0.7431 - acc: 0.5174
1792/4566 [==========>...................] - ETA: 11:19 - loss: 0.7411 - acc: 0.5190
1856/4566 [===========>..................] - ETA: 10:55 - loss: 0.7397 - acc: 0.5194
1920/4566 [===========>..................] - ETA: 10:31 - loss: 0.7373 - acc: 0.5229
1984/4566 [============>.................] - ETA: 10:10 - loss: 0.7369 - acc: 0.5232
2048/4566 [============>.................] - ETA: 9:49 - loss: 0.7358 - acc: 0.5244 
2112/4566 [============>.................] - ETA: 9:28 - loss: 0.7358 - acc: 0.5223
2176/4566 [=============>................] - ETA: 9:07 - loss: 0.7339 - acc: 0.5221
2240/4566 [=============>................] - ETA: 8:48 - loss: 0.7334 - acc: 0.5205
2304/4566 [==============>...............] - ETA: 8:33 - loss: 0.7323 - acc: 0.5230
2368/4566 [==============>...............] - ETA: 8:19 - loss: 0.7311 - acc: 0.5236
2432/4566 [==============>...............] - ETA: 8:05 - loss: 0.7313 - acc: 0.5226
2496/4566 [===============>..............] - ETA: 7:51 - loss: 0.7298 - acc: 0.5256
2560/4566 [===============>..............] - ETA: 7:36 - loss: 0.7283 - acc: 0.5266
2624/4566 [================>.............] - ETA: 7:22 - loss: 0.7299 - acc: 0.5229
2688/4566 [================>.............] - ETA: 7:07 - loss: 0.7295 - acc: 0.5231
2752/4566 [=================>............] - ETA: 6:50 - loss: 0.7283 - acc: 0.5247
2816/4566 [=================>............] - ETA: 6:32 - loss: 0.7276 - acc: 0.5256
2880/4566 [=================>............] - ETA: 6:14 - loss: 0.7273 - acc: 0.5240
2944/4566 [==================>...........] - ETA: 5:57 - loss: 0.7259 - acc: 0.5258
3008/4566 [==================>...........] - ETA: 5:41 - loss: 0.7249 - acc: 0.5259
3072/4566 [===================>..........] - ETA: 5:24 - loss: 0.7248 - acc: 0.5247
3136/4566 [===================>..........] - ETA: 5:09 - loss: 0.7238 - acc: 0.5255
3200/4566 [====================>.........] - ETA: 4:53 - loss: 0.7231 - acc: 0.5250
3264/4566 [====================>.........] - ETA: 4:37 - loss: 0.7221 - acc: 0.5267
3328/4566 [====================>.........] - ETA: 4:22 - loss: 0.7216 - acc: 0.5273
3392/4566 [=====================>........] - ETA: 4:06 - loss: 0.7217 - acc: 0.5283
3456/4566 [=====================>........] - ETA: 3:51 - loss: 0.7218 - acc: 0.5269
3520/4566 [======================>.......] - ETA: 3:36 - loss: 0.7217 - acc: 0.5259
3584/4566 [======================>.......] - ETA: 3:21 - loss: 0.7216 - acc: 0.5246
3648/4566 [======================>.......] - ETA: 3:09 - loss: 0.7206 - acc: 0.5255
3712/4566 [=======================>......] - ETA: 2:56 - loss: 0.7202 - acc: 0.5253
3776/4566 [=======================>......] - ETA: 2:43 - loss: 0.7192 - acc: 0.5270
3840/4566 [========================>.....] - ETA: 2:30 - loss: 0.7188 - acc: 0.5273
3904/4566 [========================>.....] - ETA: 2:17 - loss: 0.7181 - acc: 0.5274
3968/4566 [=========================>....] - ETA: 2:04 - loss: 0.7174 - acc: 0.5290
4032/4566 [=========================>....] - ETA: 1:51 - loss: 0.7165 - acc: 0.5310
4096/4566 [=========================>....] - ETA: 1:37 - loss: 0.7168 - acc: 0.5295
4160/4566 [==========================>...] - ETA: 1:23 - loss: 0.7161 - acc: 0.5300
4224/4566 [==========================>...] - ETA: 1:10 - loss: 0.7155 - acc: 0.5310
4288/4566 [===========================>..] - ETA: 56s - loss: 0.7150 - acc: 0.5315 
4352/4566 [===========================>..] - ETA: 43s - loss: 0.7138 - acc: 0.5335
4416/4566 [============================>.] - ETA: 30s - loss: 0.7138 - acc: 0.5335
4480/4566 [============================>.] - ETA: 17s - loss: 0.7135 - acc: 0.5337
4544/4566 [============================>.] - ETA: 4s - loss: 0.7130 - acc: 0.5339 
4566/4566 [==============================] - 934s 205ms/step - loss: 0.7132 - acc: 0.5333 - val_loss: 0.6839 - val_acc: 0.5748

Epoch 00001: val_acc improved from -inf to 0.57480, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window11/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 8:28 - loss: 0.7029 - acc: 0.5469
 128/4566 [..............................] - ETA: 9:11 - loss: 0.6910 - acc: 0.5547
 192/4566 [>.............................] - ETA: 11:58 - loss: 0.6621 - acc: 0.5938
 256/4566 [>.............................] - ETA: 13:11 - loss: 0.6573 - acc: 0.6055
 320/4566 [=>............................] - ETA: 13:31 - loss: 0.6664 - acc: 0.5969
 384/4566 [=>............................] - ETA: 13:54 - loss: 0.6733 - acc: 0.5755
 448/4566 [=>............................] - ETA: 14:04 - loss: 0.6733 - acc: 0.5737
 512/4566 [==>...........................] - ETA: 13:54 - loss: 0.6780 - acc: 0.5645
 576/4566 [==>...........................] - ETA: 13:35 - loss: 0.6810 - acc: 0.5573
 640/4566 [===>..........................] - ETA: 12:47 - loss: 0.6847 - acc: 0.5547
 704/4566 [===>..........................] - ETA: 12:07 - loss: 0.6861 - acc: 0.5554
 768/4566 [====>.........................] - ETA: 11:34 - loss: 0.6862 - acc: 0.5573
 832/4566 [====>.........................] - ETA: 11:06 - loss: 0.6852 - acc: 0.5637
 896/4566 [====>.........................] - ETA: 10:39 - loss: 0.6834 - acc: 0.5692
 960/4566 [=====>........................] - ETA: 10:18 - loss: 0.6807 - acc: 0.5813
1024/4566 [=====>........................] - ETA: 9:55 - loss: 0.6809 - acc: 0.5801 
1088/4566 [======>.......................] - ETA: 9:32 - loss: 0.6869 - acc: 0.5717
1152/4566 [======>.......................] - ETA: 9:12 - loss: 0.6893 - acc: 0.5660
1216/4566 [======>.......................] - ETA: 8:52 - loss: 0.6883 - acc: 0.5674
1280/4566 [=======>......................] - ETA: 8:35 - loss: 0.6917 - acc: 0.5617
1344/4566 [=======>......................] - ETA: 8:21 - loss: 0.6923 - acc: 0.5640
1408/4566 [========>.....................] - ETA: 8:07 - loss: 0.6914 - acc: 0.5632
1472/4566 [========>.....................] - ETA: 7:54 - loss: 0.6883 - acc: 0.5686
1536/4566 [=========>....................] - ETA: 7:42 - loss: 0.6889 - acc: 0.5658
1600/4566 [=========>....................] - ETA: 7:40 - loss: 0.6881 - acc: 0.5663
1664/4566 [=========>....................] - ETA: 7:38 - loss: 0.6880 - acc: 0.5679
1728/4566 [==========>...................] - ETA: 7:35 - loss: 0.6882 - acc: 0.5671
1792/4566 [==========>...................] - ETA: 7:30 - loss: 0.6885 - acc: 0.5670
1856/4566 [===========>..................] - ETA: 7:25 - loss: 0.6884 - acc: 0.5663
1920/4566 [===========>..................] - ETA: 7:20 - loss: 0.6891 - acc: 0.5646
1984/4566 [============>.................] - ETA: 7:14 - loss: 0.6896 - acc: 0.5640
2048/4566 [============>.................] - ETA: 7:02 - loss: 0.6895 - acc: 0.5649
2112/4566 [============>.................] - ETA: 6:47 - loss: 0.6904 - acc: 0.5634
2176/4566 [=============>................] - ETA: 6:33 - loss: 0.6890 - acc: 0.5666
2240/4566 [=============>................] - ETA: 6:20 - loss: 0.6885 - acc: 0.5679
2304/4566 [==============>...............] - ETA: 6:06 - loss: 0.6886 - acc: 0.5664
2368/4566 [==============>...............] - ETA: 5:52 - loss: 0.6896 - acc: 0.5642
2432/4566 [==============>...............] - ETA: 5:40 - loss: 0.6894 - acc: 0.5650
2496/4566 [===============>..............] - ETA: 5:28 - loss: 0.6899 - acc: 0.5641
2560/4566 [===============>..............] - ETA: 5:15 - loss: 0.6899 - acc: 0.5625
2624/4566 [================>.............] - ETA: 5:03 - loss: 0.6893 - acc: 0.5640
2688/4566 [================>.............] - ETA: 4:52 - loss: 0.6885 - acc: 0.5636
2752/4566 [=================>............] - ETA: 4:41 - loss: 0.6883 - acc: 0.5625
2816/4566 [=================>............] - ETA: 4:29 - loss: 0.6887 - acc: 0.5621
2880/4566 [=================>............] - ETA: 4:18 - loss: 0.6892 - acc: 0.5604
2944/4566 [==================>...........] - ETA: 4:07 - loss: 0.6899 - acc: 0.5591
3008/4566 [==================>...........] - ETA: 3:58 - loss: 0.6900 - acc: 0.5575
3072/4566 [===================>..........] - ETA: 3:51 - loss: 0.6914 - acc: 0.5537
3136/4566 [===================>..........] - ETA: 3:43 - loss: 0.6920 - acc: 0.5529
3200/4566 [====================>.........] - ETA: 3:35 - loss: 0.6910 - acc: 0.5553
3264/4566 [====================>.........] - ETA: 3:27 - loss: 0.6907 - acc: 0.5558
3328/4566 [====================>.........] - ETA: 3:18 - loss: 0.6902 - acc: 0.5556
3392/4566 [=====================>........] - ETA: 3:09 - loss: 0.6895 - acc: 0.5563
3456/4566 [=====================>........] - ETA: 2:58 - loss: 0.6887 - acc: 0.5584
3520/4566 [======================>.......] - ETA: 2:46 - loss: 0.6877 - acc: 0.5616
3584/4566 [======================>.......] - ETA: 2:35 - loss: 0.6875 - acc: 0.5622
3648/4566 [======================>.......] - ETA: 2:24 - loss: 0.6867 - acc: 0.5633
3712/4566 [=======================>......] - ETA: 2:14 - loss: 0.6867 - acc: 0.5633
3776/4566 [=======================>......] - ETA: 2:03 - loss: 0.6863 - acc: 0.5644
3840/4566 [========================>.....] - ETA: 1:53 - loss: 0.6854 - acc: 0.5651
3904/4566 [========================>.....] - ETA: 1:42 - loss: 0.6851 - acc: 0.5653
3968/4566 [=========================>....] - ETA: 1:32 - loss: 0.6841 - acc: 0.5665
4032/4566 [=========================>....] - ETA: 1:22 - loss: 0.6841 - acc: 0.5665
4096/4566 [=========================>....] - ETA: 1:12 - loss: 0.6830 - acc: 0.5674
4160/4566 [==========================>...] - ETA: 1:02 - loss: 0.6825 - acc: 0.5683
4224/4566 [==========================>...] - ETA: 52s - loss: 0.6815 - acc: 0.5703 
4288/4566 [===========================>..] - ETA: 42s - loss: 0.6826 - acc: 0.5695
4352/4566 [===========================>..] - ETA: 32s - loss: 0.6823 - acc: 0.5699
4416/4566 [============================>.] - ETA: 22s - loss: 0.6827 - acc: 0.5682
4480/4566 [============================>.] - ETA: 13s - loss: 0.6825 - acc: 0.5690
4544/4566 [============================>.] - ETA: 3s - loss: 0.6822 - acc: 0.5695 
4566/4566 [==============================] - 746s 163ms/step - loss: 0.6823 - acc: 0.5703 - val_loss: 0.6903 - val_acc: 0.5630

Epoch 00002: val_acc did not improve from 0.57480
Epoch 3/10

  64/4566 [..............................] - ETA: 14:07 - loss: 0.7546 - acc: 0.5000
 128/4566 [..............................] - ETA: 10:58 - loss: 0.7126 - acc: 0.5625
 192/4566 [>.............................] - ETA: 10:01 - loss: 0.6981 - acc: 0.5625
 256/4566 [>.............................] - ETA: 9:25 - loss: 0.6924 - acc: 0.5625 
 320/4566 [=>............................] - ETA: 8:58 - loss: 0.6972 - acc: 0.5656
 384/4566 [=>............................] - ETA: 8:38 - loss: 0.6902 - acc: 0.5807
 448/4566 [=>............................] - ETA: 8:28 - loss: 0.6897 - acc: 0.5848
 512/4566 [==>...........................] - ETA: 8:13 - loss: 0.6888 - acc: 0.5801
 576/4566 [==>...........................] - ETA: 8:00 - loss: 0.6861 - acc: 0.5851
 640/4566 [===>..........................] - ETA: 7:47 - loss: 0.6866 - acc: 0.5859
 704/4566 [===>..........................] - ETA: 7:34 - loss: 0.6844 - acc: 0.5895
 768/4566 [====>.........................] - ETA: 7:21 - loss: 0.6808 - acc: 0.5898
 832/4566 [====>.........................] - ETA: 7:14 - loss: 0.6833 - acc: 0.5889
 896/4566 [====>.........................] - ETA: 7:07 - loss: 0.6810 - acc: 0.5904
 960/4566 [=====>........................] - ETA: 6:58 - loss: 0.6837 - acc: 0.5865
1024/4566 [=====>........................] - ETA: 6:47 - loss: 0.6792 - acc: 0.5938
1088/4566 [======>.......................] - ETA: 6:47 - loss: 0.6797 - acc: 0.5938
1152/4566 [======>.......................] - ETA: 6:56 - loss: 0.6781 - acc: 0.5955
1216/4566 [======>.......................] - ETA: 7:04 - loss: 0.6756 - acc: 0.5995
1280/4566 [=======>......................] - ETA: 7:08 - loss: 0.6734 - acc: 0.6008
1344/4566 [=======>......................] - ETA: 7:14 - loss: 0.6705 - acc: 0.6064
1408/4566 [========>.....................] - ETA: 7:15 - loss: 0.6693 - acc: 0.6072
1472/4566 [========>.....................] - ETA: 7:17 - loss: 0.6666 - acc: 0.6128
1536/4566 [=========>....................] - ETA: 7:12 - loss: 0.6654 - acc: 0.6146
1600/4566 [=========>....................] - ETA: 6:59 - loss: 0.6626 - acc: 0.6175
1664/4566 [=========>....................] - ETA: 6:46 - loss: 0.6641 - acc: 0.6178
1728/4566 [==========>...................] - ETA: 6:34 - loss: 0.6661 - acc: 0.6140
1792/4566 [==========>...................] - ETA: 6:22 - loss: 0.6668 - acc: 0.6138
1856/4566 [===========>..................] - ETA: 6:11 - loss: 0.6681 - acc: 0.6137
1920/4566 [===========>..................] - ETA: 6:00 - loss: 0.6673 - acc: 0.6141
1984/4566 [============>.................] - ETA: 5:49 - loss: 0.6680 - acc: 0.6134
2048/4566 [============>.................] - ETA: 5:38 - loss: 0.6672 - acc: 0.6133
2112/4566 [============>.................] - ETA: 5:28 - loss: 0.6693 - acc: 0.6103
2176/4566 [=============>................] - ETA: 5:17 - loss: 0.6668 - acc: 0.6117
2240/4566 [=============>................] - ETA: 5:06 - loss: 0.6677 - acc: 0.6107
2304/4566 [==============>...............] - ETA: 4:55 - loss: 0.6690 - acc: 0.6072
2368/4566 [==============>...............] - ETA: 4:45 - loss: 0.6679 - acc: 0.6094
2432/4566 [==============>...............] - ETA: 4:34 - loss: 0.6695 - acc: 0.6077
2496/4566 [===============>..............] - ETA: 4:25 - loss: 0.6703 - acc: 0.6050
2560/4566 [===============>..............] - ETA: 4:18 - loss: 0.6693 - acc: 0.6062
2624/4566 [================>.............] - ETA: 4:15 - loss: 0.6701 - acc: 0.6059
2688/4566 [================>.............] - ETA: 4:10 - loss: 0.6704 - acc: 0.6071
2752/4566 [=================>............] - ETA: 4:05 - loss: 0.6710 - acc: 0.6054
2816/4566 [=================>............] - ETA: 4:00 - loss: 0.6711 - acc: 0.6051
2880/4566 [=================>............] - ETA: 3:54 - loss: 0.6708 - acc: 0.6062
2944/4566 [==================>...........] - ETA: 3:48 - loss: 0.6700 - acc: 0.6070
3008/4566 [==================>...........] - ETA: 3:40 - loss: 0.6710 - acc: 0.6057
3072/4566 [===================>..........] - ETA: 3:31 - loss: 0.6708 - acc: 0.6068
3136/4566 [===================>..........] - ETA: 3:21 - loss: 0.6702 - acc: 0.6059
3200/4566 [====================>.........] - ETA: 3:11 - loss: 0.6681 - acc: 0.6091
3264/4566 [====================>.........] - ETA: 3:01 - loss: 0.6690 - acc: 0.6069
3328/4566 [====================>.........] - ETA: 2:51 - loss: 0.6682 - acc: 0.6085
3392/4566 [=====================>........] - ETA: 2:42 - loss: 0.6680 - acc: 0.6088
3456/4566 [=====================>........] - ETA: 2:32 - loss: 0.6680 - acc: 0.6085
3520/4566 [======================>.......] - ETA: 2:23 - loss: 0.6660 - acc: 0.6102
3584/4566 [======================>.......] - ETA: 2:13 - loss: 0.6653 - acc: 0.6119
3648/4566 [======================>.......] - ETA: 2:04 - loss: 0.6648 - acc: 0.6124
3712/4566 [=======================>......] - ETA: 1:55 - loss: 0.6640 - acc: 0.6129
3776/4566 [=======================>......] - ETA: 1:45 - loss: 0.6642 - acc: 0.6120
3840/4566 [========================>.....] - ETA: 1:37 - loss: 0.6637 - acc: 0.6122
3904/4566 [========================>.....] - ETA: 1:28 - loss: 0.6647 - acc: 0.6104
3968/4566 [=========================>....] - ETA: 1:19 - loss: 0.6644 - acc: 0.6091
4032/4566 [=========================>....] - ETA: 1:11 - loss: 0.6650 - acc: 0.6096
4096/4566 [=========================>....] - ETA: 1:03 - loss: 0.6654 - acc: 0.6079
4160/4566 [==========================>...] - ETA: 55s - loss: 0.6651 - acc: 0.6082 
4224/4566 [==========================>...] - ETA: 46s - loss: 0.6659 - acc: 0.6070
4288/4566 [===========================>..] - ETA: 38s - loss: 0.6659 - acc: 0.6066
4352/4566 [===========================>..] - ETA: 29s - loss: 0.6658 - acc: 0.6064
4416/4566 [============================>.] - ETA: 21s - loss: 0.6648 - acc: 0.6078
4480/4566 [============================>.] - ETA: 12s - loss: 0.6646 - acc: 0.6080
4544/4566 [============================>.] - ETA: 3s - loss: 0.6646 - acc: 0.6078 
4566/4566 [==============================] - 662s 145ms/step - loss: 0.6647 - acc: 0.6075 - val_loss: 0.6452 - val_acc: 0.6339

Epoch 00003: val_acc improved from 0.57480 to 0.63386, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window11/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 4/10

  64/4566 [..............................] - ETA: 7:43 - loss: 0.7219 - acc: 0.5156
 128/4566 [..............................] - ETA: 7:22 - loss: 0.6692 - acc: 0.5938
 192/4566 [>.............................] - ETA: 7:33 - loss: 0.6700 - acc: 0.6042
 256/4566 [>.............................] - ETA: 7:30 - loss: 0.6654 - acc: 0.6094
 320/4566 [=>............................] - ETA: 7:36 - loss: 0.6651 - acc: 0.6000
 384/4566 [=>............................] - ETA: 7:28 - loss: 0.6644 - acc: 0.6042
 448/4566 [=>............................] - ETA: 7:27 - loss: 0.6613 - acc: 0.6071
 512/4566 [==>...........................] - ETA: 7:21 - loss: 0.6542 - acc: 0.6094
 576/4566 [==>...........................] - ETA: 7:14 - loss: 0.6569 - acc: 0.6111
 640/4566 [===>..........................] - ETA: 7:07 - loss: 0.6592 - acc: 0.6109
 704/4566 [===>..........................] - ETA: 6:59 - loss: 0.6575 - acc: 0.6151
 768/4566 [====>.........................] - ETA: 6:53 - loss: 0.6594 - acc: 0.6081
 832/4566 [====>.........................] - ETA: 7:07 - loss: 0.6623 - acc: 0.6070
 896/4566 [====>.........................] - ETA: 7:24 - loss: 0.6606 - acc: 0.6116
 960/4566 [=====>........................] - ETA: 7:37 - loss: 0.6647 - acc: 0.6052
1024/4566 [=====>........................] - ETA: 7:49 - loss: 0.6671 - acc: 0.6074
1088/4566 [======>.......................] - ETA: 7:57 - loss: 0.6693 - acc: 0.6075
1152/4566 [======>.......................] - ETA: 8:03 - loss: 0.6699 - acc: 0.6024
1216/4566 [======>.......................] - ETA: 8:06 - loss: 0.6709 - acc: 0.5995
1280/4566 [=======>......................] - ETA: 7:57 - loss: 0.6693 - acc: 0.6039
1344/4566 [=======>......................] - ETA: 7:43 - loss: 0.6691 - acc: 0.6034
1408/4566 [========>.....................] - ETA: 7:28 - loss: 0.6666 - acc: 0.6051
1472/4566 [========>.....................] - ETA: 7:15 - loss: 0.6671 - acc: 0.6046
1536/4566 [=========>....................] - ETA: 7:03 - loss: 0.6672 - acc: 0.6048
1600/4566 [=========>....................] - ETA: 6:52 - loss: 0.6681 - acc: 0.6038
1664/4566 [=========>....................] - ETA: 6:39 - loss: 0.6657 - acc: 0.6070
1728/4566 [==========>...................] - ETA: 6:27 - loss: 0.6664 - acc: 0.6065
1792/4566 [==========>...................] - ETA: 6:16 - loss: 0.6656 - acc: 0.6094
1856/4566 [===========>..................] - ETA: 6:04 - loss: 0.6645 - acc: 0.6110
1920/4566 [===========>..................] - ETA: 5:54 - loss: 0.6626 - acc: 0.6146
1984/4566 [============>.................] - ETA: 5:44 - loss: 0.6632 - acc: 0.6149
2048/4566 [============>.................] - ETA: 5:33 - loss: 0.6619 - acc: 0.6167
2112/4566 [============>.................] - ETA: 5:23 - loss: 0.6624 - acc: 0.6155
2176/4566 [=============>................] - ETA: 5:13 - loss: 0.6618 - acc: 0.6153
2240/4566 [=============>................] - ETA: 5:03 - loss: 0.6608 - acc: 0.6156
2304/4566 [==============>...............] - ETA: 4:58 - loss: 0.6611 - acc: 0.6159
2368/4566 [==============>...............] - ETA: 4:54 - loss: 0.6618 - acc: 0.6157
2432/4566 [==============>...............] - ETA: 4:50 - loss: 0.6614 - acc: 0.6160
2496/4566 [===============>..............] - ETA: 4:45 - loss: 0.6612 - acc: 0.6170
2560/4566 [===============>..............] - ETA: 4:40 - loss: 0.6641 - acc: 0.6125
2624/4566 [================>.............] - ETA: 4:34 - loss: 0.6631 - acc: 0.6147
2688/4566 [================>.............] - ETA: 4:28 - loss: 0.6625 - acc: 0.6157
2752/4566 [=================>............] - ETA: 4:19 - loss: 0.6628 - acc: 0.6156
2816/4566 [=================>............] - ETA: 4:09 - loss: 0.6626 - acc: 0.6172
2880/4566 [=================>............] - ETA: 3:59 - loss: 0.6619 - acc: 0.6181
2944/4566 [==================>...........] - ETA: 3:49 - loss: 0.6616 - acc: 0.6182
3008/4566 [==================>...........] - ETA: 3:39 - loss: 0.6614 - acc: 0.6193
3072/4566 [===================>..........] - ETA: 3:29 - loss: 0.6600 - acc: 0.6217
3136/4566 [===================>..........] - ETA: 3:19 - loss: 0.6583 - acc: 0.6244
3200/4566 [====================>.........] - ETA: 3:09 - loss: 0.6589 - acc: 0.6234
3264/4566 [====================>.........] - ETA: 2:59 - loss: 0.6585 - acc: 0.6235
3328/4566 [====================>.........] - ETA: 2:50 - loss: 0.6588 - acc: 0.6232
3392/4566 [=====================>........] - ETA: 2:40 - loss: 0.6580 - acc: 0.6235
3456/4566 [=====================>........] - ETA: 2:31 - loss: 0.6582 - acc: 0.6230
3520/4566 [======================>.......] - ETA: 2:21 - loss: 0.6576 - acc: 0.6233
3584/4566 [======================>.......] - ETA: 2:12 - loss: 0.6580 - acc: 0.6225
3648/4566 [======================>.......] - ETA: 2:03 - loss: 0.6573 - acc: 0.6231
3712/4566 [=======================>......] - ETA: 1:54 - loss: 0.6585 - acc: 0.6231
3776/4566 [=======================>......] - ETA: 1:45 - loss: 0.6570 - acc: 0.6250
3840/4566 [========================>.....] - ETA: 1:38 - loss: 0.6563 - acc: 0.6263
3904/4566 [========================>.....] - ETA: 1:30 - loss: 0.6567 - acc: 0.6247
3968/4566 [=========================>....] - ETA: 1:22 - loss: 0.6567 - acc: 0.6242
4032/4566 [=========================>....] - ETA: 1:14 - loss: 0.6568 - acc: 0.6233
4096/4566 [=========================>....] - ETA: 1:06 - loss: 0.6562 - acc: 0.6233
4160/4566 [==========================>...] - ETA: 57s - loss: 0.6555 - acc: 0.6231 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6549 - acc: 0.6245
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6557 - acc: 0.6236
4352/4566 [===========================>..] - ETA: 30s - loss: 0.6557 - acc: 0.6243
4416/4566 [============================>.] - ETA: 21s - loss: 0.6560 - acc: 0.6239
4480/4566 [============================>.] - ETA: 12s - loss: 0.6552 - acc: 0.6248
4544/4566 [============================>.] - ETA: 3s - loss: 0.6547 - acc: 0.6250 
4566/4566 [==============================] - 656s 144ms/step - loss: 0.6541 - acc: 0.6257 - val_loss: 0.6771 - val_acc: 0.6181

Epoch 00004: val_acc did not improve from 0.63386
Epoch 5/10

  64/4566 [..............................] - ETA: 7:28 - loss: 0.6453 - acc: 0.6250
 128/4566 [..............................] - ETA: 7:19 - loss: 0.6399 - acc: 0.6328
 192/4566 [>.............................] - ETA: 7:08 - loss: 0.6504 - acc: 0.6146
 256/4566 [>.............................] - ETA: 7:03 - loss: 0.6426 - acc: 0.6211
 320/4566 [=>............................] - ETA: 7:03 - loss: 0.6464 - acc: 0.6281
 384/4566 [=>............................] - ETA: 7:05 - loss: 0.6439 - acc: 0.6354
 448/4566 [=>............................] - ETA: 7:12 - loss: 0.6602 - acc: 0.6161
 512/4566 [==>...........................] - ETA: 7:34 - loss: 0.6665 - acc: 0.6074
 576/4566 [==>...........................] - ETA: 8:16 - loss: 0.6643 - acc: 0.6024
 640/4566 [===>..........................] - ETA: 8:42 - loss: 0.6626 - acc: 0.6078
 704/4566 [===>..........................] - ETA: 9:05 - loss: 0.6610 - acc: 0.6151
 768/4566 [====>.........................] - ETA: 9:20 - loss: 0.6633 - acc: 0.6133
 832/4566 [====>.........................] - ETA: 9:26 - loss: 0.6649 - acc: 0.6070
 896/4566 [====>.........................] - ETA: 9:33 - loss: 0.6643 - acc: 0.6038
 960/4566 [=====>........................] - ETA: 9:28 - loss: 0.6666 - acc: 0.5958
1024/4566 [=====>........................] - ETA: 9:12 - loss: 0.6713 - acc: 0.5928
1088/4566 [======>.......................] - ETA: 8:51 - loss: 0.6690 - acc: 0.5928
1152/4566 [======>.......................] - ETA: 8:32 - loss: 0.6698 - acc: 0.5903
1216/4566 [======>.......................] - ETA: 8:13 - loss: 0.6679 - acc: 0.5938
1280/4566 [=======>......................] - ETA: 7:56 - loss: 0.6704 - acc: 0.5875
1344/4566 [=======>......................] - ETA: 7:39 - loss: 0.6696 - acc: 0.5871
1408/4566 [========>.....................] - ETA: 7:24 - loss: 0.6674 - acc: 0.5930
1472/4566 [========>.....................] - ETA: 7:09 - loss: 0.6655 - acc: 0.5958
1536/4566 [=========>....................] - ETA: 6:55 - loss: 0.6652 - acc: 0.5957
1600/4566 [=========>....................] - ETA: 6:42 - loss: 0.6644 - acc: 0.5950
1664/4566 [=========>....................] - ETA: 6:31 - loss: 0.6633 - acc: 0.5956
1728/4566 [==========>...................] - ETA: 6:20 - loss: 0.6627 - acc: 0.5966
1792/4566 [==========>...................] - ETA: 6:09 - loss: 0.6637 - acc: 0.5949
1856/4566 [===========>..................] - ETA: 5:59 - loss: 0.6614 - acc: 0.5986
1920/4566 [===========>..................] - ETA: 5:50 - loss: 0.6623 - acc: 0.5974
1984/4566 [============>.................] - ETA: 5:47 - loss: 0.6586 - acc: 0.6028
2048/4566 [============>.................] - ETA: 5:46 - loss: 0.6594 - acc: 0.6011
2112/4566 [============>.................] - ETA: 5:44 - loss: 0.6581 - acc: 0.6023
2176/4566 [=============>................] - ETA: 5:40 - loss: 0.6572 - acc: 0.6039
2240/4566 [=============>................] - ETA: 5:36 - loss: 0.6581 - acc: 0.6031
2304/4566 [==============>...............] - ETA: 5:30 - loss: 0.6584 - acc: 0.6020
2368/4566 [==============>...............] - ETA: 5:26 - loss: 0.6586 - acc: 0.6001
2432/4566 [==============>...............] - ETA: 5:17 - loss: 0.6592 - acc: 0.6003
2496/4566 [===============>..............] - ETA: 5:06 - loss: 0.6577 - acc: 0.6014
2560/4566 [===============>..............] - ETA: 4:54 - loss: 0.6567 - acc: 0.6031
2624/4566 [================>.............] - ETA: 4:42 - loss: 0.6575 - acc: 0.6025
2688/4566 [================>.............] - ETA: 4:30 - loss: 0.6579 - acc: 0.6031
2752/4566 [=================>............] - ETA: 4:19 - loss: 0.6585 - acc: 0.6021
2816/4566 [=================>............] - ETA: 4:09 - loss: 0.6585 - acc: 0.6037
2880/4566 [=================>............] - ETA: 3:59 - loss: 0.6579 - acc: 0.6049
2944/4566 [==================>...........] - ETA: 3:49 - loss: 0.6572 - acc: 0.6063
3008/4566 [==================>...........] - ETA: 3:39 - loss: 0.6554 - acc: 0.6090
3072/4566 [===================>..........] - ETA: 3:29 - loss: 0.6558 - acc: 0.6090
3136/4566 [===================>..........] - ETA: 3:19 - loss: 0.6547 - acc: 0.6103
3200/4566 [====================>.........] - ETA: 3:09 - loss: 0.6546 - acc: 0.6109
3264/4566 [====================>.........] - ETA: 2:59 - loss: 0.6543 - acc: 0.6115
3328/4566 [====================>.........] - ETA: 2:50 - loss: 0.6532 - acc: 0.6136
3392/4566 [=====================>........] - ETA: 2:41 - loss: 0.6520 - acc: 0.6144
3456/4566 [=====================>........] - ETA: 2:32 - loss: 0.6519 - acc: 0.6149
3520/4566 [======================>.......] - ETA: 2:25 - loss: 0.6512 - acc: 0.6156
3584/4566 [======================>.......] - ETA: 2:18 - loss: 0.6518 - acc: 0.6150
3648/4566 [======================>.......] - ETA: 2:10 - loss: 0.6526 - acc: 0.6135
3712/4566 [=======================>......] - ETA: 2:02 - loss: 0.6520 - acc: 0.6142
3776/4566 [=======================>......] - ETA: 1:53 - loss: 0.6520 - acc: 0.6144
3840/4566 [========================>.....] - ETA: 1:45 - loss: 0.6540 - acc: 0.6133
3904/4566 [========================>.....] - ETA: 1:36 - loss: 0.6530 - acc: 0.6145
3968/4566 [=========================>....] - ETA: 1:26 - loss: 0.6527 - acc: 0.6152
4032/4566 [=========================>....] - ETA: 1:16 - loss: 0.6510 - acc: 0.6181
4096/4566 [=========================>....] - ETA: 1:07 - loss: 0.6502 - acc: 0.6182
4160/4566 [==========================>...] - ETA: 58s - loss: 0.6498 - acc: 0.6190 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6491 - acc: 0.6207
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6483 - acc: 0.6217
4352/4566 [===========================>..] - ETA: 30s - loss: 0.6488 - acc: 0.6216
4416/4566 [============================>.] - ETA: 21s - loss: 0.6477 - acc: 0.6234
4480/4566 [============================>.] - ETA: 12s - loss: 0.6473 - acc: 0.6239
4544/4566 [============================>.] - ETA: 3s - loss: 0.6473 - acc: 0.6241 
4566/4566 [==============================] - 661s 145ms/step - loss: 0.6472 - acc: 0.6240 - val_loss: 0.6473 - val_acc: 0.6378

Epoch 00005: val_acc improved from 0.63386 to 0.63780, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window11/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 6/10

  64/4566 [..............................] - ETA: 7:52 - loss: 0.6762 - acc: 0.6562
 128/4566 [..............................] - ETA: 7:30 - loss: 0.6359 - acc: 0.6719
 192/4566 [>.............................] - ETA: 7:50 - loss: 0.6129 - acc: 0.6771
 256/4566 [>.............................] - ETA: 9:26 - loss: 0.6234 - acc: 0.6602
 320/4566 [=>............................] - ETA: 10:33 - loss: 0.6285 - acc: 0.6469
 384/4566 [=>............................] - ETA: 11:10 - loss: 0.6145 - acc: 0.6484
 448/4566 [=>............................] - ETA: 11:31 - loss: 0.6327 - acc: 0.6362
 512/4566 [==>...........................] - ETA: 11:49 - loss: 0.6346 - acc: 0.6328
 576/4566 [==>...........................] - ETA: 11:53 - loss: 0.6335 - acc: 0.6389
 640/4566 [===>..........................] - ETA: 11:43 - loss: 0.6308 - acc: 0.6438
 704/4566 [===>..........................] - ETA: 11:13 - loss: 0.6264 - acc: 0.6506
 768/4566 [====>.........................] - ETA: 10:43 - loss: 0.6312 - acc: 0.6471
 832/4566 [====>.........................] - ETA: 10:15 - loss: 0.6280 - acc: 0.6502
 896/4566 [====>.........................] - ETA: 9:48 - loss: 0.6292 - acc: 0.6484 
 960/4566 [=====>........................] - ETA: 9:24 - loss: 0.6267 - acc: 0.6542
1024/4566 [=====>........................] - ETA: 9:05 - loss: 0.6226 - acc: 0.6582
1088/4566 [======>.......................] - ETA: 8:44 - loss: 0.6247 - acc: 0.6553
1152/4566 [======>.......................] - ETA: 8:25 - loss: 0.6252 - acc: 0.6545
1216/4566 [======>.......................] - ETA: 8:08 - loss: 0.6236 - acc: 0.6538
1280/4566 [=======>......................] - ETA: 7:53 - loss: 0.6250 - acc: 0.6516
1344/4566 [=======>......................] - ETA: 7:36 - loss: 0.6298 - acc: 0.6458
1408/4566 [========>.....................] - ETA: 7:22 - loss: 0.6341 - acc: 0.6413
1472/4566 [========>.....................] - ETA: 7:08 - loss: 0.6341 - acc: 0.6420
1536/4566 [=========>....................] - ETA: 6:53 - loss: 0.6356 - acc: 0.6380
1600/4566 [=========>....................] - ETA: 6:40 - loss: 0.6340 - acc: 0.6388
1664/4566 [=========>....................] - ETA: 6:29 - loss: 0.6338 - acc: 0.6418
1728/4566 [==========>...................] - ETA: 6:29 - loss: 0.6331 - acc: 0.6441
1792/4566 [==========>...................] - ETA: 6:28 - loss: 0.6347 - acc: 0.6440
1856/4566 [===========>..................] - ETA: 6:27 - loss: 0.6348 - acc: 0.6428
1920/4566 [===========>..................] - ETA: 6:25 - loss: 0.6356 - acc: 0.6411
1984/4566 [============>.................] - ETA: 6:21 - loss: 0.6366 - acc: 0.6406
2048/4566 [============>.................] - ETA: 6:16 - loss: 0.6367 - acc: 0.6406
2112/4566 [============>.................] - ETA: 6:10 - loss: 0.6380 - acc: 0.6378
2176/4566 [=============>................] - ETA: 6:00 - loss: 0.6397 - acc: 0.6365
2240/4566 [=============>................] - ETA: 5:47 - loss: 0.6402 - acc: 0.6362
2304/4566 [==============>...............] - ETA: 5:34 - loss: 0.6402 - acc: 0.6363
2368/4566 [==============>...............] - ETA: 5:23 - loss: 0.6410 - acc: 0.6360
2432/4566 [==============>...............] - ETA: 5:11 - loss: 0.6428 - acc: 0.6332
2496/4566 [===============>..............] - ETA: 4:59 - loss: 0.6434 - acc: 0.6334
2560/4566 [===============>..............] - ETA: 4:48 - loss: 0.6448 - acc: 0.6316
2624/4566 [================>.............] - ETA: 4:37 - loss: 0.6433 - acc: 0.6345
2688/4566 [================>.............] - ETA: 4:26 - loss: 0.6420 - acc: 0.6358
2752/4566 [=================>............] - ETA: 4:15 - loss: 0.6432 - acc: 0.6348
2816/4566 [=================>............] - ETA: 4:04 - loss: 0.6431 - acc: 0.6349
2880/4566 [=================>............] - ETA: 3:53 - loss: 0.6428 - acc: 0.6347
2944/4566 [==================>...........] - ETA: 3:43 - loss: 0.6435 - acc: 0.6345
3008/4566 [==================>...........] - ETA: 3:34 - loss: 0.6430 - acc: 0.6330
3072/4566 [===================>..........] - ETA: 3:24 - loss: 0.6429 - acc: 0.6325
3136/4566 [===================>..........] - ETA: 3:16 - loss: 0.6416 - acc: 0.6346
3200/4566 [====================>.........] - ETA: 3:09 - loss: 0.6413 - acc: 0.6344
3264/4566 [====================>.........] - ETA: 3:03 - loss: 0.6416 - acc: 0.6339
3328/4566 [====================>.........] - ETA: 2:55 - loss: 0.6426 - acc: 0.6322
3392/4566 [=====================>........] - ETA: 2:48 - loss: 0.6417 - acc: 0.6338
3456/4566 [=====================>........] - ETA: 2:41 - loss: 0.6416 - acc: 0.6328
3520/4566 [======================>.......] - ETA: 2:32 - loss: 0.6410 - acc: 0.6338
3584/4566 [======================>.......] - ETA: 2:24 - loss: 0.6416 - acc: 0.6334
3648/4566 [======================>.......] - ETA: 2:14 - loss: 0.6417 - acc: 0.6332
3712/4566 [=======================>......] - ETA: 2:04 - loss: 0.6419 - acc: 0.6323
3776/4566 [=======================>......] - ETA: 1:54 - loss: 0.6413 - acc: 0.6337
3840/4566 [========================>.....] - ETA: 1:44 - loss: 0.6402 - acc: 0.6362
3904/4566 [========================>.....] - ETA: 1:35 - loss: 0.6405 - acc: 0.6360
3968/4566 [=========================>....] - ETA: 1:25 - loss: 0.6413 - acc: 0.6348
4032/4566 [=========================>....] - ETA: 1:16 - loss: 0.6410 - acc: 0.6347
4096/4566 [=========================>....] - ETA: 1:06 - loss: 0.6416 - acc: 0.6340
4160/4566 [==========================>...] - ETA: 57s - loss: 0.6417 - acc: 0.6341 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6425 - acc: 0.6330
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6430 - acc: 0.6320
4352/4566 [===========================>..] - ETA: 30s - loss: 0.6431 - acc: 0.6314
4416/4566 [============================>.] - ETA: 20s - loss: 0.6434 - acc: 0.6304
4480/4566 [============================>.] - ETA: 12s - loss: 0.6426 - acc: 0.6304
4544/4566 [============================>.] - ETA: 3s - loss: 0.6428 - acc: 0.6307 
4566/4566 [==============================] - 667s 146ms/step - loss: 0.6430 - acc: 0.6303 - val_loss: 0.6947 - val_acc: 0.5630

Epoch 00006: val_acc did not improve from 0.63780
Epoch 7/10

  64/4566 [..............................] - ETA: 16:41 - loss: 0.6253 - acc: 0.6562
 128/4566 [..............................] - ETA: 16:33 - loss: 0.6414 - acc: 0.6484
 192/4566 [>.............................] - ETA: 16:02 - loss: 0.6562 - acc: 0.6250
 256/4566 [>.............................] - ETA: 15:38 - loss: 0.6553 - acc: 0.6289
 320/4566 [=>............................] - ETA: 15:11 - loss: 0.6538 - acc: 0.6250
 384/4566 [=>............................] - ETA: 13:54 - loss: 0.6593 - acc: 0.6094
 448/4566 [=>............................] - ETA: 12:41 - loss: 0.6491 - acc: 0.6205
 512/4566 [==>...........................] - ETA: 11:47 - loss: 0.6512 - acc: 0.6152
 576/4566 [==>...........................] - ETA: 11:06 - loss: 0.6481 - acc: 0.6163
 640/4566 [===>..........................] - ETA: 10:32 - loss: 0.6486 - acc: 0.6141
 704/4566 [===>..........................] - ETA: 10:10 - loss: 0.6523 - acc: 0.6051
 768/4566 [====>.........................] - ETA: 9:46 - loss: 0.6504 - acc: 0.6107 
 832/4566 [====>.........................] - ETA: 9:24 - loss: 0.6537 - acc: 0.6118
 896/4566 [====>.........................] - ETA: 9:07 - loss: 0.6553 - acc: 0.6127
 960/4566 [=====>........................] - ETA: 8:50 - loss: 0.6535 - acc: 0.6104
1024/4566 [=====>........................] - ETA: 8:33 - loss: 0.6547 - acc: 0.6084
1088/4566 [======>.......................] - ETA: 8:15 - loss: 0.6546 - acc: 0.6085
1152/4566 [======>.......................] - ETA: 8:00 - loss: 0.6551 - acc: 0.6059
1216/4566 [======>.......................] - ETA: 7:47 - loss: 0.6549 - acc: 0.6086
1280/4566 [=======>......................] - ETA: 7:33 - loss: 0.6529 - acc: 0.6109
1344/4566 [=======>......................] - ETA: 7:20 - loss: 0.6524 - acc: 0.6131
1408/4566 [========>.....................] - ETA: 7:16 - loss: 0.6545 - acc: 0.6101
1472/4566 [========>.....................] - ETA: 7:17 - loss: 0.6516 - acc: 0.6128
1536/4566 [=========>....................] - ETA: 7:15 - loss: 0.6532 - acc: 0.6107
1600/4566 [=========>....................] - ETA: 7:13 - loss: 0.6518 - acc: 0.6125
1664/4566 [=========>....................] - ETA: 7:09 - loss: 0.6529 - acc: 0.6118
1728/4566 [==========>...................] - ETA: 7:05 - loss: 0.6513 - acc: 0.6134
1792/4566 [==========>...................] - ETA: 7:01 - loss: 0.6514 - acc: 0.6150
1856/4566 [===========>..................] - ETA: 6:51 - loss: 0.6525 - acc: 0.6142
1920/4566 [===========>..................] - ETA: 6:39 - loss: 0.6505 - acc: 0.6167
1984/4566 [============>.................] - ETA: 6:25 - loss: 0.6509 - acc: 0.6184
2048/4566 [============>.................] - ETA: 6:12 - loss: 0.6496 - acc: 0.6201
2112/4566 [============>.................] - ETA: 6:00 - loss: 0.6495 - acc: 0.6184
2176/4566 [=============>................] - ETA: 5:47 - loss: 0.6483 - acc: 0.6204
2240/4566 [=============>................] - ETA: 5:36 - loss: 0.6492 - acc: 0.6192
2304/4566 [==============>...............] - ETA: 5:24 - loss: 0.6492 - acc: 0.6185
2368/4566 [==============>...............] - ETA: 5:13 - loss: 0.6481 - acc: 0.6195
2432/4566 [==============>...............] - ETA: 5:02 - loss: 0.6472 - acc: 0.6205
2496/4566 [===============>..............] - ETA: 4:51 - loss: 0.6468 - acc: 0.6206
2560/4566 [===============>..............] - ETA: 4:40 - loss: 0.6447 - acc: 0.6234
2624/4566 [================>.............] - ETA: 4:30 - loss: 0.6461 - acc: 0.6235
2688/4566 [================>.............] - ETA: 4:19 - loss: 0.6457 - acc: 0.6254
2752/4566 [=================>............] - ETA: 4:08 - loss: 0.6476 - acc: 0.6235
2816/4566 [=================>............] - ETA: 3:58 - loss: 0.6469 - acc: 0.6246
2880/4566 [=================>............] - ETA: 3:49 - loss: 0.6466 - acc: 0.6250
2944/4566 [==================>...........] - ETA: 3:43 - loss: 0.6463 - acc: 0.6250
3008/4566 [==================>...........] - ETA: 3:37 - loss: 0.6457 - acc: 0.6260
3072/4566 [===================>..........] - ETA: 3:30 - loss: 0.6435 - acc: 0.6286
3136/4566 [===================>..........] - ETA: 3:23 - loss: 0.6416 - acc: 0.6317
3200/4566 [====================>.........] - ETA: 3:16 - loss: 0.6420 - acc: 0.6319
3264/4566 [====================>.........] - ETA: 3:09 - loss: 0.6424 - acc: 0.6317
3328/4566 [====================>.........] - ETA: 3:00 - loss: 0.6439 - acc: 0.6292
3392/4566 [=====================>........] - ETA: 2:50 - loss: 0.6444 - acc: 0.6285
3456/4566 [=====================>........] - ETA: 2:40 - loss: 0.6434 - acc: 0.6296
3520/4566 [======================>.......] - ETA: 2:30 - loss: 0.6439 - acc: 0.6298
3584/4566 [======================>.......] - ETA: 2:20 - loss: 0.6451 - acc: 0.6281
3648/4566 [======================>.......] - ETA: 2:10 - loss: 0.6447 - acc: 0.6277
3712/4566 [=======================>......] - ETA: 2:01 - loss: 0.6447 - acc: 0.6274
3776/4566 [=======================>......] - ETA: 1:51 - loss: 0.6462 - acc: 0.6250
3840/4566 [========================>.....] - ETA: 1:42 - loss: 0.6454 - acc: 0.6263
3904/4566 [========================>.....] - ETA: 1:32 - loss: 0.6448 - acc: 0.6276
3968/4566 [=========================>....] - ETA: 1:23 - loss: 0.6451 - acc: 0.6278
4032/4566 [=========================>....] - ETA: 1:14 - loss: 0.6441 - acc: 0.6292
4096/4566 [=========================>....] - ETA: 1:04 - loss: 0.6441 - acc: 0.6296
4160/4566 [==========================>...] - ETA: 55s - loss: 0.6441 - acc: 0.6298 
4224/4566 [==========================>...] - ETA: 46s - loss: 0.6433 - acc: 0.6307
4288/4566 [===========================>..] - ETA: 37s - loss: 0.6429 - acc: 0.6318
4352/4566 [===========================>..] - ETA: 29s - loss: 0.6431 - acc: 0.6324
4416/4566 [============================>.] - ETA: 20s - loss: 0.6427 - acc: 0.6327
4480/4566 [============================>.] - ETA: 11s - loss: 0.6432 - acc: 0.6326
4544/4566 [============================>.] - ETA: 3s - loss: 0.6425 - acc: 0.6327 
4566/4566 [==============================] - 682s 149ms/step - loss: 0.6423 - acc: 0.6334 - val_loss: 0.6333 - val_acc: 0.6319

Epoch 00007: val_acc did not improve from 0.63780
Epoch 8/10

  64/4566 [..............................] - ETA: 14:40 - loss: 0.6295 - acc: 0.7031
 128/4566 [..............................] - ETA: 11:59 - loss: 0.6422 - acc: 0.6797
 192/4566 [>.............................] - ETA: 10:47 - loss: 0.6408 - acc: 0.6615
 256/4566 [>.............................] - ETA: 10:09 - loss: 0.6509 - acc: 0.6289
 320/4566 [=>............................] - ETA: 9:35 - loss: 0.6450 - acc: 0.6219 
 384/4566 [=>............................] - ETA: 9:08 - loss: 0.6381 - acc: 0.6276
 448/4566 [=>............................] - ETA: 8:43 - loss: 0.6417 - acc: 0.6339
 512/4566 [==>...........................] - ETA: 8:19 - loss: 0.6443 - acc: 0.6309
 576/4566 [==>...........................] - ETA: 7:59 - loss: 0.6383 - acc: 0.6389
 640/4566 [===>..........................] - ETA: 7:42 - loss: 0.6368 - acc: 0.6375
 704/4566 [===>..........................] - ETA: 7:29 - loss: 0.6330 - acc: 0.6392
 768/4566 [====>.........................] - ETA: 7:19 - loss: 0.6251 - acc: 0.6471
 832/4566 [====>.........................] - ETA: 7:08 - loss: 0.6224 - acc: 0.6454
 896/4566 [====>.........................] - ETA: 7:00 - loss: 0.6189 - acc: 0.6462
 960/4566 [=====>........................] - ETA: 6:49 - loss: 0.6209 - acc: 0.6448
1024/4566 [=====>........................] - ETA: 6:38 - loss: 0.6233 - acc: 0.6445
1088/4566 [======>.......................] - ETA: 6:33 - loss: 0.6263 - acc: 0.6397
1152/4566 [======>.......................] - ETA: 6:40 - loss: 0.6301 - acc: 0.6441
1216/4566 [======>.......................] - ETA: 6:52 - loss: 0.6291 - acc: 0.6439
1280/4566 [=======>......................] - ETA: 6:59 - loss: 0.6276 - acc: 0.6453
1344/4566 [=======>......................] - ETA: 7:04 - loss: 0.6313 - acc: 0.6391
1408/4566 [========>.....................] - ETA: 7:06 - loss: 0.6313 - acc: 0.6406
1472/4566 [========>.....................] - ETA: 7:09 - loss: 0.6348 - acc: 0.6359
1536/4566 [=========>....................] - ETA: 7:10 - loss: 0.6323 - acc: 0.6400
1600/4566 [=========>....................] - ETA: 7:01 - loss: 0.6315 - acc: 0.6419
1664/4566 [=========>....................] - ETA: 6:48 - loss: 0.6321 - acc: 0.6394
1728/4566 [==========>...................] - ETA: 6:34 - loss: 0.6312 - acc: 0.6400
1792/4566 [==========>...................] - ETA: 6:21 - loss: 0.6320 - acc: 0.6406
1856/4566 [===========>..................] - ETA: 6:08 - loss: 0.6313 - acc: 0.6395
1920/4566 [===========>..................] - ETA: 5:57 - loss: 0.6327 - acc: 0.6380
1984/4566 [============>.................] - ETA: 5:47 - loss: 0.6330 - acc: 0.6376
2048/4566 [============>.................] - ETA: 5:37 - loss: 0.6312 - acc: 0.6406
2112/4566 [============>.................] - ETA: 5:26 - loss: 0.6316 - acc: 0.6416
2176/4566 [=============>................] - ETA: 5:16 - loss: 0.6331 - acc: 0.6406
2240/4566 [=============>................] - ETA: 5:05 - loss: 0.6315 - acc: 0.6442
2304/4566 [==============>...............] - ETA: 4:55 - loss: 0.6316 - acc: 0.6432
2368/4566 [==============>...............] - ETA: 4:46 - loss: 0.6325 - acc: 0.6415
2432/4566 [==============>...............] - ETA: 4:37 - loss: 0.6320 - acc: 0.6427
2496/4566 [===============>..............] - ETA: 4:28 - loss: 0.6343 - acc: 0.6402
2560/4566 [===============>..............] - ETA: 4:18 - loss: 0.6350 - acc: 0.6406
2624/4566 [================>.............] - ETA: 4:08 - loss: 0.6343 - acc: 0.6410
2688/4566 [================>.............] - ETA: 4:02 - loss: 0.6340 - acc: 0.6425
2752/4566 [=================>............] - ETA: 3:58 - loss: 0.6359 - acc: 0.6399
2816/4566 [=================>............] - ETA: 3:53 - loss: 0.6354 - acc: 0.6413
2880/4566 [=================>............] - ETA: 3:47 - loss: 0.6361 - acc: 0.6410
2944/4566 [==================>...........] - ETA: 3:41 - loss: 0.6373 - acc: 0.6389
3008/4566 [==================>...........] - ETA: 3:35 - loss: 0.6357 - acc: 0.6410
3072/4566 [===================>..........] - ETA: 3:28 - loss: 0.6352 - acc: 0.6419
3136/4566 [===================>..........] - ETA: 3:18 - loss: 0.6354 - acc: 0.6422
3200/4566 [====================>.........] - ETA: 3:09 - loss: 0.6347 - acc: 0.6428
3264/4566 [====================>.........] - ETA: 2:59 - loss: 0.6340 - acc: 0.6437
3328/4566 [====================>.........] - ETA: 2:50 - loss: 0.6334 - acc: 0.6439
3392/4566 [=====================>........] - ETA: 2:40 - loss: 0.6324 - acc: 0.6453
3456/4566 [=====================>........] - ETA: 2:31 - loss: 0.6325 - acc: 0.6461
3520/4566 [======================>.......] - ETA: 2:22 - loss: 0.6316 - acc: 0.6466
3584/4566 [======================>.......] - ETA: 2:13 - loss: 0.6323 - acc: 0.6462
3648/4566 [======================>.......] - ETA: 2:04 - loss: 0.6320 - acc: 0.6450
3712/4566 [=======================>......] - ETA: 1:55 - loss: 0.6327 - acc: 0.6441
3776/4566 [=======================>......] - ETA: 1:46 - loss: 0.6337 - acc: 0.6435
3840/4566 [========================>.....] - ETA: 1:37 - loss: 0.6326 - acc: 0.6453
3904/4566 [========================>.....] - ETA: 1:28 - loss: 0.6325 - acc: 0.6452
3968/4566 [=========================>....] - ETA: 1:19 - loss: 0.6319 - acc: 0.6469
4032/4566 [=========================>....] - ETA: 1:10 - loss: 0.6316 - acc: 0.6481
4096/4566 [=========================>....] - ETA: 1:02 - loss: 0.6312 - acc: 0.6489
4160/4566 [==========================>...] - ETA: 53s - loss: 0.6306 - acc: 0.6493 
4224/4566 [==========================>...] - ETA: 45s - loss: 0.6303 - acc: 0.6503
4288/4566 [===========================>..] - ETA: 37s - loss: 0.6302 - acc: 0.6502
4352/4566 [===========================>..] - ETA: 29s - loss: 0.6294 - acc: 0.6514
4416/4566 [============================>.] - ETA: 20s - loss: 0.6291 - acc: 0.6517
4480/4566 [============================>.] - ETA: 11s - loss: 0.6301 - acc: 0.6504
4544/4566 [============================>.] - ETA: 3s - loss: 0.6306 - acc: 0.6496 
4566/4566 [==============================] - 658s 144ms/step - loss: 0.6311 - acc: 0.6489 - val_loss: 0.6246 - val_acc: 0.6339

Epoch 00008: val_acc did not improve from 0.63780
Epoch 9/10

  64/4566 [..............................] - ETA: 8:30 - loss: 0.6460 - acc: 0.6094
 128/4566 [..............................] - ETA: 8:02 - loss: 0.6115 - acc: 0.6562
 192/4566 [>.............................] - ETA: 7:53 - loss: 0.5916 - acc: 0.6771
 256/4566 [>.............................] - ETA: 7:38 - loss: 0.5836 - acc: 0.6875
 320/4566 [=>............................] - ETA: 7:29 - loss: 0.5915 - acc: 0.6875
 384/4566 [=>............................] - ETA: 7:19 - loss: 0.5900 - acc: 0.6875
 448/4566 [=>............................] - ETA: 7:12 - loss: 0.6009 - acc: 0.6741
 512/4566 [==>...........................] - ETA: 7:05 - loss: 0.6088 - acc: 0.6582
 576/4566 [==>...........................] - ETA: 6:57 - loss: 0.6133 - acc: 0.6528
 640/4566 [===>..........................] - ETA: 6:47 - loss: 0.6142 - acc: 0.6578
 704/4566 [===>..........................] - ETA: 6:39 - loss: 0.6227 - acc: 0.6477
 768/4566 [====>.........................] - ETA: 6:29 - loss: 0.6212 - acc: 0.6471
 832/4566 [====>.........................] - ETA: 6:24 - loss: 0.6227 - acc: 0.6490
 896/4566 [====>.........................] - ETA: 6:27 - loss: 0.6263 - acc: 0.6417
 960/4566 [=====>........................] - ETA: 6:42 - loss: 0.6337 - acc: 0.6312
1024/4566 [=====>........................] - ETA: 6:58 - loss: 0.6346 - acc: 0.6309
1088/4566 [======>.......................] - ETA: 7:10 - loss: 0.6315 - acc: 0.6333
1152/4566 [======>.......................] - ETA: 7:19 - loss: 0.6264 - acc: 0.6389
1216/4566 [======>.......................] - ETA: 7:26 - loss: 0.6239 - acc: 0.6431
1280/4566 [=======>......................] - ETA: 7:30 - loss: 0.6281 - acc: 0.6391
1344/4566 [=======>......................] - ETA: 7:33 - loss: 0.6267 - acc: 0.6399
1408/4566 [========>.....................] - ETA: 7:23 - loss: 0.6280 - acc: 0.6371
1472/4566 [========>.....................] - ETA: 7:08 - loss: 0.6276 - acc: 0.6352
1536/4566 [=========>....................] - ETA: 6:54 - loss: 0.6285 - acc: 0.6348
1600/4566 [=========>....................] - ETA: 6:42 - loss: 0.6293 - acc: 0.6338
1664/4566 [=========>....................] - ETA: 6:29 - loss: 0.6292 - acc: 0.6364
1728/4566 [==========>...................] - ETA: 6:18 - loss: 0.6295 - acc: 0.6377
1792/4566 [==========>...................] - ETA: 6:06 - loss: 0.6298 - acc: 0.6390
1856/4566 [===========>..................] - ETA: 5:54 - loss: 0.6298 - acc: 0.6401
1920/4566 [===========>..................] - ETA: 5:43 - loss: 0.6303 - acc: 0.6396
1984/4566 [============>.................] - ETA: 5:31 - loss: 0.6319 - acc: 0.6371
2048/4566 [============>.................] - ETA: 5:21 - loss: 0.6333 - acc: 0.6387
2112/4566 [============>.................] - ETA: 5:10 - loss: 0.6346 - acc: 0.6383
2176/4566 [=============>................] - ETA: 5:01 - loss: 0.6358 - acc: 0.6383
2240/4566 [=============>................] - ETA: 4:51 - loss: 0.6374 - acc: 0.6362
2304/4566 [==============>...............] - ETA: 4:42 - loss: 0.6378 - acc: 0.6367
2368/4566 [==============>...............] - ETA: 4:34 - loss: 0.6389 - acc: 0.6334
2432/4566 [==============>...............] - ETA: 4:27 - loss: 0.6391 - acc: 0.6324
2496/4566 [===============>..............] - ETA: 4:24 - loss: 0.6398 - acc: 0.6306
2560/4566 [===============>..............] - ETA: 4:20 - loss: 0.6395 - acc: 0.6305
2624/4566 [================>.............] - ETA: 4:16 - loss: 0.6389 - acc: 0.6319
2688/4566 [================>.............] - ETA: 4:12 - loss: 0.6387 - acc: 0.6324
2752/4566 [=================>............] - ETA: 4:06 - loss: 0.6371 - acc: 0.6352
2816/4566 [=================>............] - ETA: 4:01 - loss: 0.6372 - acc: 0.6346
2880/4566 [=================>............] - ETA: 3:55 - loss: 0.6385 - acc: 0.6319
2944/4566 [==================>...........] - ETA: 3:45 - loss: 0.6378 - acc: 0.6325
3008/4566 [==================>...........] - ETA: 3:34 - loss: 0.6393 - acc: 0.6310
3072/4566 [===================>..........] - ETA: 3:24 - loss: 0.6384 - acc: 0.6302
3136/4566 [===================>..........] - ETA: 3:14 - loss: 0.6404 - acc: 0.6295
3200/4566 [====================>.........] - ETA: 3:04 - loss: 0.6402 - acc: 0.6309
3264/4566 [====================>.........] - ETA: 2:54 - loss: 0.6395 - acc: 0.6302
3328/4566 [====================>.........] - ETA: 2:45 - loss: 0.6380 - acc: 0.6325
3392/4566 [=====================>........] - ETA: 2:36 - loss: 0.6374 - acc: 0.6335
3456/4566 [=====================>........] - ETA: 2:27 - loss: 0.6380 - acc: 0.6325
3520/4566 [======================>.......] - ETA: 2:18 - loss: 0.6375 - acc: 0.6327
3584/4566 [======================>.......] - ETA: 2:09 - loss: 0.6382 - acc: 0.6317
3648/4566 [======================>.......] - ETA: 2:00 - loss: 0.6384 - acc: 0.6324
3712/4566 [=======================>......] - ETA: 1:52 - loss: 0.6372 - acc: 0.6347
3776/4566 [=======================>......] - ETA: 1:43 - loss: 0.6374 - acc: 0.6340
3840/4566 [========================>.....] - ETA: 1:35 - loss: 0.6360 - acc: 0.6357
3904/4566 [========================>.....] - ETA: 1:26 - loss: 0.6340 - acc: 0.6378
3968/4566 [=========================>....] - ETA: 1:19 - loss: 0.6337 - acc: 0.6379
4032/4566 [=========================>....] - ETA: 1:11 - loss: 0.6332 - acc: 0.6376
4096/4566 [=========================>....] - ETA: 1:03 - loss: 0.6340 - acc: 0.6367
4160/4566 [==========================>...] - ETA: 55s - loss: 0.6335 - acc: 0.6370 
4224/4566 [==========================>...] - ETA: 46s - loss: 0.6334 - acc: 0.6378
4288/4566 [===========================>..] - ETA: 38s - loss: 0.6335 - acc: 0.6378
4352/4566 [===========================>..] - ETA: 29s - loss: 0.6341 - acc: 0.6386
4416/4566 [============================>.] - ETA: 20s - loss: 0.6333 - acc: 0.6397
4480/4566 [============================>.] - ETA: 11s - loss: 0.6324 - acc: 0.6408
4544/4566 [============================>.] - ETA: 3s - loss: 0.6328 - acc: 0.6406 
4566/4566 [==============================] - 650s 142ms/step - loss: 0.6328 - acc: 0.6406 - val_loss: 0.6376 - val_acc: 0.6299

Epoch 00009: val_acc did not improve from 0.63780
Epoch 10/10

  64/4566 [..............................] - ETA: 7:54 - loss: 0.6053 - acc: 0.7188
 128/4566 [..............................] - ETA: 7:43 - loss: 0.6285 - acc: 0.6719
 192/4566 [>.............................] - ETA: 7:40 - loss: 0.6264 - acc: 0.6562
 256/4566 [>.............................] - ETA: 7:34 - loss: 0.6023 - acc: 0.6797
 320/4566 [=>............................] - ETA: 7:28 - loss: 0.6118 - acc: 0.6781
 384/4566 [=>............................] - ETA: 7:28 - loss: 0.6110 - acc: 0.6771
 448/4566 [=>............................] - ETA: 7:27 - loss: 0.6202 - acc: 0.6629
 512/4566 [==>...........................] - ETA: 7:17 - loss: 0.6182 - acc: 0.6660
 576/4566 [==>...........................] - ETA: 7:09 - loss: 0.6084 - acc: 0.6753
 640/4566 [===>..........................] - ETA: 7:01 - loss: 0.6136 - acc: 0.6687
 704/4566 [===>..........................] - ETA: 7:22 - loss: 0.6074 - acc: 0.6747
 768/4566 [====>.........................] - ETA: 7:46 - loss: 0.6227 - acc: 0.6628
 832/4566 [====>.........................] - ETA: 8:03 - loss: 0.6202 - acc: 0.6611
 896/4566 [====>.........................] - ETA: 8:11 - loss: 0.6187 - acc: 0.6618
 960/4566 [=====>........................] - ETA: 8:18 - loss: 0.6164 - acc: 0.6687
1024/4566 [=====>........................] - ETA: 8:26 - loss: 0.6160 - acc: 0.6709
1088/4566 [======>.......................] - ETA: 8:29 - loss: 0.6192 - acc: 0.6691
1152/4566 [======>.......................] - ETA: 8:18 - loss: 0.6208 - acc: 0.6675
1216/4566 [======>.......................] - ETA: 8:02 - loss: 0.6277 - acc: 0.6595
1280/4566 [=======>......................] - ETA: 7:47 - loss: 0.6261 - acc: 0.6617
1344/4566 [=======>......................] - ETA: 7:34 - loss: 0.6286 - acc: 0.6577
1408/4566 [========>.....................] - ETA: 7:19 - loss: 0.6245 - acc: 0.6612
1472/4566 [========>.....................] - ETA: 7:07 - loss: 0.6234 - acc: 0.6617
1536/4566 [=========>....................] - ETA: 6:54 - loss: 0.6215 - acc: 0.6628
1600/4566 [=========>....................] - ETA: 6:42 - loss: 0.6232 - acc: 0.6600
1664/4566 [=========>....................] - ETA: 6:30 - loss: 0.6221 - acc: 0.6611
1728/4566 [==========>...................] - ETA: 6:19 - loss: 0.6219 - acc: 0.6626
1792/4566 [==========>...................] - ETA: 6:07 - loss: 0.6227 - acc: 0.6624
1856/4566 [===========>..................] - ETA: 5:57 - loss: 0.6255 - acc: 0.6595
1920/4566 [===========>..................] - ETA: 5:46 - loss: 0.6261 - acc: 0.6589
1984/4566 [============>.................] - ETA: 5:35 - loss: 0.6257 - acc: 0.6588
2048/4566 [============>.................] - ETA: 5:24 - loss: 0.6251 - acc: 0.6592
2112/4566 [============>.................] - ETA: 5:13 - loss: 0.6270 - acc: 0.6553
2176/4566 [=============>................] - ETA: 5:06 - loss: 0.6260 - acc: 0.6572
2240/4566 [=============>................] - ETA: 5:04 - loss: 0.6264 - acc: 0.6580
2304/4566 [==============>...............] - ETA: 5:02 - loss: 0.6262 - acc: 0.6584
2368/4566 [==============>...............] - ETA: 4:58 - loss: 0.6252 - acc: 0.6605
2432/4566 [==============>...............] - ETA: 4:54 - loss: 0.6240 - acc: 0.6624
2496/4566 [===============>..............] - ETA: 4:49 - loss: 0.6240 - acc: 0.6623
2560/4566 [===============>..............] - ETA: 4:43 - loss: 0.6243 - acc: 0.6621
2624/4566 [================>.............] - ETA: 4:35 - loss: 0.6251 - acc: 0.6616
2688/4566 [================>.............] - ETA: 4:24 - loss: 0.6265 - acc: 0.6592
2752/4566 [=================>............] - ETA: 4:14 - loss: 0.6269 - acc: 0.6588
2816/4566 [=================>............] - ETA: 4:03 - loss: 0.6265 - acc: 0.6591
2880/4566 [=================>............] - ETA: 3:53 - loss: 0.6271 - acc: 0.6580
2944/4566 [==================>...........] - ETA: 3:43 - loss: 0.6259 - acc: 0.6596
3008/4566 [==================>...........] - ETA: 3:34 - loss: 0.6265 - acc: 0.6582
3072/4566 [===================>..........] - ETA: 3:24 - loss: 0.6266 - acc: 0.6576
3136/4566 [===================>..........] - ETA: 3:14 - loss: 0.6272 - acc: 0.6569
3200/4566 [====================>.........] - ETA: 3:04 - loss: 0.6262 - acc: 0.6587
3264/4566 [====================>.........] - ETA: 2:55 - loss: 0.6270 - acc: 0.6569
3328/4566 [====================>.........] - ETA: 2:45 - loss: 0.6275 - acc: 0.6566
3392/4566 [=====================>........] - ETA: 2:36 - loss: 0.6268 - acc: 0.6583
3456/4566 [=====================>........] - ETA: 2:27 - loss: 0.6262 - acc: 0.6580
3520/4566 [======================>.......] - ETA: 2:18 - loss: 0.6266 - acc: 0.6571
3584/4566 [======================>.......] - ETA: 2:09 - loss: 0.6276 - acc: 0.6562
3648/4566 [======================>.......] - ETA: 2:01 - loss: 0.6270 - acc: 0.6579
3712/4566 [=======================>......] - ETA: 1:54 - loss: 0.6276 - acc: 0.6565
3776/4566 [=======================>......] - ETA: 1:46 - loss: 0.6270 - acc: 0.6568
3840/4566 [========================>.....] - ETA: 1:39 - loss: 0.6280 - acc: 0.6549
3904/4566 [========================>.....] - ETA: 1:31 - loss: 0.6283 - acc: 0.6542
3968/4566 [=========================>....] - ETA: 1:23 - loss: 0.6282 - acc: 0.6545
4032/4566 [=========================>....] - ETA: 1:14 - loss: 0.6281 - acc: 0.6543
4096/4566 [=========================>....] - ETA: 1:06 - loss: 0.6272 - acc: 0.6550
4160/4566 [==========================>...] - ETA: 57s - loss: 0.6284 - acc: 0.6534 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6291 - acc: 0.6522
4288/4566 [===========================>..] - ETA: 38s - loss: 0.6280 - acc: 0.6535
4352/4566 [===========================>..] - ETA: 29s - loss: 0.6283 - acc: 0.6535
4416/4566 [============================>.] - ETA: 20s - loss: 0.6288 - acc: 0.6531
4480/4566 [============================>.] - ETA: 11s - loss: 0.6303 - acc: 0.6509
4544/4566 [============================>.] - ETA: 3s - loss: 0.6292 - acc: 0.6512 
4566/4566 [==============================] - 647s 142ms/step - loss: 0.6293 - acc: 0.6509 - val_loss: 0.6631 - val_acc: 0.5965

Epoch 00010: val_acc did not improve from 0.63780
样本个数 634
样本个数 1268
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd1d41200d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd1d41200d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd1d406f9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd1d406f9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbdba6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbdba6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1cbcae3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1cbcae3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1cbc51710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1cbc51710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbca41d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbca41d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1cbcae510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1cbcae510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbd24e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbd24e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1cba98b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1cba98b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1cbbecad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1cbbecad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbac0350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbac0350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1cbc07390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1cbc07390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbaa8290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cbaa8290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1cbac5050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1cbac5050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1cb952e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1cb952e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cb7d9510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1cb7d9510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1cb859990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1cb859990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c3520a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c3520a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1c34165d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1c34165d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1c3325550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1c3325550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c343b750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c343b750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1c3416990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1c3416990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c34241d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c34241d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1c3233ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1c3233ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1c3121750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1c3121750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c30cce10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c30cce10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1c3404ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1c3404ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c2ff2e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c2ff2e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1badc93d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1badc93d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1bace3dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1bace3dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c2ff5290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1c2ff5290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1badc9c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1badc9c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1bab60f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1bab60f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1baaa4390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1baaa4390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1ba990090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1ba990090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1bace6f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1bace6f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1ba8a8210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1ba8a8210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1baa02f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1baa02f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1ba8a8390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1ba8a8390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1aa65da90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1aa65da90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1ba789450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1ba789450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1baaf9250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1baaf9250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1ba990490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1ba990490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1aa4e1b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1aa4e1b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1aa466e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1aa466e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1aa4afe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1aa4afe10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1ba79c310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1ba79c310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1aa249890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1aa249890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1aa69a3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1aa69a3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1aa02cd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1aa02cd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1aa17d310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1aa17d310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1aa6bd850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1aa6bd850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1a9ee8f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1a9ee8f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1a9e035d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1a9e035d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1a9d13fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1a9d13fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1a9e48bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1a9e48bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1a9e03e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1a9e03e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1a9cc5590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1a9cc5590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1a9b3d190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1a9b3d190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1a9a372d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1a9a372d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1a9b61150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1a9b61150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1a9b3d5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd1a9b3d5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1a9a20810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1a9a20810>>: AttributeError: module 'gast' has no attribute 'Str'
window11.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1268 [>.............................] - ETA: 2:39
 128/1268 [==>...........................] - ETA: 1:47
 192/1268 [===>..........................] - ETA: 1:30
 256/1268 [=====>........................] - ETA: 1:17
 320/1268 [======>.......................] - ETA: 1:08
 384/1268 [========>.....................] - ETA: 1:00
 448/1268 [=========>....................] - ETA: 54s 
 512/1268 [===========>..................] - ETA: 48s
 576/1268 [============>.................] - ETA: 43s
 640/1268 [==============>...............] - ETA: 38s
 704/1268 [===============>..............] - ETA: 34s
 768/1268 [=================>............] - ETA: 30s
 832/1268 [==================>...........] - ETA: 26s
 896/1268 [====================>.........] - ETA: 22s
 960/1268 [=====================>........] - ETA: 18s
1024/1268 [=======================>......] - ETA: 14s
1088/1268 [========================>.....] - ETA: 10s
1152/1268 [==========================>...] - ETA: 6s 
1216/1268 [===========================>..] - ETA: 2s
1268/1268 [==============================] - 72s 57ms/step
loss: 0.6587162288581535
acc: 0.6119873824555792
