nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7efd8cd8db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7efd8cd8db90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7efdfafca310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7efdfafca310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efdeac6e190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efdeac6e190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efdeac5e5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efdeac5e5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd8cd340d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd8cd340d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8cbc6f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8cbc6f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efdeac6e790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efdeac6e790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efdeac75990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efdeac75990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efdeac796d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efdeac796d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efe2525f690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efe2525f690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efdeac337d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efdeac337d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd8ccbff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd8ccbff10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c883f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c883f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd8c783250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd8c783250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd8c77fd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd8c77fd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efdeac6b090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efdeac6b090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd8c783d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd8c783d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c5ab8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c5ab8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd8c4344d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd8c4344d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd8c413610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd8c413610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c75bb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c75bb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd8c434250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd8c434250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c392b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c392b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd8c385290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd8c385290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd8c12acd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd8c12acd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8bf5a8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8bf5a8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd8ca79590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd8ca79590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c427290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c427290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd83df34d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd83df34d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd83d35810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd83d35810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd83e30050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd83e30050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd83df3f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd83df3f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c0a4990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c0a4990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd83b48610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd83b48610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd83a24d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd83a24d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd83910710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd83910710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd83b48d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd83b48d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd83c58bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd83c58bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd8c889e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd8c889e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd83b676d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd83b676d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd73626e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd73626e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd83d39d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd83d39d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd836fa050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd836fa050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd735328d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd735328d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd8370a950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd8370a950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd732ce950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd732ce950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efdeac33f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efdeac33f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8370aa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8370aa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd73424610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd73424610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd731c3d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd731c3d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd734a8510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd734a8510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd734247d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd734247d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd73614950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd73614950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd735b9ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd735b9ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd62e75150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd62e75150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c4b4790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd8c4b4790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd837fbb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd837fbb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd62dcf190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd62dcf190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd62d8ea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7efd62d8ea10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd62b58fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7efd62b58fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd62dcc4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd62dcc4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd62d8e050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7efd62d8e050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd62ba0d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7efd62ba0d90>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-17 10:07:11.465030: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-17 10:07:11.507646: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-17 10:07:11.539814: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ecd4bfeab0 executing computations on platform Host. Devices:
2022-11-17 10:07:11.539943: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-17 10:07:11.940682: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window01.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 37:02 - loss: 0.7120 - acc: 0.5156
 128/4566 [..............................] - ETA: 29:23 - loss: 0.7396 - acc: 0.5156
 192/4566 [>.............................] - ETA: 23:48 - loss: 0.7348 - acc: 0.5052
 256/4566 [>.............................] - ETA: 22:56 - loss: 0.7475 - acc: 0.5000
 320/4566 [=>............................] - ETA: 28:55 - loss: 0.7309 - acc: 0.5406
 384/4566 [=>............................] - ETA: 32:26 - loss: 0.7347 - acc: 0.5391
 448/4566 [=>............................] - ETA: 32:47 - loss: 0.7358 - acc: 0.5335
 512/4566 [==>...........................] - ETA: 32:56 - loss: 0.7300 - acc: 0.5332
 576/4566 [==>...........................] - ETA: 32:26 - loss: 0.7342 - acc: 0.5243
 640/4566 [===>..........................] - ETA: 30:41 - loss: 0.7267 - acc: 0.5266
 704/4566 [===>..........................] - ETA: 28:28 - loss: 0.7298 - acc: 0.5270
 768/4566 [====>.........................] - ETA: 26:25 - loss: 0.7311 - acc: 0.5208
 832/4566 [====>.........................] - ETA: 24:36 - loss: 0.7255 - acc: 0.5240
 896/4566 [====>.........................] - ETA: 23:03 - loss: 0.7273 - acc: 0.5223
 960/4566 [=====>........................] - ETA: 21:41 - loss: 0.7330 - acc: 0.5146
1024/4566 [=====>........................] - ETA: 20:28 - loss: 0.7335 - acc: 0.5107
1088/4566 [======>.......................] - ETA: 19:22 - loss: 0.7335 - acc: 0.5110
1152/4566 [======>.......................] - ETA: 18:23 - loss: 0.7320 - acc: 0.5104
1216/4566 [======>.......................] - ETA: 17:32 - loss: 0.7308 - acc: 0.5123
1280/4566 [=======>......................] - ETA: 16:46 - loss: 0.7360 - acc: 0.5102
1344/4566 [=======>......................] - ETA: 16:11 - loss: 0.7367 - acc: 0.5082
1408/4566 [========>.....................] - ETA: 15:30 - loss: 0.7395 - acc: 0.5057
1472/4566 [========>.....................] - ETA: 14:50 - loss: 0.7358 - acc: 0.5088
1536/4566 [=========>....................] - ETA: 14:23 - loss: 0.7390 - acc: 0.5046
1600/4566 [=========>....................] - ETA: 14:31 - loss: 0.7402 - acc: 0.5038
1664/4566 [=========>....................] - ETA: 14:41 - loss: 0.7398 - acc: 0.5030
1728/4566 [==========>...................] - ETA: 14:45 - loss: 0.7408 - acc: 0.5023
1792/4566 [==========>...................] - ETA: 14:46 - loss: 0.7395 - acc: 0.5028
1856/4566 [===========>..................] - ETA: 14:46 - loss: 0.7410 - acc: 0.4995
1920/4566 [===========>..................] - ETA: 14:38 - loss: 0.7414 - acc: 0.5000
1984/4566 [============>.................] - ETA: 14:18 - loss: 0.7417 - acc: 0.5010
2048/4566 [============>.................] - ETA: 13:45 - loss: 0.7396 - acc: 0.5024
2112/4566 [============>.................] - ETA: 13:11 - loss: 0.7401 - acc: 0.5038
2176/4566 [=============>................] - ETA: 12:37 - loss: 0.7397 - acc: 0.5037
2240/4566 [=============>................] - ETA: 12:04 - loss: 0.7396 - acc: 0.5054
2304/4566 [==============>...............] - ETA: 11:34 - loss: 0.7387 - acc: 0.5082
2368/4566 [==============>...............] - ETA: 11:04 - loss: 0.7387 - acc: 0.5089
2432/4566 [==============>...............] - ETA: 10:35 - loss: 0.7409 - acc: 0.5045
2496/4566 [===============>..............] - ETA: 10:07 - loss: 0.7395 - acc: 0.5064
2560/4566 [===============>..............] - ETA: 9:41 - loss: 0.7387 - acc: 0.5078 
2624/4566 [================>.............] - ETA: 9:15 - loss: 0.7384 - acc: 0.5057
2688/4566 [================>.............] - ETA: 8:50 - loss: 0.7381 - acc: 0.5056
2752/4566 [=================>............] - ETA: 8:25 - loss: 0.7394 - acc: 0.5036
2816/4566 [=================>............] - ETA: 8:02 - loss: 0.7395 - acc: 0.5032
2880/4566 [=================>............] - ETA: 7:40 - loss: 0.7390 - acc: 0.5024
2944/4566 [==================>...........] - ETA: 7:27 - loss: 0.7378 - acc: 0.5034
3008/4566 [==================>...........] - ETA: 7:19 - loss: 0.7384 - acc: 0.5037
3072/4566 [===================>..........] - ETA: 7:08 - loss: 0.7380 - acc: 0.5046
3136/4566 [===================>..........] - ETA: 6:57 - loss: 0.7377 - acc: 0.5051
3200/4566 [====================>.........] - ETA: 6:46 - loss: 0.7372 - acc: 0.5056
3264/4566 [====================>.........] - ETA: 6:32 - loss: 0.7349 - acc: 0.5092
3328/4566 [====================>.........] - ETA: 6:14 - loss: 0.7351 - acc: 0.5081
3392/4566 [=====================>........] - ETA: 5:52 - loss: 0.7354 - acc: 0.5074
3456/4566 [=====================>........] - ETA: 5:29 - loss: 0.7352 - acc: 0.5078
3520/4566 [======================>.......] - ETA: 5:07 - loss: 0.7353 - acc: 0.5060
3584/4566 [======================>.......] - ETA: 4:45 - loss: 0.7342 - acc: 0.5070
3648/4566 [======================>.......] - ETA: 4:25 - loss: 0.7339 - acc: 0.5071
3712/4566 [=======================>......] - ETA: 4:04 - loss: 0.7329 - acc: 0.5075
3776/4566 [=======================>......] - ETA: 3:44 - loss: 0.7323 - acc: 0.5074
3840/4566 [========================>.....] - ETA: 3:24 - loss: 0.7321 - acc: 0.5076
3904/4566 [========================>.....] - ETA: 3:05 - loss: 0.7315 - acc: 0.5079
3968/4566 [=========================>....] - ETA: 2:45 - loss: 0.7311 - acc: 0.5088
4032/4566 [=========================>....] - ETA: 2:26 - loss: 0.7297 - acc: 0.5112
4096/4566 [=========================>....] - ETA: 2:08 - loss: 0.7290 - acc: 0.5122
4160/4566 [==========================>...] - ETA: 1:50 - loss: 0.7301 - acc: 0.5111
4224/4566 [==========================>...] - ETA: 1:31 - loss: 0.7298 - acc: 0.5107
4288/4566 [===========================>..] - ETA: 1:14 - loss: 0.7300 - acc: 0.5093
4352/4566 [===========================>..] - ETA: 57s - loss: 0.7298 - acc: 0.5094 
4416/4566 [============================>.] - ETA: 41s - loss: 0.7302 - acc: 0.5091
4480/4566 [============================>.] - ETA: 23s - loss: 0.7296 - acc: 0.5085
4544/4566 [============================>.] - ETA: 6s - loss: 0.7295 - acc: 0.5092 
4566/4566 [==============================] - 1372s 300ms/step - loss: 0.7301 - acc: 0.5083 - val_loss: 0.7052 - val_acc: 0.4803

Epoch 00001: val_acc improved from -inf to 0.48031, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window01/checkpoints/final_model_25years/liver_25years_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 9:30 - loss: 0.7329 - acc: 0.4531
 128/4566 [..............................] - ETA: 9:15 - loss: 0.7192 - acc: 0.5234
 192/4566 [>.............................] - ETA: 9:09 - loss: 0.6964 - acc: 0.5573
 256/4566 [>.............................] - ETA: 9:07 - loss: 0.6850 - acc: 0.5664
 320/4566 [=>............................] - ETA: 9:05 - loss: 0.6851 - acc: 0.5781
 384/4566 [=>............................] - ETA: 8:58 - loss: 0.6931 - acc: 0.5677
 448/4566 [=>............................] - ETA: 8:52 - loss: 0.6940 - acc: 0.5692
 512/4566 [==>...........................] - ETA: 8:47 - loss: 0.6940 - acc: 0.5645
 576/4566 [==>...........................] - ETA: 8:41 - loss: 0.6928 - acc: 0.5677
 640/4566 [===>..........................] - ETA: 8:35 - loss: 0.6977 - acc: 0.5641
 704/4566 [===>..........................] - ETA: 8:25 - loss: 0.6942 - acc: 0.5625
 768/4566 [====>.........................] - ETA: 8:17 - loss: 0.6931 - acc: 0.5651
 832/4566 [====>.........................] - ETA: 8:08 - loss: 0.6914 - acc: 0.5685
 896/4566 [====>.........................] - ETA: 8:01 - loss: 0.6930 - acc: 0.5647
 960/4566 [=====>........................] - ETA: 8:38 - loss: 0.6960 - acc: 0.5583
1024/4566 [=====>........................] - ETA: 9:44 - loss: 0.6967 - acc: 0.5596
1088/4566 [======>.......................] - ETA: 10:52 - loss: 0.6971 - acc: 0.5561
1152/4566 [======>.......................] - ETA: 11:37 - loss: 0.6933 - acc: 0.5616
1216/4566 [======>.......................] - ETA: 12:18 - loss: 0.6958 - acc: 0.5559
1280/4566 [=======>......................] - ETA: 12:57 - loss: 0.6968 - acc: 0.5555
1344/4566 [=======>......................] - ETA: 13:17 - loss: 0.6973 - acc: 0.5551
1408/4566 [========>.....................] - ETA: 12:59 - loss: 0.6954 - acc: 0.5582
1472/4566 [========>.....................] - ETA: 12:28 - loss: 0.6935 - acc: 0.5591
1536/4566 [=========>....................] - ETA: 11:57 - loss: 0.6955 - acc: 0.5566
1600/4566 [=========>....................] - ETA: 11:28 - loss: 0.6941 - acc: 0.5556
1664/4566 [=========>....................] - ETA: 11:02 - loss: 0.6945 - acc: 0.5553
1728/4566 [==========>...................] - ETA: 10:37 - loss: 0.6947 - acc: 0.5538
1792/4566 [==========>...................] - ETA: 10:14 - loss: 0.6949 - acc: 0.5536
1856/4566 [===========>..................] - ETA: 9:51 - loss: 0.6943 - acc: 0.5539 
1920/4566 [===========>..................] - ETA: 9:29 - loss: 0.6950 - acc: 0.5526
1984/4566 [============>.................] - ETA: 9:09 - loss: 0.6948 - acc: 0.5529
2048/4566 [============>.................] - ETA: 8:49 - loss: 0.6955 - acc: 0.5522
2112/4566 [============>.................] - ETA: 8:29 - loss: 0.6969 - acc: 0.5502
2176/4566 [=============>................] - ETA: 8:11 - loss: 0.6970 - acc: 0.5492
2240/4566 [=============>................] - ETA: 7:52 - loss: 0.6975 - acc: 0.5473
2304/4566 [==============>...............] - ETA: 7:35 - loss: 0.6976 - acc: 0.5477
2368/4566 [==============>...............] - ETA: 7:19 - loss: 0.6975 - acc: 0.5465
2432/4566 [==============>...............] - ETA: 7:12 - loss: 0.6972 - acc: 0.5469
2496/4566 [===============>..............] - ETA: 7:13 - loss: 0.6963 - acc: 0.5485
2560/4566 [===============>..............] - ETA: 7:14 - loss: 0.6962 - acc: 0.5465
2624/4566 [================>.............] - ETA: 7:14 - loss: 0.6955 - acc: 0.5469
2688/4566 [================>.............] - ETA: 7:12 - loss: 0.6962 - acc: 0.5446
2752/4566 [=================>............] - ETA: 7:10 - loss: 0.6969 - acc: 0.5436
2816/4566 [=================>............] - ETA: 7:04 - loss: 0.6976 - acc: 0.5437
2880/4566 [=================>............] - ETA: 6:49 - loss: 0.6974 - acc: 0.5451
2944/4566 [==================>...........] - ETA: 6:30 - loss: 0.6982 - acc: 0.5455
3008/4566 [==================>...........] - ETA: 6:11 - loss: 0.6985 - acc: 0.5439
3072/4566 [===================>..........] - ETA: 5:52 - loss: 0.6990 - acc: 0.5423
3136/4566 [===================>..........] - ETA: 5:34 - loss: 0.6986 - acc: 0.5434
3200/4566 [====================>.........] - ETA: 5:16 - loss: 0.6987 - acc: 0.5428
3264/4566 [====================>.........] - ETA: 4:59 - loss: 0.6979 - acc: 0.5450
3328/4566 [====================>.........] - ETA: 4:42 - loss: 0.6973 - acc: 0.5475
3392/4566 [=====================>........] - ETA: 4:25 - loss: 0.6980 - acc: 0.5448
3456/4566 [=====================>........] - ETA: 4:09 - loss: 0.6985 - acc: 0.5434
3520/4566 [======================>.......] - ETA: 3:52 - loss: 0.6989 - acc: 0.5435
3584/4566 [======================>.......] - ETA: 3:36 - loss: 0.6978 - acc: 0.5446
3648/4566 [======================>.......] - ETA: 3:21 - loss: 0.6972 - acc: 0.5441
3712/4566 [=======================>......] - ETA: 3:05 - loss: 0.6975 - acc: 0.5434
3776/4566 [=======================>......] - ETA: 2:50 - loss: 0.6973 - acc: 0.5445
3840/4566 [========================>.....] - ETA: 2:36 - loss: 0.6977 - acc: 0.5435
3904/4566 [========================>.....] - ETA: 2:22 - loss: 0.6983 - acc: 0.5420
3968/4566 [=========================>....] - ETA: 2:10 - loss: 0.6986 - acc: 0.5416
4032/4566 [=========================>....] - ETA: 1:59 - loss: 0.6987 - acc: 0.5417
4096/4566 [=========================>....] - ETA: 1:47 - loss: 0.6989 - acc: 0.5400
4160/4566 [==========================>...] - ETA: 1:34 - loss: 0.6991 - acc: 0.5397
4224/4566 [==========================>...] - ETA: 1:20 - loss: 0.6992 - acc: 0.5391
4288/4566 [===========================>..] - ETA: 1:06 - loss: 0.6990 - acc: 0.5385
4352/4566 [===========================>..] - ETA: 51s - loss: 0.6992 - acc: 0.5372 
4416/4566 [============================>.] - ETA: 36s - loss: 0.6996 - acc: 0.5365
4480/4566 [============================>.] - ETA: 20s - loss: 0.6995 - acc: 0.5357
4544/4566 [============================>.] - ETA: 5s - loss: 0.6994 - acc: 0.5352 
4566/4566 [==============================] - 1108s 243ms/step - loss: 0.6995 - acc: 0.5350 - val_loss: 0.6831 - val_acc: 0.5453

Epoch 00002: val_acc improved from 0.48031 to 0.54528, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window01/checkpoints/final_model_25years/liver_25years_model_2_mer.hdf5
Epoch 3/10

  64/4566 [..............................] - ETA: 9:41 - loss: 0.6997 - acc: 0.5156
 128/4566 [..............................] - ETA: 9:44 - loss: 0.7250 - acc: 0.5156
 192/4566 [>.............................] - ETA: 9:33 - loss: 0.7090 - acc: 0.5208
 256/4566 [>.............................] - ETA: 9:24 - loss: 0.6984 - acc: 0.5195
 320/4566 [=>............................] - ETA: 9:19 - loss: 0.6916 - acc: 0.5469
 384/4566 [=>............................] - ETA: 9:11 - loss: 0.6989 - acc: 0.5339
 448/4566 [=>............................] - ETA: 9:05 - loss: 0.6984 - acc: 0.5156
 512/4566 [==>...........................] - ETA: 9:00 - loss: 0.7033 - acc: 0.5020
 576/4566 [==>...........................] - ETA: 8:57 - loss: 0.6987 - acc: 0.5156
 640/4566 [===>..........................] - ETA: 9:57 - loss: 0.6988 - acc: 0.5234
 704/4566 [===>..........................] - ETA: 11:47 - loss: 0.6951 - acc: 0.5327
 768/4566 [====>.........................] - ETA: 13:30 - loss: 0.6946 - acc: 0.5339
 832/4566 [====>.........................] - ETA: 14:46 - loss: 0.6953 - acc: 0.5337
 896/4566 [====>.........................] - ETA: 15:41 - loss: 0.6925 - acc: 0.5379
 960/4566 [=====>........................] - ETA: 16:26 - loss: 0.6927 - acc: 0.5365
1024/4566 [=====>........................] - ETA: 16:46 - loss: 0.6924 - acc: 0.5293
1088/4566 [======>.......................] - ETA: 16:22 - loss: 0.6922 - acc: 0.5276
1152/4566 [======>.......................] - ETA: 15:35 - loss: 0.6936 - acc: 0.5243
1216/4566 [======>.......................] - ETA: 14:52 - loss: 0.6918 - acc: 0.5312
1280/4566 [=======>......................] - ETA: 14:12 - loss: 0.6912 - acc: 0.5312
1344/4566 [=======>......................] - ETA: 13:38 - loss: 0.6912 - acc: 0.5298
1408/4566 [========>.....................] - ETA: 13:04 - loss: 0.6904 - acc: 0.5320
1472/4566 [========>.....................] - ETA: 12:33 - loss: 0.6885 - acc: 0.5380
1536/4566 [=========>....................] - ETA: 12:02 - loss: 0.6897 - acc: 0.5345
1600/4566 [=========>....................] - ETA: 11:34 - loss: 0.6894 - acc: 0.5344
1664/4566 [=========>....................] - ETA: 11:08 - loss: 0.6884 - acc: 0.5379
1728/4566 [==========>...................] - ETA: 10:42 - loss: 0.6888 - acc: 0.5394
1792/4566 [==========>...................] - ETA: 10:18 - loss: 0.6873 - acc: 0.5435
1856/4566 [===========>..................] - ETA: 9:55 - loss: 0.6882 - acc: 0.5415 
1920/4566 [===========>..................] - ETA: 9:33 - loss: 0.6868 - acc: 0.5448
1984/4566 [============>.................] - ETA: 9:12 - loss: 0.6865 - acc: 0.5469
2048/4566 [============>.................] - ETA: 9:02 - loss: 0.6858 - acc: 0.5498
2112/4566 [============>.................] - ETA: 9:09 - loss: 0.6858 - acc: 0.5511
2176/4566 [=============>................] - ETA: 9:13 - loss: 0.6858 - acc: 0.5510
2240/4566 [=============>................] - ETA: 9:14 - loss: 0.6855 - acc: 0.5509
2304/4566 [==============>...............] - ETA: 9:14 - loss: 0.6856 - acc: 0.5508
2368/4566 [==============>...............] - ETA: 9:14 - loss: 0.6854 - acc: 0.5515
2432/4566 [==============>...............] - ETA: 9:11 - loss: 0.6864 - acc: 0.5477
2496/4566 [===============>..............] - ETA: 9:01 - loss: 0.6862 - acc: 0.5485
2560/4566 [===============>..............] - ETA: 8:39 - loss: 0.6861 - acc: 0.5484
2624/4566 [================>.............] - ETA: 8:16 - loss: 0.6870 - acc: 0.5457
2688/4566 [================>.............] - ETA: 7:54 - loss: 0.6868 - acc: 0.5461
2752/4566 [=================>............] - ETA: 7:32 - loss: 0.6867 - acc: 0.5465
2816/4566 [=================>............] - ETA: 7:12 - loss: 0.6861 - acc: 0.5487
2880/4566 [=================>............] - ETA: 6:52 - loss: 0.6869 - acc: 0.5462
2944/4566 [==================>...........] - ETA: 6:32 - loss: 0.6881 - acc: 0.5445
3008/4566 [==================>...........] - ETA: 6:13 - loss: 0.6879 - acc: 0.5452
3072/4566 [===================>..........] - ETA: 5:54 - loss: 0.6882 - acc: 0.5446
3136/4566 [===================>..........] - ETA: 5:36 - loss: 0.6873 - acc: 0.5475
3200/4566 [====================>.........] - ETA: 5:18 - loss: 0.6864 - acc: 0.5487
3264/4566 [====================>.........] - ETA: 5:00 - loss: 0.6857 - acc: 0.5496
3328/4566 [====================>.........] - ETA: 4:43 - loss: 0.6848 - acc: 0.5514
3392/4566 [=====================>........] - ETA: 4:26 - loss: 0.6855 - acc: 0.5498
3456/4566 [=====================>........] - ETA: 4:10 - loss: 0.6858 - acc: 0.5492
3520/4566 [======================>.......] - ETA: 3:59 - loss: 0.6870 - acc: 0.5457
3584/4566 [======================>.......] - ETA: 3:48 - loss: 0.6862 - acc: 0.5480
3648/4566 [======================>.......] - ETA: 3:38 - loss: 0.6857 - acc: 0.5482
3712/4566 [=======================>......] - ETA: 3:27 - loss: 0.6855 - acc: 0.5490
3776/4566 [=======================>......] - ETA: 3:14 - loss: 0.6853 - acc: 0.5493
3840/4566 [========================>.....] - ETA: 3:01 - loss: 0.6853 - acc: 0.5497
3904/4566 [========================>.....] - ETA: 2:48 - loss: 0.6851 - acc: 0.5499
3968/4566 [=========================>....] - ETA: 2:31 - loss: 0.6857 - acc: 0.5484
4032/4566 [=========================>....] - ETA: 2:14 - loss: 0.6857 - acc: 0.5486
4096/4566 [=========================>....] - ETA: 1:57 - loss: 0.6855 - acc: 0.5486
4160/4566 [==========================>...] - ETA: 1:40 - loss: 0.6857 - acc: 0.5490
4224/4566 [==========================>...] - ETA: 1:24 - loss: 0.6862 - acc: 0.5481
4288/4566 [===========================>..] - ETA: 1:08 - loss: 0.6865 - acc: 0.5476
4352/4566 [===========================>..] - ETA: 52s - loss: 0.6865 - acc: 0.5480 
4416/4566 [============================>.] - ETA: 36s - loss: 0.6864 - acc: 0.5482
4480/4566 [============================>.] - ETA: 20s - loss: 0.6861 - acc: 0.5496
4544/4566 [============================>.] - ETA: 5s - loss: 0.6868 - acc: 0.5482 
4566/4566 [==============================] - 1108s 243ms/step - loss: 0.6868 - acc: 0.5480 - val_loss: 0.6757 - val_acc: 0.5846

Epoch 00003: val_acc improved from 0.54528 to 0.58465, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window01/checkpoints/final_model_25years/liver_25years_model_2_mer.hdf5
Epoch 4/10

  64/4566 [..............................] - ETA: 10:06 - loss: 0.6585 - acc: 0.6562
 128/4566 [..............................] - ETA: 10:13 - loss: 0.7001 - acc: 0.5391
 192/4566 [>.............................] - ETA: 11:19 - loss: 0.7009 - acc: 0.5417
 256/4566 [>.............................] - ETA: 15:42 - loss: 0.6833 - acc: 0.5703
 320/4566 [=>............................] - ETA: 19:30 - loss: 0.6908 - acc: 0.5687
 384/4566 [=>............................] - ETA: 22:02 - loss: 0.6861 - acc: 0.5677
 448/4566 [=>............................] - ETA: 23:32 - loss: 0.6865 - acc: 0.5692
 512/4566 [==>...........................] - ETA: 24:18 - loss: 0.6842 - acc: 0.5723
 576/4566 [==>...........................] - ETA: 24:58 - loss: 0.6837 - acc: 0.5729
 640/4566 [===>..........................] - ETA: 24:49 - loss: 0.6830 - acc: 0.5703
 704/4566 [===>..........................] - ETA: 23:24 - loss: 0.6837 - acc: 0.5682
 768/4566 [====>.........................] - ETA: 21:44 - loss: 0.6859 - acc: 0.5638
 832/4566 [====>.........................] - ETA: 20:20 - loss: 0.6883 - acc: 0.5601
 896/4566 [====>.........................] - ETA: 19:08 - loss: 0.6895 - acc: 0.5580
 960/4566 [=====>........................] - ETA: 18:03 - loss: 0.6901 - acc: 0.5542
1024/4566 [=====>........................] - ETA: 17:06 - loss: 0.6892 - acc: 0.5537
1088/4566 [======>.......................] - ETA: 16:13 - loss: 0.6873 - acc: 0.5533
1152/4566 [======>.......................] - ETA: 15:26 - loss: 0.6881 - acc: 0.5512
1216/4566 [======>.......................] - ETA: 14:43 - loss: 0.6871 - acc: 0.5510
1280/4566 [=======>......................] - ETA: 14:03 - loss: 0.6872 - acc: 0.5531
1344/4566 [=======>......................] - ETA: 13:26 - loss: 0.6868 - acc: 0.5513
1408/4566 [========>.....................] - ETA: 12:51 - loss: 0.6856 - acc: 0.5547
1472/4566 [========>.....................] - ETA: 12:21 - loss: 0.6866 - acc: 0.5516
1536/4566 [=========>....................] - ETA: 11:52 - loss: 0.6888 - acc: 0.5482
1600/4566 [=========>....................] - ETA: 11:24 - loss: 0.6895 - acc: 0.5475
1664/4566 [=========>....................] - ETA: 10:57 - loss: 0.6909 - acc: 0.5445
1728/4566 [==========>...................] - ETA: 10:32 - loss: 0.6912 - acc: 0.5422
1792/4566 [==========>...................] - ETA: 10:08 - loss: 0.6918 - acc: 0.5407
1856/4566 [===========>..................] - ETA: 9:44 - loss: 0.6930 - acc: 0.5393 
1920/4566 [===========>..................] - ETA: 9:22 - loss: 0.6921 - acc: 0.5417
1984/4566 [============>.................] - ETA: 9:02 - loss: 0.6919 - acc: 0.5408
2048/4566 [============>.................] - ETA: 8:42 - loss: 0.6919 - acc: 0.5391
2112/4566 [============>.................] - ETA: 8:22 - loss: 0.6913 - acc: 0.5398
2176/4566 [=============>................] - ETA: 8:03 - loss: 0.6911 - acc: 0.5395
2240/4566 [=============>................] - ETA: 7:45 - loss: 0.6907 - acc: 0.5397
2304/4566 [==============>...............] - ETA: 7:28 - loss: 0.6921 - acc: 0.5369
2368/4566 [==============>...............] - ETA: 7:10 - loss: 0.6918 - acc: 0.5380
2432/4566 [==============>...............] - ETA: 6:54 - loss: 0.6921 - acc: 0.5366
2496/4566 [===============>..............] - ETA: 6:38 - loss: 0.6919 - acc: 0.5369
2560/4566 [===============>..............] - ETA: 6:22 - loss: 0.6914 - acc: 0.5375
2624/4566 [================>.............] - ETA: 6:07 - loss: 0.6913 - acc: 0.5389
2688/4566 [================>.............] - ETA: 5:52 - loss: 0.6914 - acc: 0.5383
2752/4566 [=================>............] - ETA: 5:37 - loss: 0.6912 - acc: 0.5411
2816/4566 [=================>............] - ETA: 5:23 - loss: 0.6913 - acc: 0.5401
2880/4566 [=================>............] - ETA: 5:09 - loss: 0.6907 - acc: 0.5399
2944/4566 [==================>...........] - ETA: 4:55 - loss: 0.6905 - acc: 0.5404
3008/4566 [==================>...........] - ETA: 4:42 - loss: 0.6899 - acc: 0.5419
3072/4566 [===================>..........] - ETA: 4:28 - loss: 0.6902 - acc: 0.5420
3136/4566 [===================>..........] - ETA: 4:15 - loss: 0.6898 - acc: 0.5415
3200/4566 [====================>.........] - ETA: 4:02 - loss: 0.6899 - acc: 0.5397
3264/4566 [====================>.........] - ETA: 3:49 - loss: 0.6904 - acc: 0.5383
3328/4566 [====================>.........] - ETA: 3:37 - loss: 0.6894 - acc: 0.5400
3392/4566 [=====================>........] - ETA: 3:24 - loss: 0.6893 - acc: 0.5398
3456/4566 [=====================>........] - ETA: 3:12 - loss: 0.6892 - acc: 0.5402
3520/4566 [======================>.......] - ETA: 3:00 - loss: 0.6887 - acc: 0.5406
3584/4566 [======================>.......] - ETA: 2:48 - loss: 0.6887 - acc: 0.5407
3648/4566 [======================>.......] - ETA: 2:36 - loss: 0.6879 - acc: 0.5428
3712/4566 [=======================>......] - ETA: 2:25 - loss: 0.6875 - acc: 0.5442
3776/4566 [=======================>......] - ETA: 2:13 - loss: 0.6875 - acc: 0.5450
3840/4566 [========================>.....] - ETA: 2:02 - loss: 0.6874 - acc: 0.5453
3904/4566 [========================>.....] - ETA: 1:51 - loss: 0.6867 - acc: 0.5471
3968/4566 [=========================>....] - ETA: 1:39 - loss: 0.6870 - acc: 0.5469
4032/4566 [=========================>....] - ETA: 1:28 - loss: 0.6873 - acc: 0.5469
4096/4566 [=========================>....] - ETA: 1:17 - loss: 0.6872 - acc: 0.5464
4160/4566 [==========================>...] - ETA: 1:07 - loss: 0.6867 - acc: 0.5478
4224/4566 [==========================>...] - ETA: 56s - loss: 0.6866 - acc: 0.5485 
4288/4566 [===========================>..] - ETA: 45s - loss: 0.6863 - acc: 0.5478
4352/4566 [===========================>..] - ETA: 35s - loss: 0.6867 - acc: 0.5478
4416/4566 [============================>.] - ETA: 24s - loss: 0.6867 - acc: 0.5478
4480/4566 [============================>.] - ETA: 14s - loss: 0.6872 - acc: 0.5469
4544/4566 [============================>.] - ETA: 3s - loss: 0.6867 - acc: 0.5475 
4566/4566 [==============================] - 799s 175ms/step - loss: 0.6865 - acc: 0.5477 - val_loss: 0.6940 - val_acc: 0.5295

Epoch 00004: val_acc did not improve from 0.58465
Epoch 5/10

  64/4566 [..............................] - ETA: 13:00 - loss: 0.6708 - acc: 0.6094
 128/4566 [..............................] - ETA: 10:54 - loss: 0.6758 - acc: 0.6016
 192/4566 [>.............................] - ETA: 10:11 - loss: 0.6877 - acc: 0.5573
 256/4566 [>.............................] - ETA: 9:39 - loss: 0.6889 - acc: 0.5430 
 320/4566 [=>............................] - ETA: 9:25 - loss: 0.6904 - acc: 0.5469
 384/4566 [=>............................] - ETA: 9:10 - loss: 0.6855 - acc: 0.5521
 448/4566 [=>............................] - ETA: 8:53 - loss: 0.6857 - acc: 0.5446
 512/4566 [==>...........................] - ETA: 8:41 - loss: 0.6837 - acc: 0.5430
 576/4566 [==>...........................] - ETA: 8:33 - loss: 0.6809 - acc: 0.5590
 640/4566 [===>..........................] - ETA: 8:26 - loss: 0.6810 - acc: 0.5547
 704/4566 [===>..........................] - ETA: 8:15 - loss: 0.6819 - acc: 0.5540
 768/4566 [====>.........................] - ETA: 8:05 - loss: 0.6782 - acc: 0.5638
 832/4566 [====>.........................] - ETA: 7:55 - loss: 0.6780 - acc: 0.5637
 896/4566 [====>.........................] - ETA: 7:46 - loss: 0.6806 - acc: 0.5625
 960/4566 [=====>........................] - ETA: 7:54 - loss: 0.6779 - acc: 0.5677
1024/4566 [=====>........................] - ETA: 8:09 - loss: 0.6776 - acc: 0.5664
1088/4566 [======>.......................] - ETA: 8:18 - loss: 0.6801 - acc: 0.5643
1152/4566 [======>.......................] - ETA: 8:22 - loss: 0.6814 - acc: 0.5608
1216/4566 [======>.......................] - ETA: 8:26 - loss: 0.6813 - acc: 0.5609
1280/4566 [=======>......................] - ETA: 8:28 - loss: 0.6806 - acc: 0.5609
1344/4566 [=======>......................] - ETA: 8:28 - loss: 0.6798 - acc: 0.5610
1408/4566 [========>.....................] - ETA: 8:14 - loss: 0.6793 - acc: 0.5646
1472/4566 [========>.....................] - ETA: 8:00 - loss: 0.6784 - acc: 0.5645
1536/4566 [=========>....................] - ETA: 7:46 - loss: 0.6778 - acc: 0.5664
1600/4566 [=========>....................] - ETA: 7:33 - loss: 0.6773 - acc: 0.5687
1664/4566 [=========>....................] - ETA: 7:20 - loss: 0.6802 - acc: 0.5649
1728/4566 [==========>...................] - ETA: 7:08 - loss: 0.6838 - acc: 0.5590
1792/4566 [==========>...................] - ETA: 6:55 - loss: 0.6854 - acc: 0.5586
1856/4566 [===========>..................] - ETA: 6:44 - loss: 0.6852 - acc: 0.5582
1920/4566 [===========>..................] - ETA: 6:32 - loss: 0.6852 - acc: 0.5578
1984/4566 [============>.................] - ETA: 6:21 - loss: 0.6844 - acc: 0.5620
2048/4566 [============>.................] - ETA: 6:09 - loss: 0.6821 - acc: 0.5674
2112/4566 [============>.................] - ETA: 5:59 - loss: 0.6836 - acc: 0.5649
2176/4566 [=============>................] - ETA: 5:48 - loss: 0.6840 - acc: 0.5630
2240/4566 [=============>................] - ETA: 5:38 - loss: 0.6833 - acc: 0.5643
2304/4566 [==============>...............] - ETA: 5:34 - loss: 0.6830 - acc: 0.5655
2368/4566 [==============>...............] - ETA: 5:30 - loss: 0.6818 - acc: 0.5667
2432/4566 [==============>...............] - ETA: 5:25 - loss: 0.6819 - acc: 0.5678
2496/4566 [===============>..............] - ETA: 5:19 - loss: 0.6809 - acc: 0.5705
2560/4566 [===============>..............] - ETA: 5:13 - loss: 0.6816 - acc: 0.5680
2624/4566 [================>.............] - ETA: 5:06 - loss: 0.6828 - acc: 0.5652
2688/4566 [================>.............] - ETA: 4:56 - loss: 0.6821 - acc: 0.5662
2752/4566 [=================>............] - ETA: 4:45 - loss: 0.6820 - acc: 0.5658
2816/4566 [=================>............] - ETA: 4:33 - loss: 0.6819 - acc: 0.5668
2880/4566 [=================>............] - ETA: 4:22 - loss: 0.6825 - acc: 0.5663
2944/4566 [==================>...........] - ETA: 4:11 - loss: 0.6831 - acc: 0.5642
3008/4566 [==================>...........] - ETA: 4:00 - loss: 0.6831 - acc: 0.5652
3072/4566 [===================>..........] - ETA: 3:50 - loss: 0.6830 - acc: 0.5654
3136/4566 [===================>..........] - ETA: 3:39 - loss: 0.6829 - acc: 0.5638
3200/4566 [====================>.........] - ETA: 3:28 - loss: 0.6828 - acc: 0.5653
3264/4566 [====================>.........] - ETA: 3:18 - loss: 0.6838 - acc: 0.5619
3328/4566 [====================>.........] - ETA: 3:07 - loss: 0.6844 - acc: 0.5610
3392/4566 [=====================>........] - ETA: 2:57 - loss: 0.6846 - acc: 0.5604
3456/4566 [=====================>........] - ETA: 2:47 - loss: 0.6849 - acc: 0.5611
3520/4566 [======================>.......] - ETA: 2:37 - loss: 0.6848 - acc: 0.5614
3584/4566 [======================>.......] - ETA: 2:28 - loss: 0.6855 - acc: 0.5589
3648/4566 [======================>.......] - ETA: 2:19 - loss: 0.6859 - acc: 0.5592
3712/4566 [=======================>......] - ETA: 2:11 - loss: 0.6862 - acc: 0.5585
3776/4566 [=======================>......] - ETA: 2:02 - loss: 0.6862 - acc: 0.5575
3840/4566 [========================>.....] - ETA: 1:53 - loss: 0.6867 - acc: 0.5573
3904/4566 [========================>.....] - ETA: 1:43 - loss: 0.6867 - acc: 0.5571
3968/4566 [=========================>....] - ETA: 1:34 - loss: 0.6870 - acc: 0.5565
4032/4566 [=========================>....] - ETA: 1:24 - loss: 0.6868 - acc: 0.5565
4096/4566 [=========================>....] - ETA: 1:13 - loss: 0.6875 - acc: 0.5557
4160/4566 [==========================>...] - ETA: 1:03 - loss: 0.6876 - acc: 0.5555
4224/4566 [==========================>...] - ETA: 53s - loss: 0.6870 - acc: 0.5575 
4288/4566 [===========================>..] - ETA: 43s - loss: 0.6874 - acc: 0.5571
4352/4566 [===========================>..] - ETA: 33s - loss: 0.6875 - acc: 0.5565
4416/4566 [============================>.] - ETA: 23s - loss: 0.6875 - acc: 0.5559
4480/4566 [============================>.] - ETA: 13s - loss: 0.6877 - acc: 0.5545
4544/4566 [============================>.] - ETA: 3s - loss: 0.6876 - acc: 0.5548 
4566/4566 [==============================] - 725s 159ms/step - loss: 0.6874 - acc: 0.5556 - val_loss: 0.6785 - val_acc: 0.5728

Epoch 00005: val_acc did not improve from 0.58465
Epoch 6/10

  64/4566 [..............................] - ETA: 9:44 - loss: 0.6611 - acc: 0.6094
 128/4566 [..............................] - ETA: 10:49 - loss: 0.6658 - acc: 0.6094
 192/4566 [>.............................] - ETA: 12:08 - loss: 0.6688 - acc: 0.5938
 256/4566 [>.............................] - ETA: 13:00 - loss: 0.6745 - acc: 0.5977
 320/4566 [=>............................] - ETA: 13:28 - loss: 0.6762 - acc: 0.5969
 384/4566 [=>............................] - ETA: 13:25 - loss: 0.6746 - acc: 0.5885
 448/4566 [=>............................] - ETA: 13:32 - loss: 0.6756 - acc: 0.5893
 512/4566 [==>...........................] - ETA: 13:28 - loss: 0.6733 - acc: 0.5918
 576/4566 [==>...........................] - ETA: 13:09 - loss: 0.6736 - acc: 0.5885
 640/4566 [===>..........................] - ETA: 12:31 - loss: 0.6771 - acc: 0.5719
 704/4566 [===>..........................] - ETA: 11:55 - loss: 0.6746 - acc: 0.5753
 768/4566 [====>.........................] - ETA: 11:23 - loss: 0.6735 - acc: 0.5794
 832/4566 [====>.........................] - ETA: 10:56 - loss: 0.6729 - acc: 0.5817
 896/4566 [====>.........................] - ETA: 10:30 - loss: 0.6712 - acc: 0.5837
 960/4566 [=====>........................] - ETA: 10:07 - loss: 0.6725 - acc: 0.5792
1024/4566 [=====>........................] - ETA: 9:46 - loss: 0.6764 - acc: 0.5752 
1088/4566 [======>.......................] - ETA: 9:27 - loss: 0.6769 - acc: 0.5781
1152/4566 [======>.......................] - ETA: 9:09 - loss: 0.6784 - acc: 0.5738
1216/4566 [======>.......................] - ETA: 8:51 - loss: 0.6783 - acc: 0.5757
1280/4566 [=======>......................] - ETA: 8:36 - loss: 0.6799 - acc: 0.5750
1344/4566 [=======>......................] - ETA: 8:21 - loss: 0.6802 - acc: 0.5744
1408/4566 [========>.....................] - ETA: 8:06 - loss: 0.6797 - acc: 0.5753
1472/4566 [========>.....................] - ETA: 7:52 - loss: 0.6777 - acc: 0.5761
1536/4566 [=========>....................] - ETA: 7:39 - loss: 0.6782 - acc: 0.5755
1600/4566 [=========>....................] - ETA: 7:34 - loss: 0.6755 - acc: 0.5806
1664/4566 [=========>....................] - ETA: 7:31 - loss: 0.6760 - acc: 0.5811
1728/4566 [==========>...................] - ETA: 7:27 - loss: 0.6751 - acc: 0.5793
1792/4566 [==========>...................] - ETA: 7:24 - loss: 0.6762 - acc: 0.5781
1856/4566 [===========>..................] - ETA: 7:19 - loss: 0.6785 - acc: 0.5744
1920/4566 [===========>..................] - ETA: 7:14 - loss: 0.6788 - acc: 0.5734
1984/4566 [============>.................] - ETA: 7:07 - loss: 0.6774 - acc: 0.5751
2048/4566 [============>.................] - ETA: 6:55 - loss: 0.6793 - acc: 0.5718
2112/4566 [============>.................] - ETA: 6:41 - loss: 0.6784 - acc: 0.5748
2176/4566 [=============>................] - ETA: 6:27 - loss: 0.6785 - acc: 0.5763
2240/4566 [=============>................] - ETA: 6:14 - loss: 0.6785 - acc: 0.5737
2304/4566 [==============>...............] - ETA: 6:02 - loss: 0.6784 - acc: 0.5738
2368/4566 [==============>...............] - ETA: 5:49 - loss: 0.6781 - acc: 0.5752
2432/4566 [==============>...............] - ETA: 5:37 - loss: 0.6791 - acc: 0.5732
2496/4566 [===============>..............] - ETA: 5:26 - loss: 0.6799 - acc: 0.5725
2560/4566 [===============>..............] - ETA: 5:15 - loss: 0.6802 - acc: 0.5711
2624/4566 [================>.............] - ETA: 5:03 - loss: 0.6807 - acc: 0.5701
2688/4566 [================>.............] - ETA: 4:52 - loss: 0.6801 - acc: 0.5714
2752/4566 [=================>............] - ETA: 4:41 - loss: 0.6804 - acc: 0.5712
2816/4566 [=================>............] - ETA: 4:30 - loss: 0.6800 - acc: 0.5717
2880/4566 [=================>............] - ETA: 4:19 - loss: 0.6791 - acc: 0.5740
2944/4566 [==================>...........] - ETA: 4:10 - loss: 0.6793 - acc: 0.5740
3008/4566 [==================>...........] - ETA: 4:02 - loss: 0.6800 - acc: 0.5721
3072/4566 [===================>..........] - ETA: 3:54 - loss: 0.6797 - acc: 0.5719
3136/4566 [===================>..........] - ETA: 3:46 - loss: 0.6798 - acc: 0.5714
3200/4566 [====================>.........] - ETA: 3:37 - loss: 0.6803 - acc: 0.5700
3264/4566 [====================>.........] - ETA: 3:28 - loss: 0.6810 - acc: 0.5677
3328/4566 [====================>.........] - ETA: 3:20 - loss: 0.6819 - acc: 0.5661
3392/4566 [=====================>........] - ETA: 3:10 - loss: 0.6821 - acc: 0.5663
3456/4566 [=====================>........] - ETA: 2:58 - loss: 0.6821 - acc: 0.5666
3520/4566 [======================>.......] - ETA: 2:47 - loss: 0.6825 - acc: 0.5659
3584/4566 [======================>.......] - ETA: 2:36 - loss: 0.6826 - acc: 0.5667
3648/4566 [======================>.......] - ETA: 2:26 - loss: 0.6832 - acc: 0.5647
3712/4566 [=======================>......] - ETA: 2:15 - loss: 0.6835 - acc: 0.5641
3776/4566 [=======================>......] - ETA: 2:04 - loss: 0.6833 - acc: 0.5636
3840/4566 [========================>.....] - ETA: 1:54 - loss: 0.6833 - acc: 0.5635
3904/4566 [========================>.....] - ETA: 1:44 - loss: 0.6832 - acc: 0.5640
3968/4566 [=========================>....] - ETA: 1:33 - loss: 0.6838 - acc: 0.5630
4032/4566 [=========================>....] - ETA: 1:23 - loss: 0.6837 - acc: 0.5635
4096/4566 [=========================>....] - ETA: 1:13 - loss: 0.6838 - acc: 0.5630
4160/4566 [==========================>...] - ETA: 1:03 - loss: 0.6840 - acc: 0.5625
4224/4566 [==========================>...] - ETA: 52s - loss: 0.6840 - acc: 0.5623 
4288/4566 [===========================>..] - ETA: 42s - loss: 0.6843 - acc: 0.5620
4352/4566 [===========================>..] - ETA: 33s - loss: 0.6843 - acc: 0.5632
4416/4566 [============================>.] - ETA: 23s - loss: 0.6842 - acc: 0.5634
4480/4566 [============================>.] - ETA: 13s - loss: 0.6837 - acc: 0.5636
4544/4566 [============================>.] - ETA: 3s - loss: 0.6835 - acc: 0.5643 
4566/4566 [==============================] - 760s 167ms/step - loss: 0.6835 - acc: 0.5640 - val_loss: 0.6749 - val_acc: 0.5728

Epoch 00006: val_acc did not improve from 0.58465
Epoch 7/10

  64/4566 [..............................] - ETA: 9:23 - loss: 0.6923 - acc: 0.5312
 128/4566 [..............................] - ETA: 9:12 - loss: 0.7040 - acc: 0.5234
 192/4566 [>.............................] - ETA: 9:02 - loss: 0.7014 - acc: 0.5104
 256/4566 [>.............................] - ETA: 8:50 - loss: 0.6936 - acc: 0.5391
 320/4566 [=>............................] - ETA: 8:43 - loss: 0.6921 - acc: 0.5500
 384/4566 [=>............................] - ETA: 8:37 - loss: 0.6884 - acc: 0.5573
 448/4566 [=>............................] - ETA: 8:30 - loss: 0.6863 - acc: 0.5603
 512/4566 [==>...........................] - ETA: 8:24 - loss: 0.6898 - acc: 0.5469
 576/4566 [==>...........................] - ETA: 8:19 - loss: 0.6869 - acc: 0.5486
 640/4566 [===>..........................] - ETA: 8:11 - loss: 0.6897 - acc: 0.5406
 704/4566 [===>..........................] - ETA: 8:03 - loss: 0.6911 - acc: 0.5426
 768/4566 [====>.........................] - ETA: 7:53 - loss: 0.6886 - acc: 0.5443
 832/4566 [====>.........................] - ETA: 7:46 - loss: 0.6853 - acc: 0.5541
 896/4566 [====>.........................] - ETA: 7:40 - loss: 0.6854 - acc: 0.5547
 960/4566 [=====>........................] - ETA: 7:52 - loss: 0.6835 - acc: 0.5563
1024/4566 [=====>........................] - ETA: 8:04 - loss: 0.6839 - acc: 0.5518
1088/4566 [======>.......................] - ETA: 8:14 - loss: 0.6809 - acc: 0.5561
1152/4566 [======>.......................] - ETA: 8:18 - loss: 0.6783 - acc: 0.5634
1216/4566 [======>.......................] - ETA: 8:19 - loss: 0.6777 - acc: 0.5625
1280/4566 [=======>......................] - ETA: 8:22 - loss: 0.6776 - acc: 0.5633
1344/4566 [=======>......................] - ETA: 8:19 - loss: 0.6772 - acc: 0.5647
1408/4566 [========>.....................] - ETA: 8:05 - loss: 0.6789 - acc: 0.5611
1472/4566 [========>.....................] - ETA: 7:50 - loss: 0.6766 - acc: 0.5645
1536/4566 [=========>....................] - ETA: 7:37 - loss: 0.6782 - acc: 0.5618
1600/4566 [=========>....................] - ETA: 7:25 - loss: 0.6785 - acc: 0.5600
1664/4566 [=========>....................] - ETA: 7:12 - loss: 0.6779 - acc: 0.5643
1728/4566 [==========>...................] - ETA: 7:00 - loss: 0.6777 - acc: 0.5683
1792/4566 [==========>...................] - ETA: 6:49 - loss: 0.6788 - acc: 0.5653
1856/4566 [===========>..................] - ETA: 6:38 - loss: 0.6773 - acc: 0.5679
1920/4566 [===========>..................] - ETA: 6:27 - loss: 0.6788 - acc: 0.5641
1984/4566 [============>.................] - ETA: 6:16 - loss: 0.6791 - acc: 0.5625
2048/4566 [============>.................] - ETA: 6:05 - loss: 0.6792 - acc: 0.5640
2112/4566 [============>.................] - ETA: 5:54 - loss: 0.6787 - acc: 0.5644
2176/4566 [=============>................] - ETA: 5:43 - loss: 0.6784 - acc: 0.5666
2240/4566 [=============>................] - ETA: 5:33 - loss: 0.6784 - acc: 0.5674
2304/4566 [==============>...............] - ETA: 5:27 - loss: 0.6785 - acc: 0.5668
2368/4566 [==============>...............] - ETA: 5:23 - loss: 0.6792 - acc: 0.5659
2432/4566 [==============>...............] - ETA: 5:17 - loss: 0.6794 - acc: 0.5646
2496/4566 [===============>..............] - ETA: 5:12 - loss: 0.6792 - acc: 0.5649
2560/4566 [===============>..............] - ETA: 5:05 - loss: 0.6800 - acc: 0.5629
2624/4566 [================>.............] - ETA: 4:58 - loss: 0.6799 - acc: 0.5652
2688/4566 [================>.............] - ETA: 4:51 - loss: 0.6797 - acc: 0.5651
2752/4566 [=================>............] - ETA: 4:40 - loss: 0.6796 - acc: 0.5650
2816/4566 [=================>............] - ETA: 4:29 - loss: 0.6796 - acc: 0.5650
2880/4566 [=================>............] - ETA: 4:19 - loss: 0.6795 - acc: 0.5660
2944/4566 [==================>...........] - ETA: 4:08 - loss: 0.6792 - acc: 0.5669
3008/4566 [==================>...........] - ETA: 3:57 - loss: 0.6800 - acc: 0.5658
3072/4566 [===================>..........] - ETA: 3:46 - loss: 0.6789 - acc: 0.5677
3136/4566 [===================>..........] - ETA: 3:36 - loss: 0.6793 - acc: 0.5660
3200/4566 [====================>.........] - ETA: 3:25 - loss: 0.6792 - acc: 0.5659
3264/4566 [====================>.........] - ETA: 3:15 - loss: 0.6786 - acc: 0.5671
3328/4566 [====================>.........] - ETA: 3:04 - loss: 0.6788 - acc: 0.5673
3392/4566 [=====================>........] - ETA: 2:54 - loss: 0.6796 - acc: 0.5669
3456/4566 [=====================>........] - ETA: 2:44 - loss: 0.6807 - acc: 0.5668
3520/4566 [======================>.......] - ETA: 2:34 - loss: 0.6809 - acc: 0.5668
3584/4566 [======================>.......] - ETA: 2:24 - loss: 0.6806 - acc: 0.5678
3648/4566 [======================>.......] - ETA: 2:14 - loss: 0.6801 - acc: 0.5680
3712/4566 [=======================>......] - ETA: 2:06 - loss: 0.6799 - acc: 0.5684
3776/4566 [=======================>......] - ETA: 1:58 - loss: 0.6807 - acc: 0.5662
3840/4566 [========================>.....] - ETA: 1:49 - loss: 0.6805 - acc: 0.5674
3904/4566 [========================>.....] - ETA: 1:40 - loss: 0.6806 - acc: 0.5676
3968/4566 [=========================>....] - ETA: 1:31 - loss: 0.6801 - acc: 0.5688
4032/4566 [=========================>....] - ETA: 1:22 - loss: 0.6805 - acc: 0.5677
4096/4566 [=========================>....] - ETA: 1:12 - loss: 0.6795 - acc: 0.5698
4160/4566 [==========================>...] - ETA: 1:02 - loss: 0.6792 - acc: 0.5697
4224/4566 [==========================>...] - ETA: 52s - loss: 0.6797 - acc: 0.5687 
4288/4566 [===========================>..] - ETA: 42s - loss: 0.6799 - acc: 0.5681
4352/4566 [===========================>..] - ETA: 32s - loss: 0.6803 - acc: 0.5680
4416/4566 [============================>.] - ETA: 22s - loss: 0.6802 - acc: 0.5686
4480/4566 [============================>.] - ETA: 12s - loss: 0.6801 - acc: 0.5692
4544/4566 [============================>.] - ETA: 3s - loss: 0.6799 - acc: 0.5693 
4566/4566 [==============================] - 705s 154ms/step - loss: 0.6797 - acc: 0.5692 - val_loss: 0.6706 - val_acc: 0.5886

Epoch 00007: val_acc improved from 0.58465 to 0.58858, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window01/checkpoints/final_model_25years/liver_25years_model_2_mer.hdf5
Epoch 8/10

  64/4566 [..............................] - ETA: 8:57 - loss: 0.6858 - acc: 0.5781
 128/4566 [..............................] - ETA: 8:48 - loss: 0.6714 - acc: 0.6016
 192/4566 [>.............................] - ETA: 8:34 - loss: 0.6742 - acc: 0.5938
 256/4566 [>.............................] - ETA: 8:38 - loss: 0.6839 - acc: 0.5898
 320/4566 [=>............................] - ETA: 9:55 - loss: 0.6891 - acc: 0.5781
 384/4566 [=>............................] - ETA: 10:50 - loss: 0.6851 - acc: 0.5807
 448/4566 [=>............................] - ETA: 11:12 - loss: 0.6856 - acc: 0.5826
 512/4566 [==>...........................] - ETA: 11:27 - loss: 0.6874 - acc: 0.5723
 576/4566 [==>...........................] - ETA: 11:38 - loss: 0.6917 - acc: 0.5642
 640/4566 [===>..........................] - ETA: 11:40 - loss: 0.6899 - acc: 0.5656
 704/4566 [===>..........................] - ETA: 11:33 - loss: 0.6885 - acc: 0.5668
 768/4566 [====>.........................] - ETA: 10:59 - loss: 0.6896 - acc: 0.5664
 832/4566 [====>.........................] - ETA: 10:30 - loss: 0.6911 - acc: 0.5601
 896/4566 [====>.........................] - ETA: 10:05 - loss: 0.6914 - acc: 0.5536
 960/4566 [=====>........................] - ETA: 9:43 - loss: 0.6922 - acc: 0.5531 
1024/4566 [=====>........................] - ETA: 9:23 - loss: 0.6891 - acc: 0.5605
1088/4566 [======>.......................] - ETA: 9:05 - loss: 0.6871 - acc: 0.5662
1152/4566 [======>.......................] - ETA: 8:49 - loss: 0.6854 - acc: 0.5668
1216/4566 [======>.......................] - ETA: 8:32 - loss: 0.6845 - acc: 0.5691
1280/4566 [=======>......................] - ETA: 8:18 - loss: 0.6841 - acc: 0.5703
1344/4566 [=======>......................] - ETA: 8:04 - loss: 0.6829 - acc: 0.5707
1408/4566 [========>.....................] - ETA: 7:49 - loss: 0.6851 - acc: 0.5661
1472/4566 [========>.....................] - ETA: 7:35 - loss: 0.6861 - acc: 0.5639
1536/4566 [=========>....................] - ETA: 7:23 - loss: 0.6867 - acc: 0.5612
1600/4566 [=========>....................] - ETA: 7:10 - loss: 0.6864 - acc: 0.5613
1664/4566 [=========>....................] - ETA: 7:01 - loss: 0.6865 - acc: 0.5607
1728/4566 [==========>...................] - ETA: 7:00 - loss: 0.6861 - acc: 0.5613
1792/4566 [==========>...................] - ETA: 6:59 - loss: 0.6841 - acc: 0.5647
1856/4566 [===========>..................] - ETA: 6:57 - loss: 0.6844 - acc: 0.5641
1920/4566 [===========>..................] - ETA: 6:52 - loss: 0.6846 - acc: 0.5630
1984/4566 [============>.................] - ETA: 6:47 - loss: 0.6866 - acc: 0.5600
2048/4566 [============>.................] - ETA: 6:42 - loss: 0.6872 - acc: 0.5596
2112/4566 [============>.................] - ETA: 6:32 - loss: 0.6865 - acc: 0.5625
2176/4566 [=============>................] - ETA: 6:19 - loss: 0.6876 - acc: 0.5616
2240/4566 [=============>................] - ETA: 6:06 - loss: 0.6875 - acc: 0.5589
2304/4566 [==============>...............] - ETA: 5:53 - loss: 0.6875 - acc: 0.5586
2368/4566 [==============>...............] - ETA: 5:41 - loss: 0.6870 - acc: 0.5604
2432/4566 [==============>...............] - ETA: 5:30 - loss: 0.6879 - acc: 0.5609
2496/4566 [===============>..............] - ETA: 5:19 - loss: 0.6874 - acc: 0.5629
2560/4566 [===============>..............] - ETA: 5:07 - loss: 0.6856 - acc: 0.5656
2624/4566 [================>.............] - ETA: 4:56 - loss: 0.6845 - acc: 0.5694
2688/4566 [================>.............] - ETA: 4:45 - loss: 0.6838 - acc: 0.5707
2752/4566 [=================>............] - ETA: 4:34 - loss: 0.6840 - acc: 0.5690
2816/4566 [=================>............] - ETA: 4:23 - loss: 0.6835 - acc: 0.5692
2880/4566 [=================>............] - ETA: 4:13 - loss: 0.6836 - acc: 0.5674
2944/4566 [==================>...........] - ETA: 4:02 - loss: 0.6829 - acc: 0.5679
3008/4566 [==================>...........] - ETA: 3:52 - loss: 0.6826 - acc: 0.5688
3072/4566 [===================>..........] - ETA: 3:45 - loss: 0.6818 - acc: 0.5710
3136/4566 [===================>..........] - ETA: 3:37 - loss: 0.6821 - acc: 0.5695
3200/4566 [====================>.........] - ETA: 3:29 - loss: 0.6822 - acc: 0.5691
3264/4566 [====================>.........] - ETA: 3:21 - loss: 0.6823 - acc: 0.5695
3328/4566 [====================>.........] - ETA: 3:13 - loss: 0.6828 - acc: 0.5688
3392/4566 [=====================>........] - ETA: 3:04 - loss: 0.6829 - acc: 0.5678
3456/4566 [=====================>........] - ETA: 2:55 - loss: 0.6824 - acc: 0.5683
3520/4566 [======================>.......] - ETA: 2:44 - loss: 0.6824 - acc: 0.5673
3584/4566 [======================>.......] - ETA: 2:33 - loss: 0.6814 - acc: 0.5706
3648/4566 [======================>.......] - ETA: 2:23 - loss: 0.6805 - acc: 0.5724
3712/4566 [=======================>......] - ETA: 2:12 - loss: 0.6800 - acc: 0.5735
3776/4566 [=======================>......] - ETA: 2:02 - loss: 0.6799 - acc: 0.5742
3840/4566 [========================>.....] - ETA: 1:52 - loss: 0.6811 - acc: 0.5716
3904/4566 [========================>.....] - ETA: 1:41 - loss: 0.6815 - acc: 0.5717
3968/4566 [=========================>....] - ETA: 1:31 - loss: 0.6814 - acc: 0.5713
4032/4566 [=========================>....] - ETA: 1:21 - loss: 0.6807 - acc: 0.5734
4096/4566 [=========================>....] - ETA: 1:11 - loss: 0.6802 - acc: 0.5745
4160/4566 [==========================>...] - ETA: 1:01 - loss: 0.6797 - acc: 0.5760
4224/4566 [==========================>...] - ETA: 51s - loss: 0.6797 - acc: 0.5767 
4288/4566 [===========================>..] - ETA: 41s - loss: 0.6791 - acc: 0.5779
4352/4566 [===========================>..] - ETA: 32s - loss: 0.6791 - acc: 0.5777
4416/4566 [============================>.] - ETA: 22s - loss: 0.6789 - acc: 0.5779
4480/4566 [============================>.] - ETA: 13s - loss: 0.6787 - acc: 0.5779
4544/4566 [============================>.] - ETA: 3s - loss: 0.6786 - acc: 0.5775 
4566/4566 [==============================] - 743s 163ms/step - loss: 0.6785 - acc: 0.5777 - val_loss: 0.6825 - val_acc: 0.5630

Epoch 00008: val_acc did not improve from 0.58858
Epoch 9/10

  64/4566 [..............................] - ETA: 14:30 - loss: 0.6442 - acc: 0.6094
 128/4566 [..............................] - ETA: 11:57 - loss: 0.6646 - acc: 0.5625
 192/4566 [>.............................] - ETA: 10:28 - loss: 0.6486 - acc: 0.5990
 256/4566 [>.............................] - ETA: 9:53 - loss: 0.6462 - acc: 0.6016 
 320/4566 [=>............................] - ETA: 9:34 - loss: 0.6509 - acc: 0.5938
 384/4566 [=>............................] - ETA: 9:15 - loss: 0.6619 - acc: 0.5885
 448/4566 [=>............................] - ETA: 8:58 - loss: 0.6741 - acc: 0.5826
 512/4566 [==>...........................] - ETA: 8:42 - loss: 0.6707 - acc: 0.5781
 576/4566 [==>...........................] - ETA: 8:32 - loss: 0.6684 - acc: 0.5816
 640/4566 [===>..........................] - ETA: 8:20 - loss: 0.6679 - acc: 0.5844
 704/4566 [===>..........................] - ETA: 8:09 - loss: 0.6653 - acc: 0.5881
 768/4566 [====>.........................] - ETA: 7:58 - loss: 0.6660 - acc: 0.5846
 832/4566 [====>.........................] - ETA: 7:49 - loss: 0.6642 - acc: 0.5877
 896/4566 [====>.........................] - ETA: 7:39 - loss: 0.6657 - acc: 0.5915
 960/4566 [=====>........................] - ETA: 7:29 - loss: 0.6656 - acc: 0.5948
1024/4566 [=====>........................] - ETA: 7:25 - loss: 0.6637 - acc: 0.6006
1088/4566 [======>.......................] - ETA: 7:33 - loss: 0.6622 - acc: 0.6029
1152/4566 [======>.......................] - ETA: 7:40 - loss: 0.6604 - acc: 0.6068
1216/4566 [======>.......................] - ETA: 7:47 - loss: 0.6610 - acc: 0.6020
1280/4566 [=======>......................] - ETA: 7:51 - loss: 0.6658 - acc: 0.5945
1344/4566 [=======>......................] - ETA: 7:52 - loss: 0.6659 - acc: 0.5923
1408/4566 [========>.....................] - ETA: 7:51 - loss: 0.6669 - acc: 0.5916
1472/4566 [========>.....................] - ETA: 7:48 - loss: 0.6663 - acc: 0.5931
1536/4566 [=========>....................] - ETA: 7:34 - loss: 0.6665 - acc: 0.5951
1600/4566 [=========>....................] - ETA: 7:21 - loss: 0.6674 - acc: 0.5944
1664/4566 [=========>....................] - ETA: 7:08 - loss: 0.6673 - acc: 0.5974
1728/4566 [==========>...................] - ETA: 6:56 - loss: 0.6681 - acc: 0.5978
1792/4566 [==========>...................] - ETA: 6:43 - loss: 0.6666 - acc: 0.5993
1856/4566 [===========>..................] - ETA: 6:31 - loss: 0.6671 - acc: 0.5981
1920/4566 [===========>..................] - ETA: 6:19 - loss: 0.6675 - acc: 0.5984
1984/4566 [============>.................] - ETA: 6:08 - loss: 0.6676 - acc: 0.5978
2048/4566 [============>.................] - ETA: 5:57 - loss: 0.6689 - acc: 0.5962
2112/4566 [============>.................] - ETA: 5:47 - loss: 0.6695 - acc: 0.5966
2176/4566 [=============>................] - ETA: 5:36 - loss: 0.6694 - acc: 0.5960
2240/4566 [=============>................] - ETA: 5:26 - loss: 0.6700 - acc: 0.5969
2304/4566 [==============>...............] - ETA: 5:16 - loss: 0.6687 - acc: 0.5985
2368/4566 [==============>...............] - ETA: 5:05 - loss: 0.6710 - acc: 0.5946
2432/4566 [==============>...............] - ETA: 4:56 - loss: 0.6703 - acc: 0.5950
2496/4566 [===============>..............] - ETA: 4:51 - loss: 0.6696 - acc: 0.5962
2560/4566 [===============>..............] - ETA: 4:47 - loss: 0.6704 - acc: 0.5953
2624/4566 [================>.............] - ETA: 4:42 - loss: 0.6702 - acc: 0.5957
2688/4566 [================>.............] - ETA: 4:35 - loss: 0.6717 - acc: 0.5911
2752/4566 [=================>............] - ETA: 4:29 - loss: 0.6728 - acc: 0.5883
2816/4566 [=================>............] - ETA: 4:23 - loss: 0.6726 - acc: 0.5884
2880/4566 [=================>............] - ETA: 4:14 - loss: 0.6726 - acc: 0.5878
2944/4566 [==================>...........] - ETA: 4:03 - loss: 0.6720 - acc: 0.5880
3008/4566 [==================>...........] - ETA: 3:52 - loss: 0.6716 - acc: 0.5888
3072/4566 [===================>..........] - ETA: 3:42 - loss: 0.6714 - acc: 0.5885
3136/4566 [===================>..........] - ETA: 3:31 - loss: 0.6707 - acc: 0.5906
3200/4566 [====================>.........] - ETA: 3:21 - loss: 0.6703 - acc: 0.5913
3264/4566 [====================>.........] - ETA: 3:11 - loss: 0.6696 - acc: 0.5910
3328/4566 [====================>.........] - ETA: 3:01 - loss: 0.6696 - acc: 0.5904
3392/4566 [=====================>........] - ETA: 2:51 - loss: 0.6693 - acc: 0.5923
3456/4566 [=====================>........] - ETA: 2:41 - loss: 0.6692 - acc: 0.5932
3520/4566 [======================>.......] - ETA: 2:31 - loss: 0.6695 - acc: 0.5926
3584/4566 [======================>.......] - ETA: 2:21 - loss: 0.6696 - acc: 0.5921
3648/4566 [======================>.......] - ETA: 2:12 - loss: 0.6701 - acc: 0.5910
3712/4566 [=======================>......] - ETA: 2:02 - loss: 0.6707 - acc: 0.5897
3776/4566 [=======================>......] - ETA: 1:53 - loss: 0.6703 - acc: 0.5903
3840/4566 [========================>.....] - ETA: 1:43 - loss: 0.6706 - acc: 0.5906
3904/4566 [========================>.....] - ETA: 1:35 - loss: 0.6709 - acc: 0.5894
3968/4566 [=========================>....] - ETA: 1:27 - loss: 0.6716 - acc: 0.5885
4032/4566 [=========================>....] - ETA: 1:18 - loss: 0.6715 - acc: 0.5888
4096/4566 [=========================>....] - ETA: 1:09 - loss: 0.6722 - acc: 0.5881
4160/4566 [==========================>...] - ETA: 1:00 - loss: 0.6722 - acc: 0.5885
4224/4566 [==========================>...] - ETA: 51s - loss: 0.6722 - acc: 0.5890 
4288/4566 [===========================>..] - ETA: 42s - loss: 0.6726 - acc: 0.5872
4352/4566 [===========================>..] - ETA: 32s - loss: 0.6733 - acc: 0.5866
4416/4566 [============================>.] - ETA: 22s - loss: 0.6737 - acc: 0.5861
4480/4566 [============================>.] - ETA: 12s - loss: 0.6740 - acc: 0.5848
4544/4566 [============================>.] - ETA: 3s - loss: 0.6743 - acc: 0.5847 
4566/4566 [==============================] - 704s 154ms/step - loss: 0.6744 - acc: 0.5848 - val_loss: 0.6734 - val_acc: 0.5846

Epoch 00009: val_acc did not improve from 0.58858
Epoch 10/10

  64/4566 [..............................] - ETA: 9:35 - loss: 0.7043 - acc: 0.4531
 128/4566 [..............................] - ETA: 9:13 - loss: 0.6807 - acc: 0.5391
 192/4566 [>.............................] - ETA: 8:58 - loss: 0.6619 - acc: 0.6094
 256/4566 [>.............................] - ETA: 8:50 - loss: 0.6662 - acc: 0.5938
 320/4566 [=>............................] - ETA: 8:34 - loss: 0.6733 - acc: 0.5656
 384/4566 [=>............................] - ETA: 8:22 - loss: 0.6710 - acc: 0.5755
 448/4566 [=>............................] - ETA: 8:18 - loss: 0.6716 - acc: 0.5625
 512/4566 [==>...........................] - ETA: 8:45 - loss: 0.6669 - acc: 0.5742
 576/4566 [==>...........................] - ETA: 9:23 - loss: 0.6685 - acc: 0.5677
 640/4566 [===>..........................] - ETA: 9:49 - loss: 0.6699 - acc: 0.5703
 704/4566 [===>..........................] - ETA: 10:00 - loss: 0.6740 - acc: 0.5668
 768/4566 [====>.........................] - ETA: 10:09 - loss: 0.6753 - acc: 0.5677
 832/4566 [====>.........................] - ETA: 10:18 - loss: 0.6734 - acc: 0.5745
 896/4566 [====>.........................] - ETA: 10:20 - loss: 0.6768 - acc: 0.5647
 960/4566 [=====>........................] - ETA: 9:55 - loss: 0.6752 - acc: 0.5646 
1024/4566 [=====>........................] - ETA: 9:33 - loss: 0.6747 - acc: 0.5645
1088/4566 [======>.......................] - ETA: 9:14 - loss: 0.6744 - acc: 0.5643
1152/4566 [======>.......................] - ETA: 8:56 - loss: 0.6740 - acc: 0.5651
1216/4566 [======>.......................] - ETA: 8:38 - loss: 0.6747 - acc: 0.5658
1280/4566 [=======>......................] - ETA: 8:22 - loss: 0.6735 - acc: 0.5680
1344/4566 [=======>......................] - ETA: 8:05 - loss: 0.6726 - acc: 0.5677
1408/4566 [========>.....................] - ETA: 7:51 - loss: 0.6741 - acc: 0.5703
1472/4566 [========>.....................] - ETA: 7:37 - loss: 0.6733 - acc: 0.5740
1536/4566 [=========>....................] - ETA: 7:24 - loss: 0.6727 - acc: 0.5749
1600/4566 [=========>....................] - ETA: 7:11 - loss: 0.6722 - acc: 0.5763
1664/4566 [=========>....................] - ETA: 6:58 - loss: 0.6710 - acc: 0.5793
1728/4566 [==========>...................] - ETA: 6:46 - loss: 0.6712 - acc: 0.5810
1792/4566 [==========>...................] - ETA: 6:33 - loss: 0.6712 - acc: 0.5809
1856/4566 [===========>..................] - ETA: 6:24 - loss: 0.6719 - acc: 0.5787
1920/4566 [===========>..................] - ETA: 6:21 - loss: 0.6721 - acc: 0.5802
1984/4566 [============>.................] - ETA: 6:17 - loss: 0.6724 - acc: 0.5806
2048/4566 [============>.................] - ETA: 6:14 - loss: 0.6745 - acc: 0.5776
2112/4566 [============>.................] - ETA: 6:09 - loss: 0.6746 - acc: 0.5786
2176/4566 [=============>................] - ETA: 6:05 - loss: 0.6745 - acc: 0.5809
2240/4566 [=============>................] - ETA: 6:00 - loss: 0.6742 - acc: 0.5808
2304/4566 [==============>...............] - ETA: 5:51 - loss: 0.6747 - acc: 0.5803
2368/4566 [==============>...............] - ETA: 5:39 - loss: 0.6750 - acc: 0.5790
2432/4566 [==============>...............] - ETA: 5:26 - loss: 0.6750 - acc: 0.5814
2496/4566 [===============>..............] - ETA: 5:14 - loss: 0.6756 - acc: 0.5793
2560/4566 [===============>..............] - ETA: 5:03 - loss: 0.6757 - acc: 0.5797
2624/4566 [================>.............] - ETA: 4:52 - loss: 0.6765 - acc: 0.5789
2688/4566 [================>.............] - ETA: 4:41 - loss: 0.6762 - acc: 0.5792
2752/4566 [=================>............] - ETA: 4:30 - loss: 0.6764 - acc: 0.5796
2816/4566 [=================>............] - ETA: 4:19 - loss: 0.6767 - acc: 0.5788
2880/4566 [=================>............] - ETA: 4:08 - loss: 0.6769 - acc: 0.5795
2944/4566 [==================>...........] - ETA: 3:58 - loss: 0.6775 - acc: 0.5774
3008/4566 [==================>...........] - ETA: 3:47 - loss: 0.6767 - acc: 0.5785
3072/4566 [===================>..........] - ETA: 3:37 - loss: 0.6766 - acc: 0.5785
3136/4566 [===================>..........] - ETA: 3:27 - loss: 0.6764 - acc: 0.5781
3200/4566 [====================>.........] - ETA: 3:17 - loss: 0.6767 - acc: 0.5778
3264/4566 [====================>.........] - ETA: 3:07 - loss: 0.6777 - acc: 0.5751
3328/4566 [====================>.........] - ETA: 3:00 - loss: 0.6768 - acc: 0.5769
3392/4566 [=====================>........] - ETA: 2:53 - loss: 0.6772 - acc: 0.5767
3456/4566 [=====================>........] - ETA: 2:45 - loss: 0.6768 - acc: 0.5775
3520/4566 [======================>.......] - ETA: 2:36 - loss: 0.6764 - acc: 0.5781
3584/4566 [======================>.......] - ETA: 2:28 - loss: 0.6768 - acc: 0.5776
3648/4566 [======================>.......] - ETA: 2:19 - loss: 0.6768 - acc: 0.5776
3712/4566 [=======================>......] - ETA: 2:09 - loss: 0.6769 - acc: 0.5779
3776/4566 [=======================>......] - ETA: 1:59 - loss: 0.6767 - acc: 0.5787
3840/4566 [========================>.....] - ETA: 1:49 - loss: 0.6770 - acc: 0.5773
3904/4566 [========================>.....] - ETA: 1:39 - loss: 0.6769 - acc: 0.5768
3968/4566 [=========================>....] - ETA: 1:29 - loss: 0.6770 - acc: 0.5766
4032/4566 [=========================>....] - ETA: 1:19 - loss: 0.6768 - acc: 0.5776
4096/4566 [=========================>....] - ETA: 1:09 - loss: 0.6764 - acc: 0.5779
4160/4566 [==========================>...] - ETA: 59s - loss: 0.6759 - acc: 0.5779 
4224/4566 [==========================>...] - ETA: 50s - loss: 0.6761 - acc: 0.5774
4288/4566 [===========================>..] - ETA: 40s - loss: 0.6762 - acc: 0.5781
4352/4566 [===========================>..] - ETA: 31s - loss: 0.6754 - acc: 0.5802
4416/4566 [============================>.] - ETA: 21s - loss: 0.6752 - acc: 0.5804
4480/4566 [============================>.] - ETA: 12s - loss: 0.6747 - acc: 0.5810
4544/4566 [============================>.] - ETA: 3s - loss: 0.6743 - acc: 0.5821 
4566/4566 [==============================] - 683s 150ms/step - loss: 0.6742 - acc: 0.5821 - val_loss: 0.6718 - val_acc: 0.5807

Epoch 00010: val_acc did not improve from 0.58858
Saved model to disk
