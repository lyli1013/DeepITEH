/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f731c00b050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f731c00b050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f731bef3b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f731bef3b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f731bef3d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f731bef3d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f731bec37d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f731bec37d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7305f509d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7305f509d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7305df8250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7305df8250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f731bec3f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f731bec3f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7305df3590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7305df3590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7305cc3bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7305cc3bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7305b97790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7305b97790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7305cfe890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7305cfe890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7305cc33d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7305cc33d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7305a41110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7305a41110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7305ed1fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7305ed1fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72fd8b7550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72fd8b7550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7305a1b290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7305a1b290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7305992210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7305992210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72fd7266d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72fd7266d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72fd6d7e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72fd6d7e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72fd634c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72fd634c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72fd8b3850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72fd8b3850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72fd6d7310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72fd6d7310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f730590b510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f730590b510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72fd62e050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72fd62e050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72fd2be490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72fd2be490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72fd360cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72fd360cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72fd33e210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72fd33e210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72fd3e6950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72fd3e6950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72fd2cea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72fd2cea10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72fd26d350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72fd26d350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ecfec5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ecfec5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72fd171d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72fd171d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ed096c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ed096c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72ecf0f090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72ecf0f090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72ecda2650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72ecda2650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ecfef650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ecfef650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72fd26d350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72fd26d350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ecaa0e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ecaa0e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72ecfefb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72ecfefb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72ec919710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72ec919710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ecb150d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ecb150d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72eca4a4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72eca4a4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ec8c4cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72ec8c4cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f729473a110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f729473a110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72945e6f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72945e6f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72946ccd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72946ccd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f729473a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f729473a550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72944db0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72944db0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7294648850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7294648850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72943e3c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72943e3c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72943af210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72943af210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7294497c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7294497c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72fd3cac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72fd3cac90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7294339fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7294339fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7294059f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7294059f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72947bfb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72947bfb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f729419cad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f729419cad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7293ffba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7293ffba50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7293dc3cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7293dc3cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7293d96890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7293d96890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7294059cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7294059cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7293dc3d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7293dc3d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7293bb3610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7293bb3610>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-17 11:02:20.247000: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-17 11:02:20.415599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-17 11:02:20.479383: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e71cb46eb0 executing computations on platform Host. Devices:
2022-11-17 11:02:20.479481: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-17 11:02:21.143474: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window10.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 17:33 - loss: 0.7352 - acc: 0.5000
 128/4566 [..............................] - ETA: 13:41 - loss: 0.7929 - acc: 0.5078
 192/4566 [>.............................] - ETA: 12:18 - loss: 0.7588 - acc: 0.5260
 256/4566 [>.............................] - ETA: 15:06 - loss: 0.7624 - acc: 0.5000
 320/4566 [=>............................] - ETA: 15:34 - loss: 0.7559 - acc: 0.5062
 384/4566 [=>............................] - ETA: 15:39 - loss: 0.7581 - acc: 0.5052
 448/4566 [=>............................] - ETA: 15:34 - loss: 0.7460 - acc: 0.5156
 512/4566 [==>...........................] - ETA: 15:08 - loss: 0.7404 - acc: 0.5137
 576/4566 [==>...........................] - ETA: 14:10 - loss: 0.7430 - acc: 0.5139
 640/4566 [===>..........................] - ETA: 13:18 - loss: 0.7441 - acc: 0.5062
 704/4566 [===>..........................] - ETA: 12:41 - loss: 0.7425 - acc: 0.5099
 768/4566 [====>.........................] - ETA: 11:57 - loss: 0.7430 - acc: 0.5065
 832/4566 [====>.........................] - ETA: 11:19 - loss: 0.7435 - acc: 0.5024
 896/4566 [====>.........................] - ETA: 10:53 - loss: 0.7424 - acc: 0.5011
 960/4566 [=====>........................] - ETA: 10:24 - loss: 0.7387 - acc: 0.5010
1024/4566 [=====>........................] - ETA: 10:00 - loss: 0.7402 - acc: 0.4980
1088/4566 [======>.......................] - ETA: 9:36 - loss: 0.7404 - acc: 0.4917 
1152/4566 [======>.......................] - ETA: 9:12 - loss: 0.7412 - acc: 0.4896
1216/4566 [======>.......................] - ETA: 8:51 - loss: 0.7413 - acc: 0.4893
1280/4566 [=======>......................] - ETA: 8:31 - loss: 0.7384 - acc: 0.4914
1344/4566 [=======>......................] - ETA: 8:14 - loss: 0.7369 - acc: 0.4940
1408/4566 [========>.....................] - ETA: 8:17 - loss: 0.7368 - acc: 0.4936
1472/4566 [========>.....................] - ETA: 8:20 - loss: 0.7372 - acc: 0.4932
1536/4566 [=========>....................] - ETA: 8:19 - loss: 0.7360 - acc: 0.4909
1600/4566 [=========>....................] - ETA: 8:19 - loss: 0.7334 - acc: 0.4925
1664/4566 [=========>....................] - ETA: 8:15 - loss: 0.7326 - acc: 0.4940
1728/4566 [==========>...................] - ETA: 8:06 - loss: 0.7303 - acc: 0.4988
1792/4566 [==========>...................] - ETA: 7:48 - loss: 0.7301 - acc: 0.5000
1856/4566 [===========>..................] - ETA: 7:31 - loss: 0.7292 - acc: 0.5022
1920/4566 [===========>..................] - ETA: 7:14 - loss: 0.7290 - acc: 0.5005
1984/4566 [============>.................] - ETA: 6:58 - loss: 0.7293 - acc: 0.4995
2048/4566 [============>.................] - ETA: 6:42 - loss: 0.7281 - acc: 0.5024
2112/4566 [============>.................] - ETA: 6:27 - loss: 0.7264 - acc: 0.5019
2176/4566 [=============>................] - ETA: 6:13 - loss: 0.7253 - acc: 0.5023
2240/4566 [=============>................] - ETA: 6:00 - loss: 0.7244 - acc: 0.5049
2304/4566 [==============>...............] - ETA: 5:47 - loss: 0.7243 - acc: 0.5056
2368/4566 [==============>...............] - ETA: 5:33 - loss: 0.7243 - acc: 0.5059
2432/4566 [==============>...............] - ETA: 5:20 - loss: 0.7223 - acc: 0.5082
2496/4566 [===============>..............] - ETA: 5:08 - loss: 0.7229 - acc: 0.5076
2560/4566 [===============>..............] - ETA: 4:55 - loss: 0.7227 - acc: 0.5074
2624/4566 [================>.............] - ETA: 4:43 - loss: 0.7229 - acc: 0.5069
2688/4566 [================>.............] - ETA: 4:37 - loss: 0.7227 - acc: 0.5074
2752/4566 [=================>............] - ETA: 4:31 - loss: 0.7226 - acc: 0.5080
2816/4566 [=================>............] - ETA: 4:25 - loss: 0.7219 - acc: 0.5092
2880/4566 [=================>............] - ETA: 4:18 - loss: 0.7211 - acc: 0.5104
2944/4566 [==================>...........] - ETA: 4:11 - loss: 0.7204 - acc: 0.5109
3008/4566 [==================>...........] - ETA: 4:02 - loss: 0.7205 - acc: 0.5093
3072/4566 [===================>..........] - ETA: 3:50 - loss: 0.7203 - acc: 0.5098
3136/4566 [===================>..........] - ETA: 3:39 - loss: 0.7201 - acc: 0.5102
3200/4566 [====================>.........] - ETA: 3:27 - loss: 0.7200 - acc: 0.5078
3264/4566 [====================>.........] - ETA: 3:16 - loss: 0.7195 - acc: 0.5080
3328/4566 [====================>.........] - ETA: 3:05 - loss: 0.7190 - acc: 0.5069
3392/4566 [=====================>........] - ETA: 2:54 - loss: 0.7196 - acc: 0.5062
3456/4566 [=====================>........] - ETA: 2:43 - loss: 0.7197 - acc: 0.5064
3520/4566 [======================>.......] - ETA: 2:32 - loss: 0.7192 - acc: 0.5074
3584/4566 [======================>.......] - ETA: 2:22 - loss: 0.7181 - acc: 0.5086
3648/4566 [======================>.......] - ETA: 2:12 - loss: 0.7184 - acc: 0.5082
3712/4566 [=======================>......] - ETA: 2:02 - loss: 0.7179 - acc: 0.5084
3776/4566 [=======================>......] - ETA: 1:52 - loss: 0.7168 - acc: 0.5098
3840/4566 [========================>.....] - ETA: 1:42 - loss: 0.7167 - acc: 0.5094
3904/4566 [========================>.....] - ETA: 1:33 - loss: 0.7162 - acc: 0.5102
3968/4566 [=========================>....] - ETA: 1:23 - loss: 0.7160 - acc: 0.5113
4032/4566 [=========================>....] - ETA: 1:15 - loss: 0.7158 - acc: 0.5117
4096/4566 [=========================>....] - ETA: 1:07 - loss: 0.7160 - acc: 0.5107
4160/4566 [==========================>...] - ETA: 58s - loss: 0.7153 - acc: 0.5118 
4224/4566 [==========================>...] - ETA: 49s - loss: 0.7151 - acc: 0.5116
4288/4566 [===========================>..] - ETA: 40s - loss: 0.7145 - acc: 0.5126
4352/4566 [===========================>..] - ETA: 31s - loss: 0.7144 - acc: 0.5133
4416/4566 [============================>.] - ETA: 22s - loss: 0.7145 - acc: 0.5131
4480/4566 [============================>.] - ETA: 12s - loss: 0.7146 - acc: 0.5127
4544/4566 [============================>.] - ETA: 3s - loss: 0.7147 - acc: 0.5130 
4566/4566 [==============================] - 682s 149ms/step - loss: 0.7148 - acc: 0.5129 - val_loss: 0.6978 - val_acc: 0.5039

Epoch 00001: val_acc improved from -inf to 0.50394, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window10/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 6:30 - loss: 0.6743 - acc: 0.5781
 128/4566 [..............................] - ETA: 6:37 - loss: 0.6917 - acc: 0.5625
 192/4566 [>.............................] - ETA: 6:51 - loss: 0.6898 - acc: 0.5677
 256/4566 [>.............................] - ETA: 6:47 - loss: 0.7087 - acc: 0.5234
 320/4566 [=>............................] - ETA: 6:53 - loss: 0.7108 - acc: 0.5188
 384/4566 [=>............................] - ETA: 6:39 - loss: 0.7162 - acc: 0.5078
 448/4566 [=>............................] - ETA: 6:37 - loss: 0.7094 - acc: 0.5089
 512/4566 [==>...........................] - ETA: 7:06 - loss: 0.7031 - acc: 0.5254
 576/4566 [==>...........................] - ETA: 7:57 - loss: 0.7023 - acc: 0.5278
 640/4566 [===>..........................] - ETA: 8:33 - loss: 0.6970 - acc: 0.5422
 704/4566 [===>..........................] - ETA: 9:01 - loss: 0.6944 - acc: 0.5440
 768/4566 [====>.........................] - ETA: 9:20 - loss: 0.6973 - acc: 0.5378
 832/4566 [====>.........................] - ETA: 9:34 - loss: 0.6983 - acc: 0.5349
 896/4566 [====>.........................] - ETA: 9:17 - loss: 0.7026 - acc: 0.5301
 960/4566 [=====>........................] - ETA: 8:53 - loss: 0.7013 - acc: 0.5333
1024/4566 [=====>........................] - ETA: 8:32 - loss: 0.7039 - acc: 0.5322
1088/4566 [======>.......................] - ETA: 8:12 - loss: 0.7023 - acc: 0.5358
1152/4566 [======>.......................] - ETA: 7:54 - loss: 0.7010 - acc: 0.5408
1216/4566 [======>.......................] - ETA: 7:38 - loss: 0.6996 - acc: 0.5411
1280/4566 [=======>......................] - ETA: 7:21 - loss: 0.6994 - acc: 0.5398
1344/4566 [=======>......................] - ETA: 7:05 - loss: 0.6971 - acc: 0.5417
1408/4566 [========>.....................] - ETA: 6:53 - loss: 0.6982 - acc: 0.5369
1472/4566 [========>.....................] - ETA: 6:39 - loss: 0.6971 - acc: 0.5360
1536/4566 [=========>....................] - ETA: 6:26 - loss: 0.6967 - acc: 0.5378
1600/4566 [=========>....................] - ETA: 6:15 - loss: 0.6947 - acc: 0.5400
1664/4566 [=========>....................] - ETA: 6:03 - loss: 0.6965 - acc: 0.5397
1728/4566 [==========>...................] - ETA: 5:51 - loss: 0.6971 - acc: 0.5399
1792/4566 [==========>...................] - ETA: 5:40 - loss: 0.6976 - acc: 0.5413
1856/4566 [===========>..................] - ETA: 5:38 - loss: 0.6961 - acc: 0.5453
1920/4566 [===========>..................] - ETA: 5:39 - loss: 0.6949 - acc: 0.5469
1984/4566 [============>.................] - ETA: 5:40 - loss: 0.6962 - acc: 0.5433
2048/4566 [============>.................] - ETA: 5:39 - loss: 0.6949 - acc: 0.5459
2112/4566 [============>.................] - ETA: 5:37 - loss: 0.6953 - acc: 0.5412
2176/4566 [=============>................] - ETA: 5:33 - loss: 0.6954 - acc: 0.5418
2240/4566 [=============>................] - ETA: 5:22 - loss: 0.6954 - acc: 0.5420
2304/4566 [==============>...............] - ETA: 5:10 - loss: 0.6963 - acc: 0.5412
2368/4566 [==============>...............] - ETA: 4:58 - loss: 0.6975 - acc: 0.5405
2432/4566 [==============>...............] - ETA: 4:47 - loss: 0.6964 - acc: 0.5436
2496/4566 [===============>..............] - ETA: 4:36 - loss: 0.6979 - acc: 0.5401
2560/4566 [===============>..............] - ETA: 4:25 - loss: 0.6986 - acc: 0.5391
2624/4566 [================>.............] - ETA: 4:15 - loss: 0.6999 - acc: 0.5377
2688/4566 [================>.............] - ETA: 4:05 - loss: 0.6986 - acc: 0.5394
2752/4566 [=================>............] - ETA: 3:55 - loss: 0.6987 - acc: 0.5382
2816/4566 [=================>............] - ETA: 3:44 - loss: 0.6991 - acc: 0.5376
2880/4566 [=================>............] - ETA: 3:35 - loss: 0.6997 - acc: 0.5372
2944/4566 [==================>...........] - ETA: 3:25 - loss: 0.6996 - acc: 0.5377
3008/4566 [==================>...........] - ETA: 3:16 - loss: 0.7000 - acc: 0.5359
3072/4566 [===================>..........] - ETA: 3:07 - loss: 0.6991 - acc: 0.5384
3136/4566 [===================>..........] - ETA: 2:58 - loss: 0.6990 - acc: 0.5392
3200/4566 [====================>.........] - ETA: 2:52 - loss: 0.6995 - acc: 0.5369
3264/4566 [====================>.........] - ETA: 2:47 - loss: 0.6999 - acc: 0.5346
3328/4566 [====================>.........] - ETA: 2:41 - loss: 0.6997 - acc: 0.5352
3392/4566 [=====================>........] - ETA: 2:35 - loss: 0.7006 - acc: 0.5339
3456/4566 [=====================>........] - ETA: 2:28 - loss: 0.7006 - acc: 0.5344
3520/4566 [======================>.......] - ETA: 2:21 - loss: 0.6995 - acc: 0.5384
3584/4566 [======================>.......] - ETA: 2:12 - loss: 0.6989 - acc: 0.5405
3648/4566 [======================>.......] - ETA: 2:02 - loss: 0.6982 - acc: 0.5417
3712/4566 [=======================>......] - ETA: 1:53 - loss: 0.6974 - acc: 0.5423
3776/4566 [=======================>......] - ETA: 1:44 - loss: 0.6977 - acc: 0.5416
3840/4566 [========================>.....] - ETA: 1:35 - loss: 0.6976 - acc: 0.5422
3904/4566 [========================>.....] - ETA: 1:26 - loss: 0.6975 - acc: 0.5423
3968/4566 [=========================>....] - ETA: 1:18 - loss: 0.6971 - acc: 0.5433
4032/4566 [=========================>....] - ETA: 1:09 - loss: 0.6968 - acc: 0.5444
4096/4566 [=========================>....] - ETA: 1:00 - loss: 0.6965 - acc: 0.5442
4160/4566 [==========================>...] - ETA: 52s - loss: 0.6964 - acc: 0.5440 
4224/4566 [==========================>...] - ETA: 43s - loss: 0.6958 - acc: 0.5457
4288/4566 [===========================>..] - ETA: 35s - loss: 0.6966 - acc: 0.5445
4352/4566 [===========================>..] - ETA: 27s - loss: 0.6969 - acc: 0.5432
4416/4566 [============================>.] - ETA: 18s - loss: 0.6967 - acc: 0.5442
4480/4566 [============================>.] - ETA: 10s - loss: 0.6960 - acc: 0.5458
4544/4566 [============================>.] - ETA: 2s - loss: 0.6956 - acc: 0.5473 
4566/4566 [==============================] - 617s 135ms/step - loss: 0.6957 - acc: 0.5471 - val_loss: 0.6660 - val_acc: 0.6201

Epoch 00002: val_acc improved from 0.50394 to 0.62008, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window10/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 3/10

  64/4566 [..............................] - ETA: 16:21 - loss: 0.7360 - acc: 0.4062
 128/4566 [..............................] - ETA: 15:57 - loss: 0.7054 - acc: 0.5000
 192/4566 [>.............................] - ETA: 14:31 - loss: 0.6948 - acc: 0.5312
 256/4566 [>.............................] - ETA: 12:22 - loss: 0.6964 - acc: 0.5312
 320/4566 [=>............................] - ETA: 10:55 - loss: 0.7088 - acc: 0.5000
 384/4566 [=>............................] - ETA: 9:58 - loss: 0.7040 - acc: 0.5156 
 448/4566 [=>............................] - ETA: 9:19 - loss: 0.7031 - acc: 0.5223
 512/4566 [==>...........................] - ETA: 8:45 - loss: 0.7005 - acc: 0.5273
 576/4566 [==>...........................] - ETA: 8:21 - loss: 0.7002 - acc: 0.5260
 640/4566 [===>..........................] - ETA: 8:01 - loss: 0.7023 - acc: 0.5203
 704/4566 [===>..........................] - ETA: 7:39 - loss: 0.7015 - acc: 0.5270
 768/4566 [====>.........................] - ETA: 7:21 - loss: 0.7000 - acc: 0.5286
 832/4566 [====>.........................] - ETA: 7:08 - loss: 0.6973 - acc: 0.5288
 896/4566 [====>.........................] - ETA: 6:54 - loss: 0.6945 - acc: 0.5324
 960/4566 [=====>........................] - ETA: 6:43 - loss: 0.6914 - acc: 0.5448
1024/4566 [=====>........................] - ETA: 6:32 - loss: 0.6884 - acc: 0.5488
1088/4566 [======>.......................] - ETA: 6:21 - loss: 0.6855 - acc: 0.5588
1152/4566 [======>.......................] - ETA: 6:12 - loss: 0.6827 - acc: 0.5660
1216/4566 [======>.......................] - ETA: 6:25 - loss: 0.6816 - acc: 0.5674
1280/4566 [=======>......................] - ETA: 6:36 - loss: 0.6815 - acc: 0.5656
1344/4566 [=======>......................] - ETA: 6:45 - loss: 0.6819 - acc: 0.5647
1408/4566 [========>.....................] - ETA: 6:52 - loss: 0.6821 - acc: 0.5632
1472/4566 [========>.....................] - ETA: 6:56 - loss: 0.6824 - acc: 0.5611
1536/4566 [=========>....................] - ETA: 6:54 - loss: 0.6826 - acc: 0.5618
1600/4566 [=========>....................] - ETA: 6:41 - loss: 0.6837 - acc: 0.5613
1664/4566 [=========>....................] - ETA: 6:26 - loss: 0.6816 - acc: 0.5649
1728/4566 [==========>...................] - ETA: 6:14 - loss: 0.6821 - acc: 0.5660
1792/4566 [==========>...................] - ETA: 6:01 - loss: 0.6834 - acc: 0.5642
1856/4566 [===========>..................] - ETA: 5:48 - loss: 0.6829 - acc: 0.5652
1920/4566 [===========>..................] - ETA: 5:37 - loss: 0.6836 - acc: 0.5641
1984/4566 [============>.................] - ETA: 5:25 - loss: 0.6851 - acc: 0.5620
2048/4566 [============>.................] - ETA: 5:14 - loss: 0.6869 - acc: 0.5596
2112/4566 [============>.................] - ETA: 5:04 - loss: 0.6880 - acc: 0.5563
2176/4566 [=============>................] - ETA: 4:53 - loss: 0.6889 - acc: 0.5551
2240/4566 [=============>................] - ETA: 4:43 - loss: 0.6881 - acc: 0.5567
2304/4566 [==============>...............] - ETA: 4:33 - loss: 0.6892 - acc: 0.5547
2368/4566 [==============>...............] - ETA: 4:23 - loss: 0.6890 - acc: 0.5532
2432/4566 [==============>...............] - ETA: 4:14 - loss: 0.6890 - acc: 0.5535
2496/4566 [===============>..............] - ETA: 4:06 - loss: 0.6882 - acc: 0.5565
2560/4566 [===============>..............] - ETA: 4:04 - loss: 0.6886 - acc: 0.5555
2624/4566 [================>.............] - ETA: 4:01 - loss: 0.6883 - acc: 0.5560
2688/4566 [================>.............] - ETA: 3:58 - loss: 0.6881 - acc: 0.5569
2752/4566 [=================>............] - ETA: 3:53 - loss: 0.6874 - acc: 0.5589
2816/4566 [=================>............] - ETA: 3:49 - loss: 0.6876 - acc: 0.5593
2880/4566 [=================>............] - ETA: 3:42 - loss: 0.6880 - acc: 0.5576
2944/4566 [==================>...........] - ETA: 3:33 - loss: 0.6879 - acc: 0.5571
3008/4566 [==================>...........] - ETA: 3:23 - loss: 0.6890 - acc: 0.5552
3072/4566 [===================>..........] - ETA: 3:13 - loss: 0.6890 - acc: 0.5557
3136/4566 [===================>..........] - ETA: 3:04 - loss: 0.6885 - acc: 0.5558
3200/4566 [====================>.........] - ETA: 2:54 - loss: 0.6883 - acc: 0.5556
3264/4566 [====================>.........] - ETA: 2:45 - loss: 0.6877 - acc: 0.5570
3328/4566 [====================>.........] - ETA: 2:36 - loss: 0.6876 - acc: 0.5574
3392/4566 [=====================>........] - ETA: 2:27 - loss: 0.6879 - acc: 0.5572
3456/4566 [=====================>........] - ETA: 2:18 - loss: 0.6882 - acc: 0.5564
3520/4566 [======================>.......] - ETA: 2:09 - loss: 0.6881 - acc: 0.5565
3584/4566 [======================>.......] - ETA: 2:01 - loss: 0.6877 - acc: 0.5575
3648/4566 [======================>.......] - ETA: 1:52 - loss: 0.6880 - acc: 0.5562
3712/4566 [=======================>......] - ETA: 1:44 - loss: 0.6875 - acc: 0.5568
3776/4566 [=======================>......] - ETA: 1:35 - loss: 0.6876 - acc: 0.5569
3840/4566 [========================>.....] - ETA: 1:27 - loss: 0.6874 - acc: 0.5563
3904/4566 [========================>.....] - ETA: 1:20 - loss: 0.6877 - acc: 0.5556
3968/4566 [=========================>....] - ETA: 1:13 - loss: 0.6878 - acc: 0.5547
4032/4566 [=========================>....] - ETA: 1:06 - loss: 0.6883 - acc: 0.5533
4096/4566 [=========================>....] - ETA: 59s - loss: 0.6877 - acc: 0.5549 
4160/4566 [==========================>...] - ETA: 51s - loss: 0.6878 - acc: 0.5541
4224/4566 [==========================>...] - ETA: 43s - loss: 0.6878 - acc: 0.5533
4288/4566 [===========================>..] - ETA: 35s - loss: 0.6878 - acc: 0.5532
4352/4566 [===========================>..] - ETA: 27s - loss: 0.6882 - acc: 0.5528
4416/4566 [============================>.] - ETA: 19s - loss: 0.6888 - acc: 0.5510
4480/4566 [============================>.] - ETA: 10s - loss: 0.6883 - acc: 0.5518
4544/4566 [============================>.] - ETA: 2s - loss: 0.6882 - acc: 0.5517 
4566/4566 [==============================] - 597s 131ms/step - loss: 0.6885 - acc: 0.5512 - val_loss: 0.6823 - val_acc: 0.5472

Epoch 00003: val_acc did not improve from 0.62008
Epoch 4/10

  64/4566 [..............................] - ETA: 6:27 - loss: 0.6953 - acc: 0.4844
 128/4566 [..............................] - ETA: 6:18 - loss: 0.6962 - acc: 0.4844
 192/4566 [>.............................] - ETA: 6:11 - loss: 0.6933 - acc: 0.5000
 256/4566 [>.............................] - ETA: 6:07 - loss: 0.6872 - acc: 0.5273
 320/4566 [=>............................] - ETA: 5:59 - loss: 0.6860 - acc: 0.5406
 384/4566 [=>............................] - ETA: 5:59 - loss: 0.6848 - acc: 0.5521
 448/4566 [=>............................] - ETA: 5:55 - loss: 0.6877 - acc: 0.5491
 512/4566 [==>...........................] - ETA: 5:51 - loss: 0.6896 - acc: 0.5566
 576/4566 [==>...........................] - ETA: 6:23 - loss: 0.6866 - acc: 0.5521
 640/4566 [===>..........................] - ETA: 7:12 - loss: 0.6855 - acc: 0.5547
 704/4566 [===>..........................] - ETA: 7:45 - loss: 0.6873 - acc: 0.5526
 768/4566 [====>.........................] - ETA: 8:08 - loss: 0.6867 - acc: 0.5560
 832/4566 [====>.........................] - ETA: 8:24 - loss: 0.6876 - acc: 0.5589
 896/4566 [====>.........................] - ETA: 8:35 - loss: 0.6882 - acc: 0.5592
 960/4566 [=====>........................] - ETA: 8:25 - loss: 0.6889 - acc: 0.5583
1024/4566 [=====>........................] - ETA: 8:04 - loss: 0.6863 - acc: 0.5625
1088/4566 [======>.......................] - ETA: 7:44 - loss: 0.6883 - acc: 0.5607
1152/4566 [======>.......................] - ETA: 7:27 - loss: 0.6882 - acc: 0.5590
1216/4566 [======>.......................] - ETA: 7:11 - loss: 0.6897 - acc: 0.5567
1280/4566 [=======>......................] - ETA: 6:56 - loss: 0.6881 - acc: 0.5594
1344/4566 [=======>......................] - ETA: 6:41 - loss: 0.6868 - acc: 0.5595
1408/4566 [========>.....................] - ETA: 6:28 - loss: 0.6862 - acc: 0.5582
1472/4566 [========>.....................] - ETA: 6:15 - loss: 0.6861 - acc: 0.5591
1536/4566 [=========>....................] - ETA: 6:03 - loss: 0.6855 - acc: 0.5586
1600/4566 [=========>....................] - ETA: 5:51 - loss: 0.6836 - acc: 0.5625
1664/4566 [=========>....................] - ETA: 5:41 - loss: 0.6838 - acc: 0.5625
1728/4566 [==========>...................] - ETA: 5:30 - loss: 0.6843 - acc: 0.5613
1792/4566 [==========>...................] - ETA: 5:20 - loss: 0.6866 - acc: 0.5586
1856/4566 [===========>..................] - ETA: 5:10 - loss: 0.6871 - acc: 0.5582
1920/4566 [===========>..................] - ETA: 5:01 - loss: 0.6875 - acc: 0.5568
1984/4566 [============>.................] - ETA: 5:00 - loss: 0.6862 - acc: 0.5590
2048/4566 [============>.................] - ETA: 5:01 - loss: 0.6868 - acc: 0.5586
2112/4566 [============>.................] - ETA: 5:01 - loss: 0.6863 - acc: 0.5611
2176/4566 [=============>................] - ETA: 4:59 - loss: 0.6858 - acc: 0.5625
2240/4566 [=============>................] - ETA: 4:57 - loss: 0.6867 - acc: 0.5607
2304/4566 [==============>...............] - ETA: 4:54 - loss: 0.6865 - acc: 0.5608
2368/4566 [==============>...............] - ETA: 4:46 - loss: 0.6863 - acc: 0.5608
2432/4566 [==============>...............] - ETA: 4:35 - loss: 0.6860 - acc: 0.5600
2496/4566 [===============>..............] - ETA: 4:25 - loss: 0.6851 - acc: 0.5617
2560/4566 [===============>..............] - ETA: 4:14 - loss: 0.6855 - acc: 0.5617
2624/4566 [================>.............] - ETA: 4:04 - loss: 0.6858 - acc: 0.5614
2688/4566 [================>.............] - ETA: 3:54 - loss: 0.6851 - acc: 0.5636
2752/4566 [=================>............] - ETA: 3:45 - loss: 0.6851 - acc: 0.5640
2816/4566 [=================>............] - ETA: 3:35 - loss: 0.6858 - acc: 0.5621
2880/4566 [=================>............] - ETA: 3:26 - loss: 0.6854 - acc: 0.5628
2944/4566 [==================>...........] - ETA: 3:16 - loss: 0.6856 - acc: 0.5639
3008/4566 [==================>...........] - ETA: 3:07 - loss: 0.6853 - acc: 0.5648
3072/4566 [===================>..........] - ETA: 2:59 - loss: 0.6852 - acc: 0.5661
3136/4566 [===================>..........] - ETA: 2:50 - loss: 0.6856 - acc: 0.5663
3200/4566 [====================>.........] - ETA: 2:42 - loss: 0.6850 - acc: 0.5687
3264/4566 [====================>.........] - ETA: 2:33 - loss: 0.6844 - acc: 0.5702
3328/4566 [====================>.........] - ETA: 2:25 - loss: 0.6840 - acc: 0.5712
3392/4566 [=====================>........] - ETA: 2:20 - loss: 0.6839 - acc: 0.5719
3456/4566 [=====================>........] - ETA: 2:14 - loss: 0.6844 - acc: 0.5700
3520/4566 [======================>.......] - ETA: 2:08 - loss: 0.6849 - acc: 0.5693
3584/4566 [======================>.......] - ETA: 2:02 - loss: 0.6840 - acc: 0.5717
3648/4566 [======================>.......] - ETA: 1:56 - loss: 0.6839 - acc: 0.5724
3712/4566 [=======================>......] - ETA: 1:49 - loss: 0.6838 - acc: 0.5722
3776/4566 [=======================>......] - ETA: 1:40 - loss: 0.6834 - acc: 0.5736
3840/4566 [========================>.....] - ETA: 1:32 - loss: 0.6833 - acc: 0.5742
3904/4566 [========================>.....] - ETA: 1:23 - loss: 0.6832 - acc: 0.5756
3968/4566 [=========================>....] - ETA: 1:15 - loss: 0.6832 - acc: 0.5756
4032/4566 [=========================>....] - ETA: 1:06 - loss: 0.6830 - acc: 0.5766
4096/4566 [=========================>....] - ETA: 58s - loss: 0.6833 - acc: 0.5764 
4160/4566 [==========================>...] - ETA: 50s - loss: 0.6832 - acc: 0.5767
4224/4566 [==========================>...] - ETA: 42s - loss: 0.6826 - acc: 0.5781
4288/4566 [===========================>..] - ETA: 34s - loss: 0.6828 - acc: 0.5774
4352/4566 [===========================>..] - ETA: 26s - loss: 0.6827 - acc: 0.5767
4416/4566 [============================>.] - ETA: 18s - loss: 0.6826 - acc: 0.5763
4480/4566 [============================>.] - ETA: 10s - loss: 0.6824 - acc: 0.5770
4544/4566 [============================>.] - ETA: 2s - loss: 0.6821 - acc: 0.5772 
4566/4566 [==============================] - 568s 124ms/step - loss: 0.6823 - acc: 0.5773 - val_loss: 0.6614 - val_acc: 0.6122

Epoch 00004: val_acc did not improve from 0.62008
Epoch 5/10

  64/4566 [..............................] - ETA: 16:31 - loss: 0.6884 - acc: 0.5781
 128/4566 [..............................] - ETA: 16:37 - loss: 0.7077 - acc: 0.5312
 192/4566 [>.............................] - ETA: 16:23 - loss: 0.7023 - acc: 0.5365
 256/4566 [>.............................] - ETA: 15:55 - loss: 0.6962 - acc: 0.5508
 320/4566 [=>............................] - ETA: 15:40 - loss: 0.6908 - acc: 0.5625
 384/4566 [=>............................] - ETA: 15:12 - loss: 0.6817 - acc: 0.5755
 448/4566 [=>............................] - ETA: 13:47 - loss: 0.6860 - acc: 0.5692
 512/4566 [==>...........................] - ETA: 12:37 - loss: 0.6894 - acc: 0.5645
 576/4566 [==>...........................] - ETA: 11:41 - loss: 0.6948 - acc: 0.5573
 640/4566 [===>..........................] - ETA: 10:56 - loss: 0.6948 - acc: 0.5531
 704/4566 [===>..........................] - ETA: 10:21 - loss: 0.6930 - acc: 0.5597
 768/4566 [====>.........................] - ETA: 9:49 - loss: 0.6920 - acc: 0.5612 
 832/4566 [====>.........................] - ETA: 9:20 - loss: 0.6899 - acc: 0.5673
 896/4566 [====>.........................] - ETA: 8:52 - loss: 0.6848 - acc: 0.5748
 960/4566 [=====>........................] - ETA: 8:28 - loss: 0.6822 - acc: 0.5771
1024/4566 [=====>........................] - ETA: 8:06 - loss: 0.6817 - acc: 0.5771
1088/4566 [======>.......................] - ETA: 7:47 - loss: 0.6799 - acc: 0.5809
1152/4566 [======>.......................] - ETA: 7:29 - loss: 0.6779 - acc: 0.5833
1216/4566 [======>.......................] - ETA: 7:12 - loss: 0.6783 - acc: 0.5831
1280/4566 [=======>......................] - ETA: 6:57 - loss: 0.6794 - acc: 0.5813
1344/4566 [=======>......................] - ETA: 6:43 - loss: 0.6794 - acc: 0.5789
1408/4566 [========>.....................] - ETA: 6:32 - loss: 0.6794 - acc: 0.5788
1472/4566 [========>.....................] - ETA: 6:36 - loss: 0.6798 - acc: 0.5754
1536/4566 [=========>....................] - ETA: 6:41 - loss: 0.6795 - acc: 0.5749
1600/4566 [=========>....................] - ETA: 6:43 - loss: 0.6805 - acc: 0.5725
1664/4566 [=========>....................] - ETA: 6:44 - loss: 0.6794 - acc: 0.5757
1728/4566 [==========>...................] - ETA: 6:43 - loss: 0.6787 - acc: 0.5764
1792/4566 [==========>...................] - ETA: 6:40 - loss: 0.6800 - acc: 0.5737
1856/4566 [===========>..................] - ETA: 6:26 - loss: 0.6778 - acc: 0.5754
1920/4566 [===========>..................] - ETA: 6:11 - loss: 0.6772 - acc: 0.5776
1984/4566 [============>.................] - ETA: 5:58 - loss: 0.6787 - acc: 0.5761
2048/4566 [============>.................] - ETA: 5:45 - loss: 0.6801 - acc: 0.5752
2112/4566 [============>.................] - ETA: 5:32 - loss: 0.6813 - acc: 0.5729
2176/4566 [=============>................] - ETA: 5:20 - loss: 0.6817 - acc: 0.5722
2240/4566 [=============>................] - ETA: 5:08 - loss: 0.6826 - acc: 0.5696
2304/4566 [==============>...............] - ETA: 4:57 - loss: 0.6824 - acc: 0.5690
2368/4566 [==============>...............] - ETA: 4:46 - loss: 0.6814 - acc: 0.5714
2432/4566 [==============>...............] - ETA: 4:36 - loss: 0.6814 - acc: 0.5707
2496/4566 [===============>..............] - ETA: 4:25 - loss: 0.6809 - acc: 0.5713
2560/4566 [===============>..............] - ETA: 4:15 - loss: 0.6809 - acc: 0.5715
2624/4566 [================>.............] - ETA: 4:05 - loss: 0.6807 - acc: 0.5701
2688/4566 [================>.............] - ETA: 3:55 - loss: 0.6802 - acc: 0.5703
2752/4566 [=================>............] - ETA: 3:46 - loss: 0.6798 - acc: 0.5712
2816/4566 [=================>............] - ETA: 3:39 - loss: 0.6781 - acc: 0.5746
2880/4566 [=================>............] - ETA: 3:34 - loss: 0.6782 - acc: 0.5736
2944/4566 [==================>...........] - ETA: 3:30 - loss: 0.6776 - acc: 0.5751
3008/4566 [==================>...........] - ETA: 3:24 - loss: 0.6775 - acc: 0.5761
3072/4566 [===================>..........] - ETA: 3:19 - loss: 0.6769 - acc: 0.5775
3136/4566 [===================>..........] - ETA: 3:13 - loss: 0.6769 - acc: 0.5765
3200/4566 [====================>.........] - ETA: 3:05 - loss: 0.6766 - acc: 0.5753
3264/4566 [====================>.........] - ETA: 2:55 - loss: 0.6766 - acc: 0.5754
3328/4566 [====================>.........] - ETA: 2:45 - loss: 0.6775 - acc: 0.5736
3392/4566 [=====================>........] - ETA: 2:36 - loss: 0.6776 - acc: 0.5755
3456/4566 [=====================>........] - ETA: 2:26 - loss: 0.6780 - acc: 0.5752
3520/4566 [======================>.......] - ETA: 2:17 - loss: 0.6782 - acc: 0.5747
3584/4566 [======================>.......] - ETA: 2:08 - loss: 0.6781 - acc: 0.5745
3648/4566 [======================>.......] - ETA: 1:59 - loss: 0.6784 - acc: 0.5740
3712/4566 [=======================>......] - ETA: 1:50 - loss: 0.6787 - acc: 0.5735
3776/4566 [=======================>......] - ETA: 1:41 - loss: 0.6775 - acc: 0.5749
3840/4566 [========================>.....] - ETA: 1:32 - loss: 0.6778 - acc: 0.5755
3904/4566 [========================>.....] - ETA: 1:24 - loss: 0.6776 - acc: 0.5766
3968/4566 [=========================>....] - ETA: 1:15 - loss: 0.6779 - acc: 0.5756
4032/4566 [=========================>....] - ETA: 1:07 - loss: 0.6780 - acc: 0.5749
4096/4566 [=========================>....] - ETA: 58s - loss: 0.6783 - acc: 0.5742 
4160/4566 [==========================>...] - ETA: 50s - loss: 0.6776 - acc: 0.5755
4224/4566 [==========================>...] - ETA: 42s - loss: 0.6776 - acc: 0.5762
4288/4566 [===========================>..] - ETA: 35s - loss: 0.6775 - acc: 0.5765
4352/4566 [===========================>..] - ETA: 27s - loss: 0.6776 - acc: 0.5765
4416/4566 [============================>.] - ETA: 19s - loss: 0.6778 - acc: 0.5763
4480/4566 [============================>.] - ETA: 11s - loss: 0.6779 - acc: 0.5768
4544/4566 [============================>.] - ETA: 2s - loss: 0.6778 - acc: 0.5768 
4566/4566 [==============================] - 623s 137ms/step - loss: 0.6776 - acc: 0.5771 - val_loss: 0.6616 - val_acc: 0.6102

Epoch 00005: val_acc did not improve from 0.62008
Epoch 6/10

  64/4566 [..............................] - ETA: 7:03 - loss: 0.7312 - acc: 0.4688
 128/4566 [..............................] - ETA: 7:08 - loss: 0.7010 - acc: 0.5312
 192/4566 [>.............................] - ETA: 6:48 - loss: 0.6920 - acc: 0.5573
 256/4566 [>.............................] - ETA: 6:41 - loss: 0.6818 - acc: 0.5664
 320/4566 [=>............................] - ETA: 6:28 - loss: 0.6792 - acc: 0.5813
 384/4566 [=>............................] - ETA: 6:17 - loss: 0.6872 - acc: 0.5677
 448/4566 [=>............................] - ETA: 6:07 - loss: 0.6779 - acc: 0.5826
 512/4566 [==>...........................] - ETA: 6:03 - loss: 0.6800 - acc: 0.5801
 576/4566 [==>...........................] - ETA: 5:56 - loss: 0.6769 - acc: 0.5816
 640/4566 [===>..........................] - ETA: 5:48 - loss: 0.6754 - acc: 0.5859
 704/4566 [===>..........................] - ETA: 5:39 - loss: 0.6741 - acc: 0.5852
 768/4566 [====>.........................] - ETA: 5:33 - loss: 0.6726 - acc: 0.5951
 832/4566 [====>.........................] - ETA: 5:37 - loss: 0.6714 - acc: 0.5998
 896/4566 [====>.........................] - ETA: 6:03 - loss: 0.6736 - acc: 0.6004
 960/4566 [=====>........................] - ETA: 6:27 - loss: 0.6746 - acc: 0.6010
1024/4566 [=====>........................] - ETA: 6:46 - loss: 0.6753 - acc: 0.5996
1088/4566 [======>.......................] - ETA: 7:02 - loss: 0.6767 - acc: 0.5983
1152/4566 [======>.......................] - ETA: 7:15 - loss: 0.6739 - acc: 0.6042
1216/4566 [======>.......................] - ETA: 7:15 - loss: 0.6767 - acc: 0.5979
1280/4566 [=======>......................] - ETA: 7:01 - loss: 0.6782 - acc: 0.5961
1344/4566 [=======>......................] - ETA: 6:47 - loss: 0.6806 - acc: 0.5930
1408/4566 [========>.....................] - ETA: 6:33 - loss: 0.6790 - acc: 0.5952
1472/4566 [========>.....................] - ETA: 6:20 - loss: 0.6789 - acc: 0.5931
1536/4566 [=========>....................] - ETA: 6:08 - loss: 0.6810 - acc: 0.5879
1600/4566 [=========>....................] - ETA: 5:56 - loss: 0.6842 - acc: 0.5863
1664/4566 [=========>....................] - ETA: 5:44 - loss: 0.6845 - acc: 0.5841
1728/4566 [==========>...................] - ETA: 5:33 - loss: 0.6849 - acc: 0.5822
1792/4566 [==========>...................] - ETA: 5:23 - loss: 0.6850 - acc: 0.5820
1856/4566 [===========>..................] - ETA: 5:12 - loss: 0.6840 - acc: 0.5841
1920/4566 [===========>..................] - ETA: 5:02 - loss: 0.6827 - acc: 0.5854
1984/4566 [============>.................] - ETA: 4:53 - loss: 0.6830 - acc: 0.5837
2048/4566 [============>.................] - ETA: 4:43 - loss: 0.6835 - acc: 0.5815
2112/4566 [============>.................] - ETA: 4:34 - loss: 0.6826 - acc: 0.5814
2176/4566 [=============>................] - ETA: 4:25 - loss: 0.6818 - acc: 0.5827
2240/4566 [=============>................] - ETA: 4:23 - loss: 0.6821 - acc: 0.5821
2304/4566 [==============>...............] - ETA: 4:23 - loss: 0.6823 - acc: 0.5812
2368/4566 [==============>...............] - ETA: 4:21 - loss: 0.6828 - acc: 0.5798
2432/4566 [==============>...............] - ETA: 4:19 - loss: 0.6822 - acc: 0.5789
2496/4566 [===============>..............] - ETA: 4:16 - loss: 0.6809 - acc: 0.5813
2560/4566 [===============>..............] - ETA: 4:12 - loss: 0.6824 - acc: 0.5781
2624/4566 [================>.............] - ETA: 4:04 - loss: 0.6840 - acc: 0.5743
2688/4566 [================>.............] - ETA: 3:54 - loss: 0.6836 - acc: 0.5751
2752/4566 [=================>............] - ETA: 3:45 - loss: 0.6834 - acc: 0.5756
2816/4566 [=================>............] - ETA: 3:35 - loss: 0.6837 - acc: 0.5746
2880/4566 [=================>............] - ETA: 3:26 - loss: 0.6843 - acc: 0.5733
2944/4566 [==================>...........] - ETA: 3:17 - loss: 0.6841 - acc: 0.5751
3008/4566 [==================>...........] - ETA: 3:08 - loss: 0.6836 - acc: 0.5765
3072/4566 [===================>..........] - ETA: 2:59 - loss: 0.6836 - acc: 0.5768
3136/4566 [===================>..........] - ETA: 2:50 - loss: 0.6828 - acc: 0.5797
3200/4566 [====================>.........] - ETA: 2:42 - loss: 0.6827 - acc: 0.5797
3264/4566 [====================>.........] - ETA: 2:33 - loss: 0.6820 - acc: 0.5806
3328/4566 [====================>.........] - ETA: 2:25 - loss: 0.6826 - acc: 0.5790
3392/4566 [=====================>........] - ETA: 2:17 - loss: 0.6820 - acc: 0.5796
3456/4566 [=====================>........] - ETA: 2:08 - loss: 0.6819 - acc: 0.5787
3520/4566 [======================>.......] - ETA: 2:00 - loss: 0.6818 - acc: 0.5781
3584/4566 [======================>.......] - ETA: 1:52 - loss: 0.6813 - acc: 0.5784
3648/4566 [======================>.......] - ETA: 1:46 - loss: 0.6812 - acc: 0.5779
3712/4566 [=======================>......] - ETA: 1:40 - loss: 0.6805 - acc: 0.5792
3776/4566 [=======================>......] - ETA: 1:34 - loss: 0.6804 - acc: 0.5792
3840/4566 [========================>.....] - ETA: 1:28 - loss: 0.6807 - acc: 0.5784
3904/4566 [========================>.....] - ETA: 1:21 - loss: 0.6809 - acc: 0.5781
3968/4566 [=========================>....] - ETA: 1:14 - loss: 0.6804 - acc: 0.5789
4032/4566 [=========================>....] - ETA: 1:06 - loss: 0.6801 - acc: 0.5789
4096/4566 [=========================>....] - ETA: 57s - loss: 0.6797 - acc: 0.5796 
4160/4566 [==========================>...] - ETA: 49s - loss: 0.6796 - acc: 0.5803
4224/4566 [==========================>...] - ETA: 41s - loss: 0.6793 - acc: 0.5805
4288/4566 [===========================>..] - ETA: 33s - loss: 0.6796 - acc: 0.5795
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6793 - acc: 0.5797
4416/4566 [============================>.] - ETA: 17s - loss: 0.6788 - acc: 0.5802
4480/4566 [============================>.] - ETA: 10s - loss: 0.6783 - acc: 0.5806
4544/4566 [============================>.] - ETA: 2s - loss: 0.6779 - acc: 0.5808 
4566/4566 [==============================] - 555s 122ms/step - loss: 0.6777 - acc: 0.5808 - val_loss: 0.6850 - val_acc: 0.5906

Epoch 00006: val_acc did not improve from 0.62008
Epoch 7/10

  64/4566 [..............................] - ETA: 5:41 - loss: 0.6602 - acc: 0.6406
 128/4566 [..............................] - ETA: 5:25 - loss: 0.6544 - acc: 0.6484
 192/4566 [>.............................] - ETA: 5:26 - loss: 0.6583 - acc: 0.6406
 256/4566 [>.............................] - ETA: 5:25 - loss: 0.6457 - acc: 0.6680
 320/4566 [=>............................] - ETA: 6:40 - loss: 0.6582 - acc: 0.6562
 384/4566 [=>............................] - ETA: 8:01 - loss: 0.6605 - acc: 0.6510
 448/4566 [=>............................] - ETA: 8:53 - loss: 0.6744 - acc: 0.6250
 512/4566 [==>...........................] - ETA: 9:35 - loss: 0.6785 - acc: 0.6133
 576/4566 [==>...........................] - ETA: 9:57 - loss: 0.6810 - acc: 0.6059
 640/4566 [===>..........................] - ETA: 10:14 - loss: 0.6774 - acc: 0.6016
 704/4566 [===>..........................] - ETA: 9:46 - loss: 0.6740 - acc: 0.6094 
 768/4566 [====>.........................] - ETA: 9:13 - loss: 0.6758 - acc: 0.6068
 832/4566 [====>.........................] - ETA: 8:44 - loss: 0.6761 - acc: 0.6022
 896/4566 [====>.........................] - ETA: 8:19 - loss: 0.6818 - acc: 0.5938
 960/4566 [=====>........................] - ETA: 7:56 - loss: 0.6811 - acc: 0.5938
1024/4566 [=====>........................] - ETA: 7:37 - loss: 0.6830 - acc: 0.5908
1088/4566 [======>.......................] - ETA: 7:17 - loss: 0.6806 - acc: 0.5938
1152/4566 [======>.......................] - ETA: 6:59 - loss: 0.6795 - acc: 0.5911
1216/4566 [======>.......................] - ETA: 6:44 - loss: 0.6812 - acc: 0.5880
1280/4566 [=======>......................] - ETA: 6:29 - loss: 0.6800 - acc: 0.5898
1344/4566 [=======>......................] - ETA: 6:15 - loss: 0.6800 - acc: 0.5848
1408/4566 [========>.....................] - ETA: 6:02 - loss: 0.6778 - acc: 0.5895
1472/4566 [========>.....................] - ETA: 5:50 - loss: 0.6788 - acc: 0.5870
1536/4566 [=========>....................] - ETA: 5:39 - loss: 0.6771 - acc: 0.5885
1600/4566 [=========>....................] - ETA: 5:27 - loss: 0.6766 - acc: 0.5887
1664/4566 [=========>....................] - ETA: 5:16 - loss: 0.6758 - acc: 0.5895
1728/4566 [==========>...................] - ETA: 5:19 - loss: 0.6737 - acc: 0.5955
1792/4566 [==========>...................] - ETA: 5:24 - loss: 0.6755 - acc: 0.5926
1856/4566 [===========>..................] - ETA: 5:25 - loss: 0.6777 - acc: 0.5867
1920/4566 [===========>..................] - ETA: 5:25 - loss: 0.6758 - acc: 0.5896
1984/4566 [============>.................] - ETA: 5:25 - loss: 0.6770 - acc: 0.5862
2048/4566 [============>.................] - ETA: 5:22 - loss: 0.6769 - acc: 0.5859
2112/4566 [============>.................] - ETA: 5:12 - loss: 0.6783 - acc: 0.5843
2176/4566 [=============>................] - ETA: 5:00 - loss: 0.6785 - acc: 0.5832
2240/4566 [=============>................] - ETA: 4:49 - loss: 0.6794 - acc: 0.5817
2304/4566 [==============>...............] - ETA: 4:38 - loss: 0.6790 - acc: 0.5838
2368/4566 [==============>...............] - ETA: 4:28 - loss: 0.6776 - acc: 0.5870
2432/4566 [==============>...............] - ETA: 4:17 - loss: 0.6762 - acc: 0.5905
2496/4566 [===============>..............] - ETA: 4:07 - loss: 0.6762 - acc: 0.5909
2560/4566 [===============>..............] - ETA: 3:57 - loss: 0.6756 - acc: 0.5922
2624/4566 [================>.............] - ETA: 3:48 - loss: 0.6746 - acc: 0.5930
2688/4566 [================>.............] - ETA: 3:38 - loss: 0.6749 - acc: 0.5926
2752/4566 [=================>............] - ETA: 3:29 - loss: 0.6739 - acc: 0.5952
2816/4566 [=================>............] - ETA: 3:20 - loss: 0.6729 - acc: 0.5984
2880/4566 [=================>............] - ETA: 3:11 - loss: 0.6725 - acc: 0.5993
2944/4566 [==================>...........] - ETA: 3:03 - loss: 0.6731 - acc: 0.5975
3008/4566 [==================>...........] - ETA: 2:55 - loss: 0.6730 - acc: 0.5977
3072/4566 [===================>..........] - ETA: 2:46 - loss: 0.6739 - acc: 0.5977
3136/4566 [===================>..........] - ETA: 2:38 - loss: 0.6749 - acc: 0.5960
3200/4566 [====================>.........] - ETA: 2:34 - loss: 0.6753 - acc: 0.5934
3264/4566 [====================>.........] - ETA: 2:29 - loss: 0.6748 - acc: 0.5941
3328/4566 [====================>.........] - ETA: 2:24 - loss: 0.6748 - acc: 0.5931
3392/4566 [=====================>........] - ETA: 2:19 - loss: 0.6745 - acc: 0.5938
3456/4566 [=====================>........] - ETA: 2:13 - loss: 0.6734 - acc: 0.5955
3520/4566 [======================>.......] - ETA: 2:07 - loss: 0.6747 - acc: 0.5935
3584/4566 [======================>.......] - ETA: 1:58 - loss: 0.6746 - acc: 0.5935
3648/4566 [======================>.......] - ETA: 1:50 - loss: 0.6747 - acc: 0.5946
3712/4566 [=======================>......] - ETA: 1:42 - loss: 0.6745 - acc: 0.5951
3776/4566 [=======================>......] - ETA: 1:34 - loss: 0.6735 - acc: 0.5969
3840/4566 [========================>.....] - ETA: 1:26 - loss: 0.6732 - acc: 0.5982
3904/4566 [========================>.....] - ETA: 1:18 - loss: 0.6742 - acc: 0.5966
3968/4566 [=========================>....] - ETA: 1:10 - loss: 0.6746 - acc: 0.5963
4032/4566 [=========================>....] - ETA: 1:02 - loss: 0.6757 - acc: 0.5928
4096/4566 [=========================>....] - ETA: 54s - loss: 0.6750 - acc: 0.5947 
4160/4566 [==========================>...] - ETA: 46s - loss: 0.6751 - acc: 0.5940
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6754 - acc: 0.5938
4288/4566 [===========================>..] - ETA: 31s - loss: 0.6745 - acc: 0.5947
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6748 - acc: 0.5938
4416/4566 [============================>.] - ETA: 16s - loss: 0.6745 - acc: 0.5940
4480/4566 [============================>.] - ETA: 9s - loss: 0.6744 - acc: 0.5944 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6743 - acc: 0.5953
4566/4566 [==============================] - 551s 121ms/step - loss: 0.6741 - acc: 0.5951 - val_loss: 0.6737 - val_acc: 0.5846

Epoch 00007: val_acc did not improve from 0.62008
Epoch 8/10

  64/4566 [..............................] - ETA: 15:49 - loss: 0.6673 - acc: 0.6250
 128/4566 [..............................] - ETA: 15:39 - loss: 0.6691 - acc: 0.6172
 192/4566 [>.............................] - ETA: 14:57 - loss: 0.6677 - acc: 0.6094
 256/4566 [>.............................] - ETA: 12:45 - loss: 0.6846 - acc: 0.5781
 320/4566 [=>............................] - ETA: 11:20 - loss: 0.6847 - acc: 0.5687
 384/4566 [=>............................] - ETA: 10:22 - loss: 0.6777 - acc: 0.5833
 448/4566 [=>............................] - ETA: 9:29 - loss: 0.6794 - acc: 0.5737 
 512/4566 [==>...........................] - ETA: 8:48 - loss: 0.6766 - acc: 0.5762
 576/4566 [==>...........................] - ETA: 8:15 - loss: 0.6779 - acc: 0.5660
 640/4566 [===>..........................] - ETA: 7:51 - loss: 0.6806 - acc: 0.5656
 704/4566 [===>..........................] - ETA: 7:28 - loss: 0.6785 - acc: 0.5696
 768/4566 [====>.........................] - ETA: 7:09 - loss: 0.6765 - acc: 0.5703
 832/4566 [====>.........................] - ETA: 6:52 - loss: 0.6719 - acc: 0.5757
 896/4566 [====>.........................] - ETA: 6:35 - loss: 0.6710 - acc: 0.5748
 960/4566 [=====>........................] - ETA: 6:23 - loss: 0.6693 - acc: 0.5781
1024/4566 [=====>........................] - ETA: 6:10 - loss: 0.6693 - acc: 0.5771
1088/4566 [======>.......................] - ETA: 5:58 - loss: 0.6689 - acc: 0.5772
1152/4566 [======>.......................] - ETA: 5:46 - loss: 0.6678 - acc: 0.5816
1216/4566 [======>.......................] - ETA: 5:37 - loss: 0.6676 - acc: 0.5831
1280/4566 [=======>......................] - ETA: 5:45 - loss: 0.6685 - acc: 0.5836
1344/4566 [=======>......................] - ETA: 5:53 - loss: 0.6667 - acc: 0.5826
1408/4566 [========>.....................] - ETA: 6:01 - loss: 0.6671 - acc: 0.5824
1472/4566 [========>.....................] - ETA: 6:06 - loss: 0.6677 - acc: 0.5829
1536/4566 [=========>....................] - ETA: 6:10 - loss: 0.6648 - acc: 0.5885
1600/4566 [=========>....................] - ETA: 6:12 - loss: 0.6641 - acc: 0.5894
1664/4566 [=========>....................] - ETA: 6:03 - loss: 0.6642 - acc: 0.5895
1728/4566 [==========>...................] - ETA: 5:51 - loss: 0.6640 - acc: 0.5903
1792/4566 [==========>...................] - ETA: 5:39 - loss: 0.6650 - acc: 0.5904
1856/4566 [===========>..................] - ETA: 5:27 - loss: 0.6644 - acc: 0.5921
1920/4566 [===========>..................] - ETA: 5:15 - loss: 0.6654 - acc: 0.5917
1984/4566 [============>.................] - ETA: 5:04 - loss: 0.6644 - acc: 0.5932
2048/4566 [============>.................] - ETA: 4:54 - loss: 0.6658 - acc: 0.5898
2112/4566 [============>.................] - ETA: 4:43 - loss: 0.6658 - acc: 0.5904
2176/4566 [=============>................] - ETA: 4:33 - loss: 0.6669 - acc: 0.5892
2240/4566 [=============>................] - ETA: 4:24 - loss: 0.6686 - acc: 0.5875
2304/4566 [==============>...............] - ETA: 4:14 - loss: 0.6669 - acc: 0.5903
2368/4566 [==============>...............] - ETA: 4:05 - loss: 0.6659 - acc: 0.5921
2432/4566 [==============>...............] - ETA: 3:56 - loss: 0.6653 - acc: 0.5933
2496/4566 [===============>..............] - ETA: 3:47 - loss: 0.6659 - acc: 0.5946
2560/4566 [===============>..............] - ETA: 3:38 - loss: 0.6653 - acc: 0.5969
2624/4566 [================>.............] - ETA: 3:30 - loss: 0.6652 - acc: 0.5979
2688/4566 [================>.............] - ETA: 3:25 - loss: 0.6648 - acc: 0.5986
2752/4566 [=================>............] - ETA: 3:23 - loss: 0.6641 - acc: 0.5996
2816/4566 [=================>............] - ETA: 3:20 - loss: 0.6650 - acc: 0.5987
2880/4566 [=================>............] - ETA: 3:16 - loss: 0.6639 - acc: 0.6003
2944/4566 [==================>...........] - ETA: 3:12 - loss: 0.6652 - acc: 0.5985
3008/4566 [==================>...........] - ETA: 3:08 - loss: 0.6649 - acc: 0.5984
3072/4566 [===================>..........] - ETA: 3:00 - loss: 0.6651 - acc: 0.5977
3136/4566 [===================>..........] - ETA: 2:51 - loss: 0.6663 - acc: 0.5953
3200/4566 [====================>.........] - ETA: 2:43 - loss: 0.6657 - acc: 0.5966
3264/4566 [====================>.........] - ETA: 2:34 - loss: 0.6668 - acc: 0.5956
3328/4566 [====================>.........] - ETA: 2:25 - loss: 0.6674 - acc: 0.5950
3392/4566 [=====================>........] - ETA: 2:17 - loss: 0.6672 - acc: 0.5952
3456/4566 [=====================>........] - ETA: 2:09 - loss: 0.6668 - acc: 0.5969
3520/4566 [======================>.......] - ETA: 2:00 - loss: 0.6674 - acc: 0.5957
3584/4566 [======================>.......] - ETA: 1:52 - loss: 0.6672 - acc: 0.5951
3648/4566 [======================>.......] - ETA: 1:44 - loss: 0.6676 - acc: 0.5938
3712/4566 [=======================>......] - ETA: 1:36 - loss: 0.6677 - acc: 0.5935
3776/4566 [=======================>......] - ETA: 1:29 - loss: 0.6684 - acc: 0.5916
3840/4566 [========================>.....] - ETA: 1:21 - loss: 0.6676 - acc: 0.5930
3904/4566 [========================>.....] - ETA: 1:13 - loss: 0.6674 - acc: 0.5930
3968/4566 [=========================>....] - ETA: 1:06 - loss: 0.6669 - acc: 0.5935
4032/4566 [=========================>....] - ETA: 59s - loss: 0.6670 - acc: 0.5940 
4096/4566 [=========================>....] - ETA: 52s - loss: 0.6664 - acc: 0.5940
4160/4566 [==========================>...] - ETA: 45s - loss: 0.6661 - acc: 0.5952
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6655 - acc: 0.5954
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6656 - acc: 0.5954
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6661 - acc: 0.5951
4416/4566 [============================>.] - ETA: 18s - loss: 0.6660 - acc: 0.5951
4480/4566 [============================>.] - ETA: 10s - loss: 0.6661 - acc: 0.5949
4544/4566 [============================>.] - ETA: 2s - loss: 0.6663 - acc: 0.5949 
4566/4566 [==============================] - 566s 124ms/step - loss: 0.6665 - acc: 0.5944 - val_loss: 0.6907 - val_acc: 0.5768

Epoch 00008: val_acc did not improve from 0.62008
Epoch 9/10

  64/4566 [..............................] - ETA: 5:24 - loss: 0.6666 - acc: 0.6406
 128/4566 [..............................] - ETA: 5:33 - loss: 0.6715 - acc: 0.6328
 192/4566 [>.............................] - ETA: 5:25 - loss: 0.6763 - acc: 0.6042
 256/4566 [>.............................] - ETA: 5:27 - loss: 0.6620 - acc: 0.6250
 320/4566 [=>............................] - ETA: 5:24 - loss: 0.6616 - acc: 0.6219
 384/4566 [=>............................] - ETA: 5:20 - loss: 0.6533 - acc: 0.6328
 448/4566 [=>............................] - ETA: 5:15 - loss: 0.6653 - acc: 0.6205
 512/4566 [==>...........................] - ETA: 5:08 - loss: 0.6695 - acc: 0.6152
 576/4566 [==>...........................] - ETA: 5:06 - loss: 0.6661 - acc: 0.6215
 640/4566 [===>..........................] - ETA: 5:00 - loss: 0.6612 - acc: 0.6297
 704/4566 [===>..........................] - ETA: 4:56 - loss: 0.6611 - acc: 0.6307
 768/4566 [====>.........................] - ETA: 5:12 - loss: 0.6606 - acc: 0.6341
 832/4566 [====>.........................] - ETA: 5:36 - loss: 0.6625 - acc: 0.6298
 896/4566 [====>.........................] - ETA: 5:58 - loss: 0.6643 - acc: 0.6228
 960/4566 [=====>........................] - ETA: 6:20 - loss: 0.6656 - acc: 0.6177
1024/4566 [=====>........................] - ETA: 6:36 - loss: 0.6684 - acc: 0.6113
1088/4566 [======>.......................] - ETA: 6:51 - loss: 0.6686 - acc: 0.6094
1152/4566 [======>.......................] - ETA: 6:48 - loss: 0.6675 - acc: 0.6094
1216/4566 [======>.......................] - ETA: 6:37 - loss: 0.6686 - acc: 0.6086
1280/4566 [=======>......................] - ETA: 6:24 - loss: 0.6706 - acc: 0.6055
1344/4566 [=======>......................] - ETA: 6:10 - loss: 0.6695 - acc: 0.6049
1408/4566 [========>.....................] - ETA: 5:56 - loss: 0.6694 - acc: 0.6044
1472/4566 [========>.....................] - ETA: 5:44 - loss: 0.6710 - acc: 0.6033
1536/4566 [=========>....................] - ETA: 5:32 - loss: 0.6704 - acc: 0.6042
1600/4566 [=========>....................] - ETA: 5:22 - loss: 0.6696 - acc: 0.6050
1664/4566 [=========>....................] - ETA: 5:12 - loss: 0.6710 - acc: 0.6028
1728/4566 [==========>...................] - ETA: 5:01 - loss: 0.6698 - acc: 0.6030
1792/4566 [==========>...................] - ETA: 4:52 - loss: 0.6689 - acc: 0.6055
1856/4566 [===========>..................] - ETA: 4:43 - loss: 0.6694 - acc: 0.6040
1920/4566 [===========>..................] - ETA: 4:35 - loss: 0.6700 - acc: 0.6000
1984/4566 [============>.................] - ETA: 4:27 - loss: 0.6699 - acc: 0.6008
2048/4566 [============>.................] - ETA: 4:18 - loss: 0.6699 - acc: 0.6011
2112/4566 [============>.................] - ETA: 4:09 - loss: 0.6680 - acc: 0.6032
2176/4566 [=============>................] - ETA: 4:01 - loss: 0.6690 - acc: 0.6020
2240/4566 [=============>................] - ETA: 3:59 - loss: 0.6703 - acc: 0.5991
2304/4566 [==============>...............] - ETA: 3:59 - loss: 0.6705 - acc: 0.5994
2368/4566 [==============>...............] - ETA: 3:59 - loss: 0.6709 - acc: 0.5980
2432/4566 [==============>...............] - ETA: 3:58 - loss: 0.6699 - acc: 0.5995
2496/4566 [===============>..............] - ETA: 3:55 - loss: 0.6699 - acc: 0.6010
2560/4566 [===============>..............] - ETA: 3:53 - loss: 0.6710 - acc: 0.5984
2624/4566 [================>.............] - ETA: 3:45 - loss: 0.6705 - acc: 0.5979
2688/4566 [================>.............] - ETA: 3:36 - loss: 0.6707 - acc: 0.5971
2752/4566 [=================>............] - ETA: 3:27 - loss: 0.6703 - acc: 0.5970
2816/4566 [=================>............] - ETA: 3:18 - loss: 0.6720 - acc: 0.5941
2880/4566 [=================>............] - ETA: 3:10 - loss: 0.6730 - acc: 0.5934
2944/4566 [==================>...........] - ETA: 3:01 - loss: 0.6732 - acc: 0.5938
3008/4566 [==================>...........] - ETA: 2:52 - loss: 0.6716 - acc: 0.5964
3072/4566 [===================>..........] - ETA: 2:44 - loss: 0.6712 - acc: 0.5973
3136/4566 [===================>..........] - ETA: 2:36 - loss: 0.6710 - acc: 0.5969
3200/4566 [====================>.........] - ETA: 2:28 - loss: 0.6709 - acc: 0.5981
3264/4566 [====================>.........] - ETA: 2:20 - loss: 0.6710 - acc: 0.5974
3328/4566 [====================>.........] - ETA: 2:12 - loss: 0.6713 - acc: 0.5962
3392/4566 [=====================>........] - ETA: 2:05 - loss: 0.6703 - acc: 0.5970
3456/4566 [=====================>........] - ETA: 1:57 - loss: 0.6703 - acc: 0.5972
3520/4566 [======================>.......] - ETA: 1:50 - loss: 0.6703 - acc: 0.5969
3584/4566 [======================>.......] - ETA: 1:43 - loss: 0.6708 - acc: 0.5951
3648/4566 [======================>.......] - ETA: 1:35 - loss: 0.6716 - acc: 0.5932
3712/4566 [=======================>......] - ETA: 1:29 - loss: 0.6716 - acc: 0.5929
3776/4566 [=======================>......] - ETA: 1:24 - loss: 0.6716 - acc: 0.5922
3840/4566 [========================>.....] - ETA: 1:18 - loss: 0.6717 - acc: 0.5922
3904/4566 [========================>.....] - ETA: 1:12 - loss: 0.6722 - acc: 0.5917
3968/4566 [=========================>....] - ETA: 1:06 - loss: 0.6720 - acc: 0.5927
4032/4566 [=========================>....] - ETA: 1:00 - loss: 0.6719 - acc: 0.5923
4096/4566 [=========================>....] - ETA: 53s - loss: 0.6718 - acc: 0.5925 
4160/4566 [==========================>...] - ETA: 45s - loss: 0.6721 - acc: 0.5925
4224/4566 [==========================>...] - ETA: 38s - loss: 0.6720 - acc: 0.5935
4288/4566 [===========================>..] - ETA: 31s - loss: 0.6715 - acc: 0.5938
4352/4566 [===========================>..] - ETA: 23s - loss: 0.6718 - acc: 0.5926
4416/4566 [============================>.] - ETA: 16s - loss: 0.6723 - acc: 0.5919
4480/4566 [============================>.] - ETA: 9s - loss: 0.6725 - acc: 0.5913 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6724 - acc: 0.5918
4566/4566 [==============================] - 513s 112ms/step - loss: 0.6726 - acc: 0.5913 - val_loss: 0.6659 - val_acc: 0.6004

Epoch 00009: val_acc did not improve from 0.62008
Epoch 10/10

  64/4566 [..............................] - ETA: 5:26 - loss: 0.6570 - acc: 0.5938
 128/4566 [..............................] - ETA: 5:08 - loss: 0.6748 - acc: 0.5703
 192/4566 [>.............................] - ETA: 5:26 - loss: 0.6586 - acc: 0.5729
 256/4566 [>.............................] - ETA: 5:14 - loss: 0.6663 - acc: 0.5625
 320/4566 [=>............................] - ETA: 5:12 - loss: 0.6620 - acc: 0.5750
 384/4566 [=>............................] - ETA: 5:05 - loss: 0.6547 - acc: 0.5990
 448/4566 [=>............................] - ETA: 5:34 - loss: 0.6552 - acc: 0.6027
 512/4566 [==>...........................] - ETA: 6:34 - loss: 0.6538 - acc: 0.6152
 576/4566 [==>...........................] - ETA: 7:18 - loss: 0.6581 - acc: 0.6111
 640/4566 [===>..........................] - ETA: 7:51 - loss: 0.6625 - acc: 0.5984
 704/4566 [===>..........................] - ETA: 8:15 - loss: 0.6630 - acc: 0.6023
 768/4566 [====>.........................] - ETA: 8:29 - loss: 0.6606 - acc: 0.6068
 832/4566 [====>.........................] - ETA: 8:18 - loss: 0.6596 - acc: 0.6142
 896/4566 [====>.........................] - ETA: 7:51 - loss: 0.6618 - acc: 0.6083
 960/4566 [=====>........................] - ETA: 7:29 - loss: 0.6607 - acc: 0.6104
1024/4566 [=====>........................] - ETA: 7:08 - loss: 0.6631 - acc: 0.6084
1088/4566 [======>.......................] - ETA: 6:48 - loss: 0.6619 - acc: 0.6094
1152/4566 [======>.......................] - ETA: 6:29 - loss: 0.6598 - acc: 0.6102
1216/4566 [======>.......................] - ETA: 6:14 - loss: 0.6602 - acc: 0.6118
1280/4566 [=======>......................] - ETA: 5:59 - loss: 0.6634 - acc: 0.6086
1344/4566 [=======>......................] - ETA: 5:44 - loss: 0.6621 - acc: 0.6101
1408/4566 [========>.....................] - ETA: 5:31 - loss: 0.6613 - acc: 0.6129
1472/4566 [========>.....................] - ETA: 5:18 - loss: 0.6603 - acc: 0.6148
1536/4566 [=========>....................] - ETA: 5:06 - loss: 0.6588 - acc: 0.6165
1600/4566 [=========>....................] - ETA: 4:55 - loss: 0.6587 - acc: 0.6175
1664/4566 [=========>....................] - ETA: 4:44 - loss: 0.6580 - acc: 0.6202
1728/4566 [==========>...................] - ETA: 4:34 - loss: 0.6583 - acc: 0.6209
1792/4566 [==========>...................] - ETA: 4:24 - loss: 0.6584 - acc: 0.6205
1856/4566 [===========>..................] - ETA: 4:14 - loss: 0.6576 - acc: 0.6223
1920/4566 [===========>..................] - ETA: 4:06 - loss: 0.6558 - acc: 0.6240
1984/4566 [============>.................] - ETA: 3:56 - loss: 0.6546 - acc: 0.6265
2048/4566 [============>.................] - ETA: 3:53 - loss: 0.6562 - acc: 0.6245
2112/4566 [============>.................] - ETA: 3:55 - loss: 0.6565 - acc: 0.6236
2176/4566 [=============>................] - ETA: 3:56 - loss: 0.6577 - acc: 0.6227
2240/4566 [=============>................] - ETA: 3:56 - loss: 0.6570 - acc: 0.6232
2304/4566 [==============>...............] - ETA: 3:55 - loss: 0.6576 - acc: 0.6215
2368/4566 [==============>...............] - ETA: 3:54 - loss: 0.6561 - acc: 0.6229
2432/4566 [==============>...............] - ETA: 3:49 - loss: 0.6574 - acc: 0.6205
2496/4566 [===============>..............] - ETA: 3:40 - loss: 0.6585 - acc: 0.6182
2560/4566 [===============>..............] - ETA: 3:30 - loss: 0.6587 - acc: 0.6168
2624/4566 [================>.............] - ETA: 3:21 - loss: 0.6592 - acc: 0.6162
2688/4566 [================>.............] - ETA: 3:12 - loss: 0.6603 - acc: 0.6146
2752/4566 [=================>............] - ETA: 3:04 - loss: 0.6603 - acc: 0.6130
2816/4566 [=================>............] - ETA: 2:55 - loss: 0.6605 - acc: 0.6129
2880/4566 [=================>............] - ETA: 2:47 - loss: 0.6613 - acc: 0.6135
2944/4566 [==================>...........] - ETA: 2:39 - loss: 0.6618 - acc: 0.6131
3008/4566 [==================>...........] - ETA: 2:32 - loss: 0.6623 - acc: 0.6124
3072/4566 [===================>..........] - ETA: 2:24 - loss: 0.6633 - acc: 0.6123
3136/4566 [===================>..........] - ETA: 2:17 - loss: 0.6640 - acc: 0.6107
3200/4566 [====================>.........] - ETA: 2:10 - loss: 0.6644 - acc: 0.6103
3264/4566 [====================>.........] - ETA: 2:03 - loss: 0.6650 - acc: 0.6088
3328/4566 [====================>.........] - ETA: 1:56 - loss: 0.6649 - acc: 0.6082
3392/4566 [=====================>........] - ETA: 1:49 - loss: 0.6642 - acc: 0.6085
3456/4566 [=====================>........] - ETA: 1:42 - loss: 0.6641 - acc: 0.6085
3520/4566 [======================>.......] - ETA: 1:36 - loss: 0.6644 - acc: 0.6085
3584/4566 [======================>.......] - ETA: 1:29 - loss: 0.6643 - acc: 0.6085
3648/4566 [======================>.......] - ETA: 1:23 - loss: 0.6646 - acc: 0.6083
3712/4566 [=======================>......] - ETA: 1:18 - loss: 0.6645 - acc: 0.6080
3776/4566 [=======================>......] - ETA: 1:13 - loss: 0.6651 - acc: 0.6062
3840/4566 [========================>.....] - ETA: 1:09 - loss: 0.6664 - acc: 0.6042
3904/4566 [========================>.....] - ETA: 1:04 - loss: 0.6660 - acc: 0.6058
3968/4566 [=========================>....] - ETA: 58s - loss: 0.6657 - acc: 0.6069 
4032/4566 [=========================>....] - ETA: 53s - loss: 0.6661 - acc: 0.6059
4096/4566 [=========================>....] - ETA: 47s - loss: 0.6657 - acc: 0.6060
4160/4566 [==========================>...] - ETA: 40s - loss: 0.6657 - acc: 0.6065
4224/4566 [==========================>...] - ETA: 33s - loss: 0.6665 - acc: 0.6044
4288/4566 [===========================>..] - ETA: 27s - loss: 0.6667 - acc: 0.6033
4352/4566 [===========================>..] - ETA: 20s - loss: 0.6667 - acc: 0.6034
4416/4566 [============================>.] - ETA: 14s - loss: 0.6671 - acc: 0.6033
4480/4566 [============================>.] - ETA: 8s - loss: 0.6673 - acc: 0.6025 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6667 - acc: 0.6026
4566/4566 [==============================] - 447s 98ms/step - loss: 0.6668 - acc: 0.6027 - val_loss: 0.6837 - val_acc: 0.5827

Epoch 00010: val_acc did not improve from 0.62008
Saved model to disk
