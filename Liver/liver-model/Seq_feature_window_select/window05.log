/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fcbe8c86050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fcbe8c86050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fcbe8c86690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fcbe8c86690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcbf1105910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcbf1105910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcbe8d38550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcbe8d38550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcbf102be50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcbf102be50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2c3bed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2c3bed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2d84f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2d84f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2bc2cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2bc2cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcba2cb9790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcba2cb9790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcba2b17f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcba2b17f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2ca5a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2ca5a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2df8850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2df8850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2b30a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2b30a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcba2aa7a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcba2aa7a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcba28a1b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcba28a1b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba27b76d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba27b76d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2aa7690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2aa7690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2a84050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2a84050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcba2609590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcba2609590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcba2859150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcba2859150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba24fd290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba24fd290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2593910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2593910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba253ae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba253ae50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcba2564410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcba2564410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcba27723d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcba27723d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba221fed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba221fed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2955790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2955790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2189310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2189310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb99f2e350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb99f2e350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcba1fae6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcba1fae6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba201de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba201de10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb99f2e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb99f2e1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb99e17810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb99e17810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb99c40e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb99c40e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb99bc8e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb99bc8e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb99c62e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb99c62e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb99c2b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb99c2b1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb99b26910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb99b26910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb99c14550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb99c14550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb99ae4c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb99ae4c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb99b4c050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb99b4c050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2e00d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcba2e00d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb9977ead0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb9977ead0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb995cd910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb995cd910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb9964fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb9964fe10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb995edc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb995edc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb995cd6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb995cd6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb995793d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb995793d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb992c6490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb992c6490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb992a2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb992a2a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb994957d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb994957d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb997ce1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb997ce1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2013650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcba2013650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb98f82c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb98f82c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb98fa7710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb98fa7710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb98f38d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb98f38d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb99162c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb99162c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb98eace10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb98eace10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb98c5af90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fcb98c5af90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb98c70090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fcb98c70090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb98cabb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb98cabb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb98c5a2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fcb98c5a2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb98a19490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fcb98a19490>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-17 11:02:18.180553: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-17 11:02:18.315709: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-17 11:02:18.410005: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563226ac9260 executing computations on platform Host. Devices:
2022-11-17 11:02:18.410225: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-17 11:02:19.350188: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window05.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 38:49 - loss: 0.6760 - acc: 0.5156
 128/4566 [..............................] - ETA: 35:13 - loss: 0.7080 - acc: 0.5078
 192/4566 [>.............................] - ETA: 30:03 - loss: 0.7012 - acc: 0.5469
 256/4566 [>.............................] - ETA: 27:45 - loss: 0.7062 - acc: 0.5625
 320/4566 [=>............................] - ETA: 23:51 - loss: 0.7162 - acc: 0.5563
 384/4566 [=>............................] - ETA: 23:28 - loss: 0.7101 - acc: 0.5573
 448/4566 [=>............................] - ETA: 21:38 - loss: 0.7089 - acc: 0.5513
 512/4566 [==>...........................] - ETA: 20:00 - loss: 0.7191 - acc: 0.5469
 576/4566 [==>...........................] - ETA: 18:26 - loss: 0.7265 - acc: 0.5365
 640/4566 [===>..........................] - ETA: 17:19 - loss: 0.7298 - acc: 0.5375
 704/4566 [===>..........................] - ETA: 16:21 - loss: 0.7320 - acc: 0.5298
 768/4566 [====>.........................] - ETA: 15:22 - loss: 0.7264 - acc: 0.5326
 832/4566 [====>.........................] - ETA: 14:33 - loss: 0.7250 - acc: 0.5397
 896/4566 [====>.........................] - ETA: 13:49 - loss: 0.7269 - acc: 0.5335
 960/4566 [=====>........................] - ETA: 13:42 - loss: 0.7285 - acc: 0.5323
1024/4566 [=====>........................] - ETA: 13:34 - loss: 0.7252 - acc: 0.5352
1088/4566 [======>.......................] - ETA: 13:24 - loss: 0.7255 - acc: 0.5340
1152/4566 [======>.......................] - ETA: 13:11 - loss: 0.7292 - acc: 0.5286
1216/4566 [======>.......................] - ETA: 12:57 - loss: 0.7296 - acc: 0.5230
1280/4566 [=======>......................] - ETA: 12:29 - loss: 0.7300 - acc: 0.5211
1344/4566 [=======>......................] - ETA: 11:53 - loss: 0.7299 - acc: 0.5186
1408/4566 [========>.....................] - ETA: 11:21 - loss: 0.7315 - acc: 0.5156
1472/4566 [========>.....................] - ETA: 10:51 - loss: 0.7313 - acc: 0.5136
1536/4566 [=========>....................] - ETA: 10:24 - loss: 0.7297 - acc: 0.5150
1600/4566 [=========>....................] - ETA: 9:58 - loss: 0.7291 - acc: 0.5138 
1664/4566 [=========>....................] - ETA: 9:34 - loss: 0.7290 - acc: 0.5114
1728/4566 [==========>...................] - ETA: 9:11 - loss: 0.7301 - acc: 0.5081
1792/4566 [==========>...................] - ETA: 8:48 - loss: 0.7293 - acc: 0.5089
1856/4566 [===========>..................] - ETA: 8:28 - loss: 0.7301 - acc: 0.5048
1920/4566 [===========>..................] - ETA: 8:09 - loss: 0.7301 - acc: 0.5026
1984/4566 [============>.................] - ETA: 7:50 - loss: 0.7299 - acc: 0.5015
2048/4566 [============>.................] - ETA: 7:31 - loss: 0.7301 - acc: 0.5020
2112/4566 [============>.................] - ETA: 7:14 - loss: 0.7294 - acc: 0.5024
2176/4566 [=============>................] - ETA: 6:57 - loss: 0.7291 - acc: 0.5005
2240/4566 [=============>................] - ETA: 6:50 - loss: 0.7285 - acc: 0.5000
2304/4566 [==============>...............] - ETA: 6:43 - loss: 0.7287 - acc: 0.5000
2368/4566 [==============>...............] - ETA: 6:35 - loss: 0.7280 - acc: 0.5013
2432/4566 [==============>...............] - ETA: 6:28 - loss: 0.7280 - acc: 0.4996
2496/4566 [===============>..............] - ETA: 6:19 - loss: 0.7284 - acc: 0.5000
2560/4566 [===============>..............] - ETA: 6:07 - loss: 0.7272 - acc: 0.5027
2624/4566 [================>.............] - ETA: 5:51 - loss: 0.7259 - acc: 0.5042
2688/4566 [================>.............] - ETA: 5:36 - loss: 0.7256 - acc: 0.5037
2752/4566 [=================>............] - ETA: 5:21 - loss: 0.7264 - acc: 0.5029
2816/4566 [=================>............] - ETA: 5:06 - loss: 0.7267 - acc: 0.5028
2880/4566 [=================>............] - ETA: 4:52 - loss: 0.7243 - acc: 0.5045
2944/4566 [==================>...........] - ETA: 4:38 - loss: 0.7253 - acc: 0.5027
3008/4566 [==================>...........] - ETA: 4:24 - loss: 0.7238 - acc: 0.5040
3072/4566 [===================>..........] - ETA: 4:11 - loss: 0.7229 - acc: 0.5065
3136/4566 [===================>..........] - ETA: 3:58 - loss: 0.7239 - acc: 0.5041
3200/4566 [====================>.........] - ETA: 3:45 - loss: 0.7227 - acc: 0.5072
3264/4566 [====================>.........] - ETA: 3:33 - loss: 0.7230 - acc: 0.5052
3328/4566 [====================>.........] - ETA: 3:21 - loss: 0.7218 - acc: 0.5075
3392/4566 [=====================>........] - ETA: 3:09 - loss: 0.7214 - acc: 0.5080
3456/4566 [=====================>........] - ETA: 2:57 - loss: 0.7206 - acc: 0.5087
3520/4566 [======================>.......] - ETA: 2:46 - loss: 0.7203 - acc: 0.5080
3584/4566 [======================>.......] - ETA: 2:38 - loss: 0.7194 - acc: 0.5106
3648/4566 [======================>.......] - ETA: 2:29 - loss: 0.7189 - acc: 0.5093
3712/4566 [=======================>......] - ETA: 2:19 - loss: 0.7193 - acc: 0.5089
3776/4566 [=======================>......] - ETA: 2:10 - loss: 0.7190 - acc: 0.5087
3840/4566 [========================>.....] - ETA: 2:00 - loss: 0.7185 - acc: 0.5081
3904/4566 [========================>.....] - ETA: 1:49 - loss: 0.7180 - acc: 0.5082
3968/4566 [=========================>....] - ETA: 1:38 - loss: 0.7174 - acc: 0.5093
4032/4566 [=========================>....] - ETA: 1:27 - loss: 0.7166 - acc: 0.5104
4096/4566 [=========================>....] - ETA: 1:16 - loss: 0.7165 - acc: 0.5095
4160/4566 [==========================>...] - ETA: 1:05 - loss: 0.7165 - acc: 0.5096
4224/4566 [==========================>...] - ETA: 54s - loss: 0.7169 - acc: 0.5080 
4288/4566 [===========================>..] - ETA: 44s - loss: 0.7169 - acc: 0.5077
4352/4566 [===========================>..] - ETA: 33s - loss: 0.7169 - acc: 0.5067
4416/4566 [============================>.] - ETA: 23s - loss: 0.7175 - acc: 0.5052
4480/4566 [============================>.] - ETA: 13s - loss: 0.7176 - acc: 0.5054
4544/4566 [============================>.] - ETA: 3s - loss: 0.7176 - acc: 0.5044 
4566/4566 [==============================] - 728s 159ms/step - loss: 0.7172 - acc: 0.5048 - val_loss: 0.6843 - val_acc: 0.5571

Epoch 00001: val_acc improved from -inf to 0.55709, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window05/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 8:24 - loss: 0.6920 - acc: 0.5469
 128/4566 [..............................] - ETA: 12:33 - loss: 0.6984 - acc: 0.5625
 192/4566 [>.............................] - ETA: 13:48 - loss: 0.6974 - acc: 0.5469
 256/4566 [>.............................] - ETA: 14:11 - loss: 0.6932 - acc: 0.5508
 320/4566 [=>............................] - ETA: 14:29 - loss: 0.6992 - acc: 0.5469
 384/4566 [=>............................] - ETA: 14:31 - loss: 0.6969 - acc: 0.5417
 448/4566 [=>............................] - ETA: 13:48 - loss: 0.7009 - acc: 0.5357
 512/4566 [==>...........................] - ETA: 12:42 - loss: 0.7031 - acc: 0.5312
 576/4566 [==>...........................] - ETA: 11:47 - loss: 0.6966 - acc: 0.5399
 640/4566 [===>..........................] - ETA: 11:01 - loss: 0.6975 - acc: 0.5422
 704/4566 [===>..........................] - ETA: 10:23 - loss: 0.7013 - acc: 0.5327
 768/4566 [====>.........................] - ETA: 9:50 - loss: 0.7017 - acc: 0.5234 
 832/4566 [====>.........................] - ETA: 9:20 - loss: 0.7007 - acc: 0.5300
 896/4566 [====>.........................] - ETA: 8:56 - loss: 0.6996 - acc: 0.5312
 960/4566 [=====>........................] - ETA: 8:33 - loss: 0.6990 - acc: 0.5302
1024/4566 [=====>........................] - ETA: 8:12 - loss: 0.6967 - acc: 0.5361
1088/4566 [======>.......................] - ETA: 7:53 - loss: 0.6944 - acc: 0.5386
1152/4566 [======>.......................] - ETA: 7:37 - loss: 0.6939 - acc: 0.5408
1216/4566 [======>.......................] - ETA: 7:21 - loss: 0.6949 - acc: 0.5395
1280/4566 [=======>......................] - ETA: 7:06 - loss: 0.6968 - acc: 0.5367
1344/4566 [=======>......................] - ETA: 6:52 - loss: 0.6966 - acc: 0.5379
1408/4566 [========>.....................] - ETA: 6:38 - loss: 0.6979 - acc: 0.5355
1472/4566 [========>.....................] - ETA: 6:40 - loss: 0.6978 - acc: 0.5360
1536/4566 [=========>....................] - ETA: 6:44 - loss: 0.6968 - acc: 0.5378
1600/4566 [=========>....................] - ETA: 6:48 - loss: 0.6973 - acc: 0.5344
1664/4566 [=========>....................] - ETA: 6:48 - loss: 0.6968 - acc: 0.5349
1728/4566 [==========>...................] - ETA: 6:49 - loss: 0.6978 - acc: 0.5312
1792/4566 [==========>...................] - ETA: 6:46 - loss: 0.6984 - acc: 0.5312
1856/4566 [===========>..................] - ETA: 6:33 - loss: 0.6981 - acc: 0.5307
1920/4566 [===========>..................] - ETA: 6:19 - loss: 0.6982 - acc: 0.5312
1984/4566 [============>.................] - ETA: 6:06 - loss: 0.6986 - acc: 0.5318
2048/4566 [============>.................] - ETA: 5:53 - loss: 0.6985 - acc: 0.5322
2112/4566 [============>.................] - ETA: 5:40 - loss: 0.7005 - acc: 0.5256
2176/4566 [=============>................] - ETA: 5:28 - loss: 0.7004 - acc: 0.5262
2240/4566 [=============>................] - ETA: 5:16 - loss: 0.6992 - acc: 0.5304
2304/4566 [==============>...............] - ETA: 5:04 - loss: 0.6990 - acc: 0.5304
2368/4566 [==============>...............] - ETA: 4:53 - loss: 0.6981 - acc: 0.5325
2432/4566 [==============>...............] - ETA: 4:41 - loss: 0.6985 - acc: 0.5333
2496/4566 [===============>..............] - ETA: 4:30 - loss: 0.6986 - acc: 0.5317
2560/4566 [===============>..............] - ETA: 4:20 - loss: 0.6988 - acc: 0.5305
2624/4566 [================>.............] - ETA: 4:10 - loss: 0.6993 - acc: 0.5309
2688/4566 [================>.............] - ETA: 4:00 - loss: 0.7005 - acc: 0.5290
2752/4566 [=================>............] - ETA: 3:50 - loss: 0.7008 - acc: 0.5276
2816/4566 [=================>............] - ETA: 3:44 - loss: 0.7009 - acc: 0.5266
2880/4566 [=================>............] - ETA: 3:40 - loss: 0.7006 - acc: 0.5274
2944/4566 [==================>...........] - ETA: 3:34 - loss: 0.7008 - acc: 0.5265
3008/4566 [==================>...........] - ETA: 3:29 - loss: 0.7005 - acc: 0.5269
3072/4566 [===================>..........] - ETA: 3:24 - loss: 0.7017 - acc: 0.5234
3136/4566 [===================>..........] - ETA: 3:18 - loss: 0.7020 - acc: 0.5220
3200/4566 [====================>.........] - ETA: 3:09 - loss: 0.7024 - acc: 0.5219
3264/4566 [====================>.........] - ETA: 2:59 - loss: 0.7025 - acc: 0.5214
3328/4566 [====================>.........] - ETA: 2:49 - loss: 0.7027 - acc: 0.5204
3392/4566 [=====================>........] - ETA: 2:39 - loss: 0.7024 - acc: 0.5209
3456/4566 [=====================>........] - ETA: 2:29 - loss: 0.7028 - acc: 0.5205
3520/4566 [======================>.......] - ETA: 2:20 - loss: 0.7024 - acc: 0.5207
3584/4566 [======================>.......] - ETA: 2:10 - loss: 0.7024 - acc: 0.5206
3648/4566 [======================>.......] - ETA: 2:01 - loss: 0.7022 - acc: 0.5203
3712/4566 [=======================>......] - ETA: 1:52 - loss: 0.7024 - acc: 0.5199
3776/4566 [=======================>......] - ETA: 1:43 - loss: 0.7023 - acc: 0.5185
3840/4566 [========================>.....] - ETA: 1:34 - loss: 0.7025 - acc: 0.5182
3904/4566 [========================>.....] - ETA: 1:25 - loss: 0.7026 - acc: 0.5174
3968/4566 [=========================>....] - ETA: 1:17 - loss: 0.7022 - acc: 0.5176
4032/4566 [=========================>....] - ETA: 1:08 - loss: 0.7024 - acc: 0.5179
4096/4566 [=========================>....] - ETA: 1:00 - loss: 0.7019 - acc: 0.5178
4160/4566 [==========================>...] - ETA: 51s - loss: 0.7021 - acc: 0.5166 
4224/4566 [==========================>...] - ETA: 44s - loss: 0.7017 - acc: 0.5173
4288/4566 [===========================>..] - ETA: 36s - loss: 0.7010 - acc: 0.5196
4352/4566 [===========================>..] - ETA: 28s - loss: 0.7004 - acc: 0.5202
4416/4566 [============================>.] - ETA: 20s - loss: 0.7000 - acc: 0.5217
4480/4566 [============================>.] - ETA: 11s - loss: 0.7000 - acc: 0.5217
4544/4566 [============================>.] - ETA: 2s - loss: 0.7003 - acc: 0.5213 
4566/4566 [==============================] - 633s 139ms/step - loss: 0.7002 - acc: 0.5217 - val_loss: 0.6783 - val_acc: 0.5689

Epoch 00002: val_acc improved from 0.55709 to 0.56890, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window05/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 3/10

  64/4566 [..............................] - ETA: 5:54 - loss: 0.7112 - acc: 0.4531
 128/4566 [..............................] - ETA: 5:58 - loss: 0.7035 - acc: 0.4453
 192/4566 [>.............................] - ETA: 6:09 - loss: 0.7006 - acc: 0.5000
 256/4566 [>.............................] - ETA: 6:00 - loss: 0.6979 - acc: 0.5039
 320/4566 [=>............................] - ETA: 6:04 - loss: 0.6992 - acc: 0.4938
 384/4566 [=>............................] - ETA: 5:58 - loss: 0.6962 - acc: 0.5052
 448/4566 [=>............................] - ETA: 5:51 - loss: 0.6969 - acc: 0.5112
 512/4566 [==>...........................] - ETA: 5:52 - loss: 0.6921 - acc: 0.5195
 576/4566 [==>...........................] - ETA: 5:54 - loss: 0.6908 - acc: 0.5122
 640/4566 [===>..........................] - ETA: 5:49 - loss: 0.6899 - acc: 0.5234
 704/4566 [===>..........................] - ETA: 5:44 - loss: 0.6847 - acc: 0.5384
 768/4566 [====>.........................] - ETA: 5:52 - loss: 0.6850 - acc: 0.5365
 832/4566 [====>.........................] - ETA: 6:25 - loss: 0.6844 - acc: 0.5373
 896/4566 [====>.........................] - ETA: 6:54 - loss: 0.6907 - acc: 0.5234
 960/4566 [=====>........................] - ETA: 7:15 - loss: 0.6918 - acc: 0.5198
1024/4566 [=====>........................] - ETA: 7:30 - loss: 0.6906 - acc: 0.5205
1088/4566 [======>.......................] - ETA: 7:40 - loss: 0.6919 - acc: 0.5175
1152/4566 [======>.......................] - ETA: 7:37 - loss: 0.6948 - acc: 0.5113
1216/4566 [======>.......................] - ETA: 7:20 - loss: 0.6960 - acc: 0.5123
1280/4566 [=======>......................] - ETA: 7:05 - loss: 0.6943 - acc: 0.5172
1344/4566 [=======>......................] - ETA: 6:51 - loss: 0.6961 - acc: 0.5156
1408/4566 [========>.....................] - ETA: 6:36 - loss: 0.6954 - acc: 0.5163
1472/4566 [========>.....................] - ETA: 6:23 - loss: 0.6970 - acc: 0.5156
1536/4566 [=========>....................] - ETA: 6:10 - loss: 0.6985 - acc: 0.5124
1600/4566 [=========>....................] - ETA: 5:58 - loss: 0.6976 - acc: 0.5125
1664/4566 [=========>....................] - ETA: 5:48 - loss: 0.6962 - acc: 0.5174
1728/4566 [==========>...................] - ETA: 5:37 - loss: 0.6959 - acc: 0.5185
1792/4566 [==========>...................] - ETA: 5:25 - loss: 0.6951 - acc: 0.5218
1856/4566 [===========>..................] - ETA: 5:15 - loss: 0.6948 - acc: 0.5232
1920/4566 [===========>..................] - ETA: 5:05 - loss: 0.6945 - acc: 0.5229
1984/4566 [============>.................] - ETA: 4:56 - loss: 0.6950 - acc: 0.5212
2048/4566 [============>.................] - ETA: 4:47 - loss: 0.6961 - acc: 0.5176
2112/4566 [============>.................] - ETA: 4:37 - loss: 0.6963 - acc: 0.5166
2176/4566 [=============>................] - ETA: 4:33 - loss: 0.6959 - acc: 0.5175
2240/4566 [=============>................] - ETA: 4:33 - loss: 0.6960 - acc: 0.5165
2304/4566 [==============>...............] - ETA: 4:33 - loss: 0.6972 - acc: 0.5122
2368/4566 [==============>...............] - ETA: 4:31 - loss: 0.6963 - acc: 0.5139
2432/4566 [==============>...............] - ETA: 4:29 - loss: 0.6960 - acc: 0.5152
2496/4566 [===============>..............] - ETA: 4:26 - loss: 0.6965 - acc: 0.5156
2560/4566 [===============>..............] - ETA: 4:18 - loss: 0.6971 - acc: 0.5145
2624/4566 [================>.............] - ETA: 4:08 - loss: 0.6961 - acc: 0.5175
2688/4566 [================>.............] - ETA: 3:58 - loss: 0.6955 - acc: 0.5186
2752/4566 [=================>............] - ETA: 3:48 - loss: 0.6952 - acc: 0.5196
2816/4566 [=================>............] - ETA: 3:38 - loss: 0.6952 - acc: 0.5195
2880/4566 [=================>............] - ETA: 3:29 - loss: 0.6953 - acc: 0.5181
2944/4566 [==================>...........] - ETA: 3:20 - loss: 0.6958 - acc: 0.5180
3008/4566 [==================>...........] - ETA: 3:11 - loss: 0.6958 - acc: 0.5170
3072/4566 [===================>..........] - ETA: 3:02 - loss: 0.6955 - acc: 0.5169
3136/4566 [===================>..........] - ETA: 2:53 - loss: 0.6949 - acc: 0.5182
3200/4566 [====================>.........] - ETA: 2:44 - loss: 0.6948 - acc: 0.5194
3264/4566 [====================>.........] - ETA: 2:36 - loss: 0.6946 - acc: 0.5199
3328/4566 [====================>.........] - ETA: 2:28 - loss: 0.6944 - acc: 0.5213
3392/4566 [=====================>........] - ETA: 2:19 - loss: 0.6950 - acc: 0.5206
3456/4566 [=====================>........] - ETA: 2:11 - loss: 0.6957 - acc: 0.5191
3520/4566 [======================>.......] - ETA: 2:03 - loss: 0.6957 - acc: 0.5193
3584/4566 [======================>.......] - ETA: 1:57 - loss: 0.6958 - acc: 0.5204
3648/4566 [======================>.......] - ETA: 1:51 - loss: 0.6960 - acc: 0.5197
3712/4566 [=======================>......] - ETA: 1:45 - loss: 0.6962 - acc: 0.5202
3776/4566 [=======================>......] - ETA: 1:38 - loss: 0.6958 - acc: 0.5193
3840/4566 [========================>.....] - ETA: 1:31 - loss: 0.6959 - acc: 0.5206
3904/4566 [========================>.....] - ETA: 1:24 - loss: 0.6957 - acc: 0.5215
3968/4566 [=========================>....] - ETA: 1:16 - loss: 0.6954 - acc: 0.5217
4032/4566 [=========================>....] - ETA: 1:07 - loss: 0.6955 - acc: 0.5206
4096/4566 [=========================>....] - ETA: 59s - loss: 0.6959 - acc: 0.5195 
4160/4566 [==========================>...] - ETA: 50s - loss: 0.6955 - acc: 0.5204
4224/4566 [==========================>...] - ETA: 42s - loss: 0.6955 - acc: 0.5206
4288/4566 [===========================>..] - ETA: 34s - loss: 0.6960 - acc: 0.5198
4352/4566 [===========================>..] - ETA: 26s - loss: 0.6959 - acc: 0.5214
4416/4566 [============================>.] - ETA: 18s - loss: 0.6958 - acc: 0.5220
4480/4566 [============================>.] - ETA: 10s - loss: 0.6960 - acc: 0.5208
4544/4566 [============================>.] - ETA: 2s - loss: 0.6963 - acc: 0.5202 
4566/4566 [==============================] - 574s 126ms/step - loss: 0.6963 - acc: 0.5201 - val_loss: 0.6929 - val_acc: 0.5374

Epoch 00003: val_acc did not improve from 0.56890
Epoch 4/10

  64/4566 [..............................] - ETA: 6:40 - loss: 0.7016 - acc: 0.5156
 128/4566 [..............................] - ETA: 7:18 - loss: 0.6846 - acc: 0.5703
 192/4566 [>.............................] - ETA: 10:01 - loss: 0.6944 - acc: 0.5521
 256/4566 [>.............................] - ETA: 11:25 - loss: 0.7006 - acc: 0.5234
 320/4566 [=>............................] - ETA: 12:07 - loss: 0.6938 - acc: 0.5437
 384/4566 [=>............................] - ETA: 12:24 - loss: 0.6892 - acc: 0.5469
 448/4566 [=>............................] - ETA: 12:45 - loss: 0.6880 - acc: 0.5536
 512/4566 [==>...........................] - ETA: 12:43 - loss: 0.6902 - acc: 0.5508
 576/4566 [==>...........................] - ETA: 11:50 - loss: 0.6918 - acc: 0.5451
 640/4566 [===>..........................] - ETA: 11:02 - loss: 0.6955 - acc: 0.5391
 704/4566 [===>..........................] - ETA: 10:20 - loss: 0.6962 - acc: 0.5341
 768/4566 [====>.........................] - ETA: 9:45 - loss: 0.6937 - acc: 0.5391 
 832/4566 [====>.........................] - ETA: 9:15 - loss: 0.6948 - acc: 0.5349
 896/4566 [====>.........................] - ETA: 8:50 - loss: 0.6938 - acc: 0.5368
 960/4566 [=====>........................] - ETA: 8:28 - loss: 0.6916 - acc: 0.5375
1024/4566 [=====>........................] - ETA: 8:06 - loss: 0.6900 - acc: 0.5381
1088/4566 [======>.......................] - ETA: 7:46 - loss: 0.6913 - acc: 0.5331
1152/4566 [======>.......................] - ETA: 7:27 - loss: 0.6903 - acc: 0.5365
1216/4566 [======>.......................] - ETA: 7:10 - loss: 0.6884 - acc: 0.5403
1280/4566 [=======>......................] - ETA: 6:55 - loss: 0.6878 - acc: 0.5430
1344/4566 [=======>......................] - ETA: 6:41 - loss: 0.6872 - acc: 0.5439
1408/4566 [========>.....................] - ETA: 6:27 - loss: 0.6861 - acc: 0.5462
1472/4566 [========>.....................] - ETA: 6:14 - loss: 0.6856 - acc: 0.5476
1536/4566 [=========>....................] - ETA: 6:04 - loss: 0.6858 - acc: 0.5469
1600/4566 [=========>....................] - ETA: 6:03 - loss: 0.6873 - acc: 0.5444
1664/4566 [=========>....................] - ETA: 6:06 - loss: 0.6885 - acc: 0.5421
1728/4566 [==========>...................] - ETA: 6:08 - loss: 0.6875 - acc: 0.5428
1792/4566 [==========>...................] - ETA: 6:09 - loss: 0.6875 - acc: 0.5435
1856/4566 [===========>..................] - ETA: 6:09 - loss: 0.6880 - acc: 0.5463
1920/4566 [===========>..................] - ETA: 6:07 - loss: 0.6881 - acc: 0.5448
1984/4566 [============>.................] - ETA: 5:57 - loss: 0.6876 - acc: 0.5484
2048/4566 [============>.................] - ETA: 5:44 - loss: 0.6876 - acc: 0.5483
2112/4566 [============>.................] - ETA: 5:31 - loss: 0.6874 - acc: 0.5483
2176/4566 [=============>................] - ETA: 5:19 - loss: 0.6877 - acc: 0.5492
2240/4566 [=============>................] - ETA: 5:07 - loss: 0.6878 - acc: 0.5509
2304/4566 [==============>...............] - ETA: 4:56 - loss: 0.6882 - acc: 0.5486
2368/4566 [==============>...............] - ETA: 4:45 - loss: 0.6885 - acc: 0.5486
2432/4566 [==============>...............] - ETA: 4:34 - loss: 0.6883 - acc: 0.5481
2496/4566 [===============>..............] - ETA: 4:23 - loss: 0.6886 - acc: 0.5457
2560/4566 [===============>..............] - ETA: 4:13 - loss: 0.6887 - acc: 0.5453
2624/4566 [================>.............] - ETA: 4:03 - loss: 0.6876 - acc: 0.5499
2688/4566 [================>.............] - ETA: 3:53 - loss: 0.6880 - acc: 0.5484
2752/4566 [=================>............] - ETA: 3:44 - loss: 0.6874 - acc: 0.5498
2816/4566 [=================>............] - ETA: 3:34 - loss: 0.6874 - acc: 0.5504
2880/4566 [=================>............] - ETA: 3:25 - loss: 0.6880 - acc: 0.5503
2944/4566 [==================>...........] - ETA: 3:16 - loss: 0.6881 - acc: 0.5499
3008/4566 [==================>...........] - ETA: 3:08 - loss: 0.6875 - acc: 0.5502
3072/4566 [===================>..........] - ETA: 3:04 - loss: 0.6872 - acc: 0.5521
3136/4566 [===================>..........] - ETA: 2:59 - loss: 0.6867 - acc: 0.5523
3200/4566 [====================>.........] - ETA: 2:53 - loss: 0.6870 - acc: 0.5519
3264/4566 [====================>.........] - ETA: 2:48 - loss: 0.6875 - acc: 0.5506
3328/4566 [====================>.........] - ETA: 2:41 - loss: 0.6879 - acc: 0.5502
3392/4566 [=====================>........] - ETA: 2:34 - loss: 0.6881 - acc: 0.5492
3456/4566 [=====================>........] - ETA: 2:25 - loss: 0.6879 - acc: 0.5503
3520/4566 [======================>.......] - ETA: 2:16 - loss: 0.6874 - acc: 0.5511
3584/4566 [======================>.......] - ETA: 2:06 - loss: 0.6881 - acc: 0.5477
3648/4566 [======================>.......] - ETA: 1:57 - loss: 0.6882 - acc: 0.5477
3712/4566 [=======================>......] - ETA: 1:49 - loss: 0.6882 - acc: 0.5485
3776/4566 [=======================>......] - ETA: 1:40 - loss: 0.6885 - acc: 0.5479
3840/4566 [========================>.....] - ETA: 1:31 - loss: 0.6883 - acc: 0.5482
3904/4566 [========================>.....] - ETA: 1:23 - loss: 0.6882 - acc: 0.5482
3968/4566 [=========================>....] - ETA: 1:14 - loss: 0.6884 - acc: 0.5461
4032/4566 [=========================>....] - ETA: 1:06 - loss: 0.6886 - acc: 0.5464
4096/4566 [=========================>....] - ETA: 58s - loss: 0.6888 - acc: 0.5459 
4160/4566 [==========================>...] - ETA: 50s - loss: 0.6888 - acc: 0.5471
4224/4566 [==========================>...] - ETA: 41s - loss: 0.6886 - acc: 0.5473
4288/4566 [===========================>..] - ETA: 33s - loss: 0.6889 - acc: 0.5478
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6887 - acc: 0.5494
4416/4566 [============================>.] - ETA: 18s - loss: 0.6888 - acc: 0.5491
4480/4566 [============================>.] - ETA: 10s - loss: 0.6888 - acc: 0.5493
4544/4566 [============================>.] - ETA: 2s - loss: 0.6887 - acc: 0.5500 
4566/4566 [==============================] - 607s 133ms/step - loss: 0.6882 - acc: 0.5515 - val_loss: 0.6828 - val_acc: 0.5512

Epoch 00004: val_acc did not improve from 0.56890
Epoch 5/10

  64/4566 [..............................] - ETA: 14:14 - loss: 0.6952 - acc: 0.5781
 128/4566 [..............................] - ETA: 10:22 - loss: 0.7091 - acc: 0.5156
 192/4566 [>.............................] - ETA: 8:55 - loss: 0.6950 - acc: 0.5312 
 256/4566 [>.............................] - ETA: 8:01 - loss: 0.6866 - acc: 0.5391
 320/4566 [=>............................] - ETA: 7:26 - loss: 0.6865 - acc: 0.5312
 384/4566 [=>............................] - ETA: 7:02 - loss: 0.6812 - acc: 0.5469
 448/4566 [=>............................] - ETA: 6:44 - loss: 0.6843 - acc: 0.5469
 512/4566 [==>...........................] - ETA: 6:31 - loss: 0.6821 - acc: 0.5527
 576/4566 [==>...........................] - ETA: 6:18 - loss: 0.6825 - acc: 0.5573
 640/4566 [===>..........................] - ETA: 6:08 - loss: 0.6859 - acc: 0.5484
 704/4566 [===>..........................] - ETA: 6:00 - loss: 0.6808 - acc: 0.5526
 768/4566 [====>.........................] - ETA: 5:50 - loss: 0.6806 - acc: 0.5508
 832/4566 [====>.........................] - ETA: 5:42 - loss: 0.6859 - acc: 0.5385
 896/4566 [====>.........................] - ETA: 5:33 - loss: 0.6840 - acc: 0.5446
 960/4566 [=====>........................] - ETA: 5:26 - loss: 0.6833 - acc: 0.5427
1024/4566 [=====>........................] - ETA: 5:19 - loss: 0.6823 - acc: 0.5479
1088/4566 [======>.......................] - ETA: 5:14 - loss: 0.6816 - acc: 0.5496
1152/4566 [======>.......................] - ETA: 5:27 - loss: 0.6822 - acc: 0.5486
1216/4566 [======>.......................] - ETA: 5:44 - loss: 0.6822 - acc: 0.5502
1280/4566 [=======>......................] - ETA: 5:57 - loss: 0.6818 - acc: 0.5523
1344/4566 [=======>......................] - ETA: 6:06 - loss: 0.6825 - acc: 0.5506
1408/4566 [========>.....................] - ETA: 6:15 - loss: 0.6818 - acc: 0.5526
1472/4566 [========>.....................] - ETA: 6:17 - loss: 0.6829 - acc: 0.5516
1536/4566 [=========>....................] - ETA: 6:10 - loss: 0.6849 - acc: 0.5501
1600/4566 [=========>....................] - ETA: 5:59 - loss: 0.6851 - acc: 0.5475
1664/4566 [=========>....................] - ETA: 5:47 - loss: 0.6872 - acc: 0.5433
1728/4566 [==========>...................] - ETA: 5:35 - loss: 0.6872 - acc: 0.5434
1792/4566 [==========>...................] - ETA: 5:24 - loss: 0.6878 - acc: 0.5435
1856/4566 [===========>..................] - ETA: 5:13 - loss: 0.6871 - acc: 0.5458
1920/4566 [===========>..................] - ETA: 5:03 - loss: 0.6865 - acc: 0.5464
1984/4566 [============>.................] - ETA: 4:54 - loss: 0.6864 - acc: 0.5484
2048/4566 [============>.................] - ETA: 4:44 - loss: 0.6865 - acc: 0.5464
2112/4566 [============>.................] - ETA: 4:35 - loss: 0.6871 - acc: 0.5436
2176/4566 [=============>................] - ETA: 4:26 - loss: 0.6875 - acc: 0.5427
2240/4566 [=============>................] - ETA: 4:17 - loss: 0.6875 - acc: 0.5437
2304/4566 [==============>...............] - ETA: 4:08 - loss: 0.6877 - acc: 0.5451
2368/4566 [==============>...............] - ETA: 4:00 - loss: 0.6883 - acc: 0.5431
2432/4566 [==============>...............] - ETA: 3:52 - loss: 0.6891 - acc: 0.5411
2496/4566 [===============>..............] - ETA: 3:43 - loss: 0.6894 - acc: 0.5397
2560/4566 [===============>..............] - ETA: 3:37 - loss: 0.6896 - acc: 0.5391
2624/4566 [================>.............] - ETA: 3:35 - loss: 0.6898 - acc: 0.5393
2688/4566 [================>.............] - ETA: 3:32 - loss: 0.6901 - acc: 0.5394
2752/4566 [=================>............] - ETA: 3:30 - loss: 0.6901 - acc: 0.5385
2816/4566 [=================>............] - ETA: 3:26 - loss: 0.6906 - acc: 0.5366
2880/4566 [=================>............] - ETA: 3:22 - loss: 0.6910 - acc: 0.5358
2944/4566 [==================>...........] - ETA: 3:17 - loss: 0.6914 - acc: 0.5343
3008/4566 [==================>...........] - ETA: 3:08 - loss: 0.6922 - acc: 0.5322
3072/4566 [===================>..........] - ETA: 2:59 - loss: 0.6921 - acc: 0.5326
3136/4566 [===================>..........] - ETA: 2:51 - loss: 0.6911 - acc: 0.5351
3200/4566 [====================>.........] - ETA: 2:42 - loss: 0.6911 - acc: 0.5344
3264/4566 [====================>.........] - ETA: 2:34 - loss: 0.6911 - acc: 0.5343
3328/4566 [====================>.........] - ETA: 2:25 - loss: 0.6914 - acc: 0.5334
3392/4566 [=====================>........] - ETA: 2:17 - loss: 0.6905 - acc: 0.5360
3456/4566 [=====================>........] - ETA: 2:09 - loss: 0.6901 - acc: 0.5376
3520/4566 [======================>.......] - ETA: 2:01 - loss: 0.6907 - acc: 0.5378
3584/4566 [======================>.......] - ETA: 1:53 - loss: 0.6908 - acc: 0.5374
3648/4566 [======================>.......] - ETA: 1:45 - loss: 0.6915 - acc: 0.5345
3712/4566 [=======================>......] - ETA: 1:37 - loss: 0.6916 - acc: 0.5350
3776/4566 [=======================>......] - ETA: 1:30 - loss: 0.6919 - acc: 0.5347
3840/4566 [========================>.....] - ETA: 1:22 - loss: 0.6922 - acc: 0.5341
3904/4566 [========================>.....] - ETA: 1:14 - loss: 0.6918 - acc: 0.5353
3968/4566 [=========================>....] - ETA: 1:07 - loss: 0.6922 - acc: 0.5340
4032/4566 [=========================>....] - ETA: 1:00 - loss: 0.6923 - acc: 0.5335
4096/4566 [=========================>....] - ETA: 54s - loss: 0.6926 - acc: 0.5325 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6925 - acc: 0.5339
4224/4566 [==========================>...] - ETA: 40s - loss: 0.6921 - acc: 0.5353
4288/4566 [===========================>..] - ETA: 33s - loss: 0.6921 - acc: 0.5352
4352/4566 [===========================>..] - ETA: 26s - loss: 0.6923 - acc: 0.5358
4416/4566 [============================>.] - ETA: 18s - loss: 0.6920 - acc: 0.5367
4480/4566 [============================>.] - ETA: 10s - loss: 0.6919 - acc: 0.5368
4544/4566 [============================>.] - ETA: 2s - loss: 0.6918 - acc: 0.5370 
4566/4566 [==============================] - 565s 124ms/step - loss: 0.6919 - acc: 0.5366 - val_loss: 0.6910 - val_acc: 0.5197

Epoch 00005: val_acc did not improve from 0.56890
Epoch 6/10

  64/4566 [..............................] - ETA: 6:12 - loss: 0.7124 - acc: 0.5312
 128/4566 [..............................] - ETA: 6:19 - loss: 0.7149 - acc: 0.5000
 192/4566 [>.............................] - ETA: 6:09 - loss: 0.6995 - acc: 0.5312
 256/4566 [>.............................] - ETA: 6:07 - loss: 0.6977 - acc: 0.5273
 320/4566 [=>............................] - ETA: 6:01 - loss: 0.6954 - acc: 0.5312
 384/4566 [=>............................] - ETA: 6:00 - loss: 0.6927 - acc: 0.5260
 448/4566 [=>............................] - ETA: 5:52 - loss: 0.6954 - acc: 0.5312
 512/4566 [==>...........................] - ETA: 5:45 - loss: 0.6960 - acc: 0.5332
 576/4566 [==>...........................] - ETA: 5:40 - loss: 0.6964 - acc: 0.5295
 640/4566 [===>..........................] - ETA: 5:38 - loss: 0.6940 - acc: 0.5437
 704/4566 [===>..........................] - ETA: 6:09 - loss: 0.6935 - acc: 0.5455
 768/4566 [====>.........................] - ETA: 6:41 - loss: 0.6935 - acc: 0.5482
 832/4566 [====>.........................] - ETA: 7:09 - loss: 0.6911 - acc: 0.5541
 896/4566 [====>.........................] - ETA: 7:29 - loss: 0.6886 - acc: 0.5614
 960/4566 [=====>........................] - ETA: 7:42 - loss: 0.6863 - acc: 0.5646
1024/4566 [=====>........................] - ETA: 7:53 - loss: 0.6874 - acc: 0.5605
1088/4566 [======>.......................] - ETA: 7:42 - loss: 0.6859 - acc: 0.5616
1152/4566 [======>.......................] - ETA: 7:24 - loss: 0.6876 - acc: 0.5556
1216/4566 [======>.......................] - ETA: 7:06 - loss: 0.6860 - acc: 0.5576
1280/4566 [=======>......................] - ETA: 6:50 - loss: 0.6857 - acc: 0.5555
1344/4566 [=======>......................] - ETA: 6:36 - loss: 0.6862 - acc: 0.5551
1408/4566 [========>.....................] - ETA: 6:22 - loss: 0.6847 - acc: 0.5589
1472/4566 [========>.....................] - ETA: 6:10 - loss: 0.6849 - acc: 0.5611
1536/4566 [=========>....................] - ETA: 5:57 - loss: 0.6839 - acc: 0.5651
1600/4566 [=========>....................] - ETA: 5:45 - loss: 0.6846 - acc: 0.5663
1664/4566 [=========>....................] - ETA: 5:34 - loss: 0.6842 - acc: 0.5715
1728/4566 [==========>...................] - ETA: 5:23 - loss: 0.6844 - acc: 0.5723
1792/4566 [==========>...................] - ETA: 5:13 - loss: 0.6853 - acc: 0.5681
1856/4566 [===========>..................] - ETA: 5:04 - loss: 0.6842 - acc: 0.5700
1920/4566 [===========>..................] - ETA: 4:54 - loss: 0.6844 - acc: 0.5687
1984/4566 [============>.................] - ETA: 4:44 - loss: 0.6840 - acc: 0.5670
2048/4566 [============>.................] - ETA: 4:35 - loss: 0.6839 - acc: 0.5659
2112/4566 [============>.................] - ETA: 4:27 - loss: 0.6846 - acc: 0.5634
2176/4566 [=============>................] - ETA: 4:28 - loss: 0.6842 - acc: 0.5634
2240/4566 [=============>................] - ETA: 4:28 - loss: 0.6832 - acc: 0.5670
2304/4566 [==============>...............] - ETA: 4:27 - loss: 0.6831 - acc: 0.5664
2368/4566 [==============>...............] - ETA: 4:25 - loss: 0.6828 - acc: 0.5671
2432/4566 [==============>...............] - ETA: 4:23 - loss: 0.6825 - acc: 0.5670
2496/4566 [===============>..............] - ETA: 4:20 - loss: 0.6821 - acc: 0.5677
2560/4566 [===============>..............] - ETA: 4:11 - loss: 0.6812 - acc: 0.5691
2624/4566 [================>.............] - ETA: 4:01 - loss: 0.6808 - acc: 0.5697
2688/4566 [================>.............] - ETA: 3:51 - loss: 0.6812 - acc: 0.5703
2752/4566 [=================>............] - ETA: 3:42 - loss: 0.6833 - acc: 0.5676
2816/4566 [=================>............] - ETA: 3:33 - loss: 0.6835 - acc: 0.5657
2880/4566 [=================>............] - ETA: 3:24 - loss: 0.6827 - acc: 0.5667
2944/4566 [==================>...........] - ETA: 3:15 - loss: 0.6828 - acc: 0.5649
3008/4566 [==================>...........] - ETA: 3:06 - loss: 0.6826 - acc: 0.5638
3072/4566 [===================>..........] - ETA: 2:57 - loss: 0.6841 - acc: 0.5612
3136/4566 [===================>..........] - ETA: 2:48 - loss: 0.6848 - acc: 0.5596
3200/4566 [====================>.........] - ETA: 2:40 - loss: 0.6848 - acc: 0.5591
3264/4566 [====================>.........] - ETA: 2:32 - loss: 0.6847 - acc: 0.5591
3328/4566 [====================>.........] - ETA: 2:24 - loss: 0.6849 - acc: 0.5583
3392/4566 [=====================>........] - ETA: 2:15 - loss: 0.6852 - acc: 0.5578
3456/4566 [=====================>........] - ETA: 2:07 - loss: 0.6857 - acc: 0.5576
3520/4566 [======================>.......] - ETA: 1:59 - loss: 0.6859 - acc: 0.5565
3584/4566 [======================>.......] - ETA: 1:53 - loss: 0.6863 - acc: 0.5547
3648/4566 [======================>.......] - ETA: 1:47 - loss: 0.6861 - acc: 0.5543
3712/4566 [=======================>......] - ETA: 1:41 - loss: 0.6861 - acc: 0.5536
3776/4566 [=======================>......] - ETA: 1:35 - loss: 0.6860 - acc: 0.5548
3840/4566 [========================>.....] - ETA: 1:28 - loss: 0.6860 - acc: 0.5544
3904/4566 [========================>.....] - ETA: 1:21 - loss: 0.6857 - acc: 0.5548
3968/4566 [=========================>....] - ETA: 1:14 - loss: 0.6853 - acc: 0.5559
4032/4566 [=========================>....] - ETA: 1:05 - loss: 0.6854 - acc: 0.5563
4096/4566 [=========================>....] - ETA: 57s - loss: 0.6855 - acc: 0.5549 
4160/4566 [==========================>...] - ETA: 49s - loss: 0.6856 - acc: 0.5546
4224/4566 [==========================>...] - ETA: 41s - loss: 0.6857 - acc: 0.5549
4288/4566 [===========================>..] - ETA: 33s - loss: 0.6857 - acc: 0.5548
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6862 - acc: 0.5542
4416/4566 [============================>.] - ETA: 17s - loss: 0.6861 - acc: 0.5550
4480/4566 [============================>.] - ETA: 10s - loss: 0.6857 - acc: 0.5556
4544/4566 [============================>.] - ETA: 2s - loss: 0.6858 - acc: 0.5561 
4566/4566 [==============================] - 559s 122ms/step - loss: 0.6860 - acc: 0.5563 - val_loss: 0.7007 - val_acc: 0.4902

Epoch 00006: val_acc did not improve from 0.56890
Epoch 7/10

  64/4566 [..............................] - ETA: 5:56 - loss: 0.6916 - acc: 0.5469
 128/4566 [..............................] - ETA: 6:07 - loss: 0.6817 - acc: 0.5703
 192/4566 [>.............................] - ETA: 6:11 - loss: 0.6736 - acc: 0.5990
 256/4566 [>.............................] - ETA: 7:46 - loss: 0.6735 - acc: 0.5820
 320/4566 [=>............................] - ETA: 9:17 - loss: 0.6689 - acc: 0.6062
 384/4566 [=>............................] - ETA: 10:06 - loss: 0.6742 - acc: 0.5964
 448/4566 [=>............................] - ETA: 10:41 - loss: 0.6732 - acc: 0.5915
 512/4566 [==>...........................] - ETA: 10:59 - loss: 0.6753 - acc: 0.5879
 576/4566 [==>...........................] - ETA: 11:10 - loss: 0.6756 - acc: 0.5868
 640/4566 [===>..........................] - ETA: 10:40 - loss: 0.6748 - acc: 0.5953
 704/4566 [===>..........................] - ETA: 10:02 - loss: 0.6772 - acc: 0.5852
 768/4566 [====>.........................] - ETA: 9:28 - loss: 0.6773 - acc: 0.5820 
 832/4566 [====>.........................] - ETA: 8:58 - loss: 0.6769 - acc: 0.5805
 896/4566 [====>.........................] - ETA: 8:30 - loss: 0.6763 - acc: 0.5826
 960/4566 [=====>........................] - ETA: 8:07 - loss: 0.6772 - acc: 0.5792
1024/4566 [=====>........................] - ETA: 7:44 - loss: 0.6756 - acc: 0.5840
1088/4566 [======>.......................] - ETA: 7:26 - loss: 0.6768 - acc: 0.5790
1152/4566 [======>.......................] - ETA: 7:09 - loss: 0.6765 - acc: 0.5781
1216/4566 [======>.......................] - ETA: 6:53 - loss: 0.6763 - acc: 0.5773
1280/4566 [=======>......................] - ETA: 6:38 - loss: 0.6754 - acc: 0.5805
1344/4566 [=======>......................] - ETA: 6:24 - loss: 0.6761 - acc: 0.5781
1408/4566 [========>.....................] - ETA: 6:11 - loss: 0.6784 - acc: 0.5767
1472/4566 [========>.....................] - ETA: 5:58 - loss: 0.6793 - acc: 0.5747
1536/4566 [=========>....................] - ETA: 5:45 - loss: 0.6789 - acc: 0.5755
1600/4566 [=========>....................] - ETA: 5:34 - loss: 0.6793 - acc: 0.5731
1664/4566 [=========>....................] - ETA: 5:23 - loss: 0.6791 - acc: 0.5751
1728/4566 [==========>...................] - ETA: 5:24 - loss: 0.6771 - acc: 0.5781
1792/4566 [==========>...................] - ETA: 5:27 - loss: 0.6800 - acc: 0.5731
1856/4566 [===========>..................] - ETA: 5:29 - loss: 0.6822 - acc: 0.5706
1920/4566 [===========>..................] - ETA: 5:29 - loss: 0.6809 - acc: 0.5729
1984/4566 [============>.................] - ETA: 5:29 - loss: 0.6800 - acc: 0.5756
2048/4566 [============>.................] - ETA: 5:26 - loss: 0.6805 - acc: 0.5752
2112/4566 [============>.................] - ETA: 5:15 - loss: 0.6798 - acc: 0.5758
2176/4566 [=============>................] - ETA: 5:04 - loss: 0.6797 - acc: 0.5758
2240/4566 [=============>................] - ETA: 4:52 - loss: 0.6800 - acc: 0.5759
2304/4566 [==============>...............] - ETA: 4:41 - loss: 0.6807 - acc: 0.5742
2368/4566 [==============>...............] - ETA: 4:30 - loss: 0.6804 - acc: 0.5756
2432/4566 [==============>...............] - ETA: 4:20 - loss: 0.6796 - acc: 0.5785
2496/4566 [===============>..............] - ETA: 4:10 - loss: 0.6794 - acc: 0.5789
2560/4566 [===============>..............] - ETA: 4:00 - loss: 0.6797 - acc: 0.5789
2624/4566 [================>.............] - ETA: 3:50 - loss: 0.6799 - acc: 0.5766
2688/4566 [================>.............] - ETA: 3:40 - loss: 0.6801 - acc: 0.5770
2752/4566 [=================>............] - ETA: 3:31 - loss: 0.6794 - acc: 0.5781
2816/4566 [=================>............] - ETA: 3:22 - loss: 0.6791 - acc: 0.5803
2880/4566 [=================>............] - ETA: 3:13 - loss: 0.6794 - acc: 0.5819
2944/4566 [==================>...........] - ETA: 3:04 - loss: 0.6789 - acc: 0.5836
3008/4566 [==================>...........] - ETA: 2:55 - loss: 0.6788 - acc: 0.5834
3072/4566 [===================>..........] - ETA: 2:47 - loss: 0.6792 - acc: 0.5820
3136/4566 [===================>..........] - ETA: 2:39 - loss: 0.6792 - acc: 0.5813
3200/4566 [====================>.........] - ETA: 2:34 - loss: 0.6785 - acc: 0.5819
3264/4566 [====================>.........] - ETA: 2:29 - loss: 0.6787 - acc: 0.5812
3328/4566 [====================>.........] - ETA: 2:24 - loss: 0.6783 - acc: 0.5820
3392/4566 [=====================>........] - ETA: 2:19 - loss: 0.6784 - acc: 0.5817
3456/4566 [=====================>........] - ETA: 2:13 - loss: 0.6787 - acc: 0.5810
3520/4566 [======================>.......] - ETA: 2:07 - loss: 0.6791 - acc: 0.5798
3584/4566 [======================>.......] - ETA: 1:59 - loss: 0.6795 - acc: 0.5790
3648/4566 [======================>.......] - ETA: 1:50 - loss: 0.6794 - acc: 0.5787
3712/4566 [=======================>......] - ETA: 1:42 - loss: 0.6796 - acc: 0.5770
3776/4566 [=======================>......] - ETA: 1:34 - loss: 0.6796 - acc: 0.5771
3840/4566 [========================>.....] - ETA: 1:25 - loss: 0.6798 - acc: 0.5776
3904/4566 [========================>.....] - ETA: 1:17 - loss: 0.6806 - acc: 0.5756
3968/4566 [=========================>....] - ETA: 1:09 - loss: 0.6805 - acc: 0.5766
4032/4566 [=========================>....] - ETA: 1:01 - loss: 0.6810 - acc: 0.5747
4096/4566 [=========================>....] - ETA: 54s - loss: 0.6812 - acc: 0.5747 
4160/4566 [==========================>...] - ETA: 46s - loss: 0.6814 - acc: 0.5740
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6815 - acc: 0.5732
4288/4566 [===========================>..] - ETA: 31s - loss: 0.6819 - acc: 0.5725
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6817 - acc: 0.5731
4416/4566 [============================>.] - ETA: 16s - loss: 0.6820 - acc: 0.5725
4480/4566 [============================>.] - ETA: 9s - loss: 0.6824 - acc: 0.5717 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6820 - acc: 0.5731
4566/4566 [==============================] - 535s 117ms/step - loss: 0.6823 - acc: 0.5718 - val_loss: 0.6738 - val_acc: 0.5807

Epoch 00007: val_acc improved from 0.56890 to 0.58071, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window05/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 8/10

  64/4566 [..............................] - ETA: 16:15 - loss: 0.6917 - acc: 0.5156
 128/4566 [..............................] - ETA: 15:42 - loss: 0.6809 - acc: 0.5781
 192/4566 [>.............................] - ETA: 15:14 - loss: 0.6866 - acc: 0.5625
 256/4566 [>.............................] - ETA: 15:06 - loss: 0.6792 - acc: 0.5625
 320/4566 [=>............................] - ETA: 13:30 - loss: 0.6778 - acc: 0.5687
 384/4566 [=>............................] - ETA: 12:00 - loss: 0.6802 - acc: 0.5781
 448/4566 [=>............................] - ETA: 10:51 - loss: 0.6829 - acc: 0.5603
 512/4566 [==>...........................] - ETA: 9:56 - loss: 0.6844 - acc: 0.5566 
 576/4566 [==>...........................] - ETA: 9:17 - loss: 0.6858 - acc: 0.5608
 640/4566 [===>..........................] - ETA: 8:41 - loss: 0.6834 - acc: 0.5609
 704/4566 [===>..........................] - ETA: 8:12 - loss: 0.6848 - acc: 0.5526
 768/4566 [====>.........................] - ETA: 7:46 - loss: 0.6816 - acc: 0.5586
 832/4566 [====>.........................] - ETA: 7:25 - loss: 0.6804 - acc: 0.5613
 896/4566 [====>.........................] - ETA: 7:06 - loss: 0.6801 - acc: 0.5603
 960/4566 [=====>........................] - ETA: 6:48 - loss: 0.6812 - acc: 0.5573
1024/4566 [=====>........................] - ETA: 6:33 - loss: 0.6830 - acc: 0.5566
1088/4566 [======>.......................] - ETA: 6:18 - loss: 0.6825 - acc: 0.5597
1152/4566 [======>.......................] - ETA: 6:05 - loss: 0.6821 - acc: 0.5616
1216/4566 [======>.......................] - ETA: 5:52 - loss: 0.6818 - acc: 0.5633
1280/4566 [=======>......................] - ETA: 5:40 - loss: 0.6801 - acc: 0.5641
1344/4566 [=======>......................] - ETA: 5:29 - loss: 0.6819 - acc: 0.5610
1408/4566 [========>.....................] - ETA: 5:25 - loss: 0.6819 - acc: 0.5618
1472/4566 [========>.....................] - ETA: 5:31 - loss: 0.6830 - acc: 0.5584
1536/4566 [=========>....................] - ETA: 5:35 - loss: 0.6825 - acc: 0.5599
1600/4566 [=========>....................] - ETA: 5:38 - loss: 0.6813 - acc: 0.5631
1664/4566 [=========>....................] - ETA: 5:41 - loss: 0.6808 - acc: 0.5637
1728/4566 [==========>...................] - ETA: 5:42 - loss: 0.6817 - acc: 0.5619
1792/4566 [==========>...................] - ETA: 5:40 - loss: 0.6818 - acc: 0.5619
1856/4566 [===========>..................] - ETA: 5:30 - loss: 0.6818 - acc: 0.5625
1920/4566 [===========>..................] - ETA: 5:19 - loss: 0.6813 - acc: 0.5630
1984/4566 [============>.................] - ETA: 5:09 - loss: 0.6819 - acc: 0.5630
2048/4566 [============>.................] - ETA: 4:58 - loss: 0.6821 - acc: 0.5630
2112/4566 [============>.................] - ETA: 4:47 - loss: 0.6836 - acc: 0.5611
2176/4566 [=============>................] - ETA: 4:36 - loss: 0.6829 - acc: 0.5611
2240/4566 [=============>................] - ETA: 4:26 - loss: 0.6830 - acc: 0.5607
2304/4566 [==============>...............] - ETA: 4:16 - loss: 0.6828 - acc: 0.5599
2368/4566 [==============>...............] - ETA: 4:06 - loss: 0.6821 - acc: 0.5604
2432/4566 [==============>...............] - ETA: 3:57 - loss: 0.6826 - acc: 0.5604
2496/4566 [===============>..............] - ETA: 3:47 - loss: 0.6821 - acc: 0.5609
2560/4566 [===============>..............] - ETA: 3:39 - loss: 0.6819 - acc: 0.5629
2624/4566 [================>.............] - ETA: 3:30 - loss: 0.6817 - acc: 0.5633
2688/4566 [================>.............] - ETA: 3:21 - loss: 0.6814 - acc: 0.5647
2752/4566 [=================>............] - ETA: 3:13 - loss: 0.6817 - acc: 0.5629
2816/4566 [=================>............] - ETA: 3:05 - loss: 0.6821 - acc: 0.5639
2880/4566 [=================>............] - ETA: 2:58 - loss: 0.6821 - acc: 0.5635
2944/4566 [==================>...........] - ETA: 2:54 - loss: 0.6818 - acc: 0.5639
3008/4566 [==================>...........] - ETA: 2:51 - loss: 0.6819 - acc: 0.5638
3072/4566 [===================>..........] - ETA: 2:47 - loss: 0.6814 - acc: 0.5664
3136/4566 [===================>..........] - ETA: 2:43 - loss: 0.6820 - acc: 0.5654
3200/4566 [====================>.........] - ETA: 2:38 - loss: 0.6829 - acc: 0.5637
3264/4566 [====================>.........] - ETA: 2:32 - loss: 0.6831 - acc: 0.5643
3328/4566 [====================>.........] - ETA: 2:24 - loss: 0.6840 - acc: 0.5628
3392/4566 [=====================>........] - ETA: 2:16 - loss: 0.6837 - acc: 0.5637
3456/4566 [=====================>........] - ETA: 2:08 - loss: 0.6838 - acc: 0.5637
3520/4566 [======================>.......] - ETA: 2:00 - loss: 0.6834 - acc: 0.5636
3584/4566 [======================>.......] - ETA: 1:52 - loss: 0.6833 - acc: 0.5633
3648/4566 [======================>.......] - ETA: 1:44 - loss: 0.6841 - acc: 0.5614
3712/4566 [=======================>......] - ETA: 1:36 - loss: 0.6839 - acc: 0.5625
3776/4566 [=======================>......] - ETA: 1:28 - loss: 0.6844 - acc: 0.5606
3840/4566 [========================>.....] - ETA: 1:21 - loss: 0.6849 - acc: 0.5599
3904/4566 [========================>.....] - ETA: 1:13 - loss: 0.6847 - acc: 0.5605
3968/4566 [=========================>....] - ETA: 1:06 - loss: 0.6842 - acc: 0.5607
4032/4566 [=========================>....] - ETA: 58s - loss: 0.6839 - acc: 0.5618 
4096/4566 [=========================>....] - ETA: 51s - loss: 0.6843 - acc: 0.5610
4160/4566 [==========================>...] - ETA: 44s - loss: 0.6837 - acc: 0.5618
4224/4566 [==========================>...] - ETA: 37s - loss: 0.6832 - acc: 0.5634
4288/4566 [===========================>..] - ETA: 30s - loss: 0.6835 - acc: 0.5630
4352/4566 [===========================>..] - ETA: 23s - loss: 0.6842 - acc: 0.5614
4416/4566 [============================>.] - ETA: 16s - loss: 0.6844 - acc: 0.5614
4480/4566 [============================>.] - ETA: 9s - loss: 0.6844 - acc: 0.5607 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6842 - acc: 0.5607
4566/4566 [==============================] - 550s 121ms/step - loss: 0.6840 - acc: 0.5609 - val_loss: 0.6820 - val_acc: 0.5453

Epoch 00008: val_acc did not improve from 0.58071
Epoch 9/10

  64/4566 [..............................] - ETA: 6:37 - loss: 0.6850 - acc: 0.5312
 128/4566 [..............................] - ETA: 6:35 - loss: 0.7041 - acc: 0.5234
 192/4566 [>.............................] - ETA: 6:05 - loss: 0.6970 - acc: 0.5365
 256/4566 [>.............................] - ETA: 5:48 - loss: 0.6952 - acc: 0.5469
 320/4566 [=>............................] - ETA: 5:35 - loss: 0.6977 - acc: 0.5375
 384/4566 [=>............................] - ETA: 5:27 - loss: 0.6894 - acc: 0.5547
 448/4566 [=>............................] - ETA: 5:21 - loss: 0.6889 - acc: 0.5580
 512/4566 [==>...........................] - ETA: 5:12 - loss: 0.6889 - acc: 0.5586
 576/4566 [==>...........................] - ETA: 5:05 - loss: 0.6872 - acc: 0.5642
 640/4566 [===>..........................] - ETA: 4:59 - loss: 0.6888 - acc: 0.5641
 704/4566 [===>..........................] - ETA: 4:53 - loss: 0.6859 - acc: 0.5625
 768/4566 [====>.........................] - ETA: 4:50 - loss: 0.6833 - acc: 0.5677
 832/4566 [====>.........................] - ETA: 4:44 - loss: 0.6848 - acc: 0.5649
 896/4566 [====>.........................] - ETA: 4:39 - loss: 0.6826 - acc: 0.5692
 960/4566 [=====>........................] - ETA: 4:34 - loss: 0.6813 - acc: 0.5740
1024/4566 [=====>........................] - ETA: 4:27 - loss: 0.6794 - acc: 0.5742
1088/4566 [======>.......................] - ETA: 4:35 - loss: 0.6803 - acc: 0.5726
1152/4566 [======>.......................] - ETA: 4:52 - loss: 0.6798 - acc: 0.5729
1216/4566 [======>.......................] - ETA: 5:06 - loss: 0.6793 - acc: 0.5732
1280/4566 [=======>......................] - ETA: 5:20 - loss: 0.6804 - acc: 0.5680
1344/4566 [=======>......................] - ETA: 5:30 - loss: 0.6807 - acc: 0.5677
1408/4566 [========>.....................] - ETA: 5:38 - loss: 0.6794 - acc: 0.5724
1472/4566 [========>.....................] - ETA: 5:39 - loss: 0.6785 - acc: 0.5727
1536/4566 [=========>....................] - ETA: 5:29 - loss: 0.6776 - acc: 0.5755
1600/4566 [=========>....................] - ETA: 5:20 - loss: 0.6786 - acc: 0.5725
1664/4566 [=========>....................] - ETA: 5:09 - loss: 0.6793 - acc: 0.5703
1728/4566 [==========>...................] - ETA: 4:59 - loss: 0.6800 - acc: 0.5683
1792/4566 [==========>...................] - ETA: 4:49 - loss: 0.6800 - acc: 0.5698
1856/4566 [===========>..................] - ETA: 4:39 - loss: 0.6808 - acc: 0.5663
1920/4566 [===========>..................] - ETA: 4:31 - loss: 0.6798 - acc: 0.5677
1984/4566 [============>.................] - ETA: 4:22 - loss: 0.6799 - acc: 0.5675
2048/4566 [============>.................] - ETA: 4:13 - loss: 0.6793 - acc: 0.5693
2112/4566 [============>.................] - ETA: 4:05 - loss: 0.6786 - acc: 0.5710
2176/4566 [=============>................] - ETA: 3:56 - loss: 0.6780 - acc: 0.5726
2240/4566 [=============>................] - ETA: 3:48 - loss: 0.6777 - acc: 0.5746
2304/4566 [==============>...............] - ETA: 3:41 - loss: 0.6775 - acc: 0.5751
2368/4566 [==============>...............] - ETA: 3:33 - loss: 0.6762 - acc: 0.5777
2432/4566 [==============>...............] - ETA: 3:26 - loss: 0.6768 - acc: 0.5752
2496/4566 [===============>..............] - ETA: 3:19 - loss: 0.6771 - acc: 0.5753
2560/4566 [===============>..............] - ETA: 3:15 - loss: 0.6771 - acc: 0.5746
2624/4566 [================>.............] - ETA: 3:14 - loss: 0.6766 - acc: 0.5762
2688/4566 [================>.............] - ETA: 3:12 - loss: 0.6768 - acc: 0.5755
2752/4566 [=================>............] - ETA: 3:11 - loss: 0.6772 - acc: 0.5759
2816/4566 [=================>............] - ETA: 3:09 - loss: 0.6774 - acc: 0.5749
2880/4566 [=================>............] - ETA: 3:07 - loss: 0.6780 - acc: 0.5729
2944/4566 [==================>...........] - ETA: 3:03 - loss: 0.6785 - acc: 0.5720
3008/4566 [==================>...........] - ETA: 2:55 - loss: 0.6779 - acc: 0.5725
3072/4566 [===================>..........] - ETA: 2:47 - loss: 0.6779 - acc: 0.5739
3136/4566 [===================>..........] - ETA: 2:39 - loss: 0.6777 - acc: 0.5743
3200/4566 [====================>.........] - ETA: 2:32 - loss: 0.6787 - acc: 0.5722
3264/4566 [====================>.........] - ETA: 2:23 - loss: 0.6793 - acc: 0.5705
3328/4566 [====================>.........] - ETA: 2:16 - loss: 0.6801 - acc: 0.5685
3392/4566 [=====================>........] - ETA: 2:08 - loss: 0.6798 - acc: 0.5693
3456/4566 [=====================>........] - ETA: 2:00 - loss: 0.6805 - acc: 0.5671
3520/4566 [======================>.......] - ETA: 1:52 - loss: 0.6801 - acc: 0.5673
3584/4566 [======================>.......] - ETA: 1:45 - loss: 0.6807 - acc: 0.5656
3648/4566 [======================>.......] - ETA: 1:37 - loss: 0.6810 - acc: 0.5647
3712/4566 [=======================>......] - ETA: 1:30 - loss: 0.6811 - acc: 0.5647
3776/4566 [=======================>......] - ETA: 1:23 - loss: 0.6806 - acc: 0.5665
3840/4566 [========================>.....] - ETA: 1:16 - loss: 0.6805 - acc: 0.5667
3904/4566 [========================>.....] - ETA: 1:09 - loss: 0.6807 - acc: 0.5653
3968/4566 [=========================>....] - ETA: 1:02 - loss: 0.6809 - acc: 0.5643
4032/4566 [=========================>....] - ETA: 55s - loss: 0.6810 - acc: 0.5647 
4096/4566 [=========================>....] - ETA: 49s - loss: 0.6811 - acc: 0.5637
4160/4566 [==========================>...] - ETA: 43s - loss: 0.6810 - acc: 0.5639
4224/4566 [==========================>...] - ETA: 37s - loss: 0.6813 - acc: 0.5634
4288/4566 [===========================>..] - ETA: 30s - loss: 0.6814 - acc: 0.5630
4352/4566 [===========================>..] - ETA: 23s - loss: 0.6816 - acc: 0.5623
4416/4566 [============================>.] - ETA: 16s - loss: 0.6816 - acc: 0.5625
4480/4566 [============================>.] - ETA: 9s - loss: 0.6814 - acc: 0.5632 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6817 - acc: 0.5614
4566/4566 [==============================] - 524s 115ms/step - loss: 0.6814 - acc: 0.5624 - val_loss: 0.6729 - val_acc: 0.5846

Epoch 00009: val_acc improved from 0.58071 to 0.58465, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window05/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 10/10

  64/4566 [..............................] - ETA: 5:44 - loss: 0.6588 - acc: 0.6250
 128/4566 [..............................] - ETA: 5:50 - loss: 0.6612 - acc: 0.6094
 192/4566 [>.............................] - ETA: 5:42 - loss: 0.6656 - acc: 0.5885
 256/4566 [>.............................] - ETA: 5:33 - loss: 0.6711 - acc: 0.5820
 320/4566 [=>............................] - ETA: 5:33 - loss: 0.6717 - acc: 0.5875
 384/4566 [=>............................] - ETA: 5:34 - loss: 0.6709 - acc: 0.5859
 448/4566 [=>............................] - ETA: 5:28 - loss: 0.6730 - acc: 0.5692
 512/4566 [==>...........................] - ETA: 5:28 - loss: 0.6746 - acc: 0.5684
 576/4566 [==>...........................] - ETA: 5:19 - loss: 0.6731 - acc: 0.5712
 640/4566 [===>..........................] - ETA: 5:10 - loss: 0.6757 - acc: 0.5656
 704/4566 [===>..........................] - ETA: 5:01 - loss: 0.6776 - acc: 0.5625
 768/4566 [====>.........................] - ETA: 5:25 - loss: 0.6781 - acc: 0.5651
 832/4566 [====>.........................] - ETA: 5:55 - loss: 0.6773 - acc: 0.5685
 896/4566 [====>.........................] - ETA: 6:20 - loss: 0.6781 - acc: 0.5670
 960/4566 [=====>........................] - ETA: 6:39 - loss: 0.6793 - acc: 0.5656
1024/4566 [=====>........................] - ETA: 6:54 - loss: 0.6793 - acc: 0.5654
1088/4566 [======>.......................] - ETA: 7:05 - loss: 0.6798 - acc: 0.5643
1152/4566 [======>.......................] - ETA: 6:55 - loss: 0.6802 - acc: 0.5608
1216/4566 [======>.......................] - ETA: 6:41 - loss: 0.6791 - acc: 0.5617
1280/4566 [=======>......................] - ETA: 6:24 - loss: 0.6792 - acc: 0.5625
1344/4566 [=======>......................] - ETA: 6:10 - loss: 0.6788 - acc: 0.5662
1408/4566 [========>.....................] - ETA: 5:56 - loss: 0.6785 - acc: 0.5668
1472/4566 [========>.....................] - ETA: 5:42 - loss: 0.6791 - acc: 0.5673
1536/4566 [=========>....................] - ETA: 5:30 - loss: 0.6786 - acc: 0.5690
1600/4566 [=========>....................] - ETA: 5:19 - loss: 0.6788 - acc: 0.5713
1664/4566 [=========>....................] - ETA: 5:07 - loss: 0.6798 - acc: 0.5703
1728/4566 [==========>...................] - ETA: 4:57 - loss: 0.6794 - acc: 0.5694
1792/4566 [==========>...................] - ETA: 4:47 - loss: 0.6797 - acc: 0.5692
1856/4566 [===========>..................] - ETA: 4:38 - loss: 0.6797 - acc: 0.5679
1920/4566 [===========>..................] - ETA: 4:29 - loss: 0.6798 - acc: 0.5682
1984/4566 [============>.................] - ETA: 4:19 - loss: 0.6796 - acc: 0.5670
2048/4566 [============>.................] - ETA: 4:10 - loss: 0.6801 - acc: 0.5659
2112/4566 [============>.................] - ETA: 4:02 - loss: 0.6796 - acc: 0.5672
2176/4566 [=============>................] - ETA: 3:54 - loss: 0.6793 - acc: 0.5671
2240/4566 [=============>................] - ETA: 3:47 - loss: 0.6800 - acc: 0.5647
2304/4566 [==============>...............] - ETA: 3:46 - loss: 0.6801 - acc: 0.5638
2368/4566 [==============>...............] - ETA: 3:46 - loss: 0.6798 - acc: 0.5642
2432/4566 [==============>...............] - ETA: 3:45 - loss: 0.6797 - acc: 0.5650
2496/4566 [===============>..............] - ETA: 3:43 - loss: 0.6790 - acc: 0.5665
2560/4566 [===============>..............] - ETA: 3:41 - loss: 0.6796 - acc: 0.5648
2624/4566 [================>.............] - ETA: 3:38 - loss: 0.6789 - acc: 0.5659
2688/4566 [================>.............] - ETA: 3:31 - loss: 0.6796 - acc: 0.5636
2752/4566 [=================>............] - ETA: 3:22 - loss: 0.6801 - acc: 0.5625
2816/4566 [=================>............] - ETA: 3:13 - loss: 0.6794 - acc: 0.5661
2880/4566 [=================>............] - ETA: 3:05 - loss: 0.6791 - acc: 0.5663
2944/4566 [==================>...........] - ETA: 2:56 - loss: 0.6784 - acc: 0.5676
3008/4566 [==================>...........] - ETA: 2:48 - loss: 0.6785 - acc: 0.5672
3072/4566 [===================>..........] - ETA: 2:40 - loss: 0.6786 - acc: 0.5667
3136/4566 [===================>..........] - ETA: 2:32 - loss: 0.6789 - acc: 0.5670
3200/4566 [====================>.........] - ETA: 2:24 - loss: 0.6789 - acc: 0.5681
3264/4566 [====================>.........] - ETA: 2:17 - loss: 0.6797 - acc: 0.5656
3328/4566 [====================>.........] - ETA: 2:09 - loss: 0.6802 - acc: 0.5643
3392/4566 [=====================>........] - ETA: 2:02 - loss: 0.6802 - acc: 0.5634
3456/4566 [=====================>........] - ETA: 1:54 - loss: 0.6802 - acc: 0.5648
3520/4566 [======================>.......] - ETA: 1:47 - loss: 0.6798 - acc: 0.5651
3584/4566 [======================>.......] - ETA: 1:40 - loss: 0.6798 - acc: 0.5639
3648/4566 [======================>.......] - ETA: 1:33 - loss: 0.6794 - acc: 0.5641
3712/4566 [=======================>......] - ETA: 1:26 - loss: 0.6794 - acc: 0.5641
3776/4566 [=======================>......] - ETA: 1:19 - loss: 0.6799 - acc: 0.5622
3840/4566 [========================>.....] - ETA: 1:14 - loss: 0.6798 - acc: 0.5628
3904/4566 [========================>.....] - ETA: 1:08 - loss: 0.6793 - acc: 0.5635
3968/4566 [=========================>....] - ETA: 1:03 - loss: 0.6793 - acc: 0.5620
4032/4566 [=========================>....] - ETA: 57s - loss: 0.6790 - acc: 0.5632 
4096/4566 [=========================>....] - ETA: 51s - loss: 0.6795 - acc: 0.5620
4160/4566 [==========================>...] - ETA: 44s - loss: 0.6796 - acc: 0.5625
4224/4566 [==========================>...] - ETA: 37s - loss: 0.6795 - acc: 0.5630
4288/4566 [===========================>..] - ETA: 30s - loss: 0.6792 - acc: 0.5623
4352/4566 [===========================>..] - ETA: 23s - loss: 0.6794 - acc: 0.5625
4416/4566 [============================>.] - ETA: 16s - loss: 0.6790 - acc: 0.5630
4480/4566 [============================>.] - ETA: 9s - loss: 0.6795 - acc: 0.5623 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6793 - acc: 0.5632
4566/4566 [==============================] - 496s 109ms/step - loss: 0.6795 - acc: 0.5629 - val_loss: 0.6775 - val_acc: 0.5709

Epoch 00010: val_acc did not improve from 0.58465
Saved model to disk
