/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f074ebdf1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f074ebdf1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f0756fb5690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f0756fb5690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f075f9e7a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f075f9e7a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06f0dafb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06f0dafb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06f0d0a050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06f0d0a050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f074ecd3490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f074ecd3490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06f0dafa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06f0dafa90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f074ec5ab90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f074ec5ab90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06f0b01190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06f0b01190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06f0cdfcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06f0cdfcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f074ecfced0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f074ecfced0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f074ec76090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f074ec76090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06f09f4850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06f09f4850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06f09f29d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06f09f29d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06f09eb890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06f09eb890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06f0af1f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06f0af1f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06f080cf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06f080cf50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06f0728490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06f0728490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06f09eb110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06f09eb110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06d83b5950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06d83b5950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06d8696fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06d8696fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06f071f8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06f071f8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f074ec47650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f074ec47650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06d839d910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06d839d910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06d8082ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06d8082ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06f09e3d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06f09e3d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06d83b1e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06d83b1e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06d8163910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06d8163910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06d7f82b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06d7f82b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06cfd71c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06cfd71c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06cfea3490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06cfea3490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06cfd67290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06cfd67290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06d8264c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06d8264c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06cfbc54d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06cfbc54d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06cfb158d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06cfb158d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06cfc8c810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06cfc8c810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06cfd5cfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06cfd5cfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06f0b01b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06f0b01b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06cf83f7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06cf83f7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06cf7aa490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06cf7aa490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06cfae92d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06cfae92d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06cf83f9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06cf83f9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c75db7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c75db7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06c7620d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06c7620d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06c7446f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06c7446f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06cf7aa7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06cf7aa7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06cf7bd190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06cf7bd190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06cf841f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06cf841f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06c7246f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06c7246f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06c70e7850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06c70e7850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c70dc710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c70dc710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06c7222a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06c7222a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c6fb1090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c6fb1090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06c7173b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06c7173b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06c6f067d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06c6f067d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06d8420090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06d8420090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06c6f2d4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06c6f2d4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c70ea190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c70ea190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06c6c37250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f06c6c37250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06c6c77fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f06c6c77fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c6bbb310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c6bbb310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06c6c37b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f06c6c37b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c6b2c310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f06c6b2c310>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-17 11:02:20.140480: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-17 11:02:20.207886: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-17 11:02:20.292178: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5639dd094f00 executing computations on platform Host. Devices:
2022-11-17 11:02:20.292312: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-17 11:02:21.046297: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window08.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 15:51 - loss: 0.7320 - acc: 0.5469
 128/4566 [..............................] - ETA: 12:12 - loss: 0.7206 - acc: 0.5078
 192/4566 [>.............................] - ETA: 10:46 - loss: 0.7752 - acc: 0.4740
 256/4566 [>.............................] - ETA: 10:08 - loss: 0.7756 - acc: 0.4727
 320/4566 [=>............................] - ETA: 9:26 - loss: 0.7720 - acc: 0.4813 
 384/4566 [=>............................] - ETA: 8:52 - loss: 0.7699 - acc: 0.4792
 448/4566 [=>............................] - ETA: 8:33 - loss: 0.7686 - acc: 0.4777
 512/4566 [==>...........................] - ETA: 8:14 - loss: 0.7566 - acc: 0.4863
 576/4566 [==>...........................] - ETA: 8:00 - loss: 0.7574 - acc: 0.4809
 640/4566 [===>..........................] - ETA: 7:48 - loss: 0.7601 - acc: 0.4813
 704/4566 [===>..........................] - ETA: 7:41 - loss: 0.7562 - acc: 0.4801
 768/4566 [====>.........................] - ETA: 8:08 - loss: 0.7558 - acc: 0.4805
 832/4566 [====>.........................] - ETA: 8:43 - loss: 0.7508 - acc: 0.4820
 896/4566 [====>.........................] - ETA: 9:00 - loss: 0.7486 - acc: 0.4844
 960/4566 [=====>........................] - ETA: 9:07 - loss: 0.7449 - acc: 0.4833
1024/4566 [=====>........................] - ETA: 9:14 - loss: 0.7451 - acc: 0.4834
1088/4566 [======>.......................] - ETA: 9:05 - loss: 0.7400 - acc: 0.4917
1152/4566 [======>.......................] - ETA: 8:46 - loss: 0.7399 - acc: 0.4922
1216/4566 [======>.......................] - ETA: 8:30 - loss: 0.7417 - acc: 0.4901
1280/4566 [=======>......................] - ETA: 8:12 - loss: 0.7393 - acc: 0.4930
1344/4566 [=======>......................] - ETA: 7:54 - loss: 0.7401 - acc: 0.4955
1408/4566 [========>.....................] - ETA: 7:38 - loss: 0.7373 - acc: 0.4972
1472/4566 [========>.....................] - ETA: 7:21 - loss: 0.7378 - acc: 0.4980
1536/4566 [=========>....................] - ETA: 7:06 - loss: 0.7374 - acc: 0.4974
1600/4566 [=========>....................] - ETA: 6:52 - loss: 0.7370 - acc: 0.4988
1664/4566 [=========>....................] - ETA: 6:39 - loss: 0.7363 - acc: 0.5000
1728/4566 [==========>...................] - ETA: 6:27 - loss: 0.7348 - acc: 0.5012
1792/4566 [==========>...................] - ETA: 6:14 - loss: 0.7349 - acc: 0.5006
1856/4566 [===========>..................] - ETA: 6:02 - loss: 0.7344 - acc: 0.4973
1920/4566 [===========>..................] - ETA: 5:50 - loss: 0.7338 - acc: 0.4990
1984/4566 [============>.................] - ETA: 5:39 - loss: 0.7323 - acc: 0.5020
2048/4566 [============>.................] - ETA: 5:39 - loss: 0.7330 - acc: 0.5015
2112/4566 [============>.................] - ETA: 5:38 - loss: 0.7321 - acc: 0.5038
2176/4566 [=============>................] - ETA: 5:36 - loss: 0.7321 - acc: 0.5041
2240/4566 [=============>................] - ETA: 5:32 - loss: 0.7332 - acc: 0.5045
2304/4566 [==============>...............] - ETA: 5:28 - loss: 0.7305 - acc: 0.5087
2368/4566 [==============>...............] - ETA: 5:22 - loss: 0.7282 - acc: 0.5114
2432/4566 [==============>...............] - ETA: 5:10 - loss: 0.7276 - acc: 0.5099
2496/4566 [===============>..............] - ETA: 4:58 - loss: 0.7258 - acc: 0.5128
2560/4566 [===============>..............] - ETA: 4:47 - loss: 0.7251 - acc: 0.5129
2624/4566 [================>.............] - ETA: 4:35 - loss: 0.7244 - acc: 0.5133
2688/4566 [================>.............] - ETA: 4:24 - loss: 0.7237 - acc: 0.5138
2752/4566 [=================>............] - ETA: 4:13 - loss: 0.7254 - acc: 0.5116
2816/4566 [=================>............] - ETA: 4:02 - loss: 0.7250 - acc: 0.5114
2880/4566 [=================>............] - ETA: 3:51 - loss: 0.7249 - acc: 0.5111
2944/4566 [==================>...........] - ETA: 3:41 - loss: 0.7256 - acc: 0.5095
3008/4566 [==================>...........] - ETA: 3:31 - loss: 0.7254 - acc: 0.5096
3072/4566 [===================>..........] - ETA: 3:21 - loss: 0.7245 - acc: 0.5107
3136/4566 [===================>..........] - ETA: 3:11 - loss: 0.7243 - acc: 0.5105
3200/4566 [====================>.........] - ETA: 3:02 - loss: 0.7238 - acc: 0.5116
3264/4566 [====================>.........] - ETA: 2:52 - loss: 0.7236 - acc: 0.5123
3328/4566 [====================>.........] - ETA: 2:43 - loss: 0.7231 - acc: 0.5135
3392/4566 [=====================>........] - ETA: 2:37 - loss: 0.7233 - acc: 0.5118
3456/4566 [=====================>........] - ETA: 2:31 - loss: 0.7231 - acc: 0.5113
3520/4566 [======================>.......] - ETA: 2:24 - loss: 0.7224 - acc: 0.5119
3584/4566 [======================>.......] - ETA: 2:16 - loss: 0.7220 - acc: 0.5109
3648/4566 [======================>.......] - ETA: 2:08 - loss: 0.7221 - acc: 0.5096
3712/4566 [=======================>......] - ETA: 2:00 - loss: 0.7223 - acc: 0.5094
3776/4566 [=======================>......] - ETA: 1:50 - loss: 0.7229 - acc: 0.5072
3840/4566 [========================>.....] - ETA: 1:40 - loss: 0.7222 - acc: 0.5083
3904/4566 [========================>.....] - ETA: 1:31 - loss: 0.7220 - acc: 0.5085
3968/4566 [=========================>....] - ETA: 1:22 - loss: 0.7214 - acc: 0.5098
4032/4566 [=========================>....] - ETA: 1:12 - loss: 0.7213 - acc: 0.5099
4096/4566 [=========================>....] - ETA: 1:03 - loss: 0.7211 - acc: 0.5098
4160/4566 [==========================>...] - ETA: 54s - loss: 0.7209 - acc: 0.5099 
4224/4566 [==========================>...] - ETA: 45s - loss: 0.7205 - acc: 0.5116
4288/4566 [===========================>..] - ETA: 37s - loss: 0.7196 - acc: 0.5124
4352/4566 [===========================>..] - ETA: 28s - loss: 0.7194 - acc: 0.5124
4416/4566 [============================>.] - ETA: 19s - loss: 0.7191 - acc: 0.5136
4480/4566 [============================>.] - ETA: 11s - loss: 0.7195 - acc: 0.5136
4544/4566 [============================>.] - ETA: 2s - loss: 0.7202 - acc: 0.5130 
4566/4566 [==============================] - 627s 137ms/step - loss: 0.7202 - acc: 0.5134 - val_loss: 0.6929 - val_acc: 0.5472

Epoch 00001: val_acc improved from -inf to 0.54724, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window08/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 17:57 - loss: 0.6925 - acc: 0.5625
 128/4566 [..............................] - ETA: 16:58 - loss: 0.7002 - acc: 0.5547
 192/4566 [>.............................] - ETA: 16:17 - loss: 0.6933 - acc: 0.5625
 256/4566 [>.............................] - ETA: 15:57 - loss: 0.6922 - acc: 0.5664
 320/4566 [=>............................] - ETA: 14:39 - loss: 0.6876 - acc: 0.5750
 384/4566 [=>............................] - ETA: 13:05 - loss: 0.6930 - acc: 0.5677
 448/4566 [=>............................] - ETA: 12:00 - loss: 0.6864 - acc: 0.5781
 512/4566 [==>...........................] - ETA: 11:05 - loss: 0.6881 - acc: 0.5742
 576/4566 [==>...........................] - ETA: 10:21 - loss: 0.6877 - acc: 0.5747
 640/4566 [===>..........................] - ETA: 9:45 - loss: 0.6856 - acc: 0.5734 
 704/4566 [===>..........................] - ETA: 9:11 - loss: 0.6840 - acc: 0.5753
 768/4566 [====>.........................] - ETA: 8:42 - loss: 0.6860 - acc: 0.5716
 832/4566 [====>.........................] - ETA: 8:17 - loss: 0.6878 - acc: 0.5625
 896/4566 [====>.........................] - ETA: 7:55 - loss: 0.6869 - acc: 0.5625
 960/4566 [=====>........................] - ETA: 7:36 - loss: 0.6869 - acc: 0.5625
1024/4566 [=====>........................] - ETA: 7:18 - loss: 0.6889 - acc: 0.5615
1088/4566 [======>.......................] - ETA: 7:03 - loss: 0.6896 - acc: 0.5561
1152/4566 [======>.......................] - ETA: 6:49 - loss: 0.6885 - acc: 0.5582
1216/4566 [======>.......................] - ETA: 6:36 - loss: 0.6904 - acc: 0.5559
1280/4566 [=======>......................] - ETA: 6:23 - loss: 0.6911 - acc: 0.5531
1344/4566 [=======>......................] - ETA: 6:19 - loss: 0.6908 - acc: 0.5536
1408/4566 [========>.....................] - ETA: 6:26 - loss: 0.6932 - acc: 0.5518
1472/4566 [========>.....................] - ETA: 6:31 - loss: 0.6965 - acc: 0.5462
1536/4566 [=========>....................] - ETA: 6:34 - loss: 0.6966 - acc: 0.5469
1600/4566 [=========>....................] - ETA: 6:36 - loss: 0.6969 - acc: 0.5463
1664/4566 [=========>....................] - ETA: 6:36 - loss: 0.6969 - acc: 0.5451
1728/4566 [==========>...................] - ETA: 6:32 - loss: 0.6954 - acc: 0.5480
1792/4566 [==========>...................] - ETA: 6:20 - loss: 0.6957 - acc: 0.5469
1856/4566 [===========>..................] - ETA: 6:06 - loss: 0.6956 - acc: 0.5469
1920/4566 [===========>..................] - ETA: 5:54 - loss: 0.6961 - acc: 0.5443
1984/4566 [============>.................] - ETA: 5:41 - loss: 0.6967 - acc: 0.5444
2048/4566 [============>.................] - ETA: 5:30 - loss: 0.6972 - acc: 0.5420
2112/4566 [============>.................] - ETA: 5:17 - loss: 0.6968 - acc: 0.5431
2176/4566 [=============>................] - ETA: 5:06 - loss: 0.6975 - acc: 0.5414
2240/4566 [=============>................] - ETA: 4:55 - loss: 0.6965 - acc: 0.5429
2304/4566 [==============>...............] - ETA: 4:45 - loss: 0.6958 - acc: 0.5438
2368/4566 [==============>...............] - ETA: 4:34 - loss: 0.6948 - acc: 0.5456
2432/4566 [==============>...............] - ETA: 4:25 - loss: 0.6952 - acc: 0.5444
2496/4566 [===============>..............] - ETA: 4:15 - loss: 0.6951 - acc: 0.5465
2560/4566 [===============>..............] - ETA: 4:04 - loss: 0.6946 - acc: 0.5469
2624/4566 [================>.............] - ETA: 3:55 - loss: 0.6945 - acc: 0.5465
2688/4566 [================>.............] - ETA: 3:46 - loss: 0.6935 - acc: 0.5472
2752/4566 [=================>............] - ETA: 3:38 - loss: 0.6944 - acc: 0.5458
2816/4566 [=================>............] - ETA: 3:35 - loss: 0.6954 - acc: 0.5437
2880/4566 [=================>............] - ETA: 3:31 - loss: 0.6957 - acc: 0.5434
2944/4566 [==================>...........] - ETA: 3:26 - loss: 0.6961 - acc: 0.5418
3008/4566 [==================>...........] - ETA: 3:20 - loss: 0.6950 - acc: 0.5436
3072/4566 [===================>..........] - ETA: 3:14 - loss: 0.6955 - acc: 0.5433
3136/4566 [===================>..........] - ETA: 3:08 - loss: 0.6952 - acc: 0.5440
3200/4566 [====================>.........] - ETA: 2:58 - loss: 0.6949 - acc: 0.5453
3264/4566 [====================>.........] - ETA: 2:48 - loss: 0.6952 - acc: 0.5438
3328/4566 [====================>.........] - ETA: 2:39 - loss: 0.6948 - acc: 0.5448
3392/4566 [=====================>........] - ETA: 2:30 - loss: 0.6952 - acc: 0.5445
3456/4566 [=====================>........] - ETA: 2:21 - loss: 0.6954 - acc: 0.5451
3520/4566 [======================>.......] - ETA: 2:12 - loss: 0.6953 - acc: 0.5432
3584/4566 [======================>.......] - ETA: 2:03 - loss: 0.6944 - acc: 0.5446
3648/4566 [======================>.......] - ETA: 1:54 - loss: 0.6944 - acc: 0.5433
3712/4566 [=======================>......] - ETA: 1:46 - loss: 0.6937 - acc: 0.5447
3776/4566 [=======================>......] - ETA: 1:37 - loss: 0.6938 - acc: 0.5437
3840/4566 [========================>.....] - ETA: 1:29 - loss: 0.6942 - acc: 0.5430
3904/4566 [========================>.....] - ETA: 1:20 - loss: 0.6934 - acc: 0.5446
3968/4566 [=========================>....] - ETA: 1:12 - loss: 0.6941 - acc: 0.5428
4032/4566 [=========================>....] - ETA: 1:04 - loss: 0.6934 - acc: 0.5446
4096/4566 [=========================>....] - ETA: 56s - loss: 0.6925 - acc: 0.5474 
4160/4566 [==========================>...] - ETA: 48s - loss: 0.6925 - acc: 0.5464
4224/4566 [==========================>...] - ETA: 41s - loss: 0.6924 - acc: 0.5464
4288/4566 [===========================>..] - ETA: 34s - loss: 0.6924 - acc: 0.5466
4352/4566 [===========================>..] - ETA: 26s - loss: 0.6923 - acc: 0.5473
4416/4566 [============================>.] - ETA: 18s - loss: 0.6925 - acc: 0.5478
4480/4566 [============================>.] - ETA: 10s - loss: 0.6931 - acc: 0.5460
4544/4566 [============================>.] - ETA: 2s - loss: 0.6935 - acc: 0.5453 
4566/4566 [==============================] - 601s 132ms/step - loss: 0.6933 - acc: 0.5460 - val_loss: 0.6747 - val_acc: 0.5669

Epoch 00002: val_acc improved from 0.54724 to 0.56693, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window08/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 3/10

  64/4566 [..............................] - ETA: 6:03 - loss: 0.6942 - acc: 0.5469
 128/4566 [..............................] - ETA: 5:43 - loss: 0.6819 - acc: 0.5234
 192/4566 [>.............................] - ETA: 5:52 - loss: 0.6887 - acc: 0.5312
 256/4566 [>.............................] - ETA: 5:47 - loss: 0.6819 - acc: 0.5469
 320/4566 [=>............................] - ETA: 5:44 - loss: 0.6805 - acc: 0.5437
 384/4566 [=>............................] - ETA: 5:47 - loss: 0.6725 - acc: 0.5625
 448/4566 [=>............................] - ETA: 5:39 - loss: 0.6751 - acc: 0.5536
 512/4566 [==>...........................] - ETA: 5:32 - loss: 0.6776 - acc: 0.5527
 576/4566 [==>...........................] - ETA: 5:28 - loss: 0.6792 - acc: 0.5538
 640/4566 [===>..........................] - ETA: 5:23 - loss: 0.6835 - acc: 0.5469
 704/4566 [===>..........................] - ETA: 5:20 - loss: 0.6864 - acc: 0.5426
 768/4566 [====>.........................] - ETA: 5:14 - loss: 0.6844 - acc: 0.5456
 832/4566 [====>.........................] - ETA: 5:08 - loss: 0.6835 - acc: 0.5457
 896/4566 [====>.........................] - ETA: 5:15 - loss: 0.6812 - acc: 0.5513
 960/4566 [=====>........................] - ETA: 5:39 - loss: 0.6831 - acc: 0.5479
1024/4566 [=====>........................] - ETA: 5:56 - loss: 0.6810 - acc: 0.5518
1088/4566 [======>.......................] - ETA: 6:11 - loss: 0.6807 - acc: 0.5542
1152/4566 [======>.......................] - ETA: 6:23 - loss: 0.6833 - acc: 0.5503
1216/4566 [======>.......................] - ETA: 6:33 - loss: 0.6849 - acc: 0.5485
1280/4566 [=======>......................] - ETA: 6:40 - loss: 0.6858 - acc: 0.5492
1344/4566 [=======>......................] - ETA: 6:30 - loss: 0.6857 - acc: 0.5506
1408/4566 [========>.....................] - ETA: 6:18 - loss: 0.6857 - acc: 0.5526
1472/4566 [========>.....................] - ETA: 6:05 - loss: 0.6860 - acc: 0.5537
1536/4566 [=========>....................] - ETA: 5:53 - loss: 0.6867 - acc: 0.5534
1600/4566 [=========>....................] - ETA: 5:42 - loss: 0.6859 - acc: 0.5556
1664/4566 [=========>....................] - ETA: 5:31 - loss: 0.6839 - acc: 0.5583
1728/4566 [==========>...................] - ETA: 5:21 - loss: 0.6836 - acc: 0.5561
1792/4566 [==========>...................] - ETA: 5:11 - loss: 0.6835 - acc: 0.5547
1856/4566 [===========>..................] - ETA: 5:01 - loss: 0.6845 - acc: 0.5528
1920/4566 [===========>..................] - ETA: 4:51 - loss: 0.6826 - acc: 0.5573
1984/4566 [============>.................] - ETA: 4:42 - loss: 0.6823 - acc: 0.5580
2048/4566 [============>.................] - ETA: 4:33 - loss: 0.6806 - acc: 0.5620
2112/4566 [============>.................] - ETA: 4:24 - loss: 0.6811 - acc: 0.5625
2176/4566 [=============>................] - ETA: 4:16 - loss: 0.6814 - acc: 0.5611
2240/4566 [=============>................] - ETA: 4:07 - loss: 0.6834 - acc: 0.5607
2304/4566 [==============>...............] - ETA: 3:59 - loss: 0.6835 - acc: 0.5612
2368/4566 [==============>...............] - ETA: 3:53 - loss: 0.6864 - acc: 0.5570
2432/4566 [==============>...............] - ETA: 3:52 - loss: 0.6870 - acc: 0.5555
2496/4566 [===============>..............] - ETA: 3:50 - loss: 0.6873 - acc: 0.5557
2560/4566 [===============>..............] - ETA: 3:48 - loss: 0.6865 - acc: 0.5570
2624/4566 [================>.............] - ETA: 3:44 - loss: 0.6863 - acc: 0.5583
2688/4566 [================>.............] - ETA: 3:41 - loss: 0.6860 - acc: 0.5588
2752/4566 [=================>............] - ETA: 3:36 - loss: 0.6875 - acc: 0.5585
2816/4566 [=================>............] - ETA: 3:28 - loss: 0.6878 - acc: 0.5579
2880/4566 [=================>............] - ETA: 3:19 - loss: 0.6881 - acc: 0.5559
2944/4566 [==================>...........] - ETA: 3:10 - loss: 0.6884 - acc: 0.5550
3008/4566 [==================>...........] - ETA: 3:02 - loss: 0.6877 - acc: 0.5575
3072/4566 [===================>..........] - ETA: 2:54 - loss: 0.6875 - acc: 0.5576
3136/4566 [===================>..........] - ETA: 2:45 - loss: 0.6880 - acc: 0.5574
3200/4566 [====================>.........] - ETA: 2:37 - loss: 0.6875 - acc: 0.5572
3264/4566 [====================>.........] - ETA: 2:29 - loss: 0.6873 - acc: 0.5573
3328/4566 [====================>.........] - ETA: 2:20 - loss: 0.6870 - acc: 0.5589
3392/4566 [=====================>........] - ETA: 2:12 - loss: 0.6870 - acc: 0.5578
3456/4566 [=====================>........] - ETA: 2:05 - loss: 0.6860 - acc: 0.5584
3520/4566 [======================>.......] - ETA: 1:57 - loss: 0.6870 - acc: 0.5568
3584/4566 [======================>.......] - ETA: 1:49 - loss: 0.6880 - acc: 0.5555
3648/4566 [======================>.......] - ETA: 1:41 - loss: 0.6875 - acc: 0.5567
3712/4566 [=======================>......] - ETA: 1:34 - loss: 0.6870 - acc: 0.5563
3776/4566 [=======================>......] - ETA: 1:27 - loss: 0.6869 - acc: 0.5564
3840/4566 [========================>.....] - ETA: 1:21 - loss: 0.6870 - acc: 0.5563
3904/4566 [========================>.....] - ETA: 1:14 - loss: 0.6878 - acc: 0.5553
3968/4566 [=========================>....] - ETA: 1:08 - loss: 0.6881 - acc: 0.5537
4032/4566 [=========================>....] - ETA: 1:01 - loss: 0.6889 - acc: 0.5523
4096/4566 [=========================>....] - ETA: 55s - loss: 0.6899 - acc: 0.5508 
4160/4566 [==========================>...] - ETA: 48s - loss: 0.6900 - acc: 0.5510
4224/4566 [==========================>...] - ETA: 40s - loss: 0.6902 - acc: 0.5511
4288/4566 [===========================>..] - ETA: 33s - loss: 0.6901 - acc: 0.5511
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6907 - acc: 0.5499
4416/4566 [============================>.] - ETA: 17s - loss: 0.6910 - acc: 0.5491
4480/4566 [============================>.] - ETA: 10s - loss: 0.6909 - acc: 0.5493
4544/4566 [============================>.] - ETA: 2s - loss: 0.6912 - acc: 0.5493 
4566/4566 [==============================] - 548s 120ms/step - loss: 0.6913 - acc: 0.5495 - val_loss: 0.6753 - val_acc: 0.5610

Epoch 00003: val_acc did not improve from 0.56693
Epoch 4/10

  64/4566 [..............................] - ETA: 5:37 - loss: 0.6726 - acc: 0.5625
 128/4566 [..............................] - ETA: 5:25 - loss: 0.6780 - acc: 0.5312
 192/4566 [>.............................] - ETA: 5:36 - loss: 0.6810 - acc: 0.5208
 256/4566 [>.............................] - ETA: 5:35 - loss: 0.6812 - acc: 0.5273
 320/4566 [=>............................] - ETA: 5:33 - loss: 0.6791 - acc: 0.5375
 384/4566 [=>............................] - ETA: 5:31 - loss: 0.6844 - acc: 0.5234
 448/4566 [=>............................] - ETA: 5:26 - loss: 0.6849 - acc: 0.5312
 512/4566 [==>...........................] - ETA: 5:24 - loss: 0.6825 - acc: 0.5391
 576/4566 [==>...........................] - ETA: 5:45 - loss: 0.6812 - acc: 0.5451
 640/4566 [===>..........................] - ETA: 6:20 - loss: 0.6804 - acc: 0.5531
 704/4566 [===>..........................] - ETA: 6:52 - loss: 0.6786 - acc: 0.5511
 768/4566 [====>.........................] - ETA: 7:12 - loss: 0.6809 - acc: 0.5469
 832/4566 [====>.........................] - ETA: 7:27 - loss: 0.6797 - acc: 0.5493
 896/4566 [====>.........................] - ETA: 7:39 - loss: 0.6771 - acc: 0.5547
 960/4566 [=====>........................] - ETA: 7:47 - loss: 0.6770 - acc: 0.5552
1024/4566 [=====>........................] - ETA: 7:36 - loss: 0.6810 - acc: 0.5459
1088/4566 [======>.......................] - ETA: 7:18 - loss: 0.6816 - acc: 0.5450
1152/4566 [======>.......................] - ETA: 7:03 - loss: 0.6791 - acc: 0.5503
1216/4566 [======>.......................] - ETA: 6:47 - loss: 0.6797 - acc: 0.5502
1280/4566 [=======>......................] - ETA: 6:32 - loss: 0.6778 - acc: 0.5570
1344/4566 [=======>......................] - ETA: 6:19 - loss: 0.6783 - acc: 0.5565
1408/4566 [========>.....................] - ETA: 6:05 - loss: 0.6791 - acc: 0.5575
1472/4566 [========>.....................] - ETA: 5:54 - loss: 0.6796 - acc: 0.5557
1536/4566 [=========>....................] - ETA: 5:41 - loss: 0.6809 - acc: 0.5560
1600/4566 [=========>....................] - ETA: 5:31 - loss: 0.6804 - acc: 0.5575
1664/4566 [=========>....................] - ETA: 5:21 - loss: 0.6805 - acc: 0.5577
1728/4566 [==========>...................] - ETA: 5:11 - loss: 0.6800 - acc: 0.5590
1792/4566 [==========>...................] - ETA: 5:01 - loss: 0.6796 - acc: 0.5631
1856/4566 [===========>..................] - ETA: 4:51 - loss: 0.6802 - acc: 0.5614
1920/4566 [===========>..................] - ETA: 4:42 - loss: 0.6811 - acc: 0.5599
1984/4566 [============>.................] - ETA: 4:33 - loss: 0.6822 - acc: 0.5565
2048/4566 [============>.................] - ETA: 4:24 - loss: 0.6819 - acc: 0.5586
2112/4566 [============>.................] - ETA: 4:19 - loss: 0.6816 - acc: 0.5587
2176/4566 [=============>................] - ETA: 4:20 - loss: 0.6825 - acc: 0.5565
2240/4566 [=============>................] - ETA: 4:20 - loss: 0.6826 - acc: 0.5580
2304/4566 [==============>...............] - ETA: 4:20 - loss: 0.6828 - acc: 0.5577
2368/4566 [==============>...............] - ETA: 4:17 - loss: 0.6831 - acc: 0.5570
2432/4566 [==============>...............] - ETA: 4:14 - loss: 0.6832 - acc: 0.5572
2496/4566 [===============>..............] - ETA: 4:10 - loss: 0.6837 - acc: 0.5569
2560/4566 [===============>..............] - ETA: 4:00 - loss: 0.6850 - acc: 0.5551
2624/4566 [================>.............] - ETA: 3:51 - loss: 0.6856 - acc: 0.5526
2688/4566 [================>.............] - ETA: 3:41 - loss: 0.6854 - acc: 0.5513
2752/4566 [=================>............] - ETA: 3:32 - loss: 0.6844 - acc: 0.5534
2816/4566 [=================>............] - ETA: 3:24 - loss: 0.6835 - acc: 0.5558
2880/4566 [=================>............] - ETA: 3:15 - loss: 0.6839 - acc: 0.5556
2944/4566 [==================>...........] - ETA: 3:06 - loss: 0.6842 - acc: 0.5547
3008/4566 [==================>...........] - ETA: 2:57 - loss: 0.6847 - acc: 0.5535
3072/4566 [===================>..........] - ETA: 2:49 - loss: 0.6844 - acc: 0.5544
3136/4566 [===================>..........] - ETA: 2:41 - loss: 0.6846 - acc: 0.5536
3200/4566 [====================>.........] - ETA: 2:32 - loss: 0.6852 - acc: 0.5531
3264/4566 [====================>.........] - ETA: 2:24 - loss: 0.6845 - acc: 0.5548
3328/4566 [====================>.........] - ETA: 2:17 - loss: 0.6844 - acc: 0.5541
3392/4566 [=====================>........] - ETA: 2:09 - loss: 0.6844 - acc: 0.5534
3456/4566 [=====================>........] - ETA: 2:01 - loss: 0.6846 - acc: 0.5527
3520/4566 [======================>.......] - ETA: 1:54 - loss: 0.6846 - acc: 0.5517
3584/4566 [======================>.......] - ETA: 1:46 - loss: 0.6846 - acc: 0.5516
3648/4566 [======================>.......] - ETA: 1:39 - loss: 0.6846 - acc: 0.5521
3712/4566 [=======================>......] - ETA: 1:34 - loss: 0.6851 - acc: 0.5498
3776/4566 [=======================>......] - ETA: 1:28 - loss: 0.6853 - acc: 0.5479
3840/4566 [========================>.....] - ETA: 1:22 - loss: 0.6848 - acc: 0.5495
3904/4566 [========================>.....] - ETA: 1:16 - loss: 0.6849 - acc: 0.5499
3968/4566 [=========================>....] - ETA: 1:09 - loss: 0.6842 - acc: 0.5507
4032/4566 [=========================>....] - ETA: 1:02 - loss: 0.6843 - acc: 0.5508
4096/4566 [=========================>....] - ETA: 55s - loss: 0.6849 - acc: 0.5500 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6849 - acc: 0.5493
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6853 - acc: 0.5476
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6858 - acc: 0.5462
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6861 - acc: 0.5466
4416/4566 [============================>.] - ETA: 17s - loss: 0.6854 - acc: 0.5480
4480/4566 [============================>.] - ETA: 9s - loss: 0.6860 - acc: 0.5469 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6863 - acc: 0.5471
4566/4566 [==============================] - 533s 117ms/step - loss: 0.6864 - acc: 0.5473 - val_loss: 0.6688 - val_acc: 0.5906

Epoch 00004: val_acc improved from 0.56693 to 0.59055, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window08/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 5/10

  64/4566 [..............................] - ETA: 6:08 - loss: 0.6682 - acc: 0.6250
 128/4566 [..............................] - ETA: 6:01 - loss: 0.6859 - acc: 0.5859
 192/4566 [>.............................] - ETA: 5:49 - loss: 0.6749 - acc: 0.5938
 256/4566 [>.............................] - ETA: 5:36 - loss: 0.6735 - acc: 0.5703
 320/4566 [=>............................] - ETA: 5:29 - loss: 0.6748 - acc: 0.5719
 384/4566 [=>............................] - ETA: 5:28 - loss: 0.6730 - acc: 0.5833
 448/4566 [=>............................] - ETA: 6:08 - loss: 0.6749 - acc: 0.5737
 512/4566 [==>...........................] - ETA: 6:57 - loss: 0.6692 - acc: 0.5820
 576/4566 [==>...........................] - ETA: 7:30 - loss: 0.6712 - acc: 0.5747
 640/4566 [===>..........................] - ETA: 7:55 - loss: 0.6675 - acc: 0.5797
 704/4566 [===>..........................] - ETA: 8:15 - loss: 0.6692 - acc: 0.5767
 768/4566 [====>.........................] - ETA: 8:26 - loss: 0.6729 - acc: 0.5638
 832/4566 [====>.........................] - ETA: 8:31 - loss: 0.6733 - acc: 0.5661
 896/4566 [====>.........................] - ETA: 8:12 - loss: 0.6744 - acc: 0.5681
 960/4566 [=====>........................] - ETA: 7:52 - loss: 0.6761 - acc: 0.5635
1024/4566 [=====>........................] - ETA: 7:31 - loss: 0.6767 - acc: 0.5596
1088/4566 [======>.......................] - ETA: 7:14 - loss: 0.6783 - acc: 0.5579
1152/4566 [======>.......................] - ETA: 6:57 - loss: 0.6756 - acc: 0.5660
1216/4566 [======>.......................] - ETA: 6:42 - loss: 0.6783 - acc: 0.5609
1280/4566 [=======>......................] - ETA: 6:29 - loss: 0.6806 - acc: 0.5594
1344/4566 [=======>......................] - ETA: 6:16 - loss: 0.6811 - acc: 0.5603
1408/4566 [========>.....................] - ETA: 6:03 - loss: 0.6810 - acc: 0.5604
1472/4566 [========>.....................] - ETA: 5:50 - loss: 0.6820 - acc: 0.5591
1536/4566 [=========>....................] - ETA: 5:39 - loss: 0.6817 - acc: 0.5612
1600/4566 [=========>....................] - ETA: 5:27 - loss: 0.6804 - acc: 0.5656
1664/4566 [=========>....................] - ETA: 5:17 - loss: 0.6806 - acc: 0.5667
1728/4566 [==========>...................] - ETA: 5:06 - loss: 0.6813 - acc: 0.5683
1792/4566 [==========>...................] - ETA: 4:57 - loss: 0.6821 - acc: 0.5670
1856/4566 [===========>..................] - ETA: 4:47 - loss: 0.6810 - acc: 0.5673
1920/4566 [===========>..................] - ETA: 4:38 - loss: 0.6812 - acc: 0.5656
1984/4566 [============>.................] - ETA: 4:30 - loss: 0.6808 - acc: 0.5650
2048/4566 [============>.................] - ETA: 4:30 - loss: 0.6799 - acc: 0.5679
2112/4566 [============>.................] - ETA: 4:30 - loss: 0.6801 - acc: 0.5668
2176/4566 [=============>................] - ETA: 4:28 - loss: 0.6786 - acc: 0.5671
2240/4566 [=============>................] - ETA: 4:26 - loss: 0.6799 - acc: 0.5634
2304/4566 [==============>...............] - ETA: 4:24 - loss: 0.6788 - acc: 0.5664
2368/4566 [==============>...............] - ETA: 4:20 - loss: 0.6790 - acc: 0.5663
2432/4566 [==============>...............] - ETA: 4:15 - loss: 0.6800 - acc: 0.5650
2496/4566 [===============>..............] - ETA: 4:06 - loss: 0.6811 - acc: 0.5633
2560/4566 [===============>..............] - ETA: 3:56 - loss: 0.6798 - acc: 0.5656
2624/4566 [================>.............] - ETA: 3:46 - loss: 0.6793 - acc: 0.5682
2688/4566 [================>.............] - ETA: 3:37 - loss: 0.6795 - acc: 0.5688
2752/4566 [=================>............] - ETA: 3:28 - loss: 0.6798 - acc: 0.5705
2816/4566 [=================>............] - ETA: 3:19 - loss: 0.6799 - acc: 0.5703
2880/4566 [=================>............] - ETA: 3:11 - loss: 0.6798 - acc: 0.5705
2944/4566 [==================>...........] - ETA: 3:03 - loss: 0.6800 - acc: 0.5690
3008/4566 [==================>...........] - ETA: 2:54 - loss: 0.6784 - acc: 0.5728
3072/4566 [===================>..........] - ETA: 2:46 - loss: 0.6799 - acc: 0.5706
3136/4566 [===================>..........] - ETA: 2:38 - loss: 0.6807 - acc: 0.5689
3200/4566 [====================>.........] - ETA: 2:30 - loss: 0.6814 - acc: 0.5678
3264/4566 [====================>.........] - ETA: 2:22 - loss: 0.6822 - acc: 0.5662
3328/4566 [====================>.........] - ETA: 2:15 - loss: 0.6828 - acc: 0.5649
3392/4566 [=====================>........] - ETA: 2:07 - loss: 0.6826 - acc: 0.5663
3456/4566 [=====================>........] - ETA: 1:59 - loss: 0.6824 - acc: 0.5671
3520/4566 [======================>.......] - ETA: 1:52 - loss: 0.6824 - acc: 0.5670
3584/4566 [======================>.......] - ETA: 1:45 - loss: 0.6824 - acc: 0.5667
3648/4566 [======================>.......] - ETA: 1:40 - loss: 0.6817 - acc: 0.5683
3712/4566 [=======================>......] - ETA: 1:35 - loss: 0.6814 - acc: 0.5679
3776/4566 [=======================>......] - ETA: 1:29 - loss: 0.6809 - acc: 0.5681
3840/4566 [========================>.....] - ETA: 1:23 - loss: 0.6799 - acc: 0.5698
3904/4566 [========================>.....] - ETA: 1:16 - loss: 0.6804 - acc: 0.5694
3968/4566 [=========================>....] - ETA: 1:09 - loss: 0.6805 - acc: 0.5685
4032/4566 [=========================>....] - ETA: 1:02 - loss: 0.6797 - acc: 0.5689
4096/4566 [=========================>....] - ETA: 54s - loss: 0.6798 - acc: 0.5698 
4160/4566 [==========================>...] - ETA: 46s - loss: 0.6797 - acc: 0.5704
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6793 - acc: 0.5710
4288/4566 [===========================>..] - ETA: 31s - loss: 0.6790 - acc: 0.5711
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6792 - acc: 0.5715
4416/4566 [============================>.] - ETA: 16s - loss: 0.6797 - acc: 0.5711
4480/4566 [============================>.] - ETA: 9s - loss: 0.6798 - acc: 0.5717 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6798 - acc: 0.5724
4566/4566 [==============================] - 527s 115ms/step - loss: 0.6798 - acc: 0.5725 - val_loss: 0.6770 - val_acc: 0.5768

Epoch 00005: val_acc did not improve from 0.59055
Epoch 6/10

  64/4566 [..............................] - ETA: 6:03 - loss: 0.6605 - acc: 0.5469
 128/4566 [..............................] - ETA: 5:52 - loss: 0.6697 - acc: 0.5391
 192/4566 [>.............................] - ETA: 5:39 - loss: 0.6666 - acc: 0.5885
 256/4566 [>.............................] - ETA: 5:34 - loss: 0.6729 - acc: 0.5859
 320/4566 [=>............................] - ETA: 5:41 - loss: 0.6756 - acc: 0.5875
 384/4566 [=>............................] - ETA: 6:35 - loss: 0.6831 - acc: 0.5729
 448/4566 [=>............................] - ETA: 7:42 - loss: 0.6836 - acc: 0.5692
 512/4566 [==>...........................] - ETA: 8:19 - loss: 0.6826 - acc: 0.5645
 576/4566 [==>...........................] - ETA: 8:40 - loss: 0.6810 - acc: 0.5608
 640/4566 [===>..........................] - ETA: 8:59 - loss: 0.6787 - acc: 0.5625
 704/4566 [===>..........................] - ETA: 9:10 - loss: 0.6740 - acc: 0.5710
 768/4566 [====>.........................] - ETA: 9:11 - loss: 0.6714 - acc: 0.5768
 832/4566 [====>.........................] - ETA: 8:43 - loss: 0.6673 - acc: 0.5877
 896/4566 [====>.........................] - ETA: 8:20 - loss: 0.6685 - acc: 0.5915
 960/4566 [=====>........................] - ETA: 7:56 - loss: 0.6686 - acc: 0.5938
1024/4566 [=====>........................] - ETA: 7:36 - loss: 0.6708 - acc: 0.5879
1088/4566 [======>.......................] - ETA: 7:18 - loss: 0.6713 - acc: 0.5882
1152/4566 [======>.......................] - ETA: 7:02 - loss: 0.6705 - acc: 0.5903
1216/4566 [======>.......................] - ETA: 6:45 - loss: 0.6698 - acc: 0.5880
1280/4566 [=======>......................] - ETA: 6:31 - loss: 0.6703 - acc: 0.5883
1344/4566 [=======>......................] - ETA: 6:17 - loss: 0.6725 - acc: 0.5833
1408/4566 [========>.....................] - ETA: 6:03 - loss: 0.6719 - acc: 0.5845
1472/4566 [========>.....................] - ETA: 5:51 - loss: 0.6725 - acc: 0.5829
1536/4566 [=========>....................] - ETA: 5:40 - loss: 0.6732 - acc: 0.5827
1600/4566 [=========>....................] - ETA: 5:30 - loss: 0.6721 - acc: 0.5856
1664/4566 [=========>....................] - ETA: 5:19 - loss: 0.6719 - acc: 0.5859
1728/4566 [==========>...................] - ETA: 5:09 - loss: 0.6718 - acc: 0.5856
1792/4566 [==========>...................] - ETA: 4:58 - loss: 0.6707 - acc: 0.5882
1856/4566 [===========>..................] - ETA: 4:48 - loss: 0.6706 - acc: 0.5873
1920/4566 [===========>..................] - ETA: 4:46 - loss: 0.6721 - acc: 0.5859
1984/4566 [============>.................] - ETA: 4:47 - loss: 0.6722 - acc: 0.5837
2048/4566 [============>.................] - ETA: 4:48 - loss: 0.6720 - acc: 0.5830
2112/4566 [============>.................] - ETA: 4:46 - loss: 0.6706 - acc: 0.5857
2176/4566 [=============>................] - ETA: 4:44 - loss: 0.6715 - acc: 0.5836
2240/4566 [=============>................] - ETA: 4:42 - loss: 0.6718 - acc: 0.5830
2304/4566 [==============>...............] - ETA: 4:38 - loss: 0.6739 - acc: 0.5799
2368/4566 [==============>...............] - ETA: 4:29 - loss: 0.6744 - acc: 0.5798
2432/4566 [==============>...............] - ETA: 4:19 - loss: 0.6749 - acc: 0.5794
2496/4566 [===============>..............] - ETA: 4:09 - loss: 0.6751 - acc: 0.5793
2560/4566 [===============>..............] - ETA: 3:59 - loss: 0.6760 - acc: 0.5789
2624/4566 [================>.............] - ETA: 3:50 - loss: 0.6768 - acc: 0.5774
2688/4566 [================>.............] - ETA: 3:41 - loss: 0.6758 - acc: 0.5785
2752/4566 [=================>............] - ETA: 3:32 - loss: 0.6758 - acc: 0.5778
2816/4566 [=================>............] - ETA: 3:23 - loss: 0.6762 - acc: 0.5763
2880/4566 [=================>............] - ETA: 3:14 - loss: 0.6762 - acc: 0.5760
2944/4566 [==================>...........] - ETA: 3:05 - loss: 0.6762 - acc: 0.5764
3008/4566 [==================>...........] - ETA: 2:57 - loss: 0.6759 - acc: 0.5768
3072/4566 [===================>..........] - ETA: 2:48 - loss: 0.6764 - acc: 0.5762
3136/4566 [===================>..........] - ETA: 2:40 - loss: 0.6759 - acc: 0.5772
3200/4566 [====================>.........] - ETA: 2:32 - loss: 0.6756 - acc: 0.5784
3264/4566 [====================>.........] - ETA: 2:24 - loss: 0.6754 - acc: 0.5778
3328/4566 [====================>.........] - ETA: 2:16 - loss: 0.6763 - acc: 0.5754
3392/4566 [=====================>........] - ETA: 2:08 - loss: 0.6763 - acc: 0.5752
3456/4566 [=====================>........] - ETA: 2:01 - loss: 0.6767 - acc: 0.5738
3520/4566 [======================>.......] - ETA: 1:56 - loss: 0.6769 - acc: 0.5727
3584/4566 [======================>.......] - ETA: 1:50 - loss: 0.6761 - acc: 0.5748
3648/4566 [======================>.......] - ETA: 1:44 - loss: 0.6759 - acc: 0.5762
3712/4566 [=======================>......] - ETA: 1:38 - loss: 0.6766 - acc: 0.5749
3776/4566 [=======================>......] - ETA: 1:32 - loss: 0.6770 - acc: 0.5755
3840/4566 [========================>.....] - ETA: 1:25 - loss: 0.6767 - acc: 0.5755
3904/4566 [========================>.....] - ETA: 1:18 - loss: 0.6761 - acc: 0.5771
3968/4566 [=========================>....] - ETA: 1:10 - loss: 0.6761 - acc: 0.5769
4032/4566 [=========================>....] - ETA: 1:02 - loss: 0.6766 - acc: 0.5764
4096/4566 [=========================>....] - ETA: 54s - loss: 0.6765 - acc: 0.5771 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6770 - acc: 0.5762
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6766 - acc: 0.5765
4288/4566 [===========================>..] - ETA: 31s - loss: 0.6765 - acc: 0.5767
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6762 - acc: 0.5774
4416/4566 [============================>.] - ETA: 17s - loss: 0.6763 - acc: 0.5768
4480/4566 [============================>.] - ETA: 9s - loss: 0.6765 - acc: 0.5761 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6760 - acc: 0.5764
4566/4566 [==============================] - 530s 116ms/step - loss: 0.6766 - acc: 0.5751 - val_loss: 0.6663 - val_acc: 0.6201

Epoch 00006: val_acc improved from 0.59055 to 0.62008, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window08/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 7/10

  64/4566 [..............................] - ETA: 5:32 - loss: 0.6639 - acc: 0.6094
 128/4566 [..............................] - ETA: 5:43 - loss: 0.6903 - acc: 0.5781
 192/4566 [>.............................] - ETA: 5:34 - loss: 0.6753 - acc: 0.5781
 256/4566 [>.............................] - ETA: 6:37 - loss: 0.6692 - acc: 0.5781
 320/4566 [=>............................] - ETA: 8:08 - loss: 0.6823 - acc: 0.5625
 384/4566 [=>............................] - ETA: 9:00 - loss: 0.6819 - acc: 0.5677
 448/4566 [=>............................] - ETA: 9:39 - loss: 0.6744 - acc: 0.5759
 512/4566 [==>...........................] - ETA: 9:57 - loss: 0.6735 - acc: 0.5879
 576/4566 [==>...........................] - ETA: 10:11 - loss: 0.6774 - acc: 0.5816
 640/4566 [===>..........................] - ETA: 10:12 - loss: 0.6767 - acc: 0.5813
 704/4566 [===>..........................] - ETA: 9:40 - loss: 0.6764 - acc: 0.5795 
 768/4566 [====>.........................] - ETA: 9:06 - loss: 0.6786 - acc: 0.5742
 832/4566 [====>.........................] - ETA: 8:38 - loss: 0.6821 - acc: 0.5685
 896/4566 [====>.........................] - ETA: 8:14 - loss: 0.6841 - acc: 0.5703
 960/4566 [=====>........................] - ETA: 7:50 - loss: 0.6832 - acc: 0.5740
1024/4566 [=====>........................] - ETA: 7:31 - loss: 0.6831 - acc: 0.5752
1088/4566 [======>.......................] - ETA: 7:10 - loss: 0.6808 - acc: 0.5818
1152/4566 [======>.......................] - ETA: 6:52 - loss: 0.6794 - acc: 0.5842
1216/4566 [======>.......................] - ETA: 6:38 - loss: 0.6782 - acc: 0.5839
1280/4566 [=======>......................] - ETA: 6:25 - loss: 0.6770 - acc: 0.5867
1344/4566 [=======>......................] - ETA: 6:11 - loss: 0.6793 - acc: 0.5789
1408/4566 [========>.....................] - ETA: 5:58 - loss: 0.6798 - acc: 0.5753
1472/4566 [========>.....................] - ETA: 5:45 - loss: 0.6808 - acc: 0.5747
1536/4566 [=========>....................] - ETA: 5:34 - loss: 0.6800 - acc: 0.5755
1600/4566 [=========>....................] - ETA: 5:22 - loss: 0.6807 - acc: 0.5713
1664/4566 [=========>....................] - ETA: 5:11 - loss: 0.6805 - acc: 0.5739
1728/4566 [==========>...................] - ETA: 5:01 - loss: 0.6807 - acc: 0.5741
1792/4566 [==========>...................] - ETA: 4:57 - loss: 0.6800 - acc: 0.5753
1856/4566 [===========>..................] - ETA: 5:00 - loss: 0.6793 - acc: 0.5765
1920/4566 [===========>..................] - ETA: 5:00 - loss: 0.6789 - acc: 0.5766
1984/4566 [============>.................] - ETA: 4:59 - loss: 0.6793 - acc: 0.5741
2048/4566 [============>.................] - ETA: 4:58 - loss: 0.6783 - acc: 0.5767
2112/4566 [============>.................] - ETA: 4:56 - loss: 0.6782 - acc: 0.5777
2176/4566 [=============>................] - ETA: 4:52 - loss: 0.6765 - acc: 0.5809
2240/4566 [=============>................] - ETA: 4:42 - loss: 0.6764 - acc: 0.5804
2304/4566 [==============>...............] - ETA: 4:31 - loss: 0.6761 - acc: 0.5794
2368/4566 [==============>...............] - ETA: 4:21 - loss: 0.6758 - acc: 0.5802
2432/4566 [==============>...............] - ETA: 4:11 - loss: 0.6762 - acc: 0.5794
2496/4566 [===============>..............] - ETA: 4:01 - loss: 0.6771 - acc: 0.5777
2560/4566 [===============>..............] - ETA: 3:51 - loss: 0.6773 - acc: 0.5777
2624/4566 [================>.............] - ETA: 3:41 - loss: 0.6768 - acc: 0.5777
2688/4566 [================>.............] - ETA: 3:32 - loss: 0.6769 - acc: 0.5778
2752/4566 [=================>............] - ETA: 3:23 - loss: 0.6773 - acc: 0.5789
2816/4566 [=================>............] - ETA: 3:14 - loss: 0.6770 - acc: 0.5799
2880/4566 [=================>............] - ETA: 3:05 - loss: 0.6772 - acc: 0.5792
2944/4566 [==================>...........] - ETA: 2:57 - loss: 0.6766 - acc: 0.5812
3008/4566 [==================>...........] - ETA: 2:49 - loss: 0.6765 - acc: 0.5818
3072/4566 [===================>..........] - ETA: 2:41 - loss: 0.6766 - acc: 0.5817
3136/4566 [===================>..........] - ETA: 2:33 - loss: 0.6769 - acc: 0.5800
3200/4566 [====================>.........] - ETA: 2:25 - loss: 0.6765 - acc: 0.5794
3264/4566 [====================>.........] - ETA: 2:17 - loss: 0.6768 - acc: 0.5778
3328/4566 [====================>.........] - ETA: 2:10 - loss: 0.6760 - acc: 0.5799
3392/4566 [=====================>........] - ETA: 2:05 - loss: 0.6765 - acc: 0.5793
3456/4566 [=====================>........] - ETA: 2:00 - loss: 0.6765 - acc: 0.5802
3520/4566 [======================>.......] - ETA: 1:55 - loss: 0.6769 - acc: 0.5781
3584/4566 [======================>.......] - ETA: 1:49 - loss: 0.6771 - acc: 0.5773
3648/4566 [======================>.......] - ETA: 1:43 - loss: 0.6768 - acc: 0.5781
3712/4566 [=======================>......] - ETA: 1:37 - loss: 0.6760 - acc: 0.5789
3776/4566 [=======================>......] - ETA: 1:30 - loss: 0.6758 - acc: 0.5805
3840/4566 [========================>.....] - ETA: 1:22 - loss: 0.6756 - acc: 0.5813
3904/4566 [========================>.....] - ETA: 1:14 - loss: 0.6761 - acc: 0.5802
3968/4566 [=========================>....] - ETA: 1:07 - loss: 0.6757 - acc: 0.5814
4032/4566 [=========================>....] - ETA: 59s - loss: 0.6759 - acc: 0.5818 
4096/4566 [=========================>....] - ETA: 52s - loss: 0.6762 - acc: 0.5818
4160/4566 [==========================>...] - ETA: 44s - loss: 0.6753 - acc: 0.5827
4224/4566 [==========================>...] - ETA: 37s - loss: 0.6743 - acc: 0.5857
4288/4566 [===========================>..] - ETA: 30s - loss: 0.6744 - acc: 0.5861
4352/4566 [===========================>..] - ETA: 23s - loss: 0.6742 - acc: 0.5862
4416/4566 [============================>.] - ETA: 16s - loss: 0.6746 - acc: 0.5851
4480/4566 [============================>.] - ETA: 9s - loss: 0.6745 - acc: 0.5850 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6743 - acc: 0.5852
4566/4566 [==============================] - 503s 110ms/step - loss: 0.6747 - acc: 0.5852 - val_loss: 0.6644 - val_acc: 0.6024

Epoch 00007: val_acc did not improve from 0.62008
Epoch 8/10

  64/4566 [..............................] - ETA: 5:08 - loss: 0.7180 - acc: 0.5625
 128/4566 [..............................] - ETA: 5:44 - loss: 0.6989 - acc: 0.5312
 192/4566 [>.............................] - ETA: 9:00 - loss: 0.6926 - acc: 0.5365
 256/4566 [>.............................] - ETA: 10:08 - loss: 0.6842 - acc: 0.5391
 320/4566 [=>............................] - ETA: 10:38 - loss: 0.6772 - acc: 0.5656
 384/4566 [=>............................] - ETA: 10:59 - loss: 0.6762 - acc: 0.5651
 448/4566 [=>............................] - ETA: 11:10 - loss: 0.6740 - acc: 0.5692
 512/4566 [==>...........................] - ETA: 11:12 - loss: 0.6707 - acc: 0.5781
 576/4566 [==>...........................] - ETA: 10:37 - loss: 0.6723 - acc: 0.5781
 640/4566 [===>..........................] - ETA: 9:54 - loss: 0.6729 - acc: 0.5781 
 704/4566 [===>..........................] - ETA: 9:18 - loss: 0.6730 - acc: 0.5753
 768/4566 [====>.........................] - ETA: 8:45 - loss: 0.6700 - acc: 0.5872
 832/4566 [====>.........................] - ETA: 8:17 - loss: 0.6701 - acc: 0.5901
 896/4566 [====>.........................] - ETA: 7:51 - loss: 0.6715 - acc: 0.5848
 960/4566 [=====>........................] - ETA: 7:28 - loss: 0.6730 - acc: 0.5885
1024/4566 [=====>........................] - ETA: 7:07 - loss: 0.6711 - acc: 0.5898
1088/4566 [======>.......................] - ETA: 6:49 - loss: 0.6719 - acc: 0.5901
1152/4566 [======>.......................] - ETA: 6:32 - loss: 0.6724 - acc: 0.5903
1216/4566 [======>.......................] - ETA: 6:17 - loss: 0.6737 - acc: 0.5880
1280/4566 [=======>......................] - ETA: 6:02 - loss: 0.6735 - acc: 0.5867
1344/4566 [=======>......................] - ETA: 5:49 - loss: 0.6749 - acc: 0.5856
1408/4566 [========>.....................] - ETA: 5:37 - loss: 0.6754 - acc: 0.5810
1472/4566 [========>.....................] - ETA: 5:25 - loss: 0.6757 - acc: 0.5808
1536/4566 [=========>....................] - ETA: 5:14 - loss: 0.6741 - acc: 0.5833
1600/4566 [=========>....................] - ETA: 5:03 - loss: 0.6735 - acc: 0.5850
1664/4566 [=========>....................] - ETA: 4:53 - loss: 0.6727 - acc: 0.5871
1728/4566 [==========>...................] - ETA: 4:45 - loss: 0.6741 - acc: 0.5828
1792/4566 [==========>...................] - ETA: 4:46 - loss: 0.6744 - acc: 0.5837
1856/4566 [===========>..................] - ETA: 4:47 - loss: 0.6743 - acc: 0.5830
1920/4566 [===========>..................] - ETA: 4:47 - loss: 0.6739 - acc: 0.5854
1984/4566 [============>.................] - ETA: 4:47 - loss: 0.6735 - acc: 0.5857
2048/4566 [============>.................] - ETA: 4:46 - loss: 0.6744 - acc: 0.5845
2112/4566 [============>.................] - ETA: 4:45 - loss: 0.6743 - acc: 0.5829
2176/4566 [=============>................] - ETA: 4:39 - loss: 0.6750 - acc: 0.5813
2240/4566 [=============>................] - ETA: 4:30 - loss: 0.6746 - acc: 0.5826
2304/4566 [==============>...............] - ETA: 4:20 - loss: 0.6744 - acc: 0.5838
2368/4566 [==============>...............] - ETA: 4:11 - loss: 0.6727 - acc: 0.5887
2432/4566 [==============>...............] - ETA: 4:02 - loss: 0.6714 - acc: 0.5905
2496/4566 [===============>..............] - ETA: 3:52 - loss: 0.6723 - acc: 0.5897
2560/4566 [===============>..............] - ETA: 3:43 - loss: 0.6721 - acc: 0.5895
2624/4566 [================>.............] - ETA: 3:34 - loss: 0.6723 - acc: 0.5896
2688/4566 [================>.............] - ETA: 3:24 - loss: 0.6722 - acc: 0.5900
2752/4566 [=================>............] - ETA: 3:16 - loss: 0.6739 - acc: 0.5868
2816/4566 [=================>............] - ETA: 3:07 - loss: 0.6733 - acc: 0.5881
2880/4566 [=================>............] - ETA: 2:59 - loss: 0.6730 - acc: 0.5892
2944/4566 [==================>...........] - ETA: 2:51 - loss: 0.6730 - acc: 0.5900
3008/4566 [==================>...........] - ETA: 2:43 - loss: 0.6726 - acc: 0.5911
3072/4566 [===================>..........] - ETA: 2:35 - loss: 0.6720 - acc: 0.5911
3136/4566 [===================>..........] - ETA: 2:28 - loss: 0.6719 - acc: 0.5906
3200/4566 [====================>.........] - ETA: 2:20 - loss: 0.6712 - acc: 0.5919
3264/4566 [====================>.........] - ETA: 2:12 - loss: 0.6716 - acc: 0.5913
3328/4566 [====================>.........] - ETA: 2:07 - loss: 0.6713 - acc: 0.5910
3392/4566 [=====================>........] - ETA: 2:02 - loss: 0.6707 - acc: 0.5935
3456/4566 [=====================>........] - ETA: 1:57 - loss: 0.6712 - acc: 0.5926
3520/4566 [======================>.......] - ETA: 1:52 - loss: 0.6719 - acc: 0.5920
3584/4566 [======================>.......] - ETA: 1:46 - loss: 0.6725 - acc: 0.5910
3648/4566 [======================>.......] - ETA: 1:41 - loss: 0.6715 - acc: 0.5924
3712/4566 [=======================>......] - ETA: 1:35 - loss: 0.6718 - acc: 0.5927
3776/4566 [=======================>......] - ETA: 1:27 - loss: 0.6723 - acc: 0.5919
3840/4566 [========================>.....] - ETA: 1:20 - loss: 0.6716 - acc: 0.5930
3904/4566 [========================>.....] - ETA: 1:13 - loss: 0.6717 - acc: 0.5930
3968/4566 [=========================>....] - ETA: 1:05 - loss: 0.6722 - acc: 0.5917
4032/4566 [=========================>....] - ETA: 58s - loss: 0.6723 - acc: 0.5920 
4096/4566 [=========================>....] - ETA: 51s - loss: 0.6721 - acc: 0.5920
4160/4566 [==========================>...] - ETA: 43s - loss: 0.6720 - acc: 0.5925
4224/4566 [==========================>...] - ETA: 36s - loss: 0.6730 - acc: 0.5904
4288/4566 [===========================>..] - ETA: 29s - loss: 0.6730 - acc: 0.5893
4352/4566 [===========================>..] - ETA: 22s - loss: 0.6724 - acc: 0.5908
4416/4566 [============================>.] - ETA: 15s - loss: 0.6728 - acc: 0.5904
4480/4566 [============================>.] - ETA: 9s - loss: 0.6721 - acc: 0.5915 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6724 - acc: 0.5913
4566/4566 [==============================] - 490s 107ms/step - loss: 0.6724 - acc: 0.5913 - val_loss: 0.6636 - val_acc: 0.6122

Epoch 00008: val_acc did not improve from 0.62008
Epoch 9/10

  64/4566 [..............................] - ETA: 5:19 - loss: 0.6848 - acc: 0.5781
 128/4566 [..............................] - ETA: 6:04 - loss: 0.6837 - acc: 0.5703
 192/4566 [>.............................] - ETA: 8:29 - loss: 0.6865 - acc: 0.5417
 256/4566 [>.............................] - ETA: 9:46 - loss: 0.6794 - acc: 0.5859
 320/4566 [=>............................] - ETA: 10:19 - loss: 0.6793 - acc: 0.5906
 384/4566 [=>............................] - ETA: 10:40 - loss: 0.6784 - acc: 0.5938
 448/4566 [=>............................] - ETA: 10:48 - loss: 0.6811 - acc: 0.5848
 512/4566 [==>...........................] - ETA: 10:54 - loss: 0.6829 - acc: 0.5762
 576/4566 [==>...........................] - ETA: 10:31 - loss: 0.6806 - acc: 0.5851
 640/4566 [===>..........................] - ETA: 9:54 - loss: 0.6822 - acc: 0.5766 
 704/4566 [===>..........................] - ETA: 9:21 - loss: 0.6899 - acc: 0.5597
 768/4566 [====>.........................] - ETA: 8:50 - loss: 0.6907 - acc: 0.5560
 832/4566 [====>.........................] - ETA: 8:20 - loss: 0.6883 - acc: 0.5601
 896/4566 [====>.........................] - ETA: 7:53 - loss: 0.6883 - acc: 0.5592
 960/4566 [=====>........................] - ETA: 7:31 - loss: 0.6903 - acc: 0.5563
1024/4566 [=====>........................] - ETA: 7:11 - loss: 0.6908 - acc: 0.5557
1088/4566 [======>.......................] - ETA: 6:53 - loss: 0.6905 - acc: 0.5561
1152/4566 [======>.......................] - ETA: 6:36 - loss: 0.6912 - acc: 0.5530
1216/4566 [======>.......................] - ETA: 6:21 - loss: 0.6908 - acc: 0.5543
1280/4566 [=======>......................] - ETA: 6:07 - loss: 0.6912 - acc: 0.5500
1344/4566 [=======>......................] - ETA: 5:53 - loss: 0.6895 - acc: 0.5536
1408/4566 [========>.....................] - ETA: 5:41 - loss: 0.6870 - acc: 0.5597
1472/4566 [========>.....................] - ETA: 5:29 - loss: 0.6870 - acc: 0.5598
1536/4566 [=========>....................] - ETA: 5:17 - loss: 0.6836 - acc: 0.5677
1600/4566 [=========>....................] - ETA: 5:06 - loss: 0.6836 - acc: 0.5663
1664/4566 [=========>....................] - ETA: 4:56 - loss: 0.6829 - acc: 0.5685
1728/4566 [==========>...................] - ETA: 4:54 - loss: 0.6829 - acc: 0.5671
1792/4566 [==========>...................] - ETA: 4:56 - loss: 0.6823 - acc: 0.5681
1856/4566 [===========>..................] - ETA: 4:57 - loss: 0.6807 - acc: 0.5722
1920/4566 [===========>..................] - ETA: 4:57 - loss: 0.6808 - acc: 0.5714
1984/4566 [============>.................] - ETA: 4:58 - loss: 0.6790 - acc: 0.5746
2048/4566 [============>.................] - ETA: 4:56 - loss: 0.6796 - acc: 0.5718
2112/4566 [============>.................] - ETA: 4:52 - loss: 0.6791 - acc: 0.5724
2176/4566 [=============>................] - ETA: 4:42 - loss: 0.6788 - acc: 0.5726
2240/4566 [=============>................] - ETA: 4:33 - loss: 0.6789 - acc: 0.5728
2304/4566 [==============>...............] - ETA: 4:23 - loss: 0.6796 - acc: 0.5742
2368/4566 [==============>...............] - ETA: 4:13 - loss: 0.6792 - acc: 0.5760
2432/4566 [==============>...............] - ETA: 4:03 - loss: 0.6786 - acc: 0.5765
2496/4566 [===============>..............] - ETA: 3:53 - loss: 0.6767 - acc: 0.5801
2560/4566 [===============>..............] - ETA: 3:44 - loss: 0.6769 - acc: 0.5797
2624/4566 [================>.............] - ETA: 3:35 - loss: 0.6770 - acc: 0.5785
2688/4566 [================>.............] - ETA: 3:26 - loss: 0.6762 - acc: 0.5785
2752/4566 [=================>............] - ETA: 3:17 - loss: 0.6751 - acc: 0.5792
2816/4566 [=================>............] - ETA: 3:08 - loss: 0.6752 - acc: 0.5813
2880/4566 [=================>............] - ETA: 3:00 - loss: 0.6751 - acc: 0.5823
2944/4566 [==================>...........] - ETA: 2:52 - loss: 0.6753 - acc: 0.5819
3008/4566 [==================>...........] - ETA: 2:44 - loss: 0.6751 - acc: 0.5818
3072/4566 [===================>..........] - ETA: 2:36 - loss: 0.6741 - acc: 0.5843
3136/4566 [===================>..........] - ETA: 2:28 - loss: 0.6748 - acc: 0.5829
3200/4566 [====================>.........] - ETA: 2:21 - loss: 0.6747 - acc: 0.5841
3264/4566 [====================>.........] - ETA: 2:14 - loss: 0.6742 - acc: 0.5843
3328/4566 [====================>.........] - ETA: 2:10 - loss: 0.6756 - acc: 0.5826
3392/4566 [=====================>........] - ETA: 2:05 - loss: 0.6748 - acc: 0.5840
3456/4566 [=====================>........] - ETA: 2:00 - loss: 0.6750 - acc: 0.5836
3520/4566 [======================>.......] - ETA: 1:55 - loss: 0.6742 - acc: 0.5847
3584/4566 [======================>.......] - ETA: 1:50 - loss: 0.6741 - acc: 0.5851
3648/4566 [======================>.......] - ETA: 1:45 - loss: 0.6736 - acc: 0.5855
3712/4566 [=======================>......] - ETA: 1:38 - loss: 0.6738 - acc: 0.5862
3776/4566 [=======================>......] - ETA: 1:30 - loss: 0.6740 - acc: 0.5861
3840/4566 [========================>.....] - ETA: 1:22 - loss: 0.6752 - acc: 0.5852
3904/4566 [========================>.....] - ETA: 1:15 - loss: 0.6755 - acc: 0.5856
3968/4566 [=========================>....] - ETA: 1:07 - loss: 0.6758 - acc: 0.5847
4032/4566 [=========================>....] - ETA: 59s - loss: 0.6762 - acc: 0.5838 
4096/4566 [=========================>....] - ETA: 52s - loss: 0.6754 - acc: 0.5852
4160/4566 [==========================>...] - ETA: 45s - loss: 0.6749 - acc: 0.5853
4224/4566 [==========================>...] - ETA: 37s - loss: 0.6749 - acc: 0.5855
4288/4566 [===========================>..] - ETA: 30s - loss: 0.6748 - acc: 0.5858
4352/4566 [===========================>..] - ETA: 23s - loss: 0.6751 - acc: 0.5848
4416/4566 [============================>.] - ETA: 16s - loss: 0.6751 - acc: 0.5851
4480/4566 [============================>.] - ETA: 9s - loss: 0.6751 - acc: 0.5859 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6744 - acc: 0.5869
4566/4566 [==============================] - 505s 111ms/step - loss: 0.6746 - acc: 0.5861 - val_loss: 0.7171 - val_acc: 0.5118

Epoch 00009: val_acc did not improve from 0.62008
Epoch 10/10

  64/4566 [..............................] - ETA: 8:32 - loss: 0.7005 - acc: 0.5156
 128/4566 [..............................] - ETA: 11:07 - loss: 0.6843 - acc: 0.5547
 192/4566 [>.............................] - ETA: 11:38 - loss: 0.6712 - acc: 0.5833
 256/4566 [>.............................] - ETA: 11:54 - loss: 0.6701 - acc: 0.5898
 320/4566 [=>............................] - ETA: 12:04 - loss: 0.6620 - acc: 0.6094
 384/4566 [=>............................] - ETA: 12:02 - loss: 0.6627 - acc: 0.6094
 448/4566 [=>............................] - ETA: 11:58 - loss: 0.6672 - acc: 0.6049
 512/4566 [==>...........................] - ETA: 11:17 - loss: 0.6631 - acc: 0.6133
 576/4566 [==>...........................] - ETA: 10:33 - loss: 0.6666 - acc: 0.6042
 640/4566 [===>..........................] - ETA: 9:50 - loss: 0.6669 - acc: 0.6031 
 704/4566 [===>..........................] - ETA: 9:14 - loss: 0.6637 - acc: 0.6136
 768/4566 [====>.........................] - ETA: 8:40 - loss: 0.6630 - acc: 0.6107
 832/4566 [====>.........................] - ETA: 8:12 - loss: 0.6609 - acc: 0.6142
 896/4566 [====>.........................] - ETA: 7:46 - loss: 0.6607 - acc: 0.6161
 960/4566 [=====>........................] - ETA: 7:26 - loss: 0.6587 - acc: 0.6177
1024/4566 [=====>........................] - ETA: 7:07 - loss: 0.6600 - acc: 0.6162
1088/4566 [======>.......................] - ETA: 6:50 - loss: 0.6613 - acc: 0.6112
1152/4566 [======>.......................] - ETA: 6:34 - loss: 0.6574 - acc: 0.6163
1216/4566 [======>.......................] - ETA: 6:20 - loss: 0.6571 - acc: 0.6176
1280/4566 [=======>......................] - ETA: 6:06 - loss: 0.6573 - acc: 0.6180
1344/4566 [=======>......................] - ETA: 5:54 - loss: 0.6578 - acc: 0.6161
1408/4566 [========>.....................] - ETA: 5:43 - loss: 0.6587 - acc: 0.6151
1472/4566 [========>.....................] - ETA: 5:31 - loss: 0.6573 - acc: 0.6162
1536/4566 [=========>....................] - ETA: 5:19 - loss: 0.6600 - acc: 0.6107
1600/4566 [=========>....................] - ETA: 5:08 - loss: 0.6600 - acc: 0.6131
1664/4566 [=========>....................] - ETA: 5:05 - loss: 0.6602 - acc: 0.6142
1728/4566 [==========>...................] - ETA: 5:07 - loss: 0.6596 - acc: 0.6157
1792/4566 [==========>...................] - ETA: 5:08 - loss: 0.6635 - acc: 0.6094
1856/4566 [===========>..................] - ETA: 5:09 - loss: 0.6636 - acc: 0.6083
1920/4566 [===========>..................] - ETA: 5:08 - loss: 0.6651 - acc: 0.6057
1984/4566 [============>.................] - ETA: 5:06 - loss: 0.6649 - acc: 0.6048
2048/4566 [============>.................] - ETA: 5:03 - loss: 0.6643 - acc: 0.6064
2112/4566 [============>.................] - ETA: 4:52 - loss: 0.6649 - acc: 0.6070
2176/4566 [=============>................] - ETA: 4:42 - loss: 0.6641 - acc: 0.6066
2240/4566 [=============>................] - ETA: 4:31 - loss: 0.6638 - acc: 0.6089
2304/4566 [==============>...............] - ETA: 4:20 - loss: 0.6636 - acc: 0.6098
2368/4566 [==============>...............] - ETA: 4:10 - loss: 0.6637 - acc: 0.6111
2432/4566 [==============>...............] - ETA: 4:00 - loss: 0.6634 - acc: 0.6123
2496/4566 [===============>..............] - ETA: 3:50 - loss: 0.6645 - acc: 0.6114
2560/4566 [===============>..............] - ETA: 3:41 - loss: 0.6634 - acc: 0.6129
2624/4566 [================>.............] - ETA: 3:31 - loss: 0.6637 - acc: 0.6120
2688/4566 [================>.............] - ETA: 3:22 - loss: 0.6628 - acc: 0.6135
2752/4566 [=================>............] - ETA: 3:14 - loss: 0.6628 - acc: 0.6130
2816/4566 [=================>............] - ETA: 3:05 - loss: 0.6626 - acc: 0.6140
2880/4566 [=================>............] - ETA: 2:57 - loss: 0.6626 - acc: 0.6128
2944/4566 [==================>...........] - ETA: 2:49 - loss: 0.6640 - acc: 0.6101
3008/4566 [==================>...........] - ETA: 2:41 - loss: 0.6642 - acc: 0.6094
3072/4566 [===================>..........] - ETA: 2:33 - loss: 0.6656 - acc: 0.6061
3136/4566 [===================>..........] - ETA: 2:26 - loss: 0.6666 - acc: 0.6049
3200/4566 [====================>.........] - ETA: 2:18 - loss: 0.6663 - acc: 0.6041
3264/4566 [====================>.........] - ETA: 2:11 - loss: 0.6665 - acc: 0.6029
3328/4566 [====================>.........] - ETA: 2:06 - loss: 0.6661 - acc: 0.6028
3392/4566 [=====================>........] - ETA: 2:01 - loss: 0.6669 - acc: 0.6023
3456/4566 [=====================>........] - ETA: 1:56 - loss: 0.6669 - acc: 0.6039
3520/4566 [======================>.......] - ETA: 1:51 - loss: 0.6668 - acc: 0.6043
3584/4566 [======================>.......] - ETA: 1:46 - loss: 0.6666 - acc: 0.6052
3648/4566 [======================>.......] - ETA: 1:40 - loss: 0.6668 - acc: 0.6044
3712/4566 [=======================>......] - ETA: 1:34 - loss: 0.6667 - acc: 0.6053
3776/4566 [=======================>......] - ETA: 1:26 - loss: 0.6663 - acc: 0.6059
3840/4566 [========================>.....] - ETA: 1:19 - loss: 0.6660 - acc: 0.6073
3904/4566 [========================>.....] - ETA: 1:11 - loss: 0.6668 - acc: 0.6060
3968/4566 [=========================>....] - ETA: 1:04 - loss: 0.6670 - acc: 0.6051
4032/4566 [=========================>....] - ETA: 57s - loss: 0.6677 - acc: 0.6034 
4096/4566 [=========================>....] - ETA: 49s - loss: 0.6682 - acc: 0.6013
4160/4566 [==========================>...] - ETA: 42s - loss: 0.6684 - acc: 0.6002
4224/4566 [==========================>...] - ETA: 35s - loss: 0.6691 - acc: 0.5987
4288/4566 [===========================>..] - ETA: 29s - loss: 0.6690 - acc: 0.5989
4352/4566 [===========================>..] - ETA: 22s - loss: 0.6691 - acc: 0.5988
4416/4566 [============================>.] - ETA: 15s - loss: 0.6688 - acc: 0.5996
4480/4566 [============================>.] - ETA: 8s - loss: 0.6690 - acc: 0.6004 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6696 - acc: 0.5988
4566/4566 [==============================] - 479s 105ms/step - loss: 0.6696 - acc: 0.5988 - val_loss: 0.6660 - val_acc: 0.5925

Epoch 00010: val_acc did not improve from 0.62008
Saved model to disk
