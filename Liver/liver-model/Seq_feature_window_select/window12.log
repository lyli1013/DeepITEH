nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f3ad1de0510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f3ad1de0510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f3b37ea42d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f3b37ea42d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad1de0a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad1de0a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ad1d9a5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ad1d9a5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ad1d94750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ad1d94750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad1c18e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad1c18e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ad1d0cf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ad1d0cf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37cb6210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37cb6210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b6227a8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b6227a8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ad1ab1210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ad1ab1210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad1c39d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad1c39d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ad1c2aa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ad1c2aa10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad19da410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad19da410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ad1a20490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ad1a20490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ad1a29590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ad1a29590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad19d51d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad19d51d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ad1a34250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ad1a34250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac95b9b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac95b9b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac94d8ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac94d8ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ac939cad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ac939cad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac9296210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac9296210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac956fa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac956fa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac93e5d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac93e5d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac9285450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac9285450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ac91a7a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ac91a7a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad18dd590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ad18dd590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac9193cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac9193cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac910b990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac910b990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac0ebd650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac0ebd650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ac0da4990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ac0da4990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac0e1f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac0e1f610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac0eda310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac0eda310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac0d4f510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac0d4f510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac0da4f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac0da4f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ac0b497d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ac0b497d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37d42a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37d42a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac0a6c090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac0a6c090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac8fa8fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac8fa8fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac0acaad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac0acaad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ad1b15d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ad1b15d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac91a7110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac91a7110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac90b2310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac90b2310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac0875fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ac0875fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac070b350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ac070b350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ab847cc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ab847cc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ab8555750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ab8555750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac0706e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ac0706e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ab832b6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ab832b6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ab8240710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3ab8240710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ab8180fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ab8180fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ab831ea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ab831ea10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ab8240e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3ab8240e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ab80c38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3ab80c38d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3aafed0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3aafed0e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ab7f23550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3ab7f23550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3aafcc0b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3aafcc0b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3aafed0950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3aafed0950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3aafcc0250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3aafcc0250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3aafce5c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3aafce5c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3aafad56d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3aafad56d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3aaf97f890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3aaf97f890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3aafbd6490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3aafbd6490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3aafac68d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3aafac68d0>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-18 22:14:49.400849: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-18 22:14:49.442745: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-18 22:14:49.475585: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c37fd6d800 executing computations on platform Host. Devices:
2022-11-18 22:14:49.475693: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-18 22:14:50.617204: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window12.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 36:17 - loss: 0.7500 - acc: 0.4844
 128/4566 [..............................] - ETA: 28:04 - loss: 0.8603 - acc: 0.4297
 192/4566 [>.............................] - ETA: 24:32 - loss: 0.8125 - acc: 0.4635
 256/4566 [>.............................] - ETA: 21:44 - loss: 0.8081 - acc: 0.4492
 320/4566 [=>............................] - ETA: 19:55 - loss: 0.7929 - acc: 0.4719
 384/4566 [=>............................] - ETA: 18:33 - loss: 0.7800 - acc: 0.4818
 448/4566 [=>............................] - ETA: 17:39 - loss: 0.7794 - acc: 0.4710
 512/4566 [==>...........................] - ETA: 16:56 - loss: 0.7769 - acc: 0.4707
 576/4566 [==>...........................] - ETA: 16:41 - loss: 0.7744 - acc: 0.4705
 640/4566 [===>..........................] - ETA: 17:39 - loss: 0.7688 - acc: 0.4625
 704/4566 [===>..........................] - ETA: 17:34 - loss: 0.7621 - acc: 0.4659
 768/4566 [====>.........................] - ETA: 17:21 - loss: 0.7554 - acc: 0.4766
 832/4566 [====>.........................] - ETA: 17:04 - loss: 0.7503 - acc: 0.4868
 896/4566 [====>.........................] - ETA: 16:27 - loss: 0.7525 - acc: 0.4799
 960/4566 [=====>........................] - ETA: 15:41 - loss: 0.7509 - acc: 0.4781
1024/4566 [=====>........................] - ETA: 15:04 - loss: 0.7501 - acc: 0.4746
1088/4566 [======>.......................] - ETA: 14:27 - loss: 0.7477 - acc: 0.4789
1152/4566 [======>.......................] - ETA: 13:55 - loss: 0.7492 - acc: 0.4766
1216/4566 [======>.......................] - ETA: 13:24 - loss: 0.7504 - acc: 0.4762
1280/4566 [=======>......................] - ETA: 12:55 - loss: 0.7471 - acc: 0.4781
1344/4566 [=======>......................] - ETA: 12:27 - loss: 0.7461 - acc: 0.4792
1408/4566 [========>.....................] - ETA: 12:03 - loss: 0.7434 - acc: 0.4830
1472/4566 [========>.....................] - ETA: 11:38 - loss: 0.7418 - acc: 0.4823
1536/4566 [=========>....................] - ETA: 11:14 - loss: 0.7409 - acc: 0.4824
1600/4566 [=========>....................] - ETA: 10:49 - loss: 0.7406 - acc: 0.4831
1664/4566 [=========>....................] - ETA: 10:26 - loss: 0.7393 - acc: 0.4856
1728/4566 [==========>...................] - ETA: 10:19 - loss: 0.7402 - acc: 0.4850
1792/4566 [==========>...................] - ETA: 10:14 - loss: 0.7396 - acc: 0.4872
1856/4566 [===========>..................] - ETA: 10:05 - loss: 0.7378 - acc: 0.4898
1920/4566 [===========>..................] - ETA: 9:55 - loss: 0.7355 - acc: 0.4938 
1984/4566 [============>.................] - ETA: 9:46 - loss: 0.7342 - acc: 0.4934
2048/4566 [============>.................] - ETA: 9:29 - loss: 0.7340 - acc: 0.4946
2112/4566 [============>.................] - ETA: 9:08 - loss: 0.7348 - acc: 0.4934
2176/4566 [=============>................] - ETA: 8:48 - loss: 0.7353 - acc: 0.4913
2240/4566 [=============>................] - ETA: 8:28 - loss: 0.7335 - acc: 0.4942
2304/4566 [==============>...............] - ETA: 8:15 - loss: 0.7325 - acc: 0.4952
2368/4566 [==============>...............] - ETA: 7:59 - loss: 0.7320 - acc: 0.4970
2432/4566 [==============>...............] - ETA: 7:41 - loss: 0.7309 - acc: 0.4979
2496/4566 [===============>..............] - ETA: 7:24 - loss: 0.7292 - acc: 0.5024
2560/4566 [===============>..............] - ETA: 7:07 - loss: 0.7276 - acc: 0.5039
2624/4566 [================>.............] - ETA: 6:49 - loss: 0.7266 - acc: 0.5053
2688/4566 [================>.............] - ETA: 6:31 - loss: 0.7253 - acc: 0.5082
2752/4566 [=================>............] - ETA: 6:14 - loss: 0.7240 - acc: 0.5087
2816/4566 [=================>............] - ETA: 5:59 - loss: 0.7243 - acc: 0.5089
2880/4566 [=================>............] - ETA: 5:48 - loss: 0.7238 - acc: 0.5087
2944/4566 [==================>...........] - ETA: 5:38 - loss: 0.7242 - acc: 0.5065
3008/4566 [==================>...........] - ETA: 5:26 - loss: 0.7237 - acc: 0.5073
3072/4566 [===================>..........] - ETA: 5:14 - loss: 0.7233 - acc: 0.5055
3136/4566 [===================>..........] - ETA: 5:01 - loss: 0.7236 - acc: 0.5057
3200/4566 [====================>.........] - ETA: 4:48 - loss: 0.7223 - acc: 0.5081
3264/4566 [====================>.........] - ETA: 4:33 - loss: 0.7215 - acc: 0.5080
3328/4566 [====================>.........] - ETA: 4:17 - loss: 0.7216 - acc: 0.5072
3392/4566 [=====================>........] - ETA: 4:03 - loss: 0.7212 - acc: 0.5077
3456/4566 [=====================>........] - ETA: 3:47 - loss: 0.7209 - acc: 0.5075
3520/4566 [======================>.......] - ETA: 3:33 - loss: 0.7201 - acc: 0.5088
3584/4566 [======================>.......] - ETA: 3:18 - loss: 0.7197 - acc: 0.5092
3648/4566 [======================>.......] - ETA: 3:04 - loss: 0.7195 - acc: 0.5085
3712/4566 [=======================>......] - ETA: 2:50 - loss: 0.7196 - acc: 0.5078
3776/4566 [=======================>......] - ETA: 2:36 - loss: 0.7191 - acc: 0.5087
3840/4566 [========================>.....] - ETA: 2:23 - loss: 0.7189 - acc: 0.5081
3904/4566 [========================>.....] - ETA: 2:09 - loss: 0.7183 - acc: 0.5079
3968/4566 [=========================>....] - ETA: 1:56 - loss: 0.7180 - acc: 0.5083
4032/4566 [=========================>....] - ETA: 1:43 - loss: 0.7179 - acc: 0.5084
4096/4566 [=========================>....] - ETA: 1:30 - loss: 0.7180 - acc: 0.5076
4160/4566 [==========================>...] - ETA: 1:18 - loss: 0.7184 - acc: 0.5065
4224/4566 [==========================>...] - ETA: 1:06 - loss: 0.7183 - acc: 0.5066
4288/4566 [===========================>..] - ETA: 54s - loss: 0.7188 - acc: 0.5063 
4352/4566 [===========================>..] - ETA: 41s - loss: 0.7191 - acc: 0.5069
4416/4566 [============================>.] - ETA: 29s - loss: 0.7181 - acc: 0.5095
4480/4566 [============================>.] - ETA: 16s - loss: 0.7177 - acc: 0.5103
4544/4566 [============================>.] - ETA: 4s - loss: 0.7171 - acc: 0.5119 
4566/4566 [==============================] - 925s 203ms/step - loss: 0.7165 - acc: 0.5127 - val_loss: 0.6918 - val_acc: 0.5453

Epoch 00001: val_acc improved from -inf to 0.54528, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window12/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 8:30 - loss: 0.6866 - acc: 0.5156
 128/4566 [..............................] - ETA: 8:19 - loss: 0.6958 - acc: 0.5156
 192/4566 [>.............................] - ETA: 8:33 - loss: 0.6914 - acc: 0.5365
 256/4566 [>.............................] - ETA: 8:45 - loss: 0.6897 - acc: 0.5430
 320/4566 [=>............................] - ETA: 8:53 - loss: 0.6896 - acc: 0.5406
 384/4566 [=>............................] - ETA: 8:49 - loss: 0.6926 - acc: 0.5365
 448/4566 [=>............................] - ETA: 8:42 - loss: 0.6868 - acc: 0.5491
 512/4566 [==>...........................] - ETA: 8:29 - loss: 0.6959 - acc: 0.5312
 576/4566 [==>...........................] - ETA: 8:19 - loss: 0.6982 - acc: 0.5174
 640/4566 [===>..........................] - ETA: 8:07 - loss: 0.7006 - acc: 0.5094
 704/4566 [===>..........................] - ETA: 8:24 - loss: 0.6997 - acc: 0.5142
 768/4566 [====>.........................] - ETA: 8:47 - loss: 0.7032 - acc: 0.5052
 832/4566 [====>.........................] - ETA: 9:00 - loss: 0.7023 - acc: 0.5012
 896/4566 [====>.........................] - ETA: 9:14 - loss: 0.6990 - acc: 0.5056
 960/4566 [=====>........................] - ETA: 9:24 - loss: 0.6992 - acc: 0.5052
1024/4566 [=====>........................] - ETA: 9:27 - loss: 0.6974 - acc: 0.5098
1088/4566 [======>.......................] - ETA: 9:31 - loss: 0.6974 - acc: 0.5119
1152/4566 [======>.......................] - ETA: 9:13 - loss: 0.6944 - acc: 0.5174
1216/4566 [======>.......................] - ETA: 8:52 - loss: 0.6972 - acc: 0.5115
1280/4566 [=======>......................] - ETA: 8:35 - loss: 0.6952 - acc: 0.5141
1344/4566 [=======>......................] - ETA: 8:20 - loss: 0.6954 - acc: 0.5134
1408/4566 [========>.....................] - ETA: 8:06 - loss: 0.6961 - acc: 0.5142
1472/4566 [========>.....................] - ETA: 7:53 - loss: 0.6946 - acc: 0.5170
1536/4566 [=========>....................] - ETA: 7:40 - loss: 0.6943 - acc: 0.5208
1600/4566 [=========>....................] - ETA: 7:28 - loss: 0.6930 - acc: 0.5250
1664/4566 [=========>....................] - ETA: 7:15 - loss: 0.6931 - acc: 0.5258
1728/4566 [==========>...................] - ETA: 7:04 - loss: 0.6935 - acc: 0.5255
1792/4566 [==========>...................] - ETA: 6:53 - loss: 0.6924 - acc: 0.5279
1856/4566 [===========>..................] - ETA: 6:42 - loss: 0.6913 - acc: 0.5312
1920/4566 [===========>..................] - ETA: 6:30 - loss: 0.6913 - acc: 0.5339
1984/4566 [============>.................] - ETA: 6:20 - loss: 0.6929 - acc: 0.5312
2048/4566 [============>.................] - ETA: 6:14 - loss: 0.6918 - acc: 0.5347
2112/4566 [============>.................] - ETA: 6:11 - loss: 0.6925 - acc: 0.5341
2176/4566 [=============>................] - ETA: 6:07 - loss: 0.6923 - acc: 0.5326
2240/4566 [=============>................] - ETA: 6:02 - loss: 0.6931 - acc: 0.5344
2304/4566 [==============>...............] - ETA: 5:56 - loss: 0.6939 - acc: 0.5339
2368/4566 [==============>...............] - ETA: 5:50 - loss: 0.6934 - acc: 0.5363
2432/4566 [==============>...............] - ETA: 5:41 - loss: 0.6932 - acc: 0.5370
2496/4566 [===============>..............] - ETA: 5:29 - loss: 0.6945 - acc: 0.5353
2560/4566 [===============>..............] - ETA: 5:17 - loss: 0.6947 - acc: 0.5336
2624/4566 [================>.............] - ETA: 5:05 - loss: 0.6947 - acc: 0.5328
2688/4566 [================>.............] - ETA: 4:53 - loss: 0.6938 - acc: 0.5361
2752/4566 [=================>............] - ETA: 4:41 - loss: 0.6936 - acc: 0.5360
2816/4566 [=================>............] - ETA: 4:30 - loss: 0.6931 - acc: 0.5352
2880/4566 [=================>............] - ETA: 4:19 - loss: 0.6932 - acc: 0.5347
2944/4566 [==================>...........] - ETA: 4:07 - loss: 0.6926 - acc: 0.5360
3008/4566 [==================>...........] - ETA: 3:57 - loss: 0.6928 - acc: 0.5362
3072/4566 [===================>..........] - ETA: 3:46 - loss: 0.6931 - acc: 0.5371
3136/4566 [===================>..........] - ETA: 3:36 - loss: 0.6925 - acc: 0.5383
3200/4566 [====================>.........] - ETA: 3:25 - loss: 0.6921 - acc: 0.5378
3264/4566 [====================>.........] - ETA: 3:14 - loss: 0.6923 - acc: 0.5383
3328/4566 [====================>.........] - ETA: 3:04 - loss: 0.6923 - acc: 0.5382
3392/4566 [=====================>........] - ETA: 2:55 - loss: 0.6920 - acc: 0.5401
3456/4566 [=====================>........] - ETA: 2:47 - loss: 0.6916 - acc: 0.5411
3520/4566 [======================>.......] - ETA: 2:39 - loss: 0.6922 - acc: 0.5398
3584/4566 [======================>.......] - ETA: 2:31 - loss: 0.6921 - acc: 0.5391
3648/4566 [======================>.......] - ETA: 2:22 - loss: 0.6923 - acc: 0.5387
3712/4566 [=======================>......] - ETA: 2:13 - loss: 0.6923 - acc: 0.5391
3776/4566 [=======================>......] - ETA: 2:04 - loss: 0.6917 - acc: 0.5400
3840/4566 [========================>.....] - ETA: 1:53 - loss: 0.6931 - acc: 0.5380
3904/4566 [========================>.....] - ETA: 1:43 - loss: 0.6924 - acc: 0.5402
3968/4566 [=========================>....] - ETA: 1:33 - loss: 0.6910 - acc: 0.5436
4032/4566 [=========================>....] - ETA: 1:22 - loss: 0.6913 - acc: 0.5434
4096/4566 [=========================>....] - ETA: 1:12 - loss: 0.6912 - acc: 0.5435
4160/4566 [==========================>...] - ETA: 1:02 - loss: 0.6908 - acc: 0.5445
4224/4566 [==========================>...] - ETA: 52s - loss: 0.6908 - acc: 0.5433 
4288/4566 [===========================>..] - ETA: 42s - loss: 0.6910 - acc: 0.5429
4352/4566 [===========================>..] - ETA: 32s - loss: 0.6904 - acc: 0.5448
4416/4566 [============================>.] - ETA: 22s - loss: 0.6898 - acc: 0.5466
4480/4566 [============================>.] - ETA: 12s - loss: 0.6897 - acc: 0.5467
4544/4566 [============================>.] - ETA: 3s - loss: 0.6900 - acc: 0.5473 
4566/4566 [==============================] - 703s 154ms/step - loss: 0.6903 - acc: 0.5473 - val_loss: 0.6880 - val_acc: 0.5846

Epoch 00002: val_acc improved from 0.54528 to 0.58465, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window12/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 3/10

  64/4566 [..............................] - ETA: 15:59 - loss: 0.7127 - acc: 0.4688
 128/4566 [..............................] - ETA: 17:10 - loss: 0.6909 - acc: 0.5234
 192/4566 [>.............................] - ETA: 16:36 - loss: 0.6927 - acc: 0.5208
 256/4566 [>.............................] - ETA: 16:25 - loss: 0.6879 - acc: 0.5312
 320/4566 [=>............................] - ETA: 16:11 - loss: 0.6887 - acc: 0.5281
 384/4566 [=>............................] - ETA: 15:58 - loss: 0.6862 - acc: 0.5312
 448/4566 [=>............................] - ETA: 15:25 - loss: 0.6936 - acc: 0.5201
 512/4566 [==>...........................] - ETA: 14:19 - loss: 0.6959 - acc: 0.5215
 576/4566 [==>...........................] - ETA: 13:22 - loss: 0.6900 - acc: 0.5312
 640/4566 [===>..........................] - ETA: 12:33 - loss: 0.6907 - acc: 0.5312
 704/4566 [===>..........................] - ETA: 11:51 - loss: 0.6904 - acc: 0.5327
 768/4566 [====>.........................] - ETA: 11:15 - loss: 0.6913 - acc: 0.5352
 832/4566 [====>.........................] - ETA: 10:42 - loss: 0.6887 - acc: 0.5385
 896/4566 [====>.........................] - ETA: 10:14 - loss: 0.6882 - acc: 0.5391
 960/4566 [=====>........................] - ETA: 9:48 - loss: 0.6889 - acc: 0.5385 
1024/4566 [=====>........................] - ETA: 9:23 - loss: 0.6880 - acc: 0.5371
1088/4566 [======>.......................] - ETA: 8:59 - loss: 0.6865 - acc: 0.5414
1152/4566 [======>.......................] - ETA: 8:41 - loss: 0.6836 - acc: 0.5486
1216/4566 [======>.......................] - ETA: 8:24 - loss: 0.6846 - acc: 0.5469
1280/4566 [=======>......................] - ETA: 8:10 - loss: 0.6858 - acc: 0.5453
1344/4566 [=======>......................] - ETA: 7:55 - loss: 0.6854 - acc: 0.5446
1408/4566 [========>.....................] - ETA: 7:43 - loss: 0.6881 - acc: 0.5440
1472/4566 [========>.....................] - ETA: 7:41 - loss: 0.6880 - acc: 0.5435
1536/4566 [=========>....................] - ETA: 7:42 - loss: 0.6888 - acc: 0.5449
1600/4566 [=========>....................] - ETA: 7:41 - loss: 0.6881 - acc: 0.5481
1664/4566 [=========>....................] - ETA: 7:38 - loss: 0.6879 - acc: 0.5487
1728/4566 [==========>...................] - ETA: 7:36 - loss: 0.6880 - acc: 0.5463
1792/4566 [==========>...................] - ETA: 7:32 - loss: 0.6877 - acc: 0.5469
1856/4566 [===========>..................] - ETA: 7:28 - loss: 0.6873 - acc: 0.5469
1920/4566 [===========>..................] - ETA: 7:16 - loss: 0.6862 - acc: 0.5500
1984/4566 [============>.................] - ETA: 7:00 - loss: 0.6873 - acc: 0.5484
2048/4566 [============>.................] - ETA: 6:45 - loss: 0.6862 - acc: 0.5508
2112/4566 [============>.................] - ETA: 6:30 - loss: 0.6846 - acc: 0.5521
2176/4566 [=============>................] - ETA: 6:16 - loss: 0.6841 - acc: 0.5547
2240/4566 [=============>................] - ETA: 6:03 - loss: 0.6835 - acc: 0.5545
2304/4566 [==============>...............] - ETA: 5:50 - loss: 0.6837 - acc: 0.5525
2368/4566 [==============>...............] - ETA: 5:38 - loss: 0.6840 - acc: 0.5515
2432/4566 [==============>...............] - ETA: 5:26 - loss: 0.6840 - acc: 0.5518
2496/4566 [===============>..............] - ETA: 5:15 - loss: 0.6836 - acc: 0.5525
2560/4566 [===============>..............] - ETA: 5:04 - loss: 0.6836 - acc: 0.5527
2624/4566 [================>.............] - ETA: 4:53 - loss: 0.6838 - acc: 0.5530
2688/4566 [================>.............] - ETA: 4:42 - loss: 0.6834 - acc: 0.5521
2752/4566 [=================>............] - ETA: 4:31 - loss: 0.6826 - acc: 0.5534
2816/4566 [=================>............] - ETA: 4:21 - loss: 0.6814 - acc: 0.5558
2880/4566 [=================>............] - ETA: 4:14 - loss: 0.6804 - acc: 0.5590
2944/4566 [==================>...........] - ETA: 4:07 - loss: 0.6795 - acc: 0.5608
3008/4566 [==================>...........] - ETA: 3:59 - loss: 0.6801 - acc: 0.5598
3072/4566 [===================>..........] - ETA: 3:52 - loss: 0.6800 - acc: 0.5615
3136/4566 [===================>..........] - ETA: 3:43 - loss: 0.6796 - acc: 0.5619
3200/4566 [====================>.........] - ETA: 3:35 - loss: 0.6783 - acc: 0.5634
3264/4566 [====================>.........] - ETA: 3:26 - loss: 0.6779 - acc: 0.5643
3328/4566 [====================>.........] - ETA: 3:15 - loss: 0.6781 - acc: 0.5646
3392/4566 [=====================>........] - ETA: 3:04 - loss: 0.6782 - acc: 0.5649
3456/4566 [=====================>........] - ETA: 2:53 - loss: 0.6783 - acc: 0.5660
3520/4566 [======================>.......] - ETA: 2:43 - loss: 0.6785 - acc: 0.5662
3584/4566 [======================>.......] - ETA: 2:32 - loss: 0.6798 - acc: 0.5645
3648/4566 [======================>.......] - ETA: 2:22 - loss: 0.6802 - acc: 0.5644
3712/4566 [=======================>......] - ETA: 2:11 - loss: 0.6803 - acc: 0.5649
3776/4566 [=======================>......] - ETA: 2:01 - loss: 0.6801 - acc: 0.5654
3840/4566 [========================>.....] - ETA: 1:50 - loss: 0.6799 - acc: 0.5664
3904/4566 [========================>.....] - ETA: 1:40 - loss: 0.6797 - acc: 0.5676
3968/4566 [=========================>....] - ETA: 1:30 - loss: 0.6792 - acc: 0.5688
4032/4566 [=========================>....] - ETA: 1:20 - loss: 0.6789 - acc: 0.5694
4096/4566 [=========================>....] - ETA: 1:10 - loss: 0.6785 - acc: 0.5713
4160/4566 [==========================>...] - ETA: 1:00 - loss: 0.6786 - acc: 0.5712
4224/4566 [==========================>...] - ETA: 50s - loss: 0.6784 - acc: 0.5727 
4288/4566 [===========================>..] - ETA: 41s - loss: 0.6781 - acc: 0.5737
4352/4566 [===========================>..] - ETA: 32s - loss: 0.6785 - acc: 0.5731
4416/4566 [============================>.] - ETA: 22s - loss: 0.6785 - acc: 0.5736
4480/4566 [============================>.] - ETA: 13s - loss: 0.6780 - acc: 0.5752
4544/4566 [============================>.] - ETA: 3s - loss: 0.6784 - acc: 0.5746 
4566/4566 [==============================] - 735s 161ms/step - loss: 0.6782 - acc: 0.5747 - val_loss: 0.6738 - val_acc: 0.5787

Epoch 00003: val_acc did not improve from 0.58465
Epoch 4/10

  64/4566 [..............................] - ETA: 8:00 - loss: 0.6781 - acc: 0.5469
 128/4566 [..............................] - ETA: 8:19 - loss: 0.6830 - acc: 0.5625
 192/4566 [>.............................] - ETA: 8:16 - loss: 0.6677 - acc: 0.6042
 256/4566 [>.............................] - ETA: 8:03 - loss: 0.6791 - acc: 0.5859
 320/4566 [=>............................] - ETA: 7:56 - loss: 0.6673 - acc: 0.6000
 384/4566 [=>............................] - ETA: 7:48 - loss: 0.6639 - acc: 0.6016
 448/4566 [=>............................] - ETA: 7:43 - loss: 0.6669 - acc: 0.5893
 512/4566 [==>...........................] - ETA: 7:32 - loss: 0.6724 - acc: 0.5820
 576/4566 [==>...........................] - ETA: 7:22 - loss: 0.6684 - acc: 0.5885
 640/4566 [===>..........................] - ETA: 7:14 - loss: 0.6698 - acc: 0.5953
 704/4566 [===>..........................] - ETA: 7:07 - loss: 0.6683 - acc: 0.6009
 768/4566 [====>.........................] - ETA: 6:58 - loss: 0.6674 - acc: 0.6068
 832/4566 [====>.........................] - ETA: 6:49 - loss: 0.6681 - acc: 0.6010
 896/4566 [====>.........................] - ETA: 6:43 - loss: 0.6681 - acc: 0.5960
 960/4566 [=====>........................] - ETA: 6:58 - loss: 0.6671 - acc: 0.5958
1024/4566 [=====>........................] - ETA: 7:17 - loss: 0.6663 - acc: 0.5947
1088/4566 [======>.......................] - ETA: 7:28 - loss: 0.6666 - acc: 0.5956
1152/4566 [======>.......................] - ETA: 7:38 - loss: 0.6696 - acc: 0.5885
1216/4566 [======>.......................] - ETA: 7:45 - loss: 0.6689 - acc: 0.5896
1280/4566 [=======>......................] - ETA: 7:50 - loss: 0.6684 - acc: 0.5898
1344/4566 [=======>......................] - ETA: 7:49 - loss: 0.6663 - acc: 0.5938
1408/4566 [========>.....................] - ETA: 7:37 - loss: 0.6685 - acc: 0.5895
1472/4566 [========>.....................] - ETA: 7:22 - loss: 0.6686 - acc: 0.5890
1536/4566 [=========>....................] - ETA: 7:07 - loss: 0.6651 - acc: 0.5931
1600/4566 [=========>....................] - ETA: 6:52 - loss: 0.6679 - acc: 0.5913
1664/4566 [=========>....................] - ETA: 6:40 - loss: 0.6692 - acc: 0.5895
1728/4566 [==========>...................] - ETA: 6:29 - loss: 0.6690 - acc: 0.5897
1792/4566 [==========>...................] - ETA: 6:16 - loss: 0.6668 - acc: 0.5932
1856/4566 [===========>..................] - ETA: 6:04 - loss: 0.6685 - acc: 0.5900
1920/4566 [===========>..................] - ETA: 5:53 - loss: 0.6689 - acc: 0.5901
1984/4566 [============>.................] - ETA: 5:42 - loss: 0.6711 - acc: 0.5862
2048/4566 [============>.................] - ETA: 5:31 - loss: 0.6724 - acc: 0.5854
2112/4566 [============>.................] - ETA: 5:20 - loss: 0.6726 - acc: 0.5848
2176/4566 [=============>................] - ETA: 5:09 - loss: 0.6717 - acc: 0.5869
2240/4566 [=============>................] - ETA: 4:59 - loss: 0.6706 - acc: 0.5911
2304/4566 [==============>...............] - ETA: 4:50 - loss: 0.6706 - acc: 0.5903
2368/4566 [==============>...............] - ETA: 4:42 - loss: 0.6719 - acc: 0.5874
2432/4566 [==============>...............] - ETA: 4:37 - loss: 0.6715 - acc: 0.5888
2496/4566 [===============>..............] - ETA: 4:35 - loss: 0.6709 - acc: 0.5901
2560/4566 [===============>..............] - ETA: 4:31 - loss: 0.6712 - acc: 0.5895
2624/4566 [================>.............] - ETA: 4:26 - loss: 0.6720 - acc: 0.5869
2688/4566 [================>.............] - ETA: 4:20 - loss: 0.6706 - acc: 0.5893
2752/4566 [=================>............] - ETA: 4:15 - loss: 0.6715 - acc: 0.5872
2816/4566 [=================>............] - ETA: 4:09 - loss: 0.6712 - acc: 0.5888
2880/4566 [=================>............] - ETA: 4:00 - loss: 0.6715 - acc: 0.5868
2944/4566 [==================>...........] - ETA: 3:50 - loss: 0.6736 - acc: 0.5829
3008/4566 [==================>...........] - ETA: 3:40 - loss: 0.6732 - acc: 0.5844
3072/4566 [===================>..........] - ETA: 3:30 - loss: 0.6726 - acc: 0.5869
3136/4566 [===================>..........] - ETA: 3:20 - loss: 0.6726 - acc: 0.5858
3200/4566 [====================>.........] - ETA: 3:09 - loss: 0.6734 - acc: 0.5844
3264/4566 [====================>.........] - ETA: 2:59 - loss: 0.6742 - acc: 0.5846
3328/4566 [====================>.........] - ETA: 2:50 - loss: 0.6733 - acc: 0.5856
3392/4566 [=====================>........] - ETA: 2:40 - loss: 0.6741 - acc: 0.5840
3456/4566 [=====================>........] - ETA: 2:30 - loss: 0.6760 - acc: 0.5830
3520/4566 [======================>.......] - ETA: 2:21 - loss: 0.6771 - acc: 0.5807
3584/4566 [======================>.......] - ETA: 2:12 - loss: 0.6780 - acc: 0.5792
3648/4566 [======================>.......] - ETA: 2:03 - loss: 0.6782 - acc: 0.5779
3712/4566 [=======================>......] - ETA: 1:54 - loss: 0.6788 - acc: 0.5773
3776/4566 [=======================>......] - ETA: 1:45 - loss: 0.6791 - acc: 0.5765
3840/4566 [========================>.....] - ETA: 1:37 - loss: 0.6792 - acc: 0.5763
3904/4566 [========================>.....] - ETA: 1:29 - loss: 0.6790 - acc: 0.5758
3968/4566 [=========================>....] - ETA: 1:21 - loss: 0.6788 - acc: 0.5764
4032/4566 [=========================>....] - ETA: 1:13 - loss: 0.6789 - acc: 0.5759
4096/4566 [=========================>....] - ETA: 1:05 - loss: 0.6785 - acc: 0.5771
4160/4566 [==========================>...] - ETA: 57s - loss: 0.6786 - acc: 0.5772 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6789 - acc: 0.5762
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6787 - acc: 0.5756
4352/4566 [===========================>..] - ETA: 30s - loss: 0.6786 - acc: 0.5756
4416/4566 [============================>.] - ETA: 21s - loss: 0.6785 - acc: 0.5750
4480/4566 [============================>.] - ETA: 12s - loss: 0.6785 - acc: 0.5748
4544/4566 [============================>.] - ETA: 3s - loss: 0.6784 - acc: 0.5746 
4566/4566 [==============================] - 662s 145ms/step - loss: 0.6780 - acc: 0.5751 - val_loss: 0.6787 - val_acc: 0.5886

Epoch 00004: val_acc improved from 0.58465 to 0.58858, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window12/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 5/10

  64/4566 [..............................] - ETA: 8:53 - loss: 0.6292 - acc: 0.6406
 128/4566 [..............................] - ETA: 8:42 - loss: 0.6340 - acc: 0.6250
 192/4566 [>.............................] - ETA: 8:39 - loss: 0.6583 - acc: 0.6042
 256/4566 [>.............................] - ETA: 8:19 - loss: 0.6597 - acc: 0.6016
 320/4566 [=>............................] - ETA: 8:11 - loss: 0.6592 - acc: 0.6031
 384/4566 [=>............................] - ETA: 7:58 - loss: 0.6560 - acc: 0.6094
 448/4566 [=>............................] - ETA: 7:47 - loss: 0.6596 - acc: 0.6049
 512/4566 [==>...........................] - ETA: 7:37 - loss: 0.6586 - acc: 0.6035
 576/4566 [==>...........................] - ETA: 7:35 - loss: 0.6613 - acc: 0.5938
 640/4566 [===>..........................] - ETA: 8:08 - loss: 0.6629 - acc: 0.5906
 704/4566 [===>..........................] - ETA: 8:34 - loss: 0.6621 - acc: 0.5923
 768/4566 [====>.........................] - ETA: 8:51 - loss: 0.6607 - acc: 0.5964
 832/4566 [====>.........................] - ETA: 9:04 - loss: 0.6631 - acc: 0.5901
 896/4566 [====>.........................] - ETA: 9:11 - loss: 0.6660 - acc: 0.5893
 960/4566 [=====>........................] - ETA: 9:15 - loss: 0.6676 - acc: 0.5896
1024/4566 [=====>........................] - ETA: 9:14 - loss: 0.6704 - acc: 0.5850
1088/4566 [======>.......................] - ETA: 8:53 - loss: 0.6721 - acc: 0.5763
1152/4566 [======>.......................] - ETA: 8:36 - loss: 0.6726 - acc: 0.5755
1216/4566 [======>.......................] - ETA: 8:21 - loss: 0.6736 - acc: 0.5765
1280/4566 [=======>......................] - ETA: 8:07 - loss: 0.6690 - acc: 0.5867
1344/4566 [=======>......................] - ETA: 7:53 - loss: 0.6720 - acc: 0.5833
1408/4566 [========>.....................] - ETA: 7:38 - loss: 0.6746 - acc: 0.5774
1472/4566 [========>.....................] - ETA: 7:24 - loss: 0.6750 - acc: 0.5795
1536/4566 [=========>....................] - ETA: 7:10 - loss: 0.6742 - acc: 0.5807
1600/4566 [=========>....................] - ETA: 6:57 - loss: 0.6735 - acc: 0.5794
1664/4566 [=========>....................] - ETA: 6:45 - loss: 0.6731 - acc: 0.5781
1728/4566 [==========>...................] - ETA: 6:33 - loss: 0.6715 - acc: 0.5822
1792/4566 [==========>...................] - ETA: 6:22 - loss: 0.6717 - acc: 0.5815
1856/4566 [===========>..................] - ETA: 6:11 - loss: 0.6705 - acc: 0.5841
1920/4566 [===========>..................] - ETA: 5:59 - loss: 0.6709 - acc: 0.5818
1984/4566 [============>.................] - ETA: 5:48 - loss: 0.6727 - acc: 0.5796
2048/4566 [============>.................] - ETA: 5:37 - loss: 0.6707 - acc: 0.5820
2112/4566 [============>.................] - ETA: 5:32 - loss: 0.6712 - acc: 0.5800
2176/4566 [=============>................] - ETA: 5:29 - loss: 0.6701 - acc: 0.5823
2240/4566 [=============>................] - ETA: 5:25 - loss: 0.6698 - acc: 0.5808
2304/4566 [==============>...............] - ETA: 5:20 - loss: 0.6688 - acc: 0.5825
2368/4566 [==============>...............] - ETA: 5:15 - loss: 0.6667 - acc: 0.5870
2432/4566 [==============>...............] - ETA: 5:10 - loss: 0.6669 - acc: 0.5876
2496/4566 [===============>..............] - ETA: 5:03 - loss: 0.6679 - acc: 0.5869
2560/4566 [===============>..............] - ETA: 4:54 - loss: 0.6671 - acc: 0.5906
2624/4566 [================>.............] - ETA: 4:42 - loss: 0.6676 - acc: 0.5911
2688/4566 [================>.............] - ETA: 4:32 - loss: 0.6678 - acc: 0.5915
2752/4566 [=================>............] - ETA: 4:21 - loss: 0.6665 - acc: 0.5938
2816/4566 [=================>............] - ETA: 4:10 - loss: 0.6681 - acc: 0.5902
2880/4566 [=================>............] - ETA: 4:00 - loss: 0.6674 - acc: 0.5913
2944/4566 [==================>...........] - ETA: 3:50 - loss: 0.6679 - acc: 0.5914
3008/4566 [==================>...........] - ETA: 3:39 - loss: 0.6699 - acc: 0.5878
3072/4566 [===================>..........] - ETA: 3:30 - loss: 0.6697 - acc: 0.5882
3136/4566 [===================>..........] - ETA: 3:20 - loss: 0.6699 - acc: 0.5880
3200/4566 [====================>.........] - ETA: 3:10 - loss: 0.6692 - acc: 0.5903
3264/4566 [====================>.........] - ETA: 3:00 - loss: 0.6691 - acc: 0.5904
3328/4566 [====================>.........] - ETA: 2:50 - loss: 0.6694 - acc: 0.5904
3392/4566 [=====================>........] - ETA: 2:40 - loss: 0.6692 - acc: 0.5905
3456/4566 [=====================>........] - ETA: 2:31 - loss: 0.6693 - acc: 0.5911
3520/4566 [======================>.......] - ETA: 2:22 - loss: 0.6693 - acc: 0.5903
3584/4566 [======================>.......] - ETA: 2:14 - loss: 0.6695 - acc: 0.5904
3648/4566 [======================>.......] - ETA: 2:07 - loss: 0.6697 - acc: 0.5899
3712/4566 [=======================>......] - ETA: 1:59 - loss: 0.6694 - acc: 0.5913
3776/4566 [=======================>......] - ETA: 1:51 - loss: 0.6693 - acc: 0.5919
3840/4566 [========================>.....] - ETA: 1:43 - loss: 0.6690 - acc: 0.5924
3904/4566 [========================>.....] - ETA: 1:35 - loss: 0.6694 - acc: 0.5917
3968/4566 [=========================>....] - ETA: 1:26 - loss: 0.6691 - acc: 0.5927
4032/4566 [=========================>....] - ETA: 1:17 - loss: 0.6688 - acc: 0.5940
4096/4566 [=========================>....] - ETA: 1:07 - loss: 0.6689 - acc: 0.5938
4160/4566 [==========================>...] - ETA: 58s - loss: 0.6692 - acc: 0.5940 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6698 - acc: 0.5938
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6705 - acc: 0.5928
4352/4566 [===========================>..] - ETA: 30s - loss: 0.6709 - acc: 0.5915
4416/4566 [============================>.] - ETA: 21s - loss: 0.6710 - acc: 0.5917
4480/4566 [============================>.] - ETA: 12s - loss: 0.6713 - acc: 0.5915
4544/4566 [============================>.] - ETA: 3s - loss: 0.6716 - acc: 0.5907 
4566/4566 [==============================] - 658s 144ms/step - loss: 0.6713 - acc: 0.5911 - val_loss: 0.6684 - val_acc: 0.5807

Epoch 00005: val_acc did not improve from 0.58858
Epoch 6/10

  64/4566 [..............................] - ETA: 8:41 - loss: 0.6845 - acc: 0.5781
 128/4566 [..............................] - ETA: 8:41 - loss: 0.6927 - acc: 0.5391
 192/4566 [>.............................] - ETA: 8:34 - loss: 0.6869 - acc: 0.5469
 256/4566 [>.............................] - ETA: 9:24 - loss: 0.6820 - acc: 0.5391
 320/4566 [=>............................] - ETA: 10:25 - loss: 0.6788 - acc: 0.5469
 384/4566 [=>............................] - ETA: 11:06 - loss: 0.6693 - acc: 0.5651
 448/4566 [=>............................] - ETA: 11:29 - loss: 0.6719 - acc: 0.5670
 512/4566 [==>...........................] - ETA: 11:42 - loss: 0.6734 - acc: 0.5742
 576/4566 [==>...........................] - ETA: 11:54 - loss: 0.6694 - acc: 0.5781
 640/4566 [===>..........................] - ETA: 11:52 - loss: 0.6676 - acc: 0.5859
 704/4566 [===>..........................] - ETA: 11:42 - loss: 0.6692 - acc: 0.5795
 768/4566 [====>.........................] - ETA: 11:13 - loss: 0.6684 - acc: 0.5807
 832/4566 [====>.........................] - ETA: 10:40 - loss: 0.6675 - acc: 0.5841
 896/4566 [====>.........................] - ETA: 10:12 - loss: 0.6679 - acc: 0.5871
 960/4566 [=====>........................] - ETA: 9:44 - loss: 0.6705 - acc: 0.5833 
1024/4566 [=====>........................] - ETA: 9:20 - loss: 0.6688 - acc: 0.5898
1088/4566 [======>.......................] - ETA: 8:59 - loss: 0.6693 - acc: 0.5910
1152/4566 [======>.......................] - ETA: 8:38 - loss: 0.6686 - acc: 0.5920
1216/4566 [======>.......................] - ETA: 8:20 - loss: 0.6710 - acc: 0.5888
1280/4566 [=======>......................] - ETA: 8:05 - loss: 0.6714 - acc: 0.5891
1344/4566 [=======>......................] - ETA: 7:50 - loss: 0.6704 - acc: 0.5938
1408/4566 [========>.....................] - ETA: 7:36 - loss: 0.6724 - acc: 0.5938
1472/4566 [========>.....................] - ETA: 7:24 - loss: 0.6723 - acc: 0.5938
1536/4566 [=========>....................] - ETA: 7:10 - loss: 0.6724 - acc: 0.5911
1600/4566 [=========>....................] - ETA: 6:57 - loss: 0.6720 - acc: 0.5906
1664/4566 [=========>....................] - ETA: 6:44 - loss: 0.6720 - acc: 0.5895
1728/4566 [==========>...................] - ETA: 6:34 - loss: 0.6731 - acc: 0.5874
1792/4566 [==========>...................] - ETA: 6:29 - loss: 0.6722 - acc: 0.5887
1856/4566 [===========>..................] - ETA: 6:27 - loss: 0.6725 - acc: 0.5878
1920/4566 [===========>..................] - ETA: 6:24 - loss: 0.6727 - acc: 0.5870
1984/4566 [============>.................] - ETA: 6:20 - loss: 0.6725 - acc: 0.5872
2048/4566 [============>.................] - ETA: 6:16 - loss: 0.6732 - acc: 0.5850
2112/4566 [============>.................] - ETA: 6:10 - loss: 0.6737 - acc: 0.5843
2176/4566 [=============>................] - ETA: 6:05 - loss: 0.6739 - acc: 0.5846
2240/4566 [=============>................] - ETA: 5:54 - loss: 0.6740 - acc: 0.5848
2304/4566 [==============>...............] - ETA: 5:41 - loss: 0.6737 - acc: 0.5846
2368/4566 [==============>...............] - ETA: 5:28 - loss: 0.6736 - acc: 0.5849
2432/4566 [==============>...............] - ETA: 5:17 - loss: 0.6727 - acc: 0.5859
2496/4566 [===============>..............] - ETA: 5:06 - loss: 0.6724 - acc: 0.5853
2560/4566 [===============>..............] - ETA: 4:55 - loss: 0.6703 - acc: 0.5895
2624/4566 [================>.............] - ETA: 4:44 - loss: 0.6713 - acc: 0.5877
2688/4566 [================>.............] - ETA: 4:33 - loss: 0.6713 - acc: 0.5871
2752/4566 [=================>............] - ETA: 4:22 - loss: 0.6710 - acc: 0.5879
2816/4566 [=================>............] - ETA: 4:12 - loss: 0.6708 - acc: 0.5881
2880/4566 [=================>............] - ETA: 4:01 - loss: 0.6712 - acc: 0.5885
2944/4566 [==================>...........] - ETA: 3:51 - loss: 0.6700 - acc: 0.5904
3008/4566 [==================>...........] - ETA: 3:41 - loss: 0.6699 - acc: 0.5898
3072/4566 [===================>..........] - ETA: 3:31 - loss: 0.6698 - acc: 0.5905
3136/4566 [===================>..........] - ETA: 3:21 - loss: 0.6695 - acc: 0.5906
3200/4566 [====================>.........] - ETA: 3:11 - loss: 0.6695 - acc: 0.5891
3264/4566 [====================>.........] - ETA: 3:03 - loss: 0.6686 - acc: 0.5898
3328/4566 [====================>.........] - ETA: 2:56 - loss: 0.6701 - acc: 0.5874
3392/4566 [=====================>........] - ETA: 2:48 - loss: 0.6705 - acc: 0.5858
3456/4566 [=====================>........] - ETA: 2:40 - loss: 0.6700 - acc: 0.5868
3520/4566 [======================>.......] - ETA: 2:32 - loss: 0.6705 - acc: 0.5855
3584/4566 [======================>.......] - ETA: 2:24 - loss: 0.6695 - acc: 0.5882
3648/4566 [======================>.......] - ETA: 2:15 - loss: 0.6693 - acc: 0.5899
3712/4566 [=======================>......] - ETA: 2:06 - loss: 0.6695 - acc: 0.5897
3776/4566 [=======================>......] - ETA: 1:56 - loss: 0.6697 - acc: 0.5895
3840/4566 [========================>.....] - ETA: 1:46 - loss: 0.6701 - acc: 0.5888
3904/4566 [========================>.....] - ETA: 1:36 - loss: 0.6688 - acc: 0.5914
3968/4566 [=========================>....] - ETA: 1:27 - loss: 0.6686 - acc: 0.5900
4032/4566 [=========================>....] - ETA: 1:17 - loss: 0.6690 - acc: 0.5893
4096/4566 [=========================>....] - ETA: 1:07 - loss: 0.6692 - acc: 0.5884
4160/4566 [==========================>...] - ETA: 58s - loss: 0.6690 - acc: 0.5889 
4224/4566 [==========================>...] - ETA: 49s - loss: 0.6692 - acc: 0.5890
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6687 - acc: 0.5900
4352/4566 [===========================>..] - ETA: 30s - loss: 0.6681 - acc: 0.5912
4416/4566 [============================>.] - ETA: 21s - loss: 0.6676 - acc: 0.5924
4480/4566 [============================>.] - ETA: 12s - loss: 0.6672 - acc: 0.5926
4544/4566 [============================>.] - ETA: 3s - loss: 0.6675 - acc: 0.5926 
4566/4566 [==============================] - 669s 147ms/step - loss: 0.6678 - acc: 0.5920 - val_loss: 0.6779 - val_acc: 0.5925

Epoch 00006: val_acc improved from 0.58858 to 0.59252, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window12/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 7/10

  64/4566 [..............................] - ETA: 16:21 - loss: 0.6393 - acc: 0.6406
 128/4566 [..............................] - ETA: 16:05 - loss: 0.6568 - acc: 0.6406
 192/4566 [>.............................] - ETA: 16:14 - loss: 0.6503 - acc: 0.6406
 256/4566 [>.............................] - ETA: 15:58 - loss: 0.6680 - acc: 0.6211
 320/4566 [=>............................] - ETA: 15:24 - loss: 0.6640 - acc: 0.6156
 384/4566 [=>............................] - ETA: 14:36 - loss: 0.6672 - acc: 0.6042
 448/4566 [=>............................] - ETA: 13:31 - loss: 0.6656 - acc: 0.6049
 512/4566 [==>...........................] - ETA: 12:44 - loss: 0.6686 - acc: 0.6035
 576/4566 [==>...........................] - ETA: 12:00 - loss: 0.6643 - acc: 0.6111
 640/4566 [===>..........................] - ETA: 11:19 - loss: 0.6702 - acc: 0.6047
 704/4566 [===>..........................] - ETA: 10:45 - loss: 0.6753 - acc: 0.5966
 768/4566 [====>.........................] - ETA: 10:18 - loss: 0.6805 - acc: 0.5872
 832/4566 [====>.........................] - ETA: 9:56 - loss: 0.6791 - acc: 0.5889 
 896/4566 [====>.........................] - ETA: 9:33 - loss: 0.6763 - acc: 0.5960
 960/4566 [=====>........................] - ETA: 9:11 - loss: 0.6785 - acc: 0.5927
1024/4566 [=====>........................] - ETA: 8:50 - loss: 0.6757 - acc: 0.5967
1088/4566 [======>.......................] - ETA: 8:29 - loss: 0.6775 - acc: 0.5928
1152/4566 [======>.......................] - ETA: 8:11 - loss: 0.6766 - acc: 0.5938
1216/4566 [======>.......................] - ETA: 7:55 - loss: 0.6742 - acc: 0.5970
1280/4566 [=======>......................] - ETA: 7:41 - loss: 0.6727 - acc: 0.5984
1344/4566 [=======>......................] - ETA: 7:32 - loss: 0.6739 - acc: 0.5990
1408/4566 [========>.....................] - ETA: 7:33 - loss: 0.6746 - acc: 0.5994
1472/4566 [========>.....................] - ETA: 7:35 - loss: 0.6722 - acc: 0.6039
1536/4566 [=========>....................] - ETA: 7:37 - loss: 0.6738 - acc: 0.6022
1600/4566 [=========>....................] - ETA: 7:36 - loss: 0.6732 - acc: 0.6000
1664/4566 [=========>....................] - ETA: 7:34 - loss: 0.6719 - acc: 0.6016
1728/4566 [==========>...................] - ETA: 7:32 - loss: 0.6728 - acc: 0.5995
1792/4566 [==========>...................] - ETA: 7:27 - loss: 0.6720 - acc: 0.5999
1856/4566 [===========>..................] - ETA: 7:15 - loss: 0.6728 - acc: 0.5975
1920/4566 [===========>..................] - ETA: 7:00 - loss: 0.6726 - acc: 0.5974
1984/4566 [============>.................] - ETA: 6:46 - loss: 0.6717 - acc: 0.5988
2048/4566 [============>.................] - ETA: 6:33 - loss: 0.6723 - acc: 0.5967
2112/4566 [============>.................] - ETA: 6:19 - loss: 0.6722 - acc: 0.5952
2176/4566 [=============>................] - ETA: 6:06 - loss: 0.6723 - acc: 0.5933
2240/4566 [=============>................] - ETA: 5:53 - loss: 0.6723 - acc: 0.5938
2304/4566 [==============>...............] - ETA: 5:40 - loss: 0.6726 - acc: 0.5933
2368/4566 [==============>...............] - ETA: 5:27 - loss: 0.6724 - acc: 0.5929
2432/4566 [==============>...............] - ETA: 5:16 - loss: 0.6719 - acc: 0.5925
2496/4566 [===============>..............] - ETA: 5:05 - loss: 0.6713 - acc: 0.5950
2560/4566 [===============>..............] - ETA: 4:54 - loss: 0.6729 - acc: 0.5914
2624/4566 [================>.............] - ETA: 4:43 - loss: 0.6717 - acc: 0.5949
2688/4566 [================>.............] - ETA: 4:33 - loss: 0.6714 - acc: 0.5964
2752/4566 [=================>............] - ETA: 4:23 - loss: 0.6721 - acc: 0.5938
2816/4566 [=================>............] - ETA: 4:17 - loss: 0.6708 - acc: 0.5959
2880/4566 [=================>............] - ETA: 4:10 - loss: 0.6710 - acc: 0.5958
2944/4566 [==================>...........] - ETA: 4:04 - loss: 0.6715 - acc: 0.5948
3008/4566 [==================>...........] - ETA: 3:57 - loss: 0.6719 - acc: 0.5957
3072/4566 [===================>..........] - ETA: 3:49 - loss: 0.6719 - acc: 0.5951
3136/4566 [===================>..........] - ETA: 3:42 - loss: 0.6703 - acc: 0.5966
3200/4566 [====================>.........] - ETA: 3:33 - loss: 0.6703 - acc: 0.5959
3264/4566 [====================>.........] - ETA: 3:22 - loss: 0.6710 - acc: 0.5950
3328/4566 [====================>.........] - ETA: 3:11 - loss: 0.6714 - acc: 0.5947
3392/4566 [=====================>........] - ETA: 3:00 - loss: 0.6705 - acc: 0.5952
3456/4566 [=====================>........] - ETA: 2:49 - loss: 0.6702 - acc: 0.5952
3520/4566 [======================>.......] - ETA: 2:38 - loss: 0.6711 - acc: 0.5938
3584/4566 [======================>.......] - ETA: 2:28 - loss: 0.6723 - acc: 0.5915
3648/4566 [======================>.......] - ETA: 2:17 - loss: 0.6720 - acc: 0.5918
3712/4566 [=======================>......] - ETA: 2:07 - loss: 0.6715 - acc: 0.5927
3776/4566 [=======================>......] - ETA: 1:57 - loss: 0.6719 - acc: 0.5924
3840/4566 [========================>.....] - ETA: 1:47 - loss: 0.6716 - acc: 0.5935
3904/4566 [========================>.....] - ETA: 1:37 - loss: 0.6715 - acc: 0.5938
3968/4566 [=========================>....] - ETA: 1:28 - loss: 0.6715 - acc: 0.5948
4032/4566 [=========================>....] - ETA: 1:18 - loss: 0.6716 - acc: 0.5945
4096/4566 [=========================>....] - ETA: 1:08 - loss: 0.6717 - acc: 0.5938
4160/4566 [==========================>...] - ETA: 59s - loss: 0.6720 - acc: 0.5928 
4224/4566 [==========================>...] - ETA: 49s - loss: 0.6719 - acc: 0.5921
4288/4566 [===========================>..] - ETA: 40s - loss: 0.6716 - acc: 0.5926
4352/4566 [===========================>..] - ETA: 31s - loss: 0.6710 - acc: 0.5926
4416/4566 [============================>.] - ETA: 22s - loss: 0.6702 - acc: 0.5940
4480/4566 [============================>.] - ETA: 12s - loss: 0.6702 - acc: 0.5924
4544/4566 [============================>.] - ETA: 3s - loss: 0.6704 - acc: 0.5915 
4566/4566 [==============================] - 717s 157ms/step - loss: 0.6699 - acc: 0.5926 - val_loss: 0.6753 - val_acc: 0.5925

Epoch 00007: val_acc did not improve from 0.59252
Epoch 8/10

  64/4566 [..............................] - ETA: 8:39 - loss: 0.6992 - acc: 0.5469
 128/4566 [..............................] - ETA: 8:39 - loss: 0.6979 - acc: 0.5469
 192/4566 [>.............................] - ETA: 8:22 - loss: 0.6880 - acc: 0.5573
 256/4566 [>.............................] - ETA: 8:10 - loss: 0.6745 - acc: 0.5742
 320/4566 [=>............................] - ETA: 8:05 - loss: 0.6716 - acc: 0.5781
 384/4566 [=>............................] - ETA: 8:01 - loss: 0.6693 - acc: 0.5833
 448/4566 [=>............................] - ETA: 7:56 - loss: 0.6666 - acc: 0.5938
 512/4566 [==>...........................] - ETA: 7:45 - loss: 0.6664 - acc: 0.5938
 576/4566 [==>...........................] - ETA: 7:44 - loss: 0.6680 - acc: 0.5816
 640/4566 [===>..........................] - ETA: 7:35 - loss: 0.6673 - acc: 0.5828
 704/4566 [===>..........................] - ETA: 7:26 - loss: 0.6661 - acc: 0.5810
 768/4566 [====>.........................] - ETA: 7:19 - loss: 0.6623 - acc: 0.5872
 832/4566 [====>.........................] - ETA: 7:07 - loss: 0.6631 - acc: 0.5889
 896/4566 [====>.........................] - ETA: 7:03 - loss: 0.6621 - acc: 0.5871
 960/4566 [=====>........................] - ETA: 7:17 - loss: 0.6635 - acc: 0.5896
1024/4566 [=====>........................] - ETA: 7:31 - loss: 0.6652 - acc: 0.5879
1088/4566 [======>.......................] - ETA: 7:42 - loss: 0.6605 - acc: 0.5983
1152/4566 [======>.......................] - ETA: 7:52 - loss: 0.6618 - acc: 0.5920
1216/4566 [======>.......................] - ETA: 7:55 - loss: 0.6619 - acc: 0.5921
1280/4566 [=======>......................] - ETA: 7:56 - loss: 0.6626 - acc: 0.5914
1344/4566 [=======>......................] - ETA: 7:54 - loss: 0.6604 - acc: 0.5967
1408/4566 [========>.....................] - ETA: 7:41 - loss: 0.6585 - acc: 0.6016
1472/4566 [========>.....................] - ETA: 7:27 - loss: 0.6605 - acc: 0.5999
1536/4566 [=========>....................] - ETA: 7:14 - loss: 0.6576 - acc: 0.6055
1600/4566 [=========>....................] - ETA: 7:02 - loss: 0.6579 - acc: 0.6056
1664/4566 [=========>....................] - ETA: 6:48 - loss: 0.6593 - acc: 0.6046
1728/4566 [==========>...................] - ETA: 6:37 - loss: 0.6624 - acc: 0.6001
1792/4566 [==========>...................] - ETA: 6:24 - loss: 0.6643 - acc: 0.5965
1856/4566 [===========>..................] - ETA: 6:14 - loss: 0.6660 - acc: 0.5943
1920/4566 [===========>..................] - ETA: 6:02 - loss: 0.6653 - acc: 0.5964
1984/4566 [============>.................] - ETA: 5:51 - loss: 0.6654 - acc: 0.5958
2048/4566 [============>.................] - ETA: 5:40 - loss: 0.6647 - acc: 0.5962
2112/4566 [============>.................] - ETA: 5:29 - loss: 0.6653 - acc: 0.5956
2176/4566 [=============>................] - ETA: 5:18 - loss: 0.6644 - acc: 0.5960
2240/4566 [=============>................] - ETA: 5:07 - loss: 0.6641 - acc: 0.5960
2304/4566 [==============>...............] - ETA: 4:57 - loss: 0.6639 - acc: 0.5994
2368/4566 [==============>...............] - ETA: 4:50 - loss: 0.6659 - acc: 0.5954
2432/4566 [==============>...............] - ETA: 4:48 - loss: 0.6657 - acc: 0.5942
2496/4566 [===============>..............] - ETA: 4:45 - loss: 0.6665 - acc: 0.5950
2560/4566 [===============>..............] - ETA: 4:40 - loss: 0.6671 - acc: 0.5938
2624/4566 [================>.............] - ETA: 4:36 - loss: 0.6676 - acc: 0.5938
2688/4566 [================>.............] - ETA: 4:30 - loss: 0.6673 - acc: 0.5938
2752/4566 [=================>............] - ETA: 4:24 - loss: 0.6676 - acc: 0.5941
2816/4566 [=================>............] - ETA: 4:16 - loss: 0.6674 - acc: 0.5959
2880/4566 [=================>............] - ETA: 4:05 - loss: 0.6677 - acc: 0.5944
2944/4566 [==================>...........] - ETA: 3:55 - loss: 0.6669 - acc: 0.5965
3008/4566 [==================>...........] - ETA: 3:44 - loss: 0.6674 - acc: 0.5964
3072/4566 [===================>..........] - ETA: 3:34 - loss: 0.6678 - acc: 0.5957
3136/4566 [===================>..........] - ETA: 3:23 - loss: 0.6672 - acc: 0.5969
3200/4566 [====================>.........] - ETA: 3:13 - loss: 0.6665 - acc: 0.5975
3264/4566 [====================>.........] - ETA: 3:03 - loss: 0.6671 - acc: 0.5956
3328/4566 [====================>.........] - ETA: 2:53 - loss: 0.6662 - acc: 0.5983
3392/4566 [=====================>........] - ETA: 2:43 - loss: 0.6663 - acc: 0.5979
3456/4566 [=====================>........] - ETA: 2:34 - loss: 0.6659 - acc: 0.5978
3520/4566 [======================>.......] - ETA: 2:24 - loss: 0.6649 - acc: 0.5997
3584/4566 [======================>.......] - ETA: 2:15 - loss: 0.6653 - acc: 0.5999
3648/4566 [======================>.......] - ETA: 2:06 - loss: 0.6651 - acc: 0.5998
3712/4566 [=======================>......] - ETA: 1:56 - loss: 0.6644 - acc: 0.6016
3776/4566 [=======================>......] - ETA: 1:48 - loss: 0.6635 - acc: 0.6022
3840/4566 [========================>.....] - ETA: 1:40 - loss: 0.6634 - acc: 0.6031
3904/4566 [========================>.....] - ETA: 1:32 - loss: 0.6639 - acc: 0.6027
3968/4566 [=========================>....] - ETA: 1:24 - loss: 0.6641 - acc: 0.6023
4032/4566 [=========================>....] - ETA: 1:16 - loss: 0.6634 - acc: 0.6034
4096/4566 [=========================>....] - ETA: 1:07 - loss: 0.6633 - acc: 0.6030
4160/4566 [==========================>...] - ETA: 58s - loss: 0.6625 - acc: 0.6034 
4224/4566 [==========================>...] - ETA: 49s - loss: 0.6625 - acc: 0.6032
4288/4566 [===========================>..] - ETA: 40s - loss: 0.6624 - acc: 0.6028
4352/4566 [===========================>..] - ETA: 31s - loss: 0.6632 - acc: 0.6032
4416/4566 [============================>.] - ETA: 21s - loss: 0.6643 - acc: 0.6024
4480/4566 [============================>.] - ETA: 12s - loss: 0.6638 - acc: 0.6036
4544/4566 [============================>.] - ETA: 3s - loss: 0.6641 - acc: 0.6032 
4566/4566 [==============================] - 672s 147ms/step - loss: 0.6640 - acc: 0.6027 - val_loss: 0.6715 - val_acc: 0.6004

Epoch 00008: val_acc improved from 0.59252 to 0.60039, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window12/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 9/10

  64/4566 [..............................] - ETA: 9:08 - loss: 0.7685 - acc: 0.5000
 128/4566 [..............................] - ETA: 9:07 - loss: 0.6946 - acc: 0.5703
 192/4566 [>.............................] - ETA: 9:01 - loss: 0.6874 - acc: 0.5938
 256/4566 [>.............................] - ETA: 8:43 - loss: 0.6766 - acc: 0.5938
 320/4566 [=>............................] - ETA: 8:37 - loss: 0.6614 - acc: 0.6188
 384/4566 [=>............................] - ETA: 8:35 - loss: 0.6623 - acc: 0.6172
 448/4566 [=>............................] - ETA: 8:43 - loss: 0.6542 - acc: 0.6228
 512/4566 [==>...........................] - ETA: 9:19 - loss: 0.6556 - acc: 0.6191
 576/4566 [==>...........................] - ETA: 9:58 - loss: 0.6503 - acc: 0.6302
 640/4566 [===>..........................] - ETA: 10:20 - loss: 0.6596 - acc: 0.6109
 704/4566 [===>..........................] - ETA: 10:37 - loss: 0.6539 - acc: 0.6193
 768/4566 [====>.........................] - ETA: 10:45 - loss: 0.6566 - acc: 0.6172
 832/4566 [====>.........................] - ETA: 10:45 - loss: 0.6562 - acc: 0.6178
 896/4566 [====>.........................] - ETA: 10:36 - loss: 0.6579 - acc: 0.6161
 960/4566 [=====>........................] - ETA: 10:08 - loss: 0.6588 - acc: 0.6104
1024/4566 [=====>........................] - ETA: 9:43 - loss: 0.6583 - acc: 0.6133 
1088/4566 [======>.......................] - ETA: 9:22 - loss: 0.6563 - acc: 0.6131
1152/4566 [======>.......................] - ETA: 9:04 - loss: 0.6561 - acc: 0.6128
1216/4566 [======>.......................] - ETA: 8:46 - loss: 0.6564 - acc: 0.6118
1280/4566 [=======>......................] - ETA: 8:29 - loss: 0.6538 - acc: 0.6156
1344/4566 [=======>......................] - ETA: 8:14 - loss: 0.6571 - acc: 0.6109
1408/4566 [========>.....................] - ETA: 7:57 - loss: 0.6582 - acc: 0.6094
1472/4566 [========>.....................] - ETA: 7:42 - loss: 0.6592 - acc: 0.6067
1536/4566 [=========>....................] - ETA: 7:28 - loss: 0.6618 - acc: 0.6022
1600/4566 [=========>....................] - ETA: 7:13 - loss: 0.6618 - acc: 0.6000
1664/4566 [=========>....................] - ETA: 7:01 - loss: 0.6619 - acc: 0.6016
1728/4566 [==========>...................] - ETA: 6:49 - loss: 0.6638 - acc: 0.5995
1792/4566 [==========>...................] - ETA: 6:37 - loss: 0.6643 - acc: 0.5977
1856/4566 [===========>..................] - ETA: 6:24 - loss: 0.6650 - acc: 0.5975
1920/4566 [===========>..................] - ETA: 6:19 - loss: 0.6648 - acc: 0.5974
1984/4566 [============>.................] - ETA: 6:16 - loss: 0.6651 - acc: 0.5963
2048/4566 [============>.................] - ETA: 6:13 - loss: 0.6633 - acc: 0.5977
2112/4566 [============>.................] - ETA: 6:08 - loss: 0.6628 - acc: 0.5985
2176/4566 [=============>................] - ETA: 6:03 - loss: 0.6624 - acc: 0.5993
2240/4566 [=============>................] - ETA: 5:59 - loss: 0.6638 - acc: 0.5969
2304/4566 [==============>...............] - ETA: 5:51 - loss: 0.6649 - acc: 0.5951
2368/4566 [==============>...............] - ETA: 5:39 - loss: 0.6661 - acc: 0.5921
2432/4566 [==============>...............] - ETA: 5:27 - loss: 0.6659 - acc: 0.5921
2496/4566 [===============>..............] - ETA: 5:15 - loss: 0.6639 - acc: 0.5942
2560/4566 [===============>..............] - ETA: 5:03 - loss: 0.6634 - acc: 0.5957
2624/4566 [================>.............] - ETA: 4:52 - loss: 0.6630 - acc: 0.5968
2688/4566 [================>.............] - ETA: 4:41 - loss: 0.6631 - acc: 0.5982
2752/4566 [=================>............] - ETA: 4:29 - loss: 0.6631 - acc: 0.5985
2816/4566 [=================>............] - ETA: 4:18 - loss: 0.6627 - acc: 0.6001
2880/4566 [=================>............] - ETA: 4:07 - loss: 0.6615 - acc: 0.6024
2944/4566 [==================>...........] - ETA: 3:57 - loss: 0.6607 - acc: 0.6046
3008/4566 [==================>...........] - ETA: 3:46 - loss: 0.6612 - acc: 0.6051
3072/4566 [===================>..........] - ETA: 3:36 - loss: 0.6616 - acc: 0.6038
3136/4566 [===================>..........] - ETA: 3:25 - loss: 0.6615 - acc: 0.6040
3200/4566 [====================>.........] - ETA: 3:15 - loss: 0.6608 - acc: 0.6053
3264/4566 [====================>.........] - ETA: 3:05 - loss: 0.6604 - acc: 0.6060
3328/4566 [====================>.........] - ETA: 2:56 - loss: 0.6596 - acc: 0.6073
3392/4566 [=====================>........] - ETA: 2:49 - loss: 0.6594 - acc: 0.6076
3456/4566 [=====================>........] - ETA: 2:42 - loss: 0.6600 - acc: 0.6071
3520/4566 [======================>.......] - ETA: 2:34 - loss: 0.6591 - acc: 0.6085
3584/4566 [======================>.......] - ETA: 2:26 - loss: 0.6596 - acc: 0.6074
3648/4566 [======================>.......] - ETA: 2:18 - loss: 0.6601 - acc: 0.6075
3712/4566 [=======================>......] - ETA: 2:09 - loss: 0.6600 - acc: 0.6075
3776/4566 [=======================>......] - ETA: 1:59 - loss: 0.6592 - acc: 0.6102
3840/4566 [========================>.....] - ETA: 1:49 - loss: 0.6592 - acc: 0.6107
3904/4566 [========================>.....] - ETA: 1:39 - loss: 0.6602 - acc: 0.6094
3968/4566 [=========================>....] - ETA: 1:29 - loss: 0.6601 - acc: 0.6089
4032/4566 [=========================>....] - ETA: 1:19 - loss: 0.6602 - acc: 0.6089
4096/4566 [=========================>....] - ETA: 1:09 - loss: 0.6615 - acc: 0.6072
4160/4566 [==========================>...] - ETA: 59s - loss: 0.6617 - acc: 0.6079 
4224/4566 [==========================>...] - ETA: 50s - loss: 0.6623 - acc: 0.6063
4288/4566 [===========================>..] - ETA: 40s - loss: 0.6629 - acc: 0.6045
4352/4566 [===========================>..] - ETA: 31s - loss: 0.6626 - acc: 0.6043
4416/4566 [============================>.] - ETA: 21s - loss: 0.6619 - acc: 0.6053
4480/4566 [============================>.] - ETA: 12s - loss: 0.6613 - acc: 0.6062
4544/4566 [============================>.] - ETA: 3s - loss: 0.6613 - acc: 0.6054 
4566/4566 [==============================] - 682s 149ms/step - loss: 0.6613 - acc: 0.6053 - val_loss: 0.6974 - val_acc: 0.5748

Epoch 00009: val_acc did not improve from 0.60039
Epoch 10/10

  64/4566 [..............................] - ETA: 17:27 - loss: 0.6114 - acc: 0.6562
 128/4566 [..............................] - ETA: 17:14 - loss: 0.6657 - acc: 0.5547
 192/4566 [>.............................] - ETA: 17:05 - loss: 0.6530 - acc: 0.5833
 256/4566 [>.............................] - ETA: 16:37 - loss: 0.6547 - acc: 0.5977
 320/4566 [=>............................] - ETA: 16:28 - loss: 0.6572 - acc: 0.5969
 384/4566 [=>............................] - ETA: 16:01 - loss: 0.6581 - acc: 0.5964
 448/4566 [=>............................] - ETA: 15:22 - loss: 0.6510 - acc: 0.6094
 512/4566 [==>...........................] - ETA: 14:09 - loss: 0.6538 - acc: 0.6094
 576/4566 [==>...........................] - ETA: 13:07 - loss: 0.6580 - acc: 0.6059
 640/4566 [===>..........................] - ETA: 12:20 - loss: 0.6577 - acc: 0.6047
 704/4566 [===>..........................] - ETA: 11:36 - loss: 0.6607 - acc: 0.6023
 768/4566 [====>.........................] - ETA: 10:59 - loss: 0.6641 - acc: 0.6003
 832/4566 [====>.........................] - ETA: 10:27 - loss: 0.6668 - acc: 0.5950
 896/4566 [====>.........................] - ETA: 9:58 - loss: 0.6686 - acc: 0.5848 
 960/4566 [=====>........................] - ETA: 9:33 - loss: 0.6691 - acc: 0.5885
1024/4566 [=====>........................] - ETA: 9:12 - loss: 0.6690 - acc: 0.5898
1088/4566 [======>.......................] - ETA: 8:54 - loss: 0.6683 - acc: 0.5892
1152/4566 [======>.......................] - ETA: 8:37 - loss: 0.6674 - acc: 0.5929
1216/4566 [======>.......................] - ETA: 8:18 - loss: 0.6641 - acc: 0.5987
1280/4566 [=======>......................] - ETA: 8:03 - loss: 0.6661 - acc: 0.5922
1344/4566 [=======>......................] - ETA: 7:47 - loss: 0.6667 - acc: 0.5923
1408/4566 [========>.....................] - ETA: 7:32 - loss: 0.6672 - acc: 0.5916
1472/4566 [========>.....................] - ETA: 7:23 - loss: 0.6687 - acc: 0.5924
1536/4566 [=========>....................] - ETA: 7:23 - loss: 0.6680 - acc: 0.5911
1600/4566 [=========>....................] - ETA: 7:22 - loss: 0.6667 - acc: 0.5938
1664/4566 [=========>....................] - ETA: 7:20 - loss: 0.6661 - acc: 0.5962
1728/4566 [==========>...................] - ETA: 7:16 - loss: 0.6661 - acc: 0.5972
1792/4566 [==========>...................] - ETA: 7:12 - loss: 0.6644 - acc: 0.6010
1856/4566 [===========>..................] - ETA: 7:08 - loss: 0.6656 - acc: 0.5997
1920/4566 [===========>..................] - ETA: 6:59 - loss: 0.6654 - acc: 0.6000
1984/4566 [============>.................] - ETA: 6:44 - loss: 0.6650 - acc: 0.6008
2048/4566 [============>.................] - ETA: 6:29 - loss: 0.6649 - acc: 0.6040
2112/4566 [============>.................] - ETA: 6:15 - loss: 0.6658 - acc: 0.6013
2176/4566 [=============>................] - ETA: 6:02 - loss: 0.6666 - acc: 0.6006
2240/4566 [=============>................] - ETA: 5:50 - loss: 0.6655 - acc: 0.6036
2304/4566 [==============>...............] - ETA: 5:38 - loss: 0.6662 - acc: 0.6020
2368/4566 [==============>...............] - ETA: 5:26 - loss: 0.6665 - acc: 0.6009
2432/4566 [==============>...............] - ETA: 5:14 - loss: 0.6658 - acc: 0.6007
2496/4566 [===============>..............] - ETA: 5:03 - loss: 0.6653 - acc: 0.6022
2560/4566 [===============>..............] - ETA: 4:51 - loss: 0.6644 - acc: 0.6055
2624/4566 [================>.............] - ETA: 4:41 - loss: 0.6639 - acc: 0.6052
2688/4566 [================>.............] - ETA: 4:30 - loss: 0.6638 - acc: 0.6060
2752/4566 [=================>............] - ETA: 4:19 - loss: 0.6632 - acc: 0.6072
2816/4566 [=================>............] - ETA: 4:09 - loss: 0.6632 - acc: 0.6072
2880/4566 [=================>............] - ETA: 3:59 - loss: 0.6625 - acc: 0.6080
2944/4566 [==================>...........] - ETA: 3:50 - loss: 0.6623 - acc: 0.6077
3008/4566 [==================>...........] - ETA: 3:43 - loss: 0.6614 - acc: 0.6080
3072/4566 [===================>..........] - ETA: 3:36 - loss: 0.6606 - acc: 0.6090
3136/4566 [===================>..........] - ETA: 3:29 - loss: 0.6611 - acc: 0.6084
3200/4566 [====================>.........] - ETA: 3:22 - loss: 0.6621 - acc: 0.6066
3264/4566 [====================>.........] - ETA: 3:14 - loss: 0.6618 - acc: 0.6072
3328/4566 [====================>.........] - ETA: 3:06 - loss: 0.6617 - acc: 0.6073
3392/4566 [=====================>........] - ETA: 2:57 - loss: 0.6610 - acc: 0.6082
3456/4566 [=====================>........] - ETA: 2:47 - loss: 0.6600 - acc: 0.6088
3520/4566 [======================>.......] - ETA: 2:37 - loss: 0.6605 - acc: 0.6085
3584/4566 [======================>.......] - ETA: 2:27 - loss: 0.6610 - acc: 0.6085
3648/4566 [======================>.......] - ETA: 2:17 - loss: 0.6612 - acc: 0.6080
3712/4566 [=======================>......] - ETA: 2:07 - loss: 0.6614 - acc: 0.6075
3776/4566 [=======================>......] - ETA: 1:57 - loss: 0.6617 - acc: 0.6070
3840/4566 [========================>.....] - ETA: 1:47 - loss: 0.6609 - acc: 0.6089
3904/4566 [========================>.....] - ETA: 1:37 - loss: 0.6618 - acc: 0.6081
3968/4566 [=========================>....] - ETA: 1:28 - loss: 0.6619 - acc: 0.6071
4032/4566 [=========================>....] - ETA: 1:18 - loss: 0.6620 - acc: 0.6076
4096/4566 [=========================>....] - ETA: 1:08 - loss: 0.6626 - acc: 0.6062
4160/4566 [==========================>...] - ETA: 59s - loss: 0.6624 - acc: 0.6058 
4224/4566 [==========================>...] - ETA: 49s - loss: 0.6619 - acc: 0.6063
4288/4566 [===========================>..] - ETA: 40s - loss: 0.6624 - acc: 0.6049
4352/4566 [===========================>..] - ETA: 31s - loss: 0.6618 - acc: 0.6052
4416/4566 [============================>.] - ETA: 22s - loss: 0.6613 - acc: 0.6051
4480/4566 [============================>.] - ETA: 12s - loss: 0.6611 - acc: 0.6051
4544/4566 [============================>.] - ETA: 3s - loss: 0.6611 - acc: 0.6048 
4566/4566 [==============================] - 727s 159ms/step - loss: 0.6612 - acc: 0.6047 - val_loss: 0.6911 - val_acc: 0.5886

Epoch 00010: val_acc did not improve from 0.60039
样本个数 634
样本个数 1268
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f3b400a50d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f3b400a50d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f3b4003a690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f3b4003a690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37d42c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37d42c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b37c3e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b37c3e8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b37c49a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b37c49a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37b57510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37b57510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b37c3e750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b37c3e750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37bc2510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37bc2510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b37aa80d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b37aa80d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b37964b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b37964b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37c0cb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37c0cb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b3791cc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b3791cc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b3781fa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b3781fa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b37711110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b37711110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b3771c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b3771c710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3568567790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3568567790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b3791c810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b3791c810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37618cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37618cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b37437710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b37437710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b37627dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b37627dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b371daf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b371daf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b3772d810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b3772d810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37391510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37391510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b371ded50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b371ded50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b373b8590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b373b8590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37040650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b37040650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b371dec10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b371dec10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b26eb9c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b26eb9c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b26ed9b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b26ed9b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b26e05950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b26e05950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b26ce3750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b26ce3750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b37050c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b37050c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b26b9f9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b26b9f9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b26d82850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b26d82850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b26ccfe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b26ccfe90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b26df5950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b26df5950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b37184dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b37184dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b26b09b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b26b09b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b26a63e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b26a63e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b26741f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b26741f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b269939d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b269939d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b26a20810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b26a20810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b269fe390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b269fe390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b267adb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b267adb50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b1e310c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b1e310c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b269c1890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b269c1890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b1e6c4f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b1e6c4f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b1e34e110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b1e34e110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b1e1a1f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b1e1a1f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b1e11e450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b1e11e450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b1e34e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b1e34e550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b1e289290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b1e289290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f35684c2750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f35684c2750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b15e78490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b15e78490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b15da4650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b15da4650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b15bca7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b15bca7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b15e1cc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b15e1cc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b15d9cf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b15d9cf50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b15b2f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3b15b2f1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b159f6e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3b159f6e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b158ebe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b158ebe10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b15b2f9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3b15b2f9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b15b39d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3b15b39d50>>: AttributeError: module 'gast' has no attribute 'Str'
window12.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1268 [>.............................] - ETA: 2:19
 128/1268 [==>...........................] - ETA: 1:32
 192/1268 [===>..........................] - ETA: 1:14
 256/1268 [=====>........................] - ETA: 1:07
 320/1268 [======>.......................] - ETA: 59s 
 384/1268 [========>.....................] - ETA: 53s
 448/1268 [=========>....................] - ETA: 47s
 512/1268 [===========>..................] - ETA: 42s
 576/1268 [============>.................] - ETA: 38s
 640/1268 [==============>...............] - ETA: 34s
 704/1268 [===============>..............] - ETA: 30s
 768/1268 [=================>............] - ETA: 27s
 832/1268 [==================>...........] - ETA: 23s
 896/1268 [====================>.........] - ETA: 20s
 960/1268 [=====================>........] - ETA: 16s
1024/1268 [=======================>......] - ETA: 12s
1088/1268 [========================>.....] - ETA: 9s 
1152/1268 [==========================>...] - ETA: 6s
1216/1268 [===========================>..] - ETA: 2s
1268/1268 [==============================] - 67s 53ms/step
loss: 0.6776644886480145
acc: 0.5694006299746901
