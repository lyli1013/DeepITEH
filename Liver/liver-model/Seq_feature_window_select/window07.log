/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe681cbe110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe681cbe110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe681ba3850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe681ba3850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe681ba3750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe681ba3750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe681b5cf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe681b5cf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe61bb3de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe61bb3de90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61bb7b650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61bb7b650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe681b5cfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe681b5cfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe681b2fc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe681b2fc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe61b973b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe61b973b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe61b9b96d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe61b9b96d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe689f63e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe689f63e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe61b99d250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe61b99d250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61b899810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61b899810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe61b8e9f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe61b8e9f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6134f2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6134f2a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61b64c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61b64c1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe61b6c4d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe61b6c4d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe613468850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe613468850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe61b5c3110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe61b5c3110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6131fed10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6131fed10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61b5a2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61b5a2390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe61338be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe61338be10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe613252990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe613252990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6130a4c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6130a4c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe612f72d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe612f72d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61ba9c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61ba9c1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe68a03cad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe68a03cad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe612ef4910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe612ef4910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe612e08150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe612e08150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe60ad14e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe60ad14e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe612f6d4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe612f6d4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6132de7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6132de7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe60ac12350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe60ac12350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe60a9cba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe60a9cba50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe60aa5b490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe60aa5b490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe60abcdd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe60abcdd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe61bb71110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe61bb71110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe60a8e5d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe60a8e5d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe60aa5bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe60aa5bc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe60a91d990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe60a91d990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61b90c950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe61b90c950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe60a6d83d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe60a6d83d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe60a6feb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe60a6feb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe602389e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe602389e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6022f7a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6022f7a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6024c6150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6024c6150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe602389fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe602389fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe602170450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe602170450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe602415590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe602415590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe601ffca50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe601ffca50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe602296790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe602296790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe60abf7c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe60abf7c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe601f841d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe601f841d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe601d3c210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe601d3c210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe601c9fe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe601c9fe50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe601d90e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe601d90e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe601f88e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe601f88e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe601dd0510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe601dd0510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe601c89b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe601c89b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6018fa0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6018fa0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe601a44d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe601a44d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe601c89110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe601c89110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe601c83290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe601c83290>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-17 11:02:18.018535: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-17 11:02:18.094757: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-17 11:02:18.166304: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556314721da0 executing computations on platform Host. Devices:
2022-11-17 11:02:18.166400: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-17 11:02:19.258837: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window07.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 20:12 - loss: 0.7725 - acc: 0.4688
 128/4566 [..............................] - ETA: 15:26 - loss: 0.8922 - acc: 0.4375
 192/4566 [>.............................] - ETA: 13:19 - loss: 0.8296 - acc: 0.4688
 256/4566 [>.............................] - ETA: 14:38 - loss: 0.8081 - acc: 0.4844
 320/4566 [=>............................] - ETA: 15:51 - loss: 0.7918 - acc: 0.4875
 384/4566 [=>............................] - ETA: 16:45 - loss: 0.7850 - acc: 0.4948
 448/4566 [=>............................] - ETA: 16:25 - loss: 0.7846 - acc: 0.4955
 512/4566 [==>...........................] - ETA: 16:12 - loss: 0.7742 - acc: 0.4980
 576/4566 [==>...........................] - ETA: 15:00 - loss: 0.7721 - acc: 0.4948
 640/4566 [===>..........................] - ETA: 13:57 - loss: 0.7702 - acc: 0.5000
 704/4566 [===>..........................] - ETA: 13:04 - loss: 0.7609 - acc: 0.5128
 768/4566 [====>.........................] - ETA: 12:21 - loss: 0.7640 - acc: 0.5091
 832/4566 [====>.........................] - ETA: 11:48 - loss: 0.7599 - acc: 0.5084
 896/4566 [====>.........................] - ETA: 11:10 - loss: 0.7569 - acc: 0.5078
 960/4566 [=====>........................] - ETA: 10:41 - loss: 0.7563 - acc: 0.5073
1024/4566 [=====>........................] - ETA: 10:21 - loss: 0.7546 - acc: 0.5098
1088/4566 [======>.......................] - ETA: 9:56 - loss: 0.7552 - acc: 0.5083 
1152/4566 [======>.......................] - ETA: 9:32 - loss: 0.7530 - acc: 0.5130
1216/4566 [======>.......................] - ETA: 9:11 - loss: 0.7544 - acc: 0.5115
1280/4566 [=======>......................] - ETA: 8:49 - loss: 0.7535 - acc: 0.5078
1344/4566 [=======>......................] - ETA: 8:31 - loss: 0.7511 - acc: 0.5060
1408/4566 [========>.....................] - ETA: 8:27 - loss: 0.7496 - acc: 0.5057
1472/4566 [========>.....................] - ETA: 8:28 - loss: 0.7499 - acc: 0.5041
1536/4566 [=========>....................] - ETA: 8:28 - loss: 0.7497 - acc: 0.5033
1600/4566 [=========>....................] - ETA: 8:26 - loss: 0.7491 - acc: 0.5025
1664/4566 [=========>....................] - ETA: 8:23 - loss: 0.7494 - acc: 0.5000
1728/4566 [==========>...................] - ETA: 8:15 - loss: 0.7498 - acc: 0.4971
1792/4566 [==========>...................] - ETA: 7:57 - loss: 0.7492 - acc: 0.4944
1856/4566 [===========>..................] - ETA: 7:40 - loss: 0.7494 - acc: 0.4930
1920/4566 [===========>..................] - ETA: 7:24 - loss: 0.7466 - acc: 0.4969
1984/4566 [============>.................] - ETA: 7:08 - loss: 0.7451 - acc: 0.4975
2048/4566 [============>.................] - ETA: 6:52 - loss: 0.7442 - acc: 0.4985
2112/4566 [============>.................] - ETA: 6:37 - loss: 0.7437 - acc: 0.4967
2176/4566 [=============>................] - ETA: 6:22 - loss: 0.7426 - acc: 0.4968
2240/4566 [=============>................] - ETA: 6:09 - loss: 0.7405 - acc: 0.4991
2304/4566 [==============>...............] - ETA: 5:56 - loss: 0.7399 - acc: 0.4983
2368/4566 [==============>...............] - ETA: 5:43 - loss: 0.7383 - acc: 0.4987
2432/4566 [==============>...............] - ETA: 5:31 - loss: 0.7382 - acc: 0.4984
2496/4566 [===============>..............] - ETA: 5:19 - loss: 0.7372 - acc: 0.4992
2560/4566 [===============>..............] - ETA: 5:06 - loss: 0.7374 - acc: 0.4961
2624/4566 [================>.............] - ETA: 4:59 - loss: 0.7352 - acc: 0.4996
2688/4566 [================>.............] - ETA: 4:53 - loss: 0.7335 - acc: 0.5022
2752/4566 [=================>............] - ETA: 4:47 - loss: 0.7337 - acc: 0.5025
2816/4566 [=================>............] - ETA: 4:40 - loss: 0.7330 - acc: 0.5025
2880/4566 [=================>............] - ETA: 4:33 - loss: 0.7327 - acc: 0.5014
2944/4566 [==================>...........] - ETA: 4:25 - loss: 0.7320 - acc: 0.5041
3008/4566 [==================>...........] - ETA: 4:12 - loss: 0.7319 - acc: 0.5047
3072/4566 [===================>..........] - ETA: 4:00 - loss: 0.7316 - acc: 0.5052
3136/4566 [===================>..........] - ETA: 3:48 - loss: 0.7314 - acc: 0.5054
3200/4566 [====================>.........] - ETA: 3:36 - loss: 0.7306 - acc: 0.5059
3264/4566 [====================>.........] - ETA: 3:24 - loss: 0.7297 - acc: 0.5077
3328/4566 [====================>.........] - ETA: 3:13 - loss: 0.7292 - acc: 0.5081
3392/4566 [=====================>........] - ETA: 3:02 - loss: 0.7287 - acc: 0.5088
3456/4566 [=====================>........] - ETA: 2:51 - loss: 0.7288 - acc: 0.5084
3520/4566 [======================>.......] - ETA: 2:40 - loss: 0.7289 - acc: 0.5082
3584/4566 [======================>.......] - ETA: 2:29 - loss: 0.7281 - acc: 0.5089
3648/4566 [======================>.......] - ETA: 2:18 - loss: 0.7273 - acc: 0.5104
3712/4566 [=======================>......] - ETA: 2:08 - loss: 0.7275 - acc: 0.5097
3776/4566 [=======================>......] - ETA: 1:57 - loss: 0.7269 - acc: 0.5098
3840/4566 [========================>.....] - ETA: 1:47 - loss: 0.7266 - acc: 0.5102
3904/4566 [========================>.....] - ETA: 1:38 - loss: 0.7259 - acc: 0.5110
3968/4566 [=========================>....] - ETA: 1:30 - loss: 0.7258 - acc: 0.5088
4032/4566 [=========================>....] - ETA: 1:21 - loss: 0.7255 - acc: 0.5089
4096/4566 [=========================>....] - ETA: 1:12 - loss: 0.7247 - acc: 0.5098
4160/4566 [==========================>...] - ETA: 1:02 - loss: 0.7238 - acc: 0.5106
4224/4566 [==========================>...] - ETA: 53s - loss: 0.7236 - acc: 0.5114 
4288/4566 [===========================>..] - ETA: 42s - loss: 0.7232 - acc: 0.5112
4352/4566 [===========================>..] - ETA: 32s - loss: 0.7224 - acc: 0.5113
4416/4566 [============================>.] - ETA: 22s - loss: 0.7223 - acc: 0.5111
4480/4566 [============================>.] - ETA: 13s - loss: 0.7216 - acc: 0.5125
4544/4566 [============================>.] - ETA: 3s - loss: 0.7217 - acc: 0.5119 
4566/4566 [==============================] - 707s 155ms/step - loss: 0.7215 - acc: 0.5125 - val_loss: 0.6845 - val_acc: 0.5374

Epoch 00001: val_acc improved from -inf to 0.53740, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window07/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 8:13 - loss: 0.6942 - acc: 0.5625
 128/4566 [..............................] - ETA: 7:46 - loss: 0.6879 - acc: 0.5391
 192/4566 [>.............................] - ETA: 7:29 - loss: 0.6908 - acc: 0.5260
 256/4566 [>.............................] - ETA: 7:16 - loss: 0.6882 - acc: 0.5312
 320/4566 [=>............................] - ETA: 7:19 - loss: 0.6882 - acc: 0.5344
 384/4566 [=>............................] - ETA: 8:34 - loss: 0.6866 - acc: 0.5365
 448/4566 [=>............................] - ETA: 9:30 - loss: 0.6868 - acc: 0.5246
 512/4566 [==>...........................] - ETA: 10:13 - loss: 0.6852 - acc: 0.5254
 576/4566 [==>...........................] - ETA: 10:38 - loss: 0.6928 - acc: 0.5087
 640/4566 [===>..........................] - ETA: 11:01 - loss: 0.6958 - acc: 0.5109
 704/4566 [===>..........................] - ETA: 10:53 - loss: 0.6959 - acc: 0.5085
 768/4566 [====>.........................] - ETA: 10:18 - loss: 0.6944 - acc: 0.5091
 832/4566 [====>.........................] - ETA: 9:52 - loss: 0.6941 - acc: 0.5096 
 896/4566 [====>.........................] - ETA: 9:24 - loss: 0.6958 - acc: 0.5112
 960/4566 [=====>........................] - ETA: 8:59 - loss: 0.6946 - acc: 0.5167
1024/4566 [=====>........................] - ETA: 8:37 - loss: 0.6967 - acc: 0.5117
1088/4566 [======>.......................] - ETA: 8:16 - loss: 0.6980 - acc: 0.5101
1152/4566 [======>.......................] - ETA: 7:57 - loss: 0.6986 - acc: 0.5095
1216/4566 [======>.......................] - ETA: 7:39 - loss: 0.6980 - acc: 0.5090
1280/4566 [=======>......................] - ETA: 7:22 - loss: 0.6971 - acc: 0.5117
1344/4566 [=======>......................] - ETA: 7:08 - loss: 0.6980 - acc: 0.5082
1408/4566 [========>.....................] - ETA: 6:54 - loss: 0.6980 - acc: 0.5085
1472/4566 [========>.....................] - ETA: 6:42 - loss: 0.6964 - acc: 0.5136
1536/4566 [=========>....................] - ETA: 6:28 - loss: 0.6977 - acc: 0.5117
1600/4566 [=========>....................] - ETA: 6:16 - loss: 0.6979 - acc: 0.5131
1664/4566 [=========>....................] - ETA: 6:13 - loss: 0.6991 - acc: 0.5120
1728/4566 [==========>...................] - ETA: 6:17 - loss: 0.7000 - acc: 0.5122
1792/4566 [==========>...................] - ETA: 6:19 - loss: 0.7001 - acc: 0.5134
1856/4566 [===========>..................] - ETA: 6:20 - loss: 0.6999 - acc: 0.5124
1920/4566 [===========>..................] - ETA: 6:20 - loss: 0.7012 - acc: 0.5104
1984/4566 [============>.................] - ETA: 6:17 - loss: 0.6998 - acc: 0.5116
2048/4566 [============>.................] - ETA: 6:05 - loss: 0.6998 - acc: 0.5127
2112/4566 [============>.................] - ETA: 5:53 - loss: 0.6993 - acc: 0.5156
2176/4566 [=============>................] - ETA: 5:40 - loss: 0.6988 - acc: 0.5184
2240/4566 [=============>................] - ETA: 5:28 - loss: 0.6993 - acc: 0.5179
2304/4566 [==============>...............] - ETA: 5:16 - loss: 0.6984 - acc: 0.5213
2368/4566 [==============>...............] - ETA: 5:04 - loss: 0.6974 - acc: 0.5232
2432/4566 [==============>...............] - ETA: 4:53 - loss: 0.6981 - acc: 0.5226
2496/4566 [===============>..............] - ETA: 4:42 - loss: 0.6992 - acc: 0.5200
2560/4566 [===============>..............] - ETA: 4:31 - loss: 0.7000 - acc: 0.5191
2624/4566 [================>.............] - ETA: 4:21 - loss: 0.7009 - acc: 0.5194
2688/4566 [================>.............] - ETA: 4:10 - loss: 0.7012 - acc: 0.5201
2752/4566 [=================>............] - ETA: 4:00 - loss: 0.7009 - acc: 0.5211
2816/4566 [=================>............] - ETA: 3:50 - loss: 0.7002 - acc: 0.5231
2880/4566 [=================>............] - ETA: 3:40 - loss: 0.7001 - acc: 0.5226
2944/4566 [==================>...........] - ETA: 3:32 - loss: 0.7002 - acc: 0.5228
3008/4566 [==================>...........] - ETA: 3:27 - loss: 0.7001 - acc: 0.5236
3072/4566 [===================>..........] - ETA: 3:22 - loss: 0.6994 - acc: 0.5241
3136/4566 [===================>..........] - ETA: 3:16 - loss: 0.6993 - acc: 0.5242
3200/4566 [====================>.........] - ETA: 3:10 - loss: 0.6981 - acc: 0.5256
3264/4566 [====================>.........] - ETA: 3:03 - loss: 0.6981 - acc: 0.5254
3328/4566 [====================>.........] - ETA: 2:54 - loss: 0.6981 - acc: 0.5255
3392/4566 [=====================>........] - ETA: 2:44 - loss: 0.6987 - acc: 0.5245
3456/4566 [=====================>........] - ETA: 2:34 - loss: 0.6988 - acc: 0.5243
3520/4566 [======================>.......] - ETA: 2:24 - loss: 0.6989 - acc: 0.5239
3584/4566 [======================>.......] - ETA: 2:14 - loss: 0.6993 - acc: 0.5226
3648/4566 [======================>.......] - ETA: 2:04 - loss: 0.6992 - acc: 0.5225
3712/4566 [=======================>......] - ETA: 1:55 - loss: 0.6984 - acc: 0.5237
3776/4566 [=======================>......] - ETA: 1:46 - loss: 0.6990 - acc: 0.5230
3840/4566 [========================>.....] - ETA: 1:37 - loss: 0.6995 - acc: 0.5214
3904/4566 [========================>.....] - ETA: 1:28 - loss: 0.6994 - acc: 0.5225
3968/4566 [=========================>....] - ETA: 1:19 - loss: 0.6996 - acc: 0.5217
4032/4566 [=========================>....] - ETA: 1:10 - loss: 0.6990 - acc: 0.5223
4096/4566 [=========================>....] - ETA: 1:01 - loss: 0.6993 - acc: 0.5222
4160/4566 [==========================>...] - ETA: 53s - loss: 0.6996 - acc: 0.5219 
4224/4566 [==========================>...] - ETA: 44s - loss: 0.6999 - acc: 0.5227
4288/4566 [===========================>..] - ETA: 36s - loss: 0.7001 - acc: 0.5217
4352/4566 [===========================>..] - ETA: 28s - loss: 0.7001 - acc: 0.5225
4416/4566 [============================>.] - ETA: 20s - loss: 0.6996 - acc: 0.5231
4480/4566 [============================>.] - ETA: 11s - loss: 0.6994 - acc: 0.5234
4544/4566 [============================>.] - ETA: 3s - loss: 0.6993 - acc: 0.5233 
4566/4566 [==============================] - 653s 143ms/step - loss: 0.6990 - acc: 0.5241 - val_loss: 0.6821 - val_acc: 0.5787

Epoch 00002: val_acc improved from 0.53740 to 0.57874, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window07/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 3/10

  64/4566 [..............................] - ETA: 6:57 - loss: 0.6958 - acc: 0.5156
 128/4566 [..............................] - ETA: 6:48 - loss: 0.7031 - acc: 0.5000
 192/4566 [>.............................] - ETA: 6:53 - loss: 0.6887 - acc: 0.5417
 256/4566 [>.............................] - ETA: 6:41 - loss: 0.7033 - acc: 0.5078
 320/4566 [=>............................] - ETA: 6:27 - loss: 0.6996 - acc: 0.5250
 384/4566 [=>............................] - ETA: 6:28 - loss: 0.6962 - acc: 0.5182
 448/4566 [=>............................] - ETA: 6:18 - loss: 0.6970 - acc: 0.5223
 512/4566 [==>...........................] - ETA: 6:10 - loss: 0.7017 - acc: 0.5156
 576/4566 [==>...........................] - ETA: 6:03 - loss: 0.6963 - acc: 0.5208
 640/4566 [===>..........................] - ETA: 5:59 - loss: 0.6966 - acc: 0.5203
 704/4566 [===>..........................] - ETA: 5:55 - loss: 0.6968 - acc: 0.5241
 768/4566 [====>.........................] - ETA: 5:49 - loss: 0.6998 - acc: 0.5156
 832/4566 [====>.........................] - ETA: 5:52 - loss: 0.6997 - acc: 0.5144
 896/4566 [====>.........................] - ETA: 6:21 - loss: 0.7001 - acc: 0.5167
 960/4566 [=====>........................] - ETA: 6:46 - loss: 0.6989 - acc: 0.5188
1024/4566 [=====>........................] - ETA: 7:05 - loss: 0.6985 - acc: 0.5166
1088/4566 [======>.......................] - ETA: 7:20 - loss: 0.7001 - acc: 0.5110
1152/4566 [======>.......................] - ETA: 7:33 - loss: 0.7012 - acc: 0.5078
1216/4566 [======>.......................] - ETA: 7:30 - loss: 0.7010 - acc: 0.5099
1280/4566 [=======>......................] - ETA: 7:15 - loss: 0.6996 - acc: 0.5125
1344/4566 [=======>......................] - ETA: 7:00 - loss: 0.7005 - acc: 0.5126
1408/4566 [========>.....................] - ETA: 6:46 - loss: 0.7005 - acc: 0.5149
1472/4566 [========>.....................] - ETA: 6:35 - loss: 0.7022 - acc: 0.5156
1536/4566 [=========>....................] - ETA: 6:22 - loss: 0.7025 - acc: 0.5143
1600/4566 [=========>....................] - ETA: 6:10 - loss: 0.7020 - acc: 0.5150
1664/4566 [=========>....................] - ETA: 5:57 - loss: 0.7013 - acc: 0.5174
1728/4566 [==========>...................] - ETA: 5:46 - loss: 0.7001 - acc: 0.5191
1792/4566 [==========>...................] - ETA: 5:34 - loss: 0.6995 - acc: 0.5201
1856/4566 [===========>..................] - ETA: 5:24 - loss: 0.6993 - acc: 0.5210
1920/4566 [===========>..................] - ETA: 5:14 - loss: 0.6995 - acc: 0.5224
1984/4566 [============>.................] - ETA: 5:05 - loss: 0.6990 - acc: 0.5242
2048/4566 [============>.................] - ETA: 4:56 - loss: 0.7005 - acc: 0.5234
2112/4566 [============>.................] - ETA: 4:46 - loss: 0.6995 - acc: 0.5237
2176/4566 [=============>................] - ETA: 4:41 - loss: 0.6986 - acc: 0.5262
2240/4566 [=============>................] - ETA: 4:41 - loss: 0.6984 - acc: 0.5268
2304/4566 [==============>...............] - ETA: 4:40 - loss: 0.6983 - acc: 0.5265
2368/4566 [==============>...............] - ETA: 4:39 - loss: 0.6989 - acc: 0.5258
2432/4566 [==============>...............] - ETA: 4:37 - loss: 0.6976 - acc: 0.5288
2496/4566 [===============>..............] - ETA: 4:34 - loss: 0.6970 - acc: 0.5325
2560/4566 [===============>..............] - ETA: 4:25 - loss: 0.6976 - acc: 0.5312
2624/4566 [================>.............] - ETA: 4:15 - loss: 0.6976 - acc: 0.5305
2688/4566 [================>.............] - ETA: 4:05 - loss: 0.6972 - acc: 0.5305
2752/4566 [=================>............] - ETA: 3:55 - loss: 0.6977 - acc: 0.5291
2816/4566 [=================>............] - ETA: 3:45 - loss: 0.6984 - acc: 0.5277
2880/4566 [=================>............] - ETA: 3:35 - loss: 0.6983 - acc: 0.5281
2944/4566 [==================>...........] - ETA: 3:25 - loss: 0.6981 - acc: 0.5285
3008/4566 [==================>...........] - ETA: 3:16 - loss: 0.6974 - acc: 0.5303
3072/4566 [===================>..........] - ETA: 3:07 - loss: 0.6975 - acc: 0.5299
3136/4566 [===================>..........] - ETA: 2:58 - loss: 0.6975 - acc: 0.5300
3200/4566 [====================>.........] - ETA: 2:49 - loss: 0.6979 - acc: 0.5288
3264/4566 [====================>.........] - ETA: 2:40 - loss: 0.6979 - acc: 0.5288
3328/4566 [====================>.........] - ETA: 2:32 - loss: 0.6970 - acc: 0.5300
3392/4566 [=====================>........] - ETA: 2:23 - loss: 0.6967 - acc: 0.5318
3456/4566 [=====================>........] - ETA: 2:16 - loss: 0.6970 - acc: 0.5301
3520/4566 [======================>.......] - ETA: 2:10 - loss: 0.6967 - acc: 0.5312
3584/4566 [======================>.......] - ETA: 2:04 - loss: 0.6967 - acc: 0.5312
3648/4566 [======================>.......] - ETA: 1:58 - loss: 0.6979 - acc: 0.5285
3712/4566 [=======================>......] - ETA: 1:51 - loss: 0.6982 - acc: 0.5272
3776/4566 [=======================>......] - ETA: 1:44 - loss: 0.6987 - acc: 0.5254
3840/4566 [========================>.....] - ETA: 1:36 - loss: 0.6984 - acc: 0.5258
3904/4566 [========================>.....] - ETA: 1:27 - loss: 0.6986 - acc: 0.5248
3968/4566 [=========================>....] - ETA: 1:18 - loss: 0.6982 - acc: 0.5262
4032/4566 [=========================>....] - ETA: 1:09 - loss: 0.6975 - acc: 0.5268
4096/4566 [=========================>....] - ETA: 1:01 - loss: 0.6967 - acc: 0.5288
4160/4566 [==========================>...] - ETA: 52s - loss: 0.6959 - acc: 0.5303 
4224/4566 [==========================>...] - ETA: 44s - loss: 0.6959 - acc: 0.5296
4288/4566 [===========================>..] - ETA: 35s - loss: 0.6957 - acc: 0.5303
4352/4566 [===========================>..] - ETA: 27s - loss: 0.6955 - acc: 0.5306
4416/4566 [============================>.] - ETA: 19s - loss: 0.6950 - acc: 0.5319
4480/4566 [============================>.] - ETA: 10s - loss: 0.6950 - acc: 0.5317
4544/4566 [============================>.] - ETA: 2s - loss: 0.6959 - acc: 0.5304 
4566/4566 [==============================] - 591s 129ms/step - loss: 0.6958 - acc: 0.5304 - val_loss: 0.6830 - val_acc: 0.5650

Epoch 00003: val_acc did not improve from 0.57874
Epoch 4/10

  64/4566 [..............................] - ETA: 13:25 - loss: 0.6661 - acc: 0.5625
 128/4566 [..............................] - ETA: 14:47 - loss: 0.6890 - acc: 0.5391
 192/4566 [>.............................] - ETA: 15:20 - loss: 0.6873 - acc: 0.5573
 256/4566 [>.............................] - ETA: 15:30 - loss: 0.6982 - acc: 0.5391
 320/4566 [=>............................] - ETA: 15:27 - loss: 0.6982 - acc: 0.5500
 384/4566 [=>............................] - ETA: 15:19 - loss: 0.7004 - acc: 0.5391
 448/4566 [=>............................] - ETA: 14:08 - loss: 0.6969 - acc: 0.5379
 512/4566 [==>...........................] - ETA: 12:56 - loss: 0.7000 - acc: 0.5273
 576/4566 [==>...........................] - ETA: 12:01 - loss: 0.6981 - acc: 0.5260
 640/4566 [===>..........................] - ETA: 11:14 - loss: 0.6974 - acc: 0.5344
 704/4566 [===>..........................] - ETA: 10:34 - loss: 0.6919 - acc: 0.5455
 768/4566 [====>.........................] - ETA: 10:02 - loss: 0.6893 - acc: 0.5495
 832/4566 [====>.........................] - ETA: 9:34 - loss: 0.6885 - acc: 0.5445 
 896/4566 [====>.........................] - ETA: 9:05 - loss: 0.6897 - acc: 0.5402
 960/4566 [=====>........................] - ETA: 8:42 - loss: 0.6880 - acc: 0.5469
1024/4566 [=====>........................] - ETA: 8:20 - loss: 0.6902 - acc: 0.5459
1088/4566 [======>.......................] - ETA: 8:01 - loss: 0.6908 - acc: 0.5441
1152/4566 [======>.......................] - ETA: 7:43 - loss: 0.6943 - acc: 0.5391
1216/4566 [======>.......................] - ETA: 7:26 - loss: 0.6925 - acc: 0.5419
1280/4566 [=======>......................] - ETA: 7:11 - loss: 0.6951 - acc: 0.5391
1344/4566 [=======>......................] - ETA: 6:57 - loss: 0.6949 - acc: 0.5379
1408/4566 [========>.....................] - ETA: 6:52 - loss: 0.6931 - acc: 0.5398
1472/4566 [========>.....................] - ETA: 7:01 - loss: 0.6936 - acc: 0.5387
1536/4566 [=========>....................] - ETA: 7:05 - loss: 0.6933 - acc: 0.5404
1600/4566 [=========>....................] - ETA: 7:08 - loss: 0.6924 - acc: 0.5431
1664/4566 [=========>....................] - ETA: 7:08 - loss: 0.6923 - acc: 0.5409
1728/4566 [==========>...................] - ETA: 7:07 - loss: 0.6931 - acc: 0.5394
1792/4566 [==========>...................] - ETA: 6:53 - loss: 0.6918 - acc: 0.5424
1856/4566 [===========>..................] - ETA: 6:38 - loss: 0.6906 - acc: 0.5474
1920/4566 [===========>..................] - ETA: 6:23 - loss: 0.6898 - acc: 0.5500
1984/4566 [============>.................] - ETA: 6:09 - loss: 0.6896 - acc: 0.5484
2048/4566 [============>.................] - ETA: 5:56 - loss: 0.6889 - acc: 0.5518
2112/4566 [============>.................] - ETA: 5:43 - loss: 0.6888 - acc: 0.5507
2176/4566 [=============>................] - ETA: 5:30 - loss: 0.6890 - acc: 0.5483
2240/4566 [=============>................] - ETA: 5:18 - loss: 0.6890 - acc: 0.5473
2304/4566 [==============>...............] - ETA: 5:06 - loss: 0.6902 - acc: 0.5438
2368/4566 [==============>...............] - ETA: 4:55 - loss: 0.6903 - acc: 0.5439
2432/4566 [==============>...............] - ETA: 4:44 - loss: 0.6906 - acc: 0.5440
2496/4566 [===============>..............] - ETA: 4:33 - loss: 0.6907 - acc: 0.5449
2560/4566 [===============>..............] - ETA: 4:22 - loss: 0.6912 - acc: 0.5441
2624/4566 [================>.............] - ETA: 4:12 - loss: 0.6907 - acc: 0.5461
2688/4566 [================>.............] - ETA: 4:02 - loss: 0.6912 - acc: 0.5450
2752/4566 [=================>............] - ETA: 3:54 - loss: 0.6912 - acc: 0.5447
2816/4566 [=================>............] - ETA: 3:49 - loss: 0.6914 - acc: 0.5433
2880/4566 [=================>............] - ETA: 3:45 - loss: 0.6917 - acc: 0.5427
2944/4566 [==================>...........] - ETA: 3:40 - loss: 0.6910 - acc: 0.5438
3008/4566 [==================>...........] - ETA: 3:35 - loss: 0.6908 - acc: 0.5429
3072/4566 [===================>..........] - ETA: 3:29 - loss: 0.6905 - acc: 0.5436
3136/4566 [===================>..........] - ETA: 3:20 - loss: 0.6907 - acc: 0.5430
3200/4566 [====================>.........] - ETA: 3:09 - loss: 0.6905 - acc: 0.5431
3264/4566 [====================>.........] - ETA: 2:59 - loss: 0.6910 - acc: 0.5423
3328/4566 [====================>.........] - ETA: 2:49 - loss: 0.6907 - acc: 0.5424
3392/4566 [=====================>........] - ETA: 2:39 - loss: 0.6907 - acc: 0.5419
3456/4566 [=====================>........] - ETA: 2:29 - loss: 0.6908 - acc: 0.5405
3520/4566 [======================>.......] - ETA: 2:20 - loss: 0.6906 - acc: 0.5409
3584/4566 [======================>.......] - ETA: 2:11 - loss: 0.6914 - acc: 0.5382
3648/4566 [======================>.......] - ETA: 2:01 - loss: 0.6912 - acc: 0.5395
3712/4566 [=======================>......] - ETA: 1:52 - loss: 0.6905 - acc: 0.5409
3776/4566 [=======================>......] - ETA: 1:43 - loss: 0.6907 - acc: 0.5408
3840/4566 [========================>.....] - ETA: 1:34 - loss: 0.6902 - acc: 0.5419
3904/4566 [========================>.....] - ETA: 1:26 - loss: 0.6904 - acc: 0.5420
3968/4566 [=========================>....] - ETA: 1:17 - loss: 0.6903 - acc: 0.5426
4032/4566 [=========================>....] - ETA: 1:08 - loss: 0.6897 - acc: 0.5439
4096/4566 [=========================>....] - ETA: 1:00 - loss: 0.6894 - acc: 0.5449
4160/4566 [==========================>...] - ETA: 52s - loss: 0.6894 - acc: 0.5452 
4224/4566 [==========================>...] - ETA: 45s - loss: 0.6898 - acc: 0.5445
4288/4566 [===========================>..] - ETA: 37s - loss: 0.6897 - acc: 0.5448
4352/4566 [===========================>..] - ETA: 28s - loss: 0.6891 - acc: 0.5464
4416/4566 [============================>.] - ETA: 20s - loss: 0.6890 - acc: 0.5469
4480/4566 [============================>.] - ETA: 11s - loss: 0.6888 - acc: 0.5478
4544/4566 [============================>.] - ETA: 2s - loss: 0.6887 - acc: 0.5486 
4566/4566 [==============================] - 636s 139ms/step - loss: 0.6886 - acc: 0.5482 - val_loss: 0.6784 - val_acc: 0.5610

Epoch 00004: val_acc did not improve from 0.57874
Epoch 5/10

  64/4566 [..............................] - ETA: 6:24 - loss: 0.7497 - acc: 0.4219
 128/4566 [..............................] - ETA: 6:51 - loss: 0.7197 - acc: 0.5000
 192/4566 [>.............................] - ETA: 7:02 - loss: 0.7150 - acc: 0.5156
 256/4566 [>.............................] - ETA: 6:46 - loss: 0.7083 - acc: 0.5195
 320/4566 [=>............................] - ETA: 6:39 - loss: 0.7048 - acc: 0.5375
 384/4566 [=>............................] - ETA: 6:26 - loss: 0.7012 - acc: 0.5417
 448/4566 [=>............................] - ETA: 6:21 - loss: 0.6944 - acc: 0.5558
 512/4566 [==>...........................] - ETA: 6:13 - loss: 0.6951 - acc: 0.5469
 576/4566 [==>...........................] - ETA: 6:09 - loss: 0.6947 - acc: 0.5417
 640/4566 [===>..........................] - ETA: 6:04 - loss: 0.6959 - acc: 0.5297
 704/4566 [===>..........................] - ETA: 6:26 - loss: 0.6946 - acc: 0.5369
 768/4566 [====>.........................] - ETA: 7:04 - loss: 0.6946 - acc: 0.5417
 832/4566 [====>.........................] - ETA: 7:32 - loss: 0.6920 - acc: 0.5469
 896/4566 [====>.........................] - ETA: 7:52 - loss: 0.6938 - acc: 0.5469
 960/4566 [=====>........................] - ETA: 8:09 - loss: 0.6951 - acc: 0.5469
1024/4566 [=====>........................] - ETA: 8:21 - loss: 0.6950 - acc: 0.5469
1088/4566 [======>.......................] - ETA: 8:06 - loss: 0.6941 - acc: 0.5496
1152/4566 [======>.......................] - ETA: 7:48 - loss: 0.6921 - acc: 0.5538
1216/4566 [======>.......................] - ETA: 7:30 - loss: 0.6929 - acc: 0.5502
1280/4566 [=======>......................] - ETA: 7:13 - loss: 0.6938 - acc: 0.5484
1344/4566 [=======>......................] - ETA: 7:00 - loss: 0.6937 - acc: 0.5476
1408/4566 [========>.....................] - ETA: 6:47 - loss: 0.6937 - acc: 0.5469
1472/4566 [========>.....................] - ETA: 6:34 - loss: 0.6959 - acc: 0.5401
1536/4566 [=========>....................] - ETA: 6:21 - loss: 0.6967 - acc: 0.5378
1600/4566 [=========>....................] - ETA: 6:08 - loss: 0.6948 - acc: 0.5444
1664/4566 [=========>....................] - ETA: 5:56 - loss: 0.6941 - acc: 0.5457
1728/4566 [==========>...................] - ETA: 5:46 - loss: 0.6932 - acc: 0.5457
1792/4566 [==========>...................] - ETA: 5:35 - loss: 0.6919 - acc: 0.5491
1856/4566 [===========>..................] - ETA: 5:24 - loss: 0.6910 - acc: 0.5501
1920/4566 [===========>..................] - ETA: 5:13 - loss: 0.6911 - acc: 0.5479
1984/4566 [============>.................] - ETA: 5:03 - loss: 0.6916 - acc: 0.5459
2048/4566 [============>.................] - ETA: 5:00 - loss: 0.6911 - acc: 0.5454
2112/4566 [============>.................] - ETA: 5:02 - loss: 0.6911 - acc: 0.5450
2176/4566 [=============>................] - ETA: 5:01 - loss: 0.6913 - acc: 0.5427
2240/4566 [=============>................] - ETA: 5:00 - loss: 0.6913 - acc: 0.5442
2304/4566 [==============>...............] - ETA: 4:57 - loss: 0.6907 - acc: 0.5464
2368/4566 [==============>...............] - ETA: 4:54 - loss: 0.6897 - acc: 0.5490
2432/4566 [==============>...............] - ETA: 4:45 - loss: 0.6886 - acc: 0.5510
2496/4566 [===============>..............] - ETA: 4:34 - loss: 0.6889 - acc: 0.5501
2560/4566 [===============>..............] - ETA: 4:23 - loss: 0.6895 - acc: 0.5484
2624/4566 [================>.............] - ETA: 4:13 - loss: 0.6897 - acc: 0.5480
2688/4566 [================>.............] - ETA: 4:02 - loss: 0.6899 - acc: 0.5476
2752/4566 [=================>............] - ETA: 3:52 - loss: 0.6897 - acc: 0.5483
2816/4566 [=================>............] - ETA: 3:43 - loss: 0.6895 - acc: 0.5476
2880/4566 [=================>............] - ETA: 3:33 - loss: 0.6892 - acc: 0.5483
2944/4566 [==================>...........] - ETA: 3:24 - loss: 0.6891 - acc: 0.5489
3008/4566 [==================>...........] - ETA: 3:15 - loss: 0.6890 - acc: 0.5509
3072/4566 [===================>..........] - ETA: 3:06 - loss: 0.6890 - acc: 0.5521
3136/4566 [===================>..........] - ETA: 2:57 - loss: 0.6890 - acc: 0.5520
3200/4566 [====================>.........] - ETA: 2:48 - loss: 0.6891 - acc: 0.5503
3264/4566 [====================>.........] - ETA: 2:39 - loss: 0.6893 - acc: 0.5499
3328/4566 [====================>.........] - ETA: 2:31 - loss: 0.6893 - acc: 0.5496
3392/4566 [=====================>........] - ETA: 2:24 - loss: 0.6893 - acc: 0.5489
3456/4566 [=====================>........] - ETA: 2:18 - loss: 0.6889 - acc: 0.5503
3520/4566 [======================>.......] - ETA: 2:12 - loss: 0.6893 - acc: 0.5494
3584/4566 [======================>.......] - ETA: 2:06 - loss: 0.6903 - acc: 0.5460
3648/4566 [======================>.......] - ETA: 1:59 - loss: 0.6898 - acc: 0.5474
3712/4566 [=======================>......] - ETA: 1:52 - loss: 0.6893 - acc: 0.5485
3776/4566 [=======================>......] - ETA: 1:44 - loss: 0.6894 - acc: 0.5493
3840/4566 [========================>.....] - ETA: 1:35 - loss: 0.6890 - acc: 0.5500
3904/4566 [========================>.....] - ETA: 1:26 - loss: 0.6888 - acc: 0.5497
3968/4566 [=========================>....] - ETA: 1:17 - loss: 0.6890 - acc: 0.5502
4032/4566 [=========================>....] - ETA: 1:08 - loss: 0.6890 - acc: 0.5489
4096/4566 [=========================>....] - ETA: 1:00 - loss: 0.6892 - acc: 0.5491
4160/4566 [==========================>...] - ETA: 51s - loss: 0.6887 - acc: 0.5512 
4224/4566 [==========================>...] - ETA: 43s - loss: 0.6887 - acc: 0.5500
4288/4566 [===========================>..] - ETA: 35s - loss: 0.6887 - acc: 0.5497
4352/4566 [===========================>..] - ETA: 26s - loss: 0.6887 - acc: 0.5494
4416/4566 [============================>.] - ETA: 18s - loss: 0.6884 - acc: 0.5505
4480/4566 [============================>.] - ETA: 10s - loss: 0.6883 - acc: 0.5511
4544/4566 [============================>.] - ETA: 2s - loss: 0.6882 - acc: 0.5508 
4566/4566 [==============================] - 590s 129ms/step - loss: 0.6883 - acc: 0.5502 - val_loss: 0.6813 - val_acc: 0.5571

Epoch 00005: val_acc did not improve from 0.57874
Epoch 6/10

  64/4566 [..............................] - ETA: 18:08 - loss: 0.6775 - acc: 0.6094
 128/4566 [..............................] - ETA: 18:07 - loss: 0.6776 - acc: 0.6016
 192/4566 [>.............................] - ETA: 17:18 - loss: 0.6763 - acc: 0.5781
 256/4566 [>.............................] - ETA: 17:13 - loss: 0.6745 - acc: 0.5859
 320/4566 [=>............................] - ETA: 16:53 - loss: 0.6759 - acc: 0.5844
 384/4566 [=>............................] - ETA: 15:10 - loss: 0.6862 - acc: 0.5469
 448/4566 [=>............................] - ETA: 13:44 - loss: 0.6906 - acc: 0.5312
 512/4566 [==>...........................] - ETA: 12:38 - loss: 0.6907 - acc: 0.5332
 576/4566 [==>...........................] - ETA: 11:43 - loss: 0.6890 - acc: 0.5365
 640/4566 [===>..........................] - ETA: 11:04 - loss: 0.6861 - acc: 0.5391
 704/4566 [===>..........................] - ETA: 10:26 - loss: 0.6885 - acc: 0.5327
 768/4566 [====>.........................] - ETA: 9:55 - loss: 0.6916 - acc: 0.5260 
 832/4566 [====>.........................] - ETA: 9:27 - loss: 0.6922 - acc: 0.5276
 896/4566 [====>.........................] - ETA: 9:00 - loss: 0.6926 - acc: 0.5246
 960/4566 [=====>........................] - ETA: 8:37 - loss: 0.6913 - acc: 0.5292
1024/4566 [=====>........................] - ETA: 8:15 - loss: 0.6894 - acc: 0.5332
1088/4566 [======>.......................] - ETA: 7:54 - loss: 0.6906 - acc: 0.5248
1152/4566 [======>.......................] - ETA: 7:37 - loss: 0.6916 - acc: 0.5243
1216/4566 [======>.......................] - ETA: 7:21 - loss: 0.6927 - acc: 0.5271
1280/4566 [=======>......................] - ETA: 7:05 - loss: 0.6907 - acc: 0.5320
1344/4566 [=======>......................] - ETA: 7:04 - loss: 0.6882 - acc: 0.5387
1408/4566 [========>.....................] - ETA: 7:10 - loss: 0.6894 - acc: 0.5341
1472/4566 [========>.....................] - ETA: 7:13 - loss: 0.6881 - acc: 0.5380
1536/4566 [=========>....................] - ETA: 7:16 - loss: 0.6865 - acc: 0.5430
1600/4566 [=========>....................] - ETA: 7:15 - loss: 0.6867 - acc: 0.5444
1664/4566 [=========>....................] - ETA: 7:14 - loss: 0.6868 - acc: 0.5433
1728/4566 [==========>...................] - ETA: 7:03 - loss: 0.6862 - acc: 0.5451
1792/4566 [==========>...................] - ETA: 6:48 - loss: 0.6869 - acc: 0.5469
1856/4566 [===========>..................] - ETA: 6:33 - loss: 0.6872 - acc: 0.5474
1920/4566 [===========>..................] - ETA: 6:19 - loss: 0.6862 - acc: 0.5505
1984/4566 [============>.................] - ETA: 6:05 - loss: 0.6855 - acc: 0.5539
2048/4566 [============>.................] - ETA: 5:52 - loss: 0.6848 - acc: 0.5576
2112/4566 [============>.................] - ETA: 5:38 - loss: 0.6845 - acc: 0.5568
2176/4566 [=============>................] - ETA: 5:26 - loss: 0.6836 - acc: 0.5588
2240/4566 [=============>................] - ETA: 5:14 - loss: 0.6841 - acc: 0.5576
2304/4566 [==============>...............] - ETA: 5:02 - loss: 0.6846 - acc: 0.5569
2368/4566 [==============>...............] - ETA: 4:51 - loss: 0.6841 - acc: 0.5583
2432/4566 [==============>...............] - ETA: 4:40 - loss: 0.6834 - acc: 0.5600
2496/4566 [===============>..............] - ETA: 4:29 - loss: 0.6833 - acc: 0.5601
2560/4566 [===============>..............] - ETA: 4:18 - loss: 0.6825 - acc: 0.5609
2624/4566 [================>.............] - ETA: 4:08 - loss: 0.6825 - acc: 0.5610
2688/4566 [================>.............] - ETA: 4:01 - loss: 0.6830 - acc: 0.5595
2752/4566 [=================>............] - ETA: 3:57 - loss: 0.6826 - acc: 0.5592
2816/4566 [=================>............] - ETA: 3:53 - loss: 0.6821 - acc: 0.5604
2880/4566 [=================>............] - ETA: 3:48 - loss: 0.6823 - acc: 0.5601
2944/4566 [==================>...........] - ETA: 3:42 - loss: 0.6817 - acc: 0.5605
3008/4566 [==================>...........] - ETA: 3:36 - loss: 0.6820 - acc: 0.5598
3072/4566 [===================>..........] - ETA: 3:27 - loss: 0.6817 - acc: 0.5602
3136/4566 [===================>..........] - ETA: 3:17 - loss: 0.6821 - acc: 0.5599
3200/4566 [====================>.........] - ETA: 3:06 - loss: 0.6820 - acc: 0.5591
3264/4566 [====================>.........] - ETA: 2:56 - loss: 0.6814 - acc: 0.5610
3328/4566 [====================>.........] - ETA: 2:46 - loss: 0.6812 - acc: 0.5619
3392/4566 [=====================>........] - ETA: 2:37 - loss: 0.6808 - acc: 0.5628
3456/4566 [=====================>........] - ETA: 2:27 - loss: 0.6798 - acc: 0.5642
3520/4566 [======================>.......] - ETA: 2:17 - loss: 0.6798 - acc: 0.5636
3584/4566 [======================>.......] - ETA: 2:08 - loss: 0.6797 - acc: 0.5642
3648/4566 [======================>.......] - ETA: 1:59 - loss: 0.6791 - acc: 0.5658
3712/4566 [=======================>......] - ETA: 1:50 - loss: 0.6797 - acc: 0.5644
3776/4566 [=======================>......] - ETA: 1:41 - loss: 0.6796 - acc: 0.5659
3840/4566 [========================>.....] - ETA: 1:33 - loss: 0.6798 - acc: 0.5664
3904/4566 [========================>.....] - ETA: 1:24 - loss: 0.6801 - acc: 0.5663
3968/4566 [=========================>....] - ETA: 1:15 - loss: 0.6805 - acc: 0.5650
4032/4566 [=========================>....] - ETA: 1:07 - loss: 0.6814 - acc: 0.5623
4096/4566 [=========================>....] - ETA: 59s - loss: 0.6815 - acc: 0.5632 
4160/4566 [==========================>...] - ETA: 52s - loss: 0.6812 - acc: 0.5635
4224/4566 [==========================>...] - ETA: 44s - loss: 0.6806 - acc: 0.5656
4288/4566 [===========================>..] - ETA: 36s - loss: 0.6804 - acc: 0.5658
4352/4566 [===========================>..] - ETA: 28s - loss: 0.6807 - acc: 0.5653
4416/4566 [============================>.] - ETA: 20s - loss: 0.6813 - acc: 0.5634
4480/4566 [============================>.] - ETA: 11s - loss: 0.6810 - acc: 0.5643
4544/4566 [============================>.] - ETA: 2s - loss: 0.6806 - acc: 0.5654 
4566/4566 [==============================] - 619s 135ms/step - loss: 0.6808 - acc: 0.5648 - val_loss: 0.6772 - val_acc: 0.5846

Epoch 00006: val_acc improved from 0.57874 to 0.58465, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window07/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 7/10

  64/4566 [..............................] - ETA: 5:49 - loss: 0.6616 - acc: 0.5781
 128/4566 [..............................] - ETA: 5:46 - loss: 0.6793 - acc: 0.5547
 192/4566 [>.............................] - ETA: 5:44 - loss: 0.6611 - acc: 0.6094
 256/4566 [>.............................] - ETA: 5:35 - loss: 0.6719 - acc: 0.5781
 320/4566 [=>............................] - ETA: 5:30 - loss: 0.6755 - acc: 0.5875
 384/4566 [=>............................] - ETA: 5:20 - loss: 0.6786 - acc: 0.5703
 448/4566 [=>............................] - ETA: 5:19 - loss: 0.6832 - acc: 0.5580
 512/4566 [==>...........................] - ETA: 5:10 - loss: 0.6783 - acc: 0.5742
 576/4566 [==>...........................] - ETA: 5:05 - loss: 0.6775 - acc: 0.5747
 640/4566 [===>..........................] - ETA: 5:04 - loss: 0.6776 - acc: 0.5719
 704/4566 [===>..........................] - ETA: 5:12 - loss: 0.6798 - acc: 0.5696
 768/4566 [====>.........................] - ETA: 5:56 - loss: 0.6824 - acc: 0.5677
 832/4566 [====>.........................] - ETA: 6:29 - loss: 0.6830 - acc: 0.5721
 896/4566 [====>.........................] - ETA: 6:55 - loss: 0.6836 - acc: 0.5670
 960/4566 [=====>........................] - ETA: 7:17 - loss: 0.6852 - acc: 0.5625
1024/4566 [=====>........................] - ETA: 7:32 - loss: 0.6835 - acc: 0.5645
1088/4566 [======>.......................] - ETA: 7:21 - loss: 0.6836 - acc: 0.5643
1152/4566 [======>.......................] - ETA: 7:04 - loss: 0.6838 - acc: 0.5599
1216/4566 [======>.......................] - ETA: 6:47 - loss: 0.6845 - acc: 0.5584
1280/4566 [=======>......................] - ETA: 6:32 - loss: 0.6831 - acc: 0.5625
1344/4566 [=======>......................] - ETA: 6:17 - loss: 0.6818 - acc: 0.5655
1408/4566 [========>.....................] - ETA: 6:05 - loss: 0.6829 - acc: 0.5618
1472/4566 [========>.....................] - ETA: 5:52 - loss: 0.6832 - acc: 0.5591
1536/4566 [=========>....................] - ETA: 5:41 - loss: 0.6824 - acc: 0.5612
1600/4566 [=========>....................] - ETA: 5:30 - loss: 0.6833 - acc: 0.5569
1664/4566 [=========>....................] - ETA: 5:19 - loss: 0.6822 - acc: 0.5583
1728/4566 [==========>...................] - ETA: 5:08 - loss: 0.6831 - acc: 0.5584
1792/4566 [==========>...................] - ETA: 4:59 - loss: 0.6815 - acc: 0.5619
1856/4566 [===========>..................] - ETA: 4:49 - loss: 0.6814 - acc: 0.5625
1920/4566 [===========>..................] - ETA: 4:40 - loss: 0.6815 - acc: 0.5615
1984/4566 [============>.................] - ETA: 4:30 - loss: 0.6812 - acc: 0.5635
2048/4566 [============>.................] - ETA: 4:21 - loss: 0.6797 - acc: 0.5635
2112/4566 [============>.................] - ETA: 4:19 - loss: 0.6796 - acc: 0.5644
2176/4566 [=============>................] - ETA: 4:21 - loss: 0.6821 - acc: 0.5602
2240/4566 [=============>................] - ETA: 4:23 - loss: 0.6825 - acc: 0.5603
2304/4566 [==============>...............] - ETA: 4:22 - loss: 0.6834 - acc: 0.5577
2368/4566 [==============>...............] - ETA: 4:21 - loss: 0.6827 - acc: 0.5600
2432/4566 [==============>...............] - ETA: 4:18 - loss: 0.6826 - acc: 0.5584
2496/4566 [===============>..............] - ETA: 4:09 - loss: 0.6828 - acc: 0.5577
2560/4566 [===============>..............] - ETA: 4:00 - loss: 0.6832 - acc: 0.5570
2624/4566 [================>.............] - ETA: 3:50 - loss: 0.6847 - acc: 0.5556
2688/4566 [================>.............] - ETA: 3:41 - loss: 0.6846 - acc: 0.5558
2752/4566 [=================>............] - ETA: 3:31 - loss: 0.6843 - acc: 0.5556
2816/4566 [=================>............] - ETA: 3:22 - loss: 0.6834 - acc: 0.5575
2880/4566 [=================>............] - ETA: 3:13 - loss: 0.6831 - acc: 0.5576
2944/4566 [==================>...........] - ETA: 3:05 - loss: 0.6822 - acc: 0.5601
3008/4566 [==================>...........] - ETA: 2:56 - loss: 0.6820 - acc: 0.5602
3072/4566 [===================>..........] - ETA: 2:48 - loss: 0.6820 - acc: 0.5615
3136/4566 [===================>..........] - ETA: 2:40 - loss: 0.6832 - acc: 0.5596
3200/4566 [====================>.........] - ETA: 2:32 - loss: 0.6839 - acc: 0.5575
3264/4566 [====================>.........] - ETA: 2:24 - loss: 0.6836 - acc: 0.5576
3328/4566 [====================>.........] - ETA: 2:16 - loss: 0.6822 - acc: 0.5604
3392/4566 [=====================>........] - ETA: 2:08 - loss: 0.6820 - acc: 0.5610
3456/4566 [=====================>........] - ETA: 2:01 - loss: 0.6820 - acc: 0.5611
3520/4566 [======================>.......] - ETA: 1:54 - loss: 0.6810 - acc: 0.5631
3584/4566 [======================>.......] - ETA: 1:49 - loss: 0.6813 - acc: 0.5614
3648/4566 [======================>.......] - ETA: 1:44 - loss: 0.6803 - acc: 0.5633
3712/4566 [=======================>......] - ETA: 1:38 - loss: 0.6807 - acc: 0.5622
3776/4566 [=======================>......] - ETA: 1:32 - loss: 0.6811 - acc: 0.5609
3840/4566 [========================>.....] - ETA: 1:26 - loss: 0.6809 - acc: 0.5607
3904/4566 [========================>.....] - ETA: 1:18 - loss: 0.6813 - acc: 0.5589
3968/4566 [=========================>....] - ETA: 1:10 - loss: 0.6811 - acc: 0.5592
4032/4566 [=========================>....] - ETA: 1:02 - loss: 0.6811 - acc: 0.5588
4096/4566 [=========================>....] - ETA: 55s - loss: 0.6811 - acc: 0.5588 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6812 - acc: 0.5591
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6810 - acc: 0.5601
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6807 - acc: 0.5611
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6801 - acc: 0.5623
4416/4566 [============================>.] - ETA: 17s - loss: 0.6804 - acc: 0.5618
4480/4566 [============================>.] - ETA: 9s - loss: 0.6811 - acc: 0.5605 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6804 - acc: 0.5625
4566/4566 [==============================] - 531s 116ms/step - loss: 0.6803 - acc: 0.5624 - val_loss: 0.6728 - val_acc: 0.5945

Epoch 00007: val_acc improved from 0.58465 to 0.59449, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window07/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 8/10

  64/4566 [..............................] - ETA: 5:37 - loss: 0.6621 - acc: 0.6719
 128/4566 [..............................] - ETA: 5:22 - loss: 0.6583 - acc: 0.6328
 192/4566 [>.............................] - ETA: 8:03 - loss: 0.6635 - acc: 0.6042
 256/4566 [>.............................] - ETA: 9:40 - loss: 0.6605 - acc: 0.6094
 320/4566 [=>............................] - ETA: 10:40 - loss: 0.6594 - acc: 0.6000
 384/4566 [=>............................] - ETA: 11:18 - loss: 0.6639 - acc: 0.5885
 448/4566 [=>............................] - ETA: 11:40 - loss: 0.6620 - acc: 0.5871
 512/4566 [==>...........................] - ETA: 11:48 - loss: 0.6676 - acc: 0.5820
 576/4566 [==>...........................] - ETA: 11:06 - loss: 0.6697 - acc: 0.5781
 640/4566 [===>..........................] - ETA: 10:27 - loss: 0.6697 - acc: 0.5797
 704/4566 [===>..........................] - ETA: 9:53 - loss: 0.6667 - acc: 0.5881 
 768/4566 [====>.........................] - ETA: 9:19 - loss: 0.6666 - acc: 0.5859
 832/4566 [====>.........................] - ETA: 8:49 - loss: 0.6673 - acc: 0.5877
 896/4566 [====>.........................] - ETA: 8:23 - loss: 0.6656 - acc: 0.5904
 960/4566 [=====>........................] - ETA: 8:00 - loss: 0.6710 - acc: 0.5802
1024/4566 [=====>........................] - ETA: 7:39 - loss: 0.6705 - acc: 0.5820
1088/4566 [======>.......................] - ETA: 7:20 - loss: 0.6694 - acc: 0.5873
1152/4566 [======>.......................] - ETA: 7:03 - loss: 0.6711 - acc: 0.5833
1216/4566 [======>.......................] - ETA: 6:47 - loss: 0.6702 - acc: 0.5880
1280/4566 [=======>......................] - ETA: 6:33 - loss: 0.6689 - acc: 0.5906
1344/4566 [=======>......................] - ETA: 6:19 - loss: 0.6677 - acc: 0.5938
1408/4566 [========>.....................] - ETA: 6:06 - loss: 0.6686 - acc: 0.5888
1472/4566 [========>.....................] - ETA: 5:53 - loss: 0.6691 - acc: 0.5897
1536/4566 [=========>....................] - ETA: 5:42 - loss: 0.6701 - acc: 0.5892
1600/4566 [=========>....................] - ETA: 5:45 - loss: 0.6732 - acc: 0.5844
1664/4566 [=========>....................] - ETA: 5:49 - loss: 0.6722 - acc: 0.5871
1728/4566 [==========>...................] - ETA: 5:51 - loss: 0.6743 - acc: 0.5816
1792/4566 [==========>...................] - ETA: 5:53 - loss: 0.6756 - acc: 0.5804
1856/4566 [===========>..................] - ETA: 5:55 - loss: 0.6753 - acc: 0.5787
1920/4566 [===========>..................] - ETA: 5:51 - loss: 0.6755 - acc: 0.5786
1984/4566 [============>.................] - ETA: 5:40 - loss: 0.6766 - acc: 0.5771
2048/4566 [============>.................] - ETA: 5:28 - loss: 0.6799 - acc: 0.5723
2112/4566 [============>.................] - ETA: 5:17 - loss: 0.6800 - acc: 0.5734
2176/4566 [=============>................] - ETA: 5:05 - loss: 0.6785 - acc: 0.5749
2240/4566 [=============>................] - ETA: 4:53 - loss: 0.6776 - acc: 0.5777
2304/4566 [==============>...............] - ETA: 4:42 - loss: 0.6775 - acc: 0.5799
2368/4566 [==============>...............] - ETA: 4:31 - loss: 0.6774 - acc: 0.5790
2432/4566 [==============>...............] - ETA: 4:21 - loss: 0.6768 - acc: 0.5789
2496/4566 [===============>..............] - ETA: 4:11 - loss: 0.6777 - acc: 0.5765
2560/4566 [===============>..............] - ETA: 4:01 - loss: 0.6763 - acc: 0.5797
2624/4566 [================>.............] - ETA: 3:51 - loss: 0.6760 - acc: 0.5800
2688/4566 [================>.............] - ETA: 3:42 - loss: 0.6766 - acc: 0.5781
2752/4566 [=================>............] - ETA: 3:33 - loss: 0.6777 - acc: 0.5767
2816/4566 [=================>............] - ETA: 3:24 - loss: 0.6774 - acc: 0.5785
2880/4566 [=================>............] - ETA: 3:15 - loss: 0.6774 - acc: 0.5771
2944/4566 [==================>...........] - ETA: 3:07 - loss: 0.6776 - acc: 0.5764
3008/4566 [==================>...........] - ETA: 3:02 - loss: 0.6773 - acc: 0.5758
3072/4566 [===================>..........] - ETA: 2:58 - loss: 0.6771 - acc: 0.5765
3136/4566 [===================>..........] - ETA: 2:53 - loss: 0.6760 - acc: 0.5797
3200/4566 [====================>.........] - ETA: 2:48 - loss: 0.6769 - acc: 0.5784
3264/4566 [====================>.........] - ETA: 2:42 - loss: 0.6768 - acc: 0.5794
3328/4566 [====================>.........] - ETA: 2:36 - loss: 0.6766 - acc: 0.5790
3392/4566 [=====================>........] - ETA: 2:27 - loss: 0.6770 - acc: 0.5775
3456/4566 [=====================>........] - ETA: 2:18 - loss: 0.6769 - acc: 0.5773
3520/4566 [======================>.......] - ETA: 2:09 - loss: 0.6767 - acc: 0.5776
3584/4566 [======================>.......] - ETA: 2:01 - loss: 0.6771 - acc: 0.5765
3648/4566 [======================>.......] - ETA: 1:52 - loss: 0.6774 - acc: 0.5759
3712/4566 [=======================>......] - ETA: 1:43 - loss: 0.6762 - acc: 0.5781
3776/4566 [=======================>......] - ETA: 1:35 - loss: 0.6767 - acc: 0.5773
3840/4566 [========================>.....] - ETA: 1:27 - loss: 0.6777 - acc: 0.5760
3904/4566 [========================>.....] - ETA: 1:19 - loss: 0.6770 - acc: 0.5771
3968/4566 [=========================>....] - ETA: 1:11 - loss: 0.6774 - acc: 0.5764
4032/4566 [=========================>....] - ETA: 1:03 - loss: 0.6777 - acc: 0.5771
4096/4566 [=========================>....] - ETA: 55s - loss: 0.6780 - acc: 0.5774 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6783 - acc: 0.5764
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6781 - acc: 0.5774
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6778 - acc: 0.5779
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6771 - acc: 0.5784
4416/4566 [============================>.] - ETA: 17s - loss: 0.6762 - acc: 0.5806
4480/4566 [============================>.] - ETA: 10s - loss: 0.6763 - acc: 0.5806
4544/4566 [============================>.] - ETA: 2s - loss: 0.6771 - acc: 0.5794 
4566/4566 [==============================] - 593s 130ms/step - loss: 0.6773 - acc: 0.5793 - val_loss: 0.6701 - val_acc: 0.5787

Epoch 00008: val_acc did not improve from 0.59449
Epoch 9/10

  64/4566 [..............................] - ETA: 7:48 - loss: 0.6763 - acc: 0.6875
 128/4566 [..............................] - ETA: 7:06 - loss: 0.6724 - acc: 0.6172
 192/4566 [>.............................] - ETA: 6:42 - loss: 0.6729 - acc: 0.5833
 256/4566 [>.............................] - ETA: 6:25 - loss: 0.6713 - acc: 0.5859
 320/4566 [=>............................] - ETA: 6:09 - loss: 0.6722 - acc: 0.5750
 384/4566 [=>............................] - ETA: 6:01 - loss: 0.6710 - acc: 0.5807
 448/4566 [=>............................] - ETA: 5:49 - loss: 0.6749 - acc: 0.5625
 512/4566 [==>...........................] - ETA: 5:42 - loss: 0.6714 - acc: 0.5723
 576/4566 [==>...........................] - ETA: 5:34 - loss: 0.6683 - acc: 0.5799
 640/4566 [===>..........................] - ETA: 5:26 - loss: 0.6675 - acc: 0.5859
 704/4566 [===>..........................] - ETA: 5:17 - loss: 0.6657 - acc: 0.5881
 768/4566 [====>.........................] - ETA: 5:08 - loss: 0.6660 - acc: 0.5898
 832/4566 [====>.........................] - ETA: 5:04 - loss: 0.6681 - acc: 0.5889
 896/4566 [====>.........................] - ETA: 4:59 - loss: 0.6666 - acc: 0.5893
 960/4566 [=====>........................] - ETA: 4:53 - loss: 0.6669 - acc: 0.5875
1024/4566 [=====>........................] - ETA: 5:03 - loss: 0.6651 - acc: 0.5908
1088/4566 [======>.......................] - ETA: 5:24 - loss: 0.6652 - acc: 0.5919
1152/4566 [======>.......................] - ETA: 5:40 - loss: 0.6661 - acc: 0.5911
1216/4566 [======>.......................] - ETA: 5:53 - loss: 0.6655 - acc: 0.5929
1280/4566 [=======>......................] - ETA: 6:05 - loss: 0.6652 - acc: 0.5930
1344/4566 [=======>......................] - ETA: 6:12 - loss: 0.6667 - acc: 0.5915
1408/4566 [========>.....................] - ETA: 6:08 - loss: 0.6672 - acc: 0.5916
1472/4566 [========>.....................] - ETA: 5:57 - loss: 0.6663 - acc: 0.5910
1536/4566 [=========>....................] - ETA: 5:45 - loss: 0.6672 - acc: 0.5892
1600/4566 [=========>....................] - ETA: 5:34 - loss: 0.6674 - acc: 0.5894
1664/4566 [=========>....................] - ETA: 5:22 - loss: 0.6688 - acc: 0.5901
1728/4566 [==========>...................] - ETA: 5:11 - loss: 0.6713 - acc: 0.5851
1792/4566 [==========>...................] - ETA: 5:01 - loss: 0.6711 - acc: 0.5882
1856/4566 [===========>..................] - ETA: 4:52 - loss: 0.6713 - acc: 0.5867
1920/4566 [===========>..................] - ETA: 4:43 - loss: 0.6724 - acc: 0.5849
1984/4566 [============>.................] - ETA: 4:34 - loss: 0.6732 - acc: 0.5837
2048/4566 [============>.................] - ETA: 4:26 - loss: 0.6739 - acc: 0.5845
2112/4566 [============>.................] - ETA: 4:18 - loss: 0.6734 - acc: 0.5848
2176/4566 [=============>................] - ETA: 4:10 - loss: 0.6731 - acc: 0.5869
2240/4566 [=============>................] - ETA: 4:02 - loss: 0.6730 - acc: 0.5875
2304/4566 [==============>...............] - ETA: 3:53 - loss: 0.6738 - acc: 0.5872
2368/4566 [==============>...............] - ETA: 3:45 - loss: 0.6738 - acc: 0.5857
2432/4566 [==============>...............] - ETA: 3:40 - loss: 0.6731 - acc: 0.5855
2496/4566 [===============>..............] - ETA: 3:39 - loss: 0.6724 - acc: 0.5877
2560/4566 [===============>..............] - ETA: 3:38 - loss: 0.6716 - acc: 0.5898
2624/4566 [================>.............] - ETA: 3:37 - loss: 0.6701 - acc: 0.5938
2688/4566 [================>.............] - ETA: 3:35 - loss: 0.6711 - acc: 0.5923
2752/4566 [=================>............] - ETA: 3:31 - loss: 0.6705 - acc: 0.5919
2816/4566 [=================>............] - ETA: 3:24 - loss: 0.6703 - acc: 0.5920
2880/4566 [=================>............] - ETA: 3:15 - loss: 0.6701 - acc: 0.5913
2944/4566 [==================>...........] - ETA: 3:06 - loss: 0.6705 - acc: 0.5900
3008/4566 [==================>...........] - ETA: 2:58 - loss: 0.6697 - acc: 0.5918
3072/4566 [===================>..........] - ETA: 2:49 - loss: 0.6701 - acc: 0.5905
3136/4566 [===================>..........] - ETA: 2:41 - loss: 0.6702 - acc: 0.5906
3200/4566 [====================>.........] - ETA: 2:32 - loss: 0.6706 - acc: 0.5897
3264/4566 [====================>.........] - ETA: 2:24 - loss: 0.6713 - acc: 0.5873
3328/4566 [====================>.........] - ETA: 2:16 - loss: 0.6711 - acc: 0.5886
3392/4566 [=====================>........] - ETA: 2:08 - loss: 0.6713 - acc: 0.5884
3456/4566 [=====================>........] - ETA: 2:00 - loss: 0.6720 - acc: 0.5868
3520/4566 [======================>.......] - ETA: 1:53 - loss: 0.6719 - acc: 0.5869
3584/4566 [======================>.......] - ETA: 1:45 - loss: 0.6722 - acc: 0.5857
3648/4566 [======================>.......] - ETA: 1:38 - loss: 0.6726 - acc: 0.5850
3712/4566 [=======================>......] - ETA: 1:31 - loss: 0.6731 - acc: 0.5846
3776/4566 [=======================>......] - ETA: 1:23 - loss: 0.6730 - acc: 0.5847
3840/4566 [========================>.....] - ETA: 1:16 - loss: 0.6724 - acc: 0.5859
3904/4566 [========================>.....] - ETA: 1:10 - loss: 0.6729 - acc: 0.5843
3968/4566 [=========================>....] - ETA: 1:05 - loss: 0.6734 - acc: 0.5834
4032/4566 [=========================>....] - ETA: 59s - loss: 0.6733 - acc: 0.5828 
4096/4566 [=========================>....] - ETA: 52s - loss: 0.6730 - acc: 0.5833
4160/4566 [==========================>...] - ETA: 46s - loss: 0.6733 - acc: 0.5832
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6733 - acc: 0.5838
4288/4566 [===========================>..] - ETA: 31s - loss: 0.6733 - acc: 0.5837
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6730 - acc: 0.5846
4416/4566 [============================>.] - ETA: 17s - loss: 0.6732 - acc: 0.5849
4480/4566 [============================>.] - ETA: 9s - loss: 0.6734 - acc: 0.5853 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6735 - acc: 0.5845
4566/4566 [==============================] - 526s 115ms/step - loss: 0.6735 - acc: 0.5845 - val_loss: 0.6763 - val_acc: 0.5709

Epoch 00009: val_acc did not improve from 0.59449
Epoch 10/10

  64/4566 [..............................] - ETA: 5:30 - loss: 0.7070 - acc: 0.5312
 128/4566 [..............................] - ETA: 5:24 - loss: 0.7104 - acc: 0.5156
 192/4566 [>.............................] - ETA: 5:20 - loss: 0.6941 - acc: 0.5312
 256/4566 [>.............................] - ETA: 5:08 - loss: 0.6872 - acc: 0.5586
 320/4566 [=>............................] - ETA: 5:08 - loss: 0.6885 - acc: 0.5531
 384/4566 [=>............................] - ETA: 5:06 - loss: 0.6906 - acc: 0.5573
 448/4566 [=>............................] - ETA: 5:00 - loss: 0.6872 - acc: 0.5670
 512/4566 [==>...........................] - ETA: 4:52 - loss: 0.6841 - acc: 0.5723
 576/4566 [==>...........................] - ETA: 4:51 - loss: 0.6815 - acc: 0.5764
 640/4566 [===>..........................] - ETA: 5:33 - loss: 0.6789 - acc: 0.5844
 704/4566 [===>..........................] - ETA: 6:13 - loss: 0.6772 - acc: 0.5881
 768/4566 [====>.........................] - ETA: 6:40 - loss: 0.6783 - acc: 0.5898
 832/4566 [====>.........................] - ETA: 7:05 - loss: 0.6773 - acc: 0.5901
 896/4566 [====>.........................] - ETA: 7:25 - loss: 0.6768 - acc: 0.5904
 960/4566 [=====>........................] - ETA: 7:33 - loss: 0.6777 - acc: 0.5917
1024/4566 [=====>........................] - ETA: 7:11 - loss: 0.6768 - acc: 0.5947
1088/4566 [======>.......................] - ETA: 6:53 - loss: 0.6751 - acc: 0.5974
1152/4566 [======>.......................] - ETA: 6:36 - loss: 0.6753 - acc: 0.5972
1216/4566 [======>.......................] - ETA: 6:19 - loss: 0.6751 - acc: 0.5938
1280/4566 [=======>......................] - ETA: 6:03 - loss: 0.6732 - acc: 0.5961
1344/4566 [=======>......................] - ETA: 5:50 - loss: 0.6725 - acc: 0.5967
1408/4566 [========>.....................] - ETA: 5:37 - loss: 0.6747 - acc: 0.5902
1472/4566 [========>.....................] - ETA: 5:24 - loss: 0.6745 - acc: 0.5917
1536/4566 [=========>....................] - ETA: 5:12 - loss: 0.6757 - acc: 0.5892
1600/4566 [=========>....................] - ETA: 5:01 - loss: 0.6743 - acc: 0.5919
1664/4566 [=========>....................] - ETA: 4:49 - loss: 0.6739 - acc: 0.5907
1728/4566 [==========>...................] - ETA: 4:39 - loss: 0.6737 - acc: 0.5885
1792/4566 [==========>...................] - ETA: 4:30 - loss: 0.6718 - acc: 0.5926
1856/4566 [===========>..................] - ETA: 4:20 - loss: 0.6728 - acc: 0.5894
1920/4566 [===========>..................] - ETA: 4:11 - loss: 0.6737 - acc: 0.5891
1984/4566 [============>.................] - ETA: 4:02 - loss: 0.6744 - acc: 0.5882
2048/4566 [============>.................] - ETA: 3:54 - loss: 0.6745 - acc: 0.5874
2112/4566 [============>.................] - ETA: 3:45 - loss: 0.6744 - acc: 0.5881
2176/4566 [=============>................] - ETA: 3:45 - loss: 0.6744 - acc: 0.5864
2240/4566 [=============>................] - ETA: 3:47 - loss: 0.6763 - acc: 0.5808
2304/4566 [==============>...............] - ETA: 3:48 - loss: 0.6766 - acc: 0.5807
2368/4566 [==============>...............] - ETA: 3:47 - loss: 0.6769 - acc: 0.5815
2432/4566 [==============>...............] - ETA: 3:47 - loss: 0.6766 - acc: 0.5818
2496/4566 [===============>..............] - ETA: 3:44 - loss: 0.6770 - acc: 0.5797
2560/4566 [===============>..............] - ETA: 3:35 - loss: 0.6769 - acc: 0.5805
2624/4566 [================>.............] - ETA: 3:26 - loss: 0.6768 - acc: 0.5808
2688/4566 [================>.............] - ETA: 3:17 - loss: 0.6764 - acc: 0.5822
2752/4566 [=================>............] - ETA: 3:08 - loss: 0.6763 - acc: 0.5821
2816/4566 [=================>............] - ETA: 3:00 - loss: 0.6755 - acc: 0.5842
2880/4566 [=================>............] - ETA: 2:52 - loss: 0.6758 - acc: 0.5844
2944/4566 [==================>...........] - ETA: 2:44 - loss: 0.6758 - acc: 0.5853
3008/4566 [==================>...........] - ETA: 2:36 - loss: 0.6758 - acc: 0.5864
3072/4566 [===================>..........] - ETA: 2:28 - loss: 0.6752 - acc: 0.5879
3136/4566 [===================>..........] - ETA: 2:20 - loss: 0.6749 - acc: 0.5883
3200/4566 [====================>.........] - ETA: 2:13 - loss: 0.6755 - acc: 0.5881
3264/4566 [====================>.........] - ETA: 2:06 - loss: 0.6760 - acc: 0.5867
3328/4566 [====================>.........] - ETA: 1:59 - loss: 0.6758 - acc: 0.5877
3392/4566 [=====================>........] - ETA: 1:52 - loss: 0.6762 - acc: 0.5870
3456/4566 [=====================>........] - ETA: 1:45 - loss: 0.6755 - acc: 0.5880
3520/4566 [======================>.......] - ETA: 1:38 - loss: 0.6758 - acc: 0.5861
3584/4566 [======================>.......] - ETA: 1:32 - loss: 0.6761 - acc: 0.5848
3648/4566 [======================>.......] - ETA: 1:25 - loss: 0.6769 - acc: 0.5828
3712/4566 [=======================>......] - ETA: 1:20 - loss: 0.6769 - acc: 0.5824
3776/4566 [=======================>......] - ETA: 1:15 - loss: 0.6772 - acc: 0.5826
3840/4566 [========================>.....] - ETA: 1:10 - loss: 0.6768 - acc: 0.5839
3904/4566 [========================>.....] - ETA: 1:05 - loss: 0.6755 - acc: 0.5856
3968/4566 [=========================>....] - ETA: 1:00 - loss: 0.6759 - acc: 0.5849
4032/4566 [=========================>....] - ETA: 54s - loss: 0.6757 - acc: 0.5858 
4096/4566 [=========================>....] - ETA: 48s - loss: 0.6754 - acc: 0.5862
4160/4566 [==========================>...] - ETA: 41s - loss: 0.6757 - acc: 0.5853
4224/4566 [==========================>...] - ETA: 34s - loss: 0.6753 - acc: 0.5866
4288/4566 [===========================>..] - ETA: 28s - loss: 0.6752 - acc: 0.5865
4352/4566 [===========================>..] - ETA: 21s - loss: 0.6747 - acc: 0.5878
4416/4566 [============================>.] - ETA: 14s - loss: 0.6743 - acc: 0.5885
4480/4566 [============================>.] - ETA: 8s - loss: 0.6746 - acc: 0.5886 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6741 - acc: 0.5893
4566/4566 [==============================] - 460s 101ms/step - loss: 0.6741 - acc: 0.5896 - val_loss: 0.6841 - val_acc: 0.5591

Epoch 00010: val_acc did not improve from 0.59449
Saved model to disk
