/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f08b34dbd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f08b34dbd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f050e914110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f050e914110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f091943cc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f091943cc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f0919401950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f0919401950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f091943c9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f091943c9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08b32c4590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08b32c4590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f09216f9850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f09216f9850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08b3401610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08b3401610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08b323c7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08b323c7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08b30dcfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08b30dcfd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f09218cb8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f09218cb8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08b323c190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08b323c190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08b340e910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08b340e910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08b2fa1e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08b2fa1e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08aadf6750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08aadf6750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08b2ed6790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08b2ed6790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08b2fa1b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08b2fa1b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08aadf3450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08aadf3450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08aabe4790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08aabe4790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08aaacc3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08aaacc3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08aabc3510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08aabc3510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08aac07710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08aac07710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08aab4ef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08aab4ef50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f09439779d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f09439779d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08aa982f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08aa982f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08aa88c510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08aa88c510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f0919382050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f0919382050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08aa7f6c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08aa7f6c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08a25a5a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08a25a5a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08a2466f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08a2466f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08a250a110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08a250a110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08aa92db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08aa92db90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08a2352590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08a2352590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08a2246d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08a2246d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08a2175a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08a2175a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08a24972d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08a24972d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08a22d1a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08a22d1a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08a22efbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08a22efbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08a1f75890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08a1f75890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08b324fb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08b324fb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f0899d08f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f0899d08f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08b3215bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08b3215bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08a1e06ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08a1e06ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f0899c6a850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f0899c6a850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f0899c185d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f0899c185d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f0899a27910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f0899a27910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f0899c6a810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f0899c6a810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f0899b21e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f0899b21e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f0899917110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f0899917110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f089988b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f089988b1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f089980a590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f089980a590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f0899917a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f0899917a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f0899815250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f0899815250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08a1f7bb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08a1f7bb50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08915313d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08915313d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f089962bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f089962bc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f0899812a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f0899812a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f0899722810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f0899722810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08912c06d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f08912c06d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08915228d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f08915228d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f089132cd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f089132cd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08914d9190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f08914d9190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08913bec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f08913bec50>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-17 10:39:15.943414: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-17 10:39:15.997949: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-17 10:39:16.051960: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556709e35220 executing computations on platform Host. Devices:
2022-11-17 10:39:16.052087: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-17 10:39:16.590273: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window03.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 23:45 - loss: 0.7001 - acc: 0.5312
 128/4566 [..............................] - ETA: 19:37 - loss: 0.7891 - acc: 0.5391
 192/4566 [>.............................] - ETA: 17:05 - loss: 0.7707 - acc: 0.5417
 256/4566 [>.............................] - ETA: 15:45 - loss: 0.7514 - acc: 0.5391
 320/4566 [=>............................] - ETA: 14:28 - loss: 0.7558 - acc: 0.5219
 384/4566 [=>............................] - ETA: 13:31 - loss: 0.7523 - acc: 0.5312
 448/4566 [=>............................] - ETA: 12:36 - loss: 0.7356 - acc: 0.5558
 512/4566 [==>...........................] - ETA: 12:03 - loss: 0.7388 - acc: 0.5430
 576/4566 [==>...........................] - ETA: 11:50 - loss: 0.7420 - acc: 0.5365
 640/4566 [===>..........................] - ETA: 12:08 - loss: 0.7437 - acc: 0.5297
 704/4566 [===>..........................] - ETA: 12:17 - loss: 0.7445 - acc: 0.5284
 768/4566 [====>.........................] - ETA: 12:19 - loss: 0.7511 - acc: 0.5260
 832/4566 [====>.........................] - ETA: 12:10 - loss: 0.7504 - acc: 0.5264
 896/4566 [====>.........................] - ETA: 12:02 - loss: 0.7499 - acc: 0.5212
 960/4566 [=====>........................] - ETA: 11:56 - loss: 0.7467 - acc: 0.5260
1024/4566 [=====>........................] - ETA: 11:25 - loss: 0.7429 - acc: 0.5273
1088/4566 [======>.......................] - ETA: 10:56 - loss: 0.7371 - acc: 0.5331
1152/4566 [======>.......................] - ETA: 10:30 - loss: 0.7346 - acc: 0.5321
1216/4566 [======>.......................] - ETA: 10:06 - loss: 0.7343 - acc: 0.5263
1280/4566 [=======>......................] - ETA: 9:45 - loss: 0.7333 - acc: 0.5281 
1344/4566 [=======>......................] - ETA: 9:25 - loss: 0.7348 - acc: 0.5216
1408/4566 [========>.....................] - ETA: 9:05 - loss: 0.7367 - acc: 0.5213
1472/4566 [========>.....................] - ETA: 8:48 - loss: 0.7375 - acc: 0.5177
1536/4566 [=========>....................] - ETA: 8:31 - loss: 0.7363 - acc: 0.5182
1600/4566 [=========>....................] - ETA: 8:14 - loss: 0.7328 - acc: 0.5231
1664/4566 [=========>....................] - ETA: 7:59 - loss: 0.7321 - acc: 0.5246
1728/4566 [==========>...................] - ETA: 7:43 - loss: 0.7338 - acc: 0.5203
1792/4566 [==========>...................] - ETA: 7:28 - loss: 0.7350 - acc: 0.5173
1856/4566 [===========>..................] - ETA: 7:14 - loss: 0.7333 - acc: 0.5189
1920/4566 [===========>..................] - ETA: 7:02 - loss: 0.7325 - acc: 0.5177
1984/4566 [============>.................] - ETA: 6:58 - loss: 0.7342 - acc: 0.5131
2048/4566 [============>.................] - ETA: 6:54 - loss: 0.7345 - acc: 0.5122
2112/4566 [============>.................] - ETA: 6:49 - loss: 0.7327 - acc: 0.5128
2176/4566 [=============>................] - ETA: 6:54 - loss: 0.7324 - acc: 0.5115
2240/4566 [=============>................] - ETA: 6:48 - loss: 0.7322 - acc: 0.5085
2304/4566 [==============>...............] - ETA: 6:38 - loss: 0.7318 - acc: 0.5095
2368/4566 [==============>...............] - ETA: 6:24 - loss: 0.7309 - acc: 0.5106
2432/4566 [==============>...............] - ETA: 6:10 - loss: 0.7293 - acc: 0.5127
2496/4566 [===============>..............] - ETA: 5:56 - loss: 0.7284 - acc: 0.5148
2560/4566 [===============>..............] - ETA: 5:42 - loss: 0.7266 - acc: 0.5180
2624/4566 [================>.............] - ETA: 5:28 - loss: 0.7267 - acc: 0.5175
2688/4566 [================>.............] - ETA: 5:15 - loss: 0.7262 - acc: 0.5164
2752/4566 [=================>............] - ETA: 5:02 - loss: 0.7283 - acc: 0.5134
2816/4566 [=================>............] - ETA: 4:50 - loss: 0.7267 - acc: 0.5160
2880/4566 [=================>............] - ETA: 4:38 - loss: 0.7251 - acc: 0.5181
2944/4566 [==================>...........] - ETA: 4:25 - loss: 0.7236 - acc: 0.5207
3008/4566 [==================>...........] - ETA: 4:14 - loss: 0.7229 - acc: 0.5203
3072/4566 [===================>..........] - ETA: 4:02 - loss: 0.7227 - acc: 0.5202
3136/4566 [===================>..........] - ETA: 3:51 - loss: 0.7216 - acc: 0.5217
3200/4566 [====================>.........] - ETA: 3:42 - loss: 0.7219 - acc: 0.5209
3264/4566 [====================>.........] - ETA: 3:33 - loss: 0.7220 - acc: 0.5218
3328/4566 [====================>.........] - ETA: 3:25 - loss: 0.7220 - acc: 0.5213
3392/4566 [=====================>........] - ETA: 3:15 - loss: 0.7224 - acc: 0.5209
3456/4566 [=====================>........] - ETA: 3:06 - loss: 0.7224 - acc: 0.5205
3520/4566 [======================>.......] - ETA: 2:56 - loss: 0.7224 - acc: 0.5207
3584/4566 [======================>.......] - ETA: 2:46 - loss: 0.7238 - acc: 0.5179
3648/4566 [======================>.......] - ETA: 2:35 - loss: 0.7239 - acc: 0.5162
3712/4566 [=======================>......] - ETA: 2:23 - loss: 0.7239 - acc: 0.5167
3776/4566 [=======================>......] - ETA: 2:12 - loss: 0.7227 - acc: 0.5180
3840/4566 [========================>.....] - ETA: 2:00 - loss: 0.7223 - acc: 0.5177
3904/4566 [========================>.....] - ETA: 1:49 - loss: 0.7217 - acc: 0.5184
3968/4566 [=========================>....] - ETA: 1:38 - loss: 0.7218 - acc: 0.5184
4032/4566 [=========================>....] - ETA: 1:27 - loss: 0.7218 - acc: 0.5169
4096/4566 [=========================>....] - ETA: 1:16 - loss: 0.7216 - acc: 0.5173
4160/4566 [==========================>...] - ETA: 1:06 - loss: 0.7210 - acc: 0.5171
4224/4566 [==========================>...] - ETA: 55s - loss: 0.7212 - acc: 0.5170 
4288/4566 [===========================>..] - ETA: 45s - loss: 0.7211 - acc: 0.5173
4352/4566 [===========================>..] - ETA: 34s - loss: 0.7208 - acc: 0.5177
4416/4566 [============================>.] - ETA: 24s - loss: 0.7203 - acc: 0.5186
4480/4566 [============================>.] - ETA: 13s - loss: 0.7203 - acc: 0.5176
4544/4566 [============================>.] - ETA: 3s - loss: 0.7199 - acc: 0.5185 
4566/4566 [==============================] - 769s 168ms/step - loss: 0.7199 - acc: 0.5186 - val_loss: 0.6864 - val_acc: 0.5354

Epoch 00001: val_acc improved from -inf to 0.53543, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window03/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 17:53 - loss: 0.7071 - acc: 0.5469
 128/4566 [..............................] - ETA: 17:22 - loss: 0.7245 - acc: 0.4844
 192/4566 [>.............................] - ETA: 18:27 - loss: 0.6966 - acc: 0.4792
 256/4566 [>.............................] - ETA: 15:49 - loss: 0.7129 - acc: 0.4609
 320/4566 [=>............................] - ETA: 14:22 - loss: 0.7174 - acc: 0.4594
 384/4566 [=>............................] - ETA: 13:14 - loss: 0.7187 - acc: 0.4635
 448/4566 [=>............................] - ETA: 12:20 - loss: 0.7183 - acc: 0.4777
 512/4566 [==>...........................] - ETA: 11:37 - loss: 0.7172 - acc: 0.4824
 576/4566 [==>...........................] - ETA: 11:00 - loss: 0.7135 - acc: 0.4913
 640/4566 [===>..........................] - ETA: 10:33 - loss: 0.7128 - acc: 0.4922
 704/4566 [===>..........................] - ETA: 10:09 - loss: 0.7176 - acc: 0.4858
 768/4566 [====>.........................] - ETA: 9:46 - loss: 0.7179 - acc: 0.4909 
 832/4566 [====>.........................] - ETA: 9:24 - loss: 0.7160 - acc: 0.4928
 896/4566 [====>.........................] - ETA: 9:05 - loss: 0.7158 - acc: 0.4933
 960/4566 [=====>........................] - ETA: 8:49 - loss: 0.7130 - acc: 0.5000
1024/4566 [=====>........................] - ETA: 8:33 - loss: 0.7140 - acc: 0.4980
1088/4566 [======>.......................] - ETA: 8:18 - loss: 0.7144 - acc: 0.4936
1152/4566 [======>.......................] - ETA: 8:12 - loss: 0.7138 - acc: 0.4922
1216/4566 [======>.......................] - ETA: 8:19 - loss: 0.7119 - acc: 0.4975
1280/4566 [=======>......................] - ETA: 8:21 - loss: 0.7117 - acc: 0.4953
1344/4566 [=======>......................] - ETA: 8:20 - loss: 0.7084 - acc: 0.5060
1408/4566 [========>.....................] - ETA: 8:20 - loss: 0.7099 - acc: 0.5028
1472/4566 [========>.....................] - ETA: 8:18 - loss: 0.7097 - acc: 0.5048
1536/4566 [=========>....................] - ETA: 8:15 - loss: 0.7092 - acc: 0.5033
1600/4566 [=========>....................] - ETA: 8:03 - loss: 0.7098 - acc: 0.5031
1664/4566 [=========>....................] - ETA: 7:48 - loss: 0.7098 - acc: 0.5036
1728/4566 [==========>...................] - ETA: 7:34 - loss: 0.7114 - acc: 0.5012
1792/4566 [==========>...................] - ETA: 7:19 - loss: 0.7106 - acc: 0.5045
1856/4566 [===========>..................] - ETA: 7:05 - loss: 0.7104 - acc: 0.5043
1920/4566 [===========>..................] - ETA: 6:52 - loss: 0.7105 - acc: 0.5036
1984/4566 [============>.................] - ETA: 6:38 - loss: 0.7113 - acc: 0.5000
2048/4566 [============>.................] - ETA: 6:25 - loss: 0.7101 - acc: 0.5015
2112/4566 [============>.................] - ETA: 6:13 - loss: 0.7093 - acc: 0.5024
2176/4566 [=============>................] - ETA: 6:01 - loss: 0.7101 - acc: 0.5014
2240/4566 [=============>................] - ETA: 5:49 - loss: 0.7094 - acc: 0.5027
2304/4566 [==============>...............] - ETA: 5:38 - loss: 0.7083 - acc: 0.5026
2368/4566 [==============>...............] - ETA: 5:26 - loss: 0.7071 - acc: 0.5055
2432/4566 [==============>...............] - ETA: 5:14 - loss: 0.7074 - acc: 0.5053
2496/4566 [===============>..............] - ETA: 5:03 - loss: 0.7079 - acc: 0.5044
2560/4566 [===============>..............] - ETA: 4:55 - loss: 0.7080 - acc: 0.5059
2624/4566 [================>.............] - ETA: 4:49 - loss: 0.7075 - acc: 0.5069
2688/4566 [================>.............] - ETA: 4:43 - loss: 0.7077 - acc: 0.5060
2752/4566 [=================>............] - ETA: 4:36 - loss: 0.7066 - acc: 0.5065
2816/4566 [=================>............] - ETA: 4:29 - loss: 0.7053 - acc: 0.5085
2880/4566 [=================>............] - ETA: 4:21 - loss: 0.7052 - acc: 0.5080
2944/4566 [==================>...........] - ETA: 4:13 - loss: 0.7056 - acc: 0.5061
3008/4566 [==================>...........] - ETA: 4:03 - loss: 0.7052 - acc: 0.5083
3072/4566 [===================>..........] - ETA: 3:51 - loss: 0.7050 - acc: 0.5094
3136/4566 [===================>..........] - ETA: 3:40 - loss: 0.7051 - acc: 0.5089
3200/4566 [====================>.........] - ETA: 3:30 - loss: 0.7056 - acc: 0.5075
3264/4566 [====================>.........] - ETA: 3:19 - loss: 0.7052 - acc: 0.5074
3328/4566 [====================>.........] - ETA: 3:08 - loss: 0.7053 - acc: 0.5075
3392/4566 [=====================>........] - ETA: 2:58 - loss: 0.7050 - acc: 0.5071
3456/4566 [=====================>........] - ETA: 2:47 - loss: 0.7049 - acc: 0.5078
3520/4566 [======================>.......] - ETA: 2:37 - loss: 0.7053 - acc: 0.5054
3584/4566 [======================>.......] - ETA: 2:27 - loss: 0.7042 - acc: 0.5073
3648/4566 [======================>.......] - ETA: 2:16 - loss: 0.7039 - acc: 0.5082
3712/4566 [=======================>......] - ETA: 2:06 - loss: 0.7039 - acc: 0.5084
3776/4566 [=======================>......] - ETA: 1:57 - loss: 0.7041 - acc: 0.5087
3840/4566 [========================>.....] - ETA: 1:47 - loss: 0.7038 - acc: 0.5086
3904/4566 [========================>.....] - ETA: 1:37 - loss: 0.7036 - acc: 0.5090
3968/4566 [=========================>....] - ETA: 1:27 - loss: 0.7039 - acc: 0.5081
4032/4566 [=========================>....] - ETA: 1:19 - loss: 0.7037 - acc: 0.5087
4096/4566 [=========================>....] - ETA: 1:10 - loss: 0.7033 - acc: 0.5088
4160/4566 [==========================>...] - ETA: 1:01 - loss: 0.7030 - acc: 0.5091
4224/4566 [==========================>...] - ETA: 51s - loss: 0.7037 - acc: 0.5080 
4288/4566 [===========================>..] - ETA: 42s - loss: 0.7033 - acc: 0.5089
4352/4566 [===========================>..] - ETA: 32s - loss: 0.7040 - acc: 0.5078
4416/4566 [============================>.] - ETA: 23s - loss: 0.7033 - acc: 0.5093
4480/4566 [============================>.] - ETA: 13s - loss: 0.7031 - acc: 0.5100
4544/4566 [============================>.] - ETA: 3s - loss: 0.7022 - acc: 0.5121 
4566/4566 [==============================] - 719s 157ms/step - loss: 0.7020 - acc: 0.5123 - val_loss: 0.6867 - val_acc: 0.5551

Epoch 00002: val_acc improved from 0.53543 to 0.55512, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window03/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 3/10

  64/4566 [..............................] - ETA: 9:00 - loss: 0.7199 - acc: 0.4688
 128/4566 [..............................] - ETA: 8:59 - loss: 0.7204 - acc: 0.4609
 192/4566 [>.............................] - ETA: 9:02 - loss: 0.7115 - acc: 0.5052
 256/4566 [>.............................] - ETA: 8:54 - loss: 0.7093 - acc: 0.5273
 320/4566 [=>............................] - ETA: 8:42 - loss: 0.6981 - acc: 0.5406
 384/4566 [=>............................] - ETA: 8:27 - loss: 0.6926 - acc: 0.5469
 448/4566 [=>............................] - ETA: 8:16 - loss: 0.7041 - acc: 0.5335
 512/4566 [==>...........................] - ETA: 8:04 - loss: 0.7046 - acc: 0.5352
 576/4566 [==>...........................] - ETA: 7:59 - loss: 0.7010 - acc: 0.5486
 640/4566 [===>..........................] - ETA: 7:54 - loss: 0.7032 - acc: 0.5375
 704/4566 [===>..........................] - ETA: 8:22 - loss: 0.6976 - acc: 0.5497
 768/4566 [====>.........................] - ETA: 8:43 - loss: 0.7001 - acc: 0.5417
 832/4566 [====>.........................] - ETA: 9:01 - loss: 0.6998 - acc: 0.5433
 896/4566 [====>.........................] - ETA: 9:08 - loss: 0.6984 - acc: 0.5502
 960/4566 [=====>........................] - ETA: 9:15 - loss: 0.6972 - acc: 0.5521
1024/4566 [=====>........................] - ETA: 9:17 - loss: 0.6980 - acc: 0.5488
1088/4566 [======>.......................] - ETA: 9:10 - loss: 0.7007 - acc: 0.5395
1152/4566 [======>.......................] - ETA: 8:50 - loss: 0.6999 - acc: 0.5365
1216/4566 [======>.......................] - ETA: 8:32 - loss: 0.6996 - acc: 0.5354
1280/4566 [=======>......................] - ETA: 8:14 - loss: 0.6988 - acc: 0.5344
1344/4566 [=======>......................] - ETA: 8:01 - loss: 0.6973 - acc: 0.5350
1408/4566 [========>.....................] - ETA: 7:45 - loss: 0.6980 - acc: 0.5320
1472/4566 [========>.....................] - ETA: 7:30 - loss: 0.6986 - acc: 0.5306
1536/4566 [=========>....................] - ETA: 7:17 - loss: 0.6996 - acc: 0.5312
1600/4566 [=========>....................] - ETA: 7:04 - loss: 0.6988 - acc: 0.5331
1664/4566 [=========>....................] - ETA: 6:51 - loss: 0.6969 - acc: 0.5355
1728/4566 [==========>...................] - ETA: 6:38 - loss: 0.6953 - acc: 0.5347
1792/4566 [==========>...................] - ETA: 6:26 - loss: 0.6948 - acc: 0.5374
1856/4566 [===========>..................] - ETA: 6:14 - loss: 0.6944 - acc: 0.5388
1920/4566 [===========>..................] - ETA: 6:03 - loss: 0.6941 - acc: 0.5396
1984/4566 [============>.................] - ETA: 5:53 - loss: 0.6933 - acc: 0.5408
2048/4566 [============>.................] - ETA: 5:42 - loss: 0.6923 - acc: 0.5420
2112/4566 [============>.................] - ETA: 5:37 - loss: 0.6918 - acc: 0.5426
2176/4566 [=============>................] - ETA: 5:35 - loss: 0.6925 - acc: 0.5400
2240/4566 [=============>................] - ETA: 5:32 - loss: 0.6919 - acc: 0.5424
2304/4566 [==============>...............] - ETA: 5:28 - loss: 0.6937 - acc: 0.5395
2368/4566 [==============>...............] - ETA: 5:23 - loss: 0.6935 - acc: 0.5418
2432/4566 [==============>...............] - ETA: 5:17 - loss: 0.6926 - acc: 0.5424
2496/4566 [===============>..............] - ETA: 5:10 - loss: 0.6918 - acc: 0.5445
2560/4566 [===============>..............] - ETA: 4:59 - loss: 0.6920 - acc: 0.5437
2624/4566 [================>.............] - ETA: 4:47 - loss: 0.6915 - acc: 0.5461
2688/4566 [================>.............] - ETA: 4:36 - loss: 0.6928 - acc: 0.5435
2752/4566 [=================>............] - ETA: 4:25 - loss: 0.6928 - acc: 0.5422
2816/4566 [=================>............] - ETA: 4:14 - loss: 0.6926 - acc: 0.5415
2880/4566 [=================>............] - ETA: 4:03 - loss: 0.6929 - acc: 0.5420
2944/4566 [==================>...........] - ETA: 3:53 - loss: 0.6930 - acc: 0.5421
3008/4566 [==================>...........] - ETA: 3:43 - loss: 0.6936 - acc: 0.5409
3072/4566 [===================>..........] - ETA: 3:32 - loss: 0.6931 - acc: 0.5417
3136/4566 [===================>..........] - ETA: 3:22 - loss: 0.6934 - acc: 0.5405
3200/4566 [====================>.........] - ETA: 3:12 - loss: 0.6941 - acc: 0.5372
3264/4566 [====================>.........] - ETA: 3:02 - loss: 0.6938 - acc: 0.5377
3328/4566 [====================>.........] - ETA: 2:53 - loss: 0.6942 - acc: 0.5364
3392/4566 [=====================>........] - ETA: 2:43 - loss: 0.6946 - acc: 0.5357
3456/4566 [=====================>........] - ETA: 2:33 - loss: 0.6946 - acc: 0.5356
3520/4566 [======================>.......] - ETA: 2:24 - loss: 0.6944 - acc: 0.5361
3584/4566 [======================>.......] - ETA: 2:17 - loss: 0.6945 - acc: 0.5357
3648/4566 [======================>.......] - ETA: 2:09 - loss: 0.6949 - acc: 0.5348
3712/4566 [=======================>......] - ETA: 2:01 - loss: 0.6945 - acc: 0.5353
3776/4566 [=======================>......] - ETA: 1:53 - loss: 0.6941 - acc: 0.5350
3840/4566 [========================>.....] - ETA: 1:44 - loss: 0.6946 - acc: 0.5333
3904/4566 [========================>.....] - ETA: 1:35 - loss: 0.6948 - acc: 0.5318
3968/4566 [=========================>....] - ETA: 1:27 - loss: 0.6944 - acc: 0.5333
4032/4566 [=========================>....] - ETA: 1:17 - loss: 0.6939 - acc: 0.5342
4096/4566 [=========================>....] - ETA: 1:07 - loss: 0.6940 - acc: 0.5344
4160/4566 [==========================>...] - ETA: 58s - loss: 0.6935 - acc: 0.5351 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6931 - acc: 0.5343
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6926 - acc: 0.5352
4352/4566 [===========================>..] - ETA: 30s - loss: 0.6921 - acc: 0.5363
4416/4566 [============================>.] - ETA: 21s - loss: 0.6916 - acc: 0.5369
4480/4566 [============================>.] - ETA: 12s - loss: 0.6914 - acc: 0.5377
4544/4566 [============================>.] - ETA: 3s - loss: 0.6914 - acc: 0.5379 
4566/4566 [==============================] - 658s 144ms/step - loss: 0.6917 - acc: 0.5375 - val_loss: 0.6919 - val_acc: 0.5256

Epoch 00003: val_acc did not improve from 0.55512
Epoch 4/10

  64/4566 [..............................] - ETA: 8:04 - loss: 0.6904 - acc: 0.6250
 128/4566 [..............................] - ETA: 7:51 - loss: 0.6596 - acc: 0.6172
 192/4566 [>.............................] - ETA: 7:35 - loss: 0.6760 - acc: 0.5938
 256/4566 [>.............................] - ETA: 7:44 - loss: 0.6736 - acc: 0.5977
 320/4566 [=>............................] - ETA: 9:12 - loss: 0.6908 - acc: 0.5781
 384/4566 [=>............................] - ETA: 10:03 - loss: 0.6914 - acc: 0.5755
 448/4566 [=>............................] - ETA: 10:28 - loss: 0.6883 - acc: 0.5759
 512/4566 [==>...........................] - ETA: 10:42 - loss: 0.6933 - acc: 0.5605
 576/4566 [==>...........................] - ETA: 10:55 - loss: 0.6963 - acc: 0.5451
 640/4566 [===>..........................] - ETA: 11:06 - loss: 0.6994 - acc: 0.5375
 704/4566 [===>..........................] - ETA: 10:46 - loss: 0.7006 - acc: 0.5355
 768/4566 [====>.........................] - ETA: 10:14 - loss: 0.7013 - acc: 0.5312
 832/4566 [====>.........................] - ETA: 9:45 - loss: 0.7015 - acc: 0.5300 
 896/4566 [====>.........................] - ETA: 9:19 - loss: 0.7024 - acc: 0.5312
 960/4566 [=====>........................] - ETA: 8:55 - loss: 0.7023 - acc: 0.5292
1024/4566 [=====>........................] - ETA: 8:32 - loss: 0.6985 - acc: 0.5400
1088/4566 [======>.......................] - ETA: 8:13 - loss: 0.6979 - acc: 0.5395
1152/4566 [======>.......................] - ETA: 7:53 - loss: 0.6965 - acc: 0.5408
1216/4566 [======>.......................] - ETA: 7:36 - loss: 0.6956 - acc: 0.5403
1280/4566 [=======>......................] - ETA: 7:20 - loss: 0.6942 - acc: 0.5430
1344/4566 [=======>......................] - ETA: 7:05 - loss: 0.6917 - acc: 0.5499
1408/4566 [========>.....................] - ETA: 6:51 - loss: 0.6900 - acc: 0.5526
1472/4566 [========>.....................] - ETA: 6:38 - loss: 0.6907 - acc: 0.5482
1536/4566 [=========>....................] - ETA: 6:26 - loss: 0.6893 - acc: 0.5508
1600/4566 [=========>....................] - ETA: 6:15 - loss: 0.6889 - acc: 0.5525
1664/4566 [=========>....................] - ETA: 6:04 - loss: 0.6893 - acc: 0.5517
1728/4566 [==========>...................] - ETA: 5:53 - loss: 0.6903 - acc: 0.5492
1792/4566 [==========>...................] - ETA: 5:50 - loss: 0.6918 - acc: 0.5463
1856/4566 [===========>..................] - ETA: 5:50 - loss: 0.6907 - acc: 0.5501
1920/4566 [===========>..................] - ETA: 5:48 - loss: 0.6903 - acc: 0.5510
1984/4566 [============>.................] - ETA: 5:45 - loss: 0.6901 - acc: 0.5499
2048/4566 [============>.................] - ETA: 5:42 - loss: 0.6908 - acc: 0.5483
2112/4566 [============>.................] - ETA: 5:38 - loss: 0.6897 - acc: 0.5502
2176/4566 [=============>................] - ETA: 5:31 - loss: 0.6904 - acc: 0.5506
2240/4566 [=============>................] - ETA: 5:19 - loss: 0.6897 - acc: 0.5513
2304/4566 [==============>...............] - ETA: 5:08 - loss: 0.6896 - acc: 0.5530
2368/4566 [==============>...............] - ETA: 4:57 - loss: 0.6894 - acc: 0.5532
2432/4566 [==============>...............] - ETA: 4:45 - loss: 0.6891 - acc: 0.5522
2496/4566 [===============>..............] - ETA: 4:34 - loss: 0.6890 - acc: 0.5533
2560/4566 [===============>..............] - ETA: 4:24 - loss: 0.6895 - acc: 0.5516
2624/4566 [================>.............] - ETA: 4:13 - loss: 0.6891 - acc: 0.5526
2688/4566 [================>.............] - ETA: 4:03 - loss: 0.6888 - acc: 0.5536
2752/4566 [=================>............] - ETA: 3:53 - loss: 0.6886 - acc: 0.5545
2816/4566 [=================>............] - ETA: 3:43 - loss: 0.6889 - acc: 0.5543
2880/4566 [=================>............] - ETA: 3:33 - loss: 0.6891 - acc: 0.5542
2944/4566 [==================>...........] - ETA: 3:24 - loss: 0.6889 - acc: 0.5533
3008/4566 [==================>...........] - ETA: 3:14 - loss: 0.6877 - acc: 0.5562
3072/4566 [===================>..........] - ETA: 3:05 - loss: 0.6880 - acc: 0.5566
3136/4566 [===================>..........] - ETA: 2:56 - loss: 0.6885 - acc: 0.5552
3200/4566 [====================>.........] - ETA: 2:47 - loss: 0.6882 - acc: 0.5547
3264/4566 [====================>.........] - ETA: 2:39 - loss: 0.6878 - acc: 0.5551
3328/4566 [====================>.........] - ETA: 2:34 - loss: 0.6879 - acc: 0.5547
3392/4566 [=====================>........] - ETA: 2:27 - loss: 0.6886 - acc: 0.5528
3456/4566 [=====================>........] - ETA: 2:21 - loss: 0.6880 - acc: 0.5544
3520/4566 [======================>.......] - ETA: 2:14 - loss: 0.6888 - acc: 0.5520
3584/4566 [======================>.......] - ETA: 2:07 - loss: 0.6883 - acc: 0.5536
3648/4566 [======================>.......] - ETA: 2:00 - loss: 0.6879 - acc: 0.5554
3712/4566 [=======================>......] - ETA: 1:51 - loss: 0.6876 - acc: 0.5547
3776/4566 [=======================>......] - ETA: 1:42 - loss: 0.6879 - acc: 0.5535
3840/4566 [========================>.....] - ETA: 1:33 - loss: 0.6881 - acc: 0.5531
3904/4566 [========================>.....] - ETA: 1:25 - loss: 0.6880 - acc: 0.5530
3968/4566 [=========================>....] - ETA: 1:16 - loss: 0.6878 - acc: 0.5527
4032/4566 [=========================>....] - ETA: 1:07 - loss: 0.6880 - acc: 0.5526
4096/4566 [=========================>....] - ETA: 59s - loss: 0.6876 - acc: 0.5532 
4160/4566 [==========================>...] - ETA: 50s - loss: 0.6872 - acc: 0.5534
4224/4566 [==========================>...] - ETA: 42s - loss: 0.6874 - acc: 0.5528
4288/4566 [===========================>..] - ETA: 34s - loss: 0.6871 - acc: 0.5532
4352/4566 [===========================>..] - ETA: 26s - loss: 0.6866 - acc: 0.5542
4416/4566 [============================>.] - ETA: 18s - loss: 0.6865 - acc: 0.5534
4480/4566 [============================>.] - ETA: 10s - loss: 0.6871 - acc: 0.5520
4544/4566 [============================>.] - ETA: 2s - loss: 0.6868 - acc: 0.5533 
4566/4566 [==============================] - 571s 125ms/step - loss: 0.6868 - acc: 0.5532 - val_loss: 0.6812 - val_acc: 0.5650

Epoch 00004: val_acc improved from 0.55512 to 0.56496, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window03/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 5/10

  64/4566 [..............................] - ETA: 14:36 - loss: 0.7294 - acc: 0.4531
 128/4566 [..............................] - ETA: 15:16 - loss: 0.7099 - acc: 0.5078
 192/4566 [>.............................] - ETA: 14:52 - loss: 0.7109 - acc: 0.4948
 256/4566 [>.............................] - ETA: 14:41 - loss: 0.6955 - acc: 0.5273
 320/4566 [=>............................] - ETA: 14:29 - loss: 0.6876 - acc: 0.5375
 384/4566 [=>............................] - ETA: 14:16 - loss: 0.6931 - acc: 0.5260
 448/4566 [=>............................] - ETA: 13:34 - loss: 0.6913 - acc: 0.5223
 512/4566 [==>...........................] - ETA: 12:24 - loss: 0.6892 - acc: 0.5254
 576/4566 [==>...........................] - ETA: 11:28 - loss: 0.6856 - acc: 0.5399
 640/4566 [===>..........................] - ETA: 10:44 - loss: 0.6855 - acc: 0.5453
 704/4566 [===>..........................] - ETA: 10:04 - loss: 0.6838 - acc: 0.5497
 768/4566 [====>.........................] - ETA: 9:32 - loss: 0.6860 - acc: 0.5469 
 832/4566 [====>.........................] - ETA: 9:03 - loss: 0.6890 - acc: 0.5421
 896/4566 [====>.........................] - ETA: 8:38 - loss: 0.6910 - acc: 0.5413
 960/4566 [=====>........................] - ETA: 8:14 - loss: 0.6927 - acc: 0.5406
1024/4566 [=====>........................] - ETA: 7:55 - loss: 0.6925 - acc: 0.5361
1088/4566 [======>.......................] - ETA: 7:37 - loss: 0.6919 - acc: 0.5358
1152/4566 [======>.......................] - ETA: 7:19 - loss: 0.6918 - acc: 0.5373
1216/4566 [======>.......................] - ETA: 7:03 - loss: 0.6910 - acc: 0.5428
1280/4566 [=======>......................] - ETA: 6:47 - loss: 0.6909 - acc: 0.5406
1344/4566 [=======>......................] - ETA: 6:33 - loss: 0.6905 - acc: 0.5409
1408/4566 [========>.....................] - ETA: 6:19 - loss: 0.6893 - acc: 0.5405
1472/4566 [========>.....................] - ETA: 6:06 - loss: 0.6867 - acc: 0.5435
1536/4566 [=========>....................] - ETA: 6:01 - loss: 0.6856 - acc: 0.5456
1600/4566 [=========>....................] - ETA: 6:05 - loss: 0.6868 - acc: 0.5419
1664/4566 [=========>....................] - ETA: 6:06 - loss: 0.6896 - acc: 0.5355
1728/4566 [==========>...................] - ETA: 6:05 - loss: 0.6892 - acc: 0.5359
1792/4566 [==========>...................] - ETA: 6:04 - loss: 0.6889 - acc: 0.5363
1856/4566 [===========>..................] - ETA: 6:01 - loss: 0.6876 - acc: 0.5399
1920/4566 [===========>..................] - ETA: 5:57 - loss: 0.6879 - acc: 0.5380
1984/4566 [============>.................] - ETA: 5:45 - loss: 0.6861 - acc: 0.5418
2048/4566 [============>.................] - ETA: 5:33 - loss: 0.6858 - acc: 0.5405
2112/4566 [============>.................] - ETA: 5:20 - loss: 0.6859 - acc: 0.5412
2176/4566 [=============>................] - ETA: 5:09 - loss: 0.6859 - acc: 0.5423
2240/4566 [=============>................] - ETA: 4:57 - loss: 0.6859 - acc: 0.5424
2304/4566 [==============>...............] - ETA: 4:46 - loss: 0.6860 - acc: 0.5434
2368/4566 [==============>...............] - ETA: 4:35 - loss: 0.6852 - acc: 0.5456
2432/4566 [==============>...............] - ETA: 4:24 - loss: 0.6858 - acc: 0.5440
2496/4566 [===============>..............] - ETA: 4:14 - loss: 0.6865 - acc: 0.5441
2560/4566 [===============>..............] - ETA: 4:04 - loss: 0.6868 - acc: 0.5426
2624/4566 [================>.............] - ETA: 3:54 - loss: 0.6867 - acc: 0.5427
2688/4566 [================>.............] - ETA: 3:44 - loss: 0.6864 - acc: 0.5435
2752/4566 [=================>............] - ETA: 3:35 - loss: 0.6857 - acc: 0.5447
2816/4566 [=================>............] - ETA: 3:26 - loss: 0.6858 - acc: 0.5444
2880/4566 [=================>............] - ETA: 3:17 - loss: 0.6859 - acc: 0.5434
2944/4566 [==================>...........] - ETA: 3:08 - loss: 0.6856 - acc: 0.5435
3008/4566 [==================>...........] - ETA: 3:00 - loss: 0.6854 - acc: 0.5445
3072/4566 [===================>..........] - ETA: 2:53 - loss: 0.6856 - acc: 0.5449
3136/4566 [===================>..........] - ETA: 2:48 - loss: 0.6853 - acc: 0.5459
3200/4566 [====================>.........] - ETA: 2:43 - loss: 0.6849 - acc: 0.5469
3264/4566 [====================>.........] - ETA: 2:37 - loss: 0.6849 - acc: 0.5453
3328/4566 [====================>.........] - ETA: 2:31 - loss: 0.6852 - acc: 0.5448
3392/4566 [=====================>........] - ETA: 2:25 - loss: 0.6857 - acc: 0.5442
3456/4566 [=====================>........] - ETA: 2:19 - loss: 0.6855 - acc: 0.5448
3520/4566 [======================>.......] - ETA: 2:11 - loss: 0.6861 - acc: 0.5435
3584/4566 [======================>.......] - ETA: 2:02 - loss: 0.6862 - acc: 0.5438
3648/4566 [======================>.......] - ETA: 1:53 - loss: 0.6860 - acc: 0.5444
3712/4566 [=======================>......] - ETA: 1:45 - loss: 0.6858 - acc: 0.5461
3776/4566 [=======================>......] - ETA: 1:36 - loss: 0.6855 - acc: 0.5466
3840/4566 [========================>.....] - ETA: 1:28 - loss: 0.6848 - acc: 0.5490
3904/4566 [========================>.....] - ETA: 1:20 - loss: 0.6844 - acc: 0.5502
3968/4566 [=========================>....] - ETA: 1:11 - loss: 0.6841 - acc: 0.5504
4032/4566 [=========================>....] - ETA: 1:03 - loss: 0.6846 - acc: 0.5491
4096/4566 [=========================>....] - ETA: 55s - loss: 0.6848 - acc: 0.5488 
4160/4566 [==========================>...] - ETA: 48s - loss: 0.6843 - acc: 0.5498
4224/4566 [==========================>...] - ETA: 40s - loss: 0.6840 - acc: 0.5488
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6841 - acc: 0.5478
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6839 - acc: 0.5485
4416/4566 [============================>.] - ETA: 17s - loss: 0.6839 - acc: 0.5489
4480/4566 [============================>.] - ETA: 9s - loss: 0.6839 - acc: 0.5482 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6841 - acc: 0.5489
4566/4566 [==============================] - 564s 124ms/step - loss: 0.6838 - acc: 0.5495 - val_loss: 0.6771 - val_acc: 0.5571

Epoch 00005: val_acc did not improve from 0.56496
Epoch 6/10

  64/4566 [..............................] - ETA: 14:51 - loss: 0.6659 - acc: 0.7031
 128/4566 [..............................] - ETA: 14:52 - loss: 0.6696 - acc: 0.6094
 192/4566 [>.............................] - ETA: 14:32 - loss: 0.6640 - acc: 0.6094
 256/4566 [>.............................] - ETA: 13:41 - loss: 0.6757 - acc: 0.5820
 320/4566 [=>............................] - ETA: 12:10 - loss: 0.6785 - acc: 0.5781
 384/4566 [=>............................] - ETA: 10:54 - loss: 0.6772 - acc: 0.5781
 448/4566 [=>............................] - ETA: 10:00 - loss: 0.6777 - acc: 0.5737
 512/4566 [==>...........................] - ETA: 9:17 - loss: 0.6838 - acc: 0.5645 
 576/4566 [==>...........................] - ETA: 8:46 - loss: 0.6854 - acc: 0.5590
 640/4566 [===>..........................] - ETA: 8:19 - loss: 0.6842 - acc: 0.5594
 704/4566 [===>..........................] - ETA: 7:56 - loss: 0.6874 - acc: 0.5540
 768/4566 [====>.........................] - ETA: 7:35 - loss: 0.6885 - acc: 0.5443
 832/4566 [====>.........................] - ETA: 7:18 - loss: 0.6895 - acc: 0.5457
 896/4566 [====>.........................] - ETA: 7:01 - loss: 0.6892 - acc: 0.5480
 960/4566 [=====>........................] - ETA: 6:46 - loss: 0.6883 - acc: 0.5479
1024/4566 [=====>........................] - ETA: 6:31 - loss: 0.6875 - acc: 0.5459
1088/4566 [======>.......................] - ETA: 6:19 - loss: 0.6860 - acc: 0.5515
1152/4566 [======>.......................] - ETA: 6:07 - loss: 0.6835 - acc: 0.5564
1216/4566 [======>.......................] - ETA: 5:55 - loss: 0.6836 - acc: 0.5567
1280/4566 [=======>......................] - ETA: 5:44 - loss: 0.6844 - acc: 0.5547
1344/4566 [=======>......................] - ETA: 5:41 - loss: 0.6861 - acc: 0.5476
1408/4566 [========>.....................] - ETA: 5:49 - loss: 0.6858 - acc: 0.5469
1472/4566 [========>.....................] - ETA: 5:54 - loss: 0.6865 - acc: 0.5421
1536/4566 [=========>....................] - ETA: 5:57 - loss: 0.6852 - acc: 0.5449
1600/4566 [=========>....................] - ETA: 5:59 - loss: 0.6853 - acc: 0.5456
1664/4566 [=========>....................] - ETA: 6:00 - loss: 0.6856 - acc: 0.5457
1728/4566 [==========>...................] - ETA: 5:59 - loss: 0.6859 - acc: 0.5475
1792/4566 [==========>...................] - ETA: 5:49 - loss: 0.6847 - acc: 0.5485
1856/4566 [===========>..................] - ETA: 5:36 - loss: 0.6856 - acc: 0.5453
1920/4566 [===========>..................] - ETA: 5:24 - loss: 0.6846 - acc: 0.5464
1984/4566 [============>.................] - ETA: 5:13 - loss: 0.6847 - acc: 0.5464
2048/4566 [============>.................] - ETA: 5:01 - loss: 0.6849 - acc: 0.5464
2112/4566 [============>.................] - ETA: 4:51 - loss: 0.6862 - acc: 0.5436
2176/4566 [=============>................] - ETA: 4:41 - loss: 0.6863 - acc: 0.5446
2240/4566 [=============>................] - ETA: 4:31 - loss: 0.6871 - acc: 0.5433
2304/4566 [==============>...............] - ETA: 4:22 - loss: 0.6879 - acc: 0.5408
2368/4566 [==============>...............] - ETA: 4:12 - loss: 0.6878 - acc: 0.5401
2432/4566 [==============>...............] - ETA: 4:02 - loss: 0.6874 - acc: 0.5424
2496/4566 [===============>..............] - ETA: 3:54 - loss: 0.6874 - acc: 0.5429
2560/4566 [===============>..............] - ETA: 3:45 - loss: 0.6875 - acc: 0.5418
2624/4566 [================>.............] - ETA: 3:37 - loss: 0.6866 - acc: 0.5442
2688/4566 [================>.............] - ETA: 3:29 - loss: 0.6861 - acc: 0.5458
2752/4566 [=================>............] - ETA: 3:21 - loss: 0.6868 - acc: 0.5432
2816/4566 [=================>............] - ETA: 3:14 - loss: 0.6871 - acc: 0.5430
2880/4566 [=================>............] - ETA: 3:11 - loss: 0.6870 - acc: 0.5441
2944/4566 [==================>...........] - ETA: 3:07 - loss: 0.6869 - acc: 0.5452
3008/4566 [==================>...........] - ETA: 3:03 - loss: 0.6856 - acc: 0.5499
3072/4566 [===================>..........] - ETA: 2:58 - loss: 0.6853 - acc: 0.5508
3136/4566 [===================>..........] - ETA: 2:53 - loss: 0.6849 - acc: 0.5513
3200/4566 [====================>.........] - ETA: 2:47 - loss: 0.6852 - acc: 0.5509
3264/4566 [====================>.........] - ETA: 2:38 - loss: 0.6838 - acc: 0.5542
3328/4566 [====================>.........] - ETA: 2:29 - loss: 0.6841 - acc: 0.5550
3392/4566 [=====================>........] - ETA: 2:21 - loss: 0.6840 - acc: 0.5551
3456/4566 [=====================>........] - ETA: 2:12 - loss: 0.6833 - acc: 0.5576
3520/4566 [======================>.......] - ETA: 2:04 - loss: 0.6835 - acc: 0.5582
3584/4566 [======================>.......] - ETA: 1:56 - loss: 0.6837 - acc: 0.5597
3648/4566 [======================>.......] - ETA: 1:48 - loss: 0.6839 - acc: 0.5595
3712/4566 [=======================>......] - ETA: 1:39 - loss: 0.6837 - acc: 0.5598
3776/4566 [=======================>......] - ETA: 1:32 - loss: 0.6829 - acc: 0.5622
3840/4566 [========================>.....] - ETA: 1:24 - loss: 0.6830 - acc: 0.5617
3904/4566 [========================>.....] - ETA: 1:16 - loss: 0.6830 - acc: 0.5628
3968/4566 [=========================>....] - ETA: 1:08 - loss: 0.6826 - acc: 0.5640
4032/4566 [=========================>....] - ETA: 1:00 - loss: 0.6830 - acc: 0.5637
4096/4566 [=========================>....] - ETA: 53s - loss: 0.6828 - acc: 0.5645 
4160/4566 [==========================>...] - ETA: 45s - loss: 0.6826 - acc: 0.5642
4224/4566 [==========================>...] - ETA: 38s - loss: 0.6826 - acc: 0.5639
4288/4566 [===========================>..] - ETA: 31s - loss: 0.6835 - acc: 0.5625
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6838 - acc: 0.5620
4416/4566 [============================>.] - ETA: 17s - loss: 0.6839 - acc: 0.5614
4480/4566 [============================>.] - ETA: 9s - loss: 0.6846 - acc: 0.5607 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6843 - acc: 0.5618
4566/4566 [==============================] - 571s 125ms/step - loss: 0.6842 - acc: 0.5620 - val_loss: 0.6832 - val_acc: 0.5571

Epoch 00006: val_acc did not improve from 0.56496
Epoch 7/10

  64/4566 [..............................] - ETA: 6:40 - loss: 0.6768 - acc: 0.5625
 128/4566 [..............................] - ETA: 6:23 - loss: 0.6722 - acc: 0.5703
 192/4566 [>.............................] - ETA: 6:02 - loss: 0.6855 - acc: 0.5312
 256/4566 [>.............................] - ETA: 5:55 - loss: 0.6886 - acc: 0.5234
 320/4566 [=>............................] - ETA: 5:56 - loss: 0.6841 - acc: 0.5312
 384/4566 [=>............................] - ETA: 5:55 - loss: 0.6856 - acc: 0.5286
 448/4566 [=>............................] - ETA: 5:51 - loss: 0.6853 - acc: 0.5268
 512/4566 [==>...........................] - ETA: 5:43 - loss: 0.6851 - acc: 0.5254
 576/4566 [==>...........................] - ETA: 5:33 - loss: 0.6831 - acc: 0.5295
 640/4566 [===>..........................] - ETA: 5:24 - loss: 0.6819 - acc: 0.5297
 704/4566 [===>..........................] - ETA: 5:20 - loss: 0.6820 - acc: 0.5284
 768/4566 [====>.........................] - ETA: 5:15 - loss: 0.6813 - acc: 0.5352
 832/4566 [====>.........................] - ETA: 5:10 - loss: 0.6794 - acc: 0.5457
 896/4566 [====>.........................] - ETA: 5:05 - loss: 0.6787 - acc: 0.5480
 960/4566 [=====>........................] - ETA: 5:00 - loss: 0.6763 - acc: 0.5573
1024/4566 [=====>........................] - ETA: 4:55 - loss: 0.6767 - acc: 0.5576
1088/4566 [======>.......................] - ETA: 5:08 - loss: 0.6775 - acc: 0.5588
1152/4566 [======>.......................] - ETA: 5:25 - loss: 0.6792 - acc: 0.5538
1216/4566 [======>.......................] - ETA: 5:39 - loss: 0.6762 - acc: 0.5617
1280/4566 [=======>......................] - ETA: 5:49 - loss: 0.6799 - acc: 0.5578
1344/4566 [=======>......................] - ETA: 5:56 - loss: 0.6797 - acc: 0.5595
1408/4566 [========>.....................] - ETA: 6:02 - loss: 0.6785 - acc: 0.5589
1472/4566 [========>.....................] - ETA: 6:02 - loss: 0.6796 - acc: 0.5577
1536/4566 [=========>....................] - ETA: 5:51 - loss: 0.6798 - acc: 0.5612
1600/4566 [=========>....................] - ETA: 5:39 - loss: 0.6782 - acc: 0.5625
1664/4566 [=========>....................] - ETA: 5:27 - loss: 0.6798 - acc: 0.5631
1728/4566 [==========>...................] - ETA: 5:17 - loss: 0.6790 - acc: 0.5648
1792/4566 [==========>...................] - ETA: 5:06 - loss: 0.6800 - acc: 0.5642
1856/4566 [===========>..................] - ETA: 4:56 - loss: 0.6822 - acc: 0.5598
1920/4566 [===========>..................] - ETA: 4:46 - loss: 0.6815 - acc: 0.5630
1984/4566 [============>.................] - ETA: 4:37 - loss: 0.6813 - acc: 0.5630
2048/4566 [============>.................] - ETA: 4:28 - loss: 0.6815 - acc: 0.5635
2112/4566 [============>.................] - ETA: 4:19 - loss: 0.6814 - acc: 0.5649
2176/4566 [=============>................] - ETA: 4:10 - loss: 0.6830 - acc: 0.5616
2240/4566 [=============>................] - ETA: 4:02 - loss: 0.6818 - acc: 0.5625
2304/4566 [==============>...............] - ETA: 3:54 - loss: 0.6831 - acc: 0.5603
2368/4566 [==============>...............] - ETA: 3:46 - loss: 0.6828 - acc: 0.5625
2432/4566 [==============>...............] - ETA: 3:38 - loss: 0.6832 - acc: 0.5596
2496/4566 [===============>..............] - ETA: 3:30 - loss: 0.6837 - acc: 0.5597
2560/4566 [===============>..............] - ETA: 3:26 - loss: 0.6835 - acc: 0.5598
2624/4566 [================>.............] - ETA: 3:24 - loss: 0.6848 - acc: 0.5572
2688/4566 [================>.............] - ETA: 3:22 - loss: 0.6844 - acc: 0.5569
2752/4566 [=================>............] - ETA: 3:19 - loss: 0.6851 - acc: 0.5560
2816/4566 [=================>............] - ETA: 3:16 - loss: 0.6849 - acc: 0.5565
2880/4566 [=================>............] - ETA: 3:12 - loss: 0.6844 - acc: 0.5573
2944/4566 [==================>...........] - ETA: 3:07 - loss: 0.6846 - acc: 0.5574
3008/4566 [==================>...........] - ETA: 2:59 - loss: 0.6848 - acc: 0.5572
3072/4566 [===================>..........] - ETA: 2:51 - loss: 0.6854 - acc: 0.5547
3136/4566 [===================>..........] - ETA: 2:43 - loss: 0.6848 - acc: 0.5568
3200/4566 [====================>.........] - ETA: 2:34 - loss: 0.6840 - acc: 0.5587
3264/4566 [====================>.........] - ETA: 2:26 - loss: 0.6835 - acc: 0.5585
3328/4566 [====================>.........] - ETA: 2:18 - loss: 0.6832 - acc: 0.5589
3392/4566 [=====================>........] - ETA: 2:10 - loss: 0.6828 - acc: 0.5593
3456/4566 [=====================>........] - ETA: 2:03 - loss: 0.6820 - acc: 0.5616
3520/4566 [======================>.......] - ETA: 1:55 - loss: 0.6814 - acc: 0.5634
3584/4566 [======================>.......] - ETA: 1:47 - loss: 0.6819 - acc: 0.5625
3648/4566 [======================>.......] - ETA: 1:40 - loss: 0.6816 - acc: 0.5630
3712/4566 [=======================>......] - ETA: 1:32 - loss: 0.6816 - acc: 0.5636
3776/4566 [=======================>......] - ETA: 1:25 - loss: 0.6816 - acc: 0.5638
3840/4566 [========================>.....] - ETA: 1:18 - loss: 0.6820 - acc: 0.5625
3904/4566 [========================>.....] - ETA: 1:10 - loss: 0.6824 - acc: 0.5617
3968/4566 [=========================>....] - ETA: 1:03 - loss: 0.6824 - acc: 0.5607
4032/4566 [=========================>....] - ETA: 56s - loss: 0.6817 - acc: 0.5618 
4096/4566 [=========================>....] - ETA: 50s - loss: 0.6814 - acc: 0.5630
4160/4566 [==========================>...] - ETA: 43s - loss: 0.6810 - acc: 0.5630
4224/4566 [==========================>...] - ETA: 37s - loss: 0.6821 - acc: 0.5611
4288/4566 [===========================>..] - ETA: 30s - loss: 0.6820 - acc: 0.5606
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6821 - acc: 0.5602
4416/4566 [============================>.] - ETA: 17s - loss: 0.6820 - acc: 0.5609
4480/4566 [============================>.] - ETA: 9s - loss: 0.6822 - acc: 0.5605 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6820 - acc: 0.5621
4566/4566 [==============================] - 538s 118ms/step - loss: 0.6821 - acc: 0.5622 - val_loss: 0.6806 - val_acc: 0.5689

Epoch 00007: val_acc improved from 0.56496 to 0.56890, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window03/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 8/10

  64/4566 [..............................] - ETA: 5:39 - loss: 0.6907 - acc: 0.5469
 128/4566 [..............................] - ETA: 5:39 - loss: 0.6676 - acc: 0.6094
 192/4566 [>.............................] - ETA: 5:51 - loss: 0.6696 - acc: 0.5938
 256/4566 [>.............................] - ETA: 5:43 - loss: 0.6746 - acc: 0.5742
 320/4566 [=>............................] - ETA: 5:32 - loss: 0.6714 - acc: 0.5813
 384/4566 [=>............................] - ETA: 5:25 - loss: 0.6751 - acc: 0.5885
 448/4566 [=>............................] - ETA: 5:21 - loss: 0.6808 - acc: 0.5692
 512/4566 [==>...........................] - ETA: 5:18 - loss: 0.6784 - acc: 0.5781
 576/4566 [==>...........................] - ETA: 5:15 - loss: 0.6829 - acc: 0.5625
 640/4566 [===>..........................] - ETA: 5:14 - loss: 0.6809 - acc: 0.5641
 704/4566 [===>..........................] - ETA: 5:12 - loss: 0.6815 - acc: 0.5639
 768/4566 [====>.........................] - ETA: 5:08 - loss: 0.6801 - acc: 0.5612
 832/4566 [====>.........................] - ETA: 5:02 - loss: 0.6799 - acc: 0.5625
 896/4566 [====>.........................] - ETA: 5:28 - loss: 0.6797 - acc: 0.5592
 960/4566 [=====>........................] - ETA: 5:52 - loss: 0.6820 - acc: 0.5542
1024/4566 [=====>........................] - ETA: 6:08 - loss: 0.6817 - acc: 0.5576
1088/4566 [======>.......................] - ETA: 6:25 - loss: 0.6849 - acc: 0.5469
1152/4566 [======>.......................] - ETA: 6:37 - loss: 0.6837 - acc: 0.5512
1216/4566 [======>.......................] - ETA: 6:45 - loss: 0.6819 - acc: 0.5576
1280/4566 [=======>......................] - ETA: 6:38 - loss: 0.6821 - acc: 0.5539
1344/4566 [=======>......................] - ETA: 6:25 - loss: 0.6818 - acc: 0.5580
1408/4566 [========>.....................] - ETA: 6:12 - loss: 0.6807 - acc: 0.5611
1472/4566 [========>.....................] - ETA: 6:00 - loss: 0.6801 - acc: 0.5591
1536/4566 [=========>....................] - ETA: 5:49 - loss: 0.6806 - acc: 0.5547
1600/4566 [=========>....................] - ETA: 5:37 - loss: 0.6815 - acc: 0.5506
1664/4566 [=========>....................] - ETA: 5:26 - loss: 0.6801 - acc: 0.5565
1728/4566 [==========>...................] - ETA: 5:16 - loss: 0.6799 - acc: 0.5567
1792/4566 [==========>...................] - ETA: 5:06 - loss: 0.6810 - acc: 0.5564
1856/4566 [===========>..................] - ETA: 4:56 - loss: 0.6818 - acc: 0.5566
1920/4566 [===========>..................] - ETA: 4:47 - loss: 0.6819 - acc: 0.5589
1984/4566 [============>.................] - ETA: 4:37 - loss: 0.6803 - acc: 0.5615
2048/4566 [============>.................] - ETA: 4:29 - loss: 0.6805 - acc: 0.5620
2112/4566 [============>.................] - ETA: 4:20 - loss: 0.6809 - acc: 0.5620
2176/4566 [=============>................] - ETA: 4:12 - loss: 0.6825 - acc: 0.5593
2240/4566 [=============>................] - ETA: 4:04 - loss: 0.6819 - acc: 0.5607
2304/4566 [==============>...............] - ETA: 3:56 - loss: 0.6833 - acc: 0.5595
2368/4566 [==============>...............] - ETA: 3:55 - loss: 0.6817 - acc: 0.5642
2432/4566 [==============>...............] - ETA: 3:54 - loss: 0.6821 - acc: 0.5625
2496/4566 [===============>..............] - ETA: 3:52 - loss: 0.6820 - acc: 0.5621
2560/4566 [===============>..............] - ETA: 3:50 - loss: 0.6820 - acc: 0.5609
2624/4566 [================>.............] - ETA: 3:47 - loss: 0.6825 - acc: 0.5598
2688/4566 [================>.............] - ETA: 3:43 - loss: 0.6818 - acc: 0.5606
2752/4566 [=================>............] - ETA: 3:36 - loss: 0.6815 - acc: 0.5614
2816/4566 [=================>............] - ETA: 3:27 - loss: 0.6816 - acc: 0.5621
2880/4566 [=================>............] - ETA: 3:18 - loss: 0.6810 - acc: 0.5622
2944/4566 [==================>...........] - ETA: 3:09 - loss: 0.6810 - acc: 0.5628
3008/4566 [==================>...........] - ETA: 3:00 - loss: 0.6812 - acc: 0.5622
3072/4566 [===================>..........] - ETA: 2:52 - loss: 0.6814 - acc: 0.5612
3136/4566 [===================>..........] - ETA: 2:43 - loss: 0.6819 - acc: 0.5609
3200/4566 [====================>.........] - ETA: 2:35 - loss: 0.6819 - acc: 0.5603
3264/4566 [====================>.........] - ETA: 2:27 - loss: 0.6815 - acc: 0.5616
3328/4566 [====================>.........] - ETA: 2:19 - loss: 0.6809 - acc: 0.5634
3392/4566 [=====================>........] - ETA: 2:11 - loss: 0.6809 - acc: 0.5634
3456/4566 [=====================>........] - ETA: 2:03 - loss: 0.6807 - acc: 0.5637
3520/4566 [======================>.......] - ETA: 1:55 - loss: 0.6804 - acc: 0.5651
3584/4566 [======================>.......] - ETA: 1:48 - loss: 0.6806 - acc: 0.5650
3648/4566 [======================>.......] - ETA: 1:40 - loss: 0.6806 - acc: 0.5644
3712/4566 [=======================>......] - ETA: 1:33 - loss: 0.6807 - acc: 0.5655
3776/4566 [=======================>......] - ETA: 1:26 - loss: 0.6813 - acc: 0.5638
3840/4566 [========================>.....] - ETA: 1:19 - loss: 0.6814 - acc: 0.5635
3904/4566 [========================>.....] - ETA: 1:13 - loss: 0.6809 - acc: 0.5653
3968/4566 [=========================>....] - ETA: 1:07 - loss: 0.6808 - acc: 0.5653
4032/4566 [=========================>....] - ETA: 1:01 - loss: 0.6803 - acc: 0.5670
4096/4566 [=========================>....] - ETA: 54s - loss: 0.6798 - acc: 0.5679 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6805 - acc: 0.5661
4224/4566 [==========================>...] - ETA: 40s - loss: 0.6804 - acc: 0.5658
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6804 - acc: 0.5662
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6804 - acc: 0.5666
4416/4566 [============================>.] - ETA: 17s - loss: 0.6806 - acc: 0.5661
4480/4566 [============================>.] - ETA: 9s - loss: 0.6805 - acc: 0.5654 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6803 - acc: 0.5656
4566/4566 [==============================] - 543s 119ms/step - loss: 0.6804 - acc: 0.5655 - val_loss: 0.6834 - val_acc: 0.5512

Epoch 00008: val_acc did not improve from 0.56890
Epoch 9/10

  64/4566 [..............................] - ETA: 6:12 - loss: 0.6562 - acc: 0.5938
 128/4566 [..............................] - ETA: 6:06 - loss: 0.6879 - acc: 0.5156
 192/4566 [>.............................] - ETA: 6:03 - loss: 0.6668 - acc: 0.5625
 256/4566 [>.............................] - ETA: 5:59 - loss: 0.6787 - acc: 0.5430
 320/4566 [=>............................] - ETA: 5:50 - loss: 0.6672 - acc: 0.5687
 384/4566 [=>............................] - ETA: 5:46 - loss: 0.6719 - acc: 0.5677
 448/4566 [=>............................] - ETA: 5:42 - loss: 0.6744 - acc: 0.5692
 512/4566 [==>...........................] - ETA: 5:38 - loss: 0.6725 - acc: 0.5723
 576/4566 [==>...........................] - ETA: 5:51 - loss: 0.6721 - acc: 0.5781
 640/4566 [===>..........................] - ETA: 6:35 - loss: 0.6784 - acc: 0.5719
 704/4566 [===>..........................] - ETA: 7:04 - loss: 0.6834 - acc: 0.5639
 768/4566 [====>.........................] - ETA: 7:27 - loss: 0.6851 - acc: 0.5638
 832/4566 [====>.........................] - ETA: 7:46 - loss: 0.6841 - acc: 0.5649
 896/4566 [====>.........................] - ETA: 7:58 - loss: 0.6829 - acc: 0.5703
 960/4566 [=====>........................] - ETA: 8:05 - loss: 0.6844 - acc: 0.5698
1024/4566 [=====>........................] - ETA: 7:49 - loss: 0.6846 - acc: 0.5703
1088/4566 [======>.......................] - ETA: 7:29 - loss: 0.6827 - acc: 0.5754
1152/4566 [======>.......................] - ETA: 7:11 - loss: 0.6826 - acc: 0.5764
1216/4566 [======>.......................] - ETA: 6:55 - loss: 0.6825 - acc: 0.5765
1280/4566 [=======>......................] - ETA: 6:40 - loss: 0.6819 - acc: 0.5789
1344/4566 [=======>......................] - ETA: 6:27 - loss: 0.6813 - acc: 0.5796
1408/4566 [========>.....................] - ETA: 6:14 - loss: 0.6802 - acc: 0.5831
1472/4566 [========>.....................] - ETA: 6:02 - loss: 0.6806 - acc: 0.5829
1536/4566 [=========>....................] - ETA: 5:50 - loss: 0.6801 - acc: 0.5807
1600/4566 [=========>....................] - ETA: 5:39 - loss: 0.6797 - acc: 0.5813
1664/4566 [=========>....................] - ETA: 5:28 - loss: 0.6791 - acc: 0.5811
1728/4566 [==========>...................] - ETA: 5:18 - loss: 0.6796 - acc: 0.5822
1792/4566 [==========>...................] - ETA: 5:07 - loss: 0.6792 - acc: 0.5831
1856/4566 [===========>..................] - ETA: 4:57 - loss: 0.6789 - acc: 0.5835
1920/4566 [===========>..................] - ETA: 4:47 - loss: 0.6789 - acc: 0.5823
1984/4566 [============>.................] - ETA: 4:38 - loss: 0.6796 - acc: 0.5811
2048/4566 [============>.................] - ETA: 4:29 - loss: 0.6803 - acc: 0.5806
2112/4566 [============>.................] - ETA: 4:28 - loss: 0.6798 - acc: 0.5838
2176/4566 [=============>................] - ETA: 4:28 - loss: 0.6793 - acc: 0.5846
2240/4566 [=============>................] - ETA: 4:27 - loss: 0.6785 - acc: 0.5862
2304/4566 [==============>...............] - ETA: 4:26 - loss: 0.6782 - acc: 0.5868
2368/4566 [==============>...............] - ETA: 4:24 - loss: 0.6790 - acc: 0.5861
2432/4566 [==============>...............] - ETA: 4:21 - loss: 0.6783 - acc: 0.5880
2496/4566 [===============>..............] - ETA: 4:14 - loss: 0.6784 - acc: 0.5865
2560/4566 [===============>..............] - ETA: 4:05 - loss: 0.6773 - acc: 0.5867
2624/4566 [================>.............] - ETA: 3:55 - loss: 0.6777 - acc: 0.5857
2688/4566 [================>.............] - ETA: 3:46 - loss: 0.6777 - acc: 0.5859
2752/4566 [=================>............] - ETA: 3:36 - loss: 0.6780 - acc: 0.5847
2816/4566 [=================>............] - ETA: 3:27 - loss: 0.6775 - acc: 0.5856
2880/4566 [=================>............] - ETA: 3:19 - loss: 0.6788 - acc: 0.5823
2944/4566 [==================>...........] - ETA: 3:10 - loss: 0.6795 - acc: 0.5812
3008/4566 [==================>...........] - ETA: 3:01 - loss: 0.6794 - acc: 0.5814
3072/4566 [===================>..........] - ETA: 2:53 - loss: 0.6797 - acc: 0.5807
3136/4566 [===================>..........] - ETA: 2:44 - loss: 0.6794 - acc: 0.5807
3200/4566 [====================>.........] - ETA: 2:36 - loss: 0.6787 - acc: 0.5822
3264/4566 [====================>.........] - ETA: 2:28 - loss: 0.6781 - acc: 0.5830
3328/4566 [====================>.........] - ETA: 2:20 - loss: 0.6780 - acc: 0.5829
3392/4566 [=====================>........] - ETA: 2:12 - loss: 0.6779 - acc: 0.5831
3456/4566 [=====================>........] - ETA: 2:04 - loss: 0.6774 - acc: 0.5839
3520/4566 [======================>.......] - ETA: 1:56 - loss: 0.6780 - acc: 0.5832
3584/4566 [======================>.......] - ETA: 1:51 - loss: 0.6771 - acc: 0.5843
3648/4566 [======================>.......] - ETA: 1:45 - loss: 0.6769 - acc: 0.5844
3712/4566 [=======================>......] - ETA: 1:39 - loss: 0.6769 - acc: 0.5846
3776/4566 [=======================>......] - ETA: 1:32 - loss: 0.6775 - acc: 0.5829
3840/4566 [========================>.....] - ETA: 1:26 - loss: 0.6774 - acc: 0.5833
3904/4566 [========================>.....] - ETA: 1:19 - loss: 0.6772 - acc: 0.5832
3968/4566 [=========================>....] - ETA: 1:12 - loss: 0.6772 - acc: 0.5832
4032/4566 [=========================>....] - ETA: 1:04 - loss: 0.6772 - acc: 0.5823
4096/4566 [=========================>....] - ETA: 56s - loss: 0.6777 - acc: 0.5825 
4160/4566 [==========================>...] - ETA: 48s - loss: 0.6774 - acc: 0.5834
4224/4566 [==========================>...] - ETA: 40s - loss: 0.6784 - acc: 0.5817
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6781 - acc: 0.5809
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6777 - acc: 0.5809
4416/4566 [============================>.] - ETA: 17s - loss: 0.6767 - acc: 0.5822
4480/4566 [============================>.] - ETA: 10s - loss: 0.6773 - acc: 0.5810
4544/4566 [============================>.] - ETA: 2s - loss: 0.6773 - acc: 0.5805 
4566/4566 [==============================] - 544s 119ms/step - loss: 0.6772 - acc: 0.5806 - val_loss: 0.6659 - val_acc: 0.5965

Epoch 00009: val_acc improved from 0.56890 to 0.59646, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window03/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 10/10

  64/4566 [..............................] - ETA: 5:42 - loss: 0.7066 - acc: 0.5156
 128/4566 [..............................] - ETA: 5:40 - loss: 0.6915 - acc: 0.5781
 192/4566 [>.............................] - ETA: 5:31 - loss: 0.6836 - acc: 0.5833
 256/4566 [>.............................] - ETA: 5:31 - loss: 0.6795 - acc: 0.5898
 320/4566 [=>............................] - ETA: 6:44 - loss: 0.6758 - acc: 0.5969
 384/4566 [=>............................] - ETA: 8:03 - loss: 0.6772 - acc: 0.5911
 448/4566 [=>............................] - ETA: 8:48 - loss: 0.6730 - acc: 0.6004
 512/4566 [==>...........................] - ETA: 9:17 - loss: 0.6730 - acc: 0.5977
 576/4566 [==>...........................] - ETA: 9:38 - loss: 0.6673 - acc: 0.6146
 640/4566 [===>..........................] - ETA: 9:53 - loss: 0.6660 - acc: 0.6156
 704/4566 [===>..........................] - ETA: 9:53 - loss: 0.6685 - acc: 0.6108
 768/4566 [====>.........................] - ETA: 9:26 - loss: 0.6692 - acc: 0.6068
 832/4566 [====>.........................] - ETA: 8:59 - loss: 0.6709 - acc: 0.6022
 896/4566 [====>.........................] - ETA: 8:33 - loss: 0.6692 - acc: 0.5982
 960/4566 [=====>........................] - ETA: 8:11 - loss: 0.6743 - acc: 0.5896
1024/4566 [=====>........................] - ETA: 7:52 - loss: 0.6732 - acc: 0.5938
1088/4566 [======>.......................] - ETA: 7:33 - loss: 0.6702 - acc: 0.5956
1152/4566 [======>.......................] - ETA: 7:16 - loss: 0.6702 - acc: 0.5972
1216/4566 [======>.......................] - ETA: 7:00 - loss: 0.6690 - acc: 0.5987
1280/4566 [=======>......................] - ETA: 6:45 - loss: 0.6697 - acc: 0.5992
1344/4566 [=======>......................] - ETA: 6:31 - loss: 0.6724 - acc: 0.5938
1408/4566 [========>.....................] - ETA: 6:18 - loss: 0.6733 - acc: 0.5909
1472/4566 [========>.....................] - ETA: 6:06 - loss: 0.6747 - acc: 0.5870
1536/4566 [=========>....................] - ETA: 5:54 - loss: 0.6762 - acc: 0.5833
1600/4566 [=========>....................] - ETA: 5:42 - loss: 0.6756 - acc: 0.5825
1664/4566 [=========>....................] - ETA: 5:31 - loss: 0.6741 - acc: 0.5859
1728/4566 [==========>...................] - ETA: 5:20 - loss: 0.6741 - acc: 0.5874
1792/4566 [==========>...................] - ETA: 5:15 - loss: 0.6724 - acc: 0.5898
1856/4566 [===========>..................] - ETA: 5:16 - loss: 0.6722 - acc: 0.5894
1920/4566 [===========>..................] - ETA: 5:15 - loss: 0.6723 - acc: 0.5880
1984/4566 [============>.................] - ETA: 5:15 - loss: 0.6736 - acc: 0.5867
2048/4566 [============>.................] - ETA: 5:13 - loss: 0.6740 - acc: 0.5850
2112/4566 [============>.................] - ETA: 5:10 - loss: 0.6731 - acc: 0.5866
2176/4566 [=============>................] - ETA: 5:07 - loss: 0.6733 - acc: 0.5873
2240/4566 [=============>................] - ETA: 4:57 - loss: 0.6731 - acc: 0.5875
2304/4566 [==============>...............] - ETA: 4:46 - loss: 0.6722 - acc: 0.5894
2368/4566 [==============>...............] - ETA: 4:36 - loss: 0.6728 - acc: 0.5861
2432/4566 [==============>...............] - ETA: 4:25 - loss: 0.6719 - acc: 0.5876
2496/4566 [===============>..............] - ETA: 4:15 - loss: 0.6710 - acc: 0.5885
2560/4566 [===============>..............] - ETA: 4:05 - loss: 0.6712 - acc: 0.5879
2624/4566 [================>.............] - ETA: 3:55 - loss: 0.6705 - acc: 0.5869
2688/4566 [================>.............] - ETA: 3:45 - loss: 0.6707 - acc: 0.5863
2752/4566 [=================>............] - ETA: 3:36 - loss: 0.6711 - acc: 0.5832
2816/4566 [=================>............] - ETA: 3:27 - loss: 0.6718 - acc: 0.5817
2880/4566 [=================>............] - ETA: 3:18 - loss: 0.6727 - acc: 0.5788
2944/4566 [==================>...........] - ETA: 3:09 - loss: 0.6739 - acc: 0.5764
3008/4566 [==================>...........] - ETA: 3:01 - loss: 0.6746 - acc: 0.5745
3072/4566 [===================>..........] - ETA: 2:52 - loss: 0.6747 - acc: 0.5736
3136/4566 [===================>..........] - ETA: 2:43 - loss: 0.6750 - acc: 0.5737
3200/4566 [====================>.........] - ETA: 2:35 - loss: 0.6747 - acc: 0.5744
3264/4566 [====================>.........] - ETA: 2:27 - loss: 0.6751 - acc: 0.5738
3328/4566 [====================>.........] - ETA: 2:22 - loss: 0.6756 - acc: 0.5730
3392/4566 [=====================>........] - ETA: 2:17 - loss: 0.6760 - acc: 0.5716
3456/4566 [=====================>........] - ETA: 2:11 - loss: 0.6759 - acc: 0.5729
3520/4566 [======================>.......] - ETA: 2:05 - loss: 0.6760 - acc: 0.5733
3584/4566 [======================>.......] - ETA: 1:59 - loss: 0.6761 - acc: 0.5725
3648/4566 [======================>.......] - ETA: 1:52 - loss: 0.6765 - acc: 0.5715
3712/4566 [=======================>......] - ETA: 1:44 - loss: 0.6775 - acc: 0.5714
3776/4566 [=======================>......] - ETA: 1:36 - loss: 0.6773 - acc: 0.5718
3840/4566 [========================>.....] - ETA: 1:28 - loss: 0.6774 - acc: 0.5714
3904/4566 [========================>.....] - ETA: 1:19 - loss: 0.6781 - acc: 0.5699
3968/4566 [=========================>....] - ETA: 1:11 - loss: 0.6779 - acc: 0.5698
4032/4566 [=========================>....] - ETA: 1:03 - loss: 0.6781 - acc: 0.5697
4096/4566 [=========================>....] - ETA: 55s - loss: 0.6778 - acc: 0.5718 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6778 - acc: 0.5709
4224/4566 [==========================>...] - ETA: 40s - loss: 0.6777 - acc: 0.5722
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6774 - acc: 0.5732
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6776 - acc: 0.5731
4416/4566 [============================>.] - ETA: 17s - loss: 0.6773 - acc: 0.5731
4480/4566 [============================>.] - ETA: 9s - loss: 0.6771 - acc: 0.5737 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6773 - acc: 0.5731
4566/4566 [==============================] - 535s 117ms/step - loss: 0.6774 - acc: 0.5731 - val_loss: 0.6694 - val_acc: 0.5945

Epoch 00010: val_acc did not improve from 0.59646
Saved model to disk
