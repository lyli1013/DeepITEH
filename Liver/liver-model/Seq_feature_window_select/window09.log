/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa5f00eb190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa5f00eb190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa6084ad5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa6084ad5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5f0211ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5f0211ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5ea27f9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5ea27f9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5f01ca890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5f01ca890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5f01df6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5f01df6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5ea27fad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5ea27fad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5ea100f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5ea100f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5ea1ab7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5ea1ab7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e9f0aa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e9f0aa50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5ea27f3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5ea27f3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5ea219790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5ea219790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e9de6290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e9de6290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e9f63e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e9f63e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e9be98d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e9be98d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e9f21690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e9f21690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5ea1f7110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5ea1f7110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e9e1fcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e9e1fcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e9f27550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e9f27550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e995cdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e995cdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e977dcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e977dcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e9a80310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e9a80310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e98cdf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e98cdf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e987c050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e987c050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e976ba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e976ba90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e949bcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e949bcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e9675890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e9675890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e956c4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e956c4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e9485210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e9485210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e9274510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e9274510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e956f4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e956f4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e9485490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e9485490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e91398d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e91398d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e928b350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e928b350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e8f5cd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e8f5cd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e9002fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e9002fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e928b090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e928b090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8f55a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8f55a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e8d629d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e8d629d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e8d42090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e8d42090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8cedb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8cedb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e8cedd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e8cedd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8c3af10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8c3af10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e8a5ed10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e8a5ed10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e8948110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e8948110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8c2fed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8c2fed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e8958b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e8958b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e87b3190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e87b3190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e8911890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e8911890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e895f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e895f590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8901d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8901d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e8914310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e8914310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e85af550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e85af550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e8601e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e8601e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e8305710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e8305710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e85fe8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e85fe8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e8429d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e8429d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e82c9310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e82c9310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e80ac890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5e80ac890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e80a2b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5e80a2b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8396b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8396b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e80ac590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5e80ac590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8030fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5e8030fd0>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-17 11:02:18.964675: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-17 11:02:19.041889: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-17 11:02:19.118224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563a620ce930 executing computations on platform Host. Devices:
2022-11-17 11:02:19.118321: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-17 11:02:20.062972: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window09.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 52:07 - loss: 0.7300 - acc: 0.5312
 128/4566 [..............................] - ETA: 35:41 - loss: 0.7438 - acc: 0.5078
 192/4566 [>.............................] - ETA: 27:18 - loss: 0.7459 - acc: 0.4896
 256/4566 [>.............................] - ETA: 22:05 - loss: 0.7357 - acc: 0.5117
 320/4566 [=>............................] - ETA: 18:51 - loss: 0.7228 - acc: 0.5312
 384/4566 [=>............................] - ETA: 16:39 - loss: 0.7295 - acc: 0.5260
 448/4566 [=>............................] - ETA: 15:03 - loss: 0.7219 - acc: 0.5335
 512/4566 [==>...........................] - ETA: 13:46 - loss: 0.7268 - acc: 0.5312
 576/4566 [==>...........................] - ETA: 12:49 - loss: 0.7305 - acc: 0.5312
 640/4566 [===>..........................] - ETA: 12:01 - loss: 0.7314 - acc: 0.5328
 704/4566 [===>..........................] - ETA: 11:18 - loss: 0.7344 - acc: 0.5298
 768/4566 [====>.........................] - ETA: 10:42 - loss: 0.7312 - acc: 0.5312
 832/4566 [====>.........................] - ETA: 10:12 - loss: 0.7290 - acc: 0.5373
 896/4566 [====>.........................] - ETA: 9:44 - loss: 0.7288 - acc: 0.5346 
 960/4566 [=====>........................] - ETA: 9:22 - loss: 0.7256 - acc: 0.5406
1024/4566 [=====>........................] - ETA: 9:01 - loss: 0.7266 - acc: 0.5342
1088/4566 [======>.......................] - ETA: 9:12 - loss: 0.7259 - acc: 0.5340
1152/4566 [======>.......................] - ETA: 9:19 - loss: 0.7244 - acc: 0.5365
1216/4566 [======>.......................] - ETA: 9:23 - loss: 0.7245 - acc: 0.5354
1280/4566 [=======>......................] - ETA: 9:26 - loss: 0.7245 - acc: 0.5336
1344/4566 [=======>......................] - ETA: 9:26 - loss: 0.7258 - acc: 0.5290
1408/4566 [========>.....................] - ETA: 9:12 - loss: 0.7242 - acc: 0.5305
1472/4566 [========>.....................] - ETA: 8:50 - loss: 0.7247 - acc: 0.5292
1536/4566 [=========>....................] - ETA: 8:28 - loss: 0.7247 - acc: 0.5293
1600/4566 [=========>....................] - ETA: 8:10 - loss: 0.7245 - acc: 0.5319
1664/4566 [=========>....................] - ETA: 7:51 - loss: 0.7225 - acc: 0.5319
1728/4566 [==========>...................] - ETA: 7:34 - loss: 0.7228 - acc: 0.5295
1792/4566 [==========>...................] - ETA: 7:18 - loss: 0.7233 - acc: 0.5279
1856/4566 [===========>..................] - ETA: 7:02 - loss: 0.7223 - acc: 0.5291
1920/4566 [===========>..................] - ETA: 6:46 - loss: 0.7206 - acc: 0.5323
1984/4566 [============>.................] - ETA: 6:31 - loss: 0.7229 - acc: 0.5287
2048/4566 [============>.................] - ETA: 6:17 - loss: 0.7237 - acc: 0.5269
2112/4566 [============>.................] - ETA: 6:04 - loss: 0.7221 - acc: 0.5265
2176/4566 [=============>................] - ETA: 5:50 - loss: 0.7221 - acc: 0.5257
2240/4566 [=============>................] - ETA: 5:38 - loss: 0.7230 - acc: 0.5237
2304/4566 [==============>...............] - ETA: 5:26 - loss: 0.7223 - acc: 0.5260
2368/4566 [==============>...............] - ETA: 5:21 - loss: 0.7218 - acc: 0.5253
2432/4566 [==============>...............] - ETA: 5:17 - loss: 0.7221 - acc: 0.5238
2496/4566 [===============>..............] - ETA: 5:12 - loss: 0.7230 - acc: 0.5216
2560/4566 [===============>..............] - ETA: 5:07 - loss: 0.7230 - acc: 0.5203
2624/4566 [================>.............] - ETA: 5:01 - loss: 0.7229 - acc: 0.5202
2688/4566 [================>.............] - ETA: 4:52 - loss: 0.7231 - acc: 0.5167
2752/4566 [=================>............] - ETA: 4:40 - loss: 0.7209 - acc: 0.5193
2816/4566 [=================>............] - ETA: 4:28 - loss: 0.7218 - acc: 0.5195
2880/4566 [=================>............] - ETA: 4:16 - loss: 0.7221 - acc: 0.5177
2944/4566 [==================>...........] - ETA: 4:04 - loss: 0.7226 - acc: 0.5170
3008/4566 [==================>...........] - ETA: 3:53 - loss: 0.7224 - acc: 0.5170
3072/4566 [===================>..........] - ETA: 3:41 - loss: 0.7223 - acc: 0.5166
3136/4566 [===================>..........] - ETA: 3:30 - loss: 0.7223 - acc: 0.5172
3200/4566 [====================>.........] - ETA: 3:19 - loss: 0.7222 - acc: 0.5159
3264/4566 [====================>.........] - ETA: 3:08 - loss: 0.7210 - acc: 0.5156
3328/4566 [====================>.........] - ETA: 2:58 - loss: 0.7218 - acc: 0.5153
3392/4566 [=====================>........] - ETA: 2:48 - loss: 0.7211 - acc: 0.5159
3456/4566 [=====================>........] - ETA: 2:37 - loss: 0.7203 - acc: 0.5162
3520/4566 [======================>.......] - ETA: 2:27 - loss: 0.7199 - acc: 0.5170
3584/4566 [======================>.......] - ETA: 2:17 - loss: 0.7191 - acc: 0.5173
3648/4566 [======================>.......] - ETA: 2:09 - loss: 0.7197 - acc: 0.5170
3712/4566 [=======================>......] - ETA: 2:01 - loss: 0.7184 - acc: 0.5183
3776/4566 [=======================>......] - ETA: 1:53 - loss: 0.7177 - acc: 0.5183
3840/4566 [========================>.....] - ETA: 1:45 - loss: 0.7176 - acc: 0.5185
3904/4566 [========================>.....] - ETA: 1:36 - loss: 0.7180 - acc: 0.5179
3968/4566 [=========================>....] - ETA: 1:28 - loss: 0.7183 - acc: 0.5161
4032/4566 [=========================>....] - ETA: 1:18 - loss: 0.7188 - acc: 0.5146
4096/4566 [=========================>....] - ETA: 1:08 - loss: 0.7188 - acc: 0.5144
4160/4566 [==========================>...] - ETA: 59s - loss: 0.7180 - acc: 0.5156 
4224/4566 [==========================>...] - ETA: 49s - loss: 0.7178 - acc: 0.5159
4288/4566 [===========================>..] - ETA: 40s - loss: 0.7178 - acc: 0.5161
4352/4566 [===========================>..] - ETA: 30s - loss: 0.7186 - acc: 0.5145
4416/4566 [============================>.] - ETA: 21s - loss: 0.7181 - acc: 0.5149
4480/4566 [============================>.] - ETA: 12s - loss: 0.7178 - acc: 0.5152
4544/4566 [============================>.] - ETA: 3s - loss: 0.7173 - acc: 0.5158 
4566/4566 [==============================] - 663s 145ms/step - loss: 0.7174 - acc: 0.5158 - val_loss: 0.6790 - val_acc: 0.5610

Epoch 00001: val_acc improved from -inf to 0.56102, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window09/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 6:40 - loss: 0.7022 - acc: 0.5156
 128/4566 [..............................] - ETA: 8:21 - loss: 0.7087 - acc: 0.5000
 192/4566 [>.............................] - ETA: 12:47 - loss: 0.7154 - acc: 0.4896
 256/4566 [>.............................] - ETA: 14:04 - loss: 0.7088 - acc: 0.5078
 320/4566 [=>............................] - ETA: 14:37 - loss: 0.7116 - acc: 0.4906
 384/4566 [=>............................] - ETA: 15:09 - loss: 0.7091 - acc: 0.5078
 448/4566 [=>............................] - ETA: 14:50 - loss: 0.7076 - acc: 0.5134
 512/4566 [==>...........................] - ETA: 13:37 - loss: 0.6998 - acc: 0.5312
 576/4566 [==>...........................] - ETA: 12:40 - loss: 0.7048 - acc: 0.5156
 640/4566 [===>..........................] - ETA: 11:50 - loss: 0.7043 - acc: 0.5188
 704/4566 [===>..........................] - ETA: 11:04 - loss: 0.7022 - acc: 0.5298
 768/4566 [====>.........................] - ETA: 10:29 - loss: 0.7019 - acc: 0.5273
 832/4566 [====>.........................] - ETA: 9:55 - loss: 0.7068 - acc: 0.5180 
 896/4566 [====>.........................] - ETA: 9:27 - loss: 0.7037 - acc: 0.5190
 960/4566 [=====>........................] - ETA: 9:04 - loss: 0.7061 - acc: 0.5135
1024/4566 [=====>........................] - ETA: 8:41 - loss: 0.7049 - acc: 0.5215
1088/4566 [======>.......................] - ETA: 8:19 - loss: 0.7061 - acc: 0.5175
1152/4566 [======>.......................] - ETA: 8:00 - loss: 0.7042 - acc: 0.5191
1216/4566 [======>.......................] - ETA: 7:43 - loss: 0.7053 - acc: 0.5181
1280/4566 [=======>......................] - ETA: 7:26 - loss: 0.7062 - acc: 0.5164
1344/4566 [=======>......................] - ETA: 7:11 - loss: 0.7064 - acc: 0.5156
1408/4566 [========>.....................] - ETA: 6:57 - loss: 0.7075 - acc: 0.5121
1472/4566 [========>.....................] - ETA: 6:56 - loss: 0.7077 - acc: 0.5109
1536/4566 [=========>....................] - ETA: 7:00 - loss: 0.7053 - acc: 0.5156
1600/4566 [=========>....................] - ETA: 7:02 - loss: 0.7046 - acc: 0.5162
1664/4566 [=========>....................] - ETA: 7:01 - loss: 0.7056 - acc: 0.5132
1728/4566 [==========>...................] - ETA: 7:02 - loss: 0.7060 - acc: 0.5127
1792/4566 [==========>...................] - ETA: 6:59 - loss: 0.7046 - acc: 0.5145
1856/4566 [===========>..................] - ETA: 6:46 - loss: 0.7058 - acc: 0.5129
1920/4566 [===========>..................] - ETA: 6:31 - loss: 0.7052 - acc: 0.5156
1984/4566 [============>.................] - ETA: 6:17 - loss: 0.7049 - acc: 0.5141
2048/4566 [============>.................] - ETA: 6:03 - loss: 0.7042 - acc: 0.5166
2112/4566 [============>.................] - ETA: 5:50 - loss: 0.7040 - acc: 0.5152
2176/4566 [=============>................] - ETA: 5:37 - loss: 0.7041 - acc: 0.5142
2240/4566 [=============>................] - ETA: 5:25 - loss: 0.7042 - acc: 0.5152
2304/4566 [==============>...............] - ETA: 5:13 - loss: 0.7039 - acc: 0.5139
2368/4566 [==============>...............] - ETA: 5:00 - loss: 0.7043 - acc: 0.5127
2432/4566 [==============>...............] - ETA: 4:49 - loss: 0.7045 - acc: 0.5119
2496/4566 [===============>..............] - ETA: 4:38 - loss: 0.7044 - acc: 0.5120
2560/4566 [===============>..............] - ETA: 4:27 - loss: 0.7052 - acc: 0.5109
2624/4566 [================>.............] - ETA: 4:17 - loss: 0.7064 - acc: 0.5095
2688/4566 [================>.............] - ETA: 4:07 - loss: 0.7054 - acc: 0.5115
2752/4566 [=================>............] - ETA: 3:56 - loss: 0.7041 - acc: 0.5142
2816/4566 [=================>............] - ETA: 3:50 - loss: 0.7039 - acc: 0.5163
2880/4566 [=================>............] - ETA: 3:45 - loss: 0.7028 - acc: 0.5188
2944/4566 [==================>...........] - ETA: 3:40 - loss: 0.7023 - acc: 0.5211
3008/4566 [==================>...........] - ETA: 3:34 - loss: 0.7022 - acc: 0.5206
3072/4566 [===================>..........] - ETA: 3:28 - loss: 0.7018 - acc: 0.5208
3136/4566 [===================>..........] - ETA: 3:21 - loss: 0.7013 - acc: 0.5220
3200/4566 [====================>.........] - ETA: 3:12 - loss: 0.7012 - acc: 0.5238
3264/4566 [====================>.........] - ETA: 3:01 - loss: 0.7012 - acc: 0.5236
3328/4566 [====================>.........] - ETA: 2:51 - loss: 0.7022 - acc: 0.5222
3392/4566 [=====================>........] - ETA: 2:41 - loss: 0.7015 - acc: 0.5230
3456/4566 [=====================>........] - ETA: 2:31 - loss: 0.7015 - acc: 0.5217
3520/4566 [======================>.......] - ETA: 2:21 - loss: 0.7007 - acc: 0.5216
3584/4566 [======================>.......] - ETA: 2:12 - loss: 0.7009 - acc: 0.5206
3648/4566 [======================>.......] - ETA: 2:03 - loss: 0.7008 - acc: 0.5206
3712/4566 [=======================>......] - ETA: 1:53 - loss: 0.7005 - acc: 0.5213
3776/4566 [=======================>......] - ETA: 1:44 - loss: 0.7002 - acc: 0.5212
3840/4566 [========================>.....] - ETA: 1:35 - loss: 0.7000 - acc: 0.5211
3904/4566 [========================>.....] - ETA: 1:26 - loss: 0.7005 - acc: 0.5200
3968/4566 [=========================>....] - ETA: 1:17 - loss: 0.7001 - acc: 0.5207
4032/4566 [=========================>....] - ETA: 1:09 - loss: 0.7000 - acc: 0.5208
4096/4566 [=========================>....] - ETA: 1:00 - loss: 0.6996 - acc: 0.5208
4160/4566 [==========================>...] - ETA: 52s - loss: 0.6992 - acc: 0.5212 
4224/4566 [==========================>...] - ETA: 44s - loss: 0.6992 - acc: 0.5218
4288/4566 [===========================>..] - ETA: 36s - loss: 0.6993 - acc: 0.5217
4352/4566 [===========================>..] - ETA: 28s - loss: 0.6994 - acc: 0.5211
4416/4566 [============================>.] - ETA: 20s - loss: 0.6994 - acc: 0.5213
4480/4566 [============================>.] - ETA: 11s - loss: 0.6990 - acc: 0.5228
4544/4566 [============================>.] - ETA: 2s - loss: 0.6990 - acc: 0.5224 
4566/4566 [==============================] - 636s 139ms/step - loss: 0.6990 - acc: 0.5223 - val_loss: 0.6785 - val_acc: 0.5630

Epoch 00002: val_acc improved from 0.56102 to 0.56299, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window09/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 3/10

  64/4566 [..............................] - ETA: 6:02 - loss: 0.7106 - acc: 0.4688
 128/4566 [..............................] - ETA: 6:00 - loss: 0.6837 - acc: 0.5547
 192/4566 [>.............................] - ETA: 5:58 - loss: 0.6952 - acc: 0.5208
 256/4566 [>.............................] - ETA: 5:54 - loss: 0.6898 - acc: 0.5391
 320/4566 [=>............................] - ETA: 5:55 - loss: 0.6917 - acc: 0.5281
 384/4566 [=>............................] - ETA: 5:46 - loss: 0.6923 - acc: 0.5260
 448/4566 [=>............................] - ETA: 5:38 - loss: 0.6952 - acc: 0.5201
 512/4566 [==>...........................] - ETA: 5:36 - loss: 0.6937 - acc: 0.5293
 576/4566 [==>...........................] - ETA: 5:30 - loss: 0.6905 - acc: 0.5434
 640/4566 [===>..........................] - ETA: 5:26 - loss: 0.6930 - acc: 0.5312
 704/4566 [===>..........................] - ETA: 5:21 - loss: 0.6951 - acc: 0.5256
 768/4566 [====>.........................] - ETA: 5:17 - loss: 0.6918 - acc: 0.5326
 832/4566 [====>.........................] - ETA: 5:24 - loss: 0.6913 - acc: 0.5361
 896/4566 [====>.........................] - ETA: 5:48 - loss: 0.6903 - acc: 0.5402
 960/4566 [=====>........................] - ETA: 6:12 - loss: 0.6905 - acc: 0.5427
1024/4566 [=====>........................] - ETA: 6:28 - loss: 0.6937 - acc: 0.5391
1088/4566 [======>.......................] - ETA: 6:43 - loss: 0.6920 - acc: 0.5423
1152/4566 [======>.......................] - ETA: 6:56 - loss: 0.6930 - acc: 0.5399
1216/4566 [======>.......................] - ETA: 6:59 - loss: 0.6949 - acc: 0.5378
1280/4566 [=======>......................] - ETA: 6:49 - loss: 0.6955 - acc: 0.5359
1344/4566 [=======>......................] - ETA: 6:35 - loss: 0.6962 - acc: 0.5350
1408/4566 [========>.....................] - ETA: 6:21 - loss: 0.6957 - acc: 0.5362
1472/4566 [========>.....................] - ETA: 6:07 - loss: 0.6965 - acc: 0.5353
1536/4566 [=========>....................] - ETA: 5:54 - loss: 0.6963 - acc: 0.5352
1600/4566 [=========>....................] - ETA: 5:42 - loss: 0.6968 - acc: 0.5331
1664/4566 [=========>....................] - ETA: 5:32 - loss: 0.6977 - acc: 0.5319
1728/4566 [==========>...................] - ETA: 5:22 - loss: 0.6977 - acc: 0.5312
1792/4566 [==========>...................] - ETA: 5:12 - loss: 0.6991 - acc: 0.5296
1856/4566 [===========>..................] - ETA: 5:02 - loss: 0.6972 - acc: 0.5334
1920/4566 [===========>..................] - ETA: 4:52 - loss: 0.6965 - acc: 0.5333
1984/4566 [============>.................] - ETA: 4:43 - loss: 0.6981 - acc: 0.5318
2048/4566 [============>.................] - ETA: 4:34 - loss: 0.6968 - acc: 0.5342
2112/4566 [============>.................] - ETA: 4:25 - loss: 0.6964 - acc: 0.5355
2176/4566 [=============>................] - ETA: 4:16 - loss: 0.6952 - acc: 0.5377
2240/4566 [=============>................] - ETA: 4:08 - loss: 0.6934 - acc: 0.5420
2304/4566 [==============>...............] - ETA: 4:01 - loss: 0.6929 - acc: 0.5425
2368/4566 [==============>...............] - ETA: 4:00 - loss: 0.6911 - acc: 0.5460
2432/4566 [==============>...............] - ETA: 3:59 - loss: 0.6904 - acc: 0.5473
2496/4566 [===============>..............] - ETA: 3:57 - loss: 0.6907 - acc: 0.5473
2560/4566 [===============>..............] - ETA: 3:54 - loss: 0.6903 - acc: 0.5480
2624/4566 [================>.............] - ETA: 3:51 - loss: 0.6910 - acc: 0.5450
2688/4566 [================>.............] - ETA: 3:47 - loss: 0.6907 - acc: 0.5446
2752/4566 [=================>............] - ETA: 3:38 - loss: 0.6904 - acc: 0.5454
2816/4566 [=================>............] - ETA: 3:29 - loss: 0.6906 - acc: 0.5462
2880/4566 [=================>............] - ETA: 3:20 - loss: 0.6905 - acc: 0.5465
2944/4566 [==================>...........] - ETA: 3:11 - loss: 0.6906 - acc: 0.5462
3008/4566 [==================>...........] - ETA: 3:02 - loss: 0.6905 - acc: 0.5462
3072/4566 [===================>..........] - ETA: 2:53 - loss: 0.6899 - acc: 0.5472
3136/4566 [===================>..........] - ETA: 2:45 - loss: 0.6906 - acc: 0.5466
3200/4566 [====================>.........] - ETA: 2:36 - loss: 0.6908 - acc: 0.5466
3264/4566 [====================>.........] - ETA: 2:28 - loss: 0.6907 - acc: 0.5466
3328/4566 [====================>.........] - ETA: 2:20 - loss: 0.6912 - acc: 0.5454
3392/4566 [=====================>........] - ETA: 2:12 - loss: 0.6923 - acc: 0.5427
3456/4566 [=====================>........] - ETA: 2:04 - loss: 0.6923 - acc: 0.5422
3520/4566 [======================>.......] - ETA: 1:56 - loss: 0.6919 - acc: 0.5423
3584/4566 [======================>.......] - ETA: 1:49 - loss: 0.6925 - acc: 0.5410
3648/4566 [======================>.......] - ETA: 1:41 - loss: 0.6923 - acc: 0.5425
3712/4566 [=======================>......] - ETA: 1:34 - loss: 0.6917 - acc: 0.5439
3776/4566 [=======================>......] - ETA: 1:26 - loss: 0.6915 - acc: 0.5434
3840/4566 [========================>.....] - ETA: 1:19 - loss: 0.6916 - acc: 0.5430
3904/4566 [========================>.....] - ETA: 1:14 - loss: 0.6916 - acc: 0.5423
3968/4566 [=========================>....] - ETA: 1:07 - loss: 0.6913 - acc: 0.5423
4032/4566 [=========================>....] - ETA: 1:01 - loss: 0.6916 - acc: 0.5417
4096/4566 [=========================>....] - ETA: 54s - loss: 0.6917 - acc: 0.5413 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6923 - acc: 0.5404
4224/4566 [==========================>...] - ETA: 40s - loss: 0.6926 - acc: 0.5393
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6925 - acc: 0.5394
4352/4566 [===========================>..] - ETA: 25s - loss: 0.6919 - acc: 0.5400
4416/4566 [============================>.] - ETA: 17s - loss: 0.6919 - acc: 0.5403
4480/4566 [============================>.] - ETA: 10s - loss: 0.6916 - acc: 0.5408
4544/4566 [============================>.] - ETA: 2s - loss: 0.6916 - acc: 0.5405 
4566/4566 [==============================] - 545s 119ms/step - loss: 0.6916 - acc: 0.5405 - val_loss: 0.6786 - val_acc: 0.5512

Epoch 00003: val_acc did not improve from 0.56299
Epoch 4/10

  64/4566 [..............................] - ETA: 5:31 - loss: 0.6841 - acc: 0.5625
 128/4566 [..............................] - ETA: 5:36 - loss: 0.6777 - acc: 0.5781
 192/4566 [>.............................] - ETA: 5:48 - loss: 0.6803 - acc: 0.5625
 256/4566 [>.............................] - ETA: 5:35 - loss: 0.6750 - acc: 0.5664
 320/4566 [=>............................] - ETA: 5:38 - loss: 0.6795 - acc: 0.5625
 384/4566 [=>............................] - ETA: 5:30 - loss: 0.6829 - acc: 0.5677
 448/4566 [=>............................] - ETA: 5:21 - loss: 0.6868 - acc: 0.5625
 512/4566 [==>...........................] - ETA: 5:18 - loss: 0.6845 - acc: 0.5664
 576/4566 [==>...........................] - ETA: 5:26 - loss: 0.6836 - acc: 0.5642
 640/4566 [===>..........................] - ETA: 6:09 - loss: 0.6851 - acc: 0.5641
 704/4566 [===>..........................] - ETA: 6:44 - loss: 0.6865 - acc: 0.5625
 768/4566 [====>.........................] - ETA: 7:09 - loss: 0.6862 - acc: 0.5664
 832/4566 [====>.........................] - ETA: 7:28 - loss: 0.6869 - acc: 0.5625
 896/4566 [====>.........................] - ETA: 7:43 - loss: 0.6857 - acc: 0.5614
 960/4566 [=====>........................] - ETA: 7:51 - loss: 0.6837 - acc: 0.5667
1024/4566 [=====>........................] - ETA: 7:39 - loss: 0.6839 - acc: 0.5693
1088/4566 [======>.......................] - ETA: 7:19 - loss: 0.6834 - acc: 0.5689
1152/4566 [======>.......................] - ETA: 7:01 - loss: 0.6844 - acc: 0.5660
1216/4566 [======>.......................] - ETA: 6:46 - loss: 0.6870 - acc: 0.5641
1280/4566 [=======>......................] - ETA: 6:31 - loss: 0.6839 - acc: 0.5711
1344/4566 [=======>......................] - ETA: 6:17 - loss: 0.6823 - acc: 0.5737
1408/4566 [========>.....................] - ETA: 6:04 - loss: 0.6823 - acc: 0.5696
1472/4566 [========>.....................] - ETA: 5:53 - loss: 0.6829 - acc: 0.5686
1536/4566 [=========>....................] - ETA: 5:42 - loss: 0.6816 - acc: 0.5710
1600/4566 [=========>....................] - ETA: 5:31 - loss: 0.6797 - acc: 0.5781
1664/4566 [=========>....................] - ETA: 5:20 - loss: 0.6804 - acc: 0.5757
1728/4566 [==========>...................] - ETA: 5:10 - loss: 0.6813 - acc: 0.5729
1792/4566 [==========>...................] - ETA: 4:59 - loss: 0.6813 - acc: 0.5725
1856/4566 [===========>..................] - ETA: 4:50 - loss: 0.6823 - acc: 0.5738
1920/4566 [===========>..................] - ETA: 4:40 - loss: 0.6826 - acc: 0.5760
1984/4566 [============>.................] - ETA: 4:31 - loss: 0.6833 - acc: 0.5736
2048/4566 [============>.................] - ETA: 4:22 - loss: 0.6841 - acc: 0.5703
2112/4566 [============>.................] - ETA: 4:14 - loss: 0.6854 - acc: 0.5677
2176/4566 [=============>................] - ETA: 4:13 - loss: 0.6862 - acc: 0.5662
2240/4566 [=============>................] - ETA: 4:13 - loss: 0.6870 - acc: 0.5661
2304/4566 [==============>...............] - ETA: 4:12 - loss: 0.6863 - acc: 0.5664
2368/4566 [==============>...............] - ETA: 4:10 - loss: 0.6863 - acc: 0.5663
2432/4566 [==============>...............] - ETA: 4:06 - loss: 0.6862 - acc: 0.5662
2496/4566 [===============>..............] - ETA: 4:03 - loss: 0.6862 - acc: 0.5657
2560/4566 [===============>..............] - ETA: 3:59 - loss: 0.6860 - acc: 0.5664
2624/4566 [================>.............] - ETA: 3:50 - loss: 0.6871 - acc: 0.5652
2688/4566 [================>.............] - ETA: 3:41 - loss: 0.6870 - acc: 0.5647
2752/4566 [=================>............] - ETA: 3:31 - loss: 0.6856 - acc: 0.5665
2816/4566 [=================>............] - ETA: 3:22 - loss: 0.6859 - acc: 0.5661
2880/4566 [=================>............] - ETA: 3:13 - loss: 0.6865 - acc: 0.5618
2944/4566 [==================>...........] - ETA: 3:05 - loss: 0.6875 - acc: 0.5591
3008/4566 [==================>...........] - ETA: 2:57 - loss: 0.6874 - acc: 0.5598
3072/4566 [===================>..........] - ETA: 2:49 - loss: 0.6869 - acc: 0.5609
3136/4566 [===================>..........] - ETA: 2:41 - loss: 0.6867 - acc: 0.5622
3200/4566 [====================>.........] - ETA: 2:32 - loss: 0.6870 - acc: 0.5606
3264/4566 [====================>.........] - ETA: 2:24 - loss: 0.6880 - acc: 0.5591
3328/4566 [====================>.........] - ETA: 2:17 - loss: 0.6869 - acc: 0.5592
3392/4566 [=====================>........] - ETA: 2:09 - loss: 0.6861 - acc: 0.5619
3456/4566 [=====================>........] - ETA: 2:01 - loss: 0.6861 - acc: 0.5625
3520/4566 [======================>.......] - ETA: 1:54 - loss: 0.6860 - acc: 0.5622
3584/4566 [======================>.......] - ETA: 1:46 - loss: 0.6855 - acc: 0.5633
3648/4566 [======================>.......] - ETA: 1:39 - loss: 0.6854 - acc: 0.5636
3712/4566 [=======================>......] - ETA: 1:33 - loss: 0.6849 - acc: 0.5655
3776/4566 [=======================>......] - ETA: 1:28 - loss: 0.6844 - acc: 0.5662
3840/4566 [========================>.....] - ETA: 1:22 - loss: 0.6844 - acc: 0.5659
3904/4566 [========================>.....] - ETA: 1:15 - loss: 0.6842 - acc: 0.5653
3968/4566 [=========================>....] - ETA: 1:09 - loss: 0.6838 - acc: 0.5655
4032/4566 [=========================>....] - ETA: 1:02 - loss: 0.6845 - acc: 0.5650
4096/4566 [=========================>....] - ETA: 55s - loss: 0.6850 - acc: 0.5645 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6852 - acc: 0.5639
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6850 - acc: 0.5623
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6858 - acc: 0.5599
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6858 - acc: 0.5595
4416/4566 [============================>.] - ETA: 17s - loss: 0.6860 - acc: 0.5591
4480/4566 [============================>.] - ETA: 9s - loss: 0.6857 - acc: 0.5592 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6857 - acc: 0.5601
4566/4566 [==============================] - 532s 117ms/step - loss: 0.6859 - acc: 0.5600 - val_loss: 0.6728 - val_acc: 0.6102

Epoch 00004: val_acc improved from 0.56299 to 0.61024, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window09/checkpoints/final_seq_mode/liver_seq_model_2_mer.hdf5
Epoch 5/10

  64/4566 [..............................] - ETA: 6:20 - loss: 0.6597 - acc: 0.5938
 128/4566 [..............................] - ETA: 5:56 - loss: 0.6748 - acc: 0.5547
 192/4566 [>.............................] - ETA: 5:38 - loss: 0.6813 - acc: 0.5469
 256/4566 [>.............................] - ETA: 5:35 - loss: 0.6976 - acc: 0.5078
 320/4566 [=>............................] - ETA: 5:31 - loss: 0.6862 - acc: 0.5281
 384/4566 [=>............................] - ETA: 5:29 - loss: 0.6826 - acc: 0.5443
 448/4566 [=>............................] - ETA: 5:39 - loss: 0.6846 - acc: 0.5379
 512/4566 [==>...........................] - ETA: 6:33 - loss: 0.6835 - acc: 0.5371
 576/4566 [==>...........................] - ETA: 7:18 - loss: 0.6846 - acc: 0.5312
 640/4566 [===>..........................] - ETA: 7:43 - loss: 0.6806 - acc: 0.5437
 704/4566 [===>..........................] - ETA: 8:03 - loss: 0.6812 - acc: 0.5497
 768/4566 [====>.........................] - ETA: 8:18 - loss: 0.6809 - acc: 0.5495
 832/4566 [====>.........................] - ETA: 8:29 - loss: 0.6841 - acc: 0.5421
 896/4566 [====>.........................] - ETA: 8:19 - loss: 0.6868 - acc: 0.5357
 960/4566 [=====>........................] - ETA: 7:58 - loss: 0.6836 - acc: 0.5437
1024/4566 [=====>........................] - ETA: 7:37 - loss: 0.6828 - acc: 0.5459
1088/4566 [======>.......................] - ETA: 7:19 - loss: 0.6836 - acc: 0.5432
1152/4566 [======>.......................] - ETA: 7:02 - loss: 0.6823 - acc: 0.5469
1216/4566 [======>.......................] - ETA: 6:46 - loss: 0.6824 - acc: 0.5469
1280/4566 [=======>......................] - ETA: 6:31 - loss: 0.6824 - acc: 0.5516
1344/4566 [=======>......................] - ETA: 6:17 - loss: 0.6819 - acc: 0.5521
1408/4566 [========>.....................] - ETA: 6:04 - loss: 0.6804 - acc: 0.5547
1472/4566 [========>.....................] - ETA: 5:52 - loss: 0.6809 - acc: 0.5530
1536/4566 [=========>....................] - ETA: 5:40 - loss: 0.6798 - acc: 0.5547
1600/4566 [=========>....................] - ETA: 5:29 - loss: 0.6804 - acc: 0.5537
1664/4566 [=========>....................] - ETA: 5:19 - loss: 0.6814 - acc: 0.5529
1728/4566 [==========>...................] - ETA: 5:09 - loss: 0.6808 - acc: 0.5498
1792/4566 [==========>...................] - ETA: 4:59 - loss: 0.6820 - acc: 0.5491
1856/4566 [===========>..................] - ETA: 4:48 - loss: 0.6819 - acc: 0.5506
1920/4566 [===========>..................] - ETA: 4:39 - loss: 0.6822 - acc: 0.5484
1984/4566 [============>.................] - ETA: 4:32 - loss: 0.6824 - acc: 0.5484
2048/4566 [============>.................] - ETA: 4:34 - loss: 0.6848 - acc: 0.5454
2112/4566 [============>.................] - ETA: 4:34 - loss: 0.6855 - acc: 0.5440
2176/4566 [=============>................] - ETA: 4:33 - loss: 0.6878 - acc: 0.5409
2240/4566 [=============>................] - ETA: 4:33 - loss: 0.6893 - acc: 0.5393
2304/4566 [==============>...............] - ETA: 4:31 - loss: 0.6900 - acc: 0.5378
2368/4566 [==============>...............] - ETA: 4:28 - loss: 0.6902 - acc: 0.5384
2432/4566 [==============>...............] - ETA: 4:20 - loss: 0.6909 - acc: 0.5366
2496/4566 [===============>..............] - ETA: 4:10 - loss: 0.6907 - acc: 0.5377
2560/4566 [===============>..............] - ETA: 4:01 - loss: 0.6901 - acc: 0.5398
2624/4566 [================>.............] - ETA: 3:51 - loss: 0.6899 - acc: 0.5393
2688/4566 [================>.............] - ETA: 3:42 - loss: 0.6897 - acc: 0.5376
2752/4566 [=================>............] - ETA: 3:33 - loss: 0.6894 - acc: 0.5389
2816/4566 [=================>............] - ETA: 3:24 - loss: 0.6902 - acc: 0.5384
2880/4566 [=================>............] - ETA: 3:15 - loss: 0.6895 - acc: 0.5389
2944/4566 [==================>...........] - ETA: 3:07 - loss: 0.6899 - acc: 0.5391
3008/4566 [==================>...........] - ETA: 2:58 - loss: 0.6899 - acc: 0.5386
3072/4566 [===================>..........] - ETA: 2:49 - loss: 0.6903 - acc: 0.5368
3136/4566 [===================>..........] - ETA: 2:41 - loss: 0.6904 - acc: 0.5383
3200/4566 [====================>.........] - ETA: 2:33 - loss: 0.6902 - acc: 0.5387
3264/4566 [====================>.........] - ETA: 2:25 - loss: 0.6906 - acc: 0.5383
3328/4566 [====================>.........] - ETA: 2:17 - loss: 0.6908 - acc: 0.5385
3392/4566 [=====================>........] - ETA: 2:09 - loss: 0.6904 - acc: 0.5404
3456/4566 [=====================>........] - ETA: 2:01 - loss: 0.6908 - acc: 0.5396
3520/4566 [======================>.......] - ETA: 1:54 - loss: 0.6908 - acc: 0.5403
3584/4566 [======================>.......] - ETA: 1:49 - loss: 0.6910 - acc: 0.5396
3648/4566 [======================>.......] - ETA: 1:43 - loss: 0.6912 - acc: 0.5395
3712/4566 [=======================>......] - ETA: 1:37 - loss: 0.6905 - acc: 0.5412
3776/4566 [=======================>......] - ETA: 1:31 - loss: 0.6898 - acc: 0.5424
3840/4566 [========================>.....] - ETA: 1:25 - loss: 0.6903 - acc: 0.5406
3904/4566 [========================>.....] - ETA: 1:18 - loss: 0.6901 - acc: 0.5415
3968/4566 [=========================>....] - ETA: 1:11 - loss: 0.6898 - acc: 0.5423
4032/4566 [=========================>....] - ETA: 1:03 - loss: 0.6898 - acc: 0.5429
4096/4566 [=========================>....] - ETA: 55s - loss: 0.6894 - acc: 0.5439 
4160/4566 [==========================>...] - ETA: 47s - loss: 0.6885 - acc: 0.5454
4224/4566 [==========================>...] - ETA: 39s - loss: 0.6880 - acc: 0.5466
4288/4566 [===========================>..] - ETA: 32s - loss: 0.6880 - acc: 0.5459
4352/4566 [===========================>..] - ETA: 24s - loss: 0.6879 - acc: 0.5462
4416/4566 [============================>.] - ETA: 17s - loss: 0.6878 - acc: 0.5469
4480/4566 [============================>.] - ETA: 9s - loss: 0.6883 - acc: 0.5460 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6882 - acc: 0.5460
4566/4566 [==============================] - 533s 117ms/step - loss: 0.6878 - acc: 0.5466 - val_loss: 0.6822 - val_acc: 0.6024

Epoch 00005: val_acc did not improve from 0.61024
Epoch 6/10

  64/4566 [..............................] - ETA: 5:39 - loss: 0.6969 - acc: 0.5469
 128/4566 [..............................] - ETA: 5:26 - loss: 0.6661 - acc: 0.5938
 192/4566 [>.............................] - ETA: 5:35 - loss: 0.6631 - acc: 0.5885
 256/4566 [>.............................] - ETA: 5:30 - loss: 0.6811 - acc: 0.5664
 320/4566 [=>............................] - ETA: 6:29 - loss: 0.6889 - acc: 0.5656
 384/4566 [=>............................] - ETA: 7:54 - loss: 0.7040 - acc: 0.5391
 448/4566 [=>............................] - ETA: 8:37 - loss: 0.7073 - acc: 0.5357
 512/4566 [==>...........................] - ETA: 9:01 - loss: 0.7014 - acc: 0.5430
 576/4566 [==>...........................] - ETA: 9:19 - loss: 0.7021 - acc: 0.5382
 640/4566 [===>..........................] - ETA: 9:32 - loss: 0.6952 - acc: 0.5484
 704/4566 [===>..........................] - ETA: 9:33 - loss: 0.6977 - acc: 0.5455
 768/4566 [====>.........................] - ETA: 9:06 - loss: 0.6948 - acc: 0.5508
 832/4566 [====>.........................] - ETA: 8:38 - loss: 0.6921 - acc: 0.5493
 896/4566 [====>.........................] - ETA: 8:13 - loss: 0.6907 - acc: 0.5502
 960/4566 [=====>........................] - ETA: 7:50 - loss: 0.6946 - acc: 0.5417
1024/4566 [=====>........................] - ETA: 7:30 - loss: 0.6935 - acc: 0.5430
1088/4566 [======>.......................] - ETA: 7:11 - loss: 0.6947 - acc: 0.5386
1152/4566 [======>.......................] - ETA: 6:52 - loss: 0.6939 - acc: 0.5417
1216/4566 [======>.......................] - ETA: 6:36 - loss: 0.6933 - acc: 0.5403
1280/4566 [=======>......................] - ETA: 6:22 - loss: 0.6922 - acc: 0.5406
1344/4566 [=======>......................] - ETA: 6:09 - loss: 0.6929 - acc: 0.5387
1408/4566 [========>.....................] - ETA: 5:56 - loss: 0.6921 - acc: 0.5398
1472/4566 [========>.....................] - ETA: 5:43 - loss: 0.6938 - acc: 0.5346
1536/4566 [=========>....................] - ETA: 5:31 - loss: 0.6928 - acc: 0.5352
1600/4566 [=========>....................] - ETA: 5:20 - loss: 0.6931 - acc: 0.5350
1664/4566 [=========>....................] - ETA: 5:08 - loss: 0.6935 - acc: 0.5343
1728/4566 [==========>...................] - ETA: 4:58 - loss: 0.6931 - acc: 0.5341
1792/4566 [==========>...................] - ETA: 4:48 - loss: 0.6928 - acc: 0.5357
1856/4566 [===========>..................] - ETA: 4:42 - loss: 0.6931 - acc: 0.5361
1920/4566 [===========>..................] - ETA: 4:43 - loss: 0.6931 - acc: 0.5375
1984/4566 [============>.................] - ETA: 4:43 - loss: 0.6927 - acc: 0.5383
2048/4566 [============>.................] - ETA: 4:43 - loss: 0.6922 - acc: 0.5386
2112/4566 [============>.................] - ETA: 4:42 - loss: 0.6915 - acc: 0.5412
2176/4566 [=============>................] - ETA: 4:41 - loss: 0.6913 - acc: 0.5409
2240/4566 [=============>................] - ETA: 4:38 - loss: 0.6919 - acc: 0.5402
2304/4566 [==============>...............] - ETA: 4:29 - loss: 0.6914 - acc: 0.5417
2368/4566 [==============>...............] - ETA: 4:19 - loss: 0.6914 - acc: 0.5427
2432/4566 [==============>...............] - ETA: 4:09 - loss: 0.6914 - acc: 0.5407
2496/4566 [===============>..............] - ETA: 3:59 - loss: 0.6917 - acc: 0.5389
2560/4566 [===============>..............] - ETA: 3:50 - loss: 0.6913 - acc: 0.5391
2624/4566 [================>.............] - ETA: 3:40 - loss: 0.6908 - acc: 0.5408
2688/4566 [================>.............] - ETA: 3:31 - loss: 0.6900 - acc: 0.5420
2752/4566 [=================>............] - ETA: 3:22 - loss: 0.6895 - acc: 0.5436
2816/4566 [=================>............] - ETA: 3:13 - loss: 0.6896 - acc: 0.5437
2880/4566 [=================>............] - ETA: 3:04 - loss: 0.6885 - acc: 0.5458
2944/4566 [==================>...........] - ETA: 2:56 - loss: 0.6884 - acc: 0.5465
3008/4566 [==================>...........] - ETA: 2:48 - loss: 0.6882 - acc: 0.5469
3072/4566 [===================>..........] - ETA: 2:39 - loss: 0.6879 - acc: 0.5462
3136/4566 [===================>..........] - ETA: 2:32 - loss: 0.6872 - acc: 0.5466
3200/4566 [====================>.........] - ETA: 2:24 - loss: 0.6876 - acc: 0.5456
3264/4566 [====================>.........] - ETA: 2:16 - loss: 0.6883 - acc: 0.5435
3328/4566 [====================>.........] - ETA: 2:08 - loss: 0.6878 - acc: 0.5439
3392/4566 [=====================>........] - ETA: 2:01 - loss: 0.6875 - acc: 0.5442
3456/4566 [=====================>........] - ETA: 1:54 - loss: 0.6875 - acc: 0.5437
3520/4566 [======================>.......] - ETA: 1:49 - loss: 0.6871 - acc: 0.5443
3584/4566 [======================>.......] - ETA: 1:44 - loss: 0.6870 - acc: 0.5446
3648/4566 [======================>.......] - ETA: 1:38 - loss: 0.6868 - acc: 0.5444
3712/4566 [=======================>......] - ETA: 1:33 - loss: 0.6862 - acc: 0.5466
3776/4566 [=======================>......] - ETA: 1:27 - loss: 0.6854 - acc: 0.5482
3840/4566 [========================>.....] - ETA: 1:21 - loss: 0.6854 - acc: 0.5487
3904/4566 [========================>.....] - ETA: 1:14 - loss: 0.6858 - acc: 0.5487
3968/4566 [=========================>....] - ETA: 1:06 - loss: 0.6854 - acc: 0.5494
4032/4566 [=========================>....] - ETA: 59s - loss: 0.6855 - acc: 0.5484 
4096/4566 [=========================>....] - ETA: 51s - loss: 0.6852 - acc: 0.5498
4160/4566 [==========================>...] - ETA: 44s - loss: 0.6851 - acc: 0.5507
4224/4566 [==========================>...] - ETA: 37s - loss: 0.6846 - acc: 0.5526
4288/4566 [===========================>..] - ETA: 30s - loss: 0.6846 - acc: 0.5527
4352/4566 [===========================>..] - ETA: 23s - loss: 0.6847 - acc: 0.5524
4416/4566 [============================>.] - ETA: 16s - loss: 0.6842 - acc: 0.5541
4480/4566 [============================>.] - ETA: 9s - loss: 0.6845 - acc: 0.5540 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6843 - acc: 0.5550
4566/4566 [==============================] - 496s 109ms/step - loss: 0.6844 - acc: 0.5543 - val_loss: 0.6687 - val_acc: 0.5906

Epoch 00006: val_acc did not improve from 0.61024
Epoch 7/10

  64/4566 [..............................] - ETA: 4:49 - loss: 0.7073 - acc: 0.4844
 128/4566 [..............................] - ETA: 4:57 - loss: 0.7011 - acc: 0.5312
 192/4566 [>.............................] - ETA: 5:05 - loss: 0.6835 - acc: 0.5885
 256/4566 [>.............................] - ETA: 4:52 - loss: 0.6747 - acc: 0.6133
 320/4566 [=>............................] - ETA: 6:04 - loss: 0.6700 - acc: 0.6188
 384/4566 [=>............................] - ETA: 7:24 - loss: 0.6734 - acc: 0.6120
 448/4566 [=>............................] - ETA: 8:10 - loss: 0.6687 - acc: 0.6161
 512/4566 [==>...........................] - ETA: 8:41 - loss: 0.6661 - acc: 0.6152
 576/4566 [==>...........................] - ETA: 9:03 - loss: 0.6671 - acc: 0.6076
 640/4566 [===>..........................] - ETA: 9:10 - loss: 0.6633 - acc: 0.6125
 704/4566 [===>..........................] - ETA: 9:10 - loss: 0.6679 - acc: 0.6037
 768/4566 [====>.........................] - ETA: 8:42 - loss: 0.6682 - acc: 0.6055
 832/4566 [====>.........................] - ETA: 8:17 - loss: 0.6677 - acc: 0.6058
 896/4566 [====>.........................] - ETA: 7:51 - loss: 0.6684 - acc: 0.6071
 960/4566 [=====>........................] - ETA: 7:28 - loss: 0.6719 - acc: 0.6042
1024/4566 [=====>........................] - ETA: 7:08 - loss: 0.6739 - acc: 0.6006
1088/4566 [======>.......................] - ETA: 6:49 - loss: 0.6730 - acc: 0.6029
1152/4566 [======>.......................] - ETA: 6:31 - loss: 0.6732 - acc: 0.6016
1216/4566 [======>.......................] - ETA: 6:15 - loss: 0.6721 - acc: 0.6044
1280/4566 [=======>......................] - ETA: 6:02 - loss: 0.6721 - acc: 0.6016
1344/4566 [=======>......................] - ETA: 5:49 - loss: 0.6739 - acc: 0.5967
1408/4566 [========>.....................] - ETA: 5:36 - loss: 0.6759 - acc: 0.5930
1472/4566 [========>.....................] - ETA: 5:24 - loss: 0.6757 - acc: 0.5951
1536/4566 [=========>....................] - ETA: 5:14 - loss: 0.6756 - acc: 0.5951
1600/4566 [=========>....................] - ETA: 5:03 - loss: 0.6751 - acc: 0.5944
1664/4566 [=========>....................] - ETA: 4:53 - loss: 0.6749 - acc: 0.5974
1728/4566 [==========>...................] - ETA: 4:43 - loss: 0.6752 - acc: 0.5972
1792/4566 [==========>...................] - ETA: 4:34 - loss: 0.6736 - acc: 0.5971
1856/4566 [===========>..................] - ETA: 4:25 - loss: 0.6725 - acc: 0.5997
1920/4566 [===========>..................] - ETA: 4:20 - loss: 0.6720 - acc: 0.5990
1984/4566 [============>.................] - ETA: 4:22 - loss: 0.6719 - acc: 0.5973
2048/4566 [============>.................] - ETA: 4:22 - loss: 0.6710 - acc: 0.5991
2112/4566 [============>.................] - ETA: 4:21 - loss: 0.6702 - acc: 0.5985
2176/4566 [=============>................] - ETA: 4:21 - loss: 0.6718 - acc: 0.5956
2240/4566 [=============>................] - ETA: 4:19 - loss: 0.6726 - acc: 0.5946
2304/4566 [==============>...............] - ETA: 4:16 - loss: 0.6729 - acc: 0.5933
2368/4566 [==============>...............] - ETA: 4:08 - loss: 0.6725 - acc: 0.5938
2432/4566 [==============>...............] - ETA: 3:59 - loss: 0.6733 - acc: 0.5900
2496/4566 [===============>..............] - ETA: 3:50 - loss: 0.6731 - acc: 0.5901
2560/4566 [===============>..............] - ETA: 3:42 - loss: 0.6721 - acc: 0.5922
2624/4566 [================>.............] - ETA: 3:33 - loss: 0.6724 - acc: 0.5899
2688/4566 [================>.............] - ETA: 3:24 - loss: 0.6728 - acc: 0.5897
2752/4566 [=================>............] - ETA: 3:15 - loss: 0.6731 - acc: 0.5898
2816/4566 [=================>............] - ETA: 3:06 - loss: 0.6734 - acc: 0.5888
2880/4566 [=================>............] - ETA: 2:58 - loss: 0.6740 - acc: 0.5872
2944/4566 [==================>...........] - ETA: 2:49 - loss: 0.6753 - acc: 0.5853
3008/4566 [==================>...........] - ETA: 2:41 - loss: 0.6751 - acc: 0.5861
3072/4566 [===================>..........] - ETA: 2:33 - loss: 0.6748 - acc: 0.5876
3136/4566 [===================>..........] - ETA: 2:26 - loss: 0.6746 - acc: 0.5874
3200/4566 [====================>.........] - ETA: 2:18 - loss: 0.6742 - acc: 0.5891
3264/4566 [====================>.........] - ETA: 2:11 - loss: 0.6736 - acc: 0.5895
3328/4566 [====================>.........] - ETA: 2:04 - loss: 0.6735 - acc: 0.5904
3392/4566 [=====================>........] - ETA: 1:56 - loss: 0.6731 - acc: 0.5914
3456/4566 [=====================>........] - ETA: 1:49 - loss: 0.6736 - acc: 0.5900
3520/4566 [======================>.......] - ETA: 1:42 - loss: 0.6738 - acc: 0.5901
3584/4566 [======================>.......] - ETA: 1:37 - loss: 0.6737 - acc: 0.5898
3648/4566 [======================>.......] - ETA: 1:32 - loss: 0.6733 - acc: 0.5905
3712/4566 [=======================>......] - ETA: 1:27 - loss: 0.6734 - acc: 0.5911
3776/4566 [=======================>......] - ETA: 1:22 - loss: 0.6733 - acc: 0.5911
3840/4566 [========================>.....] - ETA: 1:16 - loss: 0.6737 - acc: 0.5888
3904/4566 [========================>.....] - ETA: 1:10 - loss: 0.6748 - acc: 0.5866
3968/4566 [=========================>....] - ETA: 1:04 - loss: 0.6746 - acc: 0.5864
4032/4566 [=========================>....] - ETA: 57s - loss: 0.6745 - acc: 0.5866 
4096/4566 [=========================>....] - ETA: 50s - loss: 0.6744 - acc: 0.5862
4160/4566 [==========================>...] - ETA: 43s - loss: 0.6739 - acc: 0.5865
4224/4566 [==========================>...] - ETA: 36s - loss: 0.6741 - acc: 0.5864
4288/4566 [===========================>..] - ETA: 29s - loss: 0.6751 - acc: 0.5844
4352/4566 [===========================>..] - ETA: 22s - loss: 0.6747 - acc: 0.5850
4416/4566 [============================>.] - ETA: 15s - loss: 0.6743 - acc: 0.5861
4480/4566 [============================>.] - ETA: 8s - loss: 0.6746 - acc: 0.5853 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6742 - acc: 0.5858
4566/4566 [==============================] - 484s 106ms/step - loss: 0.6741 - acc: 0.5865 - val_loss: 0.6741 - val_acc: 0.5827

Epoch 00007: val_acc did not improve from 0.61024
Epoch 8/10

  64/4566 [..............................] - ETA: 5:15 - loss: 0.6723 - acc: 0.5781
 128/4566 [..............................] - ETA: 5:03 - loss: 0.7028 - acc: 0.5391
 192/4566 [>.............................] - ETA: 4:58 - loss: 0.7174 - acc: 0.5000
 256/4566 [>.............................] - ETA: 4:56 - loss: 0.7180 - acc: 0.5000
 320/4566 [=>............................] - ETA: 4:52 - loss: 0.7142 - acc: 0.5094
 384/4566 [=>............................] - ETA: 5:08 - loss: 0.7068 - acc: 0.5234
 448/4566 [=>............................] - ETA: 6:06 - loss: 0.7033 - acc: 0.5246
 512/4566 [==>...........................] - ETA: 6:49 - loss: 0.7069 - acc: 0.5137
 576/4566 [==>...........................] - ETA: 7:17 - loss: 0.7029 - acc: 0.5226
 640/4566 [===>..........................] - ETA: 7:44 - loss: 0.7010 - acc: 0.5297
 704/4566 [===>..........................] - ETA: 8:05 - loss: 0.6979 - acc: 0.5298
 768/4566 [====>.........................] - ETA: 8:18 - loss: 0.6979 - acc: 0.5312
 832/4566 [====>.........................] - ETA: 8:08 - loss: 0.6949 - acc: 0.5373
 896/4566 [====>.........................] - ETA: 7:48 - loss: 0.6933 - acc: 0.5402
 960/4566 [=====>........................] - ETA: 7:29 - loss: 0.6909 - acc: 0.5458
1024/4566 [=====>........................] - ETA: 7:10 - loss: 0.6899 - acc: 0.5488
1088/4566 [======>.......................] - ETA: 6:52 - loss: 0.6886 - acc: 0.5551
1152/4566 [======>.......................] - ETA: 6:34 - loss: 0.6876 - acc: 0.5599
1216/4566 [======>.......................] - ETA: 6:18 - loss: 0.6864 - acc: 0.5625
1280/4566 [=======>......................] - ETA: 6:04 - loss: 0.6846 - acc: 0.5672
1344/4566 [=======>......................] - ETA: 5:51 - loss: 0.6831 - acc: 0.5685
1408/4566 [========>.....................] - ETA: 5:39 - loss: 0.6834 - acc: 0.5696
1472/4566 [========>.....................] - ETA: 5:26 - loss: 0.6847 - acc: 0.5652
1536/4566 [=========>....................] - ETA: 5:15 - loss: 0.6844 - acc: 0.5664
1600/4566 [=========>....................] - ETA: 5:05 - loss: 0.6838 - acc: 0.5675
1664/4566 [=========>....................] - ETA: 4:55 - loss: 0.6839 - acc: 0.5673
1728/4566 [==========>...................] - ETA: 4:45 - loss: 0.6840 - acc: 0.5689
1792/4566 [==========>...................] - ETA: 4:35 - loss: 0.6844 - acc: 0.5692
1856/4566 [===========>..................] - ETA: 4:27 - loss: 0.6838 - acc: 0.5717
1920/4566 [===========>..................] - ETA: 4:18 - loss: 0.6855 - acc: 0.5677
1984/4566 [============>.................] - ETA: 4:16 - loss: 0.6833 - acc: 0.5721
2048/4566 [============>.................] - ETA: 4:16 - loss: 0.6842 - acc: 0.5698
2112/4566 [============>.................] - ETA: 4:16 - loss: 0.6832 - acc: 0.5715
2176/4566 [=============>................] - ETA: 4:15 - loss: 0.6838 - acc: 0.5694
2240/4566 [=============>................] - ETA: 4:14 - loss: 0.6839 - acc: 0.5696
2304/4566 [==============>...............] - ETA: 4:12 - loss: 0.6839 - acc: 0.5694
2368/4566 [==============>...............] - ETA: 4:09 - loss: 0.6834 - acc: 0.5701
2432/4566 [==============>...............] - ETA: 4:00 - loss: 0.6818 - acc: 0.5728
2496/4566 [===============>..............] - ETA: 3:52 - loss: 0.6814 - acc: 0.5729
2560/4566 [===============>..............] - ETA: 3:43 - loss: 0.6815 - acc: 0.5723
2624/4566 [================>.............] - ETA: 3:34 - loss: 0.6808 - acc: 0.5728
2688/4566 [================>.............] - ETA: 3:25 - loss: 0.6794 - acc: 0.5729
2752/4566 [=================>............] - ETA: 3:16 - loss: 0.6798 - acc: 0.5716
2816/4566 [=================>............] - ETA: 3:07 - loss: 0.6794 - acc: 0.5717
2880/4566 [=================>............] - ETA: 2:59 - loss: 0.6803 - acc: 0.5715
2944/4566 [==================>...........] - ETA: 2:51 - loss: 0.6804 - acc: 0.5724
3008/4566 [==================>...........] - ETA: 2:43 - loss: 0.6788 - acc: 0.5755
3072/4566 [===================>..........] - ETA: 2:35 - loss: 0.6788 - acc: 0.5749
3136/4566 [===================>..........] - ETA: 2:27 - loss: 0.6788 - acc: 0.5749
3200/4566 [====================>.........] - ETA: 2:19 - loss: 0.6795 - acc: 0.5737
3264/4566 [====================>.........] - ETA: 2:12 - loss: 0.6796 - acc: 0.5738
3328/4566 [====================>.........] - ETA: 2:04 - loss: 0.6792 - acc: 0.5739
3392/4566 [=====================>........] - ETA: 1:58 - loss: 0.6790 - acc: 0.5755
3456/4566 [=====================>........] - ETA: 1:50 - loss: 0.6800 - acc: 0.5729
3520/4566 [======================>.......] - ETA: 1:44 - loss: 0.6803 - acc: 0.5722
3584/4566 [======================>.......] - ETA: 1:37 - loss: 0.6794 - acc: 0.5748
3648/4566 [======================>.......] - ETA: 1:33 - loss: 0.6795 - acc: 0.5746
3712/4566 [=======================>......] - ETA: 1:27 - loss: 0.6801 - acc: 0.5744
3776/4566 [=======================>......] - ETA: 1:22 - loss: 0.6801 - acc: 0.5742
3840/4566 [========================>.....] - ETA: 1:17 - loss: 0.6800 - acc: 0.5745
3904/4566 [========================>.....] - ETA: 1:12 - loss: 0.6794 - acc: 0.5756
3968/4566 [=========================>....] - ETA: 1:06 - loss: 0.6792 - acc: 0.5764
4032/4566 [=========================>....] - ETA: 59s - loss: 0.6792 - acc: 0.5764 
4096/4566 [=========================>....] - ETA: 52s - loss: 0.6787 - acc: 0.5771
4160/4566 [==========================>...] - ETA: 44s - loss: 0.6793 - acc: 0.5760
4224/4566 [==========================>...] - ETA: 37s - loss: 0.6791 - acc: 0.5765
4288/4566 [===========================>..] - ETA: 30s - loss: 0.6790 - acc: 0.5763
4352/4566 [===========================>..] - ETA: 23s - loss: 0.6783 - acc: 0.5781
4416/4566 [============================>.] - ETA: 16s - loss: 0.6780 - acc: 0.5781
4480/4566 [============================>.] - ETA: 9s - loss: 0.6777 - acc: 0.5788 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6777 - acc: 0.5786
4566/4566 [==============================] - 501s 110ms/step - loss: 0.6775 - acc: 0.5791 - val_loss: 0.6643 - val_acc: 0.5945

Epoch 00008: val_acc did not improve from 0.61024
Epoch 9/10

  64/4566 [..............................] - ETA: 5:25 - loss: 0.7148 - acc: 0.5469
 128/4566 [..............................] - ETA: 5:02 - loss: 0.7075 - acc: 0.5625
 192/4566 [>.............................] - ETA: 5:11 - loss: 0.7039 - acc: 0.5365
 256/4566 [>.............................] - ETA: 5:07 - loss: 0.6915 - acc: 0.5625
 320/4566 [=>............................] - ETA: 5:02 - loss: 0.6839 - acc: 0.5625
 384/4566 [=>............................] - ETA: 5:37 - loss: 0.6838 - acc: 0.5495
 448/4566 [=>............................] - ETA: 6:33 - loss: 0.6798 - acc: 0.5603
 512/4566 [==>...........................] - ETA: 7:08 - loss: 0.6738 - acc: 0.5703
 576/4566 [==>...........................] - ETA: 7:40 - loss: 0.6761 - acc: 0.5747
 640/4566 [===>..........................] - ETA: 8:04 - loss: 0.6711 - acc: 0.5813
 704/4566 [===>..........................] - ETA: 8:19 - loss: 0.6737 - acc: 0.5753
 768/4566 [====>.........................] - ETA: 8:23 - loss: 0.6705 - acc: 0.5807
 832/4566 [====>.........................] - ETA: 8:07 - loss: 0.6719 - acc: 0.5757
 896/4566 [====>.........................] - ETA: 7:47 - loss: 0.6760 - acc: 0.5714
 960/4566 [=====>........................] - ETA: 7:27 - loss: 0.6747 - acc: 0.5719
1024/4566 [=====>........................] - ETA: 7:07 - loss: 0.6750 - acc: 0.5713
1088/4566 [======>.......................] - ETA: 6:48 - loss: 0.6749 - acc: 0.5680
1152/4566 [======>.......................] - ETA: 6:31 - loss: 0.6741 - acc: 0.5703
1216/4566 [======>.......................] - ETA: 6:15 - loss: 0.6742 - acc: 0.5715
1280/4566 [=======>......................] - ETA: 6:01 - loss: 0.6731 - acc: 0.5719
1344/4566 [=======>......................] - ETA: 5:48 - loss: 0.6728 - acc: 0.5744
1408/4566 [========>.....................] - ETA: 5:36 - loss: 0.6735 - acc: 0.5703
1472/4566 [========>.....................] - ETA: 5:26 - loss: 0.6730 - acc: 0.5713
1536/4566 [=========>....................] - ETA: 5:15 - loss: 0.6722 - acc: 0.5716
1600/4566 [=========>....................] - ETA: 5:05 - loss: 0.6731 - acc: 0.5694
1664/4566 [=========>....................] - ETA: 4:55 - loss: 0.6719 - acc: 0.5721
1728/4566 [==========>...................] - ETA: 4:46 - loss: 0.6721 - acc: 0.5729
1792/4566 [==========>...................] - ETA: 4:38 - loss: 0.6719 - acc: 0.5737
1856/4566 [===========>..................] - ETA: 4:28 - loss: 0.6715 - acc: 0.5754
1920/4566 [===========>..................] - ETA: 4:19 - loss: 0.6722 - acc: 0.5745
1984/4566 [============>.................] - ETA: 4:17 - loss: 0.6722 - acc: 0.5761
2048/4566 [============>.................] - ETA: 4:17 - loss: 0.6725 - acc: 0.5762
2112/4566 [============>.................] - ETA: 4:17 - loss: 0.6724 - acc: 0.5786
2176/4566 [=============>................] - ETA: 4:16 - loss: 0.6711 - acc: 0.5827
2240/4566 [=============>................] - ETA: 4:15 - loss: 0.6715 - acc: 0.5813
2304/4566 [==============>...............] - ETA: 4:13 - loss: 0.6719 - acc: 0.5816
2368/4566 [==============>...............] - ETA: 4:09 - loss: 0.6738 - acc: 0.5781
2432/4566 [==============>...............] - ETA: 4:01 - loss: 0.6727 - acc: 0.5806
2496/4566 [===============>..............] - ETA: 3:51 - loss: 0.6737 - acc: 0.5789
2560/4566 [===============>..............] - ETA: 3:42 - loss: 0.6735 - acc: 0.5793
2624/4566 [================>.............] - ETA: 3:32 - loss: 0.6731 - acc: 0.5808
2688/4566 [================>.............] - ETA: 3:24 - loss: 0.6729 - acc: 0.5811
2752/4566 [=================>............] - ETA: 3:14 - loss: 0.6729 - acc: 0.5810
2816/4566 [=================>............] - ETA: 3:06 - loss: 0.6735 - acc: 0.5788
2880/4566 [=================>............] - ETA: 2:58 - loss: 0.6725 - acc: 0.5806
2944/4566 [==================>...........] - ETA: 2:49 - loss: 0.6724 - acc: 0.5808
3008/4566 [==================>...........] - ETA: 2:42 - loss: 0.6735 - acc: 0.5788
3072/4566 [===================>..........] - ETA: 2:33 - loss: 0.6737 - acc: 0.5801
3136/4566 [===================>..........] - ETA: 2:26 - loss: 0.6736 - acc: 0.5797
3200/4566 [====================>.........] - ETA: 2:19 - loss: 0.6730 - acc: 0.5822
3264/4566 [====================>.........] - ETA: 2:11 - loss: 0.6725 - acc: 0.5836
3328/4566 [====================>.........] - ETA: 2:04 - loss: 0.6726 - acc: 0.5847
3392/4566 [=====================>........] - ETA: 1:57 - loss: 0.6728 - acc: 0.5852
3456/4566 [=====================>........] - ETA: 1:50 - loss: 0.6731 - acc: 0.5856
3520/4566 [======================>.......] - ETA: 1:43 - loss: 0.6726 - acc: 0.5866
3584/4566 [======================>.......] - ETA: 1:36 - loss: 0.6717 - acc: 0.5890
3648/4566 [======================>.......] - ETA: 1:31 - loss: 0.6714 - acc: 0.5894
3712/4566 [=======================>......] - ETA: 1:26 - loss: 0.6712 - acc: 0.5889
3776/4566 [=======================>......] - ETA: 1:21 - loss: 0.6713 - acc: 0.5877
3840/4566 [========================>.....] - ETA: 1:16 - loss: 0.6721 - acc: 0.5865
3904/4566 [========================>.....] - ETA: 1:10 - loss: 0.6724 - acc: 0.5848
3968/4566 [=========================>....] - ETA: 1:04 - loss: 0.6730 - acc: 0.5842
4032/4566 [=========================>....] - ETA: 57s - loss: 0.6730 - acc: 0.5838 
4096/4566 [=========================>....] - ETA: 50s - loss: 0.6729 - acc: 0.5840
4160/4566 [==========================>...] - ETA: 43s - loss: 0.6727 - acc: 0.5841
4224/4566 [==========================>...] - ETA: 36s - loss: 0.6732 - acc: 0.5829
4288/4566 [===========================>..] - ETA: 29s - loss: 0.6737 - acc: 0.5823
4352/4566 [===========================>..] - ETA: 22s - loss: 0.6734 - acc: 0.5820
4416/4566 [============================>.] - ETA: 15s - loss: 0.6733 - acc: 0.5829
4480/4566 [============================>.] - ETA: 8s - loss: 0.6735 - acc: 0.5819 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6733 - acc: 0.5832
4566/4566 [==============================] - 482s 105ms/step - loss: 0.6732 - acc: 0.5830 - val_loss: 0.6729 - val_acc: 0.6024

Epoch 00009: val_acc did not improve from 0.61024
Epoch 10/10

  64/4566 [..............................] - ETA: 5:12 - loss: 0.6704 - acc: 0.5938
 128/4566 [..............................] - ETA: 4:56 - loss: 0.6782 - acc: 0.5938
 192/4566 [>.............................] - ETA: 5:00 - loss: 0.6593 - acc: 0.6198
 256/4566 [>.............................] - ETA: 4:46 - loss: 0.6595 - acc: 0.6250
 320/4566 [=>............................] - ETA: 4:41 - loss: 0.6650 - acc: 0.6094
 384/4566 [=>............................] - ETA: 4:31 - loss: 0.6752 - acc: 0.5859
 448/4566 [=>............................] - ETA: 4:26 - loss: 0.6705 - acc: 0.5960
 512/4566 [==>...........................] - ETA: 4:49 - loss: 0.6703 - acc: 0.5977
 576/4566 [==>...........................] - ETA: 5:40 - loss: 0.6743 - acc: 0.5885
 640/4566 [===>..........................] - ETA: 6:15 - loss: 0.6727 - acc: 0.5906
 704/4566 [===>..........................] - ETA: 6:41 - loss: 0.6713 - acc: 0.5881
 768/4566 [====>.........................] - ETA: 7:02 - loss: 0.6738 - acc: 0.5846
 832/4566 [====>.........................] - ETA: 7:13 - loss: 0.6737 - acc: 0.5865
 896/4566 [====>.........................] - ETA: 7:20 - loss: 0.6723 - acc: 0.5882
 960/4566 [=====>........................] - ETA: 7:01 - loss: 0.6705 - acc: 0.5927
1024/4566 [=====>........................] - ETA: 6:41 - loss: 0.6694 - acc: 0.5967
1088/4566 [======>.......................] - ETA: 6:23 - loss: 0.6727 - acc: 0.5901
1152/4566 [======>.......................] - ETA: 6:07 - loss: 0.6697 - acc: 0.5964
1216/4566 [======>.......................] - ETA: 5:51 - loss: 0.6680 - acc: 0.6020
1280/4566 [=======>......................] - ETA: 5:36 - loss: 0.6684 - acc: 0.6023
1344/4566 [=======>......................] - ETA: 5:24 - loss: 0.6677 - acc: 0.6042
1408/4566 [========>.....................] - ETA: 5:11 - loss: 0.6653 - acc: 0.6087
1472/4566 [========>.....................] - ETA: 4:59 - loss: 0.6653 - acc: 0.6080
1536/4566 [=========>....................] - ETA: 4:49 - loss: 0.6641 - acc: 0.6068
1600/4566 [=========>....................] - ETA: 4:39 - loss: 0.6660 - acc: 0.6019
1664/4566 [=========>....................] - ETA: 4:28 - loss: 0.6662 - acc: 0.6004
1728/4566 [==========>...................] - ETA: 4:18 - loss: 0.6695 - acc: 0.5943
1792/4566 [==========>...................] - ETA: 4:09 - loss: 0.6689 - acc: 0.5960
1856/4566 [===========>..................] - ETA: 4:00 - loss: 0.6664 - acc: 0.6008
1920/4566 [===========>..................] - ETA: 3:51 - loss: 0.6656 - acc: 0.6026
1984/4566 [============>.................] - ETA: 3:43 - loss: 0.6662 - acc: 0.6018
2048/4566 [============>.................] - ETA: 3:35 - loss: 0.6667 - acc: 0.6006
2112/4566 [============>.................] - ETA: 3:28 - loss: 0.6665 - acc: 0.6004
2176/4566 [=============>................] - ETA: 3:20 - loss: 0.6657 - acc: 0.6006
2240/4566 [=============>................] - ETA: 3:18 - loss: 0.6655 - acc: 0.6013
2304/4566 [==============>...............] - ETA: 3:20 - loss: 0.6648 - acc: 0.6011
2368/4566 [==============>...............] - ETA: 3:20 - loss: 0.6638 - acc: 0.6030
2432/4566 [==============>...............] - ETA: 3:19 - loss: 0.6663 - acc: 0.5987
2496/4566 [===============>..............] - ETA: 3:18 - loss: 0.6664 - acc: 0.5982
2560/4566 [===============>..............] - ETA: 3:16 - loss: 0.6664 - acc: 0.5996
2624/4566 [================>.............] - ETA: 3:12 - loss: 0.6671 - acc: 0.5979
2688/4566 [================>.............] - ETA: 3:04 - loss: 0.6674 - acc: 0.5975
2752/4566 [=================>............] - ETA: 2:56 - loss: 0.6683 - acc: 0.5956
2816/4566 [=================>............] - ETA: 2:48 - loss: 0.6686 - acc: 0.5941
2880/4566 [=================>............] - ETA: 2:40 - loss: 0.6688 - acc: 0.5934
2944/4566 [==================>...........] - ETA: 2:33 - loss: 0.6703 - acc: 0.5910
3008/4566 [==================>...........] - ETA: 2:25 - loss: 0.6700 - acc: 0.5914
3072/4566 [===================>..........] - ETA: 2:18 - loss: 0.6697 - acc: 0.5918
3136/4566 [===================>..........] - ETA: 2:11 - loss: 0.6693 - acc: 0.5928
3200/4566 [====================>.........] - ETA: 2:04 - loss: 0.6703 - acc: 0.5916
3264/4566 [====================>.........] - ETA: 1:57 - loss: 0.6699 - acc: 0.5925
3328/4566 [====================>.........] - ETA: 1:51 - loss: 0.6701 - acc: 0.5922
3392/4566 [=====================>........] - ETA: 1:44 - loss: 0.6707 - acc: 0.5914
3456/4566 [=====================>........] - ETA: 1:38 - loss: 0.6712 - acc: 0.5906
3520/4566 [======================>.......] - ETA: 1:31 - loss: 0.6711 - acc: 0.5901
3584/4566 [======================>.......] - ETA: 1:25 - loss: 0.6708 - acc: 0.5904
3648/4566 [======================>.......] - ETA: 1:19 - loss: 0.6712 - acc: 0.5891
3712/4566 [=======================>......] - ETA: 1:13 - loss: 0.6711 - acc: 0.5894
3776/4566 [=======================>......] - ETA: 1:07 - loss: 0.6710 - acc: 0.5906
3840/4566 [========================>.....] - ETA: 1:01 - loss: 0.6714 - acc: 0.5901
3904/4566 [========================>.....] - ETA: 55s - loss: 0.6718 - acc: 0.5884 
3968/4566 [=========================>....] - ETA: 50s - loss: 0.6712 - acc: 0.5905
4032/4566 [=========================>....] - ETA: 46s - loss: 0.6713 - acc: 0.5895
4096/4566 [=========================>....] - ETA: 41s - loss: 0.6717 - acc: 0.5881
4160/4566 [==========================>...] - ETA: 36s - loss: 0.6717 - acc: 0.5889
4224/4566 [==========================>...] - ETA: 31s - loss: 0.6720 - acc: 0.5885
4288/4566 [===========================>..] - ETA: 25s - loss: 0.6723 - acc: 0.5872
4352/4566 [===========================>..] - ETA: 20s - loss: 0.6721 - acc: 0.5871
4416/4566 [============================>.] - ETA: 13s - loss: 0.6723 - acc: 0.5872
4480/4566 [============================>.] - ETA: 7s - loss: 0.6720 - acc: 0.5882 
4544/4566 [============================>.] - ETA: 2s - loss: 0.6724 - acc: 0.5882
4566/4566 [==============================] - 430s 94ms/step - loss: 0.6726 - acc: 0.5878 - val_loss: 0.6639 - val_acc: 0.6063

Epoch 00010: val_acc did not improve from 0.61024
Saved model to disk
