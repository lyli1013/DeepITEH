nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2537
样本个数 5074
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7ff6299e4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7ff6299e4710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7ff67f815910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7ff67f815910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f905610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f905610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff62999bad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff62999bad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6298cbf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6298cbf90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff629910690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff629910690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff629910a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff629910a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff687de1390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff687de1390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff67f8cc790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff67f8cc790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6295d9450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6295d9450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff62971edd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff62971edd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff629942590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff629942590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff629906290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff629906290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff629642cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff629642cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6292fffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6292fffd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff629360e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff629360e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff6293dea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff6293dea10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff629617490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff629617490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff6292fffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff6292fffd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff629337a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff629337a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628ebd190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628ebd190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff6292d3390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff6292d3390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff629729f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff629729f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff628e8a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff628e8a710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff628e04610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff628e04610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628fec610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628fec610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff628df8f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff628df8f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628c86c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628c86c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff628ad6a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff628ad6a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff628958e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff628958e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628a2e7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628a2e7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff628ad60d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff628ad60d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628859110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628859110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff6287fe290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff6287fe290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff628999a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff628999a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f883690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f883690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff6289f5f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff6289f5f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628d897d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628d897d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff6284bd390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff6284bd390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6283129d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6283129d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff6283eb2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff6283eb2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff6284bd1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff6284bd1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff6201acc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff6201acc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff62015ced0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff62015ced0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff620031f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff620031f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628649b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628649b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff628759250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff628759250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff62011cb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff62011cb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff61fe285d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff61fe285d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff61fe0cdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff61fe0cdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff62000bc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff62000bc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff61fe28910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff61fe28910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628407590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff628407590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff61fbc9b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff61fbc9b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6179e2e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6179e2e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff6178e3bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff6178e3bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff629944190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff629944190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff61fb0fd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff61fb0fd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff61fda6a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff61fda6a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6176d75d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff6176d75d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff6176851d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff6176851d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff61fb5c490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff61fb5c490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff617a2eb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff617a2eb10>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-18 22:16:20.757757: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-18 22:16:21.122476: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-18 22:16:21.405425: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a72a1e8ea0 executing computations on platform Host. Devices:
2022-11-18 22:16:21.405719: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-18 22:16:23.896943: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window18.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4566 samples, validate on 508 samples
Epoch 1/10

  64/4566 [..............................] - ETA: 1:01:25 - loss: 0.7184 - acc: 0.5312
 128/4566 [..............................] - ETA: 54:48 - loss: 0.7182 - acc: 0.5469  
 192/4566 [>.............................] - ETA: 41:09 - loss: 0.7506 - acc: 0.4792
 256/4566 [>.............................] - ETA: 33:31 - loss: 0.7332 - acc: 0.4961
 320/4566 [=>............................] - ETA: 28:30 - loss: 0.7291 - acc: 0.5125
 384/4566 [=>............................] - ETA: 25:20 - loss: 0.7410 - acc: 0.5104
 448/4566 [=>............................] - ETA: 22:56 - loss: 0.7420 - acc: 0.5134
 512/4566 [==>...........................] - ETA: 20:57 - loss: 0.7402 - acc: 0.5195
 576/4566 [==>...........................] - ETA: 19:24 - loss: 0.7378 - acc: 0.5226
 640/4566 [===>..........................] - ETA: 18:01 - loss: 0.7320 - acc: 0.5250
 704/4566 [===>..........................] - ETA: 16:51 - loss: 0.7318 - acc: 0.5256
 768/4566 [====>.........................] - ETA: 15:50 - loss: 0.7332 - acc: 0.5221
 832/4566 [====>.........................] - ETA: 14:59 - loss: 0.7325 - acc: 0.5204
 896/4566 [====>.........................] - ETA: 14:27 - loss: 0.7335 - acc: 0.5223
 960/4566 [=====>........................] - ETA: 14:26 - loss: 0.7300 - acc: 0.5260
1024/4566 [=====>........................] - ETA: 14:24 - loss: 0.7257 - acc: 0.5322
1088/4566 [======>.......................] - ETA: 14:14 - loss: 0.7278 - acc: 0.5276
1152/4566 [======>.......................] - ETA: 14:02 - loss: 0.7263 - acc: 0.5243
1216/4566 [======>.......................] - ETA: 13:43 - loss: 0.7260 - acc: 0.5230
1280/4566 [=======>......................] - ETA: 13:25 - loss: 0.7291 - acc: 0.5211
1344/4566 [=======>......................] - ETA: 12:52 - loss: 0.7295 - acc: 0.5186
1408/4566 [========>.....................] - ETA: 12:20 - loss: 0.7282 - acc: 0.5192
1472/4566 [========>.....................] - ETA: 11:50 - loss: 0.7277 - acc: 0.5204
1536/4566 [=========>....................] - ETA: 11:26 - loss: 0.7273 - acc: 0.5221
1600/4566 [=========>....................] - ETA: 11:03 - loss: 0.7257 - acc: 0.5250
1664/4566 [=========>....................] - ETA: 10:41 - loss: 0.7254 - acc: 0.5252
1728/4566 [==========>...................] - ETA: 10:17 - loss: 0.7252 - acc: 0.5260
1792/4566 [==========>...................] - ETA: 9:52 - loss: 0.7250 - acc: 0.5257 
1856/4566 [===========>..................] - ETA: 9:28 - loss: 0.7235 - acc: 0.5269
1920/4566 [===========>..................] - ETA: 9:05 - loss: 0.7228 - acc: 0.5281
1984/4566 [============>.................] - ETA: 8:45 - loss: 0.7219 - acc: 0.5282
2048/4566 [============>.................] - ETA: 8:26 - loss: 0.7210 - acc: 0.5273
2112/4566 [============>.................] - ETA: 8:08 - loss: 0.7201 - acc: 0.5275
2176/4566 [=============>................] - ETA: 7:51 - loss: 0.7201 - acc: 0.5262
2240/4566 [=============>................] - ETA: 7:41 - loss: 0.7206 - acc: 0.5246
2304/4566 [==============>...............] - ETA: 7:30 - loss: 0.7209 - acc: 0.5247
2368/4566 [==============>...............] - ETA: 7:20 - loss: 0.7203 - acc: 0.5270
2432/4566 [==============>...............] - ETA: 7:10 - loss: 0.7189 - acc: 0.5280
2496/4566 [===============>..............] - ETA: 6:59 - loss: 0.7174 - acc: 0.5296
2560/4566 [===============>..............] - ETA: 6:48 - loss: 0.7165 - acc: 0.5301
2624/4566 [================>.............] - ETA: 6:36 - loss: 0.7171 - acc: 0.5278
2688/4566 [================>.............] - ETA: 6:19 - loss: 0.7171 - acc: 0.5279
2752/4566 [=================>............] - ETA: 6:03 - loss: 0.7175 - acc: 0.5265
2816/4566 [=================>............] - ETA: 5:47 - loss: 0.7170 - acc: 0.5256
2880/4566 [=================>............] - ETA: 5:31 - loss: 0.7168 - acc: 0.5250
2944/4566 [==================>...........] - ETA: 5:16 - loss: 0.7169 - acc: 0.5231
3008/4566 [==================>...........] - ETA: 5:01 - loss: 0.7169 - acc: 0.5229
3072/4566 [===================>..........] - ETA: 4:46 - loss: 0.7171 - acc: 0.5221
3136/4566 [===================>..........] - ETA: 4:32 - loss: 0.7171 - acc: 0.5210
3200/4566 [====================>.........] - ETA: 4:18 - loss: 0.7169 - acc: 0.5203
3264/4566 [====================>.........] - ETA: 4:05 - loss: 0.7169 - acc: 0.5196
3328/4566 [====================>.........] - ETA: 3:51 - loss: 0.7172 - acc: 0.5192
3392/4566 [=====================>........] - ETA: 3:38 - loss: 0.7167 - acc: 0.5209
3456/4566 [=====================>........] - ETA: 3:24 - loss: 0.7165 - acc: 0.5208
3520/4566 [======================>.......] - ETA: 3:11 - loss: 0.7159 - acc: 0.5219
3584/4566 [======================>.......] - ETA: 3:00 - loss: 0.7151 - acc: 0.5223
3648/4566 [======================>.......] - ETA: 2:49 - loss: 0.7141 - acc: 0.5236
3712/4566 [=======================>......] - ETA: 2:38 - loss: 0.7140 - acc: 0.5242
3776/4566 [=======================>......] - ETA: 2:27 - loss: 0.7140 - acc: 0.5238
3840/4566 [========================>.....] - ETA: 2:15 - loss: 0.7138 - acc: 0.5240
3904/4566 [========================>.....] - ETA: 2:04 - loss: 0.7136 - acc: 0.5243
3968/4566 [=========================>....] - ETA: 1:52 - loss: 0.7134 - acc: 0.5239
4032/4566 [=========================>....] - ETA: 1:40 - loss: 0.7125 - acc: 0.5248
4096/4566 [=========================>....] - ETA: 1:27 - loss: 0.7126 - acc: 0.5244
4160/4566 [==========================>...] - ETA: 1:15 - loss: 0.7127 - acc: 0.5236
4224/4566 [==========================>...] - ETA: 1:02 - loss: 0.7129 - acc: 0.5220
4288/4566 [===========================>..] - ETA: 50s - loss: 0.7127 - acc: 0.5226 
4352/4566 [===========================>..] - ETA: 38s - loss: 0.7130 - acc: 0.5230
4416/4566 [============================>.] - ETA: 27s - loss: 0.7124 - acc: 0.5233
4480/4566 [============================>.] - ETA: 15s - loss: 0.7120 - acc: 0.5237
4544/4566 [============================>.] - ETA: 3s - loss: 0.7118 - acc: 0.5233 
4566/4566 [==============================] - 844s 185ms/step - loss: 0.7116 - acc: 0.5239 - val_loss: 0.6812 - val_acc: 0.5650

Epoch 00001: val_acc improved from -inf to 0.56496, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window18/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 2/10

  64/4566 [..............................] - ETA: 9:09 - loss: 0.7222 - acc: 0.4531
 128/4566 [..............................] - ETA: 11:20 - loss: 0.7074 - acc: 0.4922
 192/4566 [>.............................] - ETA: 13:16 - loss: 0.6969 - acc: 0.4948
 256/4566 [>.............................] - ETA: 14:00 - loss: 0.6952 - acc: 0.5234
 320/4566 [=>............................] - ETA: 14:10 - loss: 0.7039 - acc: 0.5062
 384/4566 [=>............................] - ETA: 14:11 - loss: 0.6999 - acc: 0.5156
 448/4566 [=>............................] - ETA: 14:05 - loss: 0.6970 - acc: 0.5290
 512/4566 [==>...........................] - ETA: 13:47 - loss: 0.7020 - acc: 0.5215
 576/4566 [==>...........................] - ETA: 13:00 - loss: 0.6995 - acc: 0.5243
 640/4566 [===>..........................] - ETA: 12:15 - loss: 0.7004 - acc: 0.5312
 704/4566 [===>..........................] - ETA: 11:40 - loss: 0.7022 - acc: 0.5327
 768/4566 [====>.........................] - ETA: 11:08 - loss: 0.7021 - acc: 0.5312
 832/4566 [====>.........................] - ETA: 10:44 - loss: 0.6994 - acc: 0.5349
 896/4566 [====>.........................] - ETA: 10:19 - loss: 0.6972 - acc: 0.5379
 960/4566 [=====>........................] - ETA: 9:55 - loss: 0.6948 - acc: 0.5427 
1024/4566 [=====>........................] - ETA: 9:35 - loss: 0.6977 - acc: 0.5381
1088/4566 [======>.......................] - ETA: 9:14 - loss: 0.6993 - acc: 0.5349
1152/4566 [======>.......................] - ETA: 8:56 - loss: 0.6979 - acc: 0.5365
1216/4566 [======>.......................] - ETA: 8:38 - loss: 0.6979 - acc: 0.5387
1280/4566 [=======>......................] - ETA: 8:22 - loss: 0.6977 - acc: 0.5359
1344/4566 [=======>......................] - ETA: 8:07 - loss: 0.6975 - acc: 0.5335
1408/4566 [========>.....................] - ETA: 7:51 - loss: 0.7003 - acc: 0.5284
1472/4566 [========>.....................] - ETA: 7:43 - loss: 0.7036 - acc: 0.5258
1536/4566 [=========>....................] - ETA: 7:43 - loss: 0.7053 - acc: 0.5241
1600/4566 [=========>....................] - ETA: 7:40 - loss: 0.7069 - acc: 0.5219
1664/4566 [=========>....................] - ETA: 7:40 - loss: 0.7074 - acc: 0.5180
1728/4566 [==========>...................] - ETA: 7:37 - loss: 0.7056 - acc: 0.5191
1792/4566 [==========>...................] - ETA: 7:35 - loss: 0.7053 - acc: 0.5184
1856/4566 [===========>..................] - ETA: 7:28 - loss: 0.7041 - acc: 0.5216
1920/4566 [===========>..................] - ETA: 7:16 - loss: 0.7025 - acc: 0.5250
1984/4566 [============>.................] - ETA: 7:02 - loss: 0.7024 - acc: 0.5252
2048/4566 [============>.................] - ETA: 6:48 - loss: 0.7029 - acc: 0.5220
2112/4566 [============>.................] - ETA: 6:35 - loss: 0.7018 - acc: 0.5246
2176/4566 [=============>................] - ETA: 6:21 - loss: 0.7013 - acc: 0.5257
2240/4566 [=============>................] - ETA: 6:08 - loss: 0.7004 - acc: 0.5290
2304/4566 [==============>...............] - ETA: 5:55 - loss: 0.7005 - acc: 0.5269
2368/4566 [==============>...............] - ETA: 5:43 - loss: 0.7001 - acc: 0.5279
2432/4566 [==============>...............] - ETA: 5:30 - loss: 0.7006 - acc: 0.5296
2496/4566 [===============>..............] - ETA: 5:17 - loss: 0.7014 - acc: 0.5292
2560/4566 [===============>..............] - ETA: 5:05 - loss: 0.7014 - acc: 0.5297
2624/4566 [================>.............] - ETA: 4:52 - loss: 0.7002 - acc: 0.5309
2688/4566 [================>.............] - ETA: 4:41 - loss: 0.6997 - acc: 0.5312
2752/4566 [=================>............] - ETA: 4:29 - loss: 0.6997 - acc: 0.5302
2816/4566 [=================>............] - ETA: 4:19 - loss: 0.6992 - acc: 0.5291
2880/4566 [=================>............] - ETA: 4:11 - loss: 0.6993 - acc: 0.5281
2944/4566 [==================>...........] - ETA: 4:04 - loss: 0.6993 - acc: 0.5282
3008/4566 [==================>...........] - ETA: 3:57 - loss: 0.6999 - acc: 0.5276
3072/4566 [===================>..........] - ETA: 3:50 - loss: 0.6997 - acc: 0.5264
3136/4566 [===================>..........] - ETA: 3:42 - loss: 0.6992 - acc: 0.5268
3200/4566 [====================>.........] - ETA: 3:34 - loss: 0.6987 - acc: 0.5288
3264/4566 [====================>.........] - ETA: 3:26 - loss: 0.6989 - acc: 0.5291
3328/4566 [====================>.........] - ETA: 3:15 - loss: 0.6995 - acc: 0.5282
3392/4566 [=====================>........] - ETA: 3:04 - loss: 0.6990 - acc: 0.5280
3456/4566 [=====================>........] - ETA: 2:54 - loss: 0.6986 - acc: 0.5289
3520/4566 [======================>.......] - ETA: 2:43 - loss: 0.6988 - acc: 0.5276
3584/4566 [======================>.......] - ETA: 2:32 - loss: 0.6989 - acc: 0.5271
3648/4566 [======================>.......] - ETA: 2:21 - loss: 0.6981 - acc: 0.5293
3712/4566 [=======================>......] - ETA: 2:11 - loss: 0.6981 - acc: 0.5302
3776/4566 [=======================>......] - ETA: 2:00 - loss: 0.6986 - acc: 0.5289
3840/4566 [========================>.....] - ETA: 1:50 - loss: 0.6984 - acc: 0.5281
3904/4566 [========================>.....] - ETA: 1:40 - loss: 0.6980 - acc: 0.5300
3968/4566 [=========================>....] - ETA: 1:30 - loss: 0.6985 - acc: 0.5295
4032/4566 [=========================>....] - ETA: 1:20 - loss: 0.6992 - acc: 0.5288
4096/4566 [=========================>....] - ETA: 1:10 - loss: 0.6985 - acc: 0.5308
4160/4566 [==========================>...] - ETA: 1:00 - loss: 0.6988 - acc: 0.5303
4224/4566 [==========================>...] - ETA: 51s - loss: 0.6986 - acc: 0.5315 
4288/4566 [===========================>..] - ETA: 41s - loss: 0.6988 - acc: 0.5315
4352/4566 [===========================>..] - ETA: 32s - loss: 0.6987 - acc: 0.5319
4416/4566 [============================>.] - ETA: 22s - loss: 0.6995 - acc: 0.5306
4480/4566 [============================>.] - ETA: 13s - loss: 0.7000 - acc: 0.5297
4544/4566 [============================>.] - ETA: 3s - loss: 0.7000 - acc: 0.5306 
4566/4566 [==============================] - 741s 162ms/step - loss: 0.7000 - acc: 0.5307 - val_loss: 0.6811 - val_acc: 0.5610

Epoch 00002: val_acc did not improve from 0.56496
Epoch 3/10

  64/4566 [..............................] - ETA: 8:17 - loss: 0.7206 - acc: 0.4375
 128/4566 [..............................] - ETA: 7:58 - loss: 0.6981 - acc: 0.5234
 192/4566 [>.............................] - ETA: 7:45 - loss: 0.6916 - acc: 0.5521
 256/4566 [>.............................] - ETA: 7:35 - loss: 0.7091 - acc: 0.5234
 320/4566 [=>............................] - ETA: 7:40 - loss: 0.6979 - acc: 0.5500
 384/4566 [=>............................] - ETA: 7:44 - loss: 0.6974 - acc: 0.5469
 448/4566 [=>............................] - ETA: 7:41 - loss: 0.6987 - acc: 0.5402
 512/4566 [==>...........................] - ETA: 7:38 - loss: 0.6987 - acc: 0.5371
 576/4566 [==>...........................] - ETA: 7:31 - loss: 0.6974 - acc: 0.5312
 640/4566 [===>..........................] - ETA: 7:22 - loss: 0.6950 - acc: 0.5437
 704/4566 [===>..........................] - ETA: 7:19 - loss: 0.6952 - acc: 0.5440
 768/4566 [====>.........................] - ETA: 7:13 - loss: 0.6965 - acc: 0.5430
 832/4566 [====>.........................] - ETA: 7:07 - loss: 0.6953 - acc: 0.5457
 896/4566 [====>.........................] - ETA: 7:14 - loss: 0.6947 - acc: 0.5446
 960/4566 [=====>........................] - ETA: 7:32 - loss: 0.6932 - acc: 0.5500
1024/4566 [=====>........................] - ETA: 7:44 - loss: 0.6944 - acc: 0.5469
1088/4566 [======>.......................] - ETA: 7:53 - loss: 0.6942 - acc: 0.5515
1152/4566 [======>.......................] - ETA: 8:03 - loss: 0.6937 - acc: 0.5495
1216/4566 [======>.......................] - ETA: 8:08 - loss: 0.6923 - acc: 0.5526
1280/4566 [=======>......................] - ETA: 8:12 - loss: 0.6914 - acc: 0.5531
1344/4566 [=======>......................] - ETA: 8:03 - loss: 0.6920 - acc: 0.5513
1408/4566 [========>.....................] - ETA: 7:48 - loss: 0.6919 - acc: 0.5533
1472/4566 [========>.....................] - ETA: 7:34 - loss: 0.6896 - acc: 0.5571
1536/4566 [=========>....................] - ETA: 7:22 - loss: 0.6917 - acc: 0.5514
1600/4566 [=========>....................] - ETA: 7:09 - loss: 0.6915 - acc: 0.5544
1664/4566 [=========>....................] - ETA: 6:57 - loss: 0.6919 - acc: 0.5529
1728/4566 [==========>...................] - ETA: 6:45 - loss: 0.6919 - acc: 0.5521
1792/4566 [==========>...................] - ETA: 6:34 - loss: 0.6918 - acc: 0.5508
1856/4566 [===========>..................] - ETA: 6:22 - loss: 0.6915 - acc: 0.5528
1920/4566 [===========>..................] - ETA: 6:10 - loss: 0.6918 - acc: 0.5521
1984/4566 [============>.................] - ETA: 5:59 - loss: 0.6915 - acc: 0.5509
2048/4566 [============>.................] - ETA: 5:48 - loss: 0.6916 - acc: 0.5503
2112/4566 [============>.................] - ETA: 5:37 - loss: 0.6916 - acc: 0.5502
2176/4566 [=============>................] - ETA: 5:26 - loss: 0.6916 - acc: 0.5492
2240/4566 [=============>................] - ETA: 5:16 - loss: 0.6915 - acc: 0.5491
2304/4566 [==============>...............] - ETA: 5:09 - loss: 0.6924 - acc: 0.5451
2368/4566 [==============>...............] - ETA: 5:06 - loss: 0.6917 - acc: 0.5469
2432/4566 [==============>...............] - ETA: 5:02 - loss: 0.6920 - acc: 0.5465
2496/4566 [===============>..............] - ETA: 4:57 - loss: 0.6919 - acc: 0.5457
2560/4566 [===============>..............] - ETA: 4:52 - loss: 0.6907 - acc: 0.5477
2624/4566 [================>.............] - ETA: 4:46 - loss: 0.6907 - acc: 0.5488
2688/4566 [================>.............] - ETA: 4:40 - loss: 0.6920 - acc: 0.5472
2752/4566 [=================>............] - ETA: 4:30 - loss: 0.6927 - acc: 0.5454
2816/4566 [=================>............] - ETA: 4:19 - loss: 0.6921 - acc: 0.5458
2880/4566 [=================>............] - ETA: 4:08 - loss: 0.6925 - acc: 0.5441
2944/4566 [==================>...........] - ETA: 3:57 - loss: 0.6935 - acc: 0.5435
3008/4566 [==================>...........] - ETA: 3:46 - loss: 0.6940 - acc: 0.5439
3072/4566 [===================>..........] - ETA: 3:36 - loss: 0.6944 - acc: 0.5439
3136/4566 [===================>..........] - ETA: 3:26 - loss: 0.6953 - acc: 0.5418
3200/4566 [====================>.........] - ETA: 3:16 - loss: 0.6948 - acc: 0.5434
3264/4566 [====================>.........] - ETA: 3:06 - loss: 0.6953 - acc: 0.5423
3328/4566 [====================>.........] - ETA: 2:56 - loss: 0.6955 - acc: 0.5412
3392/4566 [=====================>........] - ETA: 2:46 - loss: 0.6951 - acc: 0.5416
3456/4566 [=====================>........] - ETA: 2:36 - loss: 0.6951 - acc: 0.5414
3520/4566 [======================>.......] - ETA: 2:27 - loss: 0.6948 - acc: 0.5418
3584/4566 [======================>.......] - ETA: 2:17 - loss: 0.6950 - acc: 0.5405
3648/4566 [======================>.......] - ETA: 2:07 - loss: 0.6954 - acc: 0.5392
3712/4566 [=======================>......] - ETA: 1:58 - loss: 0.6951 - acc: 0.5388
3776/4566 [=======================>......] - ETA: 1:51 - loss: 0.6946 - acc: 0.5400
3840/4566 [========================>.....] - ETA: 1:43 - loss: 0.6943 - acc: 0.5401
3904/4566 [========================>.....] - ETA: 1:35 - loss: 0.6946 - acc: 0.5392
3968/4566 [=========================>....] - ETA: 1:26 - loss: 0.6941 - acc: 0.5411
4032/4566 [=========================>....] - ETA: 1:18 - loss: 0.6943 - acc: 0.5407
4096/4566 [=========================>....] - ETA: 1:09 - loss: 0.6944 - acc: 0.5403
4160/4566 [==========================>...] - ETA: 1:00 - loss: 0.6946 - acc: 0.5389
4224/4566 [==========================>...] - ETA: 50s - loss: 0.6940 - acc: 0.5412 
4288/4566 [===========================>..] - ETA: 40s - loss: 0.6940 - acc: 0.5413
4352/4566 [===========================>..] - ETA: 31s - loss: 0.6938 - acc: 0.5411
4416/4566 [============================>.] - ETA: 21s - loss: 0.6935 - acc: 0.5423
4480/4566 [============================>.] - ETA: 12s - loss: 0.6932 - acc: 0.5433
4544/4566 [============================>.] - ETA: 3s - loss: 0.6933 - acc: 0.5440 
4566/4566 [==============================] - 680s 149ms/step - loss: 0.6932 - acc: 0.5442 - val_loss: 0.6789 - val_acc: 0.5866

Epoch 00003: val_acc improved from 0.56496 to 0.58661, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window18/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 4/10

  64/4566 [..............................] - ETA: 7:20 - loss: 0.6797 - acc: 0.5781
 128/4566 [..............................] - ETA: 7:21 - loss: 0.6818 - acc: 0.5703
 192/4566 [>.............................] - ETA: 7:12 - loss: 0.6879 - acc: 0.5521
 256/4566 [>.............................] - ETA: 7:26 - loss: 0.6889 - acc: 0.5430
 320/4566 [=>............................] - ETA: 7:36 - loss: 0.6841 - acc: 0.5437
 384/4566 [=>............................] - ETA: 8:12 - loss: 0.6875 - acc: 0.5339
 448/4566 [=>............................] - ETA: 8:59 - loss: 0.6919 - acc: 0.5223
 512/4566 [==>...........................] - ETA: 9:32 - loss: 0.6943 - acc: 0.5195
 576/4566 [==>...........................] - ETA: 9:51 - loss: 0.6967 - acc: 0.5156
 640/4566 [===>..........................] - ETA: 10:07 - loss: 0.6964 - acc: 0.5250
 704/4566 [===>..........................] - ETA: 10:19 - loss: 0.6969 - acc: 0.5213
 768/4566 [====>.........................] - ETA: 10:26 - loss: 0.6970 - acc: 0.5260
 832/4566 [====>.........................] - ETA: 10:23 - loss: 0.6966 - acc: 0.5204
 896/4566 [====>.........................] - ETA: 10:08 - loss: 0.6955 - acc: 0.5201
 960/4566 [=====>........................] - ETA: 9:45 - loss: 0.6937 - acc: 0.5250 
1024/4566 [=====>........................] - ETA: 9:22 - loss: 0.6930 - acc: 0.5312
1088/4566 [======>.......................] - ETA: 9:00 - loss: 0.6926 - acc: 0.5340
1152/4566 [======>.......................] - ETA: 8:40 - loss: 0.6914 - acc: 0.5382
1216/4566 [======>.......................] - ETA: 8:20 - loss: 0.6910 - acc: 0.5345
1280/4566 [=======>......................] - ETA: 8:04 - loss: 0.6895 - acc: 0.5398
1344/4566 [=======>......................] - ETA: 7:48 - loss: 0.6872 - acc: 0.5424
1408/4566 [========>.....................] - ETA: 7:33 - loss: 0.6884 - acc: 0.5384
1472/4566 [========>.....................] - ETA: 7:20 - loss: 0.6862 - acc: 0.5455
1536/4566 [=========>....................] - ETA: 7:08 - loss: 0.6854 - acc: 0.5501
1600/4566 [=========>....................] - ETA: 6:56 - loss: 0.6865 - acc: 0.5475
1664/4566 [=========>....................] - ETA: 6:45 - loss: 0.6884 - acc: 0.5451
1728/4566 [==========>...................] - ETA: 6:34 - loss: 0.6888 - acc: 0.5463
1792/4566 [==========>...................] - ETA: 6:27 - loss: 0.6895 - acc: 0.5446
1856/4566 [===========>..................] - ETA: 6:26 - loss: 0.6894 - acc: 0.5447
1920/4566 [===========>..................] - ETA: 6:24 - loss: 0.6890 - acc: 0.5474
1984/4566 [============>.................] - ETA: 6:21 - loss: 0.6886 - acc: 0.5464
2048/4566 [============>.................] - ETA: 6:18 - loss: 0.6886 - acc: 0.5469
2112/4566 [============>.................] - ETA: 6:13 - loss: 0.6891 - acc: 0.5469
2176/4566 [=============>................] - ETA: 6:08 - loss: 0.6889 - acc: 0.5460
2240/4566 [=============>................] - ETA: 6:01 - loss: 0.6894 - acc: 0.5455
2304/4566 [==============>...............] - ETA: 5:49 - loss: 0.6884 - acc: 0.5486
2368/4566 [==============>...............] - ETA: 5:36 - loss: 0.6881 - acc: 0.5490
2432/4566 [==============>...............] - ETA: 5:23 - loss: 0.6890 - acc: 0.5481
2496/4566 [===============>..............] - ETA: 5:11 - loss: 0.6895 - acc: 0.5493
2560/4566 [===============>..............] - ETA: 4:58 - loss: 0.6895 - acc: 0.5473
2624/4566 [================>.............] - ETA: 4:47 - loss: 0.6902 - acc: 0.5465
2688/4566 [================>.............] - ETA: 4:37 - loss: 0.6890 - acc: 0.5491
2752/4566 [=================>............] - ETA: 4:26 - loss: 0.6886 - acc: 0.5498
2816/4566 [=================>............] - ETA: 4:15 - loss: 0.6889 - acc: 0.5490
2880/4566 [=================>............] - ETA: 4:04 - loss: 0.6891 - acc: 0.5500
2944/4566 [==================>...........] - ETA: 3:54 - loss: 0.6899 - acc: 0.5486
3008/4566 [==================>...........] - ETA: 3:44 - loss: 0.6899 - acc: 0.5475
3072/4566 [===================>..........] - ETA: 3:34 - loss: 0.6905 - acc: 0.5469
3136/4566 [===================>..........] - ETA: 3:24 - loss: 0.6892 - acc: 0.5491
3200/4566 [====================>.........] - ETA: 3:14 - loss: 0.6896 - acc: 0.5469
3264/4566 [====================>.........] - ETA: 3:05 - loss: 0.6897 - acc: 0.5475
3328/4566 [====================>.........] - ETA: 2:58 - loss: 0.6893 - acc: 0.5487
3392/4566 [=====================>........] - ETA: 2:50 - loss: 0.6891 - acc: 0.5492
3456/4566 [=====================>........] - ETA: 2:42 - loss: 0.6884 - acc: 0.5506
3520/4566 [======================>.......] - ETA: 2:34 - loss: 0.6879 - acc: 0.5514
3584/4566 [======================>.......] - ETA: 2:26 - loss: 0.6875 - acc: 0.5530
3648/4566 [======================>.......] - ETA: 2:18 - loss: 0.6873 - acc: 0.5546
3712/4566 [=======================>......] - ETA: 2:08 - loss: 0.6874 - acc: 0.5558
3776/4566 [=======================>......] - ETA: 1:58 - loss: 0.6867 - acc: 0.5567
3840/4566 [========================>.....] - ETA: 1:48 - loss: 0.6868 - acc: 0.5560
3904/4566 [========================>.....] - ETA: 1:38 - loss: 0.6872 - acc: 0.5558
3968/4566 [=========================>....] - ETA: 1:28 - loss: 0.6867 - acc: 0.5570
4032/4566 [=========================>....] - ETA: 1:18 - loss: 0.6862 - acc: 0.5578
4096/4566 [=========================>....] - ETA: 1:09 - loss: 0.6866 - acc: 0.5574
4160/4566 [==========================>...] - ETA: 59s - loss: 0.6867 - acc: 0.5565 
4224/4566 [==========================>...] - ETA: 50s - loss: 0.6871 - acc: 0.5556
4288/4566 [===========================>..] - ETA: 40s - loss: 0.6879 - acc: 0.5543
4352/4566 [===========================>..] - ETA: 31s - loss: 0.6877 - acc: 0.5545
4416/4566 [============================>.] - ETA: 21s - loss: 0.6878 - acc: 0.5546
4480/4566 [============================>.] - ETA: 12s - loss: 0.6882 - acc: 0.5536
4544/4566 [============================>.] - ETA: 3s - loss: 0.6887 - acc: 0.5522 
4566/4566 [==============================] - 686s 150ms/step - loss: 0.6889 - acc: 0.5523 - val_loss: 0.6684 - val_acc: 0.6280

Epoch 00004: val_acc improved from 0.58661 to 0.62795, saving model to /data/lyli/Project/Liver_model/Seq_feature_exact/window18/checkpoints/final_seq_model/liver_seq_model_2_mer.hdf5
Epoch 5/10

  64/4566 [..............................] - ETA: 16:26 - loss: 0.6810 - acc: 0.6094
 128/4566 [..............................] - ETA: 15:58 - loss: 0.6705 - acc: 0.6328
 192/4566 [>.............................] - ETA: 15:47 - loss: 0.6644 - acc: 0.6146
 256/4566 [>.............................] - ETA: 15:40 - loss: 0.6680 - acc: 0.5977
 320/4566 [=>............................] - ETA: 15:21 - loss: 0.6676 - acc: 0.6000
 384/4566 [=>............................] - ETA: 14:42 - loss: 0.6770 - acc: 0.5703
 448/4566 [=>............................] - ETA: 13:29 - loss: 0.6807 - acc: 0.5625
 512/4566 [==>...........................] - ETA: 12:32 - loss: 0.6841 - acc: 0.5566
 576/4566 [==>...........................] - ETA: 11:46 - loss: 0.6877 - acc: 0.5469
 640/4566 [===>..........................] - ETA: 11:07 - loss: 0.6886 - acc: 0.5500
 704/4566 [===>..........................] - ETA: 10:37 - loss: 0.6891 - acc: 0.5469
 768/4566 [====>.........................] - ETA: 10:10 - loss: 0.6885 - acc: 0.5469
 832/4566 [====>.........................] - ETA: 9:44 - loss: 0.6885 - acc: 0.5421 
 896/4566 [====>.........................] - ETA: 9:21 - loss: 0.6902 - acc: 0.5324
 960/4566 [=====>........................] - ETA: 9:00 - loss: 0.6898 - acc: 0.5323
1024/4566 [=====>........................] - ETA: 8:41 - loss: 0.6899 - acc: 0.5312
1088/4566 [======>.......................] - ETA: 8:23 - loss: 0.6905 - acc: 0.5285
1152/4566 [======>.......................] - ETA: 8:07 - loss: 0.6914 - acc: 0.5278
1216/4566 [======>.......................] - ETA: 7:50 - loss: 0.6907 - acc: 0.5288
1280/4566 [=======>......................] - ETA: 7:34 - loss: 0.6899 - acc: 0.5312
1344/4566 [=======>......................] - ETA: 7:19 - loss: 0.6893 - acc: 0.5298
1408/4566 [========>.....................] - ETA: 7:17 - loss: 0.6890 - acc: 0.5334
1472/4566 [========>.....................] - ETA: 7:21 - loss: 0.6878 - acc: 0.5374
1536/4566 [=========>....................] - ETA: 7:24 - loss: 0.6887 - acc: 0.5332
1600/4566 [=========>....................] - ETA: 7:22 - loss: 0.6894 - acc: 0.5337
1664/4566 [=========>....................] - ETA: 7:20 - loss: 0.6887 - acc: 0.5355
1728/4566 [==========>...................] - ETA: 7:16 - loss: 0.6882 - acc: 0.5382
1792/4566 [==========>...................] - ETA: 7:12 - loss: 0.6893 - acc: 0.5357
1856/4566 [===========>..................] - ETA: 7:02 - loss: 0.6902 - acc: 0.5334
1920/4566 [===========>..................] - ETA: 6:49 - loss: 0.6896 - acc: 0.5359
1984/4566 [============>.................] - ETA: 6:35 - loss: 0.6895 - acc: 0.5358
2048/4566 [============>.................] - ETA: 6:22 - loss: 0.6900 - acc: 0.5342
2112/4566 [============>.................] - ETA: 6:10 - loss: 0.6892 - acc: 0.5341
2176/4566 [=============>................] - ETA: 5:57 - loss: 0.6886 - acc: 0.5377
2240/4566 [=============>................] - ETA: 5:45 - loss: 0.6881 - acc: 0.5411
2304/4566 [==============>...............] - ETA: 5:33 - loss: 0.6886 - acc: 0.5417
2368/4566 [==============>...............] - ETA: 5:21 - loss: 0.6891 - acc: 0.5405
2432/4566 [==============>...............] - ETA: 5:09 - loss: 0.6893 - acc: 0.5395
2496/4566 [===============>..............] - ETA: 4:57 - loss: 0.6881 - acc: 0.5425
2560/4566 [===============>..............] - ETA: 4:46 - loss: 0.6885 - acc: 0.5402
2624/4566 [================>.............] - ETA: 4:35 - loss: 0.6884 - acc: 0.5415
2688/4566 [================>.............] - ETA: 4:25 - loss: 0.6890 - acc: 0.5402
2752/4566 [=================>............] - ETA: 4:16 - loss: 0.6889 - acc: 0.5396
2816/4566 [=================>............] - ETA: 4:08 - loss: 0.6886 - acc: 0.5387
2880/4566 [=================>............] - ETA: 4:03 - loss: 0.6892 - acc: 0.5378
2944/4566 [==================>...........] - ETA: 3:56 - loss: 0.6899 - acc: 0.5370
3008/4566 [==================>...........] - ETA: 3:49 - loss: 0.6905 - acc: 0.5359
3072/4566 [===================>..........] - ETA: 3:42 - loss: 0.6906 - acc: 0.5368
3136/4566 [===================>..........] - ETA: 3:34 - loss: 0.6906 - acc: 0.5364
3200/4566 [====================>.........] - ETA: 3:26 - loss: 0.6903 - acc: 0.5381
3264/4566 [====================>.........] - ETA: 3:17 - loss: 0.6899 - acc: 0.5389
3328/4566 [====================>.........] - ETA: 3:06 - loss: 0.6894 - acc: 0.5409
3392/4566 [=====================>........] - ETA: 2:56 - loss: 0.6893 - acc: 0.5392
3456/4566 [=====================>........] - ETA: 2:45 - loss: 0.6895 - acc: 0.5385
3520/4566 [======================>.......] - ETA: 2:35 - loss: 0.6889 - acc: 0.5403
3584/4566 [======================>.......] - ETA: 2:24 - loss: 0.6889 - acc: 0.5407
3648/4566 [======================>.......] - ETA: 2:14 - loss: 0.6893 - acc: 0.5406
3712/4566 [=======================>......] - ETA: 2:04 - loss: 0.6895 - acc: 0.5407
3776/4566 [=======================>......] - ETA: 1:54 - loss: 0.6890 - acc: 0.5416
3840/4566 [========================>.....] - ETA: 1:45 - loss: 0.6888 - acc: 0.5414
3904/4566 [========================>.....] - ETA: 1:35 - loss: 0.6892 - acc: 0.5412
3968/4566 [=========================>....] - ETA: 1:26 - loss: 0.6893 - acc: 0.5413
4032/4566 [=========================>....] - ETA: 1:16 - loss: 0.6888 - acc: 0.5429
4096/4566 [=========================>....] - ETA: 1:07 - loss: 0.6886 - acc: 0.5425
4160/4566 [==========================>...] - ETA: 57s - loss: 0.6875 - acc: 0.5450 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6876 - acc: 0.5440
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6880 - acc: 0.5436
4352/4566 [===========================>..] - ETA: 30s - loss: 0.6883 - acc: 0.5437
4416/4566 [============================>.] - ETA: 21s - loss: 0.6883 - acc: 0.5439
4480/4566 [============================>.] - ETA: 12s - loss: 0.6879 - acc: 0.5453
4544/4566 [============================>.] - ETA: 3s - loss: 0.6881 - acc: 0.5453 
4566/4566 [==============================] - 705s 154ms/step - loss: 0.6879 - acc: 0.5449 - val_loss: 0.6709 - val_acc: 0.5827

Epoch 00005: val_acc did not improve from 0.62795
Epoch 6/10

  64/4566 [..............................] - ETA: 7:30 - loss: 0.7214 - acc: 0.5000
 128/4566 [..............................] - ETA: 7:27 - loss: 0.7004 - acc: 0.5625
 192/4566 [>.............................] - ETA: 7:40 - loss: 0.6970 - acc: 0.5469
 256/4566 [>.............................] - ETA: 7:42 - loss: 0.6906 - acc: 0.5859
 320/4566 [=>............................] - ETA: 7:46 - loss: 0.6868 - acc: 0.5813
 384/4566 [=>............................] - ETA: 7:43 - loss: 0.6835 - acc: 0.5911
 448/4566 [=>............................] - ETA: 7:37 - loss: 0.6867 - acc: 0.5781
 512/4566 [==>...........................] - ETA: 7:30 - loss: 0.6903 - acc: 0.5684
 576/4566 [==>...........................] - ETA: 7:21 - loss: 0.6901 - acc: 0.5712
 640/4566 [===>..........................] - ETA: 7:11 - loss: 0.6903 - acc: 0.5703
 704/4566 [===>..........................] - ETA: 7:05 - loss: 0.6955 - acc: 0.5582
 768/4566 [====>.........................] - ETA: 6:54 - loss: 0.6939 - acc: 0.5599
 832/4566 [====>.........................] - ETA: 6:49 - loss: 0.6928 - acc: 0.5601
 896/4566 [====>.........................] - ETA: 6:44 - loss: 0.6905 - acc: 0.5603
 960/4566 [=====>........................] - ETA: 6:36 - loss: 0.6935 - acc: 0.5500
1024/4566 [=====>........................] - ETA: 6:36 - loss: 0.6942 - acc: 0.5547
1088/4566 [======>.......................] - ETA: 6:52 - loss: 0.6923 - acc: 0.5597
1152/4566 [======>.......................] - ETA: 7:02 - loss: 0.6920 - acc: 0.5616
1216/4566 [======>.......................] - ETA: 7:07 - loss: 0.6923 - acc: 0.5584
1280/4566 [=======>......................] - ETA: 7:13 - loss: 0.6925 - acc: 0.5586
1344/4566 [=======>......................] - ETA: 7:16 - loss: 0.6913 - acc: 0.5558
1408/4566 [========>.....................] - ETA: 7:18 - loss: 0.6916 - acc: 0.5526
1472/4566 [========>.....................] - ETA: 7:13 - loss: 0.6913 - acc: 0.5523
1536/4566 [=========>....................] - ETA: 7:02 - loss: 0.6918 - acc: 0.5508
1600/4566 [=========>....................] - ETA: 6:50 - loss: 0.6916 - acc: 0.5494
1664/4566 [=========>....................] - ETA: 6:38 - loss: 0.6921 - acc: 0.5505
1728/4566 [==========>...................] - ETA: 6:26 - loss: 0.6920 - acc: 0.5515
1792/4566 [==========>...................] - ETA: 6:13 - loss: 0.6923 - acc: 0.5497
1856/4566 [===========>..................] - ETA: 6:01 - loss: 0.6927 - acc: 0.5496
1920/4566 [===========>..................] - ETA: 5:51 - loss: 0.6929 - acc: 0.5505
1984/4566 [============>.................] - ETA: 5:40 - loss: 0.6920 - acc: 0.5509
2048/4566 [============>.................] - ETA: 5:29 - loss: 0.6912 - acc: 0.5532
2112/4566 [============>.................] - ETA: 5:20 - loss: 0.6915 - acc: 0.5521
2176/4566 [=============>................] - ETA: 5:09 - loss: 0.6901 - acc: 0.5556
2240/4566 [=============>................] - ETA: 5:00 - loss: 0.6896 - acc: 0.5576
2304/4566 [==============>...............] - ETA: 4:50 - loss: 0.6892 - acc: 0.5590
2368/4566 [==============>...............] - ETA: 4:40 - loss: 0.6888 - acc: 0.5608
2432/4566 [==============>...............] - ETA: 4:30 - loss: 0.6871 - acc: 0.5646
2496/4566 [===============>..............] - ETA: 4:22 - loss: 0.6877 - acc: 0.5629
2560/4566 [===============>..............] - ETA: 4:18 - loss: 0.6881 - acc: 0.5617
2624/4566 [================>.............] - ETA: 4:14 - loss: 0.6876 - acc: 0.5644
2688/4566 [================>.............] - ETA: 4:09 - loss: 0.6872 - acc: 0.5647
2752/4566 [=================>............] - ETA: 4:04 - loss: 0.6863 - acc: 0.5669
2816/4566 [=================>............] - ETA: 3:58 - loss: 0.6857 - acc: 0.5671
2880/4566 [=================>............] - ETA: 3:52 - loss: 0.6853 - acc: 0.5681
2944/4566 [==================>...........] - ETA: 3:45 - loss: 0.6848 - acc: 0.5676
3008/4566 [==================>...........] - ETA: 3:35 - loss: 0.6847 - acc: 0.5688
3072/4566 [===================>..........] - ETA: 3:26 - loss: 0.6852 - acc: 0.5677
3136/4566 [===================>..........] - ETA: 3:16 - loss: 0.6843 - acc: 0.5689
3200/4566 [====================>.........] - ETA: 3:06 - loss: 0.6835 - acc: 0.5697
3264/4566 [====================>.........] - ETA: 2:57 - loss: 0.6831 - acc: 0.5695
3328/4566 [====================>.........] - ETA: 2:47 - loss: 0.6828 - acc: 0.5706
3392/4566 [=====================>........] - ETA: 2:38 - loss: 0.6833 - acc: 0.5696
3456/4566 [=====================>........] - ETA: 2:28 - loss: 0.6823 - acc: 0.5709
3520/4566 [======================>.......] - ETA: 2:19 - loss: 0.6832 - acc: 0.5687
3584/4566 [======================>.......] - ETA: 2:10 - loss: 0.6832 - acc: 0.5695
3648/4566 [======================>.......] - ETA: 2:01 - loss: 0.6829 - acc: 0.5699
3712/4566 [=======================>......] - ETA: 1:52 - loss: 0.6826 - acc: 0.5706
3776/4566 [=======================>......] - ETA: 1:43 - loss: 0.6827 - acc: 0.5712
3840/4566 [========================>.....] - ETA: 1:35 - loss: 0.6831 - acc: 0.5703
3904/4566 [========================>.....] - ETA: 1:26 - loss: 0.6833 - acc: 0.5699
3968/4566 [=========================>....] - ETA: 1:18 - loss: 0.6837 - acc: 0.5678
4032/4566 [=========================>....] - ETA: 1:10 - loss: 0.6839 - acc: 0.5667
4096/4566 [=========================>....] - ETA: 1:02 - loss: 0.6838 - acc: 0.5681
4160/4566 [==========================>...] - ETA: 54s - loss: 0.6832 - acc: 0.5685 
4224/4566 [==========================>...] - ETA: 46s - loss: 0.6834 - acc: 0.5675
4288/4566 [===========================>..] - ETA: 38s - loss: 0.6836 - acc: 0.5672
4352/4566 [===========================>..] - ETA: 29s - loss: 0.6834 - acc: 0.5673
4416/4566 [============================>.] - ETA: 20s - loss: 0.6834 - acc: 0.5682
4480/4566 [============================>.] - ETA: 11s - loss: 0.6832 - acc: 0.5681
4544/4566 [============================>.] - ETA: 3s - loss: 0.6828 - acc: 0.5691 
4566/4566 [==============================] - 656s 144ms/step - loss: 0.6830 - acc: 0.5688 - val_loss: 0.6622 - val_acc: 0.5984

Epoch 00006: val_acc did not improve from 0.62795
Epoch 7/10

  64/4566 [..............................] - ETA: 8:27 - loss: 0.6552 - acc: 0.6250
 128/4566 [..............................] - ETA: 7:56 - loss: 0.6872 - acc: 0.5234
 192/4566 [>.............................] - ETA: 7:42 - loss: 0.6729 - acc: 0.5729
 256/4566 [>.............................] - ETA: 7:33 - loss: 0.6741 - acc: 0.5977
 320/4566 [=>............................] - ETA: 7:22 - loss: 0.6754 - acc: 0.5875
 384/4566 [=>............................] - ETA: 7:19 - loss: 0.6679 - acc: 0.5885
 448/4566 [=>............................] - ETA: 7:16 - loss: 0.6680 - acc: 0.5826
 512/4566 [==>...........................] - ETA: 7:13 - loss: 0.6728 - acc: 0.5781
 576/4566 [==>...........................] - ETA: 7:11 - loss: 0.6727 - acc: 0.5851
 640/4566 [===>..........................] - ETA: 7:07 - loss: 0.6701 - acc: 0.5844
 704/4566 [===>..........................] - ETA: 7:18 - loss: 0.6698 - acc: 0.5824
 768/4566 [====>.........................] - ETA: 7:35 - loss: 0.6683 - acc: 0.5859
 832/4566 [====>.........................] - ETA: 7:57 - loss: 0.6703 - acc: 0.5805
 896/4566 [====>.........................] - ETA: 8:11 - loss: 0.6694 - acc: 0.5826
 960/4566 [=====>........................] - ETA: 8:22 - loss: 0.6673 - acc: 0.5844
1024/4566 [=====>........................] - ETA: 8:27 - loss: 0.6683 - acc: 0.5830
1088/4566 [======>.......................] - ETA: 8:35 - loss: 0.6680 - acc: 0.5827
1152/4566 [======>.......................] - ETA: 8:33 - loss: 0.6676 - acc: 0.5842
1216/4566 [======>.......................] - ETA: 8:21 - loss: 0.6679 - acc: 0.5839
1280/4566 [=======>......................] - ETA: 8:04 - loss: 0.6693 - acc: 0.5820
1344/4566 [=======>......................] - ETA: 7:46 - loss: 0.6706 - acc: 0.5789
1408/4566 [========>.....................] - ETA: 7:31 - loss: 0.6706 - acc: 0.5810
1472/4566 [========>.....................] - ETA: 7:16 - loss: 0.6713 - acc: 0.5802
1536/4566 [=========>....................] - ETA: 7:03 - loss: 0.6716 - acc: 0.5801
1600/4566 [=========>....................] - ETA: 6:51 - loss: 0.6708 - acc: 0.5813
1664/4566 [=========>....................] - ETA: 6:39 - loss: 0.6710 - acc: 0.5793
1728/4566 [==========>...................] - ETA: 6:27 - loss: 0.6715 - acc: 0.5775
1792/4566 [==========>...................] - ETA: 6:15 - loss: 0.6729 - acc: 0.5765
1856/4566 [===========>..................] - ETA: 6:04 - loss: 0.6741 - acc: 0.5738
1920/4566 [===========>..................] - ETA: 5:53 - loss: 0.6744 - acc: 0.5724
1984/4566 [============>.................] - ETA: 5:43 - loss: 0.6747 - acc: 0.5711
2048/4566 [============>.................] - ETA: 5:33 - loss: 0.6754 - acc: 0.5708
2112/4566 [============>.................] - ETA: 5:23 - loss: 0.6757 - acc: 0.5705
2176/4566 [=============>................] - ETA: 5:13 - loss: 0.6770 - acc: 0.5685
2240/4566 [=============>................] - ETA: 5:06 - loss: 0.6759 - acc: 0.5705
2304/4566 [==============>...............] - ETA: 5:04 - loss: 0.6767 - acc: 0.5707
2368/4566 [==============>...............] - ETA: 5:01 - loss: 0.6769 - acc: 0.5718
2432/4566 [==============>...............] - ETA: 4:56 - loss: 0.6766 - acc: 0.5728
2496/4566 [===============>..............] - ETA: 4:51 - loss: 0.6770 - acc: 0.5737
2560/4566 [===============>..............] - ETA: 4:45 - loss: 0.6769 - acc: 0.5746
2624/4566 [================>.............] - ETA: 4:39 - loss: 0.6767 - acc: 0.5751
2688/4566 [================>.............] - ETA: 4:30 - loss: 0.6776 - acc: 0.5733
2752/4566 [=================>............] - ETA: 4:19 - loss: 0.6776 - acc: 0.5734
2816/4566 [=================>............] - ETA: 4:09 - loss: 0.6769 - acc: 0.5763
2880/4566 [=================>............] - ETA: 3:59 - loss: 0.6774 - acc: 0.5750
2944/4566 [==================>...........] - ETA: 3:49 - loss: 0.6776 - acc: 0.5740
3008/4566 [==================>...........] - ETA: 3:39 - loss: 0.6769 - acc: 0.5755
3072/4566 [===================>..........] - ETA: 3:29 - loss: 0.6770 - acc: 0.5749
3136/4566 [===================>..........] - ETA: 3:19 - loss: 0.6772 - acc: 0.5743
3200/4566 [====================>.........] - ETA: 3:09 - loss: 0.6773 - acc: 0.5747
3264/4566 [====================>.........] - ETA: 3:00 - loss: 0.6774 - acc: 0.5741
3328/4566 [====================>.........] - ETA: 2:50 - loss: 0.6784 - acc: 0.5715
3392/4566 [=====================>........] - ETA: 2:41 - loss: 0.6789 - acc: 0.5702
3456/4566 [=====================>........] - ETA: 2:32 - loss: 0.6793 - acc: 0.5703
3520/4566 [======================>.......] - ETA: 2:22 - loss: 0.6791 - acc: 0.5707
3584/4566 [======================>.......] - ETA: 2:13 - loss: 0.6786 - acc: 0.5723
3648/4566 [======================>.......] - ETA: 2:04 - loss: 0.6783 - acc: 0.5726
3712/4566 [=======================>......] - ETA: 1:56 - loss: 0.6786 - acc: 0.5719
3776/4566 [=======================>......] - ETA: 1:48 - loss: 0.6785 - acc: 0.5728
3840/4566 [========================>.....] - ETA: 1:40 - loss: 0.6783 - acc: 0.5727
3904/4566 [========================>.....] - ETA: 1:32 - loss: 0.6785 - acc: 0.5722
3968/4566 [=========================>....] - ETA: 1:24 - loss: 0.6784 - acc: 0.5723
4032/4566 [=========================>....] - ETA: 1:15 - loss: 0.6781 - acc: 0.5724
4096/4566 [=========================>....] - ETA: 1:07 - loss: 0.6781 - acc: 0.5723
4160/4566 [==========================>...] - ETA: 58s - loss: 0.6783 - acc: 0.5724 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6780 - acc: 0.5724
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6781 - acc: 0.5716
4352/4566 [===========================>..] - ETA: 30s - loss: 0.6777 - acc: 0.5728
4416/4566 [============================>.] - ETA: 21s - loss: 0.6779 - acc: 0.5722
4480/4566 [============================>.] - ETA: 12s - loss: 0.6775 - acc: 0.5730
4544/4566 [============================>.] - ETA: 3s - loss: 0.6778 - acc: 0.5717 
4566/4566 [==============================] - 658s 144ms/step - loss: 0.6777 - acc: 0.5718 - val_loss: 0.6668 - val_acc: 0.5965

Epoch 00007: val_acc did not improve from 0.62795
Epoch 8/10

  64/4566 [..............................] - ETA: 7:56 - loss: 0.7000 - acc: 0.5469
 128/4566 [..............................] - ETA: 7:42 - loss: 0.6758 - acc: 0.5625
 192/4566 [>.............................] - ETA: 7:27 - loss: 0.6571 - acc: 0.5938
 256/4566 [>.............................] - ETA: 7:27 - loss: 0.6618 - acc: 0.5820
 320/4566 [=>............................] - ETA: 7:13 - loss: 0.6700 - acc: 0.5750
 384/4566 [=>............................] - ETA: 7:18 - loss: 0.6785 - acc: 0.5573
 448/4566 [=>............................] - ETA: 7:49 - loss: 0.6751 - acc: 0.5714
 512/4566 [==>...........................] - ETA: 8:38 - loss: 0.6741 - acc: 0.5684
 576/4566 [==>...........................] - ETA: 9:06 - loss: 0.6731 - acc: 0.5781
 640/4566 [===>..........................] - ETA: 9:26 - loss: 0.6760 - acc: 0.5672
 704/4566 [===>..........................] - ETA: 9:39 - loss: 0.6786 - acc: 0.5611
 768/4566 [====>.........................] - ETA: 9:53 - loss: 0.6795 - acc: 0.5586
 832/4566 [====>.........................] - ETA: 10:00 - loss: 0.6810 - acc: 0.5565
 896/4566 [====>.........................] - ETA: 9:54 - loss: 0.6777 - acc: 0.5614 
 960/4566 [=====>........................] - ETA: 9:31 - loss: 0.6790 - acc: 0.5615
1024/4566 [=====>........................] - ETA: 9:10 - loss: 0.6777 - acc: 0.5645
1088/4566 [======>.......................] - ETA: 8:51 - loss: 0.6781 - acc: 0.5680
1152/4566 [======>.......................] - ETA: 8:33 - loss: 0.6782 - acc: 0.5668
1216/4566 [======>.......................] - ETA: 8:16 - loss: 0.6809 - acc: 0.5641
1280/4566 [=======>......................] - ETA: 8:00 - loss: 0.6804 - acc: 0.5664
1344/4566 [=======>......................] - ETA: 7:45 - loss: 0.6817 - acc: 0.5685
1408/4566 [========>.....................] - ETA: 7:29 - loss: 0.6826 - acc: 0.5646
1472/4566 [========>.....................] - ETA: 7:14 - loss: 0.6828 - acc: 0.5666
1536/4566 [=========>....................] - ETA: 7:00 - loss: 0.6837 - acc: 0.5658
1600/4566 [=========>....................] - ETA: 6:47 - loss: 0.6825 - acc: 0.5687
1664/4566 [=========>....................] - ETA: 6:37 - loss: 0.6825 - acc: 0.5691
1728/4566 [==========>...................] - ETA: 6:27 - loss: 0.6818 - acc: 0.5683
1792/4566 [==========>...................] - ETA: 6:15 - loss: 0.6819 - acc: 0.5686
1856/4566 [===========>..................] - ETA: 6:06 - loss: 0.6817 - acc: 0.5727
1920/4566 [===========>..................] - ETA: 6:01 - loss: 0.6814 - acc: 0.5724
1984/4566 [============>.................] - ETA: 5:58 - loss: 0.6809 - acc: 0.5736
2048/4566 [============>.................] - ETA: 5:56 - loss: 0.6809 - acc: 0.5728
2112/4566 [============>.................] - ETA: 5:52 - loss: 0.6813 - acc: 0.5720
2176/4566 [=============>................] - ETA: 5:47 - loss: 0.6805 - acc: 0.5744
2240/4566 [=============>................] - ETA: 5:42 - loss: 0.6803 - acc: 0.5754
2304/4566 [==============>...............] - ETA: 5:36 - loss: 0.6808 - acc: 0.5729
2368/4566 [==============>...............] - ETA: 5:29 - loss: 0.6799 - acc: 0.5764
2432/4566 [==============>...............] - ETA: 5:17 - loss: 0.6793 - acc: 0.5748
2496/4566 [===============>..............] - ETA: 5:05 - loss: 0.6791 - acc: 0.5749
2560/4566 [===============>..............] - ETA: 4:53 - loss: 0.6799 - acc: 0.5738
2624/4566 [================>.............] - ETA: 4:42 - loss: 0.6803 - acc: 0.5736
2688/4566 [================>.............] - ETA: 4:30 - loss: 0.6798 - acc: 0.5751
2752/4566 [=================>............] - ETA: 4:19 - loss: 0.6794 - acc: 0.5756
2816/4566 [=================>............] - ETA: 4:08 - loss: 0.6799 - acc: 0.5749
2880/4566 [=================>............] - ETA: 3:58 - loss: 0.6802 - acc: 0.5719
2944/4566 [==================>...........] - ETA: 3:48 - loss: 0.6804 - acc: 0.5693
3008/4566 [==================>...........] - ETA: 3:38 - loss: 0.6803 - acc: 0.5688
3072/4566 [===================>..........] - ETA: 3:28 - loss: 0.6808 - acc: 0.5680
3136/4566 [===================>..........] - ETA: 3:19 - loss: 0.6809 - acc: 0.5673
3200/4566 [====================>.........] - ETA: 3:09 - loss: 0.6806 - acc: 0.5675
3264/4566 [====================>.........] - ETA: 3:00 - loss: 0.6809 - acc: 0.5674
3328/4566 [====================>.........] - ETA: 2:51 - loss: 0.6815 - acc: 0.5652
3392/4566 [=====================>........] - ETA: 2:42 - loss: 0.6812 - acc: 0.5657
3456/4566 [=====================>........] - ETA: 2:35 - loss: 0.6812 - acc: 0.5660
3520/4566 [======================>.......] - ETA: 2:27 - loss: 0.6810 - acc: 0.5656
3584/4566 [======================>.......] - ETA: 2:20 - loss: 0.6805 - acc: 0.5647
3648/4566 [======================>.......] - ETA: 2:12 - loss: 0.6800 - acc: 0.5661
3712/4566 [=======================>......] - ETA: 2:04 - loss: 0.6796 - acc: 0.5682
3776/4566 [=======================>......] - ETA: 1:55 - loss: 0.6794 - acc: 0.5686
3840/4566 [========================>.....] - ETA: 1:46 - loss: 0.6797 - acc: 0.5690
3904/4566 [========================>.....] - ETA: 1:36 - loss: 0.6800 - acc: 0.5681
3968/4566 [=========================>....] - ETA: 1:26 - loss: 0.6799 - acc: 0.5683
4032/4566 [=========================>....] - ETA: 1:17 - loss: 0.6802 - acc: 0.5675
4096/4566 [=========================>....] - ETA: 1:07 - loss: 0.6804 - acc: 0.5674
4160/4566 [==========================>...] - ETA: 58s - loss: 0.6804 - acc: 0.5671 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6798 - acc: 0.5687
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6804 - acc: 0.5683
4352/4566 [===========================>..] - ETA: 30s - loss: 0.6803 - acc: 0.5694
4416/4566 [============================>.] - ETA: 21s - loss: 0.6802 - acc: 0.5700
4480/4566 [============================>.] - ETA: 12s - loss: 0.6805 - acc: 0.5694
4544/4566 [============================>.] - ETA: 3s - loss: 0.6806 - acc: 0.5695 
4566/4566 [==============================] - 661s 145ms/step - loss: 0.6807 - acc: 0.5692 - val_loss: 0.6722 - val_acc: 0.5886

Epoch 00008: val_acc did not improve from 0.62795
Epoch 9/10

  64/4566 [..............................] - ETA: 7:55 - loss: 0.6668 - acc: 0.6406
 128/4566 [..............................] - ETA: 9:30 - loss: 0.6590 - acc: 0.6328
 192/4566 [>.............................] - ETA: 11:34 - loss: 0.6676 - acc: 0.6250
 256/4566 [>.............................] - ETA: 12:24 - loss: 0.6708 - acc: 0.6133
 320/4566 [=>............................] - ETA: 12:40 - loss: 0.6671 - acc: 0.6375
 384/4566 [=>............................] - ETA: 12:39 - loss: 0.6689 - acc: 0.6328
 448/4566 [=>............................] - ETA: 12:48 - loss: 0.6683 - acc: 0.6339
 512/4566 [==>...........................] - ETA: 12:47 - loss: 0.6716 - acc: 0.6133
 576/4566 [==>...........................] - ETA: 12:20 - loss: 0.6726 - acc: 0.5990
 640/4566 [===>..........................] - ETA: 11:39 - loss: 0.6744 - acc: 0.5953
 704/4566 [===>..........................] - ETA: 11:02 - loss: 0.6743 - acc: 0.5923
 768/4566 [====>.........................] - ETA: 10:29 - loss: 0.6775 - acc: 0.5859
 832/4566 [====>.........................] - ETA: 10:01 - loss: 0.6757 - acc: 0.5901
 896/4566 [====>.........................] - ETA: 9:35 - loss: 0.6756 - acc: 0.5882 
 960/4566 [=====>........................] - ETA: 9:14 - loss: 0.6783 - acc: 0.5792
1024/4566 [=====>........................] - ETA: 8:54 - loss: 0.6806 - acc: 0.5752
1088/4566 [======>.......................] - ETA: 8:37 - loss: 0.6790 - acc: 0.5781
1152/4566 [======>.......................] - ETA: 8:18 - loss: 0.6786 - acc: 0.5816
1216/4566 [======>.......................] - ETA: 8:01 - loss: 0.6805 - acc: 0.5765
1280/4566 [=======>......................] - ETA: 7:47 - loss: 0.6796 - acc: 0.5789
1344/4566 [=======>......................] - ETA: 7:33 - loss: 0.6781 - acc: 0.5818
1408/4566 [========>.....................] - ETA: 7:18 - loss: 0.6785 - acc: 0.5795
1472/4566 [========>.....................] - ETA: 7:04 - loss: 0.6767 - acc: 0.5815
1536/4566 [=========>....................] - ETA: 6:51 - loss: 0.6760 - acc: 0.5820
1600/4566 [=========>....................] - ETA: 6:42 - loss: 0.6752 - acc: 0.5837
1664/4566 [=========>....................] - ETA: 6:43 - loss: 0.6752 - acc: 0.5841
1728/4566 [==========>...................] - ETA: 6:44 - loss: 0.6742 - acc: 0.5851
1792/4566 [==========>...................] - ETA: 6:42 - loss: 0.6744 - acc: 0.5837
1856/4566 [===========>..................] - ETA: 6:39 - loss: 0.6742 - acc: 0.5862
1920/4566 [===========>..................] - ETA: 6:35 - loss: 0.6753 - acc: 0.5865
1984/4566 [============>.................] - ETA: 6:30 - loss: 0.6767 - acc: 0.5847
2048/4566 [============>.................] - ETA: 6:23 - loss: 0.6779 - acc: 0.5806
2112/4566 [============>.................] - ETA: 6:10 - loss: 0.6786 - acc: 0.5795
2176/4566 [=============>................] - ETA: 5:58 - loss: 0.6786 - acc: 0.5795
2240/4566 [=============>................] - ETA: 5:45 - loss: 0.6790 - acc: 0.5786
2304/4566 [==============>...............] - ETA: 5:33 - loss: 0.6788 - acc: 0.5777
2368/4566 [==============>...............] - ETA: 5:20 - loss: 0.6785 - acc: 0.5798
2432/4566 [==============>...............] - ETA: 5:09 - loss: 0.6774 - acc: 0.5814
2496/4566 [===============>..............] - ETA: 4:58 - loss: 0.6780 - acc: 0.5797
2560/4566 [===============>..............] - ETA: 4:46 - loss: 0.6793 - acc: 0.5754
2624/4566 [================>.............] - ETA: 4:36 - loss: 0.6793 - acc: 0.5747
2688/4566 [================>.............] - ETA: 4:25 - loss: 0.6790 - acc: 0.5740
2752/4566 [=================>............] - ETA: 4:14 - loss: 0.6789 - acc: 0.5745
2816/4566 [=================>............] - ETA: 4:03 - loss: 0.6798 - acc: 0.5721
2880/4566 [=================>............] - ETA: 3:53 - loss: 0.6791 - acc: 0.5729
2944/4566 [==================>...........] - ETA: 3:43 - loss: 0.6791 - acc: 0.5717
3008/4566 [==================>...........] - ETA: 3:34 - loss: 0.6789 - acc: 0.5715
3072/4566 [===================>..........] - ETA: 3:25 - loss: 0.6790 - acc: 0.5713
3136/4566 [===================>..........] - ETA: 3:19 - loss: 0.6796 - acc: 0.5705
3200/4566 [====================>.........] - ETA: 3:12 - loss: 0.6786 - acc: 0.5719
3264/4566 [====================>.........] - ETA: 3:05 - loss: 0.6801 - acc: 0.5702
3328/4566 [====================>.........] - ETA: 2:57 - loss: 0.6803 - acc: 0.5715
3392/4566 [=====================>........] - ETA: 2:49 - loss: 0.6805 - acc: 0.5705
3456/4566 [=====================>........] - ETA: 2:41 - loss: 0.6812 - acc: 0.5706
3520/4566 [======================>.......] - ETA: 2:33 - loss: 0.6809 - acc: 0.5713
3584/4566 [======================>.......] - ETA: 2:24 - loss: 0.6807 - acc: 0.5711
3648/4566 [======================>.......] - ETA: 2:14 - loss: 0.6810 - acc: 0.5702
3712/4566 [=======================>......] - ETA: 2:04 - loss: 0.6808 - acc: 0.5706
3776/4566 [=======================>......] - ETA: 1:54 - loss: 0.6807 - acc: 0.5702
3840/4566 [========================>.....] - ETA: 1:44 - loss: 0.6800 - acc: 0.5716
3904/4566 [========================>.....] - ETA: 1:35 - loss: 0.6796 - acc: 0.5715
3968/4566 [=========================>....] - ETA: 1:25 - loss: 0.6789 - acc: 0.5726
4032/4566 [=========================>....] - ETA: 1:16 - loss: 0.6792 - acc: 0.5704
4096/4566 [=========================>....] - ETA: 1:06 - loss: 0.6787 - acc: 0.5706
4160/4566 [==========================>...] - ETA: 57s - loss: 0.6785 - acc: 0.5707 
4224/4566 [==========================>...] - ETA: 48s - loss: 0.6789 - acc: 0.5703
4288/4566 [===========================>..] - ETA: 39s - loss: 0.6785 - acc: 0.5704
4352/4566 [===========================>..] - ETA: 29s - loss: 0.6781 - acc: 0.5722
4416/4566 [============================>.] - ETA: 20s - loss: 0.6788 - acc: 0.5711
4480/4566 [============================>.] - ETA: 11s - loss: 0.6784 - acc: 0.5708
4544/4566 [============================>.] - ETA: 3s - loss: 0.6791 - acc: 0.5695 
4566/4566 [==============================] - 667s 146ms/step - loss: 0.6791 - acc: 0.5696 - val_loss: 0.6749 - val_acc: 0.5827

Epoch 00009: val_acc did not improve from 0.62795
Epoch 10/10

  64/4566 [..............................] - ETA: 17:17 - loss: 0.6478 - acc: 0.5938
 128/4566 [..............................] - ETA: 16:45 - loss: 0.6592 - acc: 0.6094
 192/4566 [>.............................] - ETA: 16:28 - loss: 0.6565 - acc: 0.6250
 256/4566 [>.............................] - ETA: 16:01 - loss: 0.6545 - acc: 0.6055
 320/4566 [=>............................] - ETA: 14:37 - loss: 0.6520 - acc: 0.6094
 384/4566 [=>............................] - ETA: 13:06 - loss: 0.6512 - acc: 0.6146
 448/4566 [=>............................] - ETA: 12:00 - loss: 0.6534 - acc: 0.6027
 512/4566 [==>...........................] - ETA: 11:14 - loss: 0.6532 - acc: 0.6055
 576/4566 [==>...........................] - ETA: 10:39 - loss: 0.6520 - acc: 0.6094
 640/4566 [===>..........................] - ETA: 10:10 - loss: 0.6536 - acc: 0.6109
 704/4566 [===>..........................] - ETA: 9:42 - loss: 0.6551 - acc: 0.6122 
 768/4566 [====>.........................] - ETA: 9:20 - loss: 0.6521 - acc: 0.6172
 832/4566 [====>.........................] - ETA: 9:00 - loss: 0.6502 - acc: 0.6262
 896/4566 [====>.........................] - ETA: 8:43 - loss: 0.6537 - acc: 0.6217
 960/4566 [=====>........................] - ETA: 8:24 - loss: 0.6549 - acc: 0.6219
1024/4566 [=====>........................] - ETA: 8:10 - loss: 0.6557 - acc: 0.6201
1088/4566 [======>.......................] - ETA: 7:56 - loss: 0.6556 - acc: 0.6186
1152/4566 [======>.......................] - ETA: 7:42 - loss: 0.6581 - acc: 0.6137
1216/4566 [======>.......................] - ETA: 7:30 - loss: 0.6602 - acc: 0.6110
1280/4566 [=======>......................] - ETA: 7:17 - loss: 0.6623 - acc: 0.6055
1344/4566 [=======>......................] - ETA: 7:10 - loss: 0.6663 - acc: 0.6019
1408/4566 [========>.....................] - ETA: 7:14 - loss: 0.6659 - acc: 0.6016
1472/4566 [========>.....................] - ETA: 7:16 - loss: 0.6657 - acc: 0.6026
1536/4566 [=========>....................] - ETA: 7:15 - loss: 0.6662 - acc: 0.6016
1600/4566 [=========>....................] - ETA: 7:15 - loss: 0.6645 - acc: 0.6031
1664/4566 [=========>....................] - ETA: 7:13 - loss: 0.6665 - acc: 0.6010
1728/4566 [==========>...................] - ETA: 7:11 - loss: 0.6658 - acc: 0.6019
1792/4566 [==========>...................] - ETA: 7:02 - loss: 0.6653 - acc: 0.6016
1856/4566 [===========>..................] - ETA: 6:49 - loss: 0.6652 - acc: 0.6018
1920/4566 [===========>..................] - ETA: 6:36 - loss: 0.6653 - acc: 0.6026
1984/4566 [============>.................] - ETA: 6:24 - loss: 0.6659 - acc: 0.6018
2048/4566 [============>.................] - ETA: 6:12 - loss: 0.6665 - acc: 0.6021
2112/4566 [============>.................] - ETA: 6:01 - loss: 0.6694 - acc: 0.5985
2176/4566 [=============>................] - ETA: 5:49 - loss: 0.6711 - acc: 0.5960
2240/4566 [=============>................] - ETA: 5:38 - loss: 0.6719 - acc: 0.5942
2304/4566 [==============>...............] - ETA: 5:26 - loss: 0.6718 - acc: 0.5938
2368/4566 [==============>...............] - ETA: 5:15 - loss: 0.6721 - acc: 0.5921
2432/4566 [==============>...............] - ETA: 5:04 - loss: 0.6725 - acc: 0.5913
2496/4566 [===============>..............] - ETA: 4:54 - loss: 0.6732 - acc: 0.5897
2560/4566 [===============>..............] - ETA: 4:44 - loss: 0.6740 - acc: 0.5859
2624/4566 [================>.............] - ETA: 4:34 - loss: 0.6748 - acc: 0.5831
2688/4566 [================>.............] - ETA: 4:24 - loss: 0.6756 - acc: 0.5822
2752/4566 [=================>............] - ETA: 4:15 - loss: 0.6759 - acc: 0.5803
2816/4566 [=================>............] - ETA: 4:08 - loss: 0.6757 - acc: 0.5806
2880/4566 [=================>............] - ETA: 4:03 - loss: 0.6758 - acc: 0.5806
2944/4566 [==================>...........] - ETA: 3:57 - loss: 0.6754 - acc: 0.5808
3008/4566 [==================>...........] - ETA: 3:50 - loss: 0.6753 - acc: 0.5828
3072/4566 [===================>..........] - ETA: 3:43 - loss: 0.6755 - acc: 0.5811
3136/4566 [===================>..........] - ETA: 3:35 - loss: 0.6755 - acc: 0.5800
3200/4566 [====================>.........] - ETA: 3:26 - loss: 0.6755 - acc: 0.5800
3264/4566 [====================>.........] - ETA: 3:17 - loss: 0.6754 - acc: 0.5797
3328/4566 [====================>.........] - ETA: 3:06 - loss: 0.6750 - acc: 0.5808
3392/4566 [=====================>........] - ETA: 2:56 - loss: 0.6750 - acc: 0.5811
3456/4566 [=====================>........] - ETA: 2:46 - loss: 0.6747 - acc: 0.5804
3520/4566 [======================>.......] - ETA: 2:36 - loss: 0.6751 - acc: 0.5778
3584/4566 [======================>.......] - ETA: 2:26 - loss: 0.6751 - acc: 0.5776
3648/4566 [======================>.......] - ETA: 2:16 - loss: 0.6751 - acc: 0.5779
3712/4566 [=======================>......] - ETA: 2:07 - loss: 0.6752 - acc: 0.5789
3776/4566 [=======================>......] - ETA: 1:57 - loss: 0.6753 - acc: 0.5784
3840/4566 [========================>.....] - ETA: 1:47 - loss: 0.6757 - acc: 0.5784
3904/4566 [========================>.....] - ETA: 1:37 - loss: 0.6761 - acc: 0.5779
3968/4566 [=========================>....] - ETA: 1:27 - loss: 0.6767 - acc: 0.5761
4032/4566 [=========================>....] - ETA: 1:17 - loss: 0.6764 - acc: 0.5769
4096/4566 [=========================>....] - ETA: 1:08 - loss: 0.6761 - acc: 0.5769
4160/4566 [==========================>...] - ETA: 58s - loss: 0.6759 - acc: 0.5769 
4224/4566 [==========================>...] - ETA: 50s - loss: 0.6754 - acc: 0.5788
4288/4566 [===========================>..] - ETA: 41s - loss: 0.6756 - acc: 0.5781
4352/4566 [===========================>..] - ETA: 31s - loss: 0.6754 - acc: 0.5781
4416/4566 [============================>.] - ETA: 22s - loss: 0.6754 - acc: 0.5781
4480/4566 [============================>.] - ETA: 12s - loss: 0.6755 - acc: 0.5772
4544/4566 [============================>.] - ETA: 3s - loss: 0.6754 - acc: 0.5770 
4566/4566 [==============================] - 722s 158ms/step - loss: 0.6751 - acc: 0.5777 - val_loss: 0.6664 - val_acc: 0.6102

Epoch 00010: val_acc did not improve from 0.62795
样本个数 634
样本个数 1268
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7ff687cb4410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7ff687cb4410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7ff67f810950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7ff67f810950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f8c01d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f8c01d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff67f84bbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff67f84bbd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff67f607a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff67f607a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff687c4be90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff687c4be90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff67f84b390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff67f84b390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67efc5590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67efc5590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff629ade5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff629ade5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff629b9bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff629b9bc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f51c210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f51c210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff629bc4f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff629bc4f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f422110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f422110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff67f2dd1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff67f2dd1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff67f198e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff67f198e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f5ab310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f5ab310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff67f5201d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff67f5201d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f1fd2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f1fd2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff67f34fe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff67f34fe90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff67ee6aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff67ee6aed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67efe1b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67efe1b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff6299df890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff6299df890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f23a310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67f23a310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff67edcde50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff67edcde50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff67ebb4090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff67ebb4090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67ebe0950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff67ebe0950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff67ed5c190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff67ed5c190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66ea49190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66ea49190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff66e97ef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff66e97ef90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff66e9638d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff66e9638d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66e721b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66e721b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff67f01dc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff67f01dc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66e969f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66e969f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff66e6295d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff66e6295d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff66e66e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff66e66e550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66e62dad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66e62dad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff66e629ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff66e629ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66e516090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66e516090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff6662d5190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff6662d5190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff66e689210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff66e689210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66e3616d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff66e3616d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff66e3427d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff66e3427d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff6660e4990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff6660e4990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff66623b090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff66623b090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff665feed90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff665feed90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff666230a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff666230a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff66623bc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff66623bc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff665de1f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff665de1f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff665d367d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff665d367d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff665fc2dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff665fc2dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff665c88550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff665c88550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff665ff9110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff665ff9110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff665ed74d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff665ed74d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff65da96310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff65da96310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff65d854850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff65d854850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff65d9d3110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff65d9d3110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff665ba92d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff665ba92d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff65d8cab50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff65d8cab50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff65d6b7ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7ff65d6b7ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff65d578ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7ff65d578ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff65d6f5150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff65d6f5150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff65d6b7710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7ff65d6b7710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff65d433810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7ff65d433810>>: AttributeError: module 'gast' has no attribute 'Str'
window18.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1268 [>.............................] - ETA: 1:44
 128/1268 [==>...........................] - ETA: 1:14
 192/1268 [===>..........................] - ETA: 1:01
 256/1268 [=====>........................] - ETA: 52s 
 320/1268 [======>.......................] - ETA: 50s
 384/1268 [========>.....................] - ETA: 44s
 448/1268 [=========>....................] - ETA: 40s
 512/1268 [===========>..................] - ETA: 36s
 576/1268 [============>.................] - ETA: 33s
 640/1268 [==============>...............] - ETA: 29s
 704/1268 [===============>..............] - ETA: 26s
 768/1268 [=================>............] - ETA: 23s
 832/1268 [==================>...........] - ETA: 20s
 896/1268 [====================>.........] - ETA: 17s
 960/1268 [=====================>........] - ETA: 14s
1024/1268 [=======================>......] - ETA: 11s
1088/1268 [========================>.....] - ETA: 8s 
1152/1268 [==========================>...] - ETA: 5s
1216/1268 [===========================>..] - ETA: 2s
1268/1268 [==============================] - 57s 45ms/step
loss: 0.6784383562461059
acc: 0.5654574141893477
