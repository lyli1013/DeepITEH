nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 5185
样本个数 10370
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fc737a71c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fc737a71c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fc74c063f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fc74c063f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c275810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c275810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74bd4f350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74bd4f350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc74bd0d790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc74bd0d790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74bc0e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74bc0e550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc74bd4f090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc74bd4f090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c554e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c554e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73788b490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73788b490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7377a5a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7377a5a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7378f7a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7378f7a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73779d410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73779d410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc737872650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc737872650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc7377eca50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc7377eca50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc737447f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc737447f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7375bd4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7375bd4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc737919750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc737919750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7375aea50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7375aea50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc7372440d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc7372440d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73716ffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73716ffd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc737520950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc737520950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7371a9190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7371a9190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74bd16d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74bd16d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc736f54f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc736f54f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7370b6710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7370b6710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc737042110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc737042110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7371a51d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7371a51d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc736f40b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc736f40b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc736bfb3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc736bfb3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc736c42c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc736c42c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc736c76650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc736c76650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7371a6590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7371a6590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72ea00110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72ea00110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc736be8810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc736be8810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc72e86c2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc72e86c2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc736c8ff90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc736c8ff90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc736b16990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc736b16990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc736b69bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc736b69bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc72e6cab90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc72e6cab90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc72e4af2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc72e4af2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72e81a290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72e81a290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc72e86c350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc72e86c350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72e4b5550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72e4b5550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc72e316050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc72e316050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc72e162f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc72e162f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72e2a5110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72e2a5110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc72e316110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc72e316110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72e4b8350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72e4b8350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc72e50d9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc72e50d9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73705d850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73705d850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72dfa29d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72dfa29d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc72deb5410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc72deb5410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72dfeb8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72dfeb8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc72dc70810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc72dc70810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc72db941d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc72db941d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72dddbcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72dddbcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc72dc70d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc72dc70d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72da95510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72da95510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc72dfcc450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc72dfcc450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc72584b850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc72584b850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7259a6810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7259a6810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc72db68290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc72db68290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7258f0490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7258f0490>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-30 09:19:12.493826: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-30 09:19:12.660384: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-30 09:19:12.825642: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559021e7c9d0 executing computations on platform Host. Devices:
2022-11-30 09:19:12.825753: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-30 09:19:14.747514: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window_select17-21.py:152: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 9333 samples, validate on 1037 samples
Epoch 1/10

  64/9333 [..............................] - ETA: 1:15:06 - loss: 0.7710 - acc: 0.4688
 128/9333 [..............................] - ETA: 51:28 - loss: 0.7828 - acc: 0.4531  
 192/9333 [..............................] - ETA: 43:35 - loss: 0.7504 - acc: 0.4844
 256/9333 [..............................] - ETA: 38:47 - loss: 0.7649 - acc: 0.4844
 320/9333 [>.............................] - ETA: 36:49 - loss: 0.7555 - acc: 0.5125
 384/9333 [>.............................] - ETA: 35:41 - loss: 0.7447 - acc: 0.5208
 448/9333 [>.............................] - ETA: 33:56 - loss: 0.7404 - acc: 0.5268
 512/9333 [>.............................] - ETA: 32:20 - loss: 0.7377 - acc: 0.5234
 576/9333 [>.............................] - ETA: 31:13 - loss: 0.7396 - acc: 0.5208
 640/9333 [=>............................] - ETA: 30:07 - loss: 0.7400 - acc: 0.5266
 704/9333 [=>............................] - ETA: 29:18 - loss: 0.7284 - acc: 0.5355
 768/9333 [=>............................] - ETA: 28:37 - loss: 0.7248 - acc: 0.5339
 832/9333 [=>............................] - ETA: 28:16 - loss: 0.7337 - acc: 0.5288
 896/9333 [=>............................] - ETA: 27:47 - loss: 0.7310 - acc: 0.5335
 960/9333 [==>...........................] - ETA: 27:13 - loss: 0.7282 - acc: 0.5354
1024/9333 [==>...........................] - ETA: 26:38 - loss: 0.7288 - acc: 0.5312
1088/9333 [==>...........................] - ETA: 26:12 - loss: 0.7272 - acc: 0.5303
1152/9333 [==>...........................] - ETA: 25:52 - loss: 0.7282 - acc: 0.5243
1216/9333 [==>...........................] - ETA: 25:30 - loss: 0.7272 - acc: 0.5255
1280/9333 [===>..........................] - ETA: 25:07 - loss: 0.7268 - acc: 0.5234
1344/9333 [===>..........................] - ETA: 24:43 - loss: 0.7283 - acc: 0.5201
1408/9333 [===>..........................] - ETA: 24:16 - loss: 0.7278 - acc: 0.5213
1472/9333 [===>..........................] - ETA: 23:57 - loss: 0.7256 - acc: 0.5224
1536/9333 [===>..........................] - ETA: 23:36 - loss: 0.7254 - acc: 0.5228
1600/9333 [====>.........................] - ETA: 23:15 - loss: 0.7229 - acc: 0.5275
1664/9333 [====>.........................] - ETA: 22:56 - loss: 0.7214 - acc: 0.5300
1728/9333 [====>.........................] - ETA: 22:36 - loss: 0.7205 - acc: 0.5295
1792/9333 [====>.........................] - ETA: 22:15 - loss: 0.7197 - acc: 0.5324
1856/9333 [====>.........................] - ETA: 21:54 - loss: 0.7193 - acc: 0.5318
1920/9333 [=====>........................] - ETA: 21:36 - loss: 0.7194 - acc: 0.5286
1984/9333 [=====>........................] - ETA: 21:26 - loss: 0.7184 - acc: 0.5312
2048/9333 [=====>........................] - ETA: 21:10 - loss: 0.7170 - acc: 0.5308
2112/9333 [=====>........................] - ETA: 20:57 - loss: 0.7181 - acc: 0.5284
2176/9333 [=====>........................] - ETA: 20:44 - loss: 0.7170 - acc: 0.5303
2240/9333 [======>.......................] - ETA: 20:31 - loss: 0.7168 - acc: 0.5299
2304/9333 [======>.......................] - ETA: 20:16 - loss: 0.7175 - acc: 0.5278
2368/9333 [======>.......................] - ETA: 20:02 - loss: 0.7169 - acc: 0.5287
2432/9333 [======>.......................] - ETA: 19:46 - loss: 0.7175 - acc: 0.5259
2496/9333 [=======>......................] - ETA: 19:39 - loss: 0.7185 - acc: 0.5232
2560/9333 [=======>......................] - ETA: 19:24 - loss: 0.7198 - acc: 0.5203
2624/9333 [=======>......................] - ETA: 19:11 - loss: 0.7206 - acc: 0.5202
2688/9333 [=======>......................] - ETA: 18:56 - loss: 0.7210 - acc: 0.5179
2752/9333 [=======>......................] - ETA: 18:43 - loss: 0.7204 - acc: 0.5200
2816/9333 [========>.....................] - ETA: 18:28 - loss: 0.7205 - acc: 0.5202
2880/9333 [========>.....................] - ETA: 18:16 - loss: 0.7205 - acc: 0.5188
2944/9333 [========>.....................] - ETA: 18:03 - loss: 0.7204 - acc: 0.5197
3008/9333 [========>.....................] - ETA: 17:50 - loss: 0.7198 - acc: 0.5213
3072/9333 [========>.....................] - ETA: 17:37 - loss: 0.7189 - acc: 0.5234
3136/9333 [=========>....................] - ETA: 17:22 - loss: 0.7185 - acc: 0.5233
3200/9333 [=========>....................] - ETA: 17:10 - loss: 0.7177 - acc: 0.5238
3264/9333 [=========>....................] - ETA: 16:57 - loss: 0.7169 - acc: 0.5242
3328/9333 [=========>....................] - ETA: 16:44 - loss: 0.7166 - acc: 0.5249
3392/9333 [=========>....................] - ETA: 16:31 - loss: 0.7171 - acc: 0.5236
3456/9333 [==========>...................] - ETA: 16:18 - loss: 0.7172 - acc: 0.5226
3520/9333 [==========>...................] - ETA: 16:06 - loss: 0.7171 - acc: 0.5222
3584/9333 [==========>...................] - ETA: 15:53 - loss: 0.7167 - acc: 0.5232
3648/9333 [==========>...................] - ETA: 15:40 - loss: 0.7164 - acc: 0.5230
3712/9333 [==========>...................] - ETA: 15:27 - loss: 0.7164 - acc: 0.5221
3776/9333 [===========>..................] - ETA: 15:15 - loss: 0.7168 - acc: 0.5225
3840/9333 [===========>..................] - ETA: 15:04 - loss: 0.7169 - acc: 0.5211
3904/9333 [===========>..................] - ETA: 14:52 - loss: 0.7166 - acc: 0.5228
3968/9333 [===========>..................] - ETA: 14:40 - loss: 0.7169 - acc: 0.5222
4032/9333 [===========>..................] - ETA: 14:28 - loss: 0.7165 - acc: 0.5226
4096/9333 [============>.................] - ETA: 14:17 - loss: 0.7160 - acc: 0.5234
4160/9333 [============>.................] - ETA: 14:05 - loss: 0.7158 - acc: 0.5228
4224/9333 [============>.................] - ETA: 13:53 - loss: 0.7152 - acc: 0.5237
4288/9333 [============>.................] - ETA: 13:42 - loss: 0.7149 - acc: 0.5236
4352/9333 [============>.................] - ETA: 13:30 - loss: 0.7141 - acc: 0.5250
4416/9333 [=============>................] - ETA: 13:18 - loss: 0.7140 - acc: 0.5242
4480/9333 [=============>................] - ETA: 13:05 - loss: 0.7132 - acc: 0.5250
4544/9333 [=============>................] - ETA: 12:54 - loss: 0.7136 - acc: 0.5235
4608/9333 [=============>................] - ETA: 12:43 - loss: 0.7136 - acc: 0.5232
4672/9333 [==============>...............] - ETA: 12:32 - loss: 0.7137 - acc: 0.5225
4736/9333 [==============>...............] - ETA: 12:21 - loss: 0.7135 - acc: 0.5217
4800/9333 [==============>...............] - ETA: 12:09 - loss: 0.7135 - acc: 0.5215
4864/9333 [==============>...............] - ETA: 11:59 - loss: 0.7134 - acc: 0.5218
4928/9333 [==============>...............] - ETA: 11:48 - loss: 0.7134 - acc: 0.5213
4992/9333 [===============>..............] - ETA: 11:37 - loss: 0.7134 - acc: 0.5204
5056/9333 [===============>..............] - ETA: 11:26 - loss: 0.7135 - acc: 0.5194
5120/9333 [===============>..............] - ETA: 11:15 - loss: 0.7132 - acc: 0.5195
5184/9333 [===============>..............] - ETA: 11:04 - loss: 0.7135 - acc: 0.5197
5248/9333 [===============>..............] - ETA: 10:54 - loss: 0.7134 - acc: 0.5192
5312/9333 [================>.............] - ETA: 10:43 - loss: 0.7132 - acc: 0.5188
5376/9333 [================>.............] - ETA: 10:32 - loss: 0.7125 - acc: 0.5193
5440/9333 [================>.............] - ETA: 10:22 - loss: 0.7124 - acc: 0.5195
5504/9333 [================>.............] - ETA: 10:11 - loss: 0.7121 - acc: 0.5193
5568/9333 [================>.............] - ETA: 10:01 - loss: 0.7120 - acc: 0.5199
5632/9333 [=================>............] - ETA: 9:50 - loss: 0.7115 - acc: 0.5210 
5696/9333 [=================>............] - ETA: 9:41 - loss: 0.7117 - acc: 0.5198
5760/9333 [=================>............] - ETA: 9:31 - loss: 0.7115 - acc: 0.5205
5824/9333 [=================>............] - ETA: 9:21 - loss: 0.7113 - acc: 0.5204
5888/9333 [=================>............] - ETA: 9:11 - loss: 0.7109 - acc: 0.5216
5952/9333 [==================>...........] - ETA: 9:01 - loss: 0.7114 - acc: 0.5202
6016/9333 [==================>...........] - ETA: 8:50 - loss: 0.7114 - acc: 0.5204
6080/9333 [==================>...........] - ETA: 8:40 - loss: 0.7114 - acc: 0.5204
6144/9333 [==================>...........] - ETA: 8:30 - loss: 0.7110 - acc: 0.5212
6208/9333 [==================>...........] - ETA: 8:19 - loss: 0.7106 - acc: 0.5216
6272/9333 [===================>..........] - ETA: 8:09 - loss: 0.7106 - acc: 0.5212
6336/9333 [===================>..........] - ETA: 7:59 - loss: 0.7107 - acc: 0.5207
6400/9333 [===================>..........] - ETA: 7:49 - loss: 0.7109 - acc: 0.5200
6464/9333 [===================>..........] - ETA: 7:38 - loss: 0.7109 - acc: 0.5201
6528/9333 [===================>..........] - ETA: 7:28 - loss: 0.7108 - acc: 0.5204
6592/9333 [====================>.........] - ETA: 7:17 - loss: 0.7106 - acc: 0.5208
6656/9333 [====================>.........] - ETA: 7:07 - loss: 0.7104 - acc: 0.5210
6720/9333 [====================>.........] - ETA: 6:56 - loss: 0.7106 - acc: 0.5204
6784/9333 [====================>.........] - ETA: 6:46 - loss: 0.7107 - acc: 0.5199
6848/9333 [=====================>........] - ETA: 6:35 - loss: 0.7105 - acc: 0.5197
6912/9333 [=====================>........] - ETA: 6:25 - loss: 0.7104 - acc: 0.5200
6976/9333 [=====================>........] - ETA: 6:15 - loss: 0.7103 - acc: 0.5199
7040/9333 [=====================>........] - ETA: 6:04 - loss: 0.7105 - acc: 0.5196
7104/9333 [=====================>........] - ETA: 5:54 - loss: 0.7099 - acc: 0.5203
7168/9333 [======================>.......] - ETA: 5:44 - loss: 0.7098 - acc: 0.5204
7232/9333 [======================>.......] - ETA: 5:34 - loss: 0.7099 - acc: 0.5195
7296/9333 [======================>.......] - ETA: 5:24 - loss: 0.7099 - acc: 0.5185
7360/9333 [======================>.......] - ETA: 5:14 - loss: 0.7097 - acc: 0.5182
7424/9333 [======================>.......] - ETA: 5:04 - loss: 0.7096 - acc: 0.5179
7488/9333 [=======================>......] - ETA: 4:54 - loss: 0.7097 - acc: 0.5170
7552/9333 [=======================>......] - ETA: 4:43 - loss: 0.7098 - acc: 0.5173
7616/9333 [=======================>......] - ETA: 4:33 - loss: 0.7098 - acc: 0.5171
7680/9333 [=======================>......] - ETA: 4:23 - loss: 0.7096 - acc: 0.5171
7744/9333 [=======================>......] - ETA: 4:13 - loss: 0.7098 - acc: 0.5165
7808/9333 [========================>.....] - ETA: 4:03 - loss: 0.7100 - acc: 0.5160
7872/9333 [========================>.....] - ETA: 3:53 - loss: 0.7098 - acc: 0.5164
7936/9333 [========================>.....] - ETA: 3:43 - loss: 0.7102 - acc: 0.5163
8000/9333 [========================>.....] - ETA: 3:33 - loss: 0.7102 - acc: 0.5155
8064/9333 [========================>.....] - ETA: 3:22 - loss: 0.7102 - acc: 0.5157
8128/9333 [=========================>....] - ETA: 3:12 - loss: 0.7099 - acc: 0.5162
8192/9333 [=========================>....] - ETA: 3:02 - loss: 0.7099 - acc: 0.5165
8256/9333 [=========================>....] - ETA: 2:52 - loss: 0.7098 - acc: 0.5167
8320/9333 [=========================>....] - ETA: 2:42 - loss: 0.7096 - acc: 0.5173
8384/9333 [=========================>....] - ETA: 2:31 - loss: 0.7097 - acc: 0.5175
8448/9333 [==========================>...] - ETA: 2:21 - loss: 0.7096 - acc: 0.5175
8512/9333 [==========================>...] - ETA: 2:11 - loss: 0.7095 - acc: 0.5182
8576/9333 [==========================>...] - ETA: 2:01 - loss: 0.7091 - acc: 0.5189
8640/9333 [==========================>...] - ETA: 1:50 - loss: 0.7090 - acc: 0.5191
8704/9333 [==========================>...] - ETA: 1:40 - loss: 0.7093 - acc: 0.5184
8768/9333 [===========================>..] - ETA: 1:30 - loss: 0.7091 - acc: 0.5190
8832/9333 [===========================>..] - ETA: 1:20 - loss: 0.7092 - acc: 0.5183
8896/9333 [===========================>..] - ETA: 1:10 - loss: 0.7092 - acc: 0.5179
8960/9333 [===========================>..] - ETA: 59s - loss: 0.7091 - acc: 0.5181 
9024/9333 [============================>.] - ETA: 49s - loss: 0.7089 - acc: 0.5187
9088/9333 [============================>.] - ETA: 39s - loss: 0.7087 - acc: 0.5189
9152/9333 [============================>.] - ETA: 29s - loss: 0.7088 - acc: 0.5187
9216/9333 [============================>.] - ETA: 18s - loss: 0.7088 - acc: 0.5186
9280/9333 [============================>.] - ETA: 8s - loss: 0.7087 - acc: 0.5182 
9333/9333 [==============================] - 1559s 167ms/step - loss: 0.7087 - acc: 0.5179 - val_loss: 0.6939 - val_acc: 0.5014

Epoch 00001: val_acc improved from -inf to 0.50145, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window17/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 2/10

  64/9333 [..............................] - ETA: 25:28 - loss: 0.6734 - acc: 0.5312
 128/9333 [..............................] - ETA: 25:34 - loss: 0.6736 - acc: 0.5703
 192/9333 [..............................] - ETA: 25:23 - loss: 0.6908 - acc: 0.5469
 256/9333 [..............................] - ETA: 24:59 - loss: 0.6916 - acc: 0.5430
 320/9333 [>.............................] - ETA: 24:28 - loss: 0.6893 - acc: 0.5469
 384/9333 [>.............................] - ETA: 24:02 - loss: 0.6942 - acc: 0.5286
 448/9333 [>.............................] - ETA: 24:11 - loss: 0.6968 - acc: 0.5223
 512/9333 [>.............................] - ETA: 24:02 - loss: 0.6983 - acc: 0.5117
 576/9333 [>.............................] - ETA: 23:55 - loss: 0.6978 - acc: 0.5156
 640/9333 [=>............................] - ETA: 24:09 - loss: 0.6940 - acc: 0.5219
 704/9333 [=>............................] - ETA: 24:04 - loss: 0.6943 - acc: 0.5199
 768/9333 [=>............................] - ETA: 23:59 - loss: 0.6951 - acc: 0.5182
 832/9333 [=>............................] - ETA: 23:48 - loss: 0.6966 - acc: 0.5108
 896/9333 [=>............................] - ETA: 23:40 - loss: 0.6981 - acc: 0.5112
 960/9333 [==>...........................] - ETA: 23:28 - loss: 0.6976 - acc: 0.5135
1024/9333 [==>...........................] - ETA: 23:19 - loss: 0.6975 - acc: 0.5137
1088/9333 [==>...........................] - ETA: 23:06 - loss: 0.6981 - acc: 0.5147
1152/9333 [==>...........................] - ETA: 23:00 - loss: 0.6986 - acc: 0.5113
1216/9333 [==>...........................] - ETA: 22:43 - loss: 0.6987 - acc: 0.5107
1280/9333 [===>..........................] - ETA: 22:33 - loss: 0.6996 - acc: 0.5086
1344/9333 [===>..........................] - ETA: 22:18 - loss: 0.6979 - acc: 0.5097
1408/9333 [===>..........................] - ETA: 22:02 - loss: 0.6986 - acc: 0.5085
1472/9333 [===>..........................] - ETA: 21:48 - loss: 0.6981 - acc: 0.5095
1536/9333 [===>..........................] - ETA: 21:37 - loss: 0.6969 - acc: 0.5137
1600/9333 [====>.........................] - ETA: 21:25 - loss: 0.6978 - acc: 0.5156
1664/9333 [====>.........................] - ETA: 21:14 - loss: 0.6988 - acc: 0.5144
1728/9333 [====>.........................] - ETA: 20:58 - loss: 0.6987 - acc: 0.5174
1792/9333 [====>.........................] - ETA: 20:45 - loss: 0.6980 - acc: 0.5179
1856/9333 [====>.........................] - ETA: 20:34 - loss: 0.6987 - acc: 0.5156
1920/9333 [=====>........................] - ETA: 20:21 - loss: 0.7006 - acc: 0.5109
1984/9333 [=====>........................] - ETA: 20:09 - loss: 0.7003 - acc: 0.5096
2048/9333 [=====>........................] - ETA: 19:58 - loss: 0.6997 - acc: 0.5127
2112/9333 [=====>........................] - ETA: 19:47 - loss: 0.7001 - acc: 0.5128
2176/9333 [=====>........................] - ETA: 19:38 - loss: 0.7002 - acc: 0.5115
2240/9333 [======>.......................] - ETA: 19:28 - loss: 0.7014 - acc: 0.5098
2304/9333 [======>.......................] - ETA: 19:15 - loss: 0.7009 - acc: 0.5122
2368/9333 [======>.......................] - ETA: 19:06 - loss: 0.7010 - acc: 0.5093
2432/9333 [======>.......................] - ETA: 18:55 - loss: 0.7006 - acc: 0.5107
2496/9333 [=======>......................] - ETA: 18:48 - loss: 0.7002 - acc: 0.5104
2560/9333 [=======>......................] - ETA: 18:37 - loss: 0.7009 - acc: 0.5094
2624/9333 [=======>......................] - ETA: 18:26 - loss: 0.7005 - acc: 0.5111
2688/9333 [=======>......................] - ETA: 18:16 - loss: 0.7010 - acc: 0.5104
2752/9333 [=======>......................] - ETA: 18:06 - loss: 0.7003 - acc: 0.5134
2816/9333 [========>.....................] - ETA: 17:53 - loss: 0.7006 - acc: 0.5131
2880/9333 [========>.....................] - ETA: 17:43 - loss: 0.7004 - acc: 0.5142
2944/9333 [========>.....................] - ETA: 17:32 - loss: 0.7008 - acc: 0.5143
3008/9333 [========>.....................] - ETA: 17:21 - loss: 0.7002 - acc: 0.5163
3072/9333 [========>.....................] - ETA: 17:10 - loss: 0.6999 - acc: 0.5160
3136/9333 [=========>....................] - ETA: 17:00 - loss: 0.6997 - acc: 0.5166
3200/9333 [=========>....................] - ETA: 16:48 - loss: 0.6990 - acc: 0.5188
3264/9333 [=========>....................] - ETA: 16:36 - loss: 0.6994 - acc: 0.5178
3328/9333 [=========>....................] - ETA: 16:24 - loss: 0.6990 - acc: 0.5186
3392/9333 [=========>....................] - ETA: 16:15 - loss: 0.6988 - acc: 0.5189
3456/9333 [==========>...................] - ETA: 16:04 - loss: 0.6986 - acc: 0.5182
3520/9333 [==========>...................] - ETA: 15:54 - loss: 0.6986 - acc: 0.5182
3584/9333 [==========>...................] - ETA: 15:44 - loss: 0.6991 - acc: 0.5165
3648/9333 [==========>...................] - ETA: 15:32 - loss: 0.6990 - acc: 0.5167
3712/9333 [==========>...................] - ETA: 15:21 - loss: 0.6993 - acc: 0.5162
3776/9333 [===========>..................] - ETA: 15:11 - loss: 0.6992 - acc: 0.5177
3840/9333 [===========>..................] - ETA: 15:01 - loss: 0.6991 - acc: 0.5169
3904/9333 [===========>..................] - ETA: 14:49 - loss: 0.6989 - acc: 0.5174
3968/9333 [===========>..................] - ETA: 14:37 - loss: 0.6993 - acc: 0.5161
4032/9333 [===========>..................] - ETA: 14:25 - loss: 0.6986 - acc: 0.5176
4096/9333 [============>.................] - ETA: 14:14 - loss: 0.6988 - acc: 0.5168
4160/9333 [============>.................] - ETA: 14:03 - loss: 0.6987 - acc: 0.5156
4224/9333 [============>.................] - ETA: 13:54 - loss: 0.6984 - acc: 0.5168
4288/9333 [============>.................] - ETA: 13:43 - loss: 0.6989 - acc: 0.5149
4352/9333 [============>.................] - ETA: 13:31 - loss: 0.6992 - acc: 0.5149
4416/9333 [=============>................] - ETA: 13:20 - loss: 0.6993 - acc: 0.5152
4480/9333 [=============>................] - ETA: 13:10 - loss: 0.6996 - acc: 0.5147
4544/9333 [=============>................] - ETA: 13:00 - loss: 0.6993 - acc: 0.5154
4608/9333 [=============>................] - ETA: 12:49 - loss: 0.6991 - acc: 0.5158
4672/9333 [==============>...............] - ETA: 12:38 - loss: 0.6991 - acc: 0.5163
4736/9333 [==============>...............] - ETA: 12:28 - loss: 0.6991 - acc: 0.5158
4800/9333 [==============>...............] - ETA: 12:17 - loss: 0.6990 - acc: 0.5165
4864/9333 [==============>...............] - ETA: 12:06 - loss: 0.6991 - acc: 0.5154
4928/9333 [==============>...............] - ETA: 11:55 - loss: 0.6991 - acc: 0.5164
4992/9333 [===============>..............] - ETA: 11:44 - loss: 0.6990 - acc: 0.5174
5056/9333 [===============>..............] - ETA: 11:34 - loss: 0.6986 - acc: 0.5188
5120/9333 [===============>..............] - ETA: 11:24 - loss: 0.6985 - acc: 0.5189
5184/9333 [===============>..............] - ETA: 11:13 - loss: 0.6984 - acc: 0.5195
5248/9333 [===============>..............] - ETA: 11:02 - loss: 0.6983 - acc: 0.5192
5312/9333 [================>.............] - ETA: 10:52 - loss: 0.6986 - acc: 0.5192
5376/9333 [================>.............] - ETA: 10:40 - loss: 0.6986 - acc: 0.5190
5440/9333 [================>.............] - ETA: 10:30 - loss: 0.6984 - acc: 0.5191
5504/9333 [================>.............] - ETA: 10:20 - loss: 0.6985 - acc: 0.5191
5568/9333 [================>.............] - ETA: 10:10 - loss: 0.6986 - acc: 0.5185
5632/9333 [=================>............] - ETA: 9:59 - loss: 0.6984 - acc: 0.5183 
5696/9333 [=================>............] - ETA: 9:49 - loss: 0.6982 - acc: 0.5184
5760/9333 [=================>............] - ETA: 9:38 - loss: 0.6981 - acc: 0.5186
5824/9333 [=================>............] - ETA: 9:27 - loss: 0.6981 - acc: 0.5185
5888/9333 [=================>............] - ETA: 9:16 - loss: 0.6979 - acc: 0.5190
5952/9333 [==================>...........] - ETA: 9:05 - loss: 0.6982 - acc: 0.5186
6016/9333 [==================>...........] - ETA: 8:55 - loss: 0.6986 - acc: 0.5175
6080/9333 [==================>...........] - ETA: 8:45 - loss: 0.6986 - acc: 0.5179
6144/9333 [==================>...........] - ETA: 8:34 - loss: 0.6985 - acc: 0.5182
6208/9333 [==================>...........] - ETA: 8:24 - loss: 0.6986 - acc: 0.5184
6272/9333 [===================>..........] - ETA: 8:13 - loss: 0.6985 - acc: 0.5185
6336/9333 [===================>..........] - ETA: 8:02 - loss: 0.6988 - acc: 0.5175
6400/9333 [===================>..........] - ETA: 7:52 - loss: 0.6988 - acc: 0.5177
6464/9333 [===================>..........] - ETA: 7:42 - loss: 0.6985 - acc: 0.5189
6528/9333 [===================>..........] - ETA: 7:32 - loss: 0.6984 - acc: 0.5199
6592/9333 [====================>.........] - ETA: 7:22 - loss: 0.6982 - acc: 0.5209
6656/9333 [====================>.........] - ETA: 7:12 - loss: 0.6981 - acc: 0.5213
6720/9333 [====================>.........] - ETA: 7:02 - loss: 0.6978 - acc: 0.5217
6784/9333 [====================>.........] - ETA: 6:51 - loss: 0.6977 - acc: 0.5218
6848/9333 [=====================>........] - ETA: 6:41 - loss: 0.6977 - acc: 0.5222
6912/9333 [=====================>........] - ETA: 6:30 - loss: 0.6981 - acc: 0.5214
6976/9333 [=====================>........] - ETA: 6:20 - loss: 0.6976 - acc: 0.5229
7040/9333 [=====================>........] - ETA: 6:10 - loss: 0.6977 - acc: 0.5224
7104/9333 [=====================>........] - ETA: 6:00 - loss: 0.6976 - acc: 0.5227
7168/9333 [======================>.......] - ETA: 5:49 - loss: 0.6976 - acc: 0.5225
7232/9333 [======================>.......] - ETA: 5:39 - loss: 0.6974 - acc: 0.5238
7296/9333 [======================>.......] - ETA: 5:29 - loss: 0.6975 - acc: 0.5234
7360/9333 [======================>.......] - ETA: 5:18 - loss: 0.6973 - acc: 0.5242
7424/9333 [======================>.......] - ETA: 5:08 - loss: 0.6973 - acc: 0.5248
7488/9333 [=======================>......] - ETA: 4:58 - loss: 0.6973 - acc: 0.5247
7552/9333 [=======================>......] - ETA: 4:47 - loss: 0.6973 - acc: 0.5248
7616/9333 [=======================>......] - ETA: 4:37 - loss: 0.6973 - acc: 0.5249
7680/9333 [=======================>......] - ETA: 4:27 - loss: 0.6972 - acc: 0.5251
7744/9333 [=======================>......] - ETA: 4:16 - loss: 0.6972 - acc: 0.5248
7808/9333 [========================>.....] - ETA: 4:06 - loss: 0.6969 - acc: 0.5255
7872/9333 [========================>.....] - ETA: 3:56 - loss: 0.6972 - acc: 0.5249
7936/9333 [========================>.....] - ETA: 3:46 - loss: 0.6971 - acc: 0.5246
8000/9333 [========================>.....] - ETA: 3:36 - loss: 0.6970 - acc: 0.5248
8064/9333 [========================>.....] - ETA: 3:25 - loss: 0.6970 - acc: 0.5242
8128/9333 [=========================>....] - ETA: 3:15 - loss: 0.6969 - acc: 0.5244
8192/9333 [=========================>....] - ETA: 3:05 - loss: 0.6970 - acc: 0.5238
8256/9333 [=========================>....] - ETA: 2:55 - loss: 0.6971 - acc: 0.5240
8320/9333 [=========================>....] - ETA: 2:44 - loss: 0.6970 - acc: 0.5237
8384/9333 [=========================>....] - ETA: 2:34 - loss: 0.6972 - acc: 0.5236
8448/9333 [==========================>...] - ETA: 2:23 - loss: 0.6972 - acc: 0.5236
8512/9333 [==========================>...] - ETA: 2:13 - loss: 0.6970 - acc: 0.5240
8576/9333 [==========================>...] - ETA: 2:03 - loss: 0.6970 - acc: 0.5239
8640/9333 [==========================>...] - ETA: 1:52 - loss: 0.6971 - acc: 0.5236
8704/9333 [==========================>...] - ETA: 1:42 - loss: 0.6972 - acc: 0.5233
8768/9333 [===========================>..] - ETA: 1:31 - loss: 0.6970 - acc: 0.5241
8832/9333 [===========================>..] - ETA: 1:21 - loss: 0.6970 - acc: 0.5236
8896/9333 [===========================>..] - ETA: 1:10 - loss: 0.6968 - acc: 0.5238
8960/9333 [===========================>..] - ETA: 1:00 - loss: 0.6969 - acc: 0.5241
9024/9333 [============================>.] - ETA: 50s - loss: 0.6969 - acc: 0.5239 
9088/9333 [============================>.] - ETA: 39s - loss: 0.6970 - acc: 0.5232
9152/9333 [============================>.] - ETA: 29s - loss: 0.6970 - acc: 0.5233
9216/9333 [============================>.] - ETA: 18s - loss: 0.6969 - acc: 0.5232
9280/9333 [============================>.] - ETA: 8s - loss: 0.6968 - acc: 0.5237 
9333/9333 [==============================] - 1579s 169ms/step - loss: 0.6968 - acc: 0.5236 - val_loss: 0.6908 - val_acc: 0.5246

Epoch 00002: val_acc improved from 0.50145 to 0.52459, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window17/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 3/10

  64/9333 [..............................] - ETA: 25:53 - loss: 0.6836 - acc: 0.5156
 128/9333 [..............................] - ETA: 25:15 - loss: 0.7025 - acc: 0.4609
 192/9333 [..............................] - ETA: 24:34 - loss: 0.7021 - acc: 0.4792
 256/9333 [..............................] - ETA: 24:19 - loss: 0.6958 - acc: 0.5078
 320/9333 [>.............................] - ETA: 23:53 - loss: 0.6972 - acc: 0.5000
 384/9333 [>.............................] - ETA: 23:43 - loss: 0.6940 - acc: 0.5156
 448/9333 [>.............................] - ETA: 23:38 - loss: 0.6974 - acc: 0.5089
 512/9333 [>.............................] - ETA: 23:07 - loss: 0.6992 - acc: 0.5059
 576/9333 [>.............................] - ETA: 22:59 - loss: 0.7006 - acc: 0.5017
 640/9333 [=>............................] - ETA: 22:54 - loss: 0.6983 - acc: 0.5062
 704/9333 [=>............................] - ETA: 22:40 - loss: 0.6972 - acc: 0.5142
 768/9333 [=>............................] - ETA: 22:36 - loss: 0.6985 - acc: 0.5065
 832/9333 [=>............................] - ETA: 22:36 - loss: 0.6989 - acc: 0.5024
 896/9333 [=>............................] - ETA: 22:23 - loss: 0.6976 - acc: 0.5056
 960/9333 [==>...........................] - ETA: 22:10 - loss: 0.6985 - acc: 0.5042
1024/9333 [==>...........................] - ETA: 21:54 - loss: 0.6967 - acc: 0.5107
1088/9333 [==>...........................] - ETA: 21:45 - loss: 0.6949 - acc: 0.5184
1152/9333 [==>...........................] - ETA: 21:37 - loss: 0.6950 - acc: 0.5191
1216/9333 [==>...........................] - ETA: 21:30 - loss: 0.6946 - acc: 0.5214
1280/9333 [===>..........................] - ETA: 21:24 - loss: 0.6943 - acc: 0.5234
1344/9333 [===>..........................] - ETA: 21:17 - loss: 0.6932 - acc: 0.5290
1408/9333 [===>..........................] - ETA: 21:07 - loss: 0.6923 - acc: 0.5327
1472/9333 [===>..........................] - ETA: 20:54 - loss: 0.6920 - acc: 0.5319
1536/9333 [===>..........................] - ETA: 20:42 - loss: 0.6917 - acc: 0.5339
1600/9333 [====>.........................] - ETA: 20:36 - loss: 0.6915 - acc: 0.5337
1664/9333 [====>.........................] - ETA: 20:29 - loss: 0.6917 - acc: 0.5331
1728/9333 [====>.........................] - ETA: 20:22 - loss: 0.6923 - acc: 0.5312
1792/9333 [====>.........................] - ETA: 20:12 - loss: 0.6924 - acc: 0.5318
1856/9333 [====>.........................] - ETA: 19:59 - loss: 0.6922 - acc: 0.5339
1920/9333 [=====>........................] - ETA: 19:55 - loss: 0.6927 - acc: 0.5359
1984/9333 [=====>........................] - ETA: 19:47 - loss: 0.6933 - acc: 0.5358
2048/9333 [=====>........................] - ETA: 19:39 - loss: 0.6922 - acc: 0.5400
2112/9333 [=====>........................] - ETA: 19:27 - loss: 0.6916 - acc: 0.5417
2176/9333 [=====>........................] - ETA: 19:21 - loss: 0.6923 - acc: 0.5404
2240/9333 [======>.......................] - ETA: 19:14 - loss: 0.6923 - acc: 0.5397
2304/9333 [======>.......................] - ETA: 19:06 - loss: 0.6934 - acc: 0.5386
2368/9333 [======>.......................] - ETA: 18:58 - loss: 0.6944 - acc: 0.5363
2432/9333 [======>.......................] - ETA: 18:50 - loss: 0.6963 - acc: 0.5325
2496/9333 [=======>......................] - ETA: 18:39 - loss: 0.6959 - acc: 0.5321
2560/9333 [=======>......................] - ETA: 18:29 - loss: 0.6965 - acc: 0.5305
2624/9333 [=======>......................] - ETA: 18:19 - loss: 0.6969 - acc: 0.5290
2688/9333 [=======>......................] - ETA: 18:07 - loss: 0.6969 - acc: 0.5283
2752/9333 [=======>......................] - ETA: 17:56 - loss: 0.6975 - acc: 0.5273
2816/9333 [========>.....................] - ETA: 17:49 - loss: 0.6977 - acc: 0.5266
2880/9333 [========>.....................] - ETA: 17:38 - loss: 0.6975 - acc: 0.5257
2944/9333 [========>.....................] - ETA: 17:27 - loss: 0.6977 - acc: 0.5262
3008/9333 [========>.....................] - ETA: 17:16 - loss: 0.6976 - acc: 0.5256
3072/9333 [========>.....................] - ETA: 17:06 - loss: 0.6979 - acc: 0.5234
3136/9333 [=========>....................] - ETA: 16:56 - loss: 0.6977 - acc: 0.5239
3200/9333 [=========>....................] - ETA: 16:45 - loss: 0.6977 - acc: 0.5244
3264/9333 [=========>....................] - ETA: 16:37 - loss: 0.6974 - acc: 0.5245
3328/9333 [=========>....................] - ETA: 16:28 - loss: 0.6981 - acc: 0.5210
3392/9333 [=========>....................] - ETA: 16:16 - loss: 0.6979 - acc: 0.5209
3456/9333 [==========>...................] - ETA: 16:05 - loss: 0.6974 - acc: 0.5240
3520/9333 [==========>...................] - ETA: 15:56 - loss: 0.6973 - acc: 0.5227
3584/9333 [==========>...................] - ETA: 15:46 - loss: 0.6969 - acc: 0.5229
3648/9333 [==========>...................] - ETA: 15:36 - loss: 0.6966 - acc: 0.5238
3712/9333 [==========>...................] - ETA: 15:24 - loss: 0.6968 - acc: 0.5229
3776/9333 [===========>..................] - ETA: 15:14 - loss: 0.6969 - acc: 0.5222
3840/9333 [===========>..................] - ETA: 15:03 - loss: 0.6966 - acc: 0.5229
3904/9333 [===========>..................] - ETA: 14:52 - loss: 0.6966 - acc: 0.5233
3968/9333 [===========>..................] - ETA: 14:42 - loss: 0.6969 - acc: 0.5224
4032/9333 [===========>..................] - ETA: 14:33 - loss: 0.6968 - acc: 0.5241
4096/9333 [============>.................] - ETA: 14:23 - loss: 0.6968 - acc: 0.5247
4160/9333 [============>.................] - ETA: 14:13 - loss: 0.6970 - acc: 0.5236
4224/9333 [============>.................] - ETA: 14:04 - loss: 0.6970 - acc: 0.5225
4288/9333 [============>.................] - ETA: 13:52 - loss: 0.6969 - acc: 0.5231
4352/9333 [============>.................] - ETA: 13:41 - loss: 0.6969 - acc: 0.5230
4416/9333 [=============>................] - ETA: 13:30 - loss: 0.6971 - acc: 0.5217
4480/9333 [=============>................] - ETA: 13:18 - loss: 0.6975 - acc: 0.5201
4544/9333 [=============>................] - ETA: 13:07 - loss: 0.6971 - acc: 0.5209
4608/9333 [=============>................] - ETA: 12:57 - loss: 0.6969 - acc: 0.5213
4672/9333 [==============>...............] - ETA: 12:46 - loss: 0.6970 - acc: 0.5210
4736/9333 [==============>...............] - ETA: 12:35 - loss: 0.6968 - acc: 0.5222
4800/9333 [==============>...............] - ETA: 12:25 - loss: 0.6966 - acc: 0.5235
4864/9333 [==============>...............] - ETA: 12:14 - loss: 0.6965 - acc: 0.5232
4928/9333 [==============>...............] - ETA: 12:03 - loss: 0.6963 - acc: 0.5235
4992/9333 [===============>..............] - ETA: 11:52 - loss: 0.6965 - acc: 0.5224
5056/9333 [===============>..............] - ETA: 11:41 - loss: 0.6964 - acc: 0.5233
5120/9333 [===============>..............] - ETA: 11:30 - loss: 0.6965 - acc: 0.5227
5184/9333 [===============>..............] - ETA: 11:19 - loss: 0.6966 - acc: 0.5220
5248/9333 [===============>..............] - ETA: 11:08 - loss: 0.6966 - acc: 0.5219
5312/9333 [================>.............] - ETA: 10:56 - loss: 0.6963 - acc: 0.5228
5376/9333 [================>.............] - ETA: 10:45 - loss: 0.6964 - acc: 0.5227
5440/9333 [================>.............] - ETA: 10:34 - loss: 0.6966 - acc: 0.5219
5504/9333 [================>.............] - ETA: 10:24 - loss: 0.6967 - acc: 0.5214
5568/9333 [================>.............] - ETA: 10:14 - loss: 0.6968 - acc: 0.5216
5632/9333 [=================>............] - ETA: 10:03 - loss: 0.6966 - acc: 0.5220
5696/9333 [=================>............] - ETA: 9:53 - loss: 0.6966 - acc: 0.5221 
5760/9333 [=================>............] - ETA: 9:42 - loss: 0.6963 - acc: 0.5224
5824/9333 [=================>............] - ETA: 9:32 - loss: 0.6964 - acc: 0.5211
5888/9333 [=================>............] - ETA: 9:21 - loss: 0.6967 - acc: 0.5200
5952/9333 [==================>...........] - ETA: 9:10 - loss: 0.6965 - acc: 0.5200
6016/9333 [==================>...........] - ETA: 9:00 - loss: 0.6968 - acc: 0.5191
6080/9333 [==================>...........] - ETA: 8:50 - loss: 0.6968 - acc: 0.5194
6144/9333 [==================>...........] - ETA: 8:39 - loss: 0.6971 - acc: 0.5187
6208/9333 [==================>...........] - ETA: 8:29 - loss: 0.6970 - acc: 0.5192
6272/9333 [===================>..........] - ETA: 8:18 - loss: 0.6970 - acc: 0.5183
6336/9333 [===================>..........] - ETA: 8:08 - loss: 0.6969 - acc: 0.5188
6400/9333 [===================>..........] - ETA: 7:58 - loss: 0.6968 - acc: 0.5200
6464/9333 [===================>..........] - ETA: 7:47 - loss: 0.6967 - acc: 0.5198
6528/9333 [===================>..........] - ETA: 7:37 - loss: 0.6969 - acc: 0.5190
6592/9333 [====================>.........] - ETA: 7:26 - loss: 0.6972 - acc: 0.5184
6656/9333 [====================>.........] - ETA: 7:16 - loss: 0.6973 - acc: 0.5182
6720/9333 [====================>.........] - ETA: 7:05 - loss: 0.6974 - acc: 0.5176
6784/9333 [====================>.........] - ETA: 6:55 - loss: 0.6973 - acc: 0.5178
6848/9333 [=====================>........] - ETA: 6:44 - loss: 0.6973 - acc: 0.5178
6912/9333 [=====================>........] - ETA: 6:34 - loss: 0.6973 - acc: 0.5179
6976/9333 [=====================>........] - ETA: 6:23 - loss: 0.6971 - acc: 0.5185
7040/9333 [=====================>........] - ETA: 6:13 - loss: 0.6970 - acc: 0.5192
7104/9333 [=====================>........] - ETA: 6:02 - loss: 0.6968 - acc: 0.5201
7168/9333 [======================>.......] - ETA: 5:52 - loss: 0.6967 - acc: 0.5202
7232/9333 [======================>.......] - ETA: 5:42 - loss: 0.6965 - acc: 0.5203
7296/9333 [======================>.......] - ETA: 5:32 - loss: 0.6964 - acc: 0.5211
7360/9333 [======================>.......] - ETA: 5:21 - loss: 0.6964 - acc: 0.5217
7424/9333 [======================>.......] - ETA: 5:11 - loss: 0.6963 - acc: 0.5222
7488/9333 [=======================>......] - ETA: 5:00 - loss: 0.6963 - acc: 0.5224
7552/9333 [=======================>......] - ETA: 4:50 - loss: 0.6966 - acc: 0.5217
7616/9333 [=======================>......] - ETA: 4:40 - loss: 0.6967 - acc: 0.5210
7680/9333 [=======================>......] - ETA: 4:29 - loss: 0.6967 - acc: 0.5206
7744/9333 [=======================>......] - ETA: 4:19 - loss: 0.6969 - acc: 0.5203
7808/9333 [========================>.....] - ETA: 4:08 - loss: 0.6968 - acc: 0.5204
7872/9333 [========================>.....] - ETA: 3:58 - loss: 0.6967 - acc: 0.5206
7936/9333 [========================>.....] - ETA: 3:48 - loss: 0.6967 - acc: 0.5207
8000/9333 [========================>.....] - ETA: 3:37 - loss: 0.6968 - acc: 0.5198
8064/9333 [========================>.....] - ETA: 3:27 - loss: 0.6968 - acc: 0.5196
8128/9333 [=========================>....] - ETA: 3:16 - loss: 0.6970 - acc: 0.5189
8192/9333 [=========================>....] - ETA: 3:06 - loss: 0.6970 - acc: 0.5189
8256/9333 [=========================>....] - ETA: 2:55 - loss: 0.6970 - acc: 0.5193
8320/9333 [=========================>....] - ETA: 2:45 - loss: 0.6968 - acc: 0.5192
8384/9333 [=========================>....] - ETA: 2:34 - loss: 0.6968 - acc: 0.5190
8448/9333 [==========================>...] - ETA: 2:24 - loss: 0.6966 - acc: 0.5193
8512/9333 [==========================>...] - ETA: 2:13 - loss: 0.6965 - acc: 0.5202
8576/9333 [==========================>...] - ETA: 2:03 - loss: 0.6964 - acc: 0.5206
8640/9333 [==========================>...] - ETA: 1:52 - loss: 0.6965 - acc: 0.5206
8704/9333 [==========================>...] - ETA: 1:42 - loss: 0.6963 - acc: 0.5213
8768/9333 [===========================>..] - ETA: 1:32 - loss: 0.6963 - acc: 0.5210
8832/9333 [===========================>..] - ETA: 1:21 - loss: 0.6963 - acc: 0.5214
8896/9333 [===========================>..] - ETA: 1:11 - loss: 0.6962 - acc: 0.5216
8960/9333 [===========================>..] - ETA: 1:00 - loss: 0.6963 - acc: 0.5209
9024/9333 [============================>.] - ETA: 50s - loss: 0.6961 - acc: 0.5213 
9088/9333 [============================>.] - ETA: 39s - loss: 0.6961 - acc: 0.5211
9152/9333 [============================>.] - ETA: 29s - loss: 0.6961 - acc: 0.5212
9216/9333 [============================>.] - ETA: 19s - loss: 0.6962 - acc: 0.5208
9280/9333 [============================>.] - ETA: 8s - loss: 0.6961 - acc: 0.5213 
9333/9333 [==============================] - 1586s 170ms/step - loss: 0.6961 - acc: 0.5214 - val_loss: 0.6901 - val_acc: 0.5236

Epoch 00003: val_acc did not improve from 0.52459
Epoch 4/10

  64/9333 [..............................] - ETA: 25:40 - loss: 0.6857 - acc: 0.5469
 128/9333 [..............................] - ETA: 25:19 - loss: 0.6862 - acc: 0.5312
 192/9333 [..............................] - ETA: 25:42 - loss: 0.6870 - acc: 0.5365
 256/9333 [..............................] - ETA: 25:52 - loss: 0.6836 - acc: 0.5430
 320/9333 [>.............................] - ETA: 25:19 - loss: 0.6838 - acc: 0.5500
 384/9333 [>.............................] - ETA: 25:01 - loss: 0.6854 - acc: 0.5339
 448/9333 [>.............................] - ETA: 25:20 - loss: 0.6832 - acc: 0.5469
 512/9333 [>.............................] - ETA: 25:09 - loss: 0.6834 - acc: 0.5352
 576/9333 [>.............................] - ETA: 25:04 - loss: 0.6800 - acc: 0.5469
 640/9333 [=>............................] - ETA: 24:56 - loss: 0.6820 - acc: 0.5453
 704/9333 [=>............................] - ETA: 24:48 - loss: 0.6845 - acc: 0.5384
 768/9333 [=>............................] - ETA: 24:42 - loss: 0.6865 - acc: 0.5312
 832/9333 [=>............................] - ETA: 24:43 - loss: 0.6892 - acc: 0.5252
 896/9333 [=>............................] - ETA: 24:36 - loss: 0.6890 - acc: 0.5257
 960/9333 [==>...........................] - ETA: 24:28 - loss: 0.6906 - acc: 0.5260
1024/9333 [==>...........................] - ETA: 24:32 - loss: 0.6913 - acc: 0.5244
1088/9333 [==>...........................] - ETA: 24:30 - loss: 0.6912 - acc: 0.5257
1152/9333 [==>...........................] - ETA: 24:19 - loss: 0.6914 - acc: 0.5260
1216/9333 [==>...........................] - ETA: 24:01 - loss: 0.6915 - acc: 0.5247
1280/9333 [===>..........................] - ETA: 23:53 - loss: 0.6915 - acc: 0.5211
1344/9333 [===>..........................] - ETA: 23:39 - loss: 0.6923 - acc: 0.5216
1408/9333 [===>..........................] - ETA: 23:31 - loss: 0.6921 - acc: 0.5234
1472/9333 [===>..........................] - ETA: 23:20 - loss: 0.6924 - acc: 0.5238
1536/9333 [===>..........................] - ETA: 23:12 - loss: 0.6929 - acc: 0.5215
1600/9333 [====>.........................] - ETA: 22:53 - loss: 0.6926 - acc: 0.5238
1664/9333 [====>.........................] - ETA: 22:44 - loss: 0.6920 - acc: 0.5294
1728/9333 [====>.........................] - ETA: 22:31 - loss: 0.6922 - acc: 0.5289
1792/9333 [====>.........................] - ETA: 22:21 - loss: 0.6929 - acc: 0.5262
1856/9333 [====>.........................] - ETA: 22:04 - loss: 0.6933 - acc: 0.5253
1920/9333 [=====>........................] - ETA: 21:49 - loss: 0.6937 - acc: 0.5229
1984/9333 [=====>........................] - ETA: 21:38 - loss: 0.6948 - acc: 0.5197
2048/9333 [=====>........................] - ETA: 21:21 - loss: 0.6949 - acc: 0.5195
2112/9333 [=====>........................] - ETA: 21:09 - loss: 0.6946 - acc: 0.5204
2176/9333 [=====>........................] - ETA: 20:58 - loss: 0.6943 - acc: 0.5211
2240/9333 [======>.......................] - ETA: 20:45 - loss: 0.6942 - acc: 0.5210
2304/9333 [======>.......................] - ETA: 20:32 - loss: 0.6939 - acc: 0.5208
2368/9333 [======>.......................] - ETA: 20:21 - loss: 0.6936 - acc: 0.5241
2432/9333 [======>.......................] - ETA: 20:09 - loss: 0.6932 - acc: 0.5259
2496/9333 [=======>......................] - ETA: 19:56 - loss: 0.6932 - acc: 0.5244
2560/9333 [=======>......................] - ETA: 19:42 - loss: 0.6935 - acc: 0.5230
2624/9333 [=======>......................] - ETA: 19:28 - loss: 0.6935 - acc: 0.5255
2688/9333 [=======>......................] - ETA: 19:17 - loss: 0.6935 - acc: 0.5246
2752/9333 [=======>......................] - ETA: 19:06 - loss: 0.6933 - acc: 0.5254
2816/9333 [========>.....................] - ETA: 18:54 - loss: 0.6925 - acc: 0.5277
2880/9333 [========>.....................] - ETA: 18:44 - loss: 0.6923 - acc: 0.5278
2944/9333 [========>.....................] - ETA: 18:32 - loss: 0.6919 - acc: 0.5292
3008/9333 [========>.....................] - ETA: 18:21 - loss: 0.6917 - acc: 0.5283
3072/9333 [========>.....................] - ETA: 18:10 - loss: 0.6916 - acc: 0.5293
3136/9333 [=========>....................] - ETA: 17:58 - loss: 0.6919 - acc: 0.5281
3200/9333 [=========>....................] - ETA: 17:46 - loss: 0.6918 - acc: 0.5284
3264/9333 [=========>....................] - ETA: 17:34 - loss: 0.6917 - acc: 0.5273
3328/9333 [=========>....................] - ETA: 17:24 - loss: 0.6917 - acc: 0.5279
3392/9333 [=========>....................] - ETA: 17:12 - loss: 0.6916 - acc: 0.5295
3456/9333 [==========>...................] - ETA: 17:02 - loss: 0.6921 - acc: 0.5292
3520/9333 [==========>...................] - ETA: 16:52 - loss: 0.6920 - acc: 0.5307
3584/9333 [==========>...................] - ETA: 16:39 - loss: 0.6921 - acc: 0.5304
3648/9333 [==========>...................] - ETA: 16:28 - loss: 0.6922 - acc: 0.5293
3712/9333 [==========>...................] - ETA: 16:17 - loss: 0.6920 - acc: 0.5296
3776/9333 [===========>..................] - ETA: 16:05 - loss: 0.6922 - acc: 0.5299
3840/9333 [===========>..................] - ETA: 15:53 - loss: 0.6928 - acc: 0.5286
3904/9333 [===========>..................] - ETA: 15:43 - loss: 0.6929 - acc: 0.5289
3968/9333 [===========>..................] - ETA: 15:31 - loss: 0.6930 - acc: 0.5287
4032/9333 [===========>..................] - ETA: 15:18 - loss: 0.6931 - acc: 0.5280
4096/9333 [============>.................] - ETA: 15:07 - loss: 0.6929 - acc: 0.5276
4160/9333 [============>.................] - ETA: 14:55 - loss: 0.6930 - acc: 0.5272
4224/9333 [============>.................] - ETA: 14:43 - loss: 0.6934 - acc: 0.5253
4288/9333 [============>.................] - ETA: 14:31 - loss: 0.6932 - acc: 0.5266
4352/9333 [============>.................] - ETA: 14:20 - loss: 0.6933 - acc: 0.5264
4416/9333 [=============>................] - ETA: 14:08 - loss: 0.6933 - acc: 0.5272
4480/9333 [=============>................] - ETA: 13:56 - loss: 0.6932 - acc: 0.5275
4544/9333 [=============>................] - ETA: 13:45 - loss: 0.6934 - acc: 0.5273
4608/9333 [=============>................] - ETA: 13:33 - loss: 0.6930 - acc: 0.5293
4672/9333 [==============>...............] - ETA: 13:22 - loss: 0.6929 - acc: 0.5298
4736/9333 [==============>...............] - ETA: 13:11 - loss: 0.6935 - acc: 0.5291
4800/9333 [==============>...............] - ETA: 13:00 - loss: 0.6934 - acc: 0.5294
4864/9333 [==============>...............] - ETA: 12:49 - loss: 0.6937 - acc: 0.5284
4928/9333 [==============>...............] - ETA: 12:39 - loss: 0.6937 - acc: 0.5282
4992/9333 [===============>..............] - ETA: 12:28 - loss: 0.6934 - acc: 0.5290
5056/9333 [===============>..............] - ETA: 12:17 - loss: 0.6933 - acc: 0.5297
5120/9333 [===============>..............] - ETA: 12:05 - loss: 0.6932 - acc: 0.5295
5184/9333 [===============>..............] - ETA: 11:53 - loss: 0.6926 - acc: 0.5303
5248/9333 [===============>..............] - ETA: 11:41 - loss: 0.6924 - acc: 0.5307
5312/9333 [================>.............] - ETA: 11:29 - loss: 0.6921 - acc: 0.5312
5376/9333 [================>.............] - ETA: 11:18 - loss: 0.6919 - acc: 0.5314
5440/9333 [================>.............] - ETA: 11:07 - loss: 0.6916 - acc: 0.5324
5504/9333 [================>.............] - ETA: 10:56 - loss: 0.6917 - acc: 0.5322
5568/9333 [================>.............] - ETA: 10:44 - loss: 0.6918 - acc: 0.5316
5632/9333 [=================>............] - ETA: 10:32 - loss: 0.6920 - acc: 0.5305
5696/9333 [=================>............] - ETA: 10:20 - loss: 0.6921 - acc: 0.5309
5760/9333 [=================>............] - ETA: 10:09 - loss: 0.6922 - acc: 0.5307
5824/9333 [=================>............] - ETA: 9:58 - loss: 0.6922 - acc: 0.5312 
5888/9333 [=================>............] - ETA: 9:47 - loss: 0.6923 - acc: 0.5307
5952/9333 [==================>...........] - ETA: 9:36 - loss: 0.6925 - acc: 0.5304
6016/9333 [==================>...........] - ETA: 9:26 - loss: 0.6926 - acc: 0.5303
6080/9333 [==================>...........] - ETA: 9:14 - loss: 0.6922 - acc: 0.5306
6144/9333 [==================>...........] - ETA: 9:03 - loss: 0.6926 - acc: 0.5293
6208/9333 [==================>...........] - ETA: 8:52 - loss: 0.6927 - acc: 0.5287
6272/9333 [===================>..........] - ETA: 8:41 - loss: 0.6927 - acc: 0.5281
6336/9333 [===================>..........] - ETA: 8:30 - loss: 0.6927 - acc: 0.5281
6400/9333 [===================>..........] - ETA: 8:19 - loss: 0.6928 - acc: 0.5281
6464/9333 [===================>..........] - ETA: 8:08 - loss: 0.6929 - acc: 0.5278
6528/9333 [===================>..........] - ETA: 7:57 - loss: 0.6930 - acc: 0.5279
6592/9333 [====================>.........] - ETA: 7:45 - loss: 0.6932 - acc: 0.5272
6656/9333 [====================>.........] - ETA: 7:34 - loss: 0.6933 - acc: 0.5264
6720/9333 [====================>.........] - ETA: 7:23 - loss: 0.6932 - acc: 0.5263
6784/9333 [====================>.........] - ETA: 7:12 - loss: 0.6934 - acc: 0.5255
6848/9333 [=====================>........] - ETA: 7:01 - loss: 0.6935 - acc: 0.5251
6912/9333 [=====================>........] - ETA: 6:50 - loss: 0.6936 - acc: 0.5247
6976/9333 [=====================>........] - ETA: 6:39 - loss: 0.6937 - acc: 0.5244
7040/9333 [=====================>........] - ETA: 6:28 - loss: 0.6937 - acc: 0.5250
7104/9333 [=====================>........] - ETA: 6:17 - loss: 0.6936 - acc: 0.5251
7168/9333 [======================>.......] - ETA: 6:06 - loss: 0.6936 - acc: 0.5253
7232/9333 [======================>.......] - ETA: 5:55 - loss: 0.6934 - acc: 0.5259
7296/9333 [======================>.......] - ETA: 5:44 - loss: 0.6935 - acc: 0.5249
7360/9333 [======================>.......] - ETA: 5:33 - loss: 0.6935 - acc: 0.5250
7424/9333 [======================>.......] - ETA: 5:22 - loss: 0.6936 - acc: 0.5244
7488/9333 [=======================>......] - ETA: 5:11 - loss: 0.6936 - acc: 0.5246
7552/9333 [=======================>......] - ETA: 5:00 - loss: 0.6937 - acc: 0.5248
7616/9333 [=======================>......] - ETA: 4:49 - loss: 0.6938 - acc: 0.5242
7680/9333 [=======================>......] - ETA: 4:38 - loss: 0.6939 - acc: 0.5236
7744/9333 [=======================>......] - ETA: 4:27 - loss: 0.6938 - acc: 0.5240
7808/9333 [========================>.....] - ETA: 4:17 - loss: 0.6936 - acc: 0.5241
7872/9333 [========================>.....] - ETA: 4:05 - loss: 0.6936 - acc: 0.5239
7936/9333 [========================>.....] - ETA: 3:55 - loss: 0.6938 - acc: 0.5231
8000/9333 [========================>.....] - ETA: 3:44 - loss: 0.6938 - acc: 0.5228
8064/9333 [========================>.....] - ETA: 3:33 - loss: 0.6937 - acc: 0.5231
8128/9333 [=========================>....] - ETA: 3:22 - loss: 0.6937 - acc: 0.5236
8192/9333 [=========================>....] - ETA: 3:11 - loss: 0.6936 - acc: 0.5232
8256/9333 [=========================>....] - ETA: 3:01 - loss: 0.6937 - acc: 0.5228
8320/9333 [=========================>....] - ETA: 2:50 - loss: 0.6937 - acc: 0.5230
8384/9333 [=========================>....] - ETA: 2:39 - loss: 0.6937 - acc: 0.5231
8448/9333 [==========================>...] - ETA: 2:28 - loss: 0.6937 - acc: 0.5232
8512/9333 [==========================>...] - ETA: 2:17 - loss: 0.6937 - acc: 0.5230
8576/9333 [==========================>...] - ETA: 2:06 - loss: 0.6937 - acc: 0.5229
8640/9333 [==========================>...] - ETA: 1:56 - loss: 0.6936 - acc: 0.5234
8704/9333 [==========================>...] - ETA: 1:45 - loss: 0.6935 - acc: 0.5238
8768/9333 [===========================>..] - ETA: 1:34 - loss: 0.6934 - acc: 0.5242
8832/9333 [===========================>..] - ETA: 1:23 - loss: 0.6934 - acc: 0.5241
8896/9333 [===========================>..] - ETA: 1:13 - loss: 0.6934 - acc: 0.5238
8960/9333 [===========================>..] - ETA: 1:02 - loss: 0.6937 - acc: 0.5228
9024/9333 [============================>.] - ETA: 51s - loss: 0.6938 - acc: 0.5224 
9088/9333 [============================>.] - ETA: 40s - loss: 0.6938 - acc: 0.5224
9152/9333 [============================>.] - ETA: 30s - loss: 0.6939 - acc: 0.5228
9216/9333 [============================>.] - ETA: 19s - loss: 0.6938 - acc: 0.5229
9280/9333 [============================>.] - ETA: 8s - loss: 0.6938 - acc: 0.5230 
9333/9333 [==============================] - 1621s 174ms/step - loss: 0.6937 - acc: 0.5235 - val_loss: 0.6900 - val_acc: 0.5342

Epoch 00004: val_acc improved from 0.52459 to 0.53423, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window17/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 5/10

  64/9333 [..............................] - ETA: 24:39 - loss: 0.6783 - acc: 0.5469
 128/9333 [..............................] - ETA: 24:43 - loss: 0.6882 - acc: 0.5391
 192/9333 [..............................] - ETA: 24:56 - loss: 0.6857 - acc: 0.5521
 256/9333 [..............................] - ETA: 24:37 - loss: 0.6854 - acc: 0.5352
 320/9333 [>.............................] - ETA: 24:37 - loss: 0.6869 - acc: 0.5281
 384/9333 [>.............................] - ETA: 24:44 - loss: 0.6836 - acc: 0.5495
 448/9333 [>.............................] - ETA: 24:36 - loss: 0.6872 - acc: 0.5290
 512/9333 [>.............................] - ETA: 24:03 - loss: 0.6895 - acc: 0.5312
 576/9333 [>.............................] - ETA: 24:10 - loss: 0.6888 - acc: 0.5382
 640/9333 [=>............................] - ETA: 23:59 - loss: 0.6885 - acc: 0.5359
 704/9333 [=>............................] - ETA: 23:42 - loss: 0.6891 - acc: 0.5327
 768/9333 [=>............................] - ETA: 23:14 - loss: 0.6893 - acc: 0.5286
 832/9333 [=>............................] - ETA: 23:02 - loss: 0.6903 - acc: 0.5276
 896/9333 [=>............................] - ETA: 22:47 - loss: 0.6907 - acc: 0.5257
 960/9333 [==>...........................] - ETA: 22:39 - loss: 0.6906 - acc: 0.5250
1024/9333 [==>...........................] - ETA: 22:24 - loss: 0.6896 - acc: 0.5312
1088/9333 [==>...........................] - ETA: 22:19 - loss: 0.6894 - acc: 0.5322
1152/9333 [==>...........................] - ETA: 22:06 - loss: 0.6893 - acc: 0.5365
1216/9333 [==>...........................] - ETA: 21:51 - loss: 0.6894 - acc: 0.5329
1280/9333 [===>..........................] - ETA: 21:38 - loss: 0.6883 - acc: 0.5375
1344/9333 [===>..........................] - ETA: 21:23 - loss: 0.6885 - acc: 0.5365
1408/9333 [===>..........................] - ETA: 21:07 - loss: 0.6894 - acc: 0.5320
1472/9333 [===>..........................] - ETA: 20:56 - loss: 0.6895 - acc: 0.5340
1536/9333 [===>..........................] - ETA: 20:44 - loss: 0.6896 - acc: 0.5326
1600/9333 [====>.........................] - ETA: 20:36 - loss: 0.6892 - acc: 0.5337
1664/9333 [====>.........................] - ETA: 20:28 - loss: 0.6880 - acc: 0.5385
1728/9333 [====>.........................] - ETA: 20:24 - loss: 0.6885 - acc: 0.5359
1792/9333 [====>.........................] - ETA: 20:14 - loss: 0.6881 - acc: 0.5363
1856/9333 [====>.........................] - ETA: 20:02 - loss: 0.6877 - acc: 0.5399
1920/9333 [=====>........................] - ETA: 19:52 - loss: 0.6875 - acc: 0.5422
1984/9333 [=====>........................] - ETA: 19:45 - loss: 0.6878 - acc: 0.5403
2048/9333 [=====>........................] - ETA: 19:34 - loss: 0.6879 - acc: 0.5391
2112/9333 [=====>........................] - ETA: 19:22 - loss: 0.6872 - acc: 0.5417
2176/9333 [=====>........................] - ETA: 19:10 - loss: 0.6873 - acc: 0.5418
2240/9333 [======>.......................] - ETA: 19:03 - loss: 0.6878 - acc: 0.5397
2304/9333 [======>.......................] - ETA: 18:54 - loss: 0.6878 - acc: 0.5395
2368/9333 [======>.......................] - ETA: 18:46 - loss: 0.6878 - acc: 0.5405
2432/9333 [======>.......................] - ETA: 18:36 - loss: 0.6877 - acc: 0.5411
2496/9333 [=======>......................] - ETA: 18:26 - loss: 0.6878 - acc: 0.5421
2560/9333 [=======>......................] - ETA: 18:12 - loss: 0.6877 - acc: 0.5414
2624/9333 [=======>......................] - ETA: 18:03 - loss: 0.6871 - acc: 0.5427
2688/9333 [=======>......................] - ETA: 17:51 - loss: 0.6868 - acc: 0.5461
2752/9333 [=======>......................] - ETA: 17:39 - loss: 0.6869 - acc: 0.5469
2816/9333 [========>.....................] - ETA: 17:30 - loss: 0.6872 - acc: 0.5465
2880/9333 [========>.....................] - ETA: 17:20 - loss: 0.6873 - acc: 0.5462
2944/9333 [========>.....................] - ETA: 17:08 - loss: 0.6875 - acc: 0.5445
3008/9333 [========>.....................] - ETA: 16:56 - loss: 0.6877 - acc: 0.5436
3072/9333 [========>.....................] - ETA: 16:45 - loss: 0.6872 - acc: 0.5456
3136/9333 [=========>....................] - ETA: 16:32 - loss: 0.6869 - acc: 0.5456
3200/9333 [=========>....................] - ETA: 16:21 - loss: 0.6876 - acc: 0.5447
3264/9333 [=========>....................] - ETA: 16:10 - loss: 0.6875 - acc: 0.5456
3328/9333 [=========>....................] - ETA: 16:02 - loss: 0.6882 - acc: 0.5424
3392/9333 [=========>....................] - ETA: 15:50 - loss: 0.6880 - acc: 0.5427
3456/9333 [==========>...................] - ETA: 15:39 - loss: 0.6882 - acc: 0.5420
3520/9333 [==========>...................] - ETA: 15:29 - loss: 0.6881 - acc: 0.5423
3584/9333 [==========>...................] - ETA: 15:19 - loss: 0.6878 - acc: 0.5438
3648/9333 [==========>...................] - ETA: 15:09 - loss: 0.6879 - acc: 0.5422
3712/9333 [==========>...................] - ETA: 14:58 - loss: 0.6878 - acc: 0.5434
3776/9333 [===========>..................] - ETA: 14:47 - loss: 0.6879 - acc: 0.5434
3840/9333 [===========>..................] - ETA: 14:37 - loss: 0.6881 - acc: 0.5430
3904/9333 [===========>..................] - ETA: 14:26 - loss: 0.6882 - acc: 0.5435
3968/9333 [===========>..................] - ETA: 14:16 - loss: 0.6881 - acc: 0.5444
4032/9333 [===========>..................] - ETA: 14:07 - loss: 0.6881 - acc: 0.5441
4096/9333 [============>.................] - ETA: 13:57 - loss: 0.6882 - acc: 0.5442
4160/9333 [============>.................] - ETA: 13:48 - loss: 0.6878 - acc: 0.5457
4224/9333 [============>.................] - ETA: 13:38 - loss: 0.6876 - acc: 0.5459
4288/9333 [============>.................] - ETA: 13:29 - loss: 0.6882 - acc: 0.5438
4352/9333 [============>.................] - ETA: 13:19 - loss: 0.6885 - acc: 0.5434
4416/9333 [=============>................] - ETA: 13:10 - loss: 0.6885 - acc: 0.5428
4480/9333 [=============>................] - ETA: 13:01 - loss: 0.6883 - acc: 0.5433
4544/9333 [=============>................] - ETA: 12:50 - loss: 0.6884 - acc: 0.5431
4608/9333 [=============>................] - ETA: 12:41 - loss: 0.6885 - acc: 0.5430
4672/9333 [==============>...............] - ETA: 12:29 - loss: 0.6886 - acc: 0.5417
4736/9333 [==============>...............] - ETA: 12:19 - loss: 0.6886 - acc: 0.5418
4800/9333 [==============>...............] - ETA: 12:10 - loss: 0.6885 - acc: 0.5427
4864/9333 [==============>...............] - ETA: 12:00 - loss: 0.6883 - acc: 0.5434
4928/9333 [==============>...............] - ETA: 11:50 - loss: 0.6880 - acc: 0.5450
4992/9333 [===============>..............] - ETA: 11:41 - loss: 0.6882 - acc: 0.5441
5056/9333 [===============>..............] - ETA: 11:30 - loss: 0.6883 - acc: 0.5439
5120/9333 [===============>..............] - ETA: 11:18 - loss: 0.6885 - acc: 0.5428
5184/9333 [===============>..............] - ETA: 11:07 - loss: 0.6885 - acc: 0.5434
5248/9333 [===============>..............] - ETA: 10:56 - loss: 0.6884 - acc: 0.5436
5312/9333 [================>.............] - ETA: 10:45 - loss: 0.6882 - acc: 0.5442
5376/9333 [================>.............] - ETA: 10:34 - loss: 0.6883 - acc: 0.5439
5440/9333 [================>.............] - ETA: 10:23 - loss: 0.6886 - acc: 0.5423
5504/9333 [================>.............] - ETA: 10:11 - loss: 0.6888 - acc: 0.5412
5568/9333 [================>.............] - ETA: 10:01 - loss: 0.6886 - acc: 0.5426
5632/9333 [=================>............] - ETA: 9:50 - loss: 0.6888 - acc: 0.5419 
5696/9333 [=================>............] - ETA: 9:39 - loss: 0.6889 - acc: 0.5418
5760/9333 [=================>............] - ETA: 9:29 - loss: 0.6892 - acc: 0.5415
5824/9333 [=================>............] - ETA: 9:18 - loss: 0.6892 - acc: 0.5416
5888/9333 [=================>............] - ETA: 9:07 - loss: 0.6895 - acc: 0.5403
5952/9333 [==================>...........] - ETA: 8:57 - loss: 0.6895 - acc: 0.5410
6016/9333 [==================>...........] - ETA: 8:46 - loss: 0.6899 - acc: 0.5406
6080/9333 [==================>...........] - ETA: 8:35 - loss: 0.6901 - acc: 0.5400
6144/9333 [==================>...........] - ETA: 8:25 - loss: 0.6902 - acc: 0.5400
6208/9333 [==================>...........] - ETA: 8:14 - loss: 0.6901 - acc: 0.5404
6272/9333 [===================>..........] - ETA: 8:03 - loss: 0.6904 - acc: 0.5392
6336/9333 [===================>..........] - ETA: 7:53 - loss: 0.6904 - acc: 0.5385
6400/9333 [===================>..........] - ETA: 7:42 - loss: 0.6905 - acc: 0.5384
6464/9333 [===================>..........] - ETA: 7:32 - loss: 0.6903 - acc: 0.5390
6528/9333 [===================>..........] - ETA: 7:21 - loss: 0.6901 - acc: 0.5397
6592/9333 [====================>.........] - ETA: 7:11 - loss: 0.6902 - acc: 0.5399
6656/9333 [====================>.........] - ETA: 7:01 - loss: 0.6899 - acc: 0.5412
6720/9333 [====================>.........] - ETA: 6:50 - loss: 0.6898 - acc: 0.5415
6784/9333 [====================>.........] - ETA: 6:40 - loss: 0.6897 - acc: 0.5417
6848/9333 [=====================>........] - ETA: 6:30 - loss: 0.6897 - acc: 0.5421
6912/9333 [=====================>........] - ETA: 6:19 - loss: 0.6898 - acc: 0.5415
6976/9333 [=====================>........] - ETA: 6:09 - loss: 0.6899 - acc: 0.5411
7040/9333 [=====================>........] - ETA: 5:59 - loss: 0.6902 - acc: 0.5398
7104/9333 [=====================>........] - ETA: 5:49 - loss: 0.6901 - acc: 0.5397
7168/9333 [======================>.......] - ETA: 5:39 - loss: 0.6900 - acc: 0.5395
7232/9333 [======================>.......] - ETA: 5:29 - loss: 0.6902 - acc: 0.5390
7296/9333 [======================>.......] - ETA: 5:19 - loss: 0.6903 - acc: 0.5388
7360/9333 [======================>.......] - ETA: 5:08 - loss: 0.6903 - acc: 0.5393
7424/9333 [======================>.......] - ETA: 4:58 - loss: 0.6902 - acc: 0.5393
7488/9333 [=======================>......] - ETA: 4:48 - loss: 0.6902 - acc: 0.5391
7552/9333 [=======================>......] - ETA: 4:38 - loss: 0.6899 - acc: 0.5401
7616/9333 [=======================>......] - ETA: 4:28 - loss: 0.6898 - acc: 0.5400
7680/9333 [=======================>......] - ETA: 4:18 - loss: 0.6899 - acc: 0.5395
7744/9333 [=======================>......] - ETA: 4:08 - loss: 0.6901 - acc: 0.5387
7808/9333 [========================>.....] - ETA: 3:58 - loss: 0.6903 - acc: 0.5378
7872/9333 [========================>.....] - ETA: 3:48 - loss: 0.6903 - acc: 0.5382
7936/9333 [========================>.....] - ETA: 3:38 - loss: 0.6903 - acc: 0.5379
8000/9333 [========================>.....] - ETA: 3:28 - loss: 0.6905 - acc: 0.5379
8064/9333 [========================>.....] - ETA: 3:18 - loss: 0.6905 - acc: 0.5373
8128/9333 [=========================>....] - ETA: 3:08 - loss: 0.6905 - acc: 0.5376
8192/9333 [=========================>....] - ETA: 2:58 - loss: 0.6906 - acc: 0.5377
8256/9333 [=========================>....] - ETA: 2:48 - loss: 0.6904 - acc: 0.5384
8320/9333 [=========================>....] - ETA: 2:38 - loss: 0.6903 - acc: 0.5382
8384/9333 [=========================>....] - ETA: 2:28 - loss: 0.6902 - acc: 0.5388
8448/9333 [==========================>...] - ETA: 2:18 - loss: 0.6902 - acc: 0.5385
8512/9333 [==========================>...] - ETA: 2:08 - loss: 0.6901 - acc: 0.5390
8576/9333 [==========================>...] - ETA: 1:58 - loss: 0.6901 - acc: 0.5386
8640/9333 [==========================>...] - ETA: 1:48 - loss: 0.6900 - acc: 0.5391
8704/9333 [==========================>...] - ETA: 1:38 - loss: 0.6902 - acc: 0.5387
8768/9333 [===========================>..] - ETA: 1:28 - loss: 0.6903 - acc: 0.5385
8832/9333 [===========================>..] - ETA: 1:18 - loss: 0.6905 - acc: 0.5380
8896/9333 [===========================>..] - ETA: 1:08 - loss: 0.6907 - acc: 0.5377
8960/9333 [===========================>..] - ETA: 58s - loss: 0.6907 - acc: 0.5374 
9024/9333 [============================>.] - ETA: 48s - loss: 0.6907 - acc: 0.5377
9088/9333 [============================>.] - ETA: 38s - loss: 0.6908 - acc: 0.5369
9152/9333 [============================>.] - ETA: 28s - loss: 0.6910 - acc: 0.5365
9216/9333 [============================>.] - ETA: 18s - loss: 0.6909 - acc: 0.5366
9280/9333 [============================>.] - ETA: 8s - loss: 0.6909 - acc: 0.5367 
9333/9333 [==============================] - 1516s 162ms/step - loss: 0.6909 - acc: 0.5365 - val_loss: 0.6907 - val_acc: 0.5342

Epoch 00005: val_acc did not improve from 0.53423
Epoch 6/10

  64/9333 [..............................] - ETA: 27:38 - loss: 0.6939 - acc: 0.5469
 128/9333 [..............................] - ETA: 27:29 - loss: 0.7104 - acc: 0.5078
 192/9333 [..............................] - ETA: 26:43 - loss: 0.6997 - acc: 0.5365
 256/9333 [..............................] - ETA: 25:53 - loss: 0.6940 - acc: 0.5508
 320/9333 [>.............................] - ETA: 25:47 - loss: 0.6941 - acc: 0.5656
 384/9333 [>.............................] - ETA: 25:24 - loss: 0.6923 - acc: 0.5599
 448/9333 [>.............................] - ETA: 25:20 - loss: 0.6910 - acc: 0.5580
 512/9333 [>.............................] - ETA: 25:06 - loss: 0.6922 - acc: 0.5469
 576/9333 [>.............................] - ETA: 24:35 - loss: 0.6931 - acc: 0.5434
 640/9333 [=>............................] - ETA: 24:20 - loss: 0.6938 - acc: 0.5422
 704/9333 [=>............................] - ETA: 23:59 - loss: 0.6932 - acc: 0.5469
 768/9333 [=>............................] - ETA: 23:42 - loss: 0.6940 - acc: 0.5417
 832/9333 [=>............................] - ETA: 23:34 - loss: 0.6933 - acc: 0.5433
 896/9333 [=>............................] - ETA: 23:27 - loss: 0.6920 - acc: 0.5446
 960/9333 [==>...........................] - ETA: 23:12 - loss: 0.6923 - acc: 0.5427
1024/9333 [==>...........................] - ETA: 22:50 - loss: 0.6927 - acc: 0.5381
1088/9333 [==>...........................] - ETA: 22:39 - loss: 0.6923 - acc: 0.5414
1152/9333 [==>...........................] - ETA: 22:25 - loss: 0.6925 - acc: 0.5399
1216/9333 [==>...........................] - ETA: 22:19 - loss: 0.6919 - acc: 0.5403
1280/9333 [===>..........................] - ETA: 22:11 - loss: 0.6912 - acc: 0.5430
1344/9333 [===>..........................] - ETA: 22:03 - loss: 0.6917 - acc: 0.5409
1408/9333 [===>..........................] - ETA: 21:49 - loss: 0.6911 - acc: 0.5419
1472/9333 [===>..........................] - ETA: 21:44 - loss: 0.6908 - acc: 0.5408
1536/9333 [===>..........................] - ETA: 21:33 - loss: 0.6911 - acc: 0.5384
1600/9333 [====>.........................] - ETA: 21:26 - loss: 0.6908 - acc: 0.5387
1664/9333 [====>.........................] - ETA: 21:21 - loss: 0.6914 - acc: 0.5337
1728/9333 [====>.........................] - ETA: 21:14 - loss: 0.6917 - acc: 0.5301
1792/9333 [====>.........................] - ETA: 21:01 - loss: 0.6912 - acc: 0.5318
1856/9333 [====>.........................] - ETA: 20:49 - loss: 0.6912 - acc: 0.5329
1920/9333 [=====>........................] - ETA: 20:41 - loss: 0.6915 - acc: 0.5312
1984/9333 [=====>........................] - ETA: 20:32 - loss: 0.6909 - acc: 0.5338
2048/9333 [=====>........................] - ETA: 20:23 - loss: 0.6913 - acc: 0.5337
2112/9333 [=====>........................] - ETA: 20:13 - loss: 0.6913 - acc: 0.5322
2176/9333 [=====>........................] - ETA: 20:01 - loss: 0.6912 - acc: 0.5308
2240/9333 [======>.......................] - ETA: 19:52 - loss: 0.6910 - acc: 0.5304
2304/9333 [======>.......................] - ETA: 19:38 - loss: 0.6906 - acc: 0.5330
2368/9333 [======>.......................] - ETA: 19:27 - loss: 0.6904 - acc: 0.5334
2432/9333 [======>.......................] - ETA: 19:16 - loss: 0.6903 - acc: 0.5341
2496/9333 [=======>......................] - ETA: 19:05 - loss: 0.6901 - acc: 0.5345
2560/9333 [=======>......................] - ETA: 18:55 - loss: 0.6904 - acc: 0.5332
2624/9333 [=======>......................] - ETA: 18:45 - loss: 0.6905 - acc: 0.5312
2688/9333 [=======>......................] - ETA: 18:34 - loss: 0.6906 - acc: 0.5298
2752/9333 [=======>......................] - ETA: 18:24 - loss: 0.6911 - acc: 0.5280
2816/9333 [========>.....................] - ETA: 18:12 - loss: 0.6914 - acc: 0.5252
2880/9333 [========>.....................] - ETA: 18:00 - loss: 0.6912 - acc: 0.5250
2944/9333 [========>.....................] - ETA: 17:50 - loss: 0.6910 - acc: 0.5265
3008/9333 [========>.....................] - ETA: 17:39 - loss: 0.6910 - acc: 0.5266
3072/9333 [========>.....................] - ETA: 17:29 - loss: 0.6912 - acc: 0.5260
3136/9333 [=========>....................] - ETA: 17:18 - loss: 0.6914 - acc: 0.5252
3200/9333 [=========>....................] - ETA: 17:09 - loss: 0.6914 - acc: 0.5253
3264/9333 [=========>....................] - ETA: 16:57 - loss: 0.6911 - acc: 0.5267
3328/9333 [=========>....................] - ETA: 16:46 - loss: 0.6914 - acc: 0.5258
3392/9333 [=========>....................] - ETA: 16:35 - loss: 0.6915 - acc: 0.5259
3456/9333 [==========>...................] - ETA: 16:25 - loss: 0.6915 - acc: 0.5272
3520/9333 [==========>...................] - ETA: 16:13 - loss: 0.6916 - acc: 0.5270
3584/9333 [==========>...................] - ETA: 16:04 - loss: 0.6917 - acc: 0.5276
3648/9333 [==========>...................] - ETA: 15:54 - loss: 0.6918 - acc: 0.5280
3712/9333 [==========>...................] - ETA: 15:43 - loss: 0.6917 - acc: 0.5294
3776/9333 [===========>..................] - ETA: 15:33 - loss: 0.6916 - acc: 0.5294
3840/9333 [===========>..................] - ETA: 15:22 - loss: 0.6918 - acc: 0.5281
3904/9333 [===========>..................] - ETA: 15:10 - loss: 0.6919 - acc: 0.5272
3968/9333 [===========>..................] - ETA: 15:00 - loss: 0.6921 - acc: 0.5267
4032/9333 [===========>..................] - ETA: 14:50 - loss: 0.6919 - acc: 0.5280
4096/9333 [============>.................] - ETA: 14:39 - loss: 0.6917 - acc: 0.5298
4160/9333 [============>.................] - ETA: 14:29 - loss: 0.6915 - acc: 0.5303
4224/9333 [============>.................] - ETA: 14:18 - loss: 0.6916 - acc: 0.5298
4288/9333 [============>.................] - ETA: 14:07 - loss: 0.6912 - acc: 0.5312
4352/9333 [============>.................] - ETA: 13:57 - loss: 0.6910 - acc: 0.5319
4416/9333 [=============>................] - ETA: 13:45 - loss: 0.6910 - acc: 0.5317
4480/9333 [=============>................] - ETA: 13:35 - loss: 0.6908 - acc: 0.5324
4544/9333 [=============>................] - ETA: 13:25 - loss: 0.6907 - acc: 0.5335
4608/9333 [=============>................] - ETA: 13:15 - loss: 0.6907 - acc: 0.5334
4672/9333 [==============>...............] - ETA: 13:04 - loss: 0.6905 - acc: 0.5340
4736/9333 [==============>...............] - ETA: 12:53 - loss: 0.6902 - acc: 0.5351
4800/9333 [==============>...............] - ETA: 12:43 - loss: 0.6898 - acc: 0.5367
4864/9333 [==============>...............] - ETA: 12:33 - loss: 0.6897 - acc: 0.5376
4928/9333 [==============>...............] - ETA: 12:23 - loss: 0.6899 - acc: 0.5365
4992/9333 [===============>..............] - ETA: 12:13 - loss: 0.6903 - acc: 0.5355
5056/9333 [===============>..............] - ETA: 12:02 - loss: 0.6905 - acc: 0.5358
5120/9333 [===============>..............] - ETA: 11:51 - loss: 0.6905 - acc: 0.5352
5184/9333 [===============>..............] - ETA: 11:41 - loss: 0.6903 - acc: 0.5349
5248/9333 [===============>..............] - ETA: 11:30 - loss: 0.6904 - acc: 0.5341
5312/9333 [================>.............] - ETA: 11:19 - loss: 0.6903 - acc: 0.5345
5376/9333 [================>.............] - ETA: 11:08 - loss: 0.6904 - acc: 0.5344
5440/9333 [================>.............] - ETA: 10:57 - loss: 0.6905 - acc: 0.5342
5504/9333 [================>.............] - ETA: 10:46 - loss: 0.6907 - acc: 0.5340
5568/9333 [================>.............] - ETA: 10:35 - loss: 0.6908 - acc: 0.5343
5632/9333 [=================>............] - ETA: 10:24 - loss: 0.6905 - acc: 0.5348
5696/9333 [=================>............] - ETA: 10:13 - loss: 0.6906 - acc: 0.5346
5760/9333 [=================>............] - ETA: 10:03 - loss: 0.6907 - acc: 0.5347
5824/9333 [=================>............] - ETA: 9:51 - loss: 0.6906 - acc: 0.5349 
5888/9333 [=================>............] - ETA: 9:41 - loss: 0.6906 - acc: 0.5346
5952/9333 [==================>...........] - ETA: 9:30 - loss: 0.6907 - acc: 0.5341
6016/9333 [==================>...........] - ETA: 9:19 - loss: 0.6904 - acc: 0.5351
6080/9333 [==================>...........] - ETA: 9:08 - loss: 0.6904 - acc: 0.5347
6144/9333 [==================>...........] - ETA: 8:58 - loss: 0.6906 - acc: 0.5348
6208/9333 [==================>...........] - ETA: 8:48 - loss: 0.6908 - acc: 0.5345
6272/9333 [===================>..........] - ETA: 8:38 - loss: 0.6907 - acc: 0.5343
6336/9333 [===================>..........] - ETA: 8:27 - loss: 0.6908 - acc: 0.5338
6400/9333 [===================>..........] - ETA: 8:16 - loss: 0.6908 - acc: 0.5334
6464/9333 [===================>..........] - ETA: 8:06 - loss: 0.6908 - acc: 0.5337
6528/9333 [===================>..........] - ETA: 7:55 - loss: 0.6908 - acc: 0.5340
6592/9333 [====================>.........] - ETA: 7:44 - loss: 0.6905 - acc: 0.5349
6656/9333 [====================>.........] - ETA: 7:33 - loss: 0.6905 - acc: 0.5346
6720/9333 [====================>.........] - ETA: 7:22 - loss: 0.6907 - acc: 0.5342
6784/9333 [====================>.........] - ETA: 7:11 - loss: 0.6907 - acc: 0.5339
6848/9333 [=====================>........] - ETA: 7:00 - loss: 0.6909 - acc: 0.5336
6912/9333 [=====================>........] - ETA: 6:49 - loss: 0.6911 - acc: 0.5333
6976/9333 [=====================>........] - ETA: 6:38 - loss: 0.6909 - acc: 0.5335
7040/9333 [=====================>........] - ETA: 6:27 - loss: 0.6909 - acc: 0.5347
7104/9333 [=====================>........] - ETA: 6:16 - loss: 0.6908 - acc: 0.5356
7168/9333 [======================>.......] - ETA: 6:05 - loss: 0.6908 - acc: 0.5357
7232/9333 [======================>.......] - ETA: 5:54 - loss: 0.6910 - acc: 0.5351
7296/9333 [======================>.......] - ETA: 5:44 - loss: 0.6911 - acc: 0.5350
7360/9333 [======================>.......] - ETA: 5:33 - loss: 0.6912 - acc: 0.5346
7424/9333 [======================>.......] - ETA: 5:22 - loss: 0.6914 - acc: 0.5341
7488/9333 [=======================>......] - ETA: 5:11 - loss: 0.6914 - acc: 0.5346
7552/9333 [=======================>......] - ETA: 5:01 - loss: 0.6917 - acc: 0.5338
7616/9333 [=======================>......] - ETA: 4:50 - loss: 0.6921 - acc: 0.5327
7680/9333 [=======================>......] - ETA: 4:39 - loss: 0.6921 - acc: 0.5329
7744/9333 [=======================>......] - ETA: 4:28 - loss: 0.6923 - acc: 0.5325
7808/9333 [========================>.....] - ETA: 4:17 - loss: 0.6927 - acc: 0.5315
7872/9333 [========================>.....] - ETA: 4:06 - loss: 0.6927 - acc: 0.5315
7936/9333 [========================>.....] - ETA: 3:55 - loss: 0.6931 - acc: 0.5309
8000/9333 [========================>.....] - ETA: 3:44 - loss: 0.6930 - acc: 0.5311
8064/9333 [========================>.....] - ETA: 3:34 - loss: 0.6930 - acc: 0.5308
8128/9333 [=========================>....] - ETA: 3:23 - loss: 0.6930 - acc: 0.5312
8192/9333 [=========================>....] - ETA: 3:12 - loss: 0.6931 - acc: 0.5309
8256/9333 [=========================>....] - ETA: 3:01 - loss: 0.6931 - acc: 0.5306
8320/9333 [=========================>....] - ETA: 2:50 - loss: 0.6931 - acc: 0.5309
8384/9333 [=========================>....] - ETA: 2:40 - loss: 0.6931 - acc: 0.5311
8448/9333 [==========================>...] - ETA: 2:29 - loss: 0.6930 - acc: 0.5312
8512/9333 [==========================>...] - ETA: 2:18 - loss: 0.6930 - acc: 0.5311
8576/9333 [==========================>...] - ETA: 2:07 - loss: 0.6929 - acc: 0.5315
8640/9333 [==========================>...] - ETA: 1:56 - loss: 0.6929 - acc: 0.5315
8704/9333 [==========================>...] - ETA: 1:45 - loss: 0.6929 - acc: 0.5315
8768/9333 [===========================>..] - ETA: 1:35 - loss: 0.6928 - acc: 0.5316
8832/9333 [===========================>..] - ETA: 1:24 - loss: 0.6927 - acc: 0.5322
8896/9333 [===========================>..] - ETA: 1:13 - loss: 0.6926 - acc: 0.5324
8960/9333 [===========================>..] - ETA: 1:02 - loss: 0.6926 - acc: 0.5326
9024/9333 [============================>.] - ETA: 51s - loss: 0.6925 - acc: 0.5334 
9088/9333 [============================>.] - ETA: 41s - loss: 0.6926 - acc: 0.5332
9152/9333 [============================>.] - ETA: 30s - loss: 0.6925 - acc: 0.5337
9216/9333 [============================>.] - ETA: 19s - loss: 0.6925 - acc: 0.5339
9280/9333 [============================>.] - ETA: 8s - loss: 0.6925 - acc: 0.5337 
9333/9333 [==============================] - 1620s 174ms/step - loss: 0.6924 - acc: 0.5342 - val_loss: 0.6902 - val_acc: 0.5294

Epoch 00006: val_acc did not improve from 0.53423
Epoch 7/10

  64/9333 [..............................] - ETA: 24:18 - loss: 0.6920 - acc: 0.5469
 128/9333 [..............................] - ETA: 24:00 - loss: 0.6905 - acc: 0.5469
 192/9333 [..............................] - ETA: 23:41 - loss: 0.6998 - acc: 0.5208
 256/9333 [..............................] - ETA: 23:34 - loss: 0.6933 - acc: 0.5273
 320/9333 [>.............................] - ETA: 23:04 - loss: 0.6891 - acc: 0.5406
 384/9333 [>.............................] - ETA: 23:02 - loss: 0.6891 - acc: 0.5443
 448/9333 [>.............................] - ETA: 22:51 - loss: 0.6867 - acc: 0.5558
 512/9333 [>.............................] - ETA: 22:44 - loss: 0.6878 - acc: 0.5645
 576/9333 [>.............................] - ETA: 22:36 - loss: 0.6877 - acc: 0.5608
 640/9333 [=>............................] - ETA: 22:24 - loss: 0.6843 - acc: 0.5766
 704/9333 [=>............................] - ETA: 22:18 - loss: 0.6837 - acc: 0.5795
 768/9333 [=>............................] - ETA: 22:08 - loss: 0.6819 - acc: 0.5833
 832/9333 [=>............................] - ETA: 21:53 - loss: 0.6807 - acc: 0.5889
 896/9333 [=>............................] - ETA: 21:47 - loss: 0.6811 - acc: 0.5859
 960/9333 [==>...........................] - ETA: 21:37 - loss: 0.6816 - acc: 0.5854
1024/9333 [==>...........................] - ETA: 21:31 - loss: 0.6832 - acc: 0.5791
1088/9333 [==>...........................] - ETA: 21:23 - loss: 0.6827 - acc: 0.5800
1152/9333 [==>...........................] - ETA: 21:16 - loss: 0.6839 - acc: 0.5773
1216/9333 [==>...........................] - ETA: 21:01 - loss: 0.6848 - acc: 0.5724
1280/9333 [===>..........................] - ETA: 20:53 - loss: 0.6853 - acc: 0.5711
1344/9333 [===>..........................] - ETA: 20:46 - loss: 0.6863 - acc: 0.5685
1408/9333 [===>..........................] - ETA: 20:36 - loss: 0.6871 - acc: 0.5689
1472/9333 [===>..........................] - ETA: 20:23 - loss: 0.6890 - acc: 0.5632
1536/9333 [===>..........................] - ETA: 20:11 - loss: 0.6886 - acc: 0.5645
1600/9333 [====>.........................] - ETA: 20:01 - loss: 0.6896 - acc: 0.5581
1664/9333 [====>.........................] - ETA: 19:46 - loss: 0.6899 - acc: 0.5571
1728/9333 [====>.........................] - ETA: 19:35 - loss: 0.6893 - acc: 0.5584
1792/9333 [====>.........................] - ETA: 19:28 - loss: 0.6901 - acc: 0.5552
1856/9333 [====>.........................] - ETA: 19:19 - loss: 0.6903 - acc: 0.5544
1920/9333 [=====>........................] - ETA: 19:10 - loss: 0.6896 - acc: 0.5547
1984/9333 [=====>........................] - ETA: 18:57 - loss: 0.6897 - acc: 0.5534
2048/9333 [=====>........................] - ETA: 18:48 - loss: 0.6890 - acc: 0.5557
2112/9333 [=====>........................] - ETA: 18:43 - loss: 0.6893 - acc: 0.5516
2176/9333 [=====>........................] - ETA: 18:36 - loss: 0.6894 - acc: 0.5510
2240/9333 [======>.......................] - ETA: 18:23 - loss: 0.6891 - acc: 0.5504
2304/9333 [======>.......................] - ETA: 18:14 - loss: 0.6889 - acc: 0.5512
2368/9333 [======>.......................] - ETA: 18:04 - loss: 0.6893 - acc: 0.5490
2432/9333 [======>.......................] - ETA: 17:56 - loss: 0.6893 - acc: 0.5477
2496/9333 [=======>......................] - ETA: 17:47 - loss: 0.6891 - acc: 0.5489
2560/9333 [=======>......................] - ETA: 17:39 - loss: 0.6897 - acc: 0.5473
2624/9333 [=======>......................] - ETA: 17:27 - loss: 0.6897 - acc: 0.5469
2688/9333 [=======>......................] - ETA: 17:17 - loss: 0.6898 - acc: 0.5480
2752/9333 [=======>......................] - ETA: 17:08 - loss: 0.6897 - acc: 0.5501
2816/9333 [========>.....................] - ETA: 16:59 - loss: 0.6893 - acc: 0.5518
2880/9333 [========>.....................] - ETA: 16:51 - loss: 0.6891 - acc: 0.5528
2944/9333 [========>.....................] - ETA: 16:41 - loss: 0.6895 - acc: 0.5510
3008/9333 [========>.....................] - ETA: 16:32 - loss: 0.6893 - acc: 0.5512
3072/9333 [========>.....................] - ETA: 16:25 - loss: 0.6893 - acc: 0.5508
3136/9333 [=========>....................] - ETA: 16:16 - loss: 0.6894 - acc: 0.5507
3200/9333 [=========>....................] - ETA: 16:06 - loss: 0.6889 - acc: 0.5531
3264/9333 [=========>....................] - ETA: 15:58 - loss: 0.6894 - acc: 0.5506
3328/9333 [=========>....................] - ETA: 15:49 - loss: 0.6893 - acc: 0.5493
3392/9333 [=========>....................] - ETA: 15:39 - loss: 0.6895 - acc: 0.5469
3456/9333 [==========>...................] - ETA: 15:29 - loss: 0.6895 - acc: 0.5477
3520/9333 [==========>...................] - ETA: 15:18 - loss: 0.6893 - acc: 0.5483
3584/9333 [==========>...................] - ETA: 15:09 - loss: 0.6892 - acc: 0.5485
3648/9333 [==========>...................] - ETA: 15:00 - loss: 0.6892 - acc: 0.5471
3712/9333 [==========>...................] - ETA: 14:49 - loss: 0.6895 - acc: 0.5453
3776/9333 [===========>..................] - ETA: 14:39 - loss: 0.6893 - acc: 0.5448
3840/9333 [===========>..................] - ETA: 14:28 - loss: 0.6894 - acc: 0.5445
3904/9333 [===========>..................] - ETA: 14:19 - loss: 0.6899 - acc: 0.5423
3968/9333 [===========>..................] - ETA: 14:08 - loss: 0.6903 - acc: 0.5406
4032/9333 [===========>..................] - ETA: 13:57 - loss: 0.6905 - acc: 0.5402
4096/9333 [============>.................] - ETA: 13:46 - loss: 0.6903 - acc: 0.5415
4160/9333 [============>.................] - ETA: 13:34 - loss: 0.6901 - acc: 0.5421
4224/9333 [============>.................] - ETA: 13:24 - loss: 0.6902 - acc: 0.5405
4288/9333 [============>.................] - ETA: 13:13 - loss: 0.6904 - acc: 0.5399
4352/9333 [============>.................] - ETA: 13:04 - loss: 0.6906 - acc: 0.5391
4416/9333 [=============>................] - ETA: 12:52 - loss: 0.6904 - acc: 0.5389
4480/9333 [=============>................] - ETA: 12:42 - loss: 0.6904 - acc: 0.5393
4544/9333 [=============>................] - ETA: 12:31 - loss: 0.6901 - acc: 0.5403
4608/9333 [=============>................] - ETA: 12:21 - loss: 0.6902 - acc: 0.5404
4672/9333 [==============>...............] - ETA: 12:12 - loss: 0.6903 - acc: 0.5398
4736/9333 [==============>...............] - ETA: 12:02 - loss: 0.6901 - acc: 0.5410
4800/9333 [==============>...............] - ETA: 11:52 - loss: 0.6903 - acc: 0.5402
4864/9333 [==============>...............] - ETA: 11:42 - loss: 0.6903 - acc: 0.5401
4928/9333 [==============>...............] - ETA: 11:32 - loss: 0.6903 - acc: 0.5406
4992/9333 [===============>..............] - ETA: 11:22 - loss: 0.6903 - acc: 0.5397
5056/9333 [===============>..............] - ETA: 11:12 - loss: 0.6904 - acc: 0.5390
5120/9333 [===============>..............] - ETA: 11:01 - loss: 0.6905 - acc: 0.5383
5184/9333 [===============>..............] - ETA: 10:51 - loss: 0.6905 - acc: 0.5370
5248/9333 [===============>..............] - ETA: 10:41 - loss: 0.6903 - acc: 0.5375
5312/9333 [================>.............] - ETA: 10:31 - loss: 0.6903 - acc: 0.5367
5376/9333 [================>.............] - ETA: 10:21 - loss: 0.6903 - acc: 0.5368
5440/9333 [================>.............] - ETA: 10:11 - loss: 0.6905 - acc: 0.5360
5504/9333 [================>.............] - ETA: 10:01 - loss: 0.6904 - acc: 0.5363
5568/9333 [================>.............] - ETA: 9:51 - loss: 0.6904 - acc: 0.5359 
5632/9333 [=================>............] - ETA: 9:41 - loss: 0.6905 - acc: 0.5359
5696/9333 [=================>............] - ETA: 9:31 - loss: 0.6905 - acc: 0.5356
5760/9333 [=================>............] - ETA: 9:20 - loss: 0.6905 - acc: 0.5356
5824/9333 [=================>............] - ETA: 9:10 - loss: 0.6908 - acc: 0.5342
5888/9333 [=================>............] - ETA: 9:00 - loss: 0.6908 - acc: 0.5338
5952/9333 [==================>...........] - ETA: 8:50 - loss: 0.6907 - acc: 0.5341
6016/9333 [==================>...........] - ETA: 8:40 - loss: 0.6908 - acc: 0.5337
6080/9333 [==================>...........] - ETA: 8:30 - loss: 0.6909 - acc: 0.5332
6144/9333 [==================>...........] - ETA: 8:20 - loss: 0.6911 - acc: 0.5332
6208/9333 [==================>...........] - ETA: 8:10 - loss: 0.6912 - acc: 0.5327
6272/9333 [===================>..........] - ETA: 8:00 - loss: 0.6913 - acc: 0.5319
6336/9333 [===================>..........] - ETA: 7:50 - loss: 0.6913 - acc: 0.5312
6400/9333 [===================>..........] - ETA: 7:41 - loss: 0.6912 - acc: 0.5323
6464/9333 [===================>..........] - ETA: 7:30 - loss: 0.6911 - acc: 0.5323
6528/9333 [===================>..........] - ETA: 7:20 - loss: 0.6911 - acc: 0.5317
6592/9333 [====================>.........] - ETA: 7:10 - loss: 0.6912 - acc: 0.5312
6656/9333 [====================>.........] - ETA: 7:00 - loss: 0.6911 - acc: 0.5316
6720/9333 [====================>.........] - ETA: 6:51 - loss: 0.6912 - acc: 0.5320
6784/9333 [====================>.........] - ETA: 6:41 - loss: 0.6913 - acc: 0.5317
6848/9333 [=====================>........] - ETA: 6:31 - loss: 0.6914 - acc: 0.5310
6912/9333 [=====================>........] - ETA: 6:21 - loss: 0.6913 - acc: 0.5312
6976/9333 [=====================>........] - ETA: 6:11 - loss: 0.6913 - acc: 0.5308
7040/9333 [=====================>........] - ETA: 6:01 - loss: 0.6912 - acc: 0.5312
7104/9333 [=====================>........] - ETA: 5:51 - loss: 0.6912 - acc: 0.5312
7168/9333 [======================>.......] - ETA: 5:41 - loss: 0.6913 - acc: 0.5312
7232/9333 [======================>.......] - ETA: 5:30 - loss: 0.6913 - acc: 0.5311
7296/9333 [======================>.......] - ETA: 5:20 - loss: 0.6913 - acc: 0.5314
7360/9333 [======================>.......] - ETA: 5:10 - loss: 0.6912 - acc: 0.5314
7424/9333 [======================>.......] - ETA: 5:00 - loss: 0.6911 - acc: 0.5321
7488/9333 [=======================>......] - ETA: 4:50 - loss: 0.6913 - acc: 0.5314
7552/9333 [=======================>......] - ETA: 4:40 - loss: 0.6912 - acc: 0.5316
7616/9333 [=======================>......] - ETA: 4:30 - loss: 0.6911 - acc: 0.5319
7680/9333 [=======================>......] - ETA: 4:20 - loss: 0.6910 - acc: 0.5331
7744/9333 [=======================>......] - ETA: 4:10 - loss: 0.6910 - acc: 0.5329
7808/9333 [========================>.....] - ETA: 4:00 - loss: 0.6908 - acc: 0.5333
7872/9333 [========================>.....] - ETA: 3:50 - loss: 0.6909 - acc: 0.5328
7936/9333 [========================>.....] - ETA: 3:40 - loss: 0.6908 - acc: 0.5331
8000/9333 [========================>.....] - ETA: 3:30 - loss: 0.6908 - acc: 0.5333
8064/9333 [========================>.....] - ETA: 3:20 - loss: 0.6908 - acc: 0.5327
8128/9333 [=========================>....] - ETA: 3:10 - loss: 0.6908 - acc: 0.5325
8192/9333 [=========================>....] - ETA: 2:59 - loss: 0.6908 - acc: 0.5327
8256/9333 [=========================>....] - ETA: 2:49 - loss: 0.6908 - acc: 0.5332
8320/9333 [=========================>....] - ETA: 2:39 - loss: 0.6908 - acc: 0.5335
8384/9333 [=========================>....] - ETA: 2:29 - loss: 0.6908 - acc: 0.5335
8448/9333 [==========================>...] - ETA: 2:19 - loss: 0.6908 - acc: 0.5336
8512/9333 [==========================>...] - ETA: 2:09 - loss: 0.6910 - acc: 0.5332
8576/9333 [==========================>...] - ETA: 1:59 - loss: 0.6911 - acc: 0.5325
8640/9333 [==========================>...] - ETA: 1:49 - loss: 0.6910 - acc: 0.5326
8704/9333 [==========================>...] - ETA: 1:39 - loss: 0.6908 - acc: 0.5335
8768/9333 [===========================>..] - ETA: 1:29 - loss: 0.6908 - acc: 0.5334
8832/9333 [===========================>..] - ETA: 1:19 - loss: 0.6908 - acc: 0.5334
8896/9333 [===========================>..] - ETA: 1:09 - loss: 0.6909 - acc: 0.5327
8960/9333 [===========================>..] - ETA: 59s - loss: 0.6909 - acc: 0.5329 
9024/9333 [============================>.] - ETA: 49s - loss: 0.6910 - acc: 0.5324
9088/9333 [============================>.] - ETA: 38s - loss: 0.6910 - acc: 0.5325
9152/9333 [============================>.] - ETA: 28s - loss: 0.6911 - acc: 0.5320
9216/9333 [============================>.] - ETA: 18s - loss: 0.6909 - acc: 0.5328
9280/9333 [============================>.] - ETA: 8s - loss: 0.6909 - acc: 0.5328 
9333/9333 [==============================] - 1535s 165ms/step - loss: 0.6907 - acc: 0.5334 - val_loss: 0.6887 - val_acc: 0.5352

Epoch 00007: val_acc improved from 0.53423 to 0.53520, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window17/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 8/10

  64/9333 [..............................] - ETA: 22:35 - loss: 0.6859 - acc: 0.6250
 128/9333 [..............................] - ETA: 20:38 - loss: 0.6836 - acc: 0.5859
 192/9333 [..............................] - ETA: 21:10 - loss: 0.6872 - acc: 0.5938
 256/9333 [..............................] - ETA: 21:31 - loss: 0.6868 - acc: 0.5820
 320/9333 [>.............................] - ETA: 22:00 - loss: 0.6864 - acc: 0.5781
 384/9333 [>.............................] - ETA: 21:46 - loss: 0.6851 - acc: 0.5729
 448/9333 [>.............................] - ETA: 21:46 - loss: 0.6851 - acc: 0.5647
 512/9333 [>.............................] - ETA: 21:47 - loss: 0.6860 - acc: 0.5605
 576/9333 [>.............................] - ETA: 21:31 - loss: 0.6848 - acc: 0.5590
 640/9333 [=>............................] - ETA: 21:28 - loss: 0.6857 - acc: 0.5578
 704/9333 [=>............................] - ETA: 21:22 - loss: 0.6857 - acc: 0.5582
 768/9333 [=>............................] - ETA: 21:08 - loss: 0.6858 - acc: 0.5586
 832/9333 [=>............................] - ETA: 21:01 - loss: 0.6878 - acc: 0.5469
 896/9333 [=>............................] - ETA: 20:54 - loss: 0.6873 - acc: 0.5513
 960/9333 [==>...........................] - ETA: 20:41 - loss: 0.6892 - acc: 0.5458
1024/9333 [==>...........................] - ETA: 20:25 - loss: 0.6902 - acc: 0.5459
1088/9333 [==>...........................] - ETA: 20:13 - loss: 0.6908 - acc: 0.5450
1152/9333 [==>...........................] - ETA: 20:19 - loss: 0.6926 - acc: 0.5391
1216/9333 [==>...........................] - ETA: 20:12 - loss: 0.6931 - acc: 0.5362
1280/9333 [===>..........................] - ETA: 20:07 - loss: 0.6934 - acc: 0.5359
1344/9333 [===>..........................] - ETA: 20:02 - loss: 0.6930 - acc: 0.5387
1408/9333 [===>..........................] - ETA: 19:59 - loss: 0.6930 - acc: 0.5362
1472/9333 [===>..........................] - ETA: 19:57 - loss: 0.6925 - acc: 0.5401
1536/9333 [===>..........................] - ETA: 19:46 - loss: 0.6918 - acc: 0.5417
1600/9333 [====>.........................] - ETA: 19:41 - loss: 0.6920 - acc: 0.5400
1664/9333 [====>.........................] - ETA: 19:37 - loss: 0.6917 - acc: 0.5409
1728/9333 [====>.........................] - ETA: 19:28 - loss: 0.6915 - acc: 0.5411
1792/9333 [====>.........................] - ETA: 19:19 - loss: 0.6916 - acc: 0.5385
1856/9333 [====>.........................] - ETA: 19:09 - loss: 0.6913 - acc: 0.5409
1920/9333 [=====>........................] - ETA: 19:01 - loss: 0.6916 - acc: 0.5406
1984/9333 [=====>........................] - ETA: 18:53 - loss: 0.6914 - acc: 0.5408
2048/9333 [=====>........................] - ETA: 18:44 - loss: 0.6915 - acc: 0.5381
2112/9333 [=====>........................] - ETA: 18:36 - loss: 0.6912 - acc: 0.5407
2176/9333 [=====>........................] - ETA: 18:26 - loss: 0.6917 - acc: 0.5377
2240/9333 [======>.......................] - ETA: 18:16 - loss: 0.6916 - acc: 0.5379
2304/9333 [======>.......................] - ETA: 18:05 - loss: 0.6912 - acc: 0.5404
2368/9333 [======>.......................] - ETA: 17:53 - loss: 0.6915 - acc: 0.5397
2432/9333 [======>.......................] - ETA: 17:42 - loss: 0.6911 - acc: 0.5411
2496/9333 [=======>......................] - ETA: 17:29 - loss: 0.6910 - acc: 0.5397
2560/9333 [=======>......................] - ETA: 17:17 - loss: 0.6915 - acc: 0.5391
2624/9333 [=======>......................] - ETA: 17:09 - loss: 0.6915 - acc: 0.5408
2688/9333 [=======>......................] - ETA: 16:55 - loss: 0.6911 - acc: 0.5406
2752/9333 [=======>......................] - ETA: 16:43 - loss: 0.6912 - acc: 0.5396
2816/9333 [========>.....................] - ETA: 16:32 - loss: 0.6910 - acc: 0.5391
2880/9333 [========>.....................] - ETA: 16:21 - loss: 0.6911 - acc: 0.5392
2944/9333 [========>.....................] - ETA: 16:11 - loss: 0.6908 - acc: 0.5397
3008/9333 [========>.....................] - ETA: 16:01 - loss: 0.6907 - acc: 0.5409
3072/9333 [========>.....................] - ETA: 15:51 - loss: 0.6906 - acc: 0.5417
3136/9333 [=========>....................] - ETA: 15:39 - loss: 0.6906 - acc: 0.5408
3200/9333 [=========>....................] - ETA: 15:29 - loss: 0.6903 - acc: 0.5422
3264/9333 [=========>....................] - ETA: 15:19 - loss: 0.6904 - acc: 0.5426
3328/9333 [=========>....................] - ETA: 15:08 - loss: 0.6902 - acc: 0.5427
3392/9333 [=========>....................] - ETA: 14:58 - loss: 0.6901 - acc: 0.5439
3456/9333 [==========>...................] - ETA: 14:48 - loss: 0.6904 - acc: 0.5431
3520/9333 [==========>...................] - ETA: 14:37 - loss: 0.6899 - acc: 0.5429
3584/9333 [==========>...................] - ETA: 14:27 - loss: 0.6902 - acc: 0.5416
3648/9333 [==========>...................] - ETA: 14:17 - loss: 0.6904 - acc: 0.5403
3712/9333 [==========>...................] - ETA: 14:08 - loss: 0.6901 - acc: 0.5415
3776/9333 [===========>..................] - ETA: 13:59 - loss: 0.6898 - acc: 0.5424
3840/9333 [===========>..................] - ETA: 13:53 - loss: 0.6893 - acc: 0.5443
3904/9333 [===========>..................] - ETA: 13:42 - loss: 0.6891 - acc: 0.5443
3968/9333 [===========>..................] - ETA: 13:33 - loss: 0.6888 - acc: 0.5449
4032/9333 [===========>..................] - ETA: 13:25 - loss: 0.6888 - acc: 0.5441
4096/9333 [============>.................] - ETA: 13:15 - loss: 0.6892 - acc: 0.5435
4160/9333 [============>.................] - ETA: 13:04 - loss: 0.6892 - acc: 0.5437
4224/9333 [============>.................] - ETA: 12:53 - loss: 0.6890 - acc: 0.5447
4288/9333 [============>.................] - ETA: 12:43 - loss: 0.6895 - acc: 0.5429
4352/9333 [============>.................] - ETA: 12:34 - loss: 0.6898 - acc: 0.5427
4416/9333 [=============>................] - ETA: 12:25 - loss: 0.6900 - acc: 0.5419
4480/9333 [=============>................] - ETA: 12:14 - loss: 0.6902 - acc: 0.5406
4544/9333 [=============>................] - ETA: 12:05 - loss: 0.6900 - acc: 0.5414
4608/9333 [=============>................] - ETA: 11:55 - loss: 0.6898 - acc: 0.5425
4672/9333 [==============>...............] - ETA: 11:46 - loss: 0.6898 - acc: 0.5430
4736/9333 [==============>...............] - ETA: 11:36 - loss: 0.6894 - acc: 0.5437
4800/9333 [==============>...............] - ETA: 11:25 - loss: 0.6895 - acc: 0.5435
4864/9333 [==============>...............] - ETA: 11:15 - loss: 0.6894 - acc: 0.5432
4928/9333 [==============>...............] - ETA: 11:05 - loss: 0.6896 - acc: 0.5420
4992/9333 [===============>..............] - ETA: 10:55 - loss: 0.6896 - acc: 0.5417
5056/9333 [===============>..............] - ETA: 10:46 - loss: 0.6898 - acc: 0.5411
5120/9333 [===============>..............] - ETA: 10:36 - loss: 0.6899 - acc: 0.5410
5184/9333 [===============>..............] - ETA: 10:27 - loss: 0.6901 - acc: 0.5405
5248/9333 [===============>..............] - ETA: 10:18 - loss: 0.6902 - acc: 0.5402
5312/9333 [================>.............] - ETA: 10:09 - loss: 0.6903 - acc: 0.5399
5376/9333 [================>.............] - ETA: 10:00 - loss: 0.6903 - acc: 0.5394
5440/9333 [================>.............] - ETA: 9:50 - loss: 0.6902 - acc: 0.5403 
5504/9333 [================>.............] - ETA: 9:41 - loss: 0.6903 - acc: 0.5407
5568/9333 [================>.............] - ETA: 9:31 - loss: 0.6902 - acc: 0.5409
5632/9333 [=================>............] - ETA: 9:22 - loss: 0.6902 - acc: 0.5408
5696/9333 [=================>............] - ETA: 9:12 - loss: 0.6902 - acc: 0.5407
5760/9333 [=================>............] - ETA: 9:02 - loss: 0.6903 - acc: 0.5405
5824/9333 [=================>............] - ETA: 8:53 - loss: 0.6902 - acc: 0.5404
5888/9333 [=================>............] - ETA: 8:44 - loss: 0.6898 - acc: 0.5414
5952/9333 [==================>...........] - ETA: 8:34 - loss: 0.6897 - acc: 0.5417
6016/9333 [==================>...........] - ETA: 8:25 - loss: 0.6897 - acc: 0.5414
6080/9333 [==================>...........] - ETA: 8:15 - loss: 0.6897 - acc: 0.5405
6144/9333 [==================>...........] - ETA: 8:05 - loss: 0.6896 - acc: 0.5417
6208/9333 [==================>...........] - ETA: 7:55 - loss: 0.6894 - acc: 0.5424
6272/9333 [===================>..........] - ETA: 7:45 - loss: 0.6894 - acc: 0.5424
6336/9333 [===================>..........] - ETA: 7:36 - loss: 0.6893 - acc: 0.5429
6400/9333 [===================>..........] - ETA: 7:26 - loss: 0.6894 - acc: 0.5430
6464/9333 [===================>..........] - ETA: 7:16 - loss: 0.6894 - acc: 0.5421
6528/9333 [===================>..........] - ETA: 7:07 - loss: 0.6894 - acc: 0.5417
6592/9333 [====================>.........] - ETA: 6:56 - loss: 0.6894 - acc: 0.5420
6656/9333 [====================>.........] - ETA: 6:47 - loss: 0.6890 - acc: 0.5430
6720/9333 [====================>.........] - ETA: 6:37 - loss: 0.6888 - acc: 0.5439
6784/9333 [====================>.........] - ETA: 6:27 - loss: 0.6886 - acc: 0.5444
6848/9333 [=====================>........] - ETA: 6:18 - loss: 0.6884 - acc: 0.5454
6912/9333 [=====================>........] - ETA: 6:08 - loss: 0.6882 - acc: 0.5462
6976/9333 [=====================>........] - ETA: 5:58 - loss: 0.6882 - acc: 0.5463
7040/9333 [=====================>........] - ETA: 5:49 - loss: 0.6881 - acc: 0.5463
7104/9333 [=====================>........] - ETA: 5:39 - loss: 0.6885 - acc: 0.5452
7168/9333 [======================>.......] - ETA: 5:29 - loss: 0.6888 - acc: 0.5445
7232/9333 [======================>.......] - ETA: 5:20 - loss: 0.6888 - acc: 0.5445
7296/9333 [======================>.......] - ETA: 5:10 - loss: 0.6888 - acc: 0.5450
7360/9333 [======================>.......] - ETA: 5:00 - loss: 0.6888 - acc: 0.5444
7424/9333 [======================>.......] - ETA: 4:51 - loss: 0.6886 - acc: 0.5451
7488/9333 [=======================>......] - ETA: 4:41 - loss: 0.6887 - acc: 0.5450
7552/9333 [=======================>......] - ETA: 4:32 - loss: 0.6885 - acc: 0.5457
7616/9333 [=======================>......] - ETA: 4:22 - loss: 0.6885 - acc: 0.5458
7680/9333 [=======================>......] - ETA: 4:12 - loss: 0.6885 - acc: 0.5453
7744/9333 [=======================>......] - ETA: 4:03 - loss: 0.6884 - acc: 0.5458
7808/9333 [========================>.....] - ETA: 3:53 - loss: 0.6883 - acc: 0.5465
7872/9333 [========================>.....] - ETA: 3:43 - loss: 0.6881 - acc: 0.5471
7936/9333 [========================>.....] - ETA: 3:34 - loss: 0.6880 - acc: 0.5475
8000/9333 [========================>.....] - ETA: 3:24 - loss: 0.6877 - acc: 0.5481
8064/9333 [========================>.....] - ETA: 3:14 - loss: 0.6877 - acc: 0.5487
8128/9333 [=========================>....] - ETA: 3:04 - loss: 0.6878 - acc: 0.5487
8192/9333 [=========================>....] - ETA: 2:54 - loss: 0.6880 - acc: 0.5483
8256/9333 [=========================>....] - ETA: 2:45 - loss: 0.6880 - acc: 0.5488
8320/9333 [=========================>....] - ETA: 2:35 - loss: 0.6879 - acc: 0.5489
8384/9333 [=========================>....] - ETA: 2:25 - loss: 0.6881 - acc: 0.5485
8448/9333 [==========================>...] - ETA: 2:15 - loss: 0.6882 - acc: 0.5478
8512/9333 [==========================>...] - ETA: 2:06 - loss: 0.6885 - acc: 0.5469
8576/9333 [==========================>...] - ETA: 1:56 - loss: 0.6888 - acc: 0.5465
8640/9333 [==========================>...] - ETA: 1:46 - loss: 0.6887 - acc: 0.5464
8704/9333 [==========================>...] - ETA: 1:36 - loss: 0.6887 - acc: 0.5469
8768/9333 [===========================>..] - ETA: 1:26 - loss: 0.6888 - acc: 0.5468
8832/9333 [===========================>..] - ETA: 1:17 - loss: 0.6887 - acc: 0.5469
8896/9333 [===========================>..] - ETA: 1:07 - loss: 0.6887 - acc: 0.5469
8960/9333 [===========================>..] - ETA: 57s - loss: 0.6886 - acc: 0.5474 
9024/9333 [============================>.] - ETA: 47s - loss: 0.6886 - acc: 0.5471
9088/9333 [============================>.] - ETA: 37s - loss: 0.6885 - acc: 0.5472
9152/9333 [============================>.] - ETA: 27s - loss: 0.6885 - acc: 0.5470
9216/9333 [============================>.] - ETA: 18s - loss: 0.6885 - acc: 0.5473
9280/9333 [============================>.] - ETA: 8s - loss: 0.6884 - acc: 0.5477 
9333/9333 [==============================] - 1496s 160ms/step - loss: 0.6882 - acc: 0.5478 - val_loss: 0.6917 - val_acc: 0.5178

Epoch 00008: val_acc did not improve from 0.53520
Epoch 9/10

  64/9333 [..............................] - ETA: 24:41 - loss: 0.6891 - acc: 0.4375
 128/9333 [..............................] - ETA: 24:26 - loss: 0.6854 - acc: 0.5312
 192/9333 [..............................] - ETA: 23:04 - loss: 0.6813 - acc: 0.5521
 256/9333 [..............................] - ETA: 23:19 - loss: 0.6781 - acc: 0.5742
 320/9333 [>.............................] - ETA: 22:48 - loss: 0.6809 - acc: 0.5656
 384/9333 [>.............................] - ETA: 22:42 - loss: 0.6809 - acc: 0.5755
 448/9333 [>.............................] - ETA: 22:56 - loss: 0.6760 - acc: 0.5982
 512/9333 [>.............................] - ETA: 22:34 - loss: 0.6736 - acc: 0.6035
 576/9333 [>.............................] - ETA: 22:15 - loss: 0.6742 - acc: 0.5990
 640/9333 [=>............................] - ETA: 22:10 - loss: 0.6753 - acc: 0.5938
 704/9333 [=>............................] - ETA: 22:05 - loss: 0.6755 - acc: 0.5966
 768/9333 [=>............................] - ETA: 22:10 - loss: 0.6767 - acc: 0.5885
 832/9333 [=>............................] - ETA: 22:12 - loss: 0.6766 - acc: 0.5925
 896/9333 [=>............................] - ETA: 22:00 - loss: 0.6784 - acc: 0.5882
 960/9333 [==>...........................] - ETA: 21:51 - loss: 0.6803 - acc: 0.5792
1024/9333 [==>...........................] - ETA: 21:43 - loss: 0.6809 - acc: 0.5752
1088/9333 [==>...........................] - ETA: 21:35 - loss: 0.6823 - acc: 0.5717
1152/9333 [==>...........................] - ETA: 21:40 - loss: 0.6827 - acc: 0.5651
1216/9333 [==>...........................] - ETA: 21:23 - loss: 0.6824 - acc: 0.5650
1280/9333 [===>..........................] - ETA: 21:08 - loss: 0.6829 - acc: 0.5625
1344/9333 [===>..........................] - ETA: 20:55 - loss: 0.6830 - acc: 0.5603
1408/9333 [===>..........................] - ETA: 20:44 - loss: 0.6830 - acc: 0.5597
1472/9333 [===>..........................] - ETA: 20:31 - loss: 0.6830 - acc: 0.5611
1536/9333 [===>..........................] - ETA: 20:20 - loss: 0.6840 - acc: 0.5560
1600/9333 [====>.........................] - ETA: 20:10 - loss: 0.6848 - acc: 0.5506
1664/9333 [====>.........................] - ETA: 20:01 - loss: 0.6852 - acc: 0.5505
1728/9333 [====>.........................] - ETA: 19:50 - loss: 0.6853 - acc: 0.5532
1792/9333 [====>.........................] - ETA: 19:36 - loss: 0.6858 - acc: 0.5513
1856/9333 [====>.........................] - ETA: 19:27 - loss: 0.6865 - acc: 0.5490
1920/9333 [=====>........................] - ETA: 19:18 - loss: 0.6867 - acc: 0.5490
1984/9333 [=====>........................] - ETA: 19:08 - loss: 0.6861 - acc: 0.5494
2048/9333 [=====>........................] - ETA: 19:00 - loss: 0.6863 - acc: 0.5479
2112/9333 [=====>........................] - ETA: 18:51 - loss: 0.6865 - acc: 0.5469
2176/9333 [=====>........................] - ETA: 18:40 - loss: 0.6871 - acc: 0.5483
2240/9333 [======>.......................] - ETA: 18:29 - loss: 0.6869 - acc: 0.5482
2304/9333 [======>.......................] - ETA: 18:16 - loss: 0.6869 - acc: 0.5477
2368/9333 [======>.......................] - ETA: 18:05 - loss: 0.6870 - acc: 0.5460
2432/9333 [======>.......................] - ETA: 17:54 - loss: 0.6875 - acc: 0.5448
2496/9333 [=======>......................] - ETA: 17:45 - loss: 0.6880 - acc: 0.5433
2560/9333 [=======>......................] - ETA: 17:36 - loss: 0.6880 - acc: 0.5434
2624/9333 [=======>......................] - ETA: 17:25 - loss: 0.6880 - acc: 0.5442
2688/9333 [=======>......................] - ETA: 17:15 - loss: 0.6886 - acc: 0.5413
2752/9333 [=======>......................] - ETA: 17:07 - loss: 0.6889 - acc: 0.5411
2816/9333 [========>.....................] - ETA: 16:58 - loss: 0.6889 - acc: 0.5405
2880/9333 [========>.....................] - ETA: 16:50 - loss: 0.6890 - acc: 0.5385
2944/9333 [========>.....................] - ETA: 16:39 - loss: 0.6890 - acc: 0.5394
3008/9333 [========>.....................] - ETA: 16:29 - loss: 0.6887 - acc: 0.5412
3072/9333 [========>.....................] - ETA: 16:20 - loss: 0.6885 - acc: 0.5436
3136/9333 [=========>....................] - ETA: 16:10 - loss: 0.6883 - acc: 0.5440
3200/9333 [=========>....................] - ETA: 16:00 - loss: 0.6888 - acc: 0.5434
3264/9333 [=========>....................] - ETA: 15:51 - loss: 0.6888 - acc: 0.5441
3328/9333 [=========>....................] - ETA: 15:40 - loss: 0.6892 - acc: 0.5424
3392/9333 [=========>....................] - ETA: 15:30 - loss: 0.6887 - acc: 0.5445
3456/9333 [==========>...................] - ETA: 15:21 - loss: 0.6884 - acc: 0.5446
3520/9333 [==========>...................] - ETA: 15:12 - loss: 0.6883 - acc: 0.5437
3584/9333 [==========>...................] - ETA: 15:03 - loss: 0.6880 - acc: 0.5435
3648/9333 [==========>...................] - ETA: 14:55 - loss: 0.6876 - acc: 0.5433
3712/9333 [==========>...................] - ETA: 14:46 - loss: 0.6875 - acc: 0.5434
3776/9333 [===========>..................] - ETA: 14:36 - loss: 0.6875 - acc: 0.5429
3840/9333 [===========>..................] - ETA: 14:26 - loss: 0.6870 - acc: 0.5432
3904/9333 [===========>..................] - ETA: 14:16 - loss: 0.6870 - acc: 0.5435
3968/9333 [===========>..................] - ETA: 14:06 - loss: 0.6864 - acc: 0.5439
4032/9333 [===========>..................] - ETA: 13:55 - loss: 0.6863 - acc: 0.5432
4096/9333 [============>.................] - ETA: 13:46 - loss: 0.6865 - acc: 0.5420
4160/9333 [============>.................] - ETA: 13:36 - loss: 0.6868 - acc: 0.5413
4224/9333 [============>.................] - ETA: 13:25 - loss: 0.6867 - acc: 0.5412
4288/9333 [============>.................] - ETA: 13:14 - loss: 0.6872 - acc: 0.5392
4352/9333 [============>.................] - ETA: 13:03 - loss: 0.6881 - acc: 0.5375
4416/9333 [=============>................] - ETA: 12:52 - loss: 0.6880 - acc: 0.5360
4480/9333 [=============>................] - ETA: 12:41 - loss: 0.6885 - acc: 0.5344
4544/9333 [=============>................] - ETA: 12:31 - loss: 0.6888 - acc: 0.5341
4608/9333 [=============>................] - ETA: 12:21 - loss: 0.6886 - acc: 0.5341
4672/9333 [==============>...............] - ETA: 12:10 - loss: 0.6887 - acc: 0.5332
4736/9333 [==============>...............] - ETA: 12:00 - loss: 0.6887 - acc: 0.5325
4800/9333 [==============>...............] - ETA: 11:48 - loss: 0.6886 - acc: 0.5329
4864/9333 [==============>...............] - ETA: 11:38 - loss: 0.6885 - acc: 0.5331
4928/9333 [==============>...............] - ETA: 11:28 - loss: 0.6886 - acc: 0.5327
4992/9333 [===============>..............] - ETA: 11:17 - loss: 0.6887 - acc: 0.5321
5056/9333 [===============>..............] - ETA: 11:07 - loss: 0.6884 - acc: 0.5338
5120/9333 [===============>..............] - ETA: 10:57 - loss: 0.6883 - acc: 0.5348
5184/9333 [===============>..............] - ETA: 10:47 - loss: 0.6881 - acc: 0.5359
5248/9333 [===============>..............] - ETA: 10:36 - loss: 0.6880 - acc: 0.5360
5312/9333 [================>.............] - ETA: 10:25 - loss: 0.6878 - acc: 0.5371
5376/9333 [================>.............] - ETA: 10:15 - loss: 0.6877 - acc: 0.5372
5440/9333 [================>.............] - ETA: 10:05 - loss: 0.6878 - acc: 0.5381
5504/9333 [================>.............] - ETA: 9:54 - loss: 0.6877 - acc: 0.5383 
5568/9333 [================>.............] - ETA: 9:44 - loss: 0.6877 - acc: 0.5388
5632/9333 [=================>............] - ETA: 9:34 - loss: 0.6879 - acc: 0.5380
5696/9333 [=================>............] - ETA: 9:23 - loss: 0.6881 - acc: 0.5372
5760/9333 [=================>............] - ETA: 9:13 - loss: 0.6879 - acc: 0.5378
5824/9333 [=================>............] - ETA: 9:03 - loss: 0.6881 - acc: 0.5367
5888/9333 [=================>............] - ETA: 8:52 - loss: 0.6881 - acc: 0.5369
5952/9333 [==================>...........] - ETA: 8:42 - loss: 0.6882 - acc: 0.5373
6016/9333 [==================>...........] - ETA: 8:32 - loss: 0.6881 - acc: 0.5372
6080/9333 [==================>...........] - ETA: 8:21 - loss: 0.6880 - acc: 0.5375
6144/9333 [==================>...........] - ETA: 8:11 - loss: 0.6881 - acc: 0.5374
6208/9333 [==================>...........] - ETA: 8:01 - loss: 0.6881 - acc: 0.5369
6272/9333 [===================>..........] - ETA: 7:51 - loss: 0.6877 - acc: 0.5389
6336/9333 [===================>..........] - ETA: 7:41 - loss: 0.6879 - acc: 0.5384
6400/9333 [===================>..........] - ETA: 7:30 - loss: 0.6881 - acc: 0.5380
6464/9333 [===================>..........] - ETA: 7:20 - loss: 0.6881 - acc: 0.5381
6528/9333 [===================>..........] - ETA: 7:11 - loss: 0.6882 - acc: 0.5372
6592/9333 [====================>.........] - ETA: 7:01 - loss: 0.6883 - acc: 0.5379
6656/9333 [====================>.........] - ETA: 6:51 - loss: 0.6883 - acc: 0.5379
6720/9333 [====================>.........] - ETA: 6:41 - loss: 0.6883 - acc: 0.5381
6784/9333 [====================>.........] - ETA: 6:31 - loss: 0.6882 - acc: 0.5383
6848/9333 [=====================>........] - ETA: 6:21 - loss: 0.6887 - acc: 0.5364
6912/9333 [=====================>........] - ETA: 6:10 - loss: 0.6888 - acc: 0.5354
6976/9333 [=====================>........] - ETA: 6:00 - loss: 0.6887 - acc: 0.5358
7040/9333 [=====================>........] - ETA: 5:50 - loss: 0.6887 - acc: 0.5354
7104/9333 [=====================>........] - ETA: 5:40 - loss: 0.6887 - acc: 0.5358
7168/9333 [======================>.......] - ETA: 5:31 - loss: 0.6886 - acc: 0.5359
7232/9333 [======================>.......] - ETA: 5:21 - loss: 0.6886 - acc: 0.5357
7296/9333 [======================>.......] - ETA: 5:11 - loss: 0.6884 - acc: 0.5360
7360/9333 [======================>.......] - ETA: 5:01 - loss: 0.6885 - acc: 0.5356
7424/9333 [======================>.......] - ETA: 4:51 - loss: 0.6884 - acc: 0.5353
7488/9333 [=======================>......] - ETA: 4:41 - loss: 0.6884 - acc: 0.5361
7552/9333 [=======================>......] - ETA: 4:31 - loss: 0.6884 - acc: 0.5358
7616/9333 [=======================>......] - ETA: 4:21 - loss: 0.6884 - acc: 0.5358
7680/9333 [=======================>......] - ETA: 4:11 - loss: 0.6883 - acc: 0.5365
7744/9333 [=======================>......] - ETA: 4:01 - loss: 0.6882 - acc: 0.5371
7808/9333 [========================>.....] - ETA: 3:52 - loss: 0.6882 - acc: 0.5369
7872/9333 [========================>.....] - ETA: 3:42 - loss: 0.6881 - acc: 0.5368
7936/9333 [========================>.....] - ETA: 3:32 - loss: 0.6881 - acc: 0.5377
8000/9333 [========================>.....] - ETA: 3:22 - loss: 0.6882 - acc: 0.5375
8064/9333 [========================>.....] - ETA: 3:12 - loss: 0.6882 - acc: 0.5373
8128/9333 [=========================>....] - ETA: 3:02 - loss: 0.6881 - acc: 0.5373
8192/9333 [=========================>....] - ETA: 2:52 - loss: 0.6883 - acc: 0.5369
8256/9333 [=========================>....] - ETA: 2:43 - loss: 0.6883 - acc: 0.5367
8320/9333 [=========================>....] - ETA: 2:33 - loss: 0.6884 - acc: 0.5365
8384/9333 [=========================>....] - ETA: 2:23 - loss: 0.6882 - acc: 0.5376
8448/9333 [==========================>...] - ETA: 2:14 - loss: 0.6883 - acc: 0.5378
8512/9333 [==========================>...] - ETA: 2:04 - loss: 0.6883 - acc: 0.5368
8576/9333 [==========================>...] - ETA: 1:54 - loss: 0.6883 - acc: 0.5372
8640/9333 [==========================>...] - ETA: 1:44 - loss: 0.6887 - acc: 0.5365
8704/9333 [==========================>...] - ETA: 1:35 - loss: 0.6885 - acc: 0.5375
8768/9333 [===========================>..] - ETA: 1:25 - loss: 0.6884 - acc: 0.5380
8832/9333 [===========================>..] - ETA: 1:15 - loss: 0.6883 - acc: 0.5384
8896/9333 [===========================>..] - ETA: 1:05 - loss: 0.6883 - acc: 0.5389
8960/9333 [===========================>..] - ETA: 56s - loss: 0.6884 - acc: 0.5383 
9024/9333 [============================>.] - ETA: 46s - loss: 0.6884 - acc: 0.5379
9088/9333 [============================>.] - ETA: 36s - loss: 0.6884 - acc: 0.5379
9152/9333 [============================>.] - ETA: 27s - loss: 0.6884 - acc: 0.5377
9216/9333 [============================>.] - ETA: 17s - loss: 0.6886 - acc: 0.5374
9280/9333 [============================>.] - ETA: 8s - loss: 0.6885 - acc: 0.5373 
9333/9333 [==============================] - 1468s 157ms/step - loss: 0.6884 - acc: 0.5374 - val_loss: 0.6915 - val_acc: 0.5246

Epoch 00009: val_acc did not improve from 0.53520
Epoch 10/10

  64/9333 [..............................] - ETA: 24:34 - loss: 0.6970 - acc: 0.4688
 128/9333 [..............................] - ETA: 23:19 - loss: 0.6947 - acc: 0.5156
 192/9333 [..............................] - ETA: 23:20 - loss: 0.6913 - acc: 0.5260
 256/9333 [..............................] - ETA: 22:24 - loss: 0.6919 - acc: 0.5234
 320/9333 [>.............................] - ETA: 21:59 - loss: 0.6879 - acc: 0.5375
 384/9333 [>.............................] - ETA: 21:50 - loss: 0.6905 - acc: 0.5234
 448/9333 [>.............................] - ETA: 21:46 - loss: 0.6913 - acc: 0.5134
 512/9333 [>.............................] - ETA: 21:34 - loss: 0.6898 - acc: 0.5176
 576/9333 [>.............................] - ETA: 21:29 - loss: 0.6895 - acc: 0.5226
 640/9333 [=>............................] - ETA: 21:24 - loss: 0.6886 - acc: 0.5281
 704/9333 [=>............................] - ETA: 21:21 - loss: 0.6893 - acc: 0.5298
 768/9333 [=>............................] - ETA: 21:08 - loss: 0.6890 - acc: 0.5430
 832/9333 [=>............................] - ETA: 20:52 - loss: 0.6903 - acc: 0.5409
 896/9333 [=>............................] - ETA: 20:43 - loss: 0.6910 - acc: 0.5368
 960/9333 [==>...........................] - ETA: 20:30 - loss: 0.6911 - acc: 0.5344
1024/9333 [==>...........................] - ETA: 20:18 - loss: 0.6901 - acc: 0.5420
1088/9333 [==>...........................] - ETA: 20:03 - loss: 0.6904 - acc: 0.5404
1152/9333 [==>...........................] - ETA: 19:54 - loss: 0.6907 - acc: 0.5451
1216/9333 [==>...........................] - ETA: 19:48 - loss: 0.6899 - acc: 0.5452
1280/9333 [===>..........................] - ETA: 19:40 - loss: 0.6897 - acc: 0.5492
1344/9333 [===>..........................] - ETA: 19:28 - loss: 0.6894 - acc: 0.5521
1408/9333 [===>..........................] - ETA: 19:21 - loss: 0.6909 - acc: 0.5455
1472/9333 [===>..........................] - ETA: 19:13 - loss: 0.6907 - acc: 0.5455
1536/9333 [===>..........................] - ETA: 19:05 - loss: 0.6911 - acc: 0.5436
1600/9333 [====>.........................] - ETA: 19:01 - loss: 0.6907 - acc: 0.5431
1664/9333 [====>.........................] - ETA: 18:58 - loss: 0.6911 - acc: 0.5403
1728/9333 [====>.........................] - ETA: 18:49 - loss: 0.6903 - acc: 0.5440
1792/9333 [====>.........................] - ETA: 18:41 - loss: 0.6900 - acc: 0.5474
1856/9333 [====>.........................] - ETA: 18:30 - loss: 0.6899 - acc: 0.5469
1920/9333 [=====>........................] - ETA: 18:19 - loss: 0.6896 - acc: 0.5484
1984/9333 [=====>........................] - ETA: 18:12 - loss: 0.6894 - acc: 0.5499
2048/9333 [=====>........................] - ETA: 18:06 - loss: 0.6896 - acc: 0.5464
2112/9333 [=====>........................] - ETA: 17:55 - loss: 0.6890 - acc: 0.5492
2176/9333 [=====>........................] - ETA: 17:45 - loss: 0.6886 - acc: 0.5519
2240/9333 [======>.......................] - ETA: 17:35 - loss: 0.6890 - acc: 0.5500
2304/9333 [======>.......................] - ETA: 17:27 - loss: 0.6889 - acc: 0.5490
2368/9333 [======>.......................] - ETA: 17:18 - loss: 0.6888 - acc: 0.5486
2432/9333 [======>.......................] - ETA: 17:06 - loss: 0.6888 - acc: 0.5485
2496/9333 [=======>......................] - ETA: 16:57 - loss: 0.6900 - acc: 0.5469
2560/9333 [=======>......................] - ETA: 16:49 - loss: 0.6893 - acc: 0.5488
2624/9333 [=======>......................] - ETA: 16:41 - loss: 0.6890 - acc: 0.5495
2688/9333 [=======>......................] - ETA: 16:33 - loss: 0.6886 - acc: 0.5499
2752/9333 [=======>......................] - ETA: 16:26 - loss: 0.6886 - acc: 0.5494
2816/9333 [========>.....................] - ETA: 16:17 - loss: 0.6880 - acc: 0.5522
2880/9333 [========>.....................] - ETA: 16:07 - loss: 0.6876 - acc: 0.5545
2944/9333 [========>.....................] - ETA: 15:58 - loss: 0.6875 - acc: 0.5540
3008/9333 [========>.....................] - ETA: 15:50 - loss: 0.6873 - acc: 0.5559
3072/9333 [========>.....................] - ETA: 15:40 - loss: 0.6876 - acc: 0.5553
3136/9333 [=========>....................] - ETA: 15:31 - loss: 0.6875 - acc: 0.5536
3200/9333 [=========>....................] - ETA: 15:22 - loss: 0.6879 - acc: 0.5525
3264/9333 [=========>....................] - ETA: 15:11 - loss: 0.6883 - acc: 0.5518
3328/9333 [=========>....................] - ETA: 15:01 - loss: 0.6884 - acc: 0.5520
3392/9333 [=========>....................] - ETA: 14:52 - loss: 0.6880 - acc: 0.5531
3456/9333 [==========>...................] - ETA: 14:42 - loss: 0.6879 - acc: 0.5521
3520/9333 [==========>...................] - ETA: 14:33 - loss: 0.6881 - acc: 0.5517
3584/9333 [==========>...................] - ETA: 14:23 - loss: 0.6885 - acc: 0.5508
3648/9333 [==========>...................] - ETA: 14:13 - loss: 0.6884 - acc: 0.5507
3712/9333 [==========>...................] - ETA: 14:03 - loss: 0.6882 - acc: 0.5515
3776/9333 [===========>..................] - ETA: 13:53 - loss: 0.6882 - acc: 0.5501
3840/9333 [===========>..................] - ETA: 13:43 - loss: 0.6885 - acc: 0.5484
3904/9333 [===========>..................] - ETA: 13:34 - loss: 0.6886 - acc: 0.5484
3968/9333 [===========>..................] - ETA: 13:25 - loss: 0.6886 - acc: 0.5474
4032/9333 [===========>..................] - ETA: 13:16 - loss: 0.6893 - acc: 0.5449
4096/9333 [============>.................] - ETA: 13:06 - loss: 0.6887 - acc: 0.5471
4160/9333 [============>.................] - ETA: 12:58 - loss: 0.6888 - acc: 0.5466
4224/9333 [============>.................] - ETA: 12:48 - loss: 0.6887 - acc: 0.5469
4288/9333 [============>.................] - ETA: 12:40 - loss: 0.6885 - acc: 0.5471
4352/9333 [============>.................] - ETA: 12:29 - loss: 0.6883 - acc: 0.5480
4416/9333 [=============>................] - ETA: 12:20 - loss: 0.6881 - acc: 0.5489
4480/9333 [=============>................] - ETA: 12:10 - loss: 0.6883 - acc: 0.5480
4544/9333 [=============>................] - ETA: 12:01 - loss: 0.6885 - acc: 0.5478
4608/9333 [=============>................] - ETA: 11:52 - loss: 0.6888 - acc: 0.5469
4672/9333 [==============>...............] - ETA: 11:42 - loss: 0.6887 - acc: 0.5471
4736/9333 [==============>...............] - ETA: 11:33 - loss: 0.6886 - acc: 0.5473
4800/9333 [==============>...............] - ETA: 11:23 - loss: 0.6887 - acc: 0.5477
4864/9333 [==============>...............] - ETA: 11:14 - loss: 0.6883 - acc: 0.5489
4928/9333 [==============>...............] - ETA: 11:04 - loss: 0.6883 - acc: 0.5495
4992/9333 [===============>..............] - ETA: 10:54 - loss: 0.6883 - acc: 0.5495
5056/9333 [===============>..............] - ETA: 10:45 - loss: 0.6881 - acc: 0.5492
5120/9333 [===============>..............] - ETA: 10:35 - loss: 0.6884 - acc: 0.5486
5184/9333 [===============>..............] - ETA: 10:26 - loss: 0.6881 - acc: 0.5500
5248/9333 [===============>..............] - ETA: 10:17 - loss: 0.6879 - acc: 0.5511
5312/9333 [================>.............] - ETA: 10:08 - loss: 0.6878 - acc: 0.5512
5376/9333 [================>.............] - ETA: 9:57 - loss: 0.6877 - acc: 0.5515 
5440/9333 [================>.............] - ETA: 9:48 - loss: 0.6877 - acc: 0.5517
5504/9333 [================>.............] - ETA: 9:39 - loss: 0.6874 - acc: 0.5521
5568/9333 [================>.............] - ETA: 9:29 - loss: 0.6874 - acc: 0.5510
5632/9333 [=================>............] - ETA: 9:19 - loss: 0.6872 - acc: 0.5520
5696/9333 [=================>............] - ETA: 9:10 - loss: 0.6871 - acc: 0.5523
5760/9333 [=================>............] - ETA: 9:01 - loss: 0.6871 - acc: 0.5526
5824/9333 [=================>............] - ETA: 8:51 - loss: 0.6872 - acc: 0.5531
5888/9333 [=================>............] - ETA: 8:42 - loss: 0.6871 - acc: 0.5535
5952/9333 [==================>...........] - ETA: 8:32 - loss: 0.6869 - acc: 0.5543
6016/9333 [==================>...........] - ETA: 8:23 - loss: 0.6870 - acc: 0.5534
6080/9333 [==================>...........] - ETA: 8:13 - loss: 0.6869 - acc: 0.5535
6144/9333 [==================>...........] - ETA: 8:04 - loss: 0.6870 - acc: 0.5529
6208/9333 [==================>...........] - ETA: 7:55 - loss: 0.6873 - acc: 0.5512
6272/9333 [===================>..........] - ETA: 7:45 - loss: 0.6871 - acc: 0.5512
6336/9333 [===================>..........] - ETA: 7:36 - loss: 0.6870 - acc: 0.5511
6400/9333 [===================>..........] - ETA: 7:26 - loss: 0.6872 - acc: 0.5506
6464/9333 [===================>..........] - ETA: 7:17 - loss: 0.6870 - acc: 0.5503
6528/9333 [===================>..........] - ETA: 7:07 - loss: 0.6869 - acc: 0.5513
6592/9333 [====================>.........] - ETA: 6:57 - loss: 0.6868 - acc: 0.5510
6656/9333 [====================>.........] - ETA: 6:48 - loss: 0.6868 - acc: 0.5512
6720/9333 [====================>.........] - ETA: 6:38 - loss: 0.6866 - acc: 0.5524
6784/9333 [====================>.........] - ETA: 6:28 - loss: 0.6865 - acc: 0.5525
6848/9333 [=====================>........] - ETA: 6:18 - loss: 0.6867 - acc: 0.5524
6912/9333 [=====================>........] - ETA: 6:08 - loss: 0.6865 - acc: 0.5530
6976/9333 [=====================>........] - ETA: 5:58 - loss: 0.6867 - acc: 0.5520
7040/9333 [=====================>........] - ETA: 5:49 - loss: 0.6866 - acc: 0.5523
7104/9333 [=====================>........] - ETA: 5:39 - loss: 0.6868 - acc: 0.5512
7168/9333 [======================>.......] - ETA: 5:29 - loss: 0.6869 - acc: 0.5512
7232/9333 [======================>.......] - ETA: 5:19 - loss: 0.6867 - acc: 0.5520
7296/9333 [======================>.......] - ETA: 5:09 - loss: 0.6866 - acc: 0.5519
7360/9333 [======================>.......] - ETA: 5:00 - loss: 0.6867 - acc: 0.5516
7424/9333 [======================>.......] - ETA: 4:50 - loss: 0.6869 - acc: 0.5511
7488/9333 [=======================>......] - ETA: 4:40 - loss: 0.6867 - acc: 0.5515
7552/9333 [=======================>......] - ETA: 4:31 - loss: 0.6870 - acc: 0.5510
7616/9333 [=======================>......] - ETA: 4:21 - loss: 0.6871 - acc: 0.5506
7680/9333 [=======================>......] - ETA: 4:11 - loss: 0.6868 - acc: 0.5513
7744/9333 [=======================>......] - ETA: 4:01 - loss: 0.6864 - acc: 0.5522
7808/9333 [========================>.....] - ETA: 3:52 - loss: 0.6862 - acc: 0.5526
7872/9333 [========================>.....] - ETA: 3:42 - loss: 0.6861 - acc: 0.5530
7936/9333 [========================>.....] - ETA: 3:32 - loss: 0.6863 - acc: 0.5524
8000/9333 [========================>.....] - ETA: 3:23 - loss: 0.6862 - acc: 0.5523
8064/9333 [========================>.....] - ETA: 3:13 - loss: 0.6865 - acc: 0.5512
8128/9333 [=========================>....] - ETA: 3:03 - loss: 0.6865 - acc: 0.5511
8192/9333 [=========================>....] - ETA: 2:54 - loss: 0.6866 - acc: 0.5504
8256/9333 [=========================>....] - ETA: 2:44 - loss: 0.6867 - acc: 0.5499
8320/9333 [=========================>....] - ETA: 2:34 - loss: 0.6869 - acc: 0.5489
8384/9333 [=========================>....] - ETA: 2:24 - loss: 0.6869 - acc: 0.5489
8448/9333 [==========================>...] - ETA: 2:15 - loss: 0.6868 - acc: 0.5490
8512/9333 [==========================>...] - ETA: 2:05 - loss: 0.6867 - acc: 0.5491
8576/9333 [==========================>...] - ETA: 1:55 - loss: 0.6865 - acc: 0.5500
8640/9333 [==========================>...] - ETA: 1:45 - loss: 0.6862 - acc: 0.5503
8704/9333 [==========================>...] - ETA: 1:35 - loss: 0.6863 - acc: 0.5504
8768/9333 [===========================>..] - ETA: 1:26 - loss: 0.6864 - acc: 0.5503
8832/9333 [===========================>..] - ETA: 1:16 - loss: 0.6864 - acc: 0.5500
8896/9333 [===========================>..] - ETA: 1:06 - loss: 0.6863 - acc: 0.5501
8960/9333 [===========================>..] - ETA: 56s - loss: 0.6864 - acc: 0.5500 
9024/9333 [============================>.] - ETA: 47s - loss: 0.6863 - acc: 0.5501
9088/9333 [============================>.] - ETA: 37s - loss: 0.6865 - acc: 0.5496
9152/9333 [============================>.] - ETA: 27s - loss: 0.6863 - acc: 0.5502
9216/9333 [============================>.] - ETA: 17s - loss: 0.6865 - acc: 0.5494
9280/9333 [============================>.] - ETA: 8s - loss: 0.6866 - acc: 0.5490 
9333/9333 [==============================] - 1485s 159ms/step - loss: 0.6865 - acc: 0.5490 - val_loss: 0.6999 - val_acc: 0.5053

Epoch 00010: val_acc did not improve from 0.53520
样本个数 1296
样本个数 2592
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fc74c424390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fc74c424390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fc74c3b9750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fc74c3b9750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c15e790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c15e790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74bc21f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74bc21f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc737a6ce10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc737a6ce10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74bc21b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74bc21b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc74bc21c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc74bc21c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7437bd350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7437bd350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc7437a2890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc7437a2890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7439377d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7439377d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc736b4f410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc736b4f410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7437fdbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7437fdbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a83b44d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a83b44d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc743a357d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc743a357d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc743522050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc743522050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc743697cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc743697cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc74371be50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc74371be50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a835a5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a835a5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a8322a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a8322a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7432f2690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7432f2690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc743318150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc743318150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc743345890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc743345890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72e290190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc72e290190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc743017410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc743017410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc742ebf210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc742ebf210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7432013d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7432013d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc743017250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc743017250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc742ebbb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc742ebbb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74308df90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74308df90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc742b8ff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc742b8ff10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc742d133d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc742d133d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc742eec390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc742eec390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc742bdf550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc742bdf550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74299a050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74299a050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7429f45d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7429f45d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc742bc4590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc742bc4590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc74299a790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc74299a790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc742f25090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc742f25090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc742643b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc742643b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7425a36d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7425a36d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7426b0950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7426b0950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7426b5b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7426b5b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7425a3390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7425a3390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc742638210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc742638210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc742592750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc742592750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a8322b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a8322b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc742356290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc742356290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7426b8e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7426b8e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc742055450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc742055450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc741f6aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc741f6aa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7422b3210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7422b3210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7422ca450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc7422ca450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc741f170d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc741f170d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc741d1f850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc741d1f850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc741f57c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc741f57c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc741bbc490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc741bbc490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc741d1f950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc741d1f950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc741c8b950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc741c8b950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc741bfb8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc741bfb8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7419b5890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc7419b5890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc741a24550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc741a24550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc741bfb6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc741bfb6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7419246d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7419246d0>>: AttributeError: module 'gast' has no attribute 'Str'
window_select17-21.py:152: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/2592 [..............................] - ETA: 4:47
 128/2592 [>.............................] - ETA: 3:25
 192/2592 [=>............................] - ETA: 2:59
 256/2592 [=>............................] - ETA: 2:40
 320/2592 [==>...........................] - ETA: 2:27
 384/2592 [===>..........................] - ETA: 2:16
 448/2592 [====>.........................] - ETA: 2:11
 512/2592 [====>.........................] - ETA: 2:07
 576/2592 [=====>........................] - ETA: 2:00
 640/2592 [======>.......................] - ETA: 1:53
 704/2592 [=======>......................] - ETA: 1:48
 768/2592 [=======>......................] - ETA: 1:43
 832/2592 [========>.....................] - ETA: 1:38
 896/2592 [=========>....................] - ETA: 1:33
 960/2592 [==========>...................] - ETA: 1:29
1024/2592 [==========>...................] - ETA: 1:25
1088/2592 [===========>..................] - ETA: 1:21
1152/2592 [============>.................] - ETA: 1:17
1216/2592 [=============>................] - ETA: 1:13
1280/2592 [=============>................] - ETA: 1:10
1344/2592 [==============>...............] - ETA: 1:06
1408/2592 [===============>..............] - ETA: 1:03
1472/2592 [================>.............] - ETA: 59s 
1536/2592 [================>.............] - ETA: 56s
1600/2592 [=================>............] - ETA: 52s
1664/2592 [==================>...........] - ETA: 48s
1728/2592 [===================>..........] - ETA: 45s
1792/2592 [===================>..........] - ETA: 42s
1856/2592 [====================>.........] - ETA: 38s
1920/2592 [=====================>........] - ETA: 35s
1984/2592 [=====================>........] - ETA: 32s
2048/2592 [======================>.......] - ETA: 28s
2112/2592 [=======================>......] - ETA: 25s
2176/2592 [========================>.....] - ETA: 21s
2240/2592 [========================>.....] - ETA: 18s
2304/2592 [=========================>....] - ETA: 15s
2368/2592 [==========================>...] - ETA: 11s
2432/2592 [===========================>..] - ETA: 8s 
2496/2592 [===========================>..] - ETA: 5s
2560/2592 [============================>.] - ETA: 1s
2592/2592 [==============================] - 136s 52ms/step
loss: 0.68742792473899
acc: 0.5443672839506173
样本个数 5185
样本个数 10370
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fc2a8399950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fc2a8399950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fc2a83245d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fc2a83245d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73e6260d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73e6260d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73e8c4410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73e8c4410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc737156090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc737156090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73e8b9750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73e8b9750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73e8c4ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73e8c4ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c44ba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c44ba90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74bf7fed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74bf7fed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc74bd9c690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc74bd9c690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c28f690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c28f690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc741a5e310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc741a5e310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c1d8f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c1d8f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74c34e290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc74c34e290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc74c34e350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc74c34e350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c2e8390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74c2e8390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73e7d4090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73e7d4090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74be28310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74be28310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a80db950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a80db950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc2a0746bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc2a0746bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a80ee090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a80ee090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2a80db750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2a80db750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a069f390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a069f390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a05eb390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a05eb390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc2a04abf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc2a04abf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a805c910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a805c910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2a06ae150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2a06ae150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a04a1d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a04a1d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a04a98d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a04a98d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc2a0225f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc2a0225f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a0144150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a0144150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2a02838d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2a02838d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a0257d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2a0257d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a01db550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a01db550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc298616a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc298616a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2986c3890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2986c3890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2a01e3910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2a01e3910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc29862f750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc29862f750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc298635f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc298635f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc298428b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc298428b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2982c06d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2982c06d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2984776d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2984776d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2982f8b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2982f8b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2980f7890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2980f7890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc25875c2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc25875c2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc258758610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc258758610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc298111750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc298111750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2980f7cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2980f7cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc258555210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc258555210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc2584488d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc2584488d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2585514d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2585514d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc258555750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc258555750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc258297050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc258297050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc258275710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc258275710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc258163a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc258163a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2584cca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2584cca90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc258275a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc258275a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc258445690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc258445690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2407a0d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2407a0d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc2406541d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc2406541d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2406cb6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc2406cb6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc258209d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc258209d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc24058dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc24058dc50>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 9333 samples, validate on 1037 samples
Epoch 1/10

  64/9333 [..............................] - ETA: 1:17:54 - loss: 0.7871 - acc: 0.5000
 128/9333 [..............................] - ETA: 51:20 - loss: 0.7974 - acc: 0.4609  
 192/9333 [..............................] - ETA: 42:51 - loss: 0.7759 - acc: 0.5000
 256/9333 [..............................] - ETA: 38:53 - loss: 0.7621 - acc: 0.5000
 320/9333 [>.............................] - ETA: 35:36 - loss: 0.7745 - acc: 0.4813
 384/9333 [>.............................] - ETA: 33:20 - loss: 0.7653 - acc: 0.4766
 448/9333 [>.............................] - ETA: 31:42 - loss: 0.7622 - acc: 0.4821
 512/9333 [>.............................] - ETA: 30:31 - loss: 0.7523 - acc: 0.5059
 576/9333 [>.............................] - ETA: 29:38 - loss: 0.7620 - acc: 0.5052
 640/9333 [=>............................] - ETA: 28:36 - loss: 0.7542 - acc: 0.5125
 704/9333 [=>............................] - ETA: 27:54 - loss: 0.7530 - acc: 0.5085
 768/9333 [=>............................] - ETA: 27:14 - loss: 0.7514 - acc: 0.5078
 832/9333 [=>............................] - ETA: 26:46 - loss: 0.7507 - acc: 0.5072
 896/9333 [=>............................] - ETA: 26:17 - loss: 0.7503 - acc: 0.5078
 960/9333 [==>...........................] - ETA: 25:51 - loss: 0.7505 - acc: 0.5042
1024/9333 [==>...........................] - ETA: 25:21 - loss: 0.7495 - acc: 0.5039
1088/9333 [==>...........................] - ETA: 24:59 - loss: 0.7505 - acc: 0.4991
1152/9333 [==>...........................] - ETA: 24:35 - loss: 0.7495 - acc: 0.5009
1216/9333 [==>...........................] - ETA: 24:12 - loss: 0.7473 - acc: 0.5016
1280/9333 [===>..........................] - ETA: 23:53 - loss: 0.7423 - acc: 0.5047
1344/9333 [===>..........................] - ETA: 23:41 - loss: 0.7418 - acc: 0.5015
1408/9333 [===>..........................] - ETA: 23:21 - loss: 0.7411 - acc: 0.5021
1472/9333 [===>..........................] - ETA: 23:05 - loss: 0.7393 - acc: 0.5061
1536/9333 [===>..........................] - ETA: 22:50 - loss: 0.7372 - acc: 0.5091
1600/9333 [====>.........................] - ETA: 22:35 - loss: 0.7375 - acc: 0.5088
1664/9333 [====>.........................] - ETA: 22:20 - loss: 0.7360 - acc: 0.5084
1728/9333 [====>.........................] - ETA: 22:04 - loss: 0.7343 - acc: 0.5110
1792/9333 [====>.........................] - ETA: 21:50 - loss: 0.7349 - acc: 0.5084
1856/9333 [====>.........................] - ETA: 21:38 - loss: 0.7343 - acc: 0.5081
1920/9333 [=====>........................] - ETA: 21:26 - loss: 0.7335 - acc: 0.5083
1984/9333 [=====>........................] - ETA: 21:11 - loss: 0.7328 - acc: 0.5096
2048/9333 [=====>........................] - ETA: 21:01 - loss: 0.7341 - acc: 0.5068
2112/9333 [=====>........................] - ETA: 20:47 - loss: 0.7347 - acc: 0.5052
2176/9333 [=====>........................] - ETA: 20:40 - loss: 0.7330 - acc: 0.5083
2240/9333 [======>.......................] - ETA: 20:25 - loss: 0.7328 - acc: 0.5071
2304/9333 [======>.......................] - ETA: 20:13 - loss: 0.7325 - acc: 0.5078
2368/9333 [======>.......................] - ETA: 20:00 - loss: 0.7323 - acc: 0.5063
2432/9333 [======>.......................] - ETA: 19:47 - loss: 0.7308 - acc: 0.5078
2496/9333 [=======>......................] - ETA: 19:36 - loss: 0.7299 - acc: 0.5088
2560/9333 [=======>......................] - ETA: 19:24 - loss: 0.7301 - acc: 0.5066
2624/9333 [=======>......................] - ETA: 19:12 - loss: 0.7296 - acc: 0.5069
2688/9333 [=======>......................] - ETA: 19:00 - loss: 0.7297 - acc: 0.5060
2752/9333 [=======>......................] - ETA: 18:49 - loss: 0.7289 - acc: 0.5062
2816/9333 [========>.....................] - ETA: 18:35 - loss: 0.7282 - acc: 0.5071
2880/9333 [========>.....................] - ETA: 18:26 - loss: 0.7282 - acc: 0.5056
2944/9333 [========>.....................] - ETA: 18:15 - loss: 0.7271 - acc: 0.5071
3008/9333 [========>.....................] - ETA: 18:03 - loss: 0.7265 - acc: 0.5076
3072/9333 [========>.....................] - ETA: 17:51 - loss: 0.7254 - acc: 0.5081
3136/9333 [=========>....................] - ETA: 17:42 - loss: 0.7270 - acc: 0.5061
3200/9333 [=========>....................] - ETA: 17:33 - loss: 0.7275 - acc: 0.5066
3264/9333 [=========>....................] - ETA: 17:20 - loss: 0.7269 - acc: 0.5077
3328/9333 [=========>....................] - ETA: 17:10 - loss: 0.7268 - acc: 0.5066
3392/9333 [=========>....................] - ETA: 16:59 - loss: 0.7277 - acc: 0.5044
3456/9333 [==========>...................] - ETA: 16:47 - loss: 0.7279 - acc: 0.5043
3520/9333 [==========>...................] - ETA: 16:36 - loss: 0.7276 - acc: 0.5037
3584/9333 [==========>...................] - ETA: 16:23 - loss: 0.7274 - acc: 0.5039
3648/9333 [==========>...................] - ETA: 16:10 - loss: 0.7273 - acc: 0.5030
3712/9333 [==========>...................] - ETA: 16:00 - loss: 0.7267 - acc: 0.5040
3776/9333 [===========>..................] - ETA: 15:49 - loss: 0.7257 - acc: 0.5056
3840/9333 [===========>..................] - ETA: 15:39 - loss: 0.7254 - acc: 0.5047
3904/9333 [===========>..................] - ETA: 15:28 - loss: 0.7245 - acc: 0.5064
3968/9333 [===========>..................] - ETA: 15:18 - loss: 0.7235 - acc: 0.5076
4032/9333 [===========>..................] - ETA: 15:06 - loss: 0.7238 - acc: 0.5072
4096/9333 [============>.................] - ETA: 14:54 - loss: 0.7241 - acc: 0.5063
4160/9333 [============>.................] - ETA: 14:43 - loss: 0.7240 - acc: 0.5050
4224/9333 [============>.................] - ETA: 14:33 - loss: 0.7238 - acc: 0.5050
4288/9333 [============>.................] - ETA: 14:23 - loss: 0.7235 - acc: 0.5047
4352/9333 [============>.................] - ETA: 14:11 - loss: 0.7233 - acc: 0.5041
4416/9333 [=============>................] - ETA: 13:59 - loss: 0.7235 - acc: 0.5029
4480/9333 [=============>................] - ETA: 13:48 - loss: 0.7233 - acc: 0.5031
4544/9333 [=============>................] - ETA: 13:37 - loss: 0.7229 - acc: 0.5026
4608/9333 [=============>................] - ETA: 13:26 - loss: 0.7230 - acc: 0.5015
4672/9333 [==============>...............] - ETA: 13:15 - loss: 0.7230 - acc: 0.5011
4736/9333 [==============>...............] - ETA: 13:03 - loss: 0.7227 - acc: 0.5002
4800/9333 [==============>...............] - ETA: 12:52 - loss: 0.7222 - acc: 0.5010
4864/9333 [==============>...............] - ETA: 12:41 - loss: 0.7220 - acc: 0.5014
4928/9333 [==============>...............] - ETA: 12:30 - loss: 0.7219 - acc: 0.5010
4992/9333 [===============>..............] - ETA: 12:19 - loss: 0.7214 - acc: 0.5014
5056/9333 [===============>..............] - ETA: 12:08 - loss: 0.7216 - acc: 0.5008
5120/9333 [===============>..............] - ETA: 11:57 - loss: 0.7215 - acc: 0.5006
5184/9333 [===============>..............] - ETA: 11:47 - loss: 0.7210 - acc: 0.5014
5248/9333 [===============>..............] - ETA: 11:36 - loss: 0.7207 - acc: 0.5015
5312/9333 [================>.............] - ETA: 11:26 - loss: 0.7209 - acc: 0.5009
5376/9333 [================>.............] - ETA: 11:14 - loss: 0.7204 - acc: 0.5020
5440/9333 [================>.............] - ETA: 11:04 - loss: 0.7203 - acc: 0.5026
5504/9333 [================>.............] - ETA: 10:54 - loss: 0.7203 - acc: 0.5027
5568/9333 [================>.............] - ETA: 10:43 - loss: 0.7203 - acc: 0.5018
5632/9333 [=================>............] - ETA: 10:32 - loss: 0.7198 - acc: 0.5027
5696/9333 [=================>............] - ETA: 10:22 - loss: 0.7195 - acc: 0.5030
5760/9333 [=================>............] - ETA: 10:12 - loss: 0.7191 - acc: 0.5036
5824/9333 [=================>............] - ETA: 10:00 - loss: 0.7189 - acc: 0.5038
5888/9333 [=================>............] - ETA: 9:50 - loss: 0.7188 - acc: 0.5032 
5952/9333 [==================>...........] - ETA: 9:39 - loss: 0.7186 - acc: 0.5039
6016/9333 [==================>...........] - ETA: 9:28 - loss: 0.7185 - acc: 0.5040
6080/9333 [==================>...........] - ETA: 9:17 - loss: 0.7187 - acc: 0.5031
6144/9333 [==================>...........] - ETA: 9:06 - loss: 0.7187 - acc: 0.5028
6208/9333 [==================>...........] - ETA: 8:56 - loss: 0.7185 - acc: 0.5021
6272/9333 [===================>..........] - ETA: 8:45 - loss: 0.7184 - acc: 0.5016
6336/9333 [===================>..........] - ETA: 8:34 - loss: 0.7183 - acc: 0.5016
6400/9333 [===================>..........] - ETA: 8:23 - loss: 0.7180 - acc: 0.5023
6464/9333 [===================>..........] - ETA: 8:12 - loss: 0.7175 - acc: 0.5029
6528/9333 [===================>..........] - ETA: 8:01 - loss: 0.7173 - acc: 0.5034
6592/9333 [====================>.........] - ETA: 7:50 - loss: 0.7171 - acc: 0.5036
6656/9333 [====================>.........] - ETA: 7:39 - loss: 0.7170 - acc: 0.5038
6720/9333 [====================>.........] - ETA: 7:27 - loss: 0.7171 - acc: 0.5033
6784/9333 [====================>.........] - ETA: 7:16 - loss: 0.7170 - acc: 0.5038
6848/9333 [=====================>........] - ETA: 7:05 - loss: 0.7168 - acc: 0.5038
6912/9333 [=====================>........] - ETA: 6:54 - loss: 0.7167 - acc: 0.5039
6976/9333 [=====================>........] - ETA: 6:43 - loss: 0.7165 - acc: 0.5039
7040/9333 [=====================>........] - ETA: 6:32 - loss: 0.7161 - acc: 0.5053
7104/9333 [=====================>........] - ETA: 6:21 - loss: 0.7157 - acc: 0.5065
7168/9333 [======================>.......] - ETA: 6:10 - loss: 0.7155 - acc: 0.5073
7232/9333 [======================>.......] - ETA: 5:58 - loss: 0.7157 - acc: 0.5073
7296/9333 [======================>.......] - ETA: 5:47 - loss: 0.7154 - acc: 0.5078
7360/9333 [======================>.......] - ETA: 5:36 - loss: 0.7153 - acc: 0.5079
7424/9333 [======================>.......] - ETA: 5:25 - loss: 0.7152 - acc: 0.5086
7488/9333 [=======================>......] - ETA: 5:14 - loss: 0.7150 - acc: 0.5091
7552/9333 [=======================>......] - ETA: 5:03 - loss: 0.7151 - acc: 0.5087
7616/9333 [=======================>......] - ETA: 4:52 - loss: 0.7151 - acc: 0.5083
7680/9333 [=======================>......] - ETA: 4:41 - loss: 0.7149 - acc: 0.5085
7744/9333 [=======================>......] - ETA: 4:30 - loss: 0.7146 - acc: 0.5090
7808/9333 [========================>.....] - ETA: 4:19 - loss: 0.7143 - acc: 0.5095
7872/9333 [========================>.....] - ETA: 4:08 - loss: 0.7141 - acc: 0.5099
7936/9333 [========================>.....] - ETA: 3:57 - loss: 0.7140 - acc: 0.5102
8000/9333 [========================>.....] - ETA: 3:46 - loss: 0.7138 - acc: 0.5104
8064/9333 [========================>.....] - ETA: 3:35 - loss: 0.7138 - acc: 0.5098
8128/9333 [=========================>....] - ETA: 3:24 - loss: 0.7136 - acc: 0.5097
8192/9333 [=========================>....] - ETA: 3:13 - loss: 0.7134 - acc: 0.5095
8256/9333 [=========================>....] - ETA: 3:03 - loss: 0.7133 - acc: 0.5096
8320/9333 [=========================>....] - ETA: 2:52 - loss: 0.7132 - acc: 0.5096
8384/9333 [=========================>....] - ETA: 2:41 - loss: 0.7128 - acc: 0.5099
8448/9333 [==========================>...] - ETA: 2:30 - loss: 0.7126 - acc: 0.5102
8512/9333 [==========================>...] - ETA: 2:19 - loss: 0.7125 - acc: 0.5105
8576/9333 [==========================>...] - ETA: 2:08 - loss: 0.7123 - acc: 0.5107
8640/9333 [==========================>...] - ETA: 1:57 - loss: 0.7122 - acc: 0.5112
8704/9333 [==========================>...] - ETA: 1:47 - loss: 0.7122 - acc: 0.5107
8768/9333 [===========================>..] - ETA: 1:36 - loss: 0.7119 - acc: 0.5114
8832/9333 [===========================>..] - ETA: 1:25 - loss: 0.7117 - acc: 0.5118
8896/9333 [===========================>..] - ETA: 1:14 - loss: 0.7116 - acc: 0.5117
8960/9333 [===========================>..] - ETA: 1:03 - loss: 0.7114 - acc: 0.5123
9024/9333 [============================>.] - ETA: 52s - loss: 0.7115 - acc: 0.5112 
9088/9333 [============================>.] - ETA: 41s - loss: 0.7115 - acc: 0.5102
9152/9333 [============================>.] - ETA: 30s - loss: 0.7112 - acc: 0.5111
9216/9333 [============================>.] - ETA: 19s - loss: 0.7113 - acc: 0.5102
9280/9333 [============================>.] - ETA: 9s - loss: 0.7108 - acc: 0.5114 
9333/9333 [==============================] - 1658s 178ms/step - loss: 0.7105 - acc: 0.5119 - val_loss: 0.6918 - val_acc: 0.5429

Epoch 00001: val_acc improved from -inf to 0.54291, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window18/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 2/10

  64/9333 [..............................] - ETA: 26:06 - loss: 0.6992 - acc: 0.4531
 128/9333 [..............................] - ETA: 24:41 - loss: 0.7017 - acc: 0.4922
 192/9333 [..............................] - ETA: 25:59 - loss: 0.6950 - acc: 0.5260
 256/9333 [..............................] - ETA: 25:47 - loss: 0.6947 - acc: 0.5391
 320/9333 [>.............................] - ETA: 26:03 - loss: 0.6921 - acc: 0.5469
 384/9333 [>.............................] - ETA: 25:35 - loss: 0.6972 - acc: 0.5260
 448/9333 [>.............................] - ETA: 25:52 - loss: 0.7017 - acc: 0.5223
 512/9333 [>.............................] - ETA: 25:46 - loss: 0.7072 - acc: 0.5234
 576/9333 [>.............................] - ETA: 25:55 - loss: 0.7084 - acc: 0.5174
 640/9333 [=>............................] - ETA: 25:34 - loss: 0.7054 - acc: 0.5172
 704/9333 [=>............................] - ETA: 25:32 - loss: 0.7041 - acc: 0.5199
 768/9333 [=>............................] - ETA: 25:15 - loss: 0.7025 - acc: 0.5260
 832/9333 [=>............................] - ETA: 25:01 - loss: 0.6992 - acc: 0.5349
 896/9333 [=>............................] - ETA: 24:51 - loss: 0.6981 - acc: 0.5368
 960/9333 [==>...........................] - ETA: 24:43 - loss: 0.6982 - acc: 0.5365
1024/9333 [==>...........................] - ETA: 24:28 - loss: 0.6977 - acc: 0.5381
1088/9333 [==>...........................] - ETA: 24:20 - loss: 0.6990 - acc: 0.5322
1152/9333 [==>...........................] - ETA: 24:01 - loss: 0.6993 - acc: 0.5304
1216/9333 [==>...........................] - ETA: 23:51 - loss: 0.7001 - acc: 0.5280
1280/9333 [===>..........................] - ETA: 23:37 - loss: 0.7009 - acc: 0.5250
1344/9333 [===>..........................] - ETA: 23:27 - loss: 0.7013 - acc: 0.5260
1408/9333 [===>..........................] - ETA: 23:12 - loss: 0.7015 - acc: 0.5241
1472/9333 [===>..........................] - ETA: 23:01 - loss: 0.6998 - acc: 0.5299
1536/9333 [===>..........................] - ETA: 22:47 - loss: 0.7007 - acc: 0.5254
1600/9333 [====>.........................] - ETA: 22:37 - loss: 0.7012 - acc: 0.5256
1664/9333 [====>.........................] - ETA: 22:23 - loss: 0.7012 - acc: 0.5240
1728/9333 [====>.........................] - ETA: 22:12 - loss: 0.7005 - acc: 0.5243
1792/9333 [====>.........................] - ETA: 21:58 - loss: 0.7011 - acc: 0.5195
1856/9333 [====>.........................] - ETA: 21:48 - loss: 0.7015 - acc: 0.5156
1920/9333 [=====>........................] - ETA: 21:38 - loss: 0.7027 - acc: 0.5104
1984/9333 [=====>........................] - ETA: 21:32 - loss: 0.7018 - acc: 0.5156
2048/9333 [=====>........................] - ETA: 21:18 - loss: 0.7022 - acc: 0.5142
2112/9333 [=====>........................] - ETA: 21:11 - loss: 0.7036 - acc: 0.5095
2176/9333 [=====>........................] - ETA: 20:59 - loss: 0.7039 - acc: 0.5083
2240/9333 [======>.......................] - ETA: 20:50 - loss: 0.7038 - acc: 0.5076
2304/9333 [======>.......................] - ETA: 20:34 - loss: 0.7041 - acc: 0.5065
2368/9333 [======>.......................] - ETA: 20:25 - loss: 0.7041 - acc: 0.5046
2432/9333 [======>.......................] - ETA: 20:16 - loss: 0.7044 - acc: 0.5037
2496/9333 [=======>......................] - ETA: 20:02 - loss: 0.7042 - acc: 0.5036
2560/9333 [=======>......................] - ETA: 19:51 - loss: 0.7049 - acc: 0.5023
2624/9333 [=======>......................] - ETA: 19:41 - loss: 0.7051 - acc: 0.5004
2688/9333 [=======>......................] - ETA: 19:28 - loss: 0.7059 - acc: 0.4974
2752/9333 [=======>......................] - ETA: 19:16 - loss: 0.7054 - acc: 0.4996
2816/9333 [========>.....................] - ETA: 19:03 - loss: 0.7054 - acc: 0.4996
2880/9333 [========>.....................] - ETA: 18:51 - loss: 0.7055 - acc: 0.4990
2944/9333 [========>.....................] - ETA: 18:39 - loss: 0.7052 - acc: 0.5010
3008/9333 [========>.....................] - ETA: 18:30 - loss: 0.7051 - acc: 0.5023
3072/9333 [========>.....................] - ETA: 18:18 - loss: 0.7052 - acc: 0.5020
3136/9333 [=========>....................] - ETA: 18:06 - loss: 0.7056 - acc: 0.5003
3200/9333 [=========>....................] - ETA: 17:56 - loss: 0.7053 - acc: 0.5006
3264/9333 [=========>....................] - ETA: 17:44 - loss: 0.7049 - acc: 0.5012
3328/9333 [=========>....................] - ETA: 17:34 - loss: 0.7048 - acc: 0.5009
3392/9333 [=========>....................] - ETA: 17:21 - loss: 0.7044 - acc: 0.5024
3456/9333 [==========>...................] - ETA: 17:09 - loss: 0.7042 - acc: 0.5012
3520/9333 [==========>...................] - ETA: 16:59 - loss: 0.7038 - acc: 0.5026
3584/9333 [==========>...................] - ETA: 16:48 - loss: 0.7032 - acc: 0.5045
3648/9333 [==========>...................] - ETA: 16:39 - loss: 0.7027 - acc: 0.5052
3712/9333 [==========>...................] - ETA: 16:27 - loss: 0.7031 - acc: 0.5032
3776/9333 [===========>..................] - ETA: 16:15 - loss: 0.7030 - acc: 0.5032
3840/9333 [===========>..................] - ETA: 16:04 - loss: 0.7032 - acc: 0.5021
3904/9333 [===========>..................] - ETA: 15:52 - loss: 0.7029 - acc: 0.5026
3968/9333 [===========>..................] - ETA: 15:39 - loss: 0.7028 - acc: 0.5028
4032/9333 [===========>..................] - ETA: 15:29 - loss: 0.7029 - acc: 0.5030
4096/9333 [============>.................] - ETA: 15:18 - loss: 0.7026 - acc: 0.5039
4160/9333 [============>.................] - ETA: 15:06 - loss: 0.7024 - acc: 0.5046
4224/9333 [============>.................] - ETA: 14:55 - loss: 0.7026 - acc: 0.5043
4288/9333 [============>.................] - ETA: 14:43 - loss: 0.7023 - acc: 0.5047
4352/9333 [============>.................] - ETA: 14:31 - loss: 0.7021 - acc: 0.5057
4416/9333 [=============>................] - ETA: 14:19 - loss: 0.7020 - acc: 0.5068
4480/9333 [=============>................] - ETA: 14:08 - loss: 0.7020 - acc: 0.5074
4544/9333 [=============>................] - ETA: 13:56 - loss: 0.7017 - acc: 0.5077
4608/9333 [=============>................] - ETA: 13:44 - loss: 0.7018 - acc: 0.5076
4672/9333 [==============>...............] - ETA: 13:33 - loss: 0.7019 - acc: 0.5064
4736/9333 [==============>...............] - ETA: 13:21 - loss: 0.7016 - acc: 0.5076
4800/9333 [==============>...............] - ETA: 13:11 - loss: 0.7018 - acc: 0.5077
4864/9333 [==============>...............] - ETA: 12:59 - loss: 0.7018 - acc: 0.5090
4928/9333 [==============>...............] - ETA: 12:47 - loss: 0.7016 - acc: 0.5089
4992/9333 [===============>..............] - ETA: 12:35 - loss: 0.7013 - acc: 0.5090
5056/9333 [===============>..............] - ETA: 12:23 - loss: 0.7015 - acc: 0.5077
5120/9333 [===============>..............] - ETA: 12:12 - loss: 0.7012 - acc: 0.5082
5184/9333 [===============>..............] - ETA: 12:00 - loss: 0.7009 - acc: 0.5095
5248/9333 [===============>..............] - ETA: 11:49 - loss: 0.7006 - acc: 0.5099
5312/9333 [================>.............] - ETA: 11:37 - loss: 0.7007 - acc: 0.5096
5376/9333 [================>.............] - ETA: 11:25 - loss: 0.7009 - acc: 0.5095
5440/9333 [================>.............] - ETA: 11:14 - loss: 0.7006 - acc: 0.5097
5504/9333 [================>.............] - ETA: 11:03 - loss: 0.7008 - acc: 0.5093
5568/9333 [================>.............] - ETA: 10:52 - loss: 0.7008 - acc: 0.5093
5632/9333 [=================>............] - ETA: 10:40 - loss: 0.7010 - acc: 0.5085
5696/9333 [=================>............] - ETA: 10:29 - loss: 0.7005 - acc: 0.5100
5760/9333 [=================>............] - ETA: 10:17 - loss: 0.7004 - acc: 0.5102
5824/9333 [=================>............] - ETA: 10:07 - loss: 0.7002 - acc: 0.5108
5888/9333 [=================>............] - ETA: 9:56 - loss: 0.7005 - acc: 0.5097 
5952/9333 [==================>...........] - ETA: 9:44 - loss: 0.7004 - acc: 0.5109
6016/9333 [==================>...........] - ETA: 9:33 - loss: 0.7001 - acc: 0.5120
6080/9333 [==================>...........] - ETA: 9:22 - loss: 0.7000 - acc: 0.5125
6144/9333 [==================>...........] - ETA: 9:11 - loss: 0.7001 - acc: 0.5125
6208/9333 [==================>...........] - ETA: 9:00 - loss: 0.7002 - acc: 0.5127
6272/9333 [===================>..........] - ETA: 8:48 - loss: 0.7005 - acc: 0.5115
6336/9333 [===================>..........] - ETA: 8:37 - loss: 0.7008 - acc: 0.5107
6400/9333 [===================>..........] - ETA: 8:26 - loss: 0.7006 - acc: 0.5112
6464/9333 [===================>..........] - ETA: 8:15 - loss: 0.7005 - acc: 0.5122
6528/9333 [===================>..........] - ETA: 8:04 - loss: 0.7005 - acc: 0.5121
6592/9333 [====================>.........] - ETA: 7:54 - loss: 0.7007 - acc: 0.5109
6656/9333 [====================>.........] - ETA: 7:42 - loss: 0.7006 - acc: 0.5113
6720/9333 [====================>.........] - ETA: 7:32 - loss: 0.7004 - acc: 0.5121
6784/9333 [====================>.........] - ETA: 7:20 - loss: 0.7005 - acc: 0.5118
6848/9333 [=====================>........] - ETA: 7:09 - loss: 0.7003 - acc: 0.5123
6912/9333 [=====================>........] - ETA: 6:58 - loss: 0.7002 - acc: 0.5126
6976/9333 [=====================>........] - ETA: 6:47 - loss: 0.7005 - acc: 0.5123
7040/9333 [=====================>........] - ETA: 6:35 - loss: 0.7003 - acc: 0.5118
7104/9333 [=====================>........] - ETA: 6:24 - loss: 0.7003 - acc: 0.5114
7168/9333 [======================>.......] - ETA: 6:13 - loss: 0.7000 - acc: 0.5128
7232/9333 [======================>.......] - ETA: 6:02 - loss: 0.6999 - acc: 0.5129
7296/9333 [======================>.......] - ETA: 5:50 - loss: 0.6997 - acc: 0.5133
7360/9333 [======================>.......] - ETA: 5:38 - loss: 0.6998 - acc: 0.5126
7424/9333 [======================>.......] - ETA: 5:28 - loss: 0.6997 - acc: 0.5133
7488/9333 [=======================>......] - ETA: 5:16 - loss: 0.6997 - acc: 0.5136
7552/9333 [=======================>......] - ETA: 5:05 - loss: 0.6995 - acc: 0.5144
7616/9333 [=======================>......] - ETA: 4:54 - loss: 0.6994 - acc: 0.5150
7680/9333 [=======================>......] - ETA: 4:43 - loss: 0.6994 - acc: 0.5154
7744/9333 [=======================>......] - ETA: 4:32 - loss: 0.6992 - acc: 0.5156
7808/9333 [========================>.....] - ETA: 4:21 - loss: 0.6995 - acc: 0.5149
7872/9333 [========================>.....] - ETA: 4:10 - loss: 0.6995 - acc: 0.5155
7936/9333 [========================>.....] - ETA: 3:58 - loss: 0.6995 - acc: 0.5155
8000/9333 [========================>.....] - ETA: 3:47 - loss: 0.6993 - acc: 0.5161
8064/9333 [========================>.....] - ETA: 3:36 - loss: 0.6994 - acc: 0.5159
8128/9333 [=========================>....] - ETA: 3:25 - loss: 0.6994 - acc: 0.5157
8192/9333 [=========================>....] - ETA: 3:14 - loss: 0.6993 - acc: 0.5160
8256/9333 [=========================>....] - ETA: 3:03 - loss: 0.6992 - acc: 0.5161
8320/9333 [=========================>....] - ETA: 2:52 - loss: 0.6991 - acc: 0.5161
8384/9333 [=========================>....] - ETA: 2:41 - loss: 0.6992 - acc: 0.5161
8448/9333 [==========================>...] - ETA: 2:30 - loss: 0.6990 - acc: 0.5167
8512/9333 [==========================>...] - ETA: 2:19 - loss: 0.6987 - acc: 0.5176
8576/9333 [==========================>...] - ETA: 2:08 - loss: 0.6988 - acc: 0.5170
8640/9333 [==========================>...] - ETA: 1:57 - loss: 0.6991 - acc: 0.5160
8704/9333 [==========================>...] - ETA: 1:46 - loss: 0.6993 - acc: 0.5151
8768/9333 [===========================>..] - ETA: 1:35 - loss: 0.6994 - acc: 0.5151
8832/9333 [===========================>..] - ETA: 1:24 - loss: 0.6993 - acc: 0.5156
8896/9333 [===========================>..] - ETA: 1:14 - loss: 0.6993 - acc: 0.5153
8960/9333 [===========================>..] - ETA: 1:03 - loss: 0.6994 - acc: 0.5155
9024/9333 [============================>.] - ETA: 52s - loss: 0.6994 - acc: 0.5152 
9088/9333 [============================>.] - ETA: 41s - loss: 0.6996 - acc: 0.5147
9152/9333 [============================>.] - ETA: 30s - loss: 0.6995 - acc: 0.5150
9216/9333 [============================>.] - ETA: 19s - loss: 0.6998 - acc: 0.5141
9280/9333 [============================>.] - ETA: 8s - loss: 0.6998 - acc: 0.5144 
9333/9333 [==============================] - 1634s 175ms/step - loss: 0.6997 - acc: 0.5149 - val_loss: 0.6940 - val_acc: 0.4957

Epoch 00002: val_acc did not improve from 0.54291
Epoch 3/10

  64/9333 [..............................] - ETA: 22:25 - loss: 0.6944 - acc: 0.5156
 128/9333 [..............................] - ETA: 21:17 - loss: 0.6902 - acc: 0.5234
 192/9333 [..............................] - ETA: 22:33 - loss: 0.6960 - acc: 0.5312
 256/9333 [..............................] - ETA: 22:44 - loss: 0.6903 - acc: 0.5352
 320/9333 [>.............................] - ETA: 22:52 - loss: 0.6876 - acc: 0.5437
 384/9333 [>.............................] - ETA: 23:00 - loss: 0.6954 - acc: 0.5182
 448/9333 [>.............................] - ETA: 22:56 - loss: 0.6990 - acc: 0.5179
 512/9333 [>.............................] - ETA: 22:53 - loss: 0.6999 - acc: 0.5156
 576/9333 [>.............................] - ETA: 22:49 - loss: 0.7020 - acc: 0.5069
 640/9333 [=>............................] - ETA: 22:46 - loss: 0.6998 - acc: 0.5062
 704/9333 [=>............................] - ETA: 22:50 - loss: 0.6990 - acc: 0.5099
 768/9333 [=>............................] - ETA: 22:30 - loss: 0.7002 - acc: 0.5078
 832/9333 [=>............................] - ETA: 22:27 - loss: 0.6996 - acc: 0.5048
 896/9333 [=>............................] - ETA: 22:11 - loss: 0.6995 - acc: 0.5045
 960/9333 [==>...........................] - ETA: 22:01 - loss: 0.6997 - acc: 0.5010
1024/9333 [==>...........................] - ETA: 22:04 - loss: 0.6984 - acc: 0.5059
1088/9333 [==>...........................] - ETA: 21:54 - loss: 0.6978 - acc: 0.5092
1152/9333 [==>...........................] - ETA: 21:48 - loss: 0.6970 - acc: 0.5113
1216/9333 [==>...........................] - ETA: 21:39 - loss: 0.6971 - acc: 0.5148
1280/9333 [===>..........................] - ETA: 21:27 - loss: 0.6968 - acc: 0.5156
1344/9333 [===>..........................] - ETA: 21:18 - loss: 0.6971 - acc: 0.5134
1408/9333 [===>..........................] - ETA: 21:10 - loss: 0.6962 - acc: 0.5199
1472/9333 [===>..........................] - ETA: 20:56 - loss: 0.6948 - acc: 0.5285
1536/9333 [===>..........................] - ETA: 20:43 - loss: 0.6947 - acc: 0.5260
1600/9333 [====>.........................] - ETA: 20:35 - loss: 0.6927 - acc: 0.5331
1664/9333 [====>.........................] - ETA: 20:23 - loss: 0.6926 - acc: 0.5306
1728/9333 [====>.........................] - ETA: 20:09 - loss: 0.6931 - acc: 0.5307
1792/9333 [====>.........................] - ETA: 20:07 - loss: 0.6928 - acc: 0.5329
1856/9333 [====>.........................] - ETA: 19:58 - loss: 0.6929 - acc: 0.5312
1920/9333 [=====>........................] - ETA: 19:50 - loss: 0.6929 - acc: 0.5323
1984/9333 [=====>........................] - ETA: 19:32 - loss: 0.6937 - acc: 0.5287
2048/9333 [=====>........................] - ETA: 19:19 - loss: 0.6940 - acc: 0.5278
2112/9333 [=====>........................] - ETA: 19:10 - loss: 0.6940 - acc: 0.5289
2176/9333 [=====>........................] - ETA: 18:56 - loss: 0.6945 - acc: 0.5271
2240/9333 [======>.......................] - ETA: 18:49 - loss: 0.6943 - acc: 0.5281
2304/9333 [======>.......................] - ETA: 18:39 - loss: 0.6942 - acc: 0.5282
2368/9333 [======>.......................] - ETA: 18:30 - loss: 0.6944 - acc: 0.5249
2432/9333 [======>.......................] - ETA: 18:23 - loss: 0.6939 - acc: 0.5267
2496/9333 [=======>......................] - ETA: 18:14 - loss: 0.6940 - acc: 0.5272
2560/9333 [=======>......................] - ETA: 18:06 - loss: 0.6942 - acc: 0.5266
2624/9333 [=======>......................] - ETA: 17:57 - loss: 0.6940 - acc: 0.5259
2688/9333 [=======>......................] - ETA: 17:47 - loss: 0.6937 - acc: 0.5268
2752/9333 [=======>......................] - ETA: 17:33 - loss: 0.6940 - acc: 0.5254
2816/9333 [========>.....................] - ETA: 17:23 - loss: 0.6936 - acc: 0.5273
2880/9333 [========>.....................] - ETA: 17:14 - loss: 0.6940 - acc: 0.5264
2944/9333 [========>.....................] - ETA: 17:05 - loss: 0.6947 - acc: 0.5228
3008/9333 [========>.....................] - ETA: 16:58 - loss: 0.6953 - acc: 0.5206
3072/9333 [========>.....................] - ETA: 16:49 - loss: 0.6956 - acc: 0.5189
3136/9333 [=========>....................] - ETA: 16:39 - loss: 0.6957 - acc: 0.5182
3200/9333 [=========>....................] - ETA: 16:26 - loss: 0.6962 - acc: 0.5150
3264/9333 [=========>....................] - ETA: 16:14 - loss: 0.6968 - acc: 0.5135
3328/9333 [=========>....................] - ETA: 16:04 - loss: 0.6967 - acc: 0.5135
3392/9333 [=========>....................] - ETA: 15:53 - loss: 0.6966 - acc: 0.5127
3456/9333 [==========>...................] - ETA: 15:43 - loss: 0.6964 - acc: 0.5139
3520/9333 [==========>...................] - ETA: 15:33 - loss: 0.6965 - acc: 0.5145
3584/9333 [==========>...................] - ETA: 15:22 - loss: 0.6960 - acc: 0.5165
3648/9333 [==========>...................] - ETA: 15:11 - loss: 0.6958 - acc: 0.5164
3712/9333 [==========>...................] - ETA: 14:59 - loss: 0.6955 - acc: 0.5178
3776/9333 [===========>..................] - ETA: 14:49 - loss: 0.6954 - acc: 0.5180
3840/9333 [===========>..................] - ETA: 14:39 - loss: 0.6955 - acc: 0.5182
3904/9333 [===========>..................] - ETA: 14:30 - loss: 0.6957 - acc: 0.5161
3968/9333 [===========>..................] - ETA: 14:19 - loss: 0.6956 - acc: 0.5169
4032/9333 [===========>..................] - ETA: 14:10 - loss: 0.6951 - acc: 0.5181
4096/9333 [============>.................] - ETA: 14:00 - loss: 0.6950 - acc: 0.5193
4160/9333 [============>.................] - ETA: 13:49 - loss: 0.6950 - acc: 0.5197
4224/9333 [============>.................] - ETA: 13:38 - loss: 0.6948 - acc: 0.5196
4288/9333 [============>.................] - ETA: 13:28 - loss: 0.6945 - acc: 0.5203
4352/9333 [============>.................] - ETA: 13:19 - loss: 0.6946 - acc: 0.5198
4416/9333 [=============>................] - ETA: 13:09 - loss: 0.6952 - acc: 0.5186
4480/9333 [=============>................] - ETA: 13:00 - loss: 0.6952 - acc: 0.5190
4544/9333 [=============>................] - ETA: 12:50 - loss: 0.6954 - acc: 0.5185
4608/9333 [=============>................] - ETA: 12:39 - loss: 0.6953 - acc: 0.5182
4672/9333 [==============>...............] - ETA: 12:29 - loss: 0.6953 - acc: 0.5186
4736/9333 [==============>...............] - ETA: 12:18 - loss: 0.6954 - acc: 0.5186
4800/9333 [==============>...............] - ETA: 12:08 - loss: 0.6955 - acc: 0.5188
4864/9333 [==============>...............] - ETA: 11:59 - loss: 0.6956 - acc: 0.5185
4928/9333 [==============>...............] - ETA: 11:50 - loss: 0.6956 - acc: 0.5183
4992/9333 [===============>..............] - ETA: 11:40 - loss: 0.6954 - acc: 0.5186
5056/9333 [===============>..............] - ETA: 11:29 - loss: 0.6951 - acc: 0.5196
5120/9333 [===============>..............] - ETA: 11:19 - loss: 0.6951 - acc: 0.5193
5184/9333 [===============>..............] - ETA: 11:09 - loss: 0.6951 - acc: 0.5191
5248/9333 [===============>..............] - ETA: 11:00 - loss: 0.6952 - acc: 0.5181
5312/9333 [================>.............] - ETA: 10:49 - loss: 0.6953 - acc: 0.5175
5376/9333 [================>.............] - ETA: 10:38 - loss: 0.6954 - acc: 0.5173
5440/9333 [================>.............] - ETA: 10:29 - loss: 0.6954 - acc: 0.5175
5504/9333 [================>.............] - ETA: 10:18 - loss: 0.6953 - acc: 0.5184
5568/9333 [================>.............] - ETA: 10:08 - loss: 0.6951 - acc: 0.5189
5632/9333 [=================>............] - ETA: 9:58 - loss: 0.6952 - acc: 0.5183 
5696/9333 [=================>............] - ETA: 9:47 - loss: 0.6953 - acc: 0.5183
5760/9333 [=================>............] - ETA: 9:38 - loss: 0.6955 - acc: 0.5168
5824/9333 [=================>............] - ETA: 9:27 - loss: 0.6953 - acc: 0.5173
5888/9333 [=================>............] - ETA: 9:17 - loss: 0.6954 - acc: 0.5170
5952/9333 [==================>...........] - ETA: 9:06 - loss: 0.6953 - acc: 0.5171
6016/9333 [==================>...........] - ETA: 8:56 - loss: 0.6955 - acc: 0.5163
6080/9333 [==================>...........] - ETA: 8:45 - loss: 0.6954 - acc: 0.5169
6144/9333 [==================>...........] - ETA: 8:35 - loss: 0.6952 - acc: 0.5166
6208/9333 [==================>...........] - ETA: 8:25 - loss: 0.6951 - acc: 0.5172
6272/9333 [===================>..........] - ETA: 8:16 - loss: 0.6952 - acc: 0.5161
6336/9333 [===================>..........] - ETA: 8:05 - loss: 0.6950 - acc: 0.5169
6400/9333 [===================>..........] - ETA: 7:55 - loss: 0.6950 - acc: 0.5170
6464/9333 [===================>..........] - ETA: 7:44 - loss: 0.6950 - acc: 0.5170
6528/9333 [===================>..........] - ETA: 7:34 - loss: 0.6949 - acc: 0.5175
6592/9333 [====================>.........] - ETA: 7:24 - loss: 0.6950 - acc: 0.5176
6656/9333 [====================>.........] - ETA: 7:13 - loss: 0.6952 - acc: 0.5165
6720/9333 [====================>.........] - ETA: 7:03 - loss: 0.6952 - acc: 0.5167
6784/9333 [====================>.........] - ETA: 6:52 - loss: 0.6953 - acc: 0.5161
6848/9333 [=====================>........] - ETA: 6:42 - loss: 0.6952 - acc: 0.5166
6912/9333 [=====================>........] - ETA: 6:32 - loss: 0.6953 - acc: 0.5162
6976/9333 [=====================>........] - ETA: 6:22 - loss: 0.6955 - acc: 0.5153
7040/9333 [=====================>........] - ETA: 6:11 - loss: 0.6956 - acc: 0.5151
7104/9333 [=====================>........] - ETA: 6:01 - loss: 0.6955 - acc: 0.5148
7168/9333 [======================>.......] - ETA: 5:51 - loss: 0.6956 - acc: 0.5148
7232/9333 [======================>.......] - ETA: 5:40 - loss: 0.6956 - acc: 0.5144
7296/9333 [======================>.......] - ETA: 5:30 - loss: 0.6954 - acc: 0.5149
7360/9333 [======================>.......] - ETA: 5:20 - loss: 0.6952 - acc: 0.5155
7424/9333 [======================>.......] - ETA: 5:09 - loss: 0.6954 - acc: 0.5155
7488/9333 [=======================>......] - ETA: 4:59 - loss: 0.6951 - acc: 0.5166
7552/9333 [=======================>......] - ETA: 4:48 - loss: 0.6953 - acc: 0.5156
7616/9333 [=======================>......] - ETA: 4:38 - loss: 0.6953 - acc: 0.5152
7680/9333 [=======================>......] - ETA: 4:28 - loss: 0.6953 - acc: 0.5151
7744/9333 [=======================>......] - ETA: 4:17 - loss: 0.6954 - acc: 0.5154
7808/9333 [========================>.....] - ETA: 4:07 - loss: 0.6953 - acc: 0.5160
7872/9333 [========================>.....] - ETA: 3:56 - loss: 0.6950 - acc: 0.5174
7936/9333 [========================>.....] - ETA: 3:46 - loss: 0.6949 - acc: 0.5174
8000/9333 [========================>.....] - ETA: 3:36 - loss: 0.6947 - acc: 0.5188
8064/9333 [========================>.....] - ETA: 3:26 - loss: 0.6946 - acc: 0.5187
8128/9333 [=========================>....] - ETA: 3:15 - loss: 0.6945 - acc: 0.5185
8192/9333 [=========================>....] - ETA: 3:05 - loss: 0.6944 - acc: 0.5188
8256/9333 [=========================>....] - ETA: 2:54 - loss: 0.6945 - acc: 0.5183
8320/9333 [=========================>....] - ETA: 2:44 - loss: 0.6946 - acc: 0.5184
8384/9333 [=========================>....] - ETA: 2:34 - loss: 0.6945 - acc: 0.5185
8448/9333 [==========================>...] - ETA: 2:23 - loss: 0.6944 - acc: 0.5189
8512/9333 [==========================>...] - ETA: 2:13 - loss: 0.6944 - acc: 0.5189
8576/9333 [==========================>...] - ETA: 2:02 - loss: 0.6943 - acc: 0.5189
8640/9333 [==========================>...] - ETA: 1:52 - loss: 0.6946 - acc: 0.5178
8704/9333 [==========================>...] - ETA: 1:42 - loss: 0.6944 - acc: 0.5186
8768/9333 [===========================>..] - ETA: 1:31 - loss: 0.6947 - acc: 0.5179
8832/9333 [===========================>..] - ETA: 1:21 - loss: 0.6943 - acc: 0.5191
8896/9333 [===========================>..] - ETA: 1:10 - loss: 0.6945 - acc: 0.5190
8960/9333 [===========================>..] - ETA: 1:00 - loss: 0.6944 - acc: 0.5191
9024/9333 [============================>.] - ETA: 50s - loss: 0.6944 - acc: 0.5193 
9088/9333 [============================>.] - ETA: 39s - loss: 0.6943 - acc: 0.5197
9152/9333 [============================>.] - ETA: 29s - loss: 0.6943 - acc: 0.5193
9216/9333 [============================>.] - ETA: 18s - loss: 0.6942 - acc: 0.5201
9280/9333 [============================>.] - ETA: 8s - loss: 0.6943 - acc: 0.5196 
9333/9333 [==============================] - 1571s 168ms/step - loss: 0.6942 - acc: 0.5198 - val_loss: 0.6906 - val_acc: 0.5342

Epoch 00003: val_acc did not improve from 0.54291
Epoch 4/10

  64/9333 [..............................] - ETA: 33:42 - loss: 0.6854 - acc: 0.5625
 128/9333 [..............................] - ETA: 31:41 - loss: 0.6933 - acc: 0.5547
 192/9333 [..............................] - ETA: 29:45 - loss: 0.6815 - acc: 0.5781
 256/9333 [..............................] - ETA: 28:32 - loss: 0.6793 - acc: 0.5781
 320/9333 [>.............................] - ETA: 27:50 - loss: 0.6863 - acc: 0.5563
 384/9333 [>.............................] - ETA: 27:36 - loss: 0.6873 - acc: 0.5469
 448/9333 [>.............................] - ETA: 27:02 - loss: 0.6891 - acc: 0.5379
 512/9333 [>.............................] - ETA: 26:39 - loss: 0.6859 - acc: 0.5527
 576/9333 [>.............................] - ETA: 26:04 - loss: 0.6896 - acc: 0.5365
 640/9333 [=>............................] - ETA: 25:39 - loss: 0.6892 - acc: 0.5312
 704/9333 [=>............................] - ETA: 25:22 - loss: 0.6866 - acc: 0.5341
 768/9333 [=>............................] - ETA: 25:08 - loss: 0.6848 - acc: 0.5456
 832/9333 [=>............................] - ETA: 24:48 - loss: 0.6867 - acc: 0.5409
 896/9333 [=>............................] - ETA: 24:25 - loss: 0.6894 - acc: 0.5335
 960/9333 [==>...........................] - ETA: 24:16 - loss: 0.6894 - acc: 0.5312
1024/9333 [==>...........................] - ETA: 24:01 - loss: 0.6901 - acc: 0.5264
1088/9333 [==>...........................] - ETA: 23:46 - loss: 0.6900 - acc: 0.5230
1152/9333 [==>...........................] - ETA: 23:30 - loss: 0.6911 - acc: 0.5191
1216/9333 [==>...........................] - ETA: 23:18 - loss: 0.6913 - acc: 0.5164
1280/9333 [===>..........................] - ETA: 23:11 - loss: 0.6918 - acc: 0.5141
1344/9333 [===>..........................] - ETA: 22:59 - loss: 0.6923 - acc: 0.5126
1408/9333 [===>..........................] - ETA: 22:47 - loss: 0.6922 - acc: 0.5099
1472/9333 [===>..........................] - ETA: 22:31 - loss: 0.6920 - acc: 0.5122
1536/9333 [===>..........................] - ETA: 22:19 - loss: 0.6923 - acc: 0.5117
1600/9333 [====>.........................] - ETA: 22:07 - loss: 0.6925 - acc: 0.5100
1664/9333 [====>.........................] - ETA: 21:56 - loss: 0.6921 - acc: 0.5108
1728/9333 [====>.........................] - ETA: 21:41 - loss: 0.6918 - acc: 0.5116
1792/9333 [====>.........................] - ETA: 21:30 - loss: 0.6917 - acc: 0.5140
1856/9333 [====>.........................] - ETA: 21:15 - loss: 0.6915 - acc: 0.5145
1920/9333 [=====>........................] - ETA: 21:02 - loss: 0.6915 - acc: 0.5130
1984/9333 [=====>........................] - ETA: 20:48 - loss: 0.6916 - acc: 0.5141
2048/9333 [=====>........................] - ETA: 20:37 - loss: 0.6913 - acc: 0.5142
2112/9333 [=====>........................] - ETA: 20:23 - loss: 0.6910 - acc: 0.5142
2176/9333 [=====>........................] - ETA: 20:12 - loss: 0.6910 - acc: 0.5138
2240/9333 [======>.......................] - ETA: 20:00 - loss: 0.6914 - acc: 0.5125
2304/9333 [======>.......................] - ETA: 19:46 - loss: 0.6912 - acc: 0.5139
2368/9333 [======>.......................] - ETA: 19:32 - loss: 0.6904 - acc: 0.5190
2432/9333 [======>.......................] - ETA: 19:19 - loss: 0.6900 - acc: 0.5222
2496/9333 [=======>......................] - ETA: 19:08 - loss: 0.6898 - acc: 0.5224
2560/9333 [=======>......................] - ETA: 18:56 - loss: 0.6897 - acc: 0.5219
2624/9333 [=======>......................] - ETA: 18:45 - loss: 0.6896 - acc: 0.5225
2688/9333 [=======>......................] - ETA: 18:32 - loss: 0.6891 - acc: 0.5268
2752/9333 [=======>......................] - ETA: 18:21 - loss: 0.6890 - acc: 0.5280
2816/9333 [========>.....................] - ETA: 18:08 - loss: 0.6891 - acc: 0.5266
2880/9333 [========>.....................] - ETA: 17:58 - loss: 0.6895 - acc: 0.5253
2944/9333 [========>.....................] - ETA: 17:46 - loss: 0.6899 - acc: 0.5262
3008/9333 [========>.....................] - ETA: 17:35 - loss: 0.6897 - acc: 0.5286
3072/9333 [========>.....................] - ETA: 17:26 - loss: 0.6898 - acc: 0.5293
3136/9333 [=========>....................] - ETA: 17:15 - loss: 0.6897 - acc: 0.5309
3200/9333 [=========>....................] - ETA: 17:05 - loss: 0.6896 - acc: 0.5312
3264/9333 [=========>....................] - ETA: 16:54 - loss: 0.6898 - acc: 0.5309
3328/9333 [=========>....................] - ETA: 16:43 - loss: 0.6900 - acc: 0.5291
3392/9333 [=========>....................] - ETA: 16:32 - loss: 0.6901 - acc: 0.5292
3456/9333 [==========>...................] - ETA: 16:22 - loss: 0.6905 - acc: 0.5307
3520/9333 [==========>...................] - ETA: 16:11 - loss: 0.6903 - acc: 0.5310
3584/9333 [==========>...................] - ETA: 15:59 - loss: 0.6901 - acc: 0.5315
3648/9333 [==========>...................] - ETA: 15:48 - loss: 0.6902 - acc: 0.5312
3712/9333 [==========>...................] - ETA: 15:36 - loss: 0.6904 - acc: 0.5299
3776/9333 [===========>..................] - ETA: 15:25 - loss: 0.6908 - acc: 0.5289
3840/9333 [===========>..................] - ETA: 15:14 - loss: 0.6913 - acc: 0.5268
3904/9333 [===========>..................] - ETA: 15:03 - loss: 0.6916 - acc: 0.5259
3968/9333 [===========>..................] - ETA: 14:52 - loss: 0.6913 - acc: 0.5272
4032/9333 [===========>..................] - ETA: 14:42 - loss: 0.6914 - acc: 0.5280
4096/9333 [============>.................] - ETA: 14:31 - loss: 0.6911 - acc: 0.5281
4160/9333 [============>.................] - ETA: 14:21 - loss: 0.6909 - acc: 0.5305
4224/9333 [============>.................] - ETA: 14:10 - loss: 0.6911 - acc: 0.5303
4288/9333 [============>.................] - ETA: 14:00 - loss: 0.6912 - acc: 0.5303
4352/9333 [============>.................] - ETA: 13:50 - loss: 0.6909 - acc: 0.5319
4416/9333 [=============>................] - ETA: 13:40 - loss: 0.6912 - acc: 0.5308
4480/9333 [=============>................] - ETA: 13:29 - loss: 0.6915 - acc: 0.5301
4544/9333 [=============>................] - ETA: 13:19 - loss: 0.6917 - acc: 0.5299
4608/9333 [=============>................] - ETA: 13:09 - loss: 0.6917 - acc: 0.5299
4672/9333 [==============>...............] - ETA: 12:58 - loss: 0.6913 - acc: 0.5308
4736/9333 [==============>...............] - ETA: 12:49 - loss: 0.6911 - acc: 0.5312
4800/9333 [==============>...............] - ETA: 12:38 - loss: 0.6912 - acc: 0.5308
4864/9333 [==============>...............] - ETA: 12:27 - loss: 0.6911 - acc: 0.5315
4928/9333 [==============>...............] - ETA: 12:17 - loss: 0.6912 - acc: 0.5317
4992/9333 [===============>..............] - ETA: 12:05 - loss: 0.6914 - acc: 0.5306
5056/9333 [===============>..............] - ETA: 11:54 - loss: 0.6914 - acc: 0.5309
5120/9333 [===============>..............] - ETA: 11:45 - loss: 0.6914 - acc: 0.5314
5184/9333 [===============>..............] - ETA: 11:35 - loss: 0.6913 - acc: 0.5314
5248/9333 [===============>..............] - ETA: 11:24 - loss: 0.6911 - acc: 0.5322
5312/9333 [================>.............] - ETA: 11:14 - loss: 0.6909 - acc: 0.5333
5376/9333 [================>.............] - ETA: 11:03 - loss: 0.6910 - acc: 0.5337
5440/9333 [================>.............] - ETA: 10:53 - loss: 0.6910 - acc: 0.5333
5504/9333 [================>.............] - ETA: 10:43 - loss: 0.6912 - acc: 0.5325
5568/9333 [================>.............] - ETA: 10:34 - loss: 0.6912 - acc: 0.5321
5632/9333 [=================>............] - ETA: 10:23 - loss: 0.6912 - acc: 0.5327
5696/9333 [=================>............] - ETA: 10:14 - loss: 0.6913 - acc: 0.5325
5760/9333 [=================>............] - ETA: 10:03 - loss: 0.6913 - acc: 0.5321
5824/9333 [=================>............] - ETA: 9:53 - loss: 0.6915 - acc: 0.5311 
5888/9333 [=================>............] - ETA: 9:43 - loss: 0.6913 - acc: 0.5314
5952/9333 [==================>...........] - ETA: 9:31 - loss: 0.6913 - acc: 0.5312
6016/9333 [==================>...........] - ETA: 9:21 - loss: 0.6912 - acc: 0.5311
6080/9333 [==================>...........] - ETA: 9:10 - loss: 0.6911 - acc: 0.5312
6144/9333 [==================>...........] - ETA: 9:00 - loss: 0.6913 - acc: 0.5308
6208/9333 [==================>...........] - ETA: 8:49 - loss: 0.6911 - acc: 0.5322
6272/9333 [===================>..........] - ETA: 8:38 - loss: 0.6910 - acc: 0.5322
6336/9333 [===================>..........] - ETA: 8:28 - loss: 0.6911 - acc: 0.5325
6400/9333 [===================>..........] - ETA: 8:17 - loss: 0.6913 - acc: 0.5317
6464/9333 [===================>..........] - ETA: 8:07 - loss: 0.6914 - acc: 0.5308
6528/9333 [===================>..........] - ETA: 7:56 - loss: 0.6915 - acc: 0.5305
6592/9333 [====================>.........] - ETA: 7:45 - loss: 0.6916 - acc: 0.5306
6656/9333 [====================>.........] - ETA: 7:34 - loss: 0.6916 - acc: 0.5303
6720/9333 [====================>.........] - ETA: 7:23 - loss: 0.6919 - acc: 0.5292
6784/9333 [====================>.........] - ETA: 7:12 - loss: 0.6917 - acc: 0.5298
6848/9333 [=====================>........] - ETA: 7:01 - loss: 0.6917 - acc: 0.5302
6912/9333 [=====================>........] - ETA: 6:51 - loss: 0.6916 - acc: 0.5307
6976/9333 [=====================>........] - ETA: 6:40 - loss: 0.6916 - acc: 0.5312
7040/9333 [=====================>........] - ETA: 6:29 - loss: 0.6916 - acc: 0.5308
7104/9333 [=====================>........] - ETA: 6:19 - loss: 0.6917 - acc: 0.5303
7168/9333 [======================>.......] - ETA: 6:08 - loss: 0.6915 - acc: 0.5303
7232/9333 [======================>.......] - ETA: 5:57 - loss: 0.6915 - acc: 0.5303
7296/9333 [======================>.......] - ETA: 5:46 - loss: 0.6914 - acc: 0.5308
7360/9333 [======================>.......] - ETA: 5:36 - loss: 0.6914 - acc: 0.5310
7424/9333 [======================>.......] - ETA: 5:25 - loss: 0.6912 - acc: 0.5315
7488/9333 [=======================>......] - ETA: 5:14 - loss: 0.6914 - acc: 0.5308
7552/9333 [=======================>......] - ETA: 5:03 - loss: 0.6915 - acc: 0.5303
7616/9333 [=======================>......] - ETA: 4:53 - loss: 0.6915 - acc: 0.5298
7680/9333 [=======================>......] - ETA: 4:42 - loss: 0.6916 - acc: 0.5294
7744/9333 [=======================>......] - ETA: 4:31 - loss: 0.6915 - acc: 0.5302
7808/9333 [========================>.....] - ETA: 4:20 - loss: 0.6915 - acc: 0.5302
7872/9333 [========================>.....] - ETA: 4:09 - loss: 0.6914 - acc: 0.5307
7936/9333 [========================>.....] - ETA: 3:58 - loss: 0.6916 - acc: 0.5301
8000/9333 [========================>.....] - ETA: 3:47 - loss: 0.6919 - acc: 0.5296
8064/9333 [========================>.....] - ETA: 3:36 - loss: 0.6918 - acc: 0.5294
8128/9333 [=========================>....] - ETA: 3:25 - loss: 0.6918 - acc: 0.5290
8192/9333 [=========================>....] - ETA: 3:15 - loss: 0.6916 - acc: 0.5293
8256/9333 [=========================>....] - ETA: 3:04 - loss: 0.6915 - acc: 0.5296
8320/9333 [=========================>....] - ETA: 2:53 - loss: 0.6915 - acc: 0.5292
8384/9333 [=========================>....] - ETA: 2:42 - loss: 0.6915 - acc: 0.5292
8448/9333 [==========================>...] - ETA: 2:31 - loss: 0.6915 - acc: 0.5298
8512/9333 [==========================>...] - ETA: 2:20 - loss: 0.6913 - acc: 0.5298
8576/9333 [==========================>...] - ETA: 2:09 - loss: 0.6913 - acc: 0.5301
8640/9333 [==========================>...] - ETA: 1:58 - loss: 0.6912 - acc: 0.5303
8704/9333 [==========================>...] - ETA: 1:47 - loss: 0.6912 - acc: 0.5306
8768/9333 [===========================>..] - ETA: 1:36 - loss: 0.6913 - acc: 0.5307
8832/9333 [===========================>..] - ETA: 1:25 - loss: 0.6912 - acc: 0.5311
8896/9333 [===========================>..] - ETA: 1:14 - loss: 0.6912 - acc: 0.5310
8960/9333 [===========================>..] - ETA: 1:03 - loss: 0.6912 - acc: 0.5317
9024/9333 [============================>.] - ETA: 52s - loss: 0.6912 - acc: 0.5317 
9088/9333 [============================>.] - ETA: 42s - loss: 0.6911 - acc: 0.5317
9152/9333 [============================>.] - ETA: 31s - loss: 0.6910 - acc: 0.5320
9216/9333 [============================>.] - ETA: 20s - loss: 0.6910 - acc: 0.5328
9280/9333 [============================>.] - ETA: 9s - loss: 0.6908 - acc: 0.5335 
9333/9333 [==============================] - 1655s 177ms/step - loss: 0.6908 - acc: 0.5332 - val_loss: 0.6899 - val_acc: 0.5246

Epoch 00004: val_acc did not improve from 0.54291
Epoch 5/10

  64/9333 [..............................] - ETA: 27:05 - loss: 0.7075 - acc: 0.4844
 128/9333 [..............................] - ETA: 27:12 - loss: 0.6919 - acc: 0.5469
 192/9333 [..............................] - ETA: 26:48 - loss: 0.6888 - acc: 0.5677
 256/9333 [..............................] - ETA: 26:14 - loss: 0.6886 - acc: 0.5586
 320/9333 [>.............................] - ETA: 25:47 - loss: 0.6890 - acc: 0.5469
 384/9333 [>.............................] - ETA: 25:38 - loss: 0.6870 - acc: 0.5495
 448/9333 [>.............................] - ETA: 25:15 - loss: 0.6865 - acc: 0.5491
 512/9333 [>.............................] - ETA: 25:18 - loss: 0.6869 - acc: 0.5449
 576/9333 [>.............................] - ETA: 24:59 - loss: 0.6898 - acc: 0.5382
 640/9333 [=>............................] - ETA: 24:48 - loss: 0.6924 - acc: 0.5344
 704/9333 [=>............................] - ETA: 24:37 - loss: 0.6915 - acc: 0.5384
 768/9333 [=>............................] - ETA: 24:25 - loss: 0.6912 - acc: 0.5391
 832/9333 [=>............................] - ETA: 24:13 - loss: 0.6908 - acc: 0.5361
 896/9333 [=>............................] - ETA: 24:07 - loss: 0.6916 - acc: 0.5324
 960/9333 [==>...........................] - ETA: 24:00 - loss: 0.6907 - acc: 0.5354
1024/9333 [==>...........................] - ETA: 23:45 - loss: 0.6898 - acc: 0.5381
1088/9333 [==>...........................] - ETA: 23:42 - loss: 0.6898 - acc: 0.5377
1152/9333 [==>...........................] - ETA: 23:28 - loss: 0.6890 - acc: 0.5417
1216/9333 [==>...........................] - ETA: 23:21 - loss: 0.6888 - acc: 0.5428
1280/9333 [===>..........................] - ETA: 23:05 - loss: 0.6875 - acc: 0.5484
1344/9333 [===>..........................] - ETA: 23:01 - loss: 0.6877 - acc: 0.5439
1408/9333 [===>..........................] - ETA: 22:49 - loss: 0.6876 - acc: 0.5476
1472/9333 [===>..........................] - ETA: 22:41 - loss: 0.6884 - acc: 0.5448
1536/9333 [===>..........................] - ETA: 22:26 - loss: 0.6888 - acc: 0.5443
1600/9333 [====>.........................] - ETA: 22:12 - loss: 0.6875 - acc: 0.5487
1664/9333 [====>.........................] - ETA: 22:03 - loss: 0.6873 - acc: 0.5481
1728/9333 [====>.........................] - ETA: 21:53 - loss: 0.6878 - acc: 0.5446
1792/9333 [====>.........................] - ETA: 21:45 - loss: 0.6876 - acc: 0.5458
1856/9333 [====>.........................] - ETA: 21:35 - loss: 0.6876 - acc: 0.5453
1920/9333 [=====>........................] - ETA: 21:24 - loss: 0.6882 - acc: 0.5417
1984/9333 [=====>........................] - ETA: 21:15 - loss: 0.6877 - acc: 0.5449
2048/9333 [=====>........................] - ETA: 21:02 - loss: 0.6876 - acc: 0.5449
2112/9333 [=====>........................] - ETA: 20:52 - loss: 0.6878 - acc: 0.5459
2176/9333 [=====>........................] - ETA: 20:40 - loss: 0.6881 - acc: 0.5469
2240/9333 [======>.......................] - ETA: 20:28 - loss: 0.6881 - acc: 0.5469
2304/9333 [======>.......................] - ETA: 20:16 - loss: 0.6885 - acc: 0.5447
2368/9333 [======>.......................] - ETA: 20:05 - loss: 0.6879 - acc: 0.5439
2432/9333 [======>.......................] - ETA: 19:53 - loss: 0.6879 - acc: 0.5440
2496/9333 [=======>......................] - ETA: 19:44 - loss: 0.6879 - acc: 0.5417
2560/9333 [=======>......................] - ETA: 19:30 - loss: 0.6880 - acc: 0.5406
2624/9333 [=======>......................] - ETA: 19:20 - loss: 0.6873 - acc: 0.5427
2688/9333 [=======>......................] - ETA: 19:07 - loss: 0.6876 - acc: 0.5402
2752/9333 [=======>......................] - ETA: 18:54 - loss: 0.6875 - acc: 0.5425
2816/9333 [========>.....................] - ETA: 18:42 - loss: 0.6876 - acc: 0.5415
2880/9333 [========>.....................] - ETA: 18:31 - loss: 0.6883 - acc: 0.5396
2944/9333 [========>.....................] - ETA: 18:20 - loss: 0.6880 - acc: 0.5404
3008/9333 [========>.....................] - ETA: 18:07 - loss: 0.6878 - acc: 0.5412
3072/9333 [========>.....................] - ETA: 17:58 - loss: 0.6872 - acc: 0.5430
3136/9333 [=========>....................] - ETA: 17:47 - loss: 0.6869 - acc: 0.5443
3200/9333 [=========>....................] - ETA: 17:38 - loss: 0.6868 - acc: 0.5444
3264/9333 [=========>....................] - ETA: 17:27 - loss: 0.6868 - acc: 0.5435
3328/9333 [=========>....................] - ETA: 17:17 - loss: 0.6867 - acc: 0.5445
3392/9333 [=========>....................] - ETA: 17:05 - loss: 0.6868 - acc: 0.5433
3456/9333 [==========>...................] - ETA: 16:54 - loss: 0.6867 - acc: 0.5437
3520/9333 [==========>...................] - ETA: 16:43 - loss: 0.6867 - acc: 0.5432
3584/9333 [==========>...................] - ETA: 16:33 - loss: 0.6863 - acc: 0.5441
3648/9333 [==========>...................] - ETA: 16:21 - loss: 0.6865 - acc: 0.5433
3712/9333 [==========>...................] - ETA: 16:11 - loss: 0.6871 - acc: 0.5418
3776/9333 [===========>..................] - ETA: 16:00 - loss: 0.6870 - acc: 0.5416
3840/9333 [===========>..................] - ETA: 15:48 - loss: 0.6876 - acc: 0.5398
3904/9333 [===========>..................] - ETA: 15:37 - loss: 0.6872 - acc: 0.5410
3968/9333 [===========>..................] - ETA: 15:25 - loss: 0.6874 - acc: 0.5413
4032/9333 [===========>..................] - ETA: 15:13 - loss: 0.6872 - acc: 0.5419
4096/9333 [============>.................] - ETA: 15:02 - loss: 0.6872 - acc: 0.5420
4160/9333 [============>.................] - ETA: 14:52 - loss: 0.6871 - acc: 0.5423
4224/9333 [============>.................] - ETA: 14:41 - loss: 0.6877 - acc: 0.5414
4288/9333 [============>.................] - ETA: 14:30 - loss: 0.6876 - acc: 0.5415
4352/9333 [============>.................] - ETA: 14:20 - loss: 0.6877 - acc: 0.5416
4416/9333 [=============>................] - ETA: 14:08 - loss: 0.6880 - acc: 0.5396
4480/9333 [=============>................] - ETA: 13:58 - loss: 0.6882 - acc: 0.5391
4544/9333 [=============>................] - ETA: 13:48 - loss: 0.6884 - acc: 0.5390
4608/9333 [=============>................] - ETA: 13:37 - loss: 0.6888 - acc: 0.5378
4672/9333 [==============>...............] - ETA: 13:27 - loss: 0.6891 - acc: 0.5368
4736/9333 [==============>...............] - ETA: 13:16 - loss: 0.6892 - acc: 0.5365
4800/9333 [==============>...............] - ETA: 13:06 - loss: 0.6891 - acc: 0.5365
4864/9333 [==============>...............] - ETA: 12:55 - loss: 0.6888 - acc: 0.5376
4928/9333 [==============>...............] - ETA: 12:43 - loss: 0.6886 - acc: 0.5390
4992/9333 [===============>..............] - ETA: 12:32 - loss: 0.6885 - acc: 0.5395
5056/9333 [===============>..............] - ETA: 12:21 - loss: 0.6885 - acc: 0.5403
5120/9333 [===============>..............] - ETA: 12:10 - loss: 0.6883 - acc: 0.5420
5184/9333 [===============>..............] - ETA: 11:59 - loss: 0.6882 - acc: 0.5422
5248/9333 [===============>..............] - ETA: 11:48 - loss: 0.6881 - acc: 0.5433
5312/9333 [================>.............] - ETA: 11:37 - loss: 0.6882 - acc: 0.5427
5376/9333 [================>.............] - ETA: 11:27 - loss: 0.6886 - acc: 0.5420
5440/9333 [================>.............] - ETA: 11:16 - loss: 0.6887 - acc: 0.5421
5504/9333 [================>.............] - ETA: 11:04 - loss: 0.6886 - acc: 0.5422
5568/9333 [================>.............] - ETA: 10:53 - loss: 0.6884 - acc: 0.5431
5632/9333 [=================>............] - ETA: 10:42 - loss: 0.6883 - acc: 0.5433
5696/9333 [=================>............] - ETA: 10:31 - loss: 0.6883 - acc: 0.5439
5760/9333 [=================>............] - ETA: 10:20 - loss: 0.6882 - acc: 0.5441
5824/9333 [=================>............] - ETA: 10:09 - loss: 0.6881 - acc: 0.5446
5888/9333 [=================>............] - ETA: 9:58 - loss: 0.6882 - acc: 0.5442 
5952/9333 [==================>...........] - ETA: 9:47 - loss: 0.6883 - acc: 0.5442
6016/9333 [==================>...........] - ETA: 9:36 - loss: 0.6884 - acc: 0.5431
6080/9333 [==================>...........] - ETA: 9:25 - loss: 0.6886 - acc: 0.5414
6144/9333 [==================>...........] - ETA: 9:14 - loss: 0.6887 - acc: 0.5410
6208/9333 [==================>...........] - ETA: 9:03 - loss: 0.6887 - acc: 0.5404
6272/9333 [===================>..........] - ETA: 8:52 - loss: 0.6888 - acc: 0.5399
6336/9333 [===================>..........] - ETA: 8:41 - loss: 0.6888 - acc: 0.5399
6400/9333 [===================>..........] - ETA: 8:30 - loss: 0.6889 - acc: 0.5400
6464/9333 [===================>..........] - ETA: 8:19 - loss: 0.6889 - acc: 0.5401
6528/9333 [===================>..........] - ETA: 8:08 - loss: 0.6890 - acc: 0.5394
6592/9333 [====================>.........] - ETA: 7:57 - loss: 0.6891 - acc: 0.5384
6656/9333 [====================>.........] - ETA: 7:45 - loss: 0.6894 - acc: 0.5377
6720/9333 [====================>.........] - ETA: 7:34 - loss: 0.6895 - acc: 0.5371
6784/9333 [====================>.........] - ETA: 7:23 - loss: 0.6893 - acc: 0.5380
6848/9333 [=====================>........] - ETA: 7:12 - loss: 0.6895 - acc: 0.5374
6912/9333 [=====================>........] - ETA: 7:02 - loss: 0.6898 - acc: 0.5365
6976/9333 [=====================>........] - ETA: 6:50 - loss: 0.6898 - acc: 0.5364
7040/9333 [=====================>........] - ETA: 6:39 - loss: 0.6898 - acc: 0.5361
7104/9333 [=====================>........] - ETA: 6:28 - loss: 0.6898 - acc: 0.5362
7168/9333 [======================>.......] - ETA: 6:17 - loss: 0.6899 - acc: 0.5360
7232/9333 [======================>.......] - ETA: 6:06 - loss: 0.6900 - acc: 0.5366
7296/9333 [======================>.......] - ETA: 5:55 - loss: 0.6900 - acc: 0.5366
7360/9333 [======================>.......] - ETA: 5:44 - loss: 0.6901 - acc: 0.5364
7424/9333 [======================>.......] - ETA: 5:33 - loss: 0.6900 - acc: 0.5366
7488/9333 [=======================>......] - ETA: 5:22 - loss: 0.6900 - acc: 0.5366
7552/9333 [=======================>......] - ETA: 5:11 - loss: 0.6902 - acc: 0.5354
7616/9333 [=======================>......] - ETA: 4:59 - loss: 0.6901 - acc: 0.5355
7680/9333 [=======================>......] - ETA: 4:48 - loss: 0.6902 - acc: 0.5357
7744/9333 [=======================>......] - ETA: 4:37 - loss: 0.6902 - acc: 0.5356
7808/9333 [========================>.....] - ETA: 4:26 - loss: 0.6903 - acc: 0.5357
7872/9333 [========================>.....] - ETA: 4:15 - loss: 0.6904 - acc: 0.5352
7936/9333 [========================>.....] - ETA: 4:04 - loss: 0.6904 - acc: 0.5352
8000/9333 [========================>.....] - ETA: 3:52 - loss: 0.6904 - acc: 0.5351
8064/9333 [========================>.....] - ETA: 3:41 - loss: 0.6906 - acc: 0.5341
8128/9333 [=========================>....] - ETA: 3:30 - loss: 0.6907 - acc: 0.5335
8192/9333 [=========================>....] - ETA: 3:19 - loss: 0.6907 - acc: 0.5331
8256/9333 [=========================>....] - ETA: 3:08 - loss: 0.6907 - acc: 0.5327
8320/9333 [=========================>....] - ETA: 2:57 - loss: 0.6907 - acc: 0.5331
8384/9333 [=========================>....] - ETA: 2:45 - loss: 0.6907 - acc: 0.5330
8448/9333 [==========================>...] - ETA: 2:34 - loss: 0.6907 - acc: 0.5327
8512/9333 [==========================>...] - ETA: 2:23 - loss: 0.6909 - acc: 0.5327
8576/9333 [==========================>...] - ETA: 2:12 - loss: 0.6910 - acc: 0.5319
8640/9333 [==========================>...] - ETA: 2:01 - loss: 0.6910 - acc: 0.5316
8704/9333 [==========================>...] - ETA: 1:49 - loss: 0.6911 - acc: 0.5314
8768/9333 [===========================>..] - ETA: 1:38 - loss: 0.6909 - acc: 0.5317
8832/9333 [===========================>..] - ETA: 1:27 - loss: 0.6910 - acc: 0.5312
8896/9333 [===========================>..] - ETA: 1:16 - loss: 0.6911 - acc: 0.5308
8960/9333 [===========================>..] - ETA: 1:05 - loss: 0.6911 - acc: 0.5312
9024/9333 [============================>.] - ETA: 54s - loss: 0.6911 - acc: 0.5307 
9088/9333 [============================>.] - ETA: 42s - loss: 0.6911 - acc: 0.5303
9152/9333 [============================>.] - ETA: 31s - loss: 0.6912 - acc: 0.5302
9216/9333 [============================>.] - ETA: 20s - loss: 0.6912 - acc: 0.5307
9280/9333 [============================>.] - ETA: 9s - loss: 0.6911 - acc: 0.5314 
9333/9333 [==============================] - 1692s 181ms/step - loss: 0.6911 - acc: 0.5317 - val_loss: 0.6900 - val_acc: 0.5265

Epoch 00005: val_acc did not improve from 0.54291
Epoch 6/10

  64/9333 [..............................] - ETA: 28:05 - loss: 0.6963 - acc: 0.5312
 128/9333 [..............................] - ETA: 28:42 - loss: 0.6980 - acc: 0.5703
 192/9333 [..............................] - ETA: 27:44 - loss: 0.6981 - acc: 0.5469
 256/9333 [..............................] - ETA: 27:27 - loss: 0.6969 - acc: 0.5391
 320/9333 [>.............................] - ETA: 26:56 - loss: 0.6945 - acc: 0.5375
 384/9333 [>.............................] - ETA: 26:48 - loss: 0.6921 - acc: 0.5469
 448/9333 [>.............................] - ETA: 26:19 - loss: 0.6914 - acc: 0.5379
 512/9333 [>.............................] - ETA: 25:56 - loss: 0.6903 - acc: 0.5391
 576/9333 [>.............................] - ETA: 25:49 - loss: 0.6891 - acc: 0.5469
 640/9333 [=>............................] - ETA: 25:29 - loss: 0.6883 - acc: 0.5516
 704/9333 [=>............................] - ETA: 25:15 - loss: 0.6874 - acc: 0.5526
 768/9333 [=>............................] - ETA: 25:07 - loss: 0.6877 - acc: 0.5521
 832/9333 [=>............................] - ETA: 24:59 - loss: 0.6878 - acc: 0.5517
 896/9333 [=>............................] - ETA: 24:52 - loss: 0.6888 - acc: 0.5480
 960/9333 [==>...........................] - ETA: 24:45 - loss: 0.6885 - acc: 0.5479
1024/9333 [==>...........................] - ETA: 24:27 - loss: 0.6894 - acc: 0.5430
1088/9333 [==>...........................] - ETA: 24:15 - loss: 0.6882 - acc: 0.5496
1152/9333 [==>...........................] - ETA: 24:00 - loss: 0.6890 - acc: 0.5469
1216/9333 [==>...........................] - ETA: 23:50 - loss: 0.6895 - acc: 0.5452
1280/9333 [===>..........................] - ETA: 23:33 - loss: 0.6898 - acc: 0.5437
1344/9333 [===>..........................] - ETA: 23:24 - loss: 0.6890 - acc: 0.5461
1408/9333 [===>..........................] - ETA: 23:14 - loss: 0.6889 - acc: 0.5469
1472/9333 [===>..........................] - ETA: 22:59 - loss: 0.6887 - acc: 0.5496
1536/9333 [===>..........................] - ETA: 22:48 - loss: 0.6881 - acc: 0.5508
1600/9333 [====>.........................] - ETA: 22:39 - loss: 0.6884 - acc: 0.5481
1664/9333 [====>.........................] - ETA: 22:28 - loss: 0.6896 - acc: 0.5457
1728/9333 [====>.........................] - ETA: 22:21 - loss: 0.6895 - acc: 0.5475
1792/9333 [====>.........................] - ETA: 22:08 - loss: 0.6903 - acc: 0.5424
1856/9333 [====>.........................] - ETA: 22:00 - loss: 0.6906 - acc: 0.5393
1920/9333 [=====>........................] - ETA: 21:51 - loss: 0.6899 - acc: 0.5422
1984/9333 [=====>........................] - ETA: 21:39 - loss: 0.6902 - acc: 0.5403
2048/9333 [=====>........................] - ETA: 21:29 - loss: 0.6906 - acc: 0.5356
2112/9333 [=====>........................] - ETA: 21:20 - loss: 0.6907 - acc: 0.5355
2176/9333 [=====>........................] - ETA: 21:07 - loss: 0.6910 - acc: 0.5345
2240/9333 [======>.......................] - ETA: 20:53 - loss: 0.6911 - acc: 0.5312
2304/9333 [======>.......................] - ETA: 20:41 - loss: 0.6910 - acc: 0.5317
2368/9333 [======>.......................] - ETA: 20:29 - loss: 0.6904 - acc: 0.5338
2432/9333 [======>.......................] - ETA: 20:18 - loss: 0.6903 - acc: 0.5354
2496/9333 [=======>......................] - ETA: 20:06 - loss: 0.6904 - acc: 0.5329
2560/9333 [=======>......................] - ETA: 19:58 - loss: 0.6908 - acc: 0.5309
2624/9333 [=======>......................] - ETA: 19:46 - loss: 0.6905 - acc: 0.5320
2688/9333 [=======>......................] - ETA: 19:35 - loss: 0.6905 - acc: 0.5305
2752/9333 [=======>......................] - ETA: 19:25 - loss: 0.6908 - acc: 0.5291
2816/9333 [========>.....................] - ETA: 19:15 - loss: 0.6909 - acc: 0.5273
2880/9333 [========>.....................] - ETA: 19:04 - loss: 0.6906 - acc: 0.5278
2944/9333 [========>.....................] - ETA: 18:53 - loss: 0.6906 - acc: 0.5272
3008/9333 [========>.....................] - ETA: 18:41 - loss: 0.6906 - acc: 0.5259
3072/9333 [========>.....................] - ETA: 18:28 - loss: 0.6906 - acc: 0.5260
3136/9333 [=========>....................] - ETA: 18:16 - loss: 0.6904 - acc: 0.5255
3200/9333 [=========>....................] - ETA: 18:04 - loss: 0.6901 - acc: 0.5269
3264/9333 [=========>....................] - ETA: 17:53 - loss: 0.6900 - acc: 0.5267
3328/9333 [=========>....................] - ETA: 17:41 - loss: 0.6901 - acc: 0.5261
3392/9333 [=========>....................] - ETA: 17:32 - loss: 0.6900 - acc: 0.5271
3456/9333 [==========>...................] - ETA: 17:19 - loss: 0.6902 - acc: 0.5260
3520/9333 [==========>...................] - ETA: 17:07 - loss: 0.6901 - acc: 0.5261
3584/9333 [==========>...................] - ETA: 16:56 - loss: 0.6899 - acc: 0.5268
3648/9333 [==========>...................] - ETA: 16:44 - loss: 0.6901 - acc: 0.5263
3712/9333 [==========>...................] - ETA: 16:32 - loss: 0.6903 - acc: 0.5264
3776/9333 [===========>..................] - ETA: 16:20 - loss: 0.6903 - acc: 0.5265
3840/9333 [===========>..................] - ETA: 16:08 - loss: 0.6902 - acc: 0.5271
3904/9333 [===========>..................] - ETA: 15:56 - loss: 0.6901 - acc: 0.5289
3968/9333 [===========>..................] - ETA: 15:44 - loss: 0.6897 - acc: 0.5310
4032/9333 [===========>..................] - ETA: 15:32 - loss: 0.6897 - acc: 0.5308
4096/9333 [============>.................] - ETA: 15:21 - loss: 0.6896 - acc: 0.5317
4160/9333 [============>.................] - ETA: 15:09 - loss: 0.6895 - acc: 0.5322
4224/9333 [============>.................] - ETA: 14:56 - loss: 0.6897 - acc: 0.5308
4288/9333 [============>.................] - ETA: 14:45 - loss: 0.6895 - acc: 0.5312
4352/9333 [============>.................] - ETA: 14:33 - loss: 0.6892 - acc: 0.5335
4416/9333 [=============>................] - ETA: 14:21 - loss: 0.6893 - acc: 0.5326
4480/9333 [=============>................] - ETA: 14:09 - loss: 0.6895 - acc: 0.5326
4544/9333 [=============>................] - ETA: 13:56 - loss: 0.6897 - acc: 0.5319
4608/9333 [=============>................] - ETA: 13:45 - loss: 0.6896 - acc: 0.5328
4672/9333 [==============>...............] - ETA: 13:34 - loss: 0.6898 - acc: 0.5317
4736/9333 [==============>...............] - ETA: 13:22 - loss: 0.6893 - acc: 0.5342
4800/9333 [==============>...............] - ETA: 13:10 - loss: 0.6891 - acc: 0.5346
4864/9333 [==============>...............] - ETA: 12:59 - loss: 0.6895 - acc: 0.5335
4928/9333 [==============>...............] - ETA: 12:48 - loss: 0.6891 - acc: 0.5343
4992/9333 [===============>..............] - ETA: 12:36 - loss: 0.6892 - acc: 0.5339
5056/9333 [===============>..............] - ETA: 12:25 - loss: 0.6890 - acc: 0.5348
5120/9333 [===============>..............] - ETA: 12:14 - loss: 0.6891 - acc: 0.5342
5184/9333 [===============>..............] - ETA: 12:02 - loss: 0.6890 - acc: 0.5341
5248/9333 [===============>..............] - ETA: 11:50 - loss: 0.6889 - acc: 0.5345
5312/9333 [================>.............] - ETA: 11:39 - loss: 0.6886 - acc: 0.5352
5376/9333 [================>.............] - ETA: 11:28 - loss: 0.6888 - acc: 0.5348
5440/9333 [================>.............] - ETA: 11:17 - loss: 0.6889 - acc: 0.5346
5504/9333 [================>.............] - ETA: 11:06 - loss: 0.6891 - acc: 0.5340
5568/9333 [================>.............] - ETA: 10:54 - loss: 0.6892 - acc: 0.5330
5632/9333 [=================>............] - ETA: 10:44 - loss: 0.6892 - acc: 0.5325
5696/9333 [=================>............] - ETA: 10:32 - loss: 0.6893 - acc: 0.5328
5760/9333 [=================>............] - ETA: 10:21 - loss: 0.6896 - acc: 0.5318
5824/9333 [=================>............] - ETA: 10:09 - loss: 0.6893 - acc: 0.5328
5888/9333 [=================>............] - ETA: 9:58 - loss: 0.6892 - acc: 0.5335 
5952/9333 [==================>...........] - ETA: 9:47 - loss: 0.6891 - acc: 0.5334
6016/9333 [==================>...........] - ETA: 9:35 - loss: 0.6889 - acc: 0.5337
6080/9333 [==================>...........] - ETA: 9:24 - loss: 0.6891 - acc: 0.5331
6144/9333 [==================>...........] - ETA: 9:13 - loss: 0.6894 - acc: 0.5329
6208/9333 [==================>...........] - ETA: 9:01 - loss: 0.6893 - acc: 0.5327
6272/9333 [===================>..........] - ETA: 8:50 - loss: 0.6894 - acc: 0.5325
6336/9333 [===================>..........] - ETA: 8:39 - loss: 0.6893 - acc: 0.5330
6400/9333 [===================>..........] - ETA: 8:28 - loss: 0.6889 - acc: 0.5339
6464/9333 [===================>..........] - ETA: 8:17 - loss: 0.6890 - acc: 0.5340
6528/9333 [===================>..........] - ETA: 8:05 - loss: 0.6890 - acc: 0.5345
6592/9333 [====================>.........] - ETA: 7:54 - loss: 0.6889 - acc: 0.5349
6656/9333 [====================>.........] - ETA: 7:43 - loss: 0.6888 - acc: 0.5358
6720/9333 [====================>.........] - ETA: 7:32 - loss: 0.6888 - acc: 0.5357
6784/9333 [====================>.........] - ETA: 7:21 - loss: 0.6887 - acc: 0.5363
6848/9333 [=====================>........] - ETA: 7:10 - loss: 0.6890 - acc: 0.5359
6912/9333 [=====================>........] - ETA: 6:58 - loss: 0.6892 - acc: 0.5350
6976/9333 [=====================>........] - ETA: 6:47 - loss: 0.6894 - acc: 0.5338
7040/9333 [=====================>........] - ETA: 6:36 - loss: 0.6897 - acc: 0.5327
7104/9333 [=====================>........] - ETA: 6:25 - loss: 0.6897 - acc: 0.5328
7168/9333 [======================>.......] - ETA: 6:14 - loss: 0.6897 - acc: 0.5332
7232/9333 [======================>.......] - ETA: 6:03 - loss: 0.6896 - acc: 0.5332
7296/9333 [======================>.......] - ETA: 5:52 - loss: 0.6895 - acc: 0.5336
7360/9333 [======================>.......] - ETA: 5:41 - loss: 0.6895 - acc: 0.5345
7424/9333 [======================>.......] - ETA: 5:29 - loss: 0.6894 - acc: 0.5343
7488/9333 [=======================>......] - ETA: 5:18 - loss: 0.6892 - acc: 0.5355
7552/9333 [=======================>......] - ETA: 5:07 - loss: 0.6890 - acc: 0.5356
7616/9333 [=======================>......] - ETA: 4:56 - loss: 0.6889 - acc: 0.5366
7680/9333 [=======================>......] - ETA: 4:45 - loss: 0.6889 - acc: 0.5366
7744/9333 [=======================>......] - ETA: 4:34 - loss: 0.6887 - acc: 0.5376
7808/9333 [========================>.....] - ETA: 4:23 - loss: 0.6887 - acc: 0.5378
7872/9333 [========================>.....] - ETA: 4:12 - loss: 0.6889 - acc: 0.5373
7936/9333 [========================>.....] - ETA: 4:01 - loss: 0.6890 - acc: 0.5367
8000/9333 [========================>.....] - ETA: 3:50 - loss: 0.6891 - acc: 0.5367
8064/9333 [========================>.....] - ETA: 3:39 - loss: 0.6892 - acc: 0.5365
8128/9333 [=========================>....] - ETA: 3:28 - loss: 0.6895 - acc: 0.5354
8192/9333 [=========================>....] - ETA: 3:16 - loss: 0.6893 - acc: 0.5356
8256/9333 [=========================>....] - ETA: 3:05 - loss: 0.6893 - acc: 0.5354
8320/9333 [=========================>....] - ETA: 2:54 - loss: 0.6893 - acc: 0.5352
8384/9333 [=========================>....] - ETA: 2:43 - loss: 0.6892 - acc: 0.5354
8448/9333 [==========================>...] - ETA: 2:32 - loss: 0.6891 - acc: 0.5357
8512/9333 [==========================>...] - ETA: 2:21 - loss: 0.6892 - acc: 0.5355
8576/9333 [==========================>...] - ETA: 2:10 - loss: 0.6891 - acc: 0.5360
8640/9333 [==========================>...] - ETA: 1:59 - loss: 0.6890 - acc: 0.5362
8704/9333 [==========================>...] - ETA: 1:48 - loss: 0.6890 - acc: 0.5363
8768/9333 [===========================>..] - ETA: 1:37 - loss: 0.6890 - acc: 0.5360
8832/9333 [===========================>..] - ETA: 1:26 - loss: 0.6889 - acc: 0.5365
8896/9333 [===========================>..] - ETA: 1:15 - loss: 0.6890 - acc: 0.5360
8960/9333 [===========================>..] - ETA: 1:04 - loss: 0.6889 - acc: 0.5368
9024/9333 [============================>.] - ETA: 53s - loss: 0.6890 - acc: 0.5359 
9088/9333 [============================>.] - ETA: 42s - loss: 0.6890 - acc: 0.5363
9152/9333 [============================>.] - ETA: 31s - loss: 0.6890 - acc: 0.5364
9216/9333 [============================>.] - ETA: 20s - loss: 0.6890 - acc: 0.5361
9280/9333 [============================>.] - ETA: 9s - loss: 0.6889 - acc: 0.5364 
9333/9333 [==============================] - 1654s 177ms/step - loss: 0.6890 - acc: 0.5361 - val_loss: 0.6899 - val_acc: 0.5400

Epoch 00006: val_acc did not improve from 0.54291
Epoch 7/10

  64/9333 [..............................] - ETA: 24:48 - loss: 0.6756 - acc: 0.6406
 128/9333 [..............................] - ETA: 24:07 - loss: 0.6895 - acc: 0.5547
 192/9333 [..............................] - ETA: 24:09 - loss: 0.6891 - acc: 0.5573
 256/9333 [..............................] - ETA: 23:45 - loss: 0.6860 - acc: 0.5703
 320/9333 [>.............................] - ETA: 24:02 - loss: 0.6888 - acc: 0.5531
 384/9333 [>.............................] - ETA: 23:45 - loss: 0.6867 - acc: 0.5651
 448/9333 [>.............................] - ETA: 23:30 - loss: 0.6850 - acc: 0.5692
 512/9333 [>.............................] - ETA: 23:17 - loss: 0.6862 - acc: 0.5625
 576/9333 [>.............................] - ETA: 23:05 - loss: 0.6879 - acc: 0.5573
 640/9333 [=>............................] - ETA: 22:59 - loss: 0.6871 - acc: 0.5578
 704/9333 [=>............................] - ETA: 23:08 - loss: 0.6856 - acc: 0.5639
 768/9333 [=>............................] - ETA: 22:59 - loss: 0.6860 - acc: 0.5599
 832/9333 [=>............................] - ETA: 22:56 - loss: 0.6867 - acc: 0.5613
 896/9333 [=>............................] - ETA: 22:45 - loss: 0.6870 - acc: 0.5603
 960/9333 [==>...........................] - ETA: 22:41 - loss: 0.6871 - acc: 0.5583
1024/9333 [==>...........................] - ETA: 22:36 - loss: 0.6871 - acc: 0.5537
1088/9333 [==>...........................] - ETA: 22:30 - loss: 0.6893 - acc: 0.5469
1152/9333 [==>...........................] - ETA: 22:25 - loss: 0.6905 - acc: 0.5425
1216/9333 [==>...........................] - ETA: 22:10 - loss: 0.6913 - acc: 0.5419
1280/9333 [===>..........................] - ETA: 22:07 - loss: 0.6899 - acc: 0.5461
1344/9333 [===>..........................] - ETA: 21:53 - loss: 0.6905 - acc: 0.5409
1408/9333 [===>..........................] - ETA: 21:44 - loss: 0.6905 - acc: 0.5391
1472/9333 [===>..........................] - ETA: 21:38 - loss: 0.6915 - acc: 0.5360
1536/9333 [===>..........................] - ETA: 21:17 - loss: 0.6917 - acc: 0.5345
1600/9333 [====>.........................] - ETA: 38:49 - loss: 0.6904 - acc: 0.5394
1664/9333 [====>.........................] - ETA: 37:45 - loss: 0.6917 - acc: 0.5367
1728/9333 [====>.........................] - ETA: 36:46 - loss: 0.6914 - acc: 0.5365
1792/9333 [====>.........................] - ETA: 35:48 - loss: 0.6914 - acc: 0.5368
1856/9333 [====>.........................] - ETA: 34:57 - loss: 0.6916 - acc: 0.5345
1920/9333 [=====>........................] - ETA: 34:08 - loss: 0.6919 - acc: 0.5344
1984/9333 [=====>........................] - ETA: 33:18 - loss: 0.6919 - acc: 0.5353
2048/9333 [=====>........................] - ETA: 32:32 - loss: 0.6916 - acc: 0.5356
2112/9333 [=====>........................] - ETA: 31:46 - loss: 0.6918 - acc: 0.5374
2176/9333 [=====>........................] - ETA: 31:05 - loss: 0.6917 - acc: 0.5345
2240/9333 [======>.......................] - ETA: 30:24 - loss: 0.6908 - acc: 0.5366
2304/9333 [======>.......................] - ETA: 29:45 - loss: 0.6905 - acc: 0.5369
2368/9333 [======>.......................] - ETA: 29:09 - loss: 0.6904 - acc: 0.5389
2432/9333 [======>.......................] - ETA: 28:33 - loss: 0.6906 - acc: 0.5374
2496/9333 [=======>......................] - ETA: 28:02 - loss: 0.6900 - acc: 0.5393
2560/9333 [=======>......................] - ETA: 27:28 - loss: 0.6905 - acc: 0.5379
2624/9333 [=======>......................] - ETA: 26:57 - loss: 0.6901 - acc: 0.5393
2688/9333 [=======>......................] - ETA: 26:25 - loss: 0.6897 - acc: 0.5413
2752/9333 [=======>......................] - ETA: 25:56 - loss: 0.6892 - acc: 0.5432
2816/9333 [========>.....................] - ETA: 25:29 - loss: 0.6893 - acc: 0.5412
2880/9333 [========>.....................] - ETA: 25:01 - loss: 0.6896 - acc: 0.5396
2944/9333 [========>.....................] - ETA: 24:34 - loss: 0.6894 - acc: 0.5397
3008/9333 [========>.....................] - ETA: 24:07 - loss: 0.6894 - acc: 0.5382
3072/9333 [========>.....................] - ETA: 23:41 - loss: 0.6894 - acc: 0.5387
3136/9333 [=========>....................] - ETA: 23:17 - loss: 0.6894 - acc: 0.5402
3200/9333 [=========>....................] - ETA: 22:54 - loss: 0.6898 - acc: 0.5375
3264/9333 [=========>....................] - ETA: 22:31 - loss: 0.6897 - acc: 0.5374
3328/9333 [=========>....................] - ETA: 22:18 - loss: 0.6899 - acc: 0.5373
3392/9333 [=========>....................] - ETA: 21:56 - loss: 0.6901 - acc: 0.5351
3456/9333 [==========>...................] - ETA: 21:34 - loss: 0.6901 - acc: 0.5356
3520/9333 [==========>...................] - ETA: 21:08 - loss: 0.6899 - acc: 0.5364
3584/9333 [==========>...................] - ETA: 20:48 - loss: 0.6901 - acc: 0.5360
3648/9333 [==========>...................] - ETA: 20:22 - loss: 0.6903 - acc: 0.5351
3712/9333 [==========>...................] - ETA: 19:58 - loss: 0.6901 - acc: 0.5356
3776/9333 [===========>..................] - ETA: 19:33 - loss: 0.6902 - acc: 0.5358
3840/9333 [===========>..................] - ETA: 19:09 - loss: 0.6904 - acc: 0.5357
3904/9333 [===========>..................] - ETA: 18:45 - loss: 0.6904 - acc: 0.5359
3968/9333 [===========>..................] - ETA: 18:22 - loss: 0.6907 - acc: 0.5348
4032/9333 [===========>..................] - ETA: 17:59 - loss: 0.6907 - acc: 0.5355
4096/9333 [============>.................] - ETA: 17:37 - loss: 0.6905 - acc: 0.5359
4160/9333 [============>.................] - ETA: 17:16 - loss: 0.6904 - acc: 0.5368
4224/9333 [============>.................] - ETA: 16:56 - loss: 0.6903 - acc: 0.5369
4288/9333 [============>.................] - ETA: 16:35 - loss: 0.6905 - acc: 0.5361
4352/9333 [============>.................] - ETA: 16:14 - loss: 0.6901 - acc: 0.5375
4416/9333 [=============>................] - ETA: 15:54 - loss: 0.6899 - acc: 0.5389
4480/9333 [=============>................] - ETA: 15:34 - loss: 0.6900 - acc: 0.5388
4544/9333 [=============>................] - ETA: 15:18 - loss: 0.6899 - acc: 0.5385
4608/9333 [=============>................] - ETA: 15:22 - loss: 0.6898 - acc: 0.5386
4672/9333 [==============>...............] - ETA: 15:05 - loss: 0.6896 - acc: 0.5392
4736/9333 [==============>...............] - ETA: 14:49 - loss: 0.6895 - acc: 0.5391
4800/9333 [==============>...............] - ETA: 14:33 - loss: 0.6894 - acc: 0.5392
4864/9333 [==============>...............] - ETA: 14:15 - loss: 0.6890 - acc: 0.5403
4928/9333 [==============>...............] - ETA: 13:58 - loss: 0.6889 - acc: 0.5408
4992/9333 [===============>..............] - ETA: 18:36 - loss: 0.6891 - acc: 0.5403
5056/9333 [===============>..............] - ETA: 18:13 - loss: 0.6893 - acc: 0.5392
5120/9333 [===============>..............] - ETA: 18:38 - loss: 0.6895 - acc: 0.5381
5184/9333 [===============>..............] - ETA: 18:13 - loss: 0.6893 - acc: 0.5397
5248/9333 [===============>..............] - ETA: 17:50 - loss: 0.6893 - acc: 0.5396
5312/9333 [================>.............] - ETA: 17:27 - loss: 0.6894 - acc: 0.5397
5376/9333 [================>.............] - ETA: 17:13 - loss: 0.6893 - acc: 0.5394
5440/9333 [================>.............] - ETA: 16:49 - loss: 0.6890 - acc: 0.5399
5504/9333 [================>.............] - ETA: 16:26 - loss: 0.6891 - acc: 0.5398
5568/9333 [================>.............] - ETA: 16:04 - loss: 0.6890 - acc: 0.5404
5632/9333 [=================>............] - ETA: 15:42 - loss: 0.6887 - acc: 0.5414
5696/9333 [=================>............] - ETA: 15:21 - loss: 0.6889 - acc: 0.5407
5760/9333 [=================>............] - ETA: 14:59 - loss: 0.6891 - acc: 0.5401
5824/9333 [=================>............] - ETA: 15:17 - loss: 0.6891 - acc: 0.5404
5888/9333 [=================>............] - ETA: 14:55 - loss: 0.6890 - acc: 0.5404
5952/9333 [==================>...........] - ETA: 14:33 - loss: 0.6887 - acc: 0.5412
6016/9333 [==================>...........] - ETA: 14:11 - loss: 0.6885 - acc: 0.5426
6080/9333 [==================>...........] - ETA: 13:50 - loss: 0.6884 - acc: 0.5428
6144/9333 [==================>...........] - ETA: 13:28 - loss: 0.6883 - acc: 0.5433
6208/9333 [==================>...........] - ETA: 13:08 - loss: 0.6882 - acc: 0.5438
6272/9333 [===================>..........] - ETA: 12:47 - loss: 0.6883 - acc: 0.5438
6336/9333 [===================>..........] - ETA: 12:27 - loss: 0.6883 - acc: 0.5442
6400/9333 [===================>..........] - ETA: 12:07 - loss: 0.6883 - acc: 0.5439
6464/9333 [===================>..........] - ETA: 11:48 - loss: 0.6882 - acc: 0.5442
6528/9333 [===================>..........] - ETA: 11:29 - loss: 0.6880 - acc: 0.5446
6592/9333 [====================>.........] - ETA: 11:11 - loss: 0.6880 - acc: 0.5448
6656/9333 [====================>.........] - ETA: 10:52 - loss: 0.6881 - acc: 0.5451
6720/9333 [====================>.........] - ETA: 10:33 - loss: 0.6880 - acc: 0.5454
6784/9333 [====================>.........] - ETA: 10:15 - loss: 0.6880 - acc: 0.5453
6848/9333 [=====================>........] - ETA: 9:57 - loss: 0.6880 - acc: 0.5456 
6912/9333 [=====================>........] - ETA: 9:39 - loss: 0.6877 - acc: 0.5460
6976/9333 [=====================>........] - ETA: 9:21 - loss: 0.6876 - acc: 0.5462
7040/9333 [=====================>........] - ETA: 9:04 - loss: 0.6875 - acc: 0.5464
7104/9333 [=====================>........] - ETA: 8:46 - loss: 0.6875 - acc: 0.5472
7168/9333 [======================>.......] - ETA: 8:28 - loss: 0.6875 - acc: 0.5466
7232/9333 [======================>.......] - ETA: 8:10 - loss: 0.6877 - acc: 0.5460
7296/9333 [======================>.......] - ETA: 7:53 - loss: 0.6876 - acc: 0.5470
7360/9333 [======================>.......] - ETA: 7:37 - loss: 0.6874 - acc: 0.5473
7424/9333 [======================>.......] - ETA: 7:20 - loss: 0.6873 - acc: 0.5474
7488/9333 [=======================>......] - ETA: 7:04 - loss: 0.6872 - acc: 0.5475
7552/9333 [=======================>......] - ETA: 6:47 - loss: 0.6871 - acc: 0.5473
7616/9333 [=======================>......] - ETA: 6:31 - loss: 0.6869 - acc: 0.5478
7680/9333 [=======================>......] - ETA: 6:15 - loss: 0.6865 - acc: 0.5492
7744/9333 [=======================>......] - ETA: 6:00 - loss: 0.6864 - acc: 0.5498
7808/9333 [========================>.....] - ETA: 5:45 - loss: 0.6866 - acc: 0.5502
7872/9333 [========================>.....] - ETA: 5:29 - loss: 0.6862 - acc: 0.5513
7936/9333 [========================>.....] - ETA: 5:14 - loss: 0.6865 - acc: 0.5512
8000/9333 [========================>.....] - ETA: 5:00 - loss: 0.6867 - acc: 0.5510
8064/9333 [========================>.....] - ETA: 4:45 - loss: 0.6870 - acc: 0.5502
8128/9333 [=========================>....] - ETA: 4:30 - loss: 0.6869 - acc: 0.5501
8192/9333 [=========================>....] - ETA: 4:15 - loss: 0.6869 - acc: 0.5505
8256/9333 [=========================>....] - ETA: 4:00 - loss: 0.6868 - acc: 0.5509
8320/9333 [=========================>....] - ETA: 3:45 - loss: 0.6868 - acc: 0.5507
8384/9333 [=========================>....] - ETA: 3:30 - loss: 0.6870 - acc: 0.5500
8448/9333 [==========================>...] - ETA: 3:15 - loss: 0.6868 - acc: 0.5502
8512/9333 [==========================>...] - ETA: 3:01 - loss: 0.6870 - acc: 0.5500
8576/9333 [==========================>...] - ETA: 2:46 - loss: 0.6871 - acc: 0.5504
8640/9333 [==========================>...] - ETA: 2:32 - loss: 0.6869 - acc: 0.5510
8704/9333 [==========================>...] - ETA: 2:17 - loss: 0.6868 - acc: 0.5511
8768/9333 [===========================>..] - ETA: 2:03 - loss: 0.6867 - acc: 0.5516
8832/9333 [===========================>..] - ETA: 1:49 - loss: 0.6866 - acc: 0.5519
8896/9333 [===========================>..] - ETA: 1:35 - loss: 0.6866 - acc: 0.5519
8960/9333 [===========================>..] - ETA: 1:20 - loss: 0.6866 - acc: 0.5521
9024/9333 [============================>.] - ETA: 1:06 - loss: 0.6866 - acc: 0.5525
9088/9333 [============================>.] - ETA: 52s - loss: 0.6866 - acc: 0.5525 
9152/9333 [============================>.] - ETA: 39s - loss: 0.6869 - acc: 0.5518
9216/9333 [============================>.] - ETA: 25s - loss: 0.6869 - acc: 0.5513
9280/9333 [============================>.] - ETA: 11s - loss: 0.6868 - acc: 0.5515
9333/9333 [==============================] - 2053s 220ms/step - loss: 0.6869 - acc: 0.5511 - val_loss: 0.6898 - val_acc: 0.5227

Epoch 00007: val_acc did not improve from 0.54291
Epoch 8/10

  64/9333 [..............................] - ETA: 24:01 - loss: 0.6893 - acc: 0.5781
 128/9333 [..............................] - ETA: 23:57 - loss: 0.6930 - acc: 0.5547
 192/9333 [..............................] - ETA: 22:43 - loss: 0.6970 - acc: 0.5312
 256/9333 [..............................] - ETA: 22:25 - loss: 0.6967 - acc: 0.5352
 320/9333 [>.............................] - ETA: 22:18 - loss: 0.6971 - acc: 0.5344
 384/9333 [>.............................] - ETA: 22:16 - loss: 0.6942 - acc: 0.5417
 448/9333 [>.............................] - ETA: 21:50 - loss: 0.6927 - acc: 0.5379
 512/9333 [>.............................] - ETA: 21:30 - loss: 0.6962 - acc: 0.5293
 576/9333 [>.............................] - ETA: 21:17 - loss: 0.6951 - acc: 0.5330
 640/9333 [=>............................] - ETA: 21:10 - loss: 0.6940 - acc: 0.5344
 704/9333 [=>............................] - ETA: 21:16 - loss: 0.6931 - acc: 0.5341
 768/9333 [=>............................] - ETA: 21:08 - loss: 0.6909 - acc: 0.5404
 832/9333 [=>............................] - ETA: 20:58 - loss: 0.6905 - acc: 0.5385
 896/9333 [=>............................] - ETA: 20:41 - loss: 0.6897 - acc: 0.5402
 960/9333 [==>...........................] - ETA: 20:31 - loss: 0.6910 - acc: 0.5365
1024/9333 [==>...........................] - ETA: 20:21 - loss: 0.6895 - acc: 0.5439
1088/9333 [==>...........................] - ETA: 20:09 - loss: 0.6880 - acc: 0.5515
1152/9333 [==>...........................] - ETA: 20:05 - loss: 0.6883 - acc: 0.5512
1216/9333 [==>...........................] - ETA: 19:51 - loss: 0.6894 - acc: 0.5510
1280/9333 [===>..........................] - ETA: 19:48 - loss: 0.6896 - acc: 0.5508
1344/9333 [===>..........................] - ETA: 19:40 - loss: 0.6892 - acc: 0.5528
1408/9333 [===>..........................] - ETA: 19:30 - loss: 0.6907 - acc: 0.5462
1472/9333 [===>..........................] - ETA: 19:19 - loss: 0.6894 - acc: 0.5469
1536/9333 [===>..........................] - ETA: 19:16 - loss: 0.6899 - acc: 0.5462
1600/9333 [====>.........................] - ETA: 19:11 - loss: 0.6896 - acc: 0.5475
1664/9333 [====>.........................] - ETA: 19:03 - loss: 0.6893 - acc: 0.5493
1728/9333 [====>.........................] - ETA: 18:59 - loss: 0.6891 - acc: 0.5492
1792/9333 [====>.........................] - ETA: 18:49 - loss: 0.6891 - acc: 0.5485
1856/9333 [====>.........................] - ETA: 18:39 - loss: 0.6882 - acc: 0.5506
1920/9333 [=====>........................] - ETA: 18:30 - loss: 0.6884 - acc: 0.5500
1984/9333 [=====>........................] - ETA: 18:19 - loss: 0.6883 - acc: 0.5514
2048/9333 [=====>........................] - ETA: 18:11 - loss: 0.6874 - acc: 0.5547
2112/9333 [=====>........................] - ETA: 18:03 - loss: 0.6878 - acc: 0.5545
2176/9333 [=====>........................] - ETA: 17:54 - loss: 0.6875 - acc: 0.5547
2240/9333 [======>.......................] - ETA: 17:44 - loss: 0.6880 - acc: 0.5527
2304/9333 [======>.......................] - ETA: 17:34 - loss: 0.6876 - acc: 0.5525
2368/9333 [======>.......................] - ETA: 17:24 - loss: 0.6869 - acc: 0.5524
2432/9333 [======>.......................] - ETA: 17:14 - loss: 0.6867 - acc: 0.5514
2496/9333 [=======>......................] - ETA: 17:07 - loss: 0.6866 - acc: 0.5509
2560/9333 [=======>......................] - ETA: 16:56 - loss: 0.6874 - acc: 0.5488
2624/9333 [=======>......................] - ETA: 16:47 - loss: 0.6876 - acc: 0.5488
2688/9333 [=======>......................] - ETA: 16:40 - loss: 0.6875 - acc: 0.5491
2752/9333 [=======>......................] - ETA: 16:29 - loss: 0.6877 - acc: 0.5465
2816/9333 [========>.....................] - ETA: 16:15 - loss: 0.6879 - acc: 0.5458
2880/9333 [========>.....................] - ETA: 16:06 - loss: 0.6879 - acc: 0.5441
2944/9333 [========>.....................] - ETA: 15:59 - loss: 0.6881 - acc: 0.5431
3008/9333 [========>.....................] - ETA: 15:49 - loss: 0.6876 - acc: 0.5442
3072/9333 [========>.....................] - ETA: 15:39 - loss: 0.6873 - acc: 0.5456
3136/9333 [=========>....................] - ETA: 15:31 - loss: 0.6874 - acc: 0.5453
3200/9333 [=========>....................] - ETA: 15:22 - loss: 0.6875 - acc: 0.5434
3264/9333 [=========>....................] - ETA: 15:13 - loss: 0.6873 - acc: 0.5447
3328/9333 [=========>....................] - ETA: 15:05 - loss: 0.6872 - acc: 0.5451
3392/9333 [=========>....................] - ETA: 14:56 - loss: 0.6869 - acc: 0.5463
3456/9333 [==========>...................] - ETA: 14:49 - loss: 0.6869 - acc: 0.5454
3520/9333 [==========>...................] - ETA: 14:40 - loss: 0.6870 - acc: 0.5446
3584/9333 [==========>...................] - ETA: 14:32 - loss: 0.6866 - acc: 0.5458
3648/9333 [==========>...................] - ETA: 14:22 - loss: 0.6863 - acc: 0.5471
3712/9333 [==========>...................] - ETA: 14:14 - loss: 0.6865 - acc: 0.5463
3776/9333 [===========>..................] - ETA: 14:05 - loss: 0.6864 - acc: 0.5466
3840/9333 [===========>..................] - ETA: 13:55 - loss: 0.6870 - acc: 0.5451
3904/9333 [===========>..................] - ETA: 13:44 - loss: 0.6871 - acc: 0.5451
3968/9333 [===========>..................] - ETA: 13:34 - loss: 0.6871 - acc: 0.5449
4032/9333 [===========>..................] - ETA: 13:25 - loss: 0.6874 - acc: 0.5444
4096/9333 [============>.................] - ETA: 13:16 - loss: 0.6872 - acc: 0.5444
4160/9333 [============>.................] - ETA: 13:07 - loss: 0.6874 - acc: 0.5437
4224/9333 [============>.................] - ETA: 12:58 - loss: 0.6879 - acc: 0.5419
4288/9333 [============>.................] - ETA: 12:48 - loss: 0.6878 - acc: 0.5424
4352/9333 [============>.................] - ETA: 12:39 - loss: 0.6878 - acc: 0.5425
4416/9333 [=============>................] - ETA: 12:29 - loss: 0.6876 - acc: 0.5428
4480/9333 [=============>................] - ETA: 12:19 - loss: 0.6873 - acc: 0.5431
4544/9333 [=============>................] - ETA: 12:09 - loss: 0.6878 - acc: 0.5416
4608/9333 [=============>................] - ETA: 12:00 - loss: 0.6875 - acc: 0.5425
4672/9333 [==============>...............] - ETA: 11:50 - loss: 0.6879 - acc: 0.5409
4736/9333 [==============>...............] - ETA: 11:40 - loss: 0.6879 - acc: 0.5414
4800/9333 [==============>...............] - ETA: 11:29 - loss: 0.6879 - acc: 0.5415
4864/9333 [==============>...............] - ETA: 11:20 - loss: 0.6877 - acc: 0.5415
4928/9333 [==============>...............] - ETA: 11:11 - loss: 0.6875 - acc: 0.5416
4992/9333 [===============>..............] - ETA: 11:01 - loss: 0.6875 - acc: 0.5419
5056/9333 [===============>..............] - ETA: 10:52 - loss: 0.6873 - acc: 0.5417
5120/9333 [===============>..............] - ETA: 10:43 - loss: 0.6871 - acc: 0.5428
5184/9333 [===============>..............] - ETA: 10:33 - loss: 0.6872 - acc: 0.5424
5248/9333 [===============>..............] - ETA: 10:24 - loss: 0.6866 - acc: 0.5444
5312/9333 [================>.............] - ETA: 10:15 - loss: 0.6867 - acc: 0.5439
5376/9333 [================>.............] - ETA: 10:05 - loss: 0.6873 - acc: 0.5426
5440/9333 [================>.............] - ETA: 9:55 - loss: 0.6872 - acc: 0.5430 
5504/9333 [================>.............] - ETA: 9:46 - loss: 0.6872 - acc: 0.5431
5568/9333 [================>.............] - ETA: 9:36 - loss: 0.6874 - acc: 0.5426
5632/9333 [=================>............] - ETA: 9:27 - loss: 0.6879 - acc: 0.5410
5696/9333 [=================>............] - ETA: 9:17 - loss: 0.6878 - acc: 0.5413
5760/9333 [=================>............] - ETA: 9:07 - loss: 0.6879 - acc: 0.5410
5824/9333 [=================>............] - ETA: 8:58 - loss: 0.6878 - acc: 0.5419
5888/9333 [=================>............] - ETA: 8:48 - loss: 0.6878 - acc: 0.5418
5952/9333 [==================>...........] - ETA: 8:37 - loss: 0.6879 - acc: 0.5408
6016/9333 [==================>...........] - ETA: 8:28 - loss: 0.6883 - acc: 0.5401
6080/9333 [==================>...........] - ETA: 8:18 - loss: 0.6885 - acc: 0.5391
6144/9333 [==================>...........] - ETA: 8:08 - loss: 0.6883 - acc: 0.5389
6208/9333 [==================>...........] - ETA: 7:58 - loss: 0.6888 - acc: 0.5370
6272/9333 [===================>..........] - ETA: 7:48 - loss: 0.6888 - acc: 0.5365
6336/9333 [===================>..........] - ETA: 7:39 - loss: 0.6885 - acc: 0.5372
6400/9333 [===================>..........] - ETA: 7:29 - loss: 0.6884 - acc: 0.5381
6464/9333 [===================>..........] - ETA: 7:19 - loss: 0.6884 - acc: 0.5379
6528/9333 [===================>..........] - ETA: 7:09 - loss: 0.6886 - acc: 0.5375
6592/9333 [====================>.........] - ETA: 7:00 - loss: 0.6886 - acc: 0.5375
6656/9333 [====================>.........] - ETA: 6:50 - loss: 0.6885 - acc: 0.5376
6720/9333 [====================>.........] - ETA: 6:40 - loss: 0.6886 - acc: 0.5378
6784/9333 [====================>.........] - ETA: 6:31 - loss: 0.6885 - acc: 0.5377
6848/9333 [=====================>........] - ETA: 6:21 - loss: 0.6884 - acc: 0.5380
6912/9333 [=====================>........] - ETA: 6:11 - loss: 0.6881 - acc: 0.5386
6976/9333 [=====================>........] - ETA: 6:01 - loss: 0.6879 - acc: 0.5391
7040/9333 [=====================>........] - ETA: 5:51 - loss: 0.6879 - acc: 0.5393
7104/9333 [=====================>........] - ETA: 5:41 - loss: 0.6879 - acc: 0.5391
7168/9333 [======================>.......] - ETA: 5:32 - loss: 0.6879 - acc: 0.5388
7232/9333 [======================>.......] - ETA: 5:22 - loss: 0.6880 - acc: 0.5383
7296/9333 [======================>.......] - ETA: 5:12 - loss: 0.6879 - acc: 0.5388
7360/9333 [======================>.......] - ETA: 5:03 - loss: 0.6878 - acc: 0.5397
7424/9333 [======================>.......] - ETA: 4:53 - loss: 0.6877 - acc: 0.5396
7488/9333 [=======================>......] - ETA: 4:43 - loss: 0.6878 - acc: 0.5393
7552/9333 [=======================>......] - ETA: 4:33 - loss: 0.6877 - acc: 0.5392
7616/9333 [=======================>......] - ETA: 4:24 - loss: 0.6875 - acc: 0.5400
7680/9333 [=======================>......] - ETA: 4:14 - loss: 0.6877 - acc: 0.5396
7744/9333 [=======================>......] - ETA: 4:04 - loss: 0.6879 - acc: 0.5396
7808/9333 [========================>.....] - ETA: 3:54 - loss: 0.6880 - acc: 0.5393
7872/9333 [========================>.....] - ETA: 3:44 - loss: 0.6880 - acc: 0.5393
7936/9333 [========================>.....] - ETA: 3:35 - loss: 0.6882 - acc: 0.5384
8000/9333 [========================>.....] - ETA: 3:25 - loss: 0.6885 - acc: 0.5377
8064/9333 [========================>.....] - ETA: 3:15 - loss: 0.6888 - acc: 0.5366
8128/9333 [=========================>....] - ETA: 3:05 - loss: 0.6887 - acc: 0.5374
8192/9333 [=========================>....] - ETA: 2:55 - loss: 0.6887 - acc: 0.5377
8256/9333 [=========================>....] - ETA: 2:46 - loss: 0.6886 - acc: 0.5383
8320/9333 [=========================>....] - ETA: 2:36 - loss: 0.6887 - acc: 0.5380
8384/9333 [=========================>....] - ETA: 2:26 - loss: 0.6886 - acc: 0.5384
8448/9333 [==========================>...] - ETA: 2:16 - loss: 0.6885 - acc: 0.5385
8512/9333 [==========================>...] - ETA: 2:07 - loss: 0.6885 - acc: 0.5388
8576/9333 [==========================>...] - ETA: 1:57 - loss: 0.6885 - acc: 0.5388
8640/9333 [==========================>...] - ETA: 1:47 - loss: 0.6884 - acc: 0.5388
8704/9333 [==========================>...] - ETA: 1:37 - loss: 0.6883 - acc: 0.5394
8768/9333 [===========================>..] - ETA: 1:27 - loss: 0.6884 - acc: 0.5389
8832/9333 [===========================>..] - ETA: 1:17 - loss: 0.6885 - acc: 0.5386
8896/9333 [===========================>..] - ETA: 1:07 - loss: 0.6886 - acc: 0.5384
8960/9333 [===========================>..] - ETA: 57s - loss: 0.6887 - acc: 0.5383 
9024/9333 [============================>.] - ETA: 47s - loss: 0.6888 - acc: 0.5383
9088/9333 [============================>.] - ETA: 38s - loss: 0.6887 - acc: 0.5388
9152/9333 [============================>.] - ETA: 28s - loss: 0.6887 - acc: 0.5394
9216/9333 [============================>.] - ETA: 18s - loss: 0.6889 - acc: 0.5384
9280/9333 [============================>.] - ETA: 8s - loss: 0.6888 - acc: 0.5384 
9333/9333 [==============================] - 1497s 160ms/step - loss: 0.6889 - acc: 0.5383 - val_loss: 0.6939 - val_acc: 0.5178

Epoch 00008: val_acc did not improve from 0.54291
Epoch 9/10

  64/9333 [..............................] - ETA: 25:58 - loss: 0.6970 - acc: 0.4531
 128/9333 [..............................] - ETA: 25:36 - loss: 0.6969 - acc: 0.4844
 192/9333 [..............................] - ETA: 25:18 - loss: 0.6868 - acc: 0.5000
 256/9333 [..............................] - ETA: 24:28 - loss: 0.6845 - acc: 0.5508
 320/9333 [>.............................] - ETA: 24:27 - loss: 0.6915 - acc: 0.5312
 384/9333 [>.............................] - ETA: 24:14 - loss: 0.6936 - acc: 0.5260
 448/9333 [>.............................] - ETA: 24:06 - loss: 0.6998 - acc: 0.5156
 512/9333 [>.............................] - ETA: 23:43 - loss: 0.6969 - acc: 0.5254
 576/9333 [>.............................] - ETA: 23:40 - loss: 0.6940 - acc: 0.5295
 640/9333 [=>............................] - ETA: 23:36 - loss: 0.6951 - acc: 0.5281
 704/9333 [=>............................] - ETA: 23:23 - loss: 0.6946 - acc: 0.5270
 768/9333 [=>............................] - ETA: 23:13 - loss: 0.6944 - acc: 0.5234
 832/9333 [=>............................] - ETA: 22:56 - loss: 0.6936 - acc: 0.5288
 896/9333 [=>............................] - ETA: 22:43 - loss: 0.6935 - acc: 0.5312
 960/9333 [==>...........................] - ETA: 22:29 - loss: 0.6944 - acc: 0.5292
1024/9333 [==>...........................] - ETA: 22:22 - loss: 0.6926 - acc: 0.5361
1088/9333 [==>...........................] - ETA: 22:05 - loss: 0.6935 - acc: 0.5312
1152/9333 [==>...........................] - ETA: 21:51 - loss: 0.6940 - acc: 0.5304
1216/9333 [==>...........................] - ETA: 21:37 - loss: 0.6939 - acc: 0.5304
1280/9333 [===>..........................] - ETA: 21:17 - loss: 0.6928 - acc: 0.5320
1344/9333 [===>..........................] - ETA: 21:05 - loss: 0.6917 - acc: 0.5357
1408/9333 [===>..........................] - ETA: 20:51 - loss: 0.6915 - acc: 0.5384
1472/9333 [===>..........................] - ETA: 20:37 - loss: 0.6916 - acc: 0.5380
1536/9333 [===>..........................] - ETA: 20:25 - loss: 0.6920 - acc: 0.5352
1600/9333 [====>.........................] - ETA: 20:09 - loss: 0.6927 - acc: 0.5319
1664/9333 [====>.........................] - ETA: 19:58 - loss: 0.6921 - acc: 0.5325
1728/9333 [====>.........................] - ETA: 19:48 - loss: 0.6920 - acc: 0.5330
1792/9333 [====>.........................] - ETA: 19:38 - loss: 0.6919 - acc: 0.5329
1856/9333 [====>.........................] - ETA: 19:29 - loss: 0.6919 - acc: 0.5345
1920/9333 [=====>........................] - ETA: 19:17 - loss: 0.6922 - acc: 0.5318
1984/9333 [=====>........................] - ETA: 19:06 - loss: 0.6926 - acc: 0.5272
2048/9333 [=====>........................] - ETA: 18:55 - loss: 0.6921 - acc: 0.5298
2112/9333 [=====>........................] - ETA: 18:43 - loss: 0.6922 - acc: 0.5308
2176/9333 [=====>........................] - ETA: 18:33 - loss: 0.6923 - acc: 0.5303
2240/9333 [======>.......................] - ETA: 18:22 - loss: 0.6917 - acc: 0.5326
2304/9333 [======>.......................] - ETA: 18:13 - loss: 0.6915 - acc: 0.5343
2368/9333 [======>.......................] - ETA: 18:04 - loss: 0.6911 - acc: 0.5351
2432/9333 [======>.......................] - ETA: 17:54 - loss: 0.6910 - acc: 0.5354
2496/9333 [=======>......................] - ETA: 17:44 - loss: 0.6908 - acc: 0.5361
2560/9333 [=======>......................] - ETA: 17:33 - loss: 0.6897 - acc: 0.5398
2624/9333 [=======>......................] - ETA: 17:22 - loss: 0.6893 - acc: 0.5396
2688/9333 [=======>......................] - ETA: 17:13 - loss: 0.6894 - acc: 0.5402
2752/9333 [=======>......................] - ETA: 17:04 - loss: 0.6889 - acc: 0.5414
2816/9333 [========>.....................] - ETA: 16:54 - loss: 0.6894 - acc: 0.5405
2880/9333 [========>.....................] - ETA: 16:44 - loss: 0.6886 - acc: 0.5427
2944/9333 [========>.....................] - ETA: 16:33 - loss: 0.6885 - acc: 0.5428
3008/9333 [========>.....................] - ETA: 16:23 - loss: 0.6889 - acc: 0.5409
3072/9333 [========>.....................] - ETA: 16:13 - loss: 0.6886 - acc: 0.5439
3136/9333 [=========>....................] - ETA: 16:03 - loss: 0.6891 - acc: 0.5443
3200/9333 [=========>....................] - ETA: 15:54 - loss: 0.6898 - acc: 0.5422
3264/9333 [=========>....................] - ETA: 15:44 - loss: 0.6897 - acc: 0.5423
3328/9333 [=========>....................] - ETA: 15:34 - loss: 0.6901 - acc: 0.5418
3392/9333 [=========>....................] - ETA: 15:24 - loss: 0.6898 - acc: 0.5436
3456/9333 [==========>...................] - ETA: 15:16 - loss: 0.6896 - acc: 0.5451
3520/9333 [==========>...................] - ETA: 15:06 - loss: 0.6899 - acc: 0.5446
3584/9333 [==========>...................] - ETA: 14:55 - loss: 0.6901 - acc: 0.5449
3648/9333 [==========>...................] - ETA: 14:44 - loss: 0.6898 - acc: 0.5461
3712/9333 [==========>...................] - ETA: 14:35 - loss: 0.6900 - acc: 0.5458
3776/9333 [===========>..................] - ETA: 14:25 - loss: 0.6895 - acc: 0.5479
3840/9333 [===========>..................] - ETA: 14:15 - loss: 0.6894 - acc: 0.5484
3904/9333 [===========>..................] - ETA: 14:06 - loss: 0.6891 - acc: 0.5487
3968/9333 [===========>..................] - ETA: 13:57 - loss: 0.6893 - acc: 0.5476
4032/9333 [===========>..................] - ETA: 13:47 - loss: 0.6894 - acc: 0.5481
4096/9333 [============>.................] - ETA: 13:38 - loss: 0.6889 - acc: 0.5503
4160/9333 [============>.................] - ETA: 13:28 - loss: 0.6888 - acc: 0.5502
4224/9333 [============>.................] - ETA: 13:20 - loss: 0.6894 - acc: 0.5483
4288/9333 [============>.................] - ETA: 13:10 - loss: 0.6893 - acc: 0.5483
4352/9333 [============>.................] - ETA: 13:01 - loss: 0.6895 - acc: 0.5480
4416/9333 [=============>................] - ETA: 12:51 - loss: 0.6891 - acc: 0.5491
4480/9333 [=============>................] - ETA: 12:41 - loss: 0.6889 - acc: 0.5493
4544/9333 [=============>................] - ETA: 12:31 - loss: 0.6888 - acc: 0.5497
4608/9333 [=============>................] - ETA: 12:23 - loss: 0.6888 - acc: 0.5486
4672/9333 [==============>...............] - ETA: 12:13 - loss: 0.6887 - acc: 0.5486
4736/9333 [==============>...............] - ETA: 12:02 - loss: 0.6888 - acc: 0.5477
4800/9333 [==============>...............] - ETA: 11:52 - loss: 0.6888 - acc: 0.5475
4864/9333 [==============>...............] - ETA: 11:42 - loss: 0.6889 - acc: 0.5473
4928/9333 [==============>...............] - ETA: 11:31 - loss: 0.6891 - acc: 0.5467
4992/9333 [===============>..............] - ETA: 11:21 - loss: 0.6888 - acc: 0.5481
5056/9333 [===============>..............] - ETA: 11:11 - loss: 0.6887 - acc: 0.5485
5120/9333 [===============>..............] - ETA: 11:00 - loss: 0.6886 - acc: 0.5488
5184/9333 [===============>..............] - ETA: 10:51 - loss: 0.6887 - acc: 0.5484
5248/9333 [===============>..............] - ETA: 10:41 - loss: 0.6887 - acc: 0.5490
5312/9333 [================>.............] - ETA: 10:30 - loss: 0.6888 - acc: 0.5486
5376/9333 [================>.............] - ETA: 10:20 - loss: 0.6886 - acc: 0.5491
5440/9333 [================>.............] - ETA: 10:10 - loss: 0.6887 - acc: 0.5489
5504/9333 [================>.............] - ETA: 10:00 - loss: 0.6890 - acc: 0.5483
5568/9333 [================>.............] - ETA: 9:49 - loss: 0.6894 - acc: 0.5472 
5632/9333 [=================>............] - ETA: 9:39 - loss: 0.6891 - acc: 0.5485
5696/9333 [=================>............] - ETA: 9:29 - loss: 0.6890 - acc: 0.5492
5760/9333 [=================>............] - ETA: 9:19 - loss: 0.6888 - acc: 0.5497
5824/9333 [=================>............] - ETA: 9:09 - loss: 0.6891 - acc: 0.5479
5888/9333 [=================>............] - ETA: 8:59 - loss: 0.6893 - acc: 0.5467
5952/9333 [==================>...........] - ETA: 8:49 - loss: 0.6892 - acc: 0.5469
6016/9333 [==================>...........] - ETA: 8:39 - loss: 0.6892 - acc: 0.5480
6080/9333 [==================>...........] - ETA: 8:29 - loss: 0.6889 - acc: 0.5490
6144/9333 [==================>...........] - ETA: 8:19 - loss: 0.6890 - acc: 0.5485
6208/9333 [==================>...........] - ETA: 8:09 - loss: 0.6891 - acc: 0.5478
6272/9333 [===================>..........] - ETA: 7:59 - loss: 0.6891 - acc: 0.5478
6336/9333 [===================>..........] - ETA: 7:49 - loss: 0.6891 - acc: 0.5472
6400/9333 [===================>..........] - ETA: 7:38 - loss: 0.6888 - acc: 0.5483
6464/9333 [===================>..........] - ETA: 7:29 - loss: 0.6885 - acc: 0.5497
6528/9333 [===================>..........] - ETA: 7:19 - loss: 0.6883 - acc: 0.5506
6592/9333 [====================>.........] - ETA: 7:09 - loss: 0.6882 - acc: 0.5511
6656/9333 [====================>.........] - ETA: 6:59 - loss: 0.6880 - acc: 0.5518
6720/9333 [====================>.........] - ETA: 6:48 - loss: 0.6877 - acc: 0.5527
6784/9333 [====================>.........] - ETA: 6:39 - loss: 0.6877 - acc: 0.5525
6848/9333 [=====================>........] - ETA: 6:29 - loss: 0.6878 - acc: 0.5526
6912/9333 [=====================>........] - ETA: 6:19 - loss: 0.6880 - acc: 0.5515
6976/9333 [=====================>........] - ETA: 6:09 - loss: 0.6878 - acc: 0.5517
7040/9333 [=====================>........] - ETA: 5:59 - loss: 0.6879 - acc: 0.5517
7104/9333 [=====================>........] - ETA: 5:49 - loss: 0.6879 - acc: 0.5515
7168/9333 [======================>.......] - ETA: 5:39 - loss: 0.6878 - acc: 0.5519
7232/9333 [======================>.......] - ETA: 5:29 - loss: 0.6880 - acc: 0.5519
7296/9333 [======================>.......] - ETA: 5:19 - loss: 0.6880 - acc: 0.5524
7360/9333 [======================>.......] - ETA: 5:09 - loss: 0.6879 - acc: 0.5531
7424/9333 [======================>.......] - ETA: 4:59 - loss: 0.6877 - acc: 0.5537
7488/9333 [=======================>......] - ETA: 4:49 - loss: 0.6878 - acc: 0.5529
7552/9333 [=======================>......] - ETA: 4:39 - loss: 0.6878 - acc: 0.5532
7616/9333 [=======================>......] - ETA: 4:29 - loss: 0.6878 - acc: 0.5533
7680/9333 [=======================>......] - ETA: 4:19 - loss: 0.6877 - acc: 0.5535
7744/9333 [=======================>......] - ETA: 4:09 - loss: 0.6878 - acc: 0.5535
7808/9333 [========================>.....] - ETA: 3:58 - loss: 0.6877 - acc: 0.5535
7872/9333 [========================>.....] - ETA: 3:48 - loss: 0.6877 - acc: 0.5536
7936/9333 [========================>.....] - ETA: 3:38 - loss: 0.6877 - acc: 0.5538
8000/9333 [========================>.....] - ETA: 3:28 - loss: 0.6878 - acc: 0.5534
8064/9333 [========================>.....] - ETA: 3:18 - loss: 0.6877 - acc: 0.5528
8128/9333 [=========================>....] - ETA: 3:08 - loss: 0.6877 - acc: 0.5533
8192/9333 [=========================>....] - ETA: 2:58 - loss: 0.6877 - acc: 0.5533
8256/9333 [=========================>....] - ETA: 2:48 - loss: 0.6877 - acc: 0.5531
8320/9333 [=========================>....] - ETA: 2:38 - loss: 0.6879 - acc: 0.5525
8384/9333 [=========================>....] - ETA: 2:28 - loss: 0.6882 - acc: 0.5516
8448/9333 [==========================>...] - ETA: 2:18 - loss: 0.6881 - acc: 0.5520
8512/9333 [==========================>...] - ETA: 2:08 - loss: 0.6882 - acc: 0.5516
8576/9333 [==========================>...] - ETA: 1:58 - loss: 0.6882 - acc: 0.5512
8640/9333 [==========================>...] - ETA: 1:48 - loss: 0.6883 - acc: 0.5505
8704/9333 [==========================>...] - ETA: 1:38 - loss: 0.6884 - acc: 0.5501
8768/9333 [===========================>..] - ETA: 1:28 - loss: 0.6885 - acc: 0.5498
8832/9333 [===========================>..] - ETA: 1:18 - loss: 0.6885 - acc: 0.5498
8896/9333 [===========================>..] - ETA: 1:08 - loss: 0.6887 - acc: 0.5497
8960/9333 [===========================>..] - ETA: 58s - loss: 0.6888 - acc: 0.5491 
9024/9333 [============================>.] - ETA: 48s - loss: 0.6888 - acc: 0.5493
9088/9333 [============================>.] - ETA: 38s - loss: 0.6888 - acc: 0.5500
9152/9333 [============================>.] - ETA: 28s - loss: 0.6886 - acc: 0.5503
9216/9333 [============================>.] - ETA: 18s - loss: 0.6885 - acc: 0.5508
9280/9333 [============================>.] - ETA: 8s - loss: 0.6884 - acc: 0.5510 
9333/9333 [==============================] - 1514s 162ms/step - loss: 0.6884 - acc: 0.5506 - val_loss: 0.6895 - val_acc: 0.5381

Epoch 00009: val_acc did not improve from 0.54291
Epoch 10/10

  64/9333 [..............................] - ETA: 23:08 - loss: 0.6754 - acc: 0.5781
 128/9333 [..............................] - ETA: 23:11 - loss: 0.6773 - acc: 0.5625
 192/9333 [..............................] - ETA: 23:15 - loss: 0.6789 - acc: 0.5469
 256/9333 [..............................] - ETA: 23:33 - loss: 0.6816 - acc: 0.5430
 320/9333 [>.............................] - ETA: 23:22 - loss: 0.6789 - acc: 0.5719
 384/9333 [>.............................] - ETA: 23:24 - loss: 0.6780 - acc: 0.5729
 448/9333 [>.............................] - ETA: 23:22 - loss: 0.6755 - acc: 0.5848
 512/9333 [>.............................] - ETA: 23:17 - loss: 0.6775 - acc: 0.5742
 576/9333 [>.............................] - ETA: 23:00 - loss: 0.6768 - acc: 0.5764
 640/9333 [=>............................] - ETA: 22:45 - loss: 0.6783 - acc: 0.5766
 704/9333 [=>............................] - ETA: 22:28 - loss: 0.6797 - acc: 0.5724
 768/9333 [=>............................] - ETA: 22:18 - loss: 0.6793 - acc: 0.5729
 832/9333 [=>............................] - ETA: 22:07 - loss: 0.6777 - acc: 0.5769
 896/9333 [=>............................] - ETA: 22:02 - loss: 0.6773 - acc: 0.5781
 960/9333 [==>...........................] - ETA: 21:51 - loss: 0.6777 - acc: 0.5750
1024/9333 [==>...........................] - ETA: 21:46 - loss: 0.6774 - acc: 0.5762
1088/9333 [==>...........................] - ETA: 21:36 - loss: 0.6779 - acc: 0.5744
1152/9333 [==>...........................] - ETA: 21:27 - loss: 0.6799 - acc: 0.5634
1216/9333 [==>...........................] - ETA: 21:21 - loss: 0.6800 - acc: 0.5641
1280/9333 [===>..........................] - ETA: 21:19 - loss: 0.6792 - acc: 0.5633
1344/9333 [===>..........................] - ETA: 21:10 - loss: 0.6805 - acc: 0.5618
1408/9333 [===>..........................] - ETA: 21:00 - loss: 0.6818 - acc: 0.5597
1472/9333 [===>..........................] - ETA: 20:54 - loss: 0.6821 - acc: 0.5557
1536/9333 [===>..........................] - ETA: 20:47 - loss: 0.6824 - acc: 0.5540
1600/9333 [====>.........................] - ETA: 20:34 - loss: 0.6816 - acc: 0.5550
1664/9333 [====>.........................] - ETA: 20:25 - loss: 0.6824 - acc: 0.5517
1728/9333 [====>.........................] - ETA: 20:14 - loss: 0.6833 - acc: 0.5509
1792/9333 [====>.........................] - ETA: 20:05 - loss: 0.6836 - acc: 0.5513
1856/9333 [====>.........................] - ETA: 19:53 - loss: 0.6829 - acc: 0.5539
1920/9333 [=====>........................] - ETA: 19:44 - loss: 0.6822 - acc: 0.5568
1984/9333 [=====>........................] - ETA: 19:33 - loss: 0.6821 - acc: 0.5575
2048/9333 [=====>........................] - ETA: 19:19 - loss: 0.6826 - acc: 0.5552
2112/9333 [=====>........................] - ETA: 19:09 - loss: 0.6817 - acc: 0.5592
2176/9333 [=====>........................] - ETA: 19:00 - loss: 0.6814 - acc: 0.5607
2240/9333 [======>.......................] - ETA: 18:46 - loss: 0.6817 - acc: 0.5598
2304/9333 [======>.......................] - ETA: 18:36 - loss: 0.6812 - acc: 0.5625
2368/9333 [======>.......................] - ETA: 18:24 - loss: 0.6818 - acc: 0.5617
2432/9333 [======>.......................] - ETA: 18:14 - loss: 0.6818 - acc: 0.5629
2496/9333 [=======>......................] - ETA: 18:05 - loss: 0.6814 - acc: 0.5637
2560/9333 [=======>......................] - ETA: 17:55 - loss: 0.6822 - acc: 0.5605
2624/9333 [=======>......................] - ETA: 17:45 - loss: 0.6822 - acc: 0.5598
2688/9333 [=======>......................] - ETA: 17:34 - loss: 0.6822 - acc: 0.5595
2752/9333 [=======>......................] - ETA: 17:25 - loss: 0.6822 - acc: 0.5603
2816/9333 [========>.....................] - ETA: 17:14 - loss: 0.6819 - acc: 0.5607
2880/9333 [========>.....................] - ETA: 17:04 - loss: 0.6826 - acc: 0.5597
2944/9333 [========>.....................] - ETA: 16:54 - loss: 0.6832 - acc: 0.5591
3008/9333 [========>.....................] - ETA: 16:43 - loss: 0.6833 - acc: 0.5605
3072/9333 [========>.....................] - ETA: 16:34 - loss: 0.6833 - acc: 0.5602
3136/9333 [=========>....................] - ETA: 16:24 - loss: 0.6841 - acc: 0.5590
3200/9333 [=========>....................] - ETA: 16:14 - loss: 0.6849 - acc: 0.5578
3264/9333 [=========>....................] - ETA: 16:05 - loss: 0.6848 - acc: 0.5579
3328/9333 [=========>....................] - ETA: 15:54 - loss: 0.6851 - acc: 0.5586
3392/9333 [=========>....................] - ETA: 15:43 - loss: 0.6848 - acc: 0.5593
3456/9333 [==========>...................] - ETA: 15:32 - loss: 0.6847 - acc: 0.5593
3520/9333 [==========>...................] - ETA: 15:21 - loss: 0.6844 - acc: 0.5597
3584/9333 [==========>...................] - ETA: 15:11 - loss: 0.6841 - acc: 0.5614
3648/9333 [==========>...................] - ETA: 15:01 - loss: 0.6848 - acc: 0.5595
3712/9333 [==========>...................] - ETA: 14:51 - loss: 0.6845 - acc: 0.5606
3776/9333 [===========>..................] - ETA: 14:41 - loss: 0.6849 - acc: 0.5596
3840/9333 [===========>..................] - ETA: 14:30 - loss: 0.6852 - acc: 0.5583
3904/9333 [===========>..................] - ETA: 14:20 - loss: 0.6846 - acc: 0.5594
3968/9333 [===========>..................] - ETA: 14:09 - loss: 0.6849 - acc: 0.5590
4032/9333 [===========>..................] - ETA: 13:58 - loss: 0.6850 - acc: 0.5588
4096/9333 [============>.................] - ETA: 13:48 - loss: 0.6848 - acc: 0.5601
4160/9333 [============>.................] - ETA: 13:38 - loss: 0.6854 - acc: 0.5589
4224/9333 [============>.................] - ETA: 13:28 - loss: 0.6853 - acc: 0.5587
4288/9333 [============>.................] - ETA: 13:17 - loss: 0.6856 - acc: 0.5571
4352/9333 [============>.................] - ETA: 13:07 - loss: 0.6858 - acc: 0.5565
4416/9333 [=============>................] - ETA: 12:57 - loss: 0.6861 - acc: 0.5550
4480/9333 [=============>................] - ETA: 12:47 - loss: 0.6866 - acc: 0.5529
4544/9333 [=============>................] - ETA: 12:37 - loss: 0.6866 - acc: 0.5528
4608/9333 [=============>................] - ETA: 12:27 - loss: 0.6867 - acc: 0.5521
4672/9333 [==============>...............] - ETA: 12:17 - loss: 0.6864 - acc: 0.5531
4736/9333 [==============>...............] - ETA: 12:07 - loss: 0.6867 - acc: 0.5519
4800/9333 [==============>...............] - ETA: 11:57 - loss: 0.6868 - acc: 0.5519
4864/9333 [==============>...............] - ETA: 11:47 - loss: 0.6867 - acc: 0.5528
4928/9333 [==============>...............] - ETA: 11:36 - loss: 0.6868 - acc: 0.5519
4992/9333 [===============>..............] - ETA: 11:26 - loss: 0.6867 - acc: 0.5509
5056/9333 [===============>..............] - ETA: 11:16 - loss: 0.6867 - acc: 0.5508
5120/9333 [===============>..............] - ETA: 11:06 - loss: 0.6865 - acc: 0.5518
5184/9333 [===============>..............] - ETA: 10:56 - loss: 0.6867 - acc: 0.5511
5248/9333 [===============>..............] - ETA: 10:45 - loss: 0.6867 - acc: 0.5516
5312/9333 [================>.............] - ETA: 10:35 - loss: 0.6867 - acc: 0.5516
5376/9333 [================>.............] - ETA: 10:24 - loss: 0.6865 - acc: 0.5526
5440/9333 [================>.............] - ETA: 10:14 - loss: 0.6863 - acc: 0.5531
5504/9333 [================>.............] - ETA: 10:03 - loss: 0.6866 - acc: 0.5525
5568/9333 [================>.............] - ETA: 9:53 - loss: 0.6865 - acc: 0.5528 
5632/9333 [=================>............] - ETA: 9:42 - loss: 0.6866 - acc: 0.5522
5696/9333 [=================>............] - ETA: 9:32 - loss: 0.6867 - acc: 0.5516
5760/9333 [=================>............] - ETA: 9:21 - loss: 0.6868 - acc: 0.5517
5824/9333 [=================>............] - ETA: 9:11 - loss: 0.6868 - acc: 0.5519
5888/9333 [=================>............] - ETA: 9:01 - loss: 0.6866 - acc: 0.5523
5952/9333 [==================>...........] - ETA: 8:50 - loss: 0.6864 - acc: 0.5528
6016/9333 [==================>...........] - ETA: 8:40 - loss: 0.6864 - acc: 0.5524
6080/9333 [==================>...........] - ETA: 8:29 - loss: 0.6862 - acc: 0.5525
6144/9333 [==================>...........] - ETA: 8:19 - loss: 0.6864 - acc: 0.5524
6208/9333 [==================>...........] - ETA: 8:09 - loss: 0.6865 - acc: 0.5519
6272/9333 [===================>..........] - ETA: 7:59 - loss: 0.6867 - acc: 0.5521
6336/9333 [===================>..........] - ETA: 7:49 - loss: 0.6866 - acc: 0.5521
6400/9333 [===================>..........] - ETA: 7:39 - loss: 0.6866 - acc: 0.5525
6464/9333 [===================>..........] - ETA: 7:28 - loss: 0.6864 - acc: 0.5532
6528/9333 [===================>..........] - ETA: 7:18 - loss: 0.6865 - acc: 0.5525
6592/9333 [====================>.........] - ETA: 7:09 - loss: 0.6866 - acc: 0.5520
6656/9333 [====================>.........] - ETA: 6:58 - loss: 0.6865 - acc: 0.5523
6720/9333 [====================>.........] - ETA: 6:48 - loss: 0.6867 - acc: 0.5515
6784/9333 [====================>.........] - ETA: 6:39 - loss: 0.6866 - acc: 0.5519
6848/9333 [=====================>........] - ETA: 6:28 - loss: 0.6866 - acc: 0.5515
6912/9333 [=====================>........] - ETA: 6:19 - loss: 0.6864 - acc: 0.5527
6976/9333 [=====================>........] - ETA: 6:09 - loss: 0.6865 - acc: 0.5522
7040/9333 [=====================>........] - ETA: 5:59 - loss: 0.6865 - acc: 0.5517
7104/9333 [=====================>........] - ETA: 5:49 - loss: 0.6862 - acc: 0.5525
7168/9333 [======================>.......] - ETA: 5:39 - loss: 0.6863 - acc: 0.5519
7232/9333 [======================>.......] - ETA: 5:29 - loss: 0.6864 - acc: 0.5521
7296/9333 [======================>.......] - ETA: 5:19 - loss: 0.6864 - acc: 0.5522
7360/9333 [======================>.......] - ETA: 5:09 - loss: 0.6864 - acc: 0.5522
7424/9333 [======================>.......] - ETA: 4:59 - loss: 0.6865 - acc: 0.5512
7488/9333 [=======================>......] - ETA: 4:49 - loss: 0.6866 - acc: 0.5510
7552/9333 [=======================>......] - ETA: 4:39 - loss: 0.6865 - acc: 0.5514
7616/9333 [=======================>......] - ETA: 4:29 - loss: 0.6864 - acc: 0.5509
7680/9333 [=======================>......] - ETA: 4:19 - loss: 0.6865 - acc: 0.5507
7744/9333 [=======================>......] - ETA: 4:09 - loss: 0.6866 - acc: 0.5510
7808/9333 [========================>.....] - ETA: 3:59 - loss: 0.6865 - acc: 0.5511
7872/9333 [========================>.....] - ETA: 3:49 - loss: 0.6866 - acc: 0.5504
7936/9333 [========================>.....] - ETA: 3:39 - loss: 0.6866 - acc: 0.5507
8000/9333 [========================>.....] - ETA: 3:29 - loss: 0.6865 - acc: 0.5506
8064/9333 [========================>.....] - ETA: 3:19 - loss: 0.6865 - acc: 0.5505
8128/9333 [=========================>....] - ETA: 3:09 - loss: 0.6864 - acc: 0.5509
8192/9333 [=========================>....] - ETA: 2:59 - loss: 0.6866 - acc: 0.5499
8256/9333 [=========================>....] - ETA: 2:49 - loss: 0.6865 - acc: 0.5506
8320/9333 [=========================>....] - ETA: 2:39 - loss: 0.6865 - acc: 0.5506
8384/9333 [=========================>....] - ETA: 2:29 - loss: 0.6866 - acc: 0.5505
8448/9333 [==========================>...] - ETA: 2:19 - loss: 0.6866 - acc: 0.5500
8512/9333 [==========================>...] - ETA: 2:09 - loss: 0.6866 - acc: 0.5496
8576/9333 [==========================>...] - ETA: 1:59 - loss: 0.6866 - acc: 0.5493
8640/9333 [==========================>...] - ETA: 1:49 - loss: 0.6866 - acc: 0.5494
8704/9333 [==========================>...] - ETA: 1:39 - loss: 0.6867 - acc: 0.5491
8768/9333 [===========================>..] - ETA: 1:29 - loss: 0.6867 - acc: 0.5486
8832/9333 [===========================>..] - ETA: 1:19 - loss: 0.6866 - acc: 0.5490
8896/9333 [===========================>..] - ETA: 1:08 - loss: 0.6868 - acc: 0.5483
8960/9333 [===========================>..] - ETA: 58s - loss: 0.6871 - acc: 0.5469 
9024/9333 [============================>.] - ETA: 48s - loss: 0.6870 - acc: 0.5469
9088/9333 [============================>.] - ETA: 38s - loss: 0.6871 - acc: 0.5468
9152/9333 [============================>.] - ETA: 28s - loss: 0.6872 - acc: 0.5460
9216/9333 [============================>.] - ETA: 18s - loss: 0.6872 - acc: 0.5465
9280/9333 [============================>.] - ETA: 8s - loss: 0.6871 - acc: 0.5467 
9333/9333 [==============================] - 1527s 164ms/step - loss: 0.6871 - acc: 0.5470 - val_loss: 0.6903 - val_acc: 0.5564

Epoch 00010: val_acc improved from 0.54291 to 0.55641, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window18/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
样本个数 1296
样本个数 2592
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fc73e96e190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fc73e96e190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fc73e7da190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fc73e7da190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73e4bb510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73e4bb510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a8154b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a8154b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73e0b2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73e0b2a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7437ff710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc7437ff710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2a8154210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc2a8154210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74165aad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc74165aad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a829eed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc2a829eed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc040360850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc040360850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc0085be810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc0085be810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73f541b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73f541b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc040376750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc040376750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73dd39c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73dd39c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73e067a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73e067a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73dcd2bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73dcd2bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73dd39ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73dd39ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73e060f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73e060f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73da05cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73da05cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73d91dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73d91dfd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73dc47a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73dc47a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73dda2490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73dda2490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73da5e690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73da5e690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73d6f75d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73d6f75d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73d6ac8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73d6ac8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73d728c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73d728c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73e0e20d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73e0e20d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73d7c3d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73d7c3d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73d424390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73d424390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73d5c8e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73d5c8e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73d19b190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73d19b190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73d6cd910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73d6cd910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73d36de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73d36de10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73d0adcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73d0adcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73d013910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73d013910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc25879ef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc25879ef90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73d3e6110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73d3e6110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73d0c7c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73d0c7c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73cf89e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73cf89e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73cd0ed50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73cd0ed50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73cf3fd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73cf3fd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73cf89d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73cf89d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73cd8f210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73cd8f210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73ca6dd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73ca6dd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73c90f790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73c90f790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73cd8ca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73cd8ca90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73cf894d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73cf894d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c9ec210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c9ec210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73ca5c0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73ca5c0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73c5b7150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73c5b7150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c7a4f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c7a4f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73c8051d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73c8051d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c705f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c705f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73c5f8fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73c5f8fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73c769f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73c769f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c2cb2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c2cb2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73c5f8750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73c5f8750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c2d8590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c2d8590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73c0f6c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fc73c0f6c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73c042f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fc73c042f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c335750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c335750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73c479fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fc73c479fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c0d7250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fc73c0d7250>>: AttributeError: module 'gast' has no attribute 'Str'
window_select17-21.py:152: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/2592 [..............................] - ETA: 6:00
 128/2592 [>.............................] - ETA: 3:56
 192/2592 [=>............................] - ETA: 3:11
 256/2592 [=>............................] - ETA: 2:43
 320/2592 [==>...........................] - ETA: 2:32
 384/2592 [===>..........................] - ETA: 2:20
 448/2592 [====>.........................] - ETA: 2:10
 512/2592 [====>.........................] - ETA: 2:01
 576/2592 [=====>........................] - ETA: 1:54
 640/2592 [======>.......................] - ETA: 1:48
 704/2592 [=======>......................] - ETA: 1:43
 768/2592 [=======>......................] - ETA: 1:39
 832/2592 [========>.....................] - ETA: 1:34
 896/2592 [=========>....................] - ETA: 1:30
 960/2592 [==========>...................] - ETA: 1:26
1024/2592 [==========>...................] - ETA: 1:22
1088/2592 [===========>..................] - ETA: 1:17
1152/2592 [============>.................] - ETA: 1:13
1216/2592 [=============>................] - ETA: 1:10
1280/2592 [=============>................] - ETA: 1:06
1344/2592 [==============>...............] - ETA: 1:02
1408/2592 [===============>..............] - ETA: 59s 
1472/2592 [================>.............] - ETA: 55s
1536/2592 [================>.............] - ETA: 52s
1600/2592 [=================>............] - ETA: 49s
1664/2592 [==================>...........] - ETA: 45s
1728/2592 [===================>..........] - ETA: 42s
1792/2592 [===================>..........] - ETA: 39s
1856/2592 [====================>.........] - ETA: 35s
1920/2592 [=====================>........] - ETA: 32s
1984/2592 [=====================>........] - ETA: 29s
2048/2592 [======================>.......] - ETA: 26s
2112/2592 [=======================>......] - ETA: 23s
2176/2592 [========================>.....] - ETA: 19s
2240/2592 [========================>.....] - ETA: 16s
2304/2592 [=========================>....] - ETA: 13s
2368/2592 [==========================>...] - ETA: 10s
2432/2592 [===========================>..] - ETA: 7s 
2496/2592 [===========================>..] - ETA: 4s
2560/2592 [============================>.] - ETA: 1s
2592/2592 [==============================] - 124s 48ms/step
loss: 0.6880177715678274
acc: 0.5466820987654321
nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 5185
样本个数 10370
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f80f7b72d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f80f7b72d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f81cc0d8d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f81cc0d8d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81d49cc250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81d49cc250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cbc1eb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cbc1eb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81cbdf1a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81cbdf1a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cbd8a350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cbd8a350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81cbc1e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81cbc1e8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cbdfc7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cbdfc7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cbc1a0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cbc1a0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81cbe0f110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81cbe0f110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cbd32610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cbd32610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f7a06f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f7a06f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f78dc510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f78dc510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f7686f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f7686f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f76c3dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f76c3dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f7986510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f7986510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81cbd78790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81cbd78790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f76946d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f76946d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cbd78b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cbd78b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f731efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f731efd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f74b70d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f74b70d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f769ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f769ded0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f761cad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f761cad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f78f5650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f78f5650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f6fbd410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f6fbd410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f702b910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f702b910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f715ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f715ac50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f6f4cb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f6f4cb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f6d60a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f6d60a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f6d79750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f6d79750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f6cf8cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f6cf8cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f6d60810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f6d60810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f6cec250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f6cec250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f6d6e690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f6d6e690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f6938a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f6938a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f68335d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f68335d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f6d93950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f6d93950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f6ab9190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f6ab9190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f675a790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f675a790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f6736810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f6736810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f6933510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f6933510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f64d2d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f64d2d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f647ced0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f647ced0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f66b6750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f66b6750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80e63249d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80e63249d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f63e36d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f63e36d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80e6319e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80e6319e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f63bf410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f63bf410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f63bca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80f63bca90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81cbd0c510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81cbd0c510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f732c350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f732c350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80e62edd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80e62edd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80e5ef3b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80e5ef3b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80e5d99c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80e5d99c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80e5e2cbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80e5e2cbd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80e60583d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80e60583d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f66b6e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80f66b6e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80e5ca74d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80e5ca74d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80ddaf4350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80ddaf4350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80dda1b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80dda1b1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80dd988390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80dd988390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80ddaf4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80ddaf4fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80dd7e3e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80dd7e3e10>>: AttributeError: module 'gast' has no attribute 'Str'
2022-12-02 19:37:01.709249: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-12-02 19:37:02.083179: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-12-02 19:37:02.212443: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5564103be130 executing computations on platform Host. Devices:
2022-12-02 19:37:02.212912: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-12-02 19:37:03.282739: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window_select17-21.py:152: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 9333 samples, validate on 1037 samples
Epoch 1/10

  64/9333 [..............................] - ETA: 1:15:20 - loss: 0.6985 - acc: 0.5469
 128/9333 [..............................] - ETA: 54:02 - loss: 0.7305 - acc: 0.5391  
 192/9333 [..............................] - ETA: 44:05 - loss: 0.7924 - acc: 0.5104
 256/9333 [..............................] - ETA: 39:05 - loss: 0.7858 - acc: 0.5078
 320/9333 [>.............................] - ETA: 36:13 - loss: 0.7878 - acc: 0.5094
 384/9333 [>.............................] - ETA: 35:13 - loss: 0.7798 - acc: 0.5130
 448/9333 [>.............................] - ETA: 34:08 - loss: 0.7689 - acc: 0.5223
 512/9333 [>.............................] - ETA: 32:34 - loss: 0.7695 - acc: 0.5176
 576/9333 [>.............................] - ETA: 31:27 - loss: 0.7602 - acc: 0.5295
 640/9333 [=>............................] - ETA: 31:36 - loss: 0.7592 - acc: 0.5281
 704/9333 [=>............................] - ETA: 30:45 - loss: 0.7503 - acc: 0.5270
 768/9333 [=>............................] - ETA: 29:52 - loss: 0.7464 - acc: 0.5326
 832/9333 [=>............................] - ETA: 30:24 - loss: 0.7445 - acc: 0.5325
 896/9333 [=>............................] - ETA: 29:42 - loss: 0.7451 - acc: 0.5335
 960/9333 [==>...........................] - ETA: 29:02 - loss: 0.7416 - acc: 0.5344
1024/9333 [==>...........................] - ETA: 28:22 - loss: 0.7398 - acc: 0.5391
1088/9333 [==>...........................] - ETA: 27:46 - loss: 0.7400 - acc: 0.5358
1152/9333 [==>...........................] - ETA: 27:08 - loss: 0.7342 - acc: 0.5443
1216/9333 [==>...........................] - ETA: 26:35 - loss: 0.7361 - acc: 0.5395
1280/9333 [===>..........................] - ETA: 26:00 - loss: 0.7341 - acc: 0.5398
1344/9333 [===>..........................] - ETA: 25:28 - loss: 0.7317 - acc: 0.5379
1408/9333 [===>..........................] - ETA: 25:04 - loss: 0.7297 - acc: 0.5384
1472/9333 [===>..........................] - ETA: 24:42 - loss: 0.7278 - acc: 0.5387
1536/9333 [===>..........................] - ETA: 24:21 - loss: 0.7274 - acc: 0.5378
1600/9333 [====>.........................] - ETA: 23:58 - loss: 0.7260 - acc: 0.5387
1664/9333 [====>.........................] - ETA: 23:38 - loss: 0.7253 - acc: 0.5379
1728/9333 [====>.........................] - ETA: 23:17 - loss: 0.7227 - acc: 0.5394
1792/9333 [====>.........................] - ETA: 22:53 - loss: 0.7228 - acc: 0.5385
1856/9333 [====>.........................] - ETA: 22:31 - loss: 0.7233 - acc: 0.5356
1920/9333 [=====>........................] - ETA: 22:12 - loss: 0.7252 - acc: 0.5328
1984/9333 [=====>........................] - ETA: 21:50 - loss: 0.7247 - acc: 0.5333
2048/9333 [=====>........................] - ETA: 21:30 - loss: 0.7237 - acc: 0.5342
2112/9333 [=====>........................] - ETA: 21:12 - loss: 0.7222 - acc: 0.5341
2176/9333 [=====>........................] - ETA: 20:52 - loss: 0.7214 - acc: 0.5349
2240/9333 [======>.......................] - ETA: 20:34 - loss: 0.7224 - acc: 0.5330
2304/9333 [======>.......................] - ETA: 20:17 - loss: 0.7226 - acc: 0.5339
2368/9333 [======>.......................] - ETA: 19:59 - loss: 0.7232 - acc: 0.5325
2432/9333 [======>.......................] - ETA: 19:41 - loss: 0.7227 - acc: 0.5325
2496/9333 [=======>......................] - ETA: 19:24 - loss: 0.7215 - acc: 0.5333
2560/9333 [=======>......................] - ETA: 19:08 - loss: 0.7213 - acc: 0.5316
2624/9333 [=======>......................] - ETA: 18:51 - loss: 0.7213 - acc: 0.5312
2688/9333 [=======>......................] - ETA: 18:36 - loss: 0.7212 - acc: 0.5312
2752/9333 [=======>......................] - ETA: 18:21 - loss: 0.7205 - acc: 0.5345
2816/9333 [========>.....................] - ETA: 18:06 - loss: 0.7214 - acc: 0.5320
2880/9333 [========>.....................] - ETA: 17:51 - loss: 0.7212 - acc: 0.5326
2944/9333 [========>.....................] - ETA: 17:35 - loss: 0.7219 - acc: 0.5316
3008/9333 [========>.....................] - ETA: 17:23 - loss: 0.7219 - acc: 0.5309
3072/9333 [========>.....................] - ETA: 17:10 - loss: 0.7222 - acc: 0.5299
3136/9333 [=========>....................] - ETA: 16:56 - loss: 0.7219 - acc: 0.5306
3200/9333 [=========>....................] - ETA: 16:42 - loss: 0.7217 - acc: 0.5303
3264/9333 [=========>....................] - ETA: 16:27 - loss: 0.7216 - acc: 0.5297
3328/9333 [=========>....................] - ETA: 16:14 - loss: 0.7214 - acc: 0.5297
3392/9333 [=========>....................] - ETA: 15:59 - loss: 0.7213 - acc: 0.5289
3456/9333 [==========>...................] - ETA: 15:47 - loss: 0.7212 - acc: 0.5269
3520/9333 [==========>...................] - ETA: 15:34 - loss: 0.7211 - acc: 0.5259
3584/9333 [==========>...................] - ETA: 15:21 - loss: 0.7207 - acc: 0.5262
3648/9333 [==========>...................] - ETA: 15:07 - loss: 0.7206 - acc: 0.5260
3712/9333 [==========>...................] - ETA: 14:54 - loss: 0.7207 - acc: 0.5251
3776/9333 [===========>..................] - ETA: 14:42 - loss: 0.7209 - acc: 0.5244
3840/9333 [===========>..................] - ETA: 14:30 - loss: 0.7203 - acc: 0.5255
3904/9333 [===========>..................] - ETA: 14:17 - loss: 0.7204 - acc: 0.5248
3968/9333 [===========>..................] - ETA: 14:05 - loss: 0.7207 - acc: 0.5234
4032/9333 [===========>..................] - ETA: 13:53 - loss: 0.7203 - acc: 0.5226
4096/9333 [============>.................] - ETA: 13:41 - loss: 0.7200 - acc: 0.5222
4160/9333 [============>.................] - ETA: 13:29 - loss: 0.7205 - acc: 0.5214
4224/9333 [============>.................] - ETA: 13:17 - loss: 0.7200 - acc: 0.5213
4288/9333 [============>.................] - ETA: 13:05 - loss: 0.7197 - acc: 0.5215
4352/9333 [============>.................] - ETA: 12:53 - loss: 0.7204 - acc: 0.5191
4416/9333 [=============>................] - ETA: 12:42 - loss: 0.7201 - acc: 0.5195
4480/9333 [=============>................] - ETA: 12:30 - loss: 0.7199 - acc: 0.5188
4544/9333 [=============>................] - ETA: 12:19 - loss: 0.7194 - acc: 0.5200
4608/9333 [=============>................] - ETA: 12:08 - loss: 0.7188 - acc: 0.5208
4672/9333 [==============>...............] - ETA: 11:56 - loss: 0.7186 - acc: 0.5205
4736/9333 [==============>...............] - ETA: 11:45 - loss: 0.7181 - acc: 0.5203
4800/9333 [==============>...............] - ETA: 11:34 - loss: 0.7174 - acc: 0.5208
4864/9333 [==============>...............] - ETA: 11:23 - loss: 0.7173 - acc: 0.5208
4928/9333 [==============>...............] - ETA: 11:12 - loss: 0.7169 - acc: 0.5203
4992/9333 [===============>..............] - ETA: 11:01 - loss: 0.7162 - acc: 0.5218
5056/9333 [===============>..............] - ETA: 10:50 - loss: 0.7157 - acc: 0.5231
5120/9333 [===============>..............] - ETA: 10:40 - loss: 0.7153 - acc: 0.5234
5184/9333 [===============>..............] - ETA: 10:29 - loss: 0.7155 - acc: 0.5224
5248/9333 [===============>..............] - ETA: 10:19 - loss: 0.7157 - acc: 0.5208
5312/9333 [================>.............] - ETA: 10:08 - loss: 0.7155 - acc: 0.5207
5376/9333 [================>.............] - ETA: 9:58 - loss: 0.7151 - acc: 0.5197 
5440/9333 [================>.............] - ETA: 9:47 - loss: 0.7150 - acc: 0.5193
5504/9333 [================>.............] - ETA: 9:37 - loss: 0.7151 - acc: 0.5185
5568/9333 [================>.............] - ETA: 9:27 - loss: 0.7150 - acc: 0.5181
5632/9333 [=================>............] - ETA: 9:16 - loss: 0.7149 - acc: 0.5181
5696/9333 [=================>............] - ETA: 9:06 - loss: 0.7147 - acc: 0.5181
5760/9333 [=================>............] - ETA: 8:56 - loss: 0.7147 - acc: 0.5179
5824/9333 [=================>............] - ETA: 8:46 - loss: 0.7145 - acc: 0.5187
5888/9333 [=================>............] - ETA: 8:36 - loss: 0.7143 - acc: 0.5183
5952/9333 [==================>...........] - ETA: 8:26 - loss: 0.7142 - acc: 0.5186
6016/9333 [==================>...........] - ETA: 8:16 - loss: 0.7144 - acc: 0.5166
6080/9333 [==================>...........] - ETA: 8:06 - loss: 0.7146 - acc: 0.5155
6144/9333 [==================>...........] - ETA: 7:56 - loss: 0.7143 - acc: 0.5160
6208/9333 [==================>...........] - ETA: 7:46 - loss: 0.7143 - acc: 0.5156
6272/9333 [===================>..........] - ETA: 7:36 - loss: 0.7144 - acc: 0.5147
6336/9333 [===================>..........] - ETA: 7:26 - loss: 0.7137 - acc: 0.5158
6400/9333 [===================>..........] - ETA: 7:16 - loss: 0.7139 - acc: 0.5148
6464/9333 [===================>..........] - ETA: 7:06 - loss: 0.7138 - acc: 0.5155
6528/9333 [===================>..........] - ETA: 6:56 - loss: 0.7134 - acc: 0.5162
6592/9333 [====================>.........] - ETA: 6:46 - loss: 0.7132 - acc: 0.5159
6656/9333 [====================>.........] - ETA: 6:38 - loss: 0.7127 - acc: 0.5165
6720/9333 [====================>.........] - ETA: 6:28 - loss: 0.7132 - acc: 0.5159
6784/9333 [====================>.........] - ETA: 6:18 - loss: 0.7128 - acc: 0.5167
6848/9333 [=====================>........] - ETA: 6:08 - loss: 0.7128 - acc: 0.5166
6912/9333 [=====================>........] - ETA: 5:59 - loss: 0.7126 - acc: 0.5163
6976/9333 [=====================>........] - ETA: 5:49 - loss: 0.7126 - acc: 0.5162
7040/9333 [=====================>........] - ETA: 5:40 - loss: 0.7124 - acc: 0.5166
7104/9333 [=====================>........] - ETA: 5:30 - loss: 0.7126 - acc: 0.5155
7168/9333 [======================>.......] - ETA: 5:20 - loss: 0.7125 - acc: 0.5158
7232/9333 [======================>.......] - ETA: 5:11 - loss: 0.7125 - acc: 0.5151
7296/9333 [======================>.......] - ETA: 5:01 - loss: 0.7119 - acc: 0.5158
7360/9333 [======================>.......] - ETA: 4:51 - loss: 0.7119 - acc: 0.5151
7424/9333 [======================>.......] - ETA: 4:42 - loss: 0.7116 - acc: 0.5156
7488/9333 [=======================>......] - ETA: 4:32 - loss: 0.7114 - acc: 0.5159
7552/9333 [=======================>......] - ETA: 4:22 - loss: 0.7115 - acc: 0.5151
7616/9333 [=======================>......] - ETA: 4:13 - loss: 0.7114 - acc: 0.5151
7680/9333 [=======================>......] - ETA: 4:03 - loss: 0.7113 - acc: 0.5145
7744/9333 [=======================>......] - ETA: 3:54 - loss: 0.7112 - acc: 0.5143
7808/9333 [========================>.....] - ETA: 3:44 - loss: 0.7110 - acc: 0.5150
7872/9333 [========================>.....] - ETA: 3:35 - loss: 0.7108 - acc: 0.5155
7936/9333 [========================>.....] - ETA: 3:25 - loss: 0.7108 - acc: 0.5152
8000/9333 [========================>.....] - ETA: 3:16 - loss: 0.7109 - acc: 0.5149
8064/9333 [========================>.....] - ETA: 3:06 - loss: 0.7111 - acc: 0.5140
8128/9333 [=========================>....] - ETA: 2:57 - loss: 0.7109 - acc: 0.5144
8192/9333 [=========================>....] - ETA: 2:47 - loss: 0.7106 - acc: 0.5151
8256/9333 [=========================>....] - ETA: 2:37 - loss: 0.7105 - acc: 0.5151
8320/9333 [=========================>....] - ETA: 2:28 - loss: 0.7104 - acc: 0.5150
8384/9333 [=========================>....] - ETA: 2:19 - loss: 0.7102 - acc: 0.5155
8448/9333 [==========================>...] - ETA: 2:09 - loss: 0.7099 - acc: 0.5162
8512/9333 [==========================>...] - ETA: 2:00 - loss: 0.7100 - acc: 0.5164
8576/9333 [==========================>...] - ETA: 1:50 - loss: 0.7097 - acc: 0.5164
8640/9333 [==========================>...] - ETA: 1:41 - loss: 0.7097 - acc: 0.5162
8704/9333 [==========================>...] - ETA: 1:31 - loss: 0.7101 - acc: 0.5154
8768/9333 [===========================>..] - ETA: 1:22 - loss: 0.7102 - acc: 0.5146
8832/9333 [===========================>..] - ETA: 1:13 - loss: 0.7101 - acc: 0.5143
8896/9333 [===========================>..] - ETA: 1:03 - loss: 0.7102 - acc: 0.5139
8960/9333 [===========================>..] - ETA: 54s - loss: 0.7101 - acc: 0.5144 
9024/9333 [============================>.] - ETA: 45s - loss: 0.7100 - acc: 0.5147
9088/9333 [============================>.] - ETA: 35s - loss: 0.7100 - acc: 0.5147
9152/9333 [============================>.] - ETA: 26s - loss: 0.7099 - acc: 0.5148
9216/9333 [============================>.] - ETA: 17s - loss: 0.7099 - acc: 0.5145
9280/9333 [============================>.] - ETA: 7s - loss: 0.7098 - acc: 0.5144 
9333/9333 [==============================] - 1399s 150ms/step - loss: 0.7096 - acc: 0.5149 - val_loss: 0.6907 - val_acc: 0.5371

Epoch 00001: val_acc improved from -inf to 0.53713, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window19/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 2/10

  64/9333 [..............................] - ETA: 20:50 - loss: 0.7183 - acc: 0.4375
 128/9333 [..............................] - ETA: 19:53 - loss: 0.7052 - acc: 0.5078
 192/9333 [..............................] - ETA: 19:55 - loss: 0.7012 - acc: 0.5156
 256/9333 [..............................] - ETA: 19:37 - loss: 0.6990 - acc: 0.5195
 320/9333 [>.............................] - ETA: 19:41 - loss: 0.6981 - acc: 0.5188
 384/9333 [>.............................] - ETA: 19:47 - loss: 0.6958 - acc: 0.5234
 448/9333 [>.............................] - ETA: 19:25 - loss: 0.6949 - acc: 0.5312
 512/9333 [>.............................] - ETA: 19:10 - loss: 0.6974 - acc: 0.5215
 576/9333 [>.............................] - ETA: 19:05 - loss: 0.6996 - acc: 0.5208
 640/9333 [=>............................] - ETA: 18:58 - loss: 0.7000 - acc: 0.5156
 704/9333 [=>............................] - ETA: 18:37 - loss: 0.7001 - acc: 0.5128
 768/9333 [=>............................] - ETA: 18:31 - loss: 0.6997 - acc: 0.5182
 832/9333 [=>............................] - ETA: 18:23 - loss: 0.6985 - acc: 0.5228
 896/9333 [=>............................] - ETA: 18:13 - loss: 0.6998 - acc: 0.5201
 960/9333 [==>...........................] - ETA: 18:07 - loss: 0.6994 - acc: 0.5188
1024/9333 [==>...........................] - ETA: 17:56 - loss: 0.6985 - acc: 0.5195
1088/9333 [==>...........................] - ETA: 17:44 - loss: 0.6969 - acc: 0.5239
1152/9333 [==>...........................] - ETA: 17:32 - loss: 0.6971 - acc: 0.5200
1216/9333 [==>...........................] - ETA: 17:25 - loss: 0.6962 - acc: 0.5222
1280/9333 [===>..........................] - ETA: 17:11 - loss: 0.6967 - acc: 0.5195
1344/9333 [===>..........................] - ETA: 17:04 - loss: 0.6970 - acc: 0.5179
1408/9333 [===>..........................] - ETA: 16:55 - loss: 0.6962 - acc: 0.5199
1472/9333 [===>..........................] - ETA: 16:45 - loss: 0.6956 - acc: 0.5211
1536/9333 [===>..........................] - ETA: 16:39 - loss: 0.6939 - acc: 0.5267
1600/9333 [====>.........................] - ETA: 16:31 - loss: 0.6918 - acc: 0.5312
1664/9333 [====>.........................] - ETA: 16:24 - loss: 0.6939 - acc: 0.5258
1728/9333 [====>.........................] - ETA: 16:16 - loss: 0.6946 - acc: 0.5249
1792/9333 [====>.........................] - ETA: 16:06 - loss: 0.6953 - acc: 0.5234
1856/9333 [====>.........................] - ETA: 15:59 - loss: 0.6963 - acc: 0.5237
1920/9333 [=====>........................] - ETA: 15:51 - loss: 0.6960 - acc: 0.5255
1984/9333 [=====>........................] - ETA: 15:43 - loss: 0.6962 - acc: 0.5232
2048/9333 [=====>........................] - ETA: 15:36 - loss: 0.6957 - acc: 0.5254
2112/9333 [=====>........................] - ETA: 15:27 - loss: 0.6941 - acc: 0.5279
2176/9333 [=====>........................] - ETA: 15:16 - loss: 0.6929 - acc: 0.5290
2240/9333 [======>.......................] - ETA: 15:08 - loss: 0.6938 - acc: 0.5254
2304/9333 [======>.......................] - ETA: 15:00 - loss: 0.6944 - acc: 0.5239
2368/9333 [======>.......................] - ETA: 14:53 - loss: 0.6948 - acc: 0.5224
2432/9333 [======>.......................] - ETA: 14:45 - loss: 0.6950 - acc: 0.5218
2496/9333 [=======>......................] - ETA: 14:37 - loss: 0.6947 - acc: 0.5224
2560/9333 [=======>......................] - ETA: 14:30 - loss: 0.6945 - acc: 0.5223
2624/9333 [=======>......................] - ETA: 14:22 - loss: 0.6948 - acc: 0.5206
2688/9333 [=======>......................] - ETA: 14:13 - loss: 0.6949 - acc: 0.5190
2752/9333 [=======>......................] - ETA: 14:05 - loss: 0.6942 - acc: 0.5214
2816/9333 [========>.....................] - ETA: 13:57 - loss: 0.6943 - acc: 0.5202
2880/9333 [========>.....................] - ETA: 13:49 - loss: 0.6942 - acc: 0.5208
2944/9333 [========>.....................] - ETA: 13:41 - loss: 0.6938 - acc: 0.5217
3008/9333 [========>.....................] - ETA: 13:33 - loss: 0.6940 - acc: 0.5199
3072/9333 [========>.....................] - ETA: 13:25 - loss: 0.6940 - acc: 0.5212
3136/9333 [=========>....................] - ETA: 13:16 - loss: 0.6942 - acc: 0.5207
3200/9333 [=========>....................] - ETA: 13:08 - loss: 0.6941 - acc: 0.5216
3264/9333 [=========>....................] - ETA: 13:00 - loss: 0.6938 - acc: 0.5214
3328/9333 [=========>....................] - ETA: 12:51 - loss: 0.6938 - acc: 0.5207
3392/9333 [=========>....................] - ETA: 12:43 - loss: 0.6936 - acc: 0.5218
3456/9333 [==========>...................] - ETA: 12:34 - loss: 0.6937 - acc: 0.5220
3520/9333 [==========>...................] - ETA: 12:26 - loss: 0.6938 - acc: 0.5219
3584/9333 [==========>...................] - ETA: 12:19 - loss: 0.6939 - acc: 0.5201
3648/9333 [==========>...................] - ETA: 12:11 - loss: 0.6939 - acc: 0.5211
3712/9333 [==========>...................] - ETA: 12:04 - loss: 0.6940 - acc: 0.5207
3776/9333 [===========>..................] - ETA: 11:56 - loss: 0.6941 - acc: 0.5220
3840/9333 [===========>..................] - ETA: 11:49 - loss: 0.6939 - acc: 0.5232
3904/9333 [===========>..................] - ETA: 11:42 - loss: 0.6942 - acc: 0.5236
3968/9333 [===========>..................] - ETA: 11:34 - loss: 0.6936 - acc: 0.5265
4032/9333 [===========>..................] - ETA: 11:26 - loss: 0.6940 - acc: 0.5260
4096/9333 [============>.................] - ETA: 11:18 - loss: 0.6949 - acc: 0.5234
4160/9333 [============>.................] - ETA: 11:10 - loss: 0.6948 - acc: 0.5236
4224/9333 [============>.................] - ETA: 11:03 - loss: 0.6947 - acc: 0.5239
4288/9333 [============>.................] - ETA: 10:54 - loss: 0.6947 - acc: 0.5238
4352/9333 [============>.................] - ETA: 10:45 - loss: 0.6946 - acc: 0.5248
4416/9333 [=============>................] - ETA: 10:38 - loss: 0.6943 - acc: 0.5254
4480/9333 [=============>................] - ETA: 10:30 - loss: 0.6937 - acc: 0.5270
4544/9333 [=============>................] - ETA: 10:22 - loss: 0.6933 - acc: 0.5275
4608/9333 [=============>................] - ETA: 10:15 - loss: 0.6939 - acc: 0.5265
4672/9333 [==============>...............] - ETA: 10:06 - loss: 0.6940 - acc: 0.5259
4736/9333 [==============>...............] - ETA: 9:59 - loss: 0.6936 - acc: 0.5264 
4800/9333 [==============>...............] - ETA: 9:51 - loss: 0.6941 - acc: 0.5250
4864/9333 [==============>...............] - ETA: 9:43 - loss: 0.6938 - acc: 0.5257
4928/9333 [==============>...............] - ETA: 9:36 - loss: 0.6937 - acc: 0.5268
4992/9333 [===============>..............] - ETA: 9:28 - loss: 0.6941 - acc: 0.5258
5056/9333 [===============>..............] - ETA: 9:19 - loss: 0.6942 - acc: 0.5257
5120/9333 [===============>..............] - ETA: 9:11 - loss: 0.6940 - acc: 0.5258
5184/9333 [===============>..............] - ETA: 9:02 - loss: 0.6942 - acc: 0.5257
5248/9333 [===============>..............] - ETA: 8:54 - loss: 0.6942 - acc: 0.5259
5312/9333 [================>.............] - ETA: 8:46 - loss: 0.6943 - acc: 0.5260
5376/9333 [================>.............] - ETA: 8:38 - loss: 0.6943 - acc: 0.5268
5440/9333 [================>.............] - ETA: 8:30 - loss: 0.6943 - acc: 0.5265
5504/9333 [================>.............] - ETA: 8:22 - loss: 0.6945 - acc: 0.5262
5568/9333 [================>.............] - ETA: 8:14 - loss: 0.6945 - acc: 0.5262
5632/9333 [=================>............] - ETA: 8:06 - loss: 0.6943 - acc: 0.5263
5696/9333 [=================>............] - ETA: 7:58 - loss: 0.6941 - acc: 0.5279
5760/9333 [=================>............] - ETA: 7:49 - loss: 0.6940 - acc: 0.5280
5824/9333 [=================>............] - ETA: 7:41 - loss: 0.6940 - acc: 0.5282
5888/9333 [=================>............] - ETA: 7:33 - loss: 0.6936 - acc: 0.5290
5952/9333 [==================>...........] - ETA: 7:25 - loss: 0.6934 - acc: 0.5296
6016/9333 [==================>...........] - ETA: 7:17 - loss: 0.6936 - acc: 0.5298
6080/9333 [==================>...........] - ETA: 7:09 - loss: 0.6936 - acc: 0.5294
6144/9333 [==================>...........] - ETA: 7:01 - loss: 0.6937 - acc: 0.5291
6208/9333 [==================>...........] - ETA: 6:53 - loss: 0.6938 - acc: 0.5293
6272/9333 [===================>..........] - ETA: 6:44 - loss: 0.6941 - acc: 0.5281
6336/9333 [===================>..........] - ETA: 6:36 - loss: 0.6944 - acc: 0.5271
6400/9333 [===================>..........] - ETA: 6:28 - loss: 0.6946 - acc: 0.5267
6464/9333 [===================>..........] - ETA: 6:19 - loss: 0.6946 - acc: 0.5263
6528/9333 [===================>..........] - ETA: 6:11 - loss: 0.6946 - acc: 0.5260
6592/9333 [====================>.........] - ETA: 6:03 - loss: 0.6947 - acc: 0.5261
6656/9333 [====================>.........] - ETA: 5:54 - loss: 0.6947 - acc: 0.5261
6720/9333 [====================>.........] - ETA: 5:46 - loss: 0.6946 - acc: 0.5262
6784/9333 [====================>.........] - ETA: 5:37 - loss: 0.6947 - acc: 0.5256
6848/9333 [=====================>........] - ETA: 5:29 - loss: 0.6946 - acc: 0.5257
6912/9333 [=====================>........] - ETA: 5:21 - loss: 0.6947 - acc: 0.5252
6976/9333 [=====================>........] - ETA: 5:12 - loss: 0.6947 - acc: 0.5252
7040/9333 [=====================>........] - ETA: 5:04 - loss: 0.6945 - acc: 0.5254
7104/9333 [=====================>........] - ETA: 4:55 - loss: 0.6944 - acc: 0.5265
7168/9333 [======================>.......] - ETA: 4:47 - loss: 0.6944 - acc: 0.5261
7232/9333 [======================>.......] - ETA: 4:38 - loss: 0.6942 - acc: 0.5260
7296/9333 [======================>.......] - ETA: 4:30 - loss: 0.6943 - acc: 0.5259
7360/9333 [======================>.......] - ETA: 4:21 - loss: 0.6944 - acc: 0.5258
7424/9333 [======================>.......] - ETA: 4:13 - loss: 0.6944 - acc: 0.5256
7488/9333 [=======================>......] - ETA: 4:04 - loss: 0.6945 - acc: 0.5248
7552/9333 [=======================>......] - ETA: 3:56 - loss: 0.6946 - acc: 0.5241
7616/9333 [=======================>......] - ETA: 3:47 - loss: 0.6948 - acc: 0.5235
7680/9333 [=======================>......] - ETA: 3:39 - loss: 0.6949 - acc: 0.5225
7744/9333 [=======================>......] - ETA: 3:31 - loss: 0.6950 - acc: 0.5221
7808/9333 [========================>.....] - ETA: 3:22 - loss: 0.6950 - acc: 0.5224
7872/9333 [========================>.....] - ETA: 3:14 - loss: 0.6950 - acc: 0.5220
7936/9333 [========================>.....] - ETA: 3:05 - loss: 0.6949 - acc: 0.5219
8000/9333 [========================>.....] - ETA: 2:57 - loss: 0.6950 - acc: 0.5216
8064/9333 [========================>.....] - ETA: 2:48 - loss: 0.6950 - acc: 0.5216
8128/9333 [=========================>....] - ETA: 2:40 - loss: 0.6949 - acc: 0.5217
8192/9333 [=========================>....] - ETA: 2:31 - loss: 0.6948 - acc: 0.5217
8256/9333 [=========================>....] - ETA: 2:23 - loss: 0.6951 - acc: 0.5205
8320/9333 [=========================>....] - ETA: 2:14 - loss: 0.6951 - acc: 0.5206
8384/9333 [=========================>....] - ETA: 2:06 - loss: 0.6952 - acc: 0.5198
8448/9333 [==========================>...] - ETA: 1:57 - loss: 0.6952 - acc: 0.5199
8512/9333 [==========================>...] - ETA: 1:49 - loss: 0.6952 - acc: 0.5197
8576/9333 [==========================>...] - ETA: 1:40 - loss: 0.6952 - acc: 0.5197
8640/9333 [==========================>...] - ETA: 1:32 - loss: 0.6951 - acc: 0.5203
8704/9333 [==========================>...] - ETA: 1:23 - loss: 0.6951 - acc: 0.5202
8768/9333 [===========================>..] - ETA: 1:15 - loss: 0.6951 - acc: 0.5203
8832/9333 [===========================>..] - ETA: 1:06 - loss: 0.6952 - acc: 0.5196
8896/9333 [===========================>..] - ETA: 58s - loss: 0.6952 - acc: 0.5196 
8960/9333 [===========================>..] - ETA: 49s - loss: 0.6953 - acc: 0.5192
9024/9333 [============================>.] - ETA: 41s - loss: 0.6952 - acc: 0.5195
9088/9333 [============================>.] - ETA: 32s - loss: 0.6950 - acc: 0.5202
9152/9333 [============================>.] - ETA: 24s - loss: 0.6950 - acc: 0.5201
9216/9333 [============================>.] - ETA: 15s - loss: 0.6950 - acc: 0.5199
9280/9333 [============================>.] - ETA: 7s - loss: 0.6951 - acc: 0.5197 
9333/9333 [==============================] - 1285s 138ms/step - loss: 0.6951 - acc: 0.5198 - val_loss: 0.6912 - val_acc: 0.5178

Epoch 00002: val_acc did not improve from 0.53713
Epoch 3/10

  64/9333 [..............................] - ETA: 21:52 - loss: 0.6893 - acc: 0.5156
 128/9333 [..............................] - ETA: 21:42 - loss: 0.6758 - acc: 0.5859
 192/9333 [..............................] - ETA: 21:14 - loss: 0.6820 - acc: 0.5573
 256/9333 [..............................] - ETA: 20:49 - loss: 0.6919 - acc: 0.5195
 320/9333 [>.............................] - ETA: 20:23 - loss: 0.6892 - acc: 0.5375
 384/9333 [>.............................] - ETA: 20:18 - loss: 0.6908 - acc: 0.5286
 448/9333 [>.............................] - ETA: 20:08 - loss: 0.6897 - acc: 0.5290
 512/9333 [>.............................] - ETA: 20:04 - loss: 0.6944 - acc: 0.5254
 576/9333 [>.............................] - ETA: 19:58 - loss: 0.6930 - acc: 0.5278
 640/9333 [=>............................] - ETA: 19:53 - loss: 0.6939 - acc: 0.5297
 704/9333 [=>............................] - ETA: 19:37 - loss: 0.6927 - acc: 0.5341
 768/9333 [=>............................] - ETA: 19:28 - loss: 0.6919 - acc: 0.5352
 832/9333 [=>............................] - ETA: 19:20 - loss: 0.6932 - acc: 0.5325
 896/9333 [=>............................] - ETA: 19:09 - loss: 0.6929 - acc: 0.5335
 960/9333 [==>...........................] - ETA: 19:05 - loss: 0.6924 - acc: 0.5323
1024/9333 [==>...........................] - ETA: 18:56 - loss: 0.6928 - acc: 0.5332
1088/9333 [==>...........................] - ETA: 18:43 - loss: 0.6924 - acc: 0.5368
1152/9333 [==>...........................] - ETA: 18:31 - loss: 0.6917 - acc: 0.5417
1216/9333 [==>...........................] - ETA: 18:17 - loss: 0.6917 - acc: 0.5411
1280/9333 [===>..........................] - ETA: 18:10 - loss: 0.6904 - acc: 0.5477
1344/9333 [===>..........................] - ETA: 18:04 - loss: 0.6912 - acc: 0.5461
1408/9333 [===>..........................] - ETA: 17:56 - loss: 0.6912 - acc: 0.5462
1472/9333 [===>..........................] - ETA: 17:48 - loss: 0.6915 - acc: 0.5455
1536/9333 [===>..........................] - ETA: 17:37 - loss: 0.6919 - acc: 0.5430
1600/9333 [====>.........................] - ETA: 17:29 - loss: 0.6917 - acc: 0.5413
1664/9333 [====>.........................] - ETA: 17:15 - loss: 0.6920 - acc: 0.5385
1728/9333 [====>.........................] - ETA: 17:07 - loss: 0.6924 - acc: 0.5370
1792/9333 [====>.........................] - ETA: 16:56 - loss: 0.6927 - acc: 0.5340
1856/9333 [====>.........................] - ETA: 16:48 - loss: 0.6926 - acc: 0.5318
1920/9333 [=====>........................] - ETA: 16:39 - loss: 0.6914 - acc: 0.5354
1984/9333 [=====>........................] - ETA: 16:30 - loss: 0.6917 - acc: 0.5348
2048/9333 [=====>........................] - ETA: 16:21 - loss: 0.6921 - acc: 0.5361
2112/9333 [=====>........................] - ETA: 16:11 - loss: 0.6916 - acc: 0.5393
2176/9333 [=====>........................] - ETA: 16:01 - loss: 0.6917 - acc: 0.5391
2240/9333 [======>.......................] - ETA: 15:51 - loss: 0.6914 - acc: 0.5415
2304/9333 [======>.......................] - ETA: 15:44 - loss: 0.6909 - acc: 0.5417
2368/9333 [======>.......................] - ETA: 15:34 - loss: 0.6909 - acc: 0.5410
2432/9333 [======>.......................] - ETA: 15:24 - loss: 0.6902 - acc: 0.5452
2496/9333 [=======>......................] - ETA: 15:14 - loss: 0.6904 - acc: 0.5437
2560/9333 [=======>......................] - ETA: 15:06 - loss: 0.6904 - acc: 0.5418
2624/9333 [=======>......................] - ETA: 14:56 - loss: 0.6903 - acc: 0.5412
2688/9333 [=======>......................] - ETA: 14:46 - loss: 0.6901 - acc: 0.5413
2752/9333 [=======>......................] - ETA: 14:37 - loss: 0.6903 - acc: 0.5396
2816/9333 [========>.....................] - ETA: 14:28 - loss: 0.6903 - acc: 0.5387
2880/9333 [========>.....................] - ETA: 14:20 - loss: 0.6906 - acc: 0.5382
2944/9333 [========>.....................] - ETA: 14:11 - loss: 0.6907 - acc: 0.5374
3008/9333 [========>.....................] - ETA: 14:02 - loss: 0.6907 - acc: 0.5369
3072/9333 [========>.....................] - ETA: 13:52 - loss: 0.6912 - acc: 0.5365
3136/9333 [=========>....................] - ETA: 13:41 - loss: 0.6920 - acc: 0.5338
3200/9333 [=========>....................] - ETA: 13:32 - loss: 0.6923 - acc: 0.5334
3264/9333 [=========>....................] - ETA: 13:24 - loss: 0.6924 - acc: 0.5334
3328/9333 [=========>....................] - ETA: 13:15 - loss: 0.6922 - acc: 0.5334
3392/9333 [=========>....................] - ETA: 13:06 - loss: 0.6917 - acc: 0.5342
3456/9333 [==========>...................] - ETA: 12:57 - loss: 0.6916 - acc: 0.5350
3520/9333 [==========>...................] - ETA: 12:47 - loss: 0.6918 - acc: 0.5349
3584/9333 [==========>...................] - ETA: 12:38 - loss: 0.6921 - acc: 0.5338
3648/9333 [==========>...................] - ETA: 12:29 - loss: 0.6920 - acc: 0.5343
3712/9333 [==========>...................] - ETA: 12:21 - loss: 0.6924 - acc: 0.5331
3776/9333 [===========>..................] - ETA: 12:12 - loss: 0.6929 - acc: 0.5323
3840/9333 [===========>..................] - ETA: 12:03 - loss: 0.6930 - acc: 0.5323
3904/9333 [===========>..................] - ETA: 11:54 - loss: 0.6932 - acc: 0.5310
3968/9333 [===========>..................] - ETA: 11:45 - loss: 0.6933 - acc: 0.5320
4032/9333 [===========>..................] - ETA: 11:36 - loss: 0.6932 - acc: 0.5315
4096/9333 [============>.................] - ETA: 11:28 - loss: 0.6930 - acc: 0.5312
4160/9333 [============>.................] - ETA: 11:20 - loss: 0.6932 - acc: 0.5308
4224/9333 [============>.................] - ETA: 11:11 - loss: 0.6934 - acc: 0.5298
4288/9333 [============>.................] - ETA: 11:04 - loss: 0.6937 - acc: 0.5287
4352/9333 [============>.................] - ETA: 10:56 - loss: 0.6939 - acc: 0.5285
4416/9333 [=============>................] - ETA: 10:47 - loss: 0.6942 - acc: 0.5267
4480/9333 [=============>................] - ETA: 10:38 - loss: 0.6941 - acc: 0.5268
4544/9333 [=============>................] - ETA: 10:30 - loss: 0.6942 - acc: 0.5260
4608/9333 [=============>................] - ETA: 10:22 - loss: 0.6939 - acc: 0.5271
4672/9333 [==============>...............] - ETA: 10:13 - loss: 0.6938 - acc: 0.5270
4736/9333 [==============>...............] - ETA: 10:04 - loss: 0.6939 - acc: 0.5274
4800/9333 [==============>...............] - ETA: 9:55 - loss: 0.6938 - acc: 0.5275 
4864/9333 [==============>...............] - ETA: 9:47 - loss: 0.6935 - acc: 0.5298
4928/9333 [==============>...............] - ETA: 9:39 - loss: 0.6935 - acc: 0.5300
4992/9333 [===============>..............] - ETA: 9:30 - loss: 0.6935 - acc: 0.5290
5056/9333 [===============>..............] - ETA: 9:21 - loss: 0.6934 - acc: 0.5285
5120/9333 [===============>..............] - ETA: 9:12 - loss: 0.6937 - acc: 0.5273
5184/9333 [===============>..............] - ETA: 9:04 - loss: 0.6936 - acc: 0.5270
5248/9333 [===============>..............] - ETA: 8:55 - loss: 0.6935 - acc: 0.5269
5312/9333 [================>.............] - ETA: 8:46 - loss: 0.6934 - acc: 0.5277
5376/9333 [================>.............] - ETA: 8:37 - loss: 0.6936 - acc: 0.5270
5440/9333 [================>.............] - ETA: 8:29 - loss: 0.6933 - acc: 0.5278
5504/9333 [================>.............] - ETA: 8:20 - loss: 0.6934 - acc: 0.5267
5568/9333 [================>.............] - ETA: 8:12 - loss: 0.6934 - acc: 0.5275
5632/9333 [=================>............] - ETA: 8:03 - loss: 0.6934 - acc: 0.5277
5696/9333 [=================>............] - ETA: 7:55 - loss: 0.6936 - acc: 0.5272
5760/9333 [=================>............] - ETA: 7:46 - loss: 0.6933 - acc: 0.5281
5824/9333 [=================>............] - ETA: 7:38 - loss: 0.6930 - acc: 0.5285
5888/9333 [=================>............] - ETA: 7:29 - loss: 0.6930 - acc: 0.5285
5952/9333 [==================>...........] - ETA: 7:21 - loss: 0.6932 - acc: 0.5277
6016/9333 [==================>...........] - ETA: 7:12 - loss: 0.6931 - acc: 0.5274
6080/9333 [==================>...........] - ETA: 7:04 - loss: 0.6932 - acc: 0.5275
6144/9333 [==================>...........] - ETA: 6:55 - loss: 0.6926 - acc: 0.5293
6208/9333 [==================>...........] - ETA: 6:46 - loss: 0.6928 - acc: 0.5293
6272/9333 [===================>..........] - ETA: 6:38 - loss: 0.6928 - acc: 0.5292
6336/9333 [===================>..........] - ETA: 6:29 - loss: 0.6928 - acc: 0.5289
6400/9333 [===================>..........] - ETA: 6:21 - loss: 0.6927 - acc: 0.5283
6464/9333 [===================>..........] - ETA: 6:13 - loss: 0.6932 - acc: 0.5271
6528/9333 [===================>..........] - ETA: 6:04 - loss: 0.6934 - acc: 0.5265
6592/9333 [====================>.........] - ETA: 5:56 - loss: 0.6933 - acc: 0.5270
6656/9333 [====================>.........] - ETA: 5:48 - loss: 0.6931 - acc: 0.5272
6720/9333 [====================>.........] - ETA: 5:40 - loss: 0.6931 - acc: 0.5271
6784/9333 [====================>.........] - ETA: 5:31 - loss: 0.6930 - acc: 0.5273
6848/9333 [=====================>........] - ETA: 5:23 - loss: 0.6931 - acc: 0.5272
6912/9333 [=====================>........] - ETA: 5:14 - loss: 0.6930 - acc: 0.5272
6976/9333 [=====================>........] - ETA: 5:06 - loss: 0.6930 - acc: 0.5274
7040/9333 [=====================>........] - ETA: 4:58 - loss: 0.6929 - acc: 0.5277
7104/9333 [=====================>........] - ETA: 4:49 - loss: 0.6930 - acc: 0.5280
7168/9333 [======================>.......] - ETA: 4:41 - loss: 0.6930 - acc: 0.5282
7232/9333 [======================>.......] - ETA: 4:33 - loss: 0.6931 - acc: 0.5279
7296/9333 [======================>.......] - ETA: 4:24 - loss: 0.6931 - acc: 0.5282
7360/9333 [======================>.......] - ETA: 4:16 - loss: 0.6933 - acc: 0.5281
7424/9333 [======================>.......] - ETA: 4:08 - loss: 0.6932 - acc: 0.5283
7488/9333 [=======================>......] - ETA: 4:00 - loss: 0.6932 - acc: 0.5280
7552/9333 [=======================>......] - ETA: 3:51 - loss: 0.6933 - acc: 0.5277
7616/9333 [=======================>......] - ETA: 3:43 - loss: 0.6935 - acc: 0.5265
7680/9333 [=======================>......] - ETA: 3:34 - loss: 0.6936 - acc: 0.5258
7744/9333 [=======================>......] - ETA: 3:26 - loss: 0.6937 - acc: 0.5254
7808/9333 [========================>.....] - ETA: 3:18 - loss: 0.6936 - acc: 0.5257
7872/9333 [========================>.....] - ETA: 3:09 - loss: 0.6936 - acc: 0.5259
7936/9333 [========================>.....] - ETA: 3:01 - loss: 0.6935 - acc: 0.5262
8000/9333 [========================>.....] - ETA: 2:53 - loss: 0.6934 - acc: 0.5268
8064/9333 [========================>.....] - ETA: 2:44 - loss: 0.6933 - acc: 0.5273
8128/9333 [=========================>....] - ETA: 2:36 - loss: 0.6932 - acc: 0.5274
8192/9333 [=========================>....] - ETA: 2:28 - loss: 0.6931 - acc: 0.5278
8256/9333 [=========================>....] - ETA: 2:19 - loss: 0.6933 - acc: 0.5269
8320/9333 [=========================>....] - ETA: 2:11 - loss: 0.6935 - acc: 0.5264
8384/9333 [=========================>....] - ETA: 2:03 - loss: 0.6935 - acc: 0.5261
8448/9333 [==========================>...] - ETA: 1:54 - loss: 0.6935 - acc: 0.5262
8512/9333 [==========================>...] - ETA: 1:46 - loss: 0.6935 - acc: 0.5264
8576/9333 [==========================>...] - ETA: 1:38 - loss: 0.6937 - acc: 0.5257
8640/9333 [==========================>...] - ETA: 1:29 - loss: 0.6938 - acc: 0.5253
8704/9333 [==========================>...] - ETA: 1:21 - loss: 0.6937 - acc: 0.5255
8768/9333 [===========================>..] - ETA: 1:13 - loss: 0.6938 - acc: 0.5251
8832/9333 [===========================>..] - ETA: 1:05 - loss: 0.6937 - acc: 0.5254
8896/9333 [===========================>..] - ETA: 56s - loss: 0.6939 - acc: 0.5246 
8960/9333 [===========================>..] - ETA: 48s - loss: 0.6938 - acc: 0.5253
9024/9333 [============================>.] - ETA: 40s - loss: 0.6937 - acc: 0.5256
9088/9333 [============================>.] - ETA: 31s - loss: 0.6935 - acc: 0.5267
9152/9333 [============================>.] - ETA: 23s - loss: 0.6935 - acc: 0.5267
9216/9333 [============================>.] - ETA: 15s - loss: 0.6934 - acc: 0.5270
9280/9333 [============================>.] - ETA: 6s - loss: 0.6933 - acc: 0.5275 
9333/9333 [==============================] - 1256s 135ms/step - loss: 0.6932 - acc: 0.5275 - val_loss: 0.6901 - val_acc: 0.5304

Epoch 00003: val_acc did not improve from 0.53713
Epoch 4/10

  64/9333 [..............................] - ETA: 20:54 - loss: 0.6910 - acc: 0.5312
 128/9333 [..............................] - ETA: 20:54 - loss: 0.6922 - acc: 0.5156
 192/9333 [..............................] - ETA: 20:29 - loss: 0.6872 - acc: 0.5260
 256/9333 [..............................] - ETA: 20:06 - loss: 0.6941 - acc: 0.5078
 320/9333 [>.............................] - ETA: 19:59 - loss: 0.6939 - acc: 0.5156
 384/9333 [>.............................] - ETA: 19:51 - loss: 0.6859 - acc: 0.5495
 448/9333 [>.............................] - ETA: 19:30 - loss: 0.6851 - acc: 0.5446
 512/9333 [>.............................] - ETA: 19:16 - loss: 0.6867 - acc: 0.5371
 576/9333 [>.............................] - ETA: 19:00 - loss: 0.6898 - acc: 0.5330
 640/9333 [=>............................] - ETA: 18:52 - loss: 0.6895 - acc: 0.5359
 704/9333 [=>............................] - ETA: 18:42 - loss: 0.6880 - acc: 0.5426
 768/9333 [=>............................] - ETA: 18:30 - loss: 0.6882 - acc: 0.5404
 832/9333 [=>............................] - ETA: 18:21 - loss: 0.6882 - acc: 0.5445
 896/9333 [=>............................] - ETA: 18:13 - loss: 0.6879 - acc: 0.5435
 960/9333 [==>...........................] - ETA: 18:14 - loss: 0.6884 - acc: 0.5406
1024/9333 [==>...........................] - ETA: 18:07 - loss: 0.6883 - acc: 0.5430
1088/9333 [==>...........................] - ETA: 18:02 - loss: 0.6884 - acc: 0.5423
1152/9333 [==>...........................] - ETA: 17:55 - loss: 0.6887 - acc: 0.5373
1216/9333 [==>...........................] - ETA: 17:49 - loss: 0.6891 - acc: 0.5354
1280/9333 [===>..........................] - ETA: 17:38 - loss: 0.6895 - acc: 0.5328
1344/9333 [===>..........................] - ETA: 17:31 - loss: 0.6878 - acc: 0.5402
1408/9333 [===>..........................] - ETA: 17:20 - loss: 0.6885 - acc: 0.5362
1472/9333 [===>..........................] - ETA: 17:13 - loss: 0.6881 - acc: 0.5387
1536/9333 [===>..........................] - ETA: 17:04 - loss: 0.6877 - acc: 0.5391
1600/9333 [====>.........................] - ETA: 16:54 - loss: 0.6881 - acc: 0.5387
1664/9333 [====>.........................] - ETA: 16:46 - loss: 0.6888 - acc: 0.5355
1728/9333 [====>.........................] - ETA: 16:36 - loss: 0.6888 - acc: 0.5370
1792/9333 [====>.........................] - ETA: 16:28 - loss: 0.6896 - acc: 0.5374
1856/9333 [====>.........................] - ETA: 16:20 - loss: 0.6900 - acc: 0.5356
1920/9333 [=====>........................] - ETA: 16:12 - loss: 0.6911 - acc: 0.5328
1984/9333 [=====>........................] - ETA: 16:04 - loss: 0.6909 - acc: 0.5348
2048/9333 [=====>........................] - ETA: 15:56 - loss: 0.6914 - acc: 0.5347
2112/9333 [=====>........................] - ETA: 15:49 - loss: 0.6915 - acc: 0.5331
2176/9333 [=====>........................] - ETA: 15:39 - loss: 0.6915 - acc: 0.5331
2240/9333 [======>.......................] - ETA: 15:31 - loss: 0.6912 - acc: 0.5330
2304/9333 [======>.......................] - ETA: 15:21 - loss: 0.6911 - acc: 0.5343
2368/9333 [======>.......................] - ETA: 15:14 - loss: 0.6910 - acc: 0.5334
2432/9333 [======>.......................] - ETA: 15:05 - loss: 0.6910 - acc: 0.5329
2496/9333 [=======>......................] - ETA: 14:56 - loss: 0.6912 - acc: 0.5325
2560/9333 [=======>......................] - ETA: 14:47 - loss: 0.6917 - acc: 0.5273
2624/9333 [=======>......................] - ETA: 14:37 - loss: 0.6920 - acc: 0.5267
2688/9333 [=======>......................] - ETA: 14:28 - loss: 0.6922 - acc: 0.5264
2752/9333 [=======>......................] - ETA: 14:19 - loss: 0.6922 - acc: 0.5254
2816/9333 [========>.....................] - ETA: 14:11 - loss: 0.6918 - acc: 0.5266
2880/9333 [========>.....................] - ETA: 14:02 - loss: 0.6916 - acc: 0.5267
2944/9333 [========>.....................] - ETA: 13:55 - loss: 0.6915 - acc: 0.5282
3008/9333 [========>.....................] - ETA: 13:45 - loss: 0.6913 - acc: 0.5286
3072/9333 [========>.....................] - ETA: 13:37 - loss: 0.6912 - acc: 0.5290
3136/9333 [=========>....................] - ETA: 13:29 - loss: 0.6912 - acc: 0.5303
3200/9333 [=========>....................] - ETA: 13:20 - loss: 0.6910 - acc: 0.5309
3264/9333 [=========>....................] - ETA: 13:12 - loss: 0.6909 - acc: 0.5288
3328/9333 [=========>....................] - ETA: 13:04 - loss: 0.6912 - acc: 0.5276
3392/9333 [=========>....................] - ETA: 12:54 - loss: 0.6914 - acc: 0.5274
3456/9333 [==========>...................] - ETA: 12:46 - loss: 0.6916 - acc: 0.5258
3520/9333 [==========>...................] - ETA: 12:38 - loss: 0.6917 - acc: 0.5259
3584/9333 [==========>...................] - ETA: 12:29 - loss: 0.6916 - acc: 0.5259
3648/9333 [==========>...................] - ETA: 12:21 - loss: 0.6915 - acc: 0.5274
3712/9333 [==========>...................] - ETA: 12:12 - loss: 0.6913 - acc: 0.5280
3776/9333 [===========>..................] - ETA: 12:03 - loss: 0.6914 - acc: 0.5273
3840/9333 [===========>..................] - ETA: 11:54 - loss: 0.6915 - acc: 0.5268
3904/9333 [===========>..................] - ETA: 11:46 - loss: 0.6914 - acc: 0.5272
3968/9333 [===========>..................] - ETA: 11:38 - loss: 0.6913 - acc: 0.5285
4032/9333 [===========>..................] - ETA: 11:29 - loss: 0.6914 - acc: 0.5275
4096/9333 [============>.................] - ETA: 11:20 - loss: 0.6911 - acc: 0.5291
4160/9333 [============>.................] - ETA: 11:12 - loss: 0.6913 - acc: 0.5279
4224/9333 [============>.................] - ETA: 11:04 - loss: 0.6913 - acc: 0.5282
4288/9333 [============>.................] - ETA: 10:56 - loss: 0.6911 - acc: 0.5292
4352/9333 [============>.................] - ETA: 10:47 - loss: 0.6909 - acc: 0.5301
4416/9333 [=============>................] - ETA: 10:39 - loss: 0.6908 - acc: 0.5317
4480/9333 [=============>................] - ETA: 10:31 - loss: 0.6909 - acc: 0.5304
4544/9333 [=============>................] - ETA: 10:22 - loss: 0.6908 - acc: 0.5301
4608/9333 [=============>................] - ETA: 10:13 - loss: 0.6906 - acc: 0.5312
4672/9333 [==============>...............] - ETA: 10:04 - loss: 0.6903 - acc: 0.5321
4736/9333 [==============>...............] - ETA: 9:56 - loss: 0.6901 - acc: 0.5325 
4800/9333 [==============>...............] - ETA: 9:47 - loss: 0.6898 - acc: 0.5325
4864/9333 [==============>...............] - ETA: 9:38 - loss: 0.6895 - acc: 0.5331
4928/9333 [==============>...............] - ETA: 9:30 - loss: 0.6896 - acc: 0.5331
4992/9333 [===============>..............] - ETA: 9:21 - loss: 0.6895 - acc: 0.5333
5056/9333 [===============>..............] - ETA: 9:13 - loss: 0.6896 - acc: 0.5330
5120/9333 [===============>..............] - ETA: 9:04 - loss: 0.6896 - acc: 0.5334
5184/9333 [===============>..............] - ETA: 8:56 - loss: 0.6897 - acc: 0.5334
5248/9333 [===============>..............] - ETA: 8:48 - loss: 0.6897 - acc: 0.5328
5312/9333 [================>.............] - ETA: 8:39 - loss: 0.6898 - acc: 0.5318
5376/9333 [================>.............] - ETA: 8:31 - loss: 0.6902 - acc: 0.5307
5440/9333 [================>.............] - ETA: 8:23 - loss: 0.6902 - acc: 0.5301
5504/9333 [================>.............] - ETA: 8:14 - loss: 0.6903 - acc: 0.5300
5568/9333 [================>.............] - ETA: 8:06 - loss: 0.6903 - acc: 0.5302
5632/9333 [=================>............] - ETA: 7:58 - loss: 0.6901 - acc: 0.5305
5696/9333 [=================>............] - ETA: 7:49 - loss: 0.6902 - acc: 0.5318
5760/9333 [=================>............] - ETA: 7:41 - loss: 0.6901 - acc: 0.5325
5824/9333 [=================>............] - ETA: 7:33 - loss: 0.6901 - acc: 0.5323
5888/9333 [=================>............] - ETA: 7:24 - loss: 0.6902 - acc: 0.5319
5952/9333 [==================>...........] - ETA: 7:16 - loss: 0.6900 - acc: 0.5328
6016/9333 [==================>...........] - ETA: 7:08 - loss: 0.6901 - acc: 0.5316
6080/9333 [==================>...........] - ETA: 7:00 - loss: 0.6899 - acc: 0.5317
6144/9333 [==================>...........] - ETA: 6:51 - loss: 0.6900 - acc: 0.5322
6208/9333 [==================>...........] - ETA: 6:43 - loss: 0.6902 - acc: 0.5319
6272/9333 [===================>..........] - ETA: 6:35 - loss: 0.6903 - acc: 0.5316
6336/9333 [===================>..........] - ETA: 6:27 - loss: 0.6904 - acc: 0.5312
6400/9333 [===================>..........] - ETA: 6:18 - loss: 0.6906 - acc: 0.5322
6464/9333 [===================>..........] - ETA: 6:10 - loss: 0.6904 - acc: 0.5323
6528/9333 [===================>..........] - ETA: 6:02 - loss: 0.6903 - acc: 0.5335
6592/9333 [====================>.........] - ETA: 5:54 - loss: 0.6903 - acc: 0.5334
6656/9333 [====================>.........] - ETA: 5:45 - loss: 0.6902 - acc: 0.5337
6720/9333 [====================>.........] - ETA: 5:37 - loss: 0.6902 - acc: 0.5332
6784/9333 [====================>.........] - ETA: 5:29 - loss: 0.6901 - acc: 0.5338
6848/9333 [=====================>........] - ETA: 5:21 - loss: 0.6900 - acc: 0.5342
6912/9333 [=====================>........] - ETA: 5:13 - loss: 0.6901 - acc: 0.5337
6976/9333 [=====================>........] - ETA: 5:05 - loss: 0.6899 - acc: 0.5344
7040/9333 [=====================>........] - ETA: 4:57 - loss: 0.6900 - acc: 0.5339
7104/9333 [=====================>........] - ETA: 4:48 - loss: 0.6900 - acc: 0.5349
7168/9333 [======================>.......] - ETA: 4:40 - loss: 0.6898 - acc: 0.5350
7232/9333 [======================>.......] - ETA: 4:32 - loss: 0.6900 - acc: 0.5348
7296/9333 [======================>.......] - ETA: 4:23 - loss: 0.6902 - acc: 0.5345
7360/9333 [======================>.......] - ETA: 4:15 - loss: 0.6904 - acc: 0.5333
7424/9333 [======================>.......] - ETA: 4:07 - loss: 0.6904 - acc: 0.5334
7488/9333 [=======================>......] - ETA: 3:58 - loss: 0.6905 - acc: 0.5327
7552/9333 [=======================>......] - ETA: 3:50 - loss: 0.6904 - acc: 0.5334
7616/9333 [=======================>......] - ETA: 3:42 - loss: 0.6903 - acc: 0.5335
7680/9333 [=======================>......] - ETA: 3:33 - loss: 0.6905 - acc: 0.5328
7744/9333 [=======================>......] - ETA: 3:25 - loss: 0.6903 - acc: 0.5332
7808/9333 [========================>.....] - ETA: 3:17 - loss: 0.6904 - acc: 0.5330
7872/9333 [========================>.....] - ETA: 3:09 - loss: 0.6905 - acc: 0.5329
7936/9333 [========================>.....] - ETA: 3:00 - loss: 0.6904 - acc: 0.5328
8000/9333 [========================>.....] - ETA: 2:52 - loss: 0.6905 - acc: 0.5319
8064/9333 [========================>.....] - ETA: 2:44 - loss: 0.6906 - acc: 0.5312
8128/9333 [=========================>....] - ETA: 2:35 - loss: 0.6906 - acc: 0.5306
8192/9333 [=========================>....] - ETA: 2:27 - loss: 0.6907 - acc: 0.5303
8256/9333 [=========================>....] - ETA: 2:19 - loss: 0.6908 - acc: 0.5299
8320/9333 [=========================>....] - ETA: 2:11 - loss: 0.6906 - acc: 0.5306
8384/9333 [=========================>....] - ETA: 2:02 - loss: 0.6905 - acc: 0.5308
8448/9333 [==========================>...] - ETA: 1:54 - loss: 0.6904 - acc: 0.5305
8512/9333 [==========================>...] - ETA: 1:46 - loss: 0.6906 - acc: 0.5301
8576/9333 [==========================>...] - ETA: 1:37 - loss: 0.6907 - acc: 0.5300
8640/9333 [==========================>...] - ETA: 1:29 - loss: 0.6905 - acc: 0.5309
8704/9333 [==========================>...] - ETA: 1:21 - loss: 0.6904 - acc: 0.5312
8768/9333 [===========================>..] - ETA: 1:13 - loss: 0.6904 - acc: 0.5317
8832/9333 [===========================>..] - ETA: 1:04 - loss: 0.6905 - acc: 0.5315
8896/9333 [===========================>..] - ETA: 56s - loss: 0.6905 - acc: 0.5315 
8960/9333 [===========================>..] - ETA: 48s - loss: 0.6906 - acc: 0.5310
9024/9333 [============================>.] - ETA: 39s - loss: 0.6904 - acc: 0.5318
9088/9333 [============================>.] - ETA: 31s - loss: 0.6904 - acc: 0.5315
9152/9333 [============================>.] - ETA: 23s - loss: 0.6905 - acc: 0.5310
9216/9333 [============================>.] - ETA: 15s - loss: 0.6905 - acc: 0.5311
9280/9333 [============================>.] - ETA: 6s - loss: 0.6905 - acc: 0.5312 
9333/9333 [==============================] - 1248s 134ms/step - loss: 0.6905 - acc: 0.5313 - val_loss: 0.6870 - val_acc: 0.5381

Epoch 00004: val_acc improved from 0.53713 to 0.53809, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window19/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 5/10

  64/9333 [..............................] - ETA: 21:18 - loss: 0.6988 - acc: 0.5469
 128/9333 [..............................] - ETA: 20:39 - loss: 0.6912 - acc: 0.5625
 192/9333 [..............................] - ETA: 20:17 - loss: 0.6966 - acc: 0.5365
 256/9333 [..............................] - ETA: 20:12 - loss: 0.6950 - acc: 0.5195
 320/9333 [>.............................] - ETA: 19:54 - loss: 0.6961 - acc: 0.5156
 384/9333 [>.............................] - ETA: 19:45 - loss: 0.6975 - acc: 0.5208
 448/9333 [>.............................] - ETA: 19:42 - loss: 0.6991 - acc: 0.5112
 512/9333 [>.............................] - ETA: 19:38 - loss: 0.6961 - acc: 0.5195
 576/9333 [>.............................] - ETA: 19:22 - loss: 0.6963 - acc: 0.5104
 640/9333 [=>............................] - ETA: 19:18 - loss: 0.6970 - acc: 0.5000
 704/9333 [=>............................] - ETA: 19:09 - loss: 0.6960 - acc: 0.5043
 768/9333 [=>............................] - ETA: 18:59 - loss: 0.6948 - acc: 0.5091
 832/9333 [=>............................] - ETA: 18:52 - loss: 0.6950 - acc: 0.5060
 896/9333 [=>............................] - ETA: 18:42 - loss: 0.6942 - acc: 0.5112
 960/9333 [==>...........................] - ETA: 18:37 - loss: 0.6934 - acc: 0.5146
1024/9333 [==>...........................] - ETA: 18:25 - loss: 0.6911 - acc: 0.5254
1088/9333 [==>...........................] - ETA: 18:16 - loss: 0.6910 - acc: 0.5230
1152/9333 [==>...........................] - ETA: 18:06 - loss: 0.6912 - acc: 0.5269
1216/9333 [==>...........................] - ETA: 17:57 - loss: 0.6914 - acc: 0.5238
1280/9333 [===>..........................] - ETA: 17:51 - loss: 0.6915 - acc: 0.5234
1344/9333 [===>..........................] - ETA: 17:44 - loss: 0.6916 - acc: 0.5253
1408/9333 [===>..........................] - ETA: 17:33 - loss: 0.6913 - acc: 0.5256
1472/9333 [===>..........................] - ETA: 17:26 - loss: 0.6917 - acc: 0.5265
1536/9333 [===>..........................] - ETA: 17:18 - loss: 0.6930 - acc: 0.5221
1600/9333 [====>.........................] - ETA: 17:08 - loss: 0.6918 - acc: 0.5269
1664/9333 [====>.........................] - ETA: 16:58 - loss: 0.6916 - acc: 0.5258
1728/9333 [====>.........................] - ETA: 16:52 - loss: 0.6912 - acc: 0.5289
1792/9333 [====>.........................] - ETA: 16:44 - loss: 0.6904 - acc: 0.5312
1856/9333 [====>.........................] - ETA: 16:36 - loss: 0.6891 - acc: 0.5334
1920/9333 [=====>........................] - ETA: 16:27 - loss: 0.6885 - acc: 0.5359
1984/9333 [=====>........................] - ETA: 16:20 - loss: 0.6892 - acc: 0.5343
2048/9333 [=====>........................] - ETA: 16:14 - loss: 0.6894 - acc: 0.5332
2112/9333 [=====>........................] - ETA: 16:08 - loss: 0.6890 - acc: 0.5336
2176/9333 [=====>........................] - ETA: 15:59 - loss: 0.6901 - acc: 0.5303
2240/9333 [======>.......................] - ETA: 15:51 - loss: 0.6891 - acc: 0.5344
2304/9333 [======>.......................] - ETA: 15:46 - loss: 0.6893 - acc: 0.5339
2368/9333 [======>.......................] - ETA: 15:38 - loss: 0.6889 - acc: 0.5359
2432/9333 [======>.......................] - ETA: 15:30 - loss: 0.6890 - acc: 0.5362
2496/9333 [=======>......................] - ETA: 15:22 - loss: 0.6889 - acc: 0.5369
2560/9333 [=======>......................] - ETA: 15:14 - loss: 0.6894 - acc: 0.5359
2624/9333 [=======>......................] - ETA: 15:05 - loss: 0.6892 - acc: 0.5377
2688/9333 [=======>......................] - ETA: 14:57 - loss: 0.6886 - acc: 0.5391
2752/9333 [=======>......................] - ETA: 14:49 - loss: 0.6883 - acc: 0.5389
2816/9333 [========>.....................] - ETA: 14:42 - loss: 0.6883 - acc: 0.5380
2880/9333 [========>.....................] - ETA: 14:35 - loss: 0.6883 - acc: 0.5375
2944/9333 [========>.....................] - ETA: 14:27 - loss: 0.6879 - acc: 0.5387
3008/9333 [========>.....................] - ETA: 14:20 - loss: 0.6875 - acc: 0.5389
3072/9333 [========>.....................] - ETA: 14:12 - loss: 0.6874 - acc: 0.5387
3136/9333 [=========>....................] - ETA: 14:05 - loss: 0.6882 - acc: 0.5370
3200/9333 [=========>....................] - ETA: 13:58 - loss: 0.6882 - acc: 0.5366
3264/9333 [=========>....................] - ETA: 13:49 - loss: 0.6895 - acc: 0.5337
3328/9333 [=========>....................] - ETA: 13:40 - loss: 0.6892 - acc: 0.5343
3392/9333 [=========>....................] - ETA: 13:31 - loss: 0.6896 - acc: 0.5339
3456/9333 [==========>...................] - ETA: 13:23 - loss: 0.6894 - acc: 0.5344
3520/9333 [==========>...................] - ETA: 13:15 - loss: 0.6896 - acc: 0.5335
3584/9333 [==========>...................] - ETA: 13:06 - loss: 0.6900 - acc: 0.5332
3648/9333 [==========>...................] - ETA: 12:58 - loss: 0.6898 - acc: 0.5334
3712/9333 [==========>...................] - ETA: 12:50 - loss: 0.6900 - acc: 0.5323
3776/9333 [===========>..................] - ETA: 12:41 - loss: 0.6898 - acc: 0.5331
3840/9333 [===========>..................] - ETA: 12:33 - loss: 0.6898 - acc: 0.5336
3904/9333 [===========>..................] - ETA: 12:24 - loss: 0.6897 - acc: 0.5338
3968/9333 [===========>..................] - ETA: 12:15 - loss: 0.6897 - acc: 0.5340
4032/9333 [===========>..................] - ETA: 12:07 - loss: 0.6902 - acc: 0.5327
4096/9333 [============>.................] - ETA: 11:58 - loss: 0.6908 - acc: 0.5315
4160/9333 [============>.................] - ETA: 11:50 - loss: 0.6906 - acc: 0.5322
4224/9333 [============>.................] - ETA: 11:42 - loss: 0.6908 - acc: 0.5312
4288/9333 [============>.................] - ETA: 11:33 - loss: 0.6909 - acc: 0.5308
4352/9333 [============>.................] - ETA: 11:24 - loss: 0.6909 - acc: 0.5306
4416/9333 [=============>................] - ETA: 11:16 - loss: 0.6908 - acc: 0.5306
4480/9333 [=============>................] - ETA: 11:07 - loss: 0.6908 - acc: 0.5308
4544/9333 [=============>................] - ETA: 10:58 - loss: 0.6907 - acc: 0.5299
4608/9333 [=============>................] - ETA: 10:50 - loss: 0.6909 - acc: 0.5302
4672/9333 [==============>...............] - ETA: 10:41 - loss: 0.6909 - acc: 0.5298
4736/9333 [==============>...............] - ETA: 10:33 - loss: 0.6910 - acc: 0.5296
4800/9333 [==============>...............] - ETA: 10:25 - loss: 0.6907 - acc: 0.5315
4864/9333 [==============>...............] - ETA: 10:16 - loss: 0.6911 - acc: 0.5292
4928/9333 [==============>...............] - ETA: 10:08 - loss: 0.6913 - acc: 0.5280
4992/9333 [===============>..............] - ETA: 9:59 - loss: 0.6910 - acc: 0.5292 
5056/9333 [===============>..............] - ETA: 9:51 - loss: 0.6909 - acc: 0.5293
5120/9333 [===============>..............] - ETA: 9:42 - loss: 0.6909 - acc: 0.5287
5184/9333 [===============>..............] - ETA: 9:33 - loss: 0.6909 - acc: 0.5291
5248/9333 [===============>..............] - ETA: 9:24 - loss: 0.6905 - acc: 0.5307
5312/9333 [================>.............] - ETA: 9:16 - loss: 0.6906 - acc: 0.5299
5376/9333 [================>.............] - ETA: 9:07 - loss: 0.6904 - acc: 0.5307
5440/9333 [================>.............] - ETA: 8:58 - loss: 0.6904 - acc: 0.5318
5504/9333 [================>.............] - ETA: 8:49 - loss: 0.6905 - acc: 0.5311
5568/9333 [================>.............] - ETA: 8:40 - loss: 0.6904 - acc: 0.5307
5632/9333 [=================>............] - ETA: 8:32 - loss: 0.6904 - acc: 0.5304
5696/9333 [=================>............] - ETA: 8:23 - loss: 0.6906 - acc: 0.5297
5760/9333 [=================>............] - ETA: 8:14 - loss: 0.6907 - acc: 0.5295
5824/9333 [=================>............] - ETA: 8:06 - loss: 0.6903 - acc: 0.5306
5888/9333 [=================>............] - ETA: 7:57 - loss: 0.6904 - acc: 0.5311
5952/9333 [==================>...........] - ETA: 7:48 - loss: 0.6904 - acc: 0.5316
6016/9333 [==================>...........] - ETA: 7:40 - loss: 0.6905 - acc: 0.5319
6080/9333 [==================>...........] - ETA: 7:31 - loss: 0.6905 - acc: 0.5317
6144/9333 [==================>...........] - ETA: 7:22 - loss: 0.6905 - acc: 0.5316
6208/9333 [==================>...........] - ETA: 7:13 - loss: 0.6904 - acc: 0.5317
6272/9333 [===================>..........] - ETA: 7:04 - loss: 0.6904 - acc: 0.5309
6336/9333 [===================>..........] - ETA: 6:55 - loss: 0.6905 - acc: 0.5303
6400/9333 [===================>..........] - ETA: 6:46 - loss: 0.6906 - acc: 0.5297
6464/9333 [===================>..........] - ETA: 6:37 - loss: 0.6906 - acc: 0.5295
6528/9333 [===================>..........] - ETA: 6:28 - loss: 0.6905 - acc: 0.5296
6592/9333 [====================>.........] - ETA: 6:19 - loss: 0.6905 - acc: 0.5293
6656/9333 [====================>.........] - ETA: 6:10 - loss: 0.6904 - acc: 0.5291
6720/9333 [====================>.........] - ETA: 6:01 - loss: 0.6907 - acc: 0.5280
6784/9333 [====================>.........] - ETA: 5:53 - loss: 0.6905 - acc: 0.5289
6848/9333 [=====================>........] - ETA: 5:44 - loss: 0.6904 - acc: 0.5295
6912/9333 [=====================>........] - ETA: 5:35 - loss: 0.6905 - acc: 0.5278
6976/9333 [=====================>........] - ETA: 5:26 - loss: 0.6905 - acc: 0.5271
7040/9333 [=====================>........] - ETA: 5:17 - loss: 0.6905 - acc: 0.5270
7104/9333 [=====================>........] - ETA: 5:08 - loss: 0.6904 - acc: 0.5277
7168/9333 [======================>.......] - ETA: 4:59 - loss: 0.6904 - acc: 0.5276
7232/9333 [======================>.......] - ETA: 4:50 - loss: 0.6904 - acc: 0.5274
7296/9333 [======================>.......] - ETA: 4:42 - loss: 0.6903 - acc: 0.5280
7360/9333 [======================>.......] - ETA: 4:33 - loss: 0.6904 - acc: 0.5273
7424/9333 [======================>.......] - ETA: 4:24 - loss: 0.6904 - acc: 0.5280
7488/9333 [=======================>......] - ETA: 4:15 - loss: 0.6904 - acc: 0.5287
7552/9333 [=======================>......] - ETA: 4:06 - loss: 0.6903 - acc: 0.5293
7616/9333 [=======================>......] - ETA: 3:57 - loss: 0.6902 - acc: 0.5289
7680/9333 [=======================>......] - ETA: 3:48 - loss: 0.6903 - acc: 0.5281
7744/9333 [=======================>......] - ETA: 3:40 - loss: 0.6903 - acc: 0.5287
7808/9333 [========================>.....] - ETA: 3:31 - loss: 0.6903 - acc: 0.5279
7872/9333 [========================>.....] - ETA: 3:22 - loss: 0.6900 - acc: 0.5285
7936/9333 [========================>.....] - ETA: 3:13 - loss: 0.6898 - acc: 0.5291
8000/9333 [========================>.....] - ETA: 3:04 - loss: 0.6898 - acc: 0.5294
8064/9333 [========================>.....] - ETA: 2:55 - loss: 0.6897 - acc: 0.5296
8128/9333 [=========================>....] - ETA: 2:46 - loss: 0.6898 - acc: 0.5297
8192/9333 [=========================>....] - ETA: 2:37 - loss: 0.6898 - acc: 0.5300
8256/9333 [=========================>....] - ETA: 2:28 - loss: 0.6897 - acc: 0.5308
8320/9333 [=========================>....] - ETA: 2:20 - loss: 0.6896 - acc: 0.5312
8384/9333 [=========================>....] - ETA: 2:11 - loss: 0.6897 - acc: 0.5304
8448/9333 [==========================>...] - ETA: 2:02 - loss: 0.6896 - acc: 0.5307
8512/9333 [==========================>...] - ETA: 1:53 - loss: 0.6897 - acc: 0.5303
8576/9333 [==========================>...] - ETA: 1:44 - loss: 0.6899 - acc: 0.5297
8640/9333 [==========================>...] - ETA: 1:35 - loss: 0.6898 - acc: 0.5300
8704/9333 [==========================>...] - ETA: 1:26 - loss: 0.6898 - acc: 0.5300
8768/9333 [===========================>..] - ETA: 1:17 - loss: 0.6897 - acc: 0.5305
8832/9333 [===========================>..] - ETA: 1:09 - loss: 0.6898 - acc: 0.5302
8896/9333 [===========================>..] - ETA: 1:00 - loss: 0.6898 - acc: 0.5305
8960/9333 [===========================>..] - ETA: 51s - loss: 0.6898 - acc: 0.5301 
9024/9333 [============================>.] - ETA: 42s - loss: 0.6898 - acc: 0.5301
9088/9333 [============================>.] - ETA: 33s - loss: 0.6898 - acc: 0.5305
9152/9333 [============================>.] - ETA: 24s - loss: 0.6898 - acc: 0.5303
9216/9333 [============================>.] - ETA: 16s - loss: 0.6898 - acc: 0.5303
9280/9333 [============================>.] - ETA: 7s - loss: 0.6898 - acc: 0.5303 
9333/9333 [==============================] - 1329s 142ms/step - loss: 0.6896 - acc: 0.5311 - val_loss: 0.6911 - val_acc: 0.5400

Epoch 00005: val_acc improved from 0.53809 to 0.54002, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window19/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 6/10

  64/9333 [..............................] - ETA: 22:18 - loss: 0.6400 - acc: 0.6406
 128/9333 [..............................] - ETA: 22:12 - loss: 0.6738 - acc: 0.5625
 192/9333 [..............................] - ETA: 21:52 - loss: 0.6797 - acc: 0.5417
 256/9333 [..............................] - ETA: 21:35 - loss: 0.6891 - acc: 0.5352
 320/9333 [>.............................] - ETA: 21:08 - loss: 0.6929 - acc: 0.5250
 384/9333 [>.............................] - ETA: 20:57 - loss: 0.6975 - acc: 0.5130
 448/9333 [>.............................] - ETA: 20:30 - loss: 0.6939 - acc: 0.5268
 512/9333 [>.............................] - ETA: 20:00 - loss: 0.6973 - acc: 0.5254
 576/9333 [>.............................] - ETA: 19:46 - loss: 0.6985 - acc: 0.5191
 640/9333 [=>............................] - ETA: 19:31 - loss: 0.6993 - acc: 0.5141
 704/9333 [=>............................] - ETA: 19:26 - loss: 0.6973 - acc: 0.5199
 768/9333 [=>............................] - ETA: 19:16 - loss: 0.6972 - acc: 0.5260
 832/9333 [=>............................] - ETA: 19:05 - loss: 0.6986 - acc: 0.5204
 896/9333 [=>............................] - ETA: 18:49 - loss: 0.6994 - acc: 0.5134
 960/9333 [==>...........................] - ETA: 18:42 - loss: 0.6968 - acc: 0.5188
1024/9333 [==>...........................] - ETA: 18:28 - loss: 0.6958 - acc: 0.5215
1088/9333 [==>...........................] - ETA: 18:19 - loss: 0.6944 - acc: 0.5294
1152/9333 [==>...........................] - ETA: 18:13 - loss: 0.6953 - acc: 0.5234
1216/9333 [==>...........................] - ETA: 18:08 - loss: 0.6946 - acc: 0.5247
1280/9333 [===>..........................] - ETA: 18:01 - loss: 0.6949 - acc: 0.5250
1344/9333 [===>..........................] - ETA: 17:53 - loss: 0.6949 - acc: 0.5275
1408/9333 [===>..........................] - ETA: 17:43 - loss: 0.6937 - acc: 0.5305
1472/9333 [===>..........................] - ETA: 17:40 - loss: 0.6949 - acc: 0.5265
1536/9333 [===>..........................] - ETA: 17:33 - loss: 0.6954 - acc: 0.5254
1600/9333 [====>.........................] - ETA: 17:29 - loss: 0.6957 - acc: 0.5244
1664/9333 [====>.........................] - ETA: 17:24 - loss: 0.6966 - acc: 0.5240
1728/9333 [====>.........................] - ETA: 17:15 - loss: 0.6961 - acc: 0.5272
1792/9333 [====>.........................] - ETA: 17:07 - loss: 0.6958 - acc: 0.5279
1856/9333 [====>.........................] - ETA: 17:02 - loss: 0.6955 - acc: 0.5269
1920/9333 [=====>........................] - ETA: 16:51 - loss: 0.6954 - acc: 0.5271
1984/9333 [=====>........................] - ETA: 16:44 - loss: 0.6956 - acc: 0.5277
2048/9333 [=====>........................] - ETA: 16:37 - loss: 0.6945 - acc: 0.5317
2112/9333 [=====>........................] - ETA: 16:26 - loss: 0.6942 - acc: 0.5322
2176/9333 [=====>........................] - ETA: 16:18 - loss: 0.6943 - acc: 0.5312
2240/9333 [======>.......................] - ETA: 16:09 - loss: 0.6934 - acc: 0.5326
2304/9333 [======>.......................] - ETA: 16:00 - loss: 0.6932 - acc: 0.5339
2368/9333 [======>.......................] - ETA: 15:54 - loss: 0.6929 - acc: 0.5346
2432/9333 [======>.......................] - ETA: 15:46 - loss: 0.6930 - acc: 0.5325
2496/9333 [=======>......................] - ETA: 15:38 - loss: 0.6931 - acc: 0.5312
2560/9333 [=======>......................] - ETA: 15:29 - loss: 0.6922 - acc: 0.5332
2624/9333 [=======>......................] - ETA: 15:20 - loss: 0.6925 - acc: 0.5332
2688/9333 [=======>......................] - ETA: 15:11 - loss: 0.6918 - acc: 0.5357
2752/9333 [=======>......................] - ETA: 15:01 - loss: 0.6918 - acc: 0.5349
2816/9333 [========>.....................] - ETA: 14:52 - loss: 0.6919 - acc: 0.5348
2880/9333 [========>.....................] - ETA: 14:45 - loss: 0.6920 - acc: 0.5337
2944/9333 [========>.....................] - ETA: 14:37 - loss: 0.6918 - acc: 0.5336
3008/9333 [========>.....................] - ETA: 14:28 - loss: 0.6920 - acc: 0.5342
3072/9333 [========>.....................] - ETA: 14:20 - loss: 0.6925 - acc: 0.5326
3136/9333 [=========>....................] - ETA: 14:10 - loss: 0.6926 - acc: 0.5322
3200/9333 [=========>....................] - ETA: 14:01 - loss: 0.6921 - acc: 0.5334
3264/9333 [=========>....................] - ETA: 13:52 - loss: 0.6922 - acc: 0.5340
3328/9333 [=========>....................] - ETA: 13:43 - loss: 0.6921 - acc: 0.5337
3392/9333 [=========>....................] - ETA: 13:35 - loss: 0.6916 - acc: 0.5351
3456/9333 [==========>...................] - ETA: 13:26 - loss: 0.6910 - acc: 0.5376
3520/9333 [==========>...................] - ETA: 13:18 - loss: 0.6911 - acc: 0.5361
3584/9333 [==========>...................] - ETA: 13:08 - loss: 0.6908 - acc: 0.5357
3648/9333 [==========>...................] - ETA: 13:00 - loss: 0.6911 - acc: 0.5334
3712/9333 [==========>...................] - ETA: 12:51 - loss: 0.6908 - acc: 0.5348
3776/9333 [===========>..................] - ETA: 12:42 - loss: 0.6905 - acc: 0.5352
3840/9333 [===========>..................] - ETA: 12:33 - loss: 0.6907 - acc: 0.5349
3904/9333 [===========>..................] - ETA: 12:24 - loss: 0.6906 - acc: 0.5338
3968/9333 [===========>..................] - ETA: 12:15 - loss: 0.6904 - acc: 0.5335
4032/9333 [===========>..................] - ETA: 12:06 - loss: 0.6904 - acc: 0.5335
4096/9333 [============>.................] - ETA: 11:57 - loss: 0.6902 - acc: 0.5344
4160/9333 [============>.................] - ETA: 11:49 - loss: 0.6902 - acc: 0.5329
4224/9333 [============>.................] - ETA: 11:41 - loss: 0.6899 - acc: 0.5346
4288/9333 [============>.................] - ETA: 11:32 - loss: 0.6900 - acc: 0.5343
4352/9333 [============>.................] - ETA: 11:25 - loss: 0.6896 - acc: 0.5349
4416/9333 [=============>................] - ETA: 11:16 - loss: 0.6891 - acc: 0.5360
4480/9333 [=============>................] - ETA: 11:08 - loss: 0.6892 - acc: 0.5350
4544/9333 [=============>................] - ETA: 10:59 - loss: 0.6895 - acc: 0.5346
4608/9333 [=============>................] - ETA: 10:50 - loss: 0.6899 - acc: 0.5330
4672/9333 [==============>...............] - ETA: 10:42 - loss: 0.6897 - acc: 0.5345
4736/9333 [==============>...............] - ETA: 10:33 - loss: 0.6897 - acc: 0.5346
4800/9333 [==============>...............] - ETA: 10:25 - loss: 0.6896 - acc: 0.5352
4864/9333 [==============>...............] - ETA: 10:16 - loss: 0.6900 - acc: 0.5339
4928/9333 [==============>...............] - ETA: 10:08 - loss: 0.6900 - acc: 0.5343
4992/9333 [===============>..............] - ETA: 10:00 - loss: 0.6900 - acc: 0.5345
5056/9333 [===============>..............] - ETA: 9:51 - loss: 0.6899 - acc: 0.5350 
5120/9333 [===============>..............] - ETA: 9:43 - loss: 0.6897 - acc: 0.5365
5184/9333 [===============>..............] - ETA: 9:34 - loss: 0.6894 - acc: 0.5370
5248/9333 [===============>..............] - ETA: 9:25 - loss: 0.6896 - acc: 0.5372
5312/9333 [================>.............] - ETA: 9:16 - loss: 0.6895 - acc: 0.5375
5376/9333 [================>.............] - ETA: 9:07 - loss: 0.6892 - acc: 0.5383
5440/9333 [================>.............] - ETA: 8:59 - loss: 0.6894 - acc: 0.5369
5504/9333 [================>.............] - ETA: 8:50 - loss: 0.6891 - acc: 0.5378
5568/9333 [================>.............] - ETA: 8:41 - loss: 0.6892 - acc: 0.5379
5632/9333 [=================>............] - ETA: 8:32 - loss: 0.6891 - acc: 0.5376
5696/9333 [=================>............] - ETA: 8:23 - loss: 0.6891 - acc: 0.5379
5760/9333 [=================>............] - ETA: 8:14 - loss: 0.6892 - acc: 0.5384
5824/9333 [=================>............] - ETA: 8:05 - loss: 0.6890 - acc: 0.5391
5888/9333 [=================>............] - ETA: 7:56 - loss: 0.6890 - acc: 0.5391
5952/9333 [==================>...........] - ETA: 7:47 - loss: 0.6893 - acc: 0.5386
6016/9333 [==================>...........] - ETA: 7:38 - loss: 0.6890 - acc: 0.5397
6080/9333 [==================>...........] - ETA: 7:29 - loss: 0.6893 - acc: 0.5388
6144/9333 [==================>...........] - ETA: 7:20 - loss: 0.6890 - acc: 0.5389
6208/9333 [==================>...........] - ETA: 7:11 - loss: 0.6890 - acc: 0.5383
6272/9333 [===================>..........] - ETA: 7:02 - loss: 0.6889 - acc: 0.5383
6336/9333 [===================>..........] - ETA: 6:53 - loss: 0.6888 - acc: 0.5393
6400/9333 [===================>..........] - ETA: 6:44 - loss: 0.6882 - acc: 0.5414
6464/9333 [===================>..........] - ETA: 6:35 - loss: 0.6882 - acc: 0.5415
6528/9333 [===================>..........] - ETA: 6:26 - loss: 0.6885 - acc: 0.5411
6592/9333 [====================>.........] - ETA: 6:18 - loss: 0.6883 - acc: 0.5420
6656/9333 [====================>.........] - ETA: 6:09 - loss: 0.6884 - acc: 0.5416
6720/9333 [====================>.........] - ETA: 6:00 - loss: 0.6882 - acc: 0.5424
6784/9333 [====================>.........] - ETA: 5:51 - loss: 0.6885 - acc: 0.5420
6848/9333 [=====================>........] - ETA: 5:42 - loss: 0.6884 - acc: 0.5425
6912/9333 [=====================>........] - ETA: 5:34 - loss: 0.6887 - acc: 0.5418
6976/9333 [=====================>........] - ETA: 5:25 - loss: 0.6889 - acc: 0.5419
7040/9333 [=====================>........] - ETA: 5:16 - loss: 0.6889 - acc: 0.5419
7104/9333 [=====================>........] - ETA: 5:07 - loss: 0.6888 - acc: 0.5417
7168/9333 [======================>.......] - ETA: 4:58 - loss: 0.6886 - acc: 0.5421
7232/9333 [======================>.......] - ETA: 4:49 - loss: 0.6885 - acc: 0.5425
7296/9333 [======================>.......] - ETA: 4:41 - loss: 0.6885 - acc: 0.5425
7360/9333 [======================>.......] - ETA: 4:32 - loss: 0.6887 - acc: 0.5418
7424/9333 [======================>.......] - ETA: 4:23 - loss: 0.6886 - acc: 0.5420
7488/9333 [=======================>......] - ETA: 4:14 - loss: 0.6888 - acc: 0.5422
7552/9333 [=======================>......] - ETA: 4:05 - loss: 0.6888 - acc: 0.5424
7616/9333 [=======================>......] - ETA: 3:56 - loss: 0.6886 - acc: 0.5424
7680/9333 [=======================>......] - ETA: 3:48 - loss: 0.6889 - acc: 0.5417
7744/9333 [=======================>......] - ETA: 3:39 - loss: 0.6890 - acc: 0.5409
7808/9333 [========================>.....] - ETA: 3:30 - loss: 0.6894 - acc: 0.5402
7872/9333 [========================>.....] - ETA: 3:21 - loss: 0.6893 - acc: 0.5405
7936/9333 [========================>.....] - ETA: 3:12 - loss: 0.6894 - acc: 0.5403
8000/9333 [========================>.....] - ETA: 3:04 - loss: 0.6894 - acc: 0.5403
8064/9333 [========================>.....] - ETA: 2:55 - loss: 0.6894 - acc: 0.5409
8128/9333 [=========================>....] - ETA: 2:46 - loss: 0.6892 - acc: 0.5413
8192/9333 [=========================>....] - ETA: 2:37 - loss: 0.6891 - acc: 0.5409
8256/9333 [=========================>....] - ETA: 2:28 - loss: 0.6890 - acc: 0.5412
8320/9333 [=========================>....] - ETA: 2:20 - loss: 0.6891 - acc: 0.5410
8384/9333 [=========================>....] - ETA: 2:11 - loss: 0.6891 - acc: 0.5409
8448/9333 [==========================>...] - ETA: 2:02 - loss: 0.6890 - acc: 0.5414
8512/9333 [==========================>...] - ETA: 1:53 - loss: 0.6891 - acc: 0.5408
8576/9333 [==========================>...] - ETA: 1:44 - loss: 0.6892 - acc: 0.5409
8640/9333 [==========================>...] - ETA: 1:35 - loss: 0.6892 - acc: 0.5409
8704/9333 [==========================>...] - ETA: 1:27 - loss: 0.6891 - acc: 0.5412
8768/9333 [===========================>..] - ETA: 1:18 - loss: 0.6892 - acc: 0.5409
8832/9333 [===========================>..] - ETA: 1:09 - loss: 0.6893 - acc: 0.5406
8896/9333 [===========================>..] - ETA: 1:00 - loss: 0.6892 - acc: 0.5413
8960/9333 [===========================>..] - ETA: 51s - loss: 0.6893 - acc: 0.5413 
9024/9333 [============================>.] - ETA: 42s - loss: 0.6894 - acc: 0.5411
9088/9333 [============================>.] - ETA: 33s - loss: 0.6896 - acc: 0.5409
9152/9333 [============================>.] - ETA: 25s - loss: 0.6897 - acc: 0.5401
9216/9333 [============================>.] - ETA: 16s - loss: 0.6897 - acc: 0.5400
9280/9333 [============================>.] - ETA: 7s - loss: 0.6898 - acc: 0.5399 
9333/9333 [==============================] - 1336s 143ms/step - loss: 0.6898 - acc: 0.5399 - val_loss: 0.6895 - val_acc: 0.5217

Epoch 00006: val_acc did not improve from 0.54002
Epoch 7/10

  64/9333 [..............................] - ETA: 21:42 - loss: 0.6742 - acc: 0.5625
 128/9333 [..............................] - ETA: 22:04 - loss: 0.6922 - acc: 0.5078
 192/9333 [..............................] - ETA: 21:43 - loss: 0.6906 - acc: 0.5208
 256/9333 [..............................] - ETA: 21:52 - loss: 0.6893 - acc: 0.5391
 320/9333 [>.............................] - ETA: 21:59 - loss: 0.6912 - acc: 0.5344
 384/9333 [>.............................] - ETA: 21:50 - loss: 0.6962 - acc: 0.5130
 448/9333 [>.............................] - ETA: 21:37 - loss: 0.6971 - acc: 0.5022
 512/9333 [>.............................] - ETA: 21:23 - loss: 0.6974 - acc: 0.5020
 576/9333 [>.............................] - ETA: 21:04 - loss: 0.6963 - acc: 0.5087
 640/9333 [=>............................] - ETA: 20:46 - loss: 0.6947 - acc: 0.5109
 704/9333 [=>............................] - ETA: 20:39 - loss: 0.6945 - acc: 0.5156
 768/9333 [=>............................] - ETA: 20:24 - loss: 0.6933 - acc: 0.5247
 832/9333 [=>............................] - ETA: 20:14 - loss: 0.6932 - acc: 0.5264
 896/9333 [=>............................] - ETA: 20:09 - loss: 0.6920 - acc: 0.5279
 960/9333 [==>...........................] - ETA: 19:55 - loss: 0.6930 - acc: 0.5250
1024/9333 [==>...........................] - ETA: 19:46 - loss: 0.6923 - acc: 0.5215
1088/9333 [==>...........................] - ETA: 19:31 - loss: 0.6927 - acc: 0.5193
1152/9333 [==>...........................] - ETA: 19:22 - loss: 0.6916 - acc: 0.5269
1216/9333 [==>...........................] - ETA: 19:11 - loss: 0.6916 - acc: 0.5312
1280/9333 [===>..........................] - ETA: 18:58 - loss: 0.6920 - acc: 0.5320
1344/9333 [===>..........................] - ETA: 18:48 - loss: 0.6927 - acc: 0.5320
1408/9333 [===>..........................] - ETA: 18:39 - loss: 0.6919 - acc: 0.5369
1472/9333 [===>..........................] - ETA: 18:27 - loss: 0.6921 - acc: 0.5360
1536/9333 [===>..........................] - ETA: 18:13 - loss: 0.6919 - acc: 0.5378
1600/9333 [====>.........................] - ETA: 18:04 - loss: 0.6916 - acc: 0.5375
1664/9333 [====>.........................] - ETA: 17:53 - loss: 0.6913 - acc: 0.5385
1728/9333 [====>.........................] - ETA: 17:45 - loss: 0.6910 - acc: 0.5382
1792/9333 [====>.........................] - ETA: 17:35 - loss: 0.6907 - acc: 0.5396
1856/9333 [====>.........................] - ETA: 17:24 - loss: 0.6913 - acc: 0.5345
1920/9333 [=====>........................] - ETA: 17:16 - loss: 0.6914 - acc: 0.5333
1984/9333 [=====>........................] - ETA: 17:07 - loss: 0.6916 - acc: 0.5328
2048/9333 [=====>........................] - ETA: 16:59 - loss: 0.6921 - acc: 0.5312
2112/9333 [=====>........................] - ETA: 16:49 - loss: 0.6917 - acc: 0.5322
2176/9333 [=====>........................] - ETA: 16:41 - loss: 0.6917 - acc: 0.5326
2240/9333 [======>.......................] - ETA: 16:32 - loss: 0.6917 - acc: 0.5321
2304/9333 [======>.......................] - ETA: 16:21 - loss: 0.6917 - acc: 0.5312
2368/9333 [======>.......................] - ETA: 16:13 - loss: 0.6917 - acc: 0.5308
2432/9333 [======>.......................] - ETA: 16:04 - loss: 0.6911 - acc: 0.5325
2496/9333 [=======>......................] - ETA: 15:54 - loss: 0.6909 - acc: 0.5321
2560/9333 [=======>......................] - ETA: 15:47 - loss: 0.6915 - acc: 0.5293
2624/9333 [=======>......................] - ETA: 15:38 - loss: 0.6916 - acc: 0.5282
2688/9333 [=======>......................] - ETA: 15:29 - loss: 0.6909 - acc: 0.5290
2752/9333 [=======>......................] - ETA: 15:22 - loss: 0.6908 - acc: 0.5298
2816/9333 [========>.....................] - ETA: 15:14 - loss: 0.6904 - acc: 0.5305
2880/9333 [========>.....................] - ETA: 15:06 - loss: 0.6900 - acc: 0.5312
2944/9333 [========>.....................] - ETA: 14:58 - loss: 0.6894 - acc: 0.5329
3008/9333 [========>.....................] - ETA: 14:50 - loss: 0.6893 - acc: 0.5342
3072/9333 [========>.....................] - ETA: 14:40 - loss: 0.6898 - acc: 0.5322
3136/9333 [=========>....................] - ETA: 14:30 - loss: 0.6901 - acc: 0.5319
3200/9333 [=========>....................] - ETA: 14:22 - loss: 0.6899 - acc: 0.5331
3264/9333 [=========>....................] - ETA: 14:13 - loss: 0.6906 - acc: 0.5312
3328/9333 [=========>....................] - ETA: 14:04 - loss: 0.6907 - acc: 0.5316
3392/9333 [=========>....................] - ETA: 13:54 - loss: 0.6910 - acc: 0.5310
3456/9333 [==========>...................] - ETA: 13:45 - loss: 0.6904 - acc: 0.5336
3520/9333 [==========>...................] - ETA: 13:35 - loss: 0.6901 - acc: 0.5358
3584/9333 [==========>...................] - ETA: 13:26 - loss: 0.6903 - acc: 0.5357
3648/9333 [==========>...................] - ETA: 13:18 - loss: 0.6905 - acc: 0.5354
3712/9333 [==========>...................] - ETA: 13:08 - loss: 0.6907 - acc: 0.5342
3776/9333 [===========>..................] - ETA: 12:59 - loss: 0.6909 - acc: 0.5331
3840/9333 [===========>..................] - ETA: 12:50 - loss: 0.6907 - acc: 0.5341
3904/9333 [===========>..................] - ETA: 12:40 - loss: 0.6908 - acc: 0.5341
3968/9333 [===========>..................] - ETA: 12:30 - loss: 0.6906 - acc: 0.5348
4032/9333 [===========>..................] - ETA: 12:20 - loss: 0.6908 - acc: 0.5327
4096/9333 [============>.................] - ETA: 12:11 - loss: 0.6909 - acc: 0.5322
4160/9333 [============>.................] - ETA: 12:02 - loss: 0.6910 - acc: 0.5325
4224/9333 [============>.................] - ETA: 11:53 - loss: 0.6911 - acc: 0.5315
4288/9333 [============>.................] - ETA: 11:44 - loss: 0.6908 - acc: 0.5315
4352/9333 [============>.................] - ETA: 11:34 - loss: 0.6907 - acc: 0.5319
4416/9333 [=============>................] - ETA: 11:25 - loss: 0.6906 - acc: 0.5322
4480/9333 [=============>................] - ETA: 11:16 - loss: 0.6908 - acc: 0.5319
4544/9333 [=============>................] - ETA: 11:06 - loss: 0.6908 - acc: 0.5308
4608/9333 [=============>................] - ETA: 10:58 - loss: 0.6911 - acc: 0.5293
4672/9333 [==============>...............] - ETA: 10:49 - loss: 0.6910 - acc: 0.5287
4736/9333 [==============>...............] - ETA: 10:39 - loss: 0.6909 - acc: 0.5302
4800/9333 [==============>...............] - ETA: 10:31 - loss: 0.6906 - acc: 0.5319
4864/9333 [==============>...............] - ETA: 10:22 - loss: 0.6906 - acc: 0.5319
4928/9333 [==============>...............] - ETA: 10:12 - loss: 0.6905 - acc: 0.5312
4992/9333 [===============>..............] - ETA: 10:03 - loss: 0.6904 - acc: 0.5317
5056/9333 [===============>..............] - ETA: 9:55 - loss: 0.6905 - acc: 0.5312 
5120/9333 [===============>..............] - ETA: 9:46 - loss: 0.6906 - acc: 0.5301
5184/9333 [===============>..............] - ETA: 9:37 - loss: 0.6905 - acc: 0.5297
5248/9333 [===============>..............] - ETA: 9:28 - loss: 0.6904 - acc: 0.5301
5312/9333 [================>.............] - ETA: 9:18 - loss: 0.6904 - acc: 0.5305
5376/9333 [================>.............] - ETA: 9:09 - loss: 0.6904 - acc: 0.5299
5440/9333 [================>.............] - ETA: 9:00 - loss: 0.6905 - acc: 0.5298
5504/9333 [================>.............] - ETA: 8:51 - loss: 0.6903 - acc: 0.5305
5568/9333 [================>.............] - ETA: 8:43 - loss: 0.6904 - acc: 0.5307
5632/9333 [=================>............] - ETA: 8:34 - loss: 0.6904 - acc: 0.5305
5696/9333 [=================>............] - ETA: 8:26 - loss: 0.6904 - acc: 0.5309
5760/9333 [=================>............] - ETA: 8:17 - loss: 0.6904 - acc: 0.5314
5824/9333 [=================>............] - ETA: 8:08 - loss: 0.6903 - acc: 0.5325
5888/9333 [=================>............] - ETA: 7:59 - loss: 0.6902 - acc: 0.5335
5952/9333 [==================>...........] - ETA: 7:50 - loss: 0.6901 - acc: 0.5336
6016/9333 [==================>...........] - ETA: 7:41 - loss: 0.6901 - acc: 0.5329
6080/9333 [==================>...........] - ETA: 7:32 - loss: 0.6900 - acc: 0.5336
6144/9333 [==================>...........] - ETA: 7:23 - loss: 0.6898 - acc: 0.5345
6208/9333 [==================>...........] - ETA: 7:15 - loss: 0.6897 - acc: 0.5348
6272/9333 [===================>..........] - ETA: 7:06 - loss: 0.6895 - acc: 0.5360
6336/9333 [===================>..........] - ETA: 6:57 - loss: 0.6895 - acc: 0.5358
6400/9333 [===================>..........] - ETA: 6:48 - loss: 0.6894 - acc: 0.5364
6464/9333 [===================>..........] - ETA: 6:39 - loss: 0.6894 - acc: 0.5371
6528/9333 [===================>..........] - ETA: 6:30 - loss: 0.6894 - acc: 0.5374
6592/9333 [====================>.........] - ETA: 6:21 - loss: 0.6893 - acc: 0.5382
6656/9333 [====================>.........] - ETA: 6:12 - loss: 0.6894 - acc: 0.5380
6720/9333 [====================>.........] - ETA: 6:03 - loss: 0.6895 - acc: 0.5369
6784/9333 [====================>.........] - ETA: 5:54 - loss: 0.6897 - acc: 0.5360
6848/9333 [=====================>........] - ETA: 5:45 - loss: 0.6898 - acc: 0.5353
6912/9333 [=====================>........] - ETA: 5:36 - loss: 0.6899 - acc: 0.5354
6976/9333 [=====================>........] - ETA: 5:27 - loss: 0.6896 - acc: 0.5363
7040/9333 [=====================>........] - ETA: 5:18 - loss: 0.6894 - acc: 0.5366
7104/9333 [=====================>........] - ETA: 5:10 - loss: 0.6892 - acc: 0.5379
7168/9333 [======================>.......] - ETA: 5:01 - loss: 0.6894 - acc: 0.5372
7232/9333 [======================>.......] - ETA: 4:52 - loss: 0.6896 - acc: 0.5372
7296/9333 [======================>.......] - ETA: 4:43 - loss: 0.6897 - acc: 0.5376
7360/9333 [======================>.......] - ETA: 4:34 - loss: 0.6898 - acc: 0.5370
7424/9333 [======================>.......] - ETA: 4:25 - loss: 0.6898 - acc: 0.5376
7488/9333 [=======================>......] - ETA: 4:16 - loss: 0.6898 - acc: 0.5373
7552/9333 [=======================>......] - ETA: 4:07 - loss: 0.6898 - acc: 0.5376
7616/9333 [=======================>......] - ETA: 3:58 - loss: 0.6898 - acc: 0.5377
7680/9333 [=======================>......] - ETA: 3:49 - loss: 0.6896 - acc: 0.5385
7744/9333 [=======================>......] - ETA: 3:40 - loss: 0.6895 - acc: 0.5389
7808/9333 [========================>.....] - ETA: 3:31 - loss: 0.6895 - acc: 0.5387
7872/9333 [========================>.....] - ETA: 3:23 - loss: 0.6894 - acc: 0.5393
7936/9333 [========================>.....] - ETA: 3:14 - loss: 0.6895 - acc: 0.5389
8000/9333 [========================>.....] - ETA: 3:05 - loss: 0.6895 - acc: 0.5387
8064/9333 [========================>.....] - ETA: 2:56 - loss: 0.6894 - acc: 0.5389
8128/9333 [=========================>....] - ETA: 2:47 - loss: 0.6892 - acc: 0.5401
8192/9333 [=========================>....] - ETA: 2:38 - loss: 0.6893 - acc: 0.5398
8256/9333 [=========================>....] - ETA: 2:29 - loss: 0.6893 - acc: 0.5400
8320/9333 [=========================>....] - ETA: 2:20 - loss: 0.6891 - acc: 0.5406
8384/9333 [=========================>....] - ETA: 2:12 - loss: 0.6893 - acc: 0.5400
8448/9333 [==========================>...] - ETA: 2:03 - loss: 0.6893 - acc: 0.5399
8512/9333 [==========================>...] - ETA: 1:54 - loss: 0.6894 - acc: 0.5397
8576/9333 [==========================>...] - ETA: 1:45 - loss: 0.6895 - acc: 0.5392
8640/9333 [==========================>...] - ETA: 1:36 - loss: 0.6896 - acc: 0.5390
8704/9333 [==========================>...] - ETA: 1:27 - loss: 0.6894 - acc: 0.5393
8768/9333 [===========================>..] - ETA: 1:18 - loss: 0.6892 - acc: 0.5396
8832/9333 [===========================>..] - ETA: 1:09 - loss: 0.6892 - acc: 0.5388
8896/9333 [===========================>..] - ETA: 1:00 - loss: 0.6891 - acc: 0.5390
8960/9333 [===========================>..] - ETA: 51s - loss: 0.6889 - acc: 0.5398 
9024/9333 [============================>.] - ETA: 42s - loss: 0.6888 - acc: 0.5399
9088/9333 [============================>.] - ETA: 34s - loss: 0.6889 - acc: 0.5398
9152/9333 [============================>.] - ETA: 25s - loss: 0.6888 - acc: 0.5404
9216/9333 [============================>.] - ETA: 16s - loss: 0.6886 - acc: 0.5410
9280/9333 [============================>.] - ETA: 7s - loss: 0.6886 - acc: 0.5407 
9333/9333 [==============================] - 1339s 143ms/step - loss: 0.6885 - acc: 0.5408 - val_loss: 0.6903 - val_acc: 0.5381

Epoch 00007: val_acc did not improve from 0.54002
Epoch 8/10

  64/9333 [..............................] - ETA: 23:24 - loss: 0.6816 - acc: 0.5781
 128/9333 [..............................] - ETA: 21:57 - loss: 0.6837 - acc: 0.5703
 192/9333 [..............................] - ETA: 21:49 - loss: 0.6857 - acc: 0.5677
 256/9333 [..............................] - ETA: 21:22 - loss: 0.6971 - acc: 0.5391
 320/9333 [>.............................] - ETA: 21:12 - loss: 0.7009 - acc: 0.5312
 384/9333 [>.............................] - ETA: 20:57 - loss: 0.6962 - acc: 0.5365
 448/9333 [>.............................] - ETA: 20:41 - loss: 0.6929 - acc: 0.5513
 512/9333 [>.............................] - ETA: 20:29 - loss: 0.6847 - acc: 0.5703
 576/9333 [>.............................] - ETA: 20:18 - loss: 0.6840 - acc: 0.5677
 640/9333 [=>............................] - ETA: 20:12 - loss: 0.6804 - acc: 0.5672
 704/9333 [=>............................] - ETA: 19:59 - loss: 0.6845 - acc: 0.5526
 768/9333 [=>............................] - ETA: 19:52 - loss: 0.6883 - acc: 0.5443
 832/9333 [=>............................] - ETA: 19:41 - loss: 0.6917 - acc: 0.5337
 896/9333 [=>............................] - ETA: 19:38 - loss: 0.6899 - acc: 0.5379
 960/9333 [==>...........................] - ETA: 19:31 - loss: 0.6911 - acc: 0.5302
1024/9333 [==>...........................] - ETA: 19:19 - loss: 0.6902 - acc: 0.5342
1088/9333 [==>...........................] - ETA: 19:10 - loss: 0.6909 - acc: 0.5331
1152/9333 [==>...........................] - ETA: 18:57 - loss: 0.6901 - acc: 0.5312
1216/9333 [==>...........................] - ETA: 18:52 - loss: 0.6912 - acc: 0.5280
1280/9333 [===>..........................] - ETA: 18:42 - loss: 0.6904 - acc: 0.5266
1344/9333 [===>..........................] - ETA: 18:31 - loss: 0.6896 - acc: 0.5283
1408/9333 [===>..........................] - ETA: 18:23 - loss: 0.6882 - acc: 0.5369
1472/9333 [===>..........................] - ETA: 18:17 - loss: 0.6885 - acc: 0.5380
1536/9333 [===>..........................] - ETA: 18:04 - loss: 0.6879 - acc: 0.5397
1600/9333 [====>.........................] - ETA: 17:55 - loss: 0.6876 - acc: 0.5406
1664/9333 [====>.........................] - ETA: 17:46 - loss: 0.6870 - acc: 0.5439
1728/9333 [====>.........................] - ETA: 17:34 - loss: 0.6875 - acc: 0.5422
1792/9333 [====>.........................] - ETA: 17:24 - loss: 0.6871 - acc: 0.5446
1856/9333 [====>.........................] - ETA: 17:15 - loss: 0.6870 - acc: 0.5453
1920/9333 [=====>........................] - ETA: 17:09 - loss: 0.6863 - acc: 0.5490
1984/9333 [=====>........................] - ETA: 16:58 - loss: 0.6857 - acc: 0.5509
2048/9333 [=====>........................] - ETA: 16:49 - loss: 0.6864 - acc: 0.5474
2112/9333 [=====>........................] - ETA: 16:39 - loss: 0.6868 - acc: 0.5455
2176/9333 [=====>........................] - ETA: 16:29 - loss: 0.6862 - acc: 0.5455
2240/9333 [======>.......................] - ETA: 16:18 - loss: 0.6858 - acc: 0.5482
2304/9333 [======>.......................] - ETA: 16:09 - loss: 0.6863 - acc: 0.5477
2368/9333 [======>.......................] - ETA: 16:00 - loss: 0.6856 - acc: 0.5507
2432/9333 [======>.......................] - ETA: 15:52 - loss: 0.6852 - acc: 0.5514
2496/9333 [=======>......................] - ETA: 15:44 - loss: 0.6853 - acc: 0.5501
2560/9333 [=======>......................] - ETA: 15:36 - loss: 0.6857 - acc: 0.5480
2624/9333 [=======>......................] - ETA: 15:28 - loss: 0.6853 - acc: 0.5492
2688/9333 [=======>......................] - ETA: 15:18 - loss: 0.6849 - acc: 0.5502
2752/9333 [=======>......................] - ETA: 15:09 - loss: 0.6853 - acc: 0.5498
2816/9333 [========>.....................] - ETA: 15:00 - loss: 0.6849 - acc: 0.5508
2880/9333 [========>.....................] - ETA: 14:51 - loss: 0.6851 - acc: 0.5503
2944/9333 [========>.....................] - ETA: 14:41 - loss: 0.6845 - acc: 0.5526
3008/9333 [========>.....................] - ETA: 14:33 - loss: 0.6847 - acc: 0.5519
3072/9333 [========>.....................] - ETA: 14:24 - loss: 0.6854 - acc: 0.5501
3136/9333 [=========>....................] - ETA: 14:16 - loss: 0.6855 - acc: 0.5507
3200/9333 [=========>....................] - ETA: 14:07 - loss: 0.6854 - acc: 0.5509
3264/9333 [=========>....................] - ETA: 13:59 - loss: 0.6856 - acc: 0.5496
3328/9333 [=========>....................] - ETA: 13:50 - loss: 0.6861 - acc: 0.5487
3392/9333 [=========>....................] - ETA: 13:40 - loss: 0.6864 - acc: 0.5475
3456/9333 [==========>...................] - ETA: 13:31 - loss: 0.6863 - acc: 0.5472
3520/9333 [==========>...................] - ETA: 13:22 - loss: 0.6864 - acc: 0.5472
3584/9333 [==========>...................] - ETA: 13:13 - loss: 0.6861 - acc: 0.5474
3648/9333 [==========>...................] - ETA: 13:04 - loss: 0.6867 - acc: 0.5458
3712/9333 [==========>...................] - ETA: 12:56 - loss: 0.6867 - acc: 0.5466
3776/9333 [===========>..................] - ETA: 12:47 - loss: 0.6868 - acc: 0.5466
3840/9333 [===========>..................] - ETA: 12:38 - loss: 0.6865 - acc: 0.5477
3904/9333 [===========>..................] - ETA: 12:30 - loss: 0.6866 - acc: 0.5474
3968/9333 [===========>..................] - ETA: 12:21 - loss: 0.6867 - acc: 0.5466
4032/9333 [===========>..................] - ETA: 12:11 - loss: 0.6869 - acc: 0.5459
4096/9333 [============>.................] - ETA: 12:04 - loss: 0.6868 - acc: 0.5469
4160/9333 [============>.................] - ETA: 11:55 - loss: 0.6866 - acc: 0.5466
4224/9333 [============>.................] - ETA: 11:47 - loss: 0.6867 - acc: 0.5466
4288/9333 [============>.................] - ETA: 11:38 - loss: 0.6862 - acc: 0.5483
4352/9333 [============>.................] - ETA: 11:28 - loss: 0.6862 - acc: 0.5485
4416/9333 [=============>................] - ETA: 11:20 - loss: 0.6863 - acc: 0.5485
4480/9333 [=============>................] - ETA: 11:11 - loss: 0.6864 - acc: 0.5482
4544/9333 [=============>................] - ETA: 11:03 - loss: 0.6861 - acc: 0.5495
4608/9333 [=============>................] - ETA: 10:53 - loss: 0.6862 - acc: 0.5488
4672/9333 [==============>...............] - ETA: 10:44 - loss: 0.6864 - acc: 0.5477
4736/9333 [==============>...............] - ETA: 10:34 - loss: 0.6861 - acc: 0.5488
4800/9333 [==============>...............] - ETA: 10:25 - loss: 0.6865 - acc: 0.5473
4864/9333 [==============>...............] - ETA: 10:17 - loss: 0.6864 - acc: 0.5473
4928/9333 [==============>...............] - ETA: 10:09 - loss: 0.6863 - acc: 0.5481
4992/9333 [===============>..............] - ETA: 10:00 - loss: 0.6861 - acc: 0.5491
5056/9333 [===============>..............] - ETA: 9:51 - loss: 0.6861 - acc: 0.5489 
5120/9333 [===============>..............] - ETA: 9:42 - loss: 0.6862 - acc: 0.5479
5184/9333 [===============>..............] - ETA: 9:34 - loss: 0.6861 - acc: 0.5484
5248/9333 [===============>..............] - ETA: 9:26 - loss: 0.6862 - acc: 0.5476
5312/9333 [================>.............] - ETA: 9:18 - loss: 0.6865 - acc: 0.5469
5376/9333 [================>.............] - ETA: 9:09 - loss: 0.6868 - acc: 0.5465
5440/9333 [================>.............] - ETA: 9:00 - loss: 0.6870 - acc: 0.5463
5504/9333 [================>.............] - ETA: 8:51 - loss: 0.6873 - acc: 0.5449
5568/9333 [================>.............] - ETA: 8:42 - loss: 0.6876 - acc: 0.5438
5632/9333 [=================>............] - ETA: 8:33 - loss: 0.6877 - acc: 0.5435
5696/9333 [=================>............] - ETA: 8:24 - loss: 0.6876 - acc: 0.5442
5760/9333 [=================>............] - ETA: 8:15 - loss: 0.6876 - acc: 0.5439
5824/9333 [=================>............] - ETA: 8:07 - loss: 0.6876 - acc: 0.5436
5888/9333 [=================>............] - ETA: 7:58 - loss: 0.6874 - acc: 0.5452
5952/9333 [==================>...........] - ETA: 7:50 - loss: 0.6873 - acc: 0.5455
6016/9333 [==================>...........] - ETA: 7:41 - loss: 0.6873 - acc: 0.5459
6080/9333 [==================>...........] - ETA: 7:33 - loss: 0.6874 - acc: 0.5459
6144/9333 [==================>...........] - ETA: 7:24 - loss: 0.6875 - acc: 0.5449
6208/9333 [==================>...........] - ETA: 7:16 - loss: 0.6876 - acc: 0.5449
6272/9333 [===================>..........] - ETA: 7:07 - loss: 0.6873 - acc: 0.5456
6336/9333 [===================>..........] - ETA: 6:59 - loss: 0.6875 - acc: 0.5451
6400/9333 [===================>..........] - ETA: 6:50 - loss: 0.6879 - acc: 0.5439
6464/9333 [===================>..........] - ETA: 6:41 - loss: 0.6881 - acc: 0.5436
6528/9333 [===================>..........] - ETA: 6:33 - loss: 0.6885 - acc: 0.5427
6592/9333 [====================>.........] - ETA: 6:24 - loss: 0.6882 - acc: 0.5440
6656/9333 [====================>.........] - ETA: 6:15 - loss: 0.6883 - acc: 0.5439
6720/9333 [====================>.........] - ETA: 6:06 - loss: 0.6886 - acc: 0.5427
6784/9333 [====================>.........] - ETA: 5:57 - loss: 0.6887 - acc: 0.5422
6848/9333 [=====================>........] - ETA: 5:49 - loss: 0.6886 - acc: 0.5426
6912/9333 [=====================>........] - ETA: 5:40 - loss: 0.6886 - acc: 0.5421
6976/9333 [=====================>........] - ETA: 5:31 - loss: 0.6886 - acc: 0.5421
7040/9333 [=====================>........] - ETA: 5:22 - loss: 0.6885 - acc: 0.5425
7104/9333 [=====================>........] - ETA: 5:13 - loss: 0.6883 - acc: 0.5434
7168/9333 [======================>.......] - ETA: 5:04 - loss: 0.6884 - acc: 0.5435
7232/9333 [======================>.......] - ETA: 4:56 - loss: 0.6884 - acc: 0.5437
7296/9333 [======================>.......] - ETA: 4:47 - loss: 0.6883 - acc: 0.5439
7360/9333 [======================>.......] - ETA: 4:38 - loss: 0.6884 - acc: 0.5444
7424/9333 [======================>.......] - ETA: 4:29 - loss: 0.6884 - acc: 0.5440
7488/9333 [=======================>......] - ETA: 4:20 - loss: 0.6882 - acc: 0.5454
7552/9333 [=======================>......] - ETA: 4:11 - loss: 0.6880 - acc: 0.5467
7616/9333 [=======================>......] - ETA: 4:02 - loss: 0.6879 - acc: 0.5473
7680/9333 [=======================>......] - ETA: 3:53 - loss: 0.6878 - acc: 0.5474
7744/9333 [=======================>......] - ETA: 3:44 - loss: 0.6878 - acc: 0.5478
7808/9333 [========================>.....] - ETA: 3:35 - loss: 0.6878 - acc: 0.5475
7872/9333 [========================>.....] - ETA: 3:26 - loss: 0.6878 - acc: 0.5476
7936/9333 [========================>.....] - ETA: 3:17 - loss: 0.6878 - acc: 0.5479
8000/9333 [========================>.....] - ETA: 3:08 - loss: 0.6879 - acc: 0.5474
8064/9333 [========================>.....] - ETA: 2:59 - loss: 0.6879 - acc: 0.5472
8128/9333 [=========================>....] - ETA: 2:50 - loss: 0.6879 - acc: 0.5469
8192/9333 [=========================>....] - ETA: 2:41 - loss: 0.6880 - acc: 0.5466
8256/9333 [=========================>....] - ETA: 2:32 - loss: 0.6878 - acc: 0.5474
8320/9333 [=========================>....] - ETA: 2:23 - loss: 0.6881 - acc: 0.5462
8384/9333 [=========================>....] - ETA: 2:14 - loss: 0.6879 - acc: 0.5465
8448/9333 [==========================>...] - ETA: 2:05 - loss: 0.6879 - acc: 0.5464
8512/9333 [==========================>...] - ETA: 1:56 - loss: 0.6878 - acc: 0.5466
8576/9333 [==========================>...] - ETA: 1:47 - loss: 0.6877 - acc: 0.5471
8640/9333 [==========================>...] - ETA: 1:38 - loss: 0.6876 - acc: 0.5475
8704/9333 [==========================>...] - ETA: 1:29 - loss: 0.6878 - acc: 0.5472
8768/9333 [===========================>..] - ETA: 1:20 - loss: 0.6877 - acc: 0.5476
8832/9333 [===========================>..] - ETA: 1:11 - loss: 0.6878 - acc: 0.5473
8896/9333 [===========================>..] - ETA: 1:01 - loss: 0.6879 - acc: 0.5467
8960/9333 [===========================>..] - ETA: 52s - loss: 0.6879 - acc: 0.5464 
9024/9333 [============================>.] - ETA: 43s - loss: 0.6878 - acc: 0.5467
9088/9333 [============================>.] - ETA: 34s - loss: 0.6876 - acc: 0.5470
9152/9333 [============================>.] - ETA: 25s - loss: 0.6874 - acc: 0.5475
9216/9333 [============================>.] - ETA: 16s - loss: 0.6875 - acc: 0.5473
9280/9333 [============================>.] - ETA: 7s - loss: 0.6876 - acc: 0.5472 
9333/9333 [==============================] - 1362s 146ms/step - loss: 0.6874 - acc: 0.5476 - val_loss: 0.6855 - val_acc: 0.5497

Epoch 00008: val_acc improved from 0.54002 to 0.54966, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window19/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 9/10

  64/9333 [..............................] - ETA: 21:31 - loss: 0.6849 - acc: 0.5469
 128/9333 [..............................] - ETA: 21:38 - loss: 0.6861 - acc: 0.5391
 192/9333 [..............................] - ETA: 21:10 - loss: 0.6914 - acc: 0.5312
 256/9333 [..............................] - ETA: 20:47 - loss: 0.6818 - acc: 0.5742
 320/9333 [>.............................] - ETA: 20:38 - loss: 0.6888 - acc: 0.5563
 384/9333 [>.............................] - ETA: 20:34 - loss: 0.6927 - acc: 0.5391
 448/9333 [>.............................] - ETA: 20:39 - loss: 0.6964 - acc: 0.5246
 512/9333 [>.............................] - ETA: 20:24 - loss: 0.6924 - acc: 0.5273
 576/9333 [>.............................] - ETA: 20:10 - loss: 0.6939 - acc: 0.5191
 640/9333 [=>............................] - ETA: 20:05 - loss: 0.6947 - acc: 0.5156
 704/9333 [=>............................] - ETA: 19:51 - loss: 0.6942 - acc: 0.5227
 768/9333 [=>............................] - ETA: 19:42 - loss: 0.6917 - acc: 0.5247
 832/9333 [=>............................] - ETA: 19:34 - loss: 0.6897 - acc: 0.5300
 896/9333 [=>............................] - ETA: 19:29 - loss: 0.6887 - acc: 0.5335
 960/9333 [==>...........................] - ETA: 19:17 - loss: 0.6902 - acc: 0.5302
1024/9333 [==>...........................] - ETA: 19:09 - loss: 0.6902 - acc: 0.5322
1088/9333 [==>...........................] - ETA: 19:05 - loss: 0.6901 - acc: 0.5322
1152/9333 [==>...........................] - ETA: 18:54 - loss: 0.6904 - acc: 0.5312
1216/9333 [==>...........................] - ETA: 18:43 - loss: 0.6907 - acc: 0.5296
1280/9333 [===>..........................] - ETA: 18:35 - loss: 0.6909 - acc: 0.5297
1344/9333 [===>..........................] - ETA: 18:21 - loss: 0.6909 - acc: 0.5298
1408/9333 [===>..........................] - ETA: 18:15 - loss: 0.6903 - acc: 0.5334
1472/9333 [===>..........................] - ETA: 18:05 - loss: 0.6905 - acc: 0.5326
1536/9333 [===>..........................] - ETA: 17:55 - loss: 0.6899 - acc: 0.5345
1600/9333 [====>.........................] - ETA: 17:47 - loss: 0.6902 - acc: 0.5350
1664/9333 [====>.........................] - ETA: 17:39 - loss: 0.6899 - acc: 0.5385
1728/9333 [====>.........................] - ETA: 17:29 - loss: 0.6894 - acc: 0.5422
1792/9333 [====>.........................] - ETA: 17:18 - loss: 0.6888 - acc: 0.5435
1856/9333 [====>.........................] - ETA: 17:09 - loss: 0.6898 - acc: 0.5415
1920/9333 [=====>........................] - ETA: 16:59 - loss: 0.6897 - acc: 0.5427
1984/9333 [=====>........................] - ETA: 16:49 - loss: 0.6887 - acc: 0.5454
2048/9333 [=====>........................] - ETA: 16:40 - loss: 0.6890 - acc: 0.5420
2112/9333 [=====>........................] - ETA: 16:35 - loss: 0.6880 - acc: 0.5445
2176/9333 [=====>........................] - ETA: 16:26 - loss: 0.6895 - acc: 0.5404
2240/9333 [======>.......................] - ETA: 16:18 - loss: 0.6887 - acc: 0.5424
2304/9333 [======>.......................] - ETA: 16:10 - loss: 0.6880 - acc: 0.5430
2368/9333 [======>.......................] - ETA: 16:01 - loss: 0.6881 - acc: 0.5427
2432/9333 [======>.......................] - ETA: 15:51 - loss: 0.6872 - acc: 0.5461
2496/9333 [=======>......................] - ETA: 15:39 - loss: 0.6880 - acc: 0.5469
2560/9333 [=======>......................] - ETA: 15:31 - loss: 0.6892 - acc: 0.5410
2624/9333 [=======>......................] - ETA: 15:22 - loss: 0.6891 - acc: 0.5427
2688/9333 [=======>......................] - ETA: 15:13 - loss: 0.6886 - acc: 0.5435
2752/9333 [=======>......................] - ETA: 15:05 - loss: 0.6882 - acc: 0.5443
2816/9333 [========>.....................] - ETA: 14:54 - loss: 0.6885 - acc: 0.5440
2880/9333 [========>.....................] - ETA: 14:46 - loss: 0.6875 - acc: 0.5462
2944/9333 [========>.....................] - ETA: 14:37 - loss: 0.6886 - acc: 0.5448
3008/9333 [========>.....................] - ETA: 14:29 - loss: 0.6891 - acc: 0.5439
3072/9333 [========>.....................] - ETA: 14:20 - loss: 0.6902 - acc: 0.5417
3136/9333 [=========>....................] - ETA: 14:12 - loss: 0.6901 - acc: 0.5418
3200/9333 [=========>....................] - ETA: 14:02 - loss: 0.6892 - acc: 0.5450
3264/9333 [=========>....................] - ETA: 13:53 - loss: 0.6898 - acc: 0.5429
3328/9333 [=========>....................] - ETA: 13:44 - loss: 0.6897 - acc: 0.5433
3392/9333 [=========>....................] - ETA: 13:37 - loss: 0.6893 - acc: 0.5445
3456/9333 [==========>...................] - ETA: 13:27 - loss: 0.6894 - acc: 0.5443
3520/9333 [==========>...................] - ETA: 13:19 - loss: 0.6889 - acc: 0.5452
3584/9333 [==========>...................] - ETA: 13:11 - loss: 0.6889 - acc: 0.5452
3648/9333 [==========>...................] - ETA: 13:02 - loss: 0.6895 - acc: 0.5441
3712/9333 [==========>...................] - ETA: 12:52 - loss: 0.6893 - acc: 0.5455
3776/9333 [===========>..................] - ETA: 12:43 - loss: 0.6892 - acc: 0.5461
3840/9333 [===========>..................] - ETA: 12:33 - loss: 0.6889 - acc: 0.5479
3904/9333 [===========>..................] - ETA: 12:23 - loss: 0.6891 - acc: 0.5474
3968/9333 [===========>..................] - ETA: 12:15 - loss: 0.6894 - acc: 0.5461
4032/9333 [===========>..................] - ETA: 12:06 - loss: 0.6894 - acc: 0.5454
4096/9333 [============>.................] - ETA: 11:56 - loss: 0.6891 - acc: 0.5464
4160/9333 [============>.................] - ETA: 11:47 - loss: 0.6889 - acc: 0.5471
4224/9333 [============>.................] - ETA: 11:39 - loss: 0.6892 - acc: 0.5452
4288/9333 [============>.................] - ETA: 11:30 - loss: 0.6896 - acc: 0.5448
4352/9333 [============>.................] - ETA: 11:21 - loss: 0.6899 - acc: 0.5437
4416/9333 [=============>................] - ETA: 11:12 - loss: 0.6899 - acc: 0.5442
4480/9333 [=============>................] - ETA: 11:04 - loss: 0.6899 - acc: 0.5437
4544/9333 [=============>................] - ETA: 10:55 - loss: 0.6897 - acc: 0.5449
4608/9333 [=============>................] - ETA: 10:45 - loss: 0.6902 - acc: 0.5430
4672/9333 [==============>...............] - ETA: 10:36 - loss: 0.6905 - acc: 0.5424
4736/9333 [==============>...............] - ETA: 10:27 - loss: 0.6902 - acc: 0.5433
4800/9333 [==============>...............] - ETA: 10:18 - loss: 0.6901 - acc: 0.5442
4864/9333 [==============>...............] - ETA: 10:10 - loss: 0.6900 - acc: 0.5446
4928/9333 [==============>...............] - ETA: 10:02 - loss: 0.6898 - acc: 0.5450
4992/9333 [===============>..............] - ETA: 9:53 - loss: 0.6897 - acc: 0.5455 
5056/9333 [===============>..............] - ETA: 9:44 - loss: 0.6895 - acc: 0.5459
5120/9333 [===============>..............] - ETA: 9:35 - loss: 0.6894 - acc: 0.5467
5184/9333 [===============>..............] - ETA: 9:26 - loss: 0.6895 - acc: 0.5463
5248/9333 [===============>..............] - ETA: 9:17 - loss: 0.6892 - acc: 0.5476
5312/9333 [================>.............] - ETA: 9:09 - loss: 0.6890 - acc: 0.5484
5376/9333 [================>.............] - ETA: 9:00 - loss: 0.6891 - acc: 0.5484
5440/9333 [================>.............] - ETA: 8:51 - loss: 0.6889 - acc: 0.5489
5504/9333 [================>.............] - ETA: 8:42 - loss: 0.6886 - acc: 0.5492
5568/9333 [================>.............] - ETA: 8:33 - loss: 0.6888 - acc: 0.5483
5632/9333 [=================>............] - ETA: 8:24 - loss: 0.6889 - acc: 0.5479
5696/9333 [=================>............] - ETA: 8:15 - loss: 0.6893 - acc: 0.5463
5760/9333 [=================>............] - ETA: 8:07 - loss: 0.6893 - acc: 0.5462
5824/9333 [=================>............] - ETA: 7:58 - loss: 0.6893 - acc: 0.5457
5888/9333 [=================>............] - ETA: 7:49 - loss: 0.6889 - acc: 0.5460
5952/9333 [==================>...........] - ETA: 7:41 - loss: 0.6890 - acc: 0.5460
6016/9333 [==================>...........] - ETA: 7:32 - loss: 0.6892 - acc: 0.5455
6080/9333 [==================>...........] - ETA: 7:24 - loss: 0.6896 - acc: 0.5442
6144/9333 [==================>...........] - ETA: 7:15 - loss: 0.6896 - acc: 0.5443
6208/9333 [==================>...........] - ETA: 7:06 - loss: 0.6896 - acc: 0.5446
6272/9333 [===================>..........] - ETA: 6:57 - loss: 0.6898 - acc: 0.5443
6336/9333 [===================>..........] - ETA: 6:48 - loss: 0.6897 - acc: 0.5443
6400/9333 [===================>..........] - ETA: 6:39 - loss: 0.6897 - acc: 0.5447
6464/9333 [===================>..........] - ETA: 6:31 - loss: 0.6900 - acc: 0.5439
6528/9333 [===================>..........] - ETA: 6:22 - loss: 0.6900 - acc: 0.5437
6592/9333 [====================>.........] - ETA: 6:13 - loss: 0.6902 - acc: 0.5432
6656/9333 [====================>.........] - ETA: 6:04 - loss: 0.6902 - acc: 0.5427
6720/9333 [====================>.........] - ETA: 5:55 - loss: 0.6902 - acc: 0.5423
6784/9333 [====================>.........] - ETA: 5:46 - loss: 0.6901 - acc: 0.5426
6848/9333 [=====================>........] - ETA: 5:37 - loss: 0.6899 - acc: 0.5429
6912/9333 [=====================>........] - ETA: 5:28 - loss: 0.6898 - acc: 0.5434
6976/9333 [=====================>........] - ETA: 5:19 - loss: 0.6898 - acc: 0.5439
7040/9333 [=====================>........] - ETA: 5:11 - loss: 0.6898 - acc: 0.5435
7104/9333 [=====================>........] - ETA: 5:02 - loss: 0.6896 - acc: 0.5445
7168/9333 [======================>.......] - ETA: 4:53 - loss: 0.6898 - acc: 0.5442
7232/9333 [======================>.......] - ETA: 4:44 - loss: 0.6899 - acc: 0.5442
7296/9333 [======================>.......] - ETA: 4:36 - loss: 0.6898 - acc: 0.5439
7360/9333 [======================>.......] - ETA: 4:27 - loss: 0.6896 - acc: 0.5450
7424/9333 [======================>.......] - ETA: 4:19 - loss: 0.6896 - acc: 0.5454
7488/9333 [=======================>......] - ETA: 4:10 - loss: 0.6898 - acc: 0.5453
7552/9333 [=======================>......] - ETA: 4:01 - loss: 0.6896 - acc: 0.5459
7616/9333 [=======================>......] - ETA: 3:52 - loss: 0.6897 - acc: 0.5452
7680/9333 [=======================>......] - ETA: 3:44 - loss: 0.6897 - acc: 0.5453
7744/9333 [=======================>......] - ETA: 3:35 - loss: 0.6898 - acc: 0.5455
7808/9333 [========================>.....] - ETA: 3:26 - loss: 0.6897 - acc: 0.5460
7872/9333 [========================>.....] - ETA: 3:18 - loss: 0.6894 - acc: 0.5474
7936/9333 [========================>.....] - ETA: 3:09 - loss: 0.6892 - acc: 0.5480
8000/9333 [========================>.....] - ETA: 3:00 - loss: 0.6891 - acc: 0.5481
8064/9333 [========================>.....] - ETA: 2:52 - loss: 0.6890 - acc: 0.5484
8128/9333 [=========================>....] - ETA: 2:43 - loss: 0.6892 - acc: 0.5472
8192/9333 [=========================>....] - ETA: 2:34 - loss: 0.6891 - acc: 0.5471
8256/9333 [=========================>....] - ETA: 2:25 - loss: 0.6892 - acc: 0.5471
8320/9333 [=========================>....] - ETA: 2:17 - loss: 0.6890 - acc: 0.5478
8384/9333 [=========================>....] - ETA: 2:08 - loss: 0.6890 - acc: 0.5475
8448/9333 [==========================>...] - ETA: 1:59 - loss: 0.6889 - acc: 0.5477
8512/9333 [==========================>...] - ETA: 1:51 - loss: 0.6887 - acc: 0.5484
8576/9333 [==========================>...] - ETA: 1:42 - loss: 0.6887 - acc: 0.5485
8640/9333 [==========================>...] - ETA: 1:33 - loss: 0.6886 - acc: 0.5497
8704/9333 [==========================>...] - ETA: 1:25 - loss: 0.6884 - acc: 0.5502
8768/9333 [===========================>..] - ETA: 1:16 - loss: 0.6884 - acc: 0.5500
8832/9333 [===========================>..] - ETA: 1:07 - loss: 0.6883 - acc: 0.5499
8896/9333 [===========================>..] - ETA: 59s - loss: 0.6884 - acc: 0.5495 
8960/9333 [===========================>..] - ETA: 50s - loss: 0.6883 - acc: 0.5496
9024/9333 [============================>.] - ETA: 41s - loss: 0.6883 - acc: 0.5496
9088/9333 [============================>.] - ETA: 33s - loss: 0.6883 - acc: 0.5494
9152/9333 [============================>.] - ETA: 24s - loss: 0.6884 - acc: 0.5486
9216/9333 [============================>.] - ETA: 15s - loss: 0.6885 - acc: 0.5480
9280/9333 [============================>.] - ETA: 7s - loss: 0.6883 - acc: 0.5484 
9333/9333 [==============================] - 1305s 140ms/step - loss: 0.6884 - acc: 0.5482 - val_loss: 0.6872 - val_acc: 0.5400

Epoch 00009: val_acc did not improve from 0.54966
Epoch 10/10

  64/9333 [..............................] - ETA: 19:46 - loss: 0.6856 - acc: 0.5781
 128/9333 [..............................] - ETA: 20:06 - loss: 0.7040 - acc: 0.5000
 192/9333 [..............................] - ETA: 20:25 - loss: 0.6902 - acc: 0.5625
 256/9333 [..............................] - ETA: 20:19 - loss: 0.6798 - acc: 0.5703
 320/9333 [>.............................] - ETA: 19:59 - loss: 0.6806 - acc: 0.5656
 384/9333 [>.............................] - ETA: 19:51 - loss: 0.6829 - acc: 0.5599
 448/9333 [>.............................] - ETA: 19:43 - loss: 0.6835 - acc: 0.5603
 512/9333 [>.............................] - ETA: 19:31 - loss: 0.6852 - acc: 0.5527
 576/9333 [>.............................] - ETA: 19:23 - loss: 0.6845 - acc: 0.5556
 640/9333 [=>............................] - ETA: 19:08 - loss: 0.6829 - acc: 0.5578
 704/9333 [=>............................] - ETA: 18:55 - loss: 0.6827 - acc: 0.5568
 768/9333 [=>............................] - ETA: 18:47 - loss: 0.6819 - acc: 0.5625
 832/9333 [=>............................] - ETA: 18:37 - loss: 0.6830 - acc: 0.5601
 896/9333 [=>............................] - ETA: 18:42 - loss: 0.6826 - acc: 0.5658
 960/9333 [==>...........................] - ETA: 18:30 - loss: 0.6825 - acc: 0.5646
1024/9333 [==>...........................] - ETA: 18:17 - loss: 0.6824 - acc: 0.5615
1088/9333 [==>...........................] - ETA: 18:07 - loss: 0.6825 - acc: 0.5607
1152/9333 [==>...........................] - ETA: 17:57 - loss: 0.6828 - acc: 0.5616
1216/9333 [==>...........................] - ETA: 17:48 - loss: 0.6831 - acc: 0.5617
1280/9333 [===>..........................] - ETA: 17:39 - loss: 0.6846 - acc: 0.5586
1344/9333 [===>..........................] - ETA: 17:28 - loss: 0.6853 - acc: 0.5595
1408/9333 [===>..........................] - ETA: 17:17 - loss: 0.6846 - acc: 0.5611
1472/9333 [===>..........................] - ETA: 17:09 - loss: 0.6848 - acc: 0.5591
1536/9333 [===>..........................] - ETA: 16:59 - loss: 0.6849 - acc: 0.5566
1600/9333 [====>.........................] - ETA: 16:51 - loss: 0.6856 - acc: 0.5519
1664/9333 [====>.........................] - ETA: 16:42 - loss: 0.6858 - acc: 0.5517
1728/9333 [====>.........................] - ETA: 16:35 - loss: 0.6856 - acc: 0.5527
1792/9333 [====>.........................] - ETA: 16:29 - loss: 0.6848 - acc: 0.5564
1856/9333 [====>.........................] - ETA: 16:20 - loss: 0.6855 - acc: 0.5533
1920/9333 [=====>........................] - ETA: 16:12 - loss: 0.6851 - acc: 0.5542
1984/9333 [=====>........................] - ETA: 16:03 - loss: 0.6852 - acc: 0.5534
2048/9333 [=====>........................] - ETA: 15:55 - loss: 0.6848 - acc: 0.5542
2112/9333 [=====>........................] - ETA: 15:47 - loss: 0.6856 - acc: 0.5502
2176/9333 [=====>........................] - ETA: 15:38 - loss: 0.6861 - acc: 0.5487
2240/9333 [======>.......................] - ETA: 15:31 - loss: 0.6863 - acc: 0.5478
2304/9333 [======>.......................] - ETA: 15:23 - loss: 0.6866 - acc: 0.5473
2368/9333 [======>.......................] - ETA: 15:14 - loss: 0.6863 - acc: 0.5490
2432/9333 [======>.......................] - ETA: 15:04 - loss: 0.6860 - acc: 0.5498
2496/9333 [=======>......................] - ETA: 14:56 - loss: 0.6857 - acc: 0.5517
2560/9333 [=======>......................] - ETA: 14:48 - loss: 0.6857 - acc: 0.5516
2624/9333 [=======>......................] - ETA: 14:39 - loss: 0.6845 - acc: 0.5549
2688/9333 [=======>......................] - ETA: 14:30 - loss: 0.6841 - acc: 0.5580
2752/9333 [=======>......................] - ETA: 14:20 - loss: 0.6841 - acc: 0.5574
2816/9333 [========>.....................] - ETA: 14:11 - loss: 0.6845 - acc: 0.5565
2880/9333 [========>.....................] - ETA: 14:05 - loss: 0.6842 - acc: 0.5556
2944/9333 [========>.....................] - ETA: 13:58 - loss: 0.6844 - acc: 0.5537
3008/9333 [========>.....................] - ETA: 13:51 - loss: 0.6845 - acc: 0.5532
3072/9333 [========>.....................] - ETA: 13:43 - loss: 0.6849 - acc: 0.5534
3136/9333 [=========>....................] - ETA: 13:34 - loss: 0.6855 - acc: 0.5507
3200/9333 [=========>....................] - ETA: 13:27 - loss: 0.6853 - acc: 0.5513
3264/9333 [=========>....................] - ETA: 13:18 - loss: 0.6847 - acc: 0.5545
3328/9333 [=========>....................] - ETA: 13:10 - loss: 0.6856 - acc: 0.5517
3392/9333 [=========>....................] - ETA: 13:02 - loss: 0.6854 - acc: 0.5525
3456/9333 [==========>...................] - ETA: 12:53 - loss: 0.6855 - acc: 0.5524
3520/9333 [==========>...................] - ETA: 12:45 - loss: 0.6855 - acc: 0.5534
3584/9333 [==========>...................] - ETA: 12:36 - loss: 0.6854 - acc: 0.5539
3648/9333 [==========>...................] - ETA: 12:28 - loss: 0.6857 - acc: 0.5532
3712/9333 [==========>...................] - ETA: 12:19 - loss: 0.6858 - acc: 0.5531
3776/9333 [===========>..................] - ETA: 12:10 - loss: 0.6861 - acc: 0.5522
3840/9333 [===========>..................] - ETA: 12:02 - loss: 0.6866 - acc: 0.5521
3904/9333 [===========>..................] - ETA: 11:53 - loss: 0.6868 - acc: 0.5525
3968/9333 [===========>..................] - ETA: 11:44 - loss: 0.6869 - acc: 0.5524
4032/9333 [===========>..................] - ETA: 11:36 - loss: 0.6864 - acc: 0.5538
4096/9333 [============>.................] - ETA: 11:27 - loss: 0.6863 - acc: 0.5537
4160/9333 [============>.................] - ETA: 11:19 - loss: 0.6862 - acc: 0.5538
4224/9333 [============>.................] - ETA: 11:11 - loss: 0.6859 - acc: 0.5542
4288/9333 [============>.................] - ETA: 11:02 - loss: 0.6861 - acc: 0.5539
4352/9333 [============>.................] - ETA: 10:54 - loss: 0.6860 - acc: 0.5533
4416/9333 [=============>................] - ETA: 10:45 - loss: 0.6859 - acc: 0.5532
4480/9333 [=============>................] - ETA: 10:37 - loss: 0.6859 - acc: 0.5538
4544/9333 [=============>................] - ETA: 10:29 - loss: 0.6857 - acc: 0.5544
4608/9333 [=============>................] - ETA: 10:20 - loss: 0.6859 - acc: 0.5536
4672/9333 [==============>...............] - ETA: 10:12 - loss: 0.6860 - acc: 0.5535
4736/9333 [==============>...............] - ETA: 10:04 - loss: 0.6858 - acc: 0.5541
4800/9333 [==============>...............] - ETA: 9:56 - loss: 0.6859 - acc: 0.5535 
4864/9333 [==============>...............] - ETA: 9:47 - loss: 0.6860 - acc: 0.5528
4928/9333 [==============>...............] - ETA: 9:39 - loss: 0.6867 - acc: 0.5503
4992/9333 [===============>..............] - ETA: 9:31 - loss: 0.6871 - acc: 0.5489
5056/9333 [===============>..............] - ETA: 9:23 - loss: 0.6868 - acc: 0.5500
5120/9333 [===============>..............] - ETA: 9:16 - loss: 0.6867 - acc: 0.5506
5184/9333 [===============>..............] - ETA: 9:08 - loss: 0.6870 - acc: 0.5502
5248/9333 [===============>..............] - ETA: 9:00 - loss: 0.6871 - acc: 0.5499
5312/9333 [================>.............] - ETA: 8:52 - loss: 0.6871 - acc: 0.5499
5376/9333 [================>.............] - ETA: 8:44 - loss: 0.6875 - acc: 0.5484
5440/9333 [================>.............] - ETA: 8:35 - loss: 0.6869 - acc: 0.5502
5504/9333 [================>.............] - ETA: 8:27 - loss: 0.6871 - acc: 0.5492
5568/9333 [================>.............] - ETA: 8:18 - loss: 0.6870 - acc: 0.5503
5632/9333 [=================>............] - ETA: 8:10 - loss: 0.6872 - acc: 0.5494
5696/9333 [=================>............] - ETA: 8:02 - loss: 0.6872 - acc: 0.5485
5760/9333 [=================>............] - ETA: 7:54 - loss: 0.6875 - acc: 0.5474
5824/9333 [=================>............] - ETA: 7:45 - loss: 0.6874 - acc: 0.5481
5888/9333 [=================>............] - ETA: 7:37 - loss: 0.6875 - acc: 0.5479
5952/9333 [==================>...........] - ETA: 7:28 - loss: 0.6876 - acc: 0.5472
6016/9333 [==================>...........] - ETA: 7:20 - loss: 0.6877 - acc: 0.5472
6080/9333 [==================>...........] - ETA: 7:12 - loss: 0.6875 - acc: 0.5484
6144/9333 [==================>...........] - ETA: 7:04 - loss: 0.6876 - acc: 0.5482
6208/9333 [==================>...........] - ETA: 6:55 - loss: 0.6877 - acc: 0.5478
6272/9333 [===================>..........] - ETA: 6:47 - loss: 0.6877 - acc: 0.5485
6336/9333 [===================>..........] - ETA: 6:39 - loss: 0.6875 - acc: 0.5494
6400/9333 [===================>..........] - ETA: 6:30 - loss: 0.6873 - acc: 0.5500
6464/9333 [===================>..........] - ETA: 6:22 - loss: 0.6873 - acc: 0.5500
6528/9333 [===================>..........] - ETA: 6:14 - loss: 0.6874 - acc: 0.5492
6592/9333 [====================>.........] - ETA: 6:06 - loss: 0.6878 - acc: 0.5479
6656/9333 [====================>.........] - ETA: 5:58 - loss: 0.6878 - acc: 0.5479
6720/9333 [====================>.........] - ETA: 5:49 - loss: 0.6880 - acc: 0.5478
6784/9333 [====================>.........] - ETA: 5:41 - loss: 0.6880 - acc: 0.5478
6848/9333 [=====================>........] - ETA: 5:32 - loss: 0.6877 - acc: 0.5488
6912/9333 [=====================>........] - ETA: 5:24 - loss: 0.6880 - acc: 0.5475
6976/9333 [=====================>........] - ETA: 5:15 - loss: 0.6881 - acc: 0.5472
7040/9333 [=====================>........] - ETA: 5:07 - loss: 0.6882 - acc: 0.5473
7104/9333 [=====================>........] - ETA: 4:58 - loss: 0.6883 - acc: 0.5472
7168/9333 [======================>.......] - ETA: 4:50 - loss: 0.6881 - acc: 0.5470
7232/9333 [======================>.......] - ETA: 4:41 - loss: 0.6882 - acc: 0.5465
7296/9333 [======================>.......] - ETA: 4:33 - loss: 0.6880 - acc: 0.5469
7360/9333 [======================>.......] - ETA: 4:24 - loss: 0.6880 - acc: 0.5467
7424/9333 [======================>.......] - ETA: 4:16 - loss: 0.6879 - acc: 0.5467
7488/9333 [=======================>......] - ETA: 4:07 - loss: 0.6878 - acc: 0.5478
7552/9333 [=======================>......] - ETA: 3:59 - loss: 0.6877 - acc: 0.5485
7616/9333 [=======================>......] - ETA: 3:50 - loss: 0.6876 - acc: 0.5496
7680/9333 [=======================>......] - ETA: 3:41 - loss: 0.6879 - acc: 0.5490
7744/9333 [=======================>......] - ETA: 3:33 - loss: 0.6879 - acc: 0.5487
7808/9333 [========================>.....] - ETA: 3:24 - loss: 0.6880 - acc: 0.5485
7872/9333 [========================>.....] - ETA: 3:16 - loss: 0.6877 - acc: 0.5490
7936/9333 [========================>.....] - ETA: 3:07 - loss: 0.6876 - acc: 0.5491
8000/9333 [========================>.....] - ETA: 2:59 - loss: 0.6875 - acc: 0.5495
8064/9333 [========================>.....] - ETA: 2:50 - loss: 0.6874 - acc: 0.5496
8128/9333 [=========================>....] - ETA: 2:41 - loss: 0.6872 - acc: 0.5506
8192/9333 [=========================>....] - ETA: 2:33 - loss: 0.6872 - acc: 0.5507
8256/9333 [=========================>....] - ETA: 2:24 - loss: 0.6875 - acc: 0.5504
8320/9333 [=========================>....] - ETA: 2:16 - loss: 0.6875 - acc: 0.5505
8384/9333 [=========================>....] - ETA: 2:07 - loss: 0.6875 - acc: 0.5505
8448/9333 [==========================>...] - ETA: 1:59 - loss: 0.6874 - acc: 0.5508
8512/9333 [==========================>...] - ETA: 1:50 - loss: 0.6873 - acc: 0.5510
8576/9333 [==========================>...] - ETA: 1:41 - loss: 0.6875 - acc: 0.5504
8640/9333 [==========================>...] - ETA: 1:33 - loss: 0.6877 - acc: 0.5495
8704/9333 [==========================>...] - ETA: 1:24 - loss: 0.6877 - acc: 0.5495
8768/9333 [===========================>..] - ETA: 1:16 - loss: 0.6874 - acc: 0.5501
8832/9333 [===========================>..] - ETA: 1:07 - loss: 0.6875 - acc: 0.5498
8896/9333 [===========================>..] - ETA: 58s - loss: 0.6874 - acc: 0.5499 
8960/9333 [===========================>..] - ETA: 50s - loss: 0.6872 - acc: 0.5504
9024/9333 [============================>.] - ETA: 41s - loss: 0.6872 - acc: 0.5502
9088/9333 [============================>.] - ETA: 32s - loss: 0.6870 - acc: 0.5508
9152/9333 [============================>.] - ETA: 24s - loss: 0.6869 - acc: 0.5511
9216/9333 [============================>.] - ETA: 15s - loss: 0.6869 - acc: 0.5510
9280/9333 [============================>.] - ETA: 7s - loss: 0.6870 - acc: 0.5509 
9333/9333 [==============================] - 1299s 139ms/step - loss: 0.6868 - acc: 0.5517 - val_loss: 0.6847 - val_acc: 0.5333

Epoch 00010: val_acc did not improve from 0.54966
样本个数 1296
样本个数 2592
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f81d4537450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f81d4537450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f81d44cc810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f81d44cc810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cc1f0750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cc1f0750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cc127250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cc127250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81cbd1da90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81cbd1da90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81c3acd5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81c3acd5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81c3b8d950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81c3b8d950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81c3af1dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81c3af1dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81c34c8310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81c34c8310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81c3a1c550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81c3a1c550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ca0673950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ca0673950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81cbbf51d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81cbbf51d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ca04509d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ca04509d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81c3a1c590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81c3a1c590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81c367bdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81c367bdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81c3752850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81c3752850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ca0420f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ca0420f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81c386ccd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81c386ccd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81c34a91d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81c34a91d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81c345ee50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81c345ee50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81c3432410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81c3432410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81c374da50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81c374da50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81bb200190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81bb200190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81bb0da150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81bb0da150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81bb1aa450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81bb1aa450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81bb38cb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81bb38cb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81bb0dadd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81bb0dadd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81bae746d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81bae746d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81bae226d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81bae226d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81bafd52d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81bafd52d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81bacf72d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81bacf72d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81bb0dc610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81bb0dc610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81badeb2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81badeb2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a2b68e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a2b68e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a2a91b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a2a91b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a2a0e510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a2a0e510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81bad83a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81bad83a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a2998d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a2998d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a27c4b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a27c4b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a2759990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a2759990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a278efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a278efd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81a27c8c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81a27c8c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a2667690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a2667690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a25c3510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a25c3510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a2315e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a2315e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a24baf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a24baf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81a2b1ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81a2b1ded0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ca0429e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ca0429e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a2459510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a2459510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a23cfe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a23cfe10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a2205e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a2205e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81a2265850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81a2265850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a2446210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a2446210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a1e434d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a1e434d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a1f4b9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a1f4b9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a1d1c650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a1d1c650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81a20e7dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81a20e7dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a1d19350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a1d19350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a1adbbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81a1adbbd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a19fba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81a19fba90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81baffe790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81baffe790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81a1adbed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81a1adbed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a19f30d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a19f30d0>>: AttributeError: module 'gast' has no attribute 'Str'
window_select17-21.py:152: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/2592 [..............................] - ETA: 6:19
 128/2592 [>.............................] - ETA: 4:01
 192/2592 [=>............................] - ETA: 3:13
 256/2592 [=>............................] - ETA: 2:49
 320/2592 [==>...........................] - ETA: 2:33
 384/2592 [===>..........................] - ETA: 2:23
 448/2592 [====>.........................] - ETA: 2:13
 512/2592 [====>.........................] - ETA: 2:04
 576/2592 [=====>........................] - ETA: 2:00
 640/2592 [======>.......................] - ETA: 1:54
 704/2592 [=======>......................] - ETA: 1:47
 768/2592 [=======>......................] - ETA: 1:42
 832/2592 [========>.....................] - ETA: 1:37
 896/2592 [=========>....................] - ETA: 1:32
 960/2592 [==========>...................] - ETA: 1:28
1024/2592 [==========>...................] - ETA: 1:24
1088/2592 [===========>..................] - ETA: 1:20
1152/2592 [============>.................] - ETA: 1:16
1216/2592 [=============>................] - ETA: 1:12
1280/2592 [=============>................] - ETA: 1:08
1344/2592 [==============>...............] - ETA: 1:04
1408/2592 [===============>..............] - ETA: 1:00
1472/2592 [================>.............] - ETA: 57s 
1536/2592 [================>.............] - ETA: 53s
1600/2592 [=================>............] - ETA: 50s
1664/2592 [==================>...........] - ETA: 46s
1728/2592 [===================>..........] - ETA: 43s
1792/2592 [===================>..........] - ETA: 40s
1856/2592 [====================>.........] - ETA: 36s
1920/2592 [=====================>........] - ETA: 33s
1984/2592 [=====================>........] - ETA: 30s
2048/2592 [======================>.......] - ETA: 26s
2112/2592 [=======================>......] - ETA: 23s
2176/2592 [========================>.....] - ETA: 20s
2240/2592 [========================>.....] - ETA: 17s
2304/2592 [=========================>....] - ETA: 14s
2368/2592 [==========================>...] - ETA: 10s
2432/2592 [===========================>..] - ETA: 7s 
2496/2592 [===========================>..] - ETA: 4s
2560/2592 [============================>.] - ETA: 1s
2592/2592 [==============================] - 127s 49ms/step
loss: 0.6897077398535646
acc: 0.5420524691358025
样本个数 5185
样本个数 10370
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7ca041de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7ca041de90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7ca04245d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7ca04245d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cc1f0cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cc1f0cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f817e95d990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f817e95d990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ca0249a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ca0249a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ca042fad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ca042fad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f817e95ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f817e95ded0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817e979c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817e979c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cc058f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cc058f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81d449c450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81d449c450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cc120b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cc120b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f817e979e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f817e979e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cbf98890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cbf98890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cbebb050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cbebb050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81cc0fc110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81cc0fc110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81d4406e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81d4406e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81cbebbd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81cbebbd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81d4437910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81d4437910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cbf231d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81cbf231d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ca0087e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ca0087e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cc2a9d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cc2a9d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81cc0e6fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81cc0e6fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ca007d490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ca007d490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ca00ed790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ca00ed790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c98652e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c98652e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c98460610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c98460610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ca00ed250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ca00ed250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c985ab610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c985ab610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c98553d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c98553d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c9855ef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c9855ef50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c98369e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c98369e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c985538d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c985538d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c98251a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c98251a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c98241f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c98241f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ca00ed650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ca00ed650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c7872a750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c7872a750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c98241150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c98241150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c786d86d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c786d86d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c78528550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c78528550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c78715350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c78715350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c78725b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c78725b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c78528290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c78528290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c98241b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c98241b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c781c76d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c781c76d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c784ec650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c784ec650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c7821b510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c7821b510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c98549290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c98549290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c780ebd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c780ebd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c781815d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c781815d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c780ef510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c780ef510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c60759110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c60759110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c7845f810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c7845f810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c78299590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c78299590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c6038b410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c6038b410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c603eb590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c603eb590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c602574d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c602574d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c60458bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c60458bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c6020fe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c6020fe50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c602deed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c602deed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c207e4b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c207e4b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c602120d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c602120d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c602f3e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c602f3e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c207a9ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c207a9ad0>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 9333 samples, validate on 1037 samples
Epoch 1/10

  64/9333 [..............................] - ETA: 1:19:23 - loss: 0.7550 - acc: 0.5312
 128/9333 [..............................] - ETA: 51:15 - loss: 0.7699 - acc: 0.5781  
 192/9333 [..............................] - ETA: 41:35 - loss: 0.8072 - acc: 0.5365
 256/9333 [..............................] - ETA: 37:09 - loss: 0.7867 - acc: 0.5352
 320/9333 [>.............................] - ETA: 34:13 - loss: 0.7837 - acc: 0.5281
 384/9333 [>.............................] - ETA: 32:10 - loss: 0.7782 - acc: 0.5260
 448/9333 [>.............................] - ETA: 30:42 - loss: 0.7756 - acc: 0.5156
 512/9333 [>.............................] - ETA: 29:29 - loss: 0.7585 - acc: 0.5254
 576/9333 [>.............................] - ETA: 28:38 - loss: 0.7578 - acc: 0.5156
 640/9333 [=>............................] - ETA: 27:56 - loss: 0.7556 - acc: 0.5172
 704/9333 [=>............................] - ETA: 27:14 - loss: 0.7513 - acc: 0.5256
 768/9333 [=>............................] - ETA: 26:36 - loss: 0.7497 - acc: 0.5234
 832/9333 [=>............................] - ETA: 26:06 - loss: 0.7483 - acc: 0.5204
 896/9333 [=>............................] - ETA: 25:29 - loss: 0.7422 - acc: 0.5290
 960/9333 [==>...........................] - ETA: 24:58 - loss: 0.7429 - acc: 0.5250
1024/9333 [==>...........................] - ETA: 24:32 - loss: 0.7422 - acc: 0.5293
1088/9333 [==>...........................] - ETA: 24:08 - loss: 0.7390 - acc: 0.5285
1152/9333 [==>...........................] - ETA: 23:48 - loss: 0.7397 - acc: 0.5269
1216/9333 [==>...........................] - ETA: 23:24 - loss: 0.7408 - acc: 0.5206
1280/9333 [===>..........................] - ETA: 23:06 - loss: 0.7388 - acc: 0.5242
1344/9333 [===>..........................] - ETA: 22:50 - loss: 0.7371 - acc: 0.5246
1408/9333 [===>..........................] - ETA: 22:32 - loss: 0.7366 - acc: 0.5270
1472/9333 [===>..........................] - ETA: 22:16 - loss: 0.7360 - acc: 0.5265
1536/9333 [===>..........................] - ETA: 22:05 - loss: 0.7358 - acc: 0.5241
1600/9333 [====>.........................] - ETA: 21:52 - loss: 0.7346 - acc: 0.5238
1664/9333 [====>.........................] - ETA: 21:38 - loss: 0.7340 - acc: 0.5228
1728/9333 [====>.........................] - ETA: 21:28 - loss: 0.7332 - acc: 0.5231
1792/9333 [====>.........................] - ETA: 21:14 - loss: 0.7314 - acc: 0.5246
1856/9333 [====>.........................] - ETA: 21:01 - loss: 0.7327 - acc: 0.5216
1920/9333 [=====>........................] - ETA: 20:47 - loss: 0.7325 - acc: 0.5224
1984/9333 [=====>........................] - ETA: 20:33 - loss: 0.7326 - acc: 0.5222
2048/9333 [=====>........................] - ETA: 20:19 - loss: 0.7335 - acc: 0.5205
2112/9333 [=====>........................] - ETA: 20:07 - loss: 0.7313 - acc: 0.5208
2176/9333 [=====>........................] - ETA: 19:55 - loss: 0.7315 - acc: 0.5179
2240/9333 [======>.......................] - ETA: 19:44 - loss: 0.7299 - acc: 0.5210
2304/9333 [======>.......................] - ETA: 19:30 - loss: 0.7289 - acc: 0.5204
2368/9333 [======>.......................] - ETA: 19:16 - loss: 0.7295 - acc: 0.5186
2432/9333 [======>.......................] - ETA: 19:04 - loss: 0.7287 - acc: 0.5193
2496/9333 [=======>......................] - ETA: 18:50 - loss: 0.7276 - acc: 0.5208
2560/9333 [=======>......................] - ETA: 18:38 - loss: 0.7275 - acc: 0.5215
2624/9333 [=======>......................] - ETA: 18:24 - loss: 0.7262 - acc: 0.5225
2688/9333 [=======>......................] - ETA: 18:09 - loss: 0.7259 - acc: 0.5231
2752/9333 [=======>......................] - ETA: 17:57 - loss: 0.7267 - acc: 0.5214
2816/9333 [========>.....................] - ETA: 17:47 - loss: 0.7257 - acc: 0.5224
2880/9333 [========>.....................] - ETA: 17:36 - loss: 0.7235 - acc: 0.5247
2944/9333 [========>.....................] - ETA: 17:23 - loss: 0.7233 - acc: 0.5241
3008/9333 [========>.....................] - ETA: 17:11 - loss: 0.7226 - acc: 0.5246
3072/9333 [========>.....................] - ETA: 17:00 - loss: 0.7217 - acc: 0.5267
3136/9333 [=========>....................] - ETA: 16:48 - loss: 0.7225 - acc: 0.5258
3200/9333 [=========>....................] - ETA: 16:37 - loss: 0.7217 - acc: 0.5266
3264/9333 [=========>....................] - ETA: 16:25 - loss: 0.7209 - acc: 0.5273
3328/9333 [=========>....................] - ETA: 16:14 - loss: 0.7209 - acc: 0.5255
3392/9333 [=========>....................] - ETA: 16:02 - loss: 0.7208 - acc: 0.5256
3456/9333 [==========>...................] - ETA: 15:51 - loss: 0.7209 - acc: 0.5258
3520/9333 [==========>...................] - ETA: 15:41 - loss: 0.7212 - acc: 0.5259
3584/9333 [==========>...................] - ETA: 15:30 - loss: 0.7211 - acc: 0.5262
3648/9333 [==========>...................] - ETA: 15:18 - loss: 0.7211 - acc: 0.5258
3712/9333 [==========>...................] - ETA: 15:06 - loss: 0.7214 - acc: 0.5242
3776/9333 [===========>..................] - ETA: 14:55 - loss: 0.7221 - acc: 0.5241
3840/9333 [===========>..................] - ETA: 14:44 - loss: 0.7225 - acc: 0.5232
3904/9333 [===========>..................] - ETA: 14:34 - loss: 0.7227 - acc: 0.5223
3968/9333 [===========>..................] - ETA: 14:23 - loss: 0.7224 - acc: 0.5217
4032/9333 [===========>..................] - ETA: 14:13 - loss: 0.7219 - acc: 0.5223
4096/9333 [============>.................] - ETA: 14:02 - loss: 0.7216 - acc: 0.5220
4160/9333 [============>.................] - ETA: 13:50 - loss: 0.7217 - acc: 0.5212
4224/9333 [============>.................] - ETA: 13:40 - loss: 0.7218 - acc: 0.5211
4288/9333 [============>.................] - ETA: 13:29 - loss: 0.7215 - acc: 0.5215
4352/9333 [============>.................] - ETA: 13:18 - loss: 0.7215 - acc: 0.5200
4416/9333 [=============>................] - ETA: 13:07 - loss: 0.7212 - acc: 0.5206
4480/9333 [=============>................] - ETA: 12:56 - loss: 0.7210 - acc: 0.5199
4544/9333 [=============>................] - ETA: 12:46 - loss: 0.7207 - acc: 0.5207
4608/9333 [=============>................] - ETA: 12:35 - loss: 0.7206 - acc: 0.5206
4672/9333 [==============>...............] - ETA: 12:25 - loss: 0.7206 - acc: 0.5199
4736/9333 [==============>...............] - ETA: 12:14 - loss: 0.7206 - acc: 0.5201
4800/9333 [==============>...............] - ETA: 12:05 - loss: 0.7208 - acc: 0.5200
4864/9333 [==============>...............] - ETA: 11:54 - loss: 0.7205 - acc: 0.5191
4928/9333 [==============>...............] - ETA: 11:43 - loss: 0.7205 - acc: 0.5187
4992/9333 [===============>..............] - ETA: 11:33 - loss: 0.7204 - acc: 0.5182
5056/9333 [===============>..............] - ETA: 11:23 - loss: 0.7197 - acc: 0.5190
5120/9333 [===============>..............] - ETA: 11:13 - loss: 0.7195 - acc: 0.5193
5184/9333 [===============>..............] - ETA: 11:02 - loss: 0.7191 - acc: 0.5204
5248/9333 [===============>..............] - ETA: 10:52 - loss: 0.7194 - acc: 0.5189
5312/9333 [================>.............] - ETA: 10:42 - loss: 0.7195 - acc: 0.5184
5376/9333 [================>.............] - ETA: 10:32 - loss: 0.7194 - acc: 0.5175
5440/9333 [================>.............] - ETA: 10:21 - loss: 0.7195 - acc: 0.5162
5504/9333 [================>.............] - ETA: 10:11 - loss: 0.7191 - acc: 0.5173
5568/9333 [================>.............] - ETA: 10:01 - loss: 0.7190 - acc: 0.5167
5632/9333 [=================>............] - ETA: 9:50 - loss: 0.7188 - acc: 0.5170 
5696/9333 [=================>............] - ETA: 9:39 - loss: 0.7183 - acc: 0.5186
5760/9333 [=================>............] - ETA: 9:28 - loss: 0.7179 - acc: 0.5188
5824/9333 [=================>............] - ETA: 9:18 - loss: 0.7176 - acc: 0.5191
5888/9333 [=================>............] - ETA: 9:08 - loss: 0.7176 - acc: 0.5182
5952/9333 [==================>...........] - ETA: 8:58 - loss: 0.7175 - acc: 0.5178
6016/9333 [==================>...........] - ETA: 8:48 - loss: 0.7171 - acc: 0.5185
6080/9333 [==================>...........] - ETA: 8:37 - loss: 0.7170 - acc: 0.5184
6144/9333 [==================>...........] - ETA: 8:27 - loss: 0.7173 - acc: 0.5182
6208/9333 [==================>...........] - ETA: 8:17 - loss: 0.7173 - acc: 0.5174
6272/9333 [===================>..........] - ETA: 8:06 - loss: 0.7170 - acc: 0.5175
6336/9333 [===================>..........] - ETA: 7:56 - loss: 0.7172 - acc: 0.5170
6400/9333 [===================>..........] - ETA: 7:46 - loss: 0.7169 - acc: 0.5177
6464/9333 [===================>..........] - ETA: 7:36 - loss: 0.7163 - acc: 0.5186
6528/9333 [===================>..........] - ETA: 7:26 - loss: 0.7163 - acc: 0.5188
6592/9333 [====================>.........] - ETA: 7:15 - loss: 0.7158 - acc: 0.5193
6656/9333 [====================>.........] - ETA: 7:05 - loss: 0.7160 - acc: 0.5186
6720/9333 [====================>.........] - ETA: 6:55 - loss: 0.7160 - acc: 0.5180
6784/9333 [====================>.........] - ETA: 6:44 - loss: 0.7158 - acc: 0.5174
6848/9333 [=====================>........] - ETA: 6:34 - loss: 0.7156 - acc: 0.5177
6912/9333 [=====================>........] - ETA: 6:24 - loss: 0.7154 - acc: 0.5177
6976/9333 [=====================>........] - ETA: 6:14 - loss: 0.7155 - acc: 0.5171
7040/9333 [=====================>........] - ETA: 6:03 - loss: 0.7155 - acc: 0.5168
7104/9333 [=====================>........] - ETA: 5:53 - loss: 0.7156 - acc: 0.5155
7168/9333 [======================>.......] - ETA: 5:43 - loss: 0.7151 - acc: 0.5167
7232/9333 [======================>.......] - ETA: 5:32 - loss: 0.7152 - acc: 0.5160
7296/9333 [======================>.......] - ETA: 5:22 - loss: 0.7151 - acc: 0.5164
7360/9333 [======================>.......] - ETA: 5:12 - loss: 0.7152 - acc: 0.5170
7424/9333 [======================>.......] - ETA: 5:02 - loss: 0.7151 - acc: 0.5168
7488/9333 [=======================>......] - ETA: 4:51 - loss: 0.7150 - acc: 0.5168
7552/9333 [=======================>......] - ETA: 4:41 - loss: 0.7151 - acc: 0.5160
7616/9333 [=======================>......] - ETA: 4:31 - loss: 0.7151 - acc: 0.5160
7680/9333 [=======================>......] - ETA: 4:21 - loss: 0.7151 - acc: 0.5159
7744/9333 [=======================>......] - ETA: 4:11 - loss: 0.7150 - acc: 0.5154
7808/9333 [========================>.....] - ETA: 4:01 - loss: 0.7147 - acc: 0.5154
7872/9333 [========================>.....] - ETA: 3:50 - loss: 0.7144 - acc: 0.5158
7936/9333 [========================>.....] - ETA: 3:40 - loss: 0.7143 - acc: 0.5158
8000/9333 [========================>.....] - ETA: 3:30 - loss: 0.7143 - acc: 0.5152
8064/9333 [========================>.....] - ETA: 3:20 - loss: 0.7142 - acc: 0.5148
8128/9333 [=========================>....] - ETA: 3:10 - loss: 0.7138 - acc: 0.5148
8192/9333 [=========================>....] - ETA: 3:00 - loss: 0.7137 - acc: 0.5148
8256/9333 [=========================>....] - ETA: 2:50 - loss: 0.7135 - acc: 0.5150
8320/9333 [=========================>....] - ETA: 2:39 - loss: 0.7134 - acc: 0.5149
8384/9333 [=========================>....] - ETA: 2:29 - loss: 0.7134 - acc: 0.5147
8448/9333 [==========================>...] - ETA: 2:19 - loss: 0.7132 - acc: 0.5148
8512/9333 [==========================>...] - ETA: 2:09 - loss: 0.7132 - acc: 0.5146
8576/9333 [==========================>...] - ETA: 1:59 - loss: 0.7130 - acc: 0.5153
8640/9333 [==========================>...] - ETA: 1:49 - loss: 0.7127 - acc: 0.5155
8704/9333 [==========================>...] - ETA: 1:39 - loss: 0.7126 - acc: 0.5154
8768/9333 [===========================>..] - ETA: 1:29 - loss: 0.7127 - acc: 0.5147
8832/9333 [===========================>..] - ETA: 1:18 - loss: 0.7128 - acc: 0.5147
8896/9333 [===========================>..] - ETA: 1:08 - loss: 0.7129 - acc: 0.5137
8960/9333 [===========================>..] - ETA: 58s - loss: 0.7126 - acc: 0.5135 
9024/9333 [============================>.] - ETA: 48s - loss: 0.7125 - acc: 0.5137
9088/9333 [============================>.] - ETA: 38s - loss: 0.7124 - acc: 0.5138
9152/9333 [============================>.] - ETA: 28s - loss: 0.7122 - acc: 0.5140
9216/9333 [============================>.] - ETA: 18s - loss: 0.7120 - acc: 0.5141
9280/9333 [============================>.] - ETA: 8s - loss: 0.7120 - acc: 0.5141 
9333/9333 [==============================] - 1522s 163ms/step - loss: 0.7118 - acc: 0.5147 - val_loss: 0.6932 - val_acc: 0.5227

Epoch 00001: val_acc improved from -inf to 0.52266, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window20/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 2/10

  64/9333 [..............................] - ETA: 22:18 - loss: 0.6961 - acc: 0.4688
 128/9333 [..............................] - ETA: 22:47 - loss: 0.6992 - acc: 0.5078
 192/9333 [..............................] - ETA: 23:02 - loss: 0.6935 - acc: 0.5469
 256/9333 [..............................] - ETA: 22:46 - loss: 0.6949 - acc: 0.5430
 320/9333 [>.............................] - ETA: 22:52 - loss: 0.6936 - acc: 0.5437
 384/9333 [>.............................] - ETA: 22:55 - loss: 0.6994 - acc: 0.5208
 448/9333 [>.............................] - ETA: 22:46 - loss: 0.6957 - acc: 0.5201
 512/9333 [>.............................] - ETA: 22:48 - loss: 0.6979 - acc: 0.5195
 576/9333 [>.............................] - ETA: 22:25 - loss: 0.6960 - acc: 0.5226
 640/9333 [=>............................] - ETA: 22:19 - loss: 0.6974 - acc: 0.5203
 704/9333 [=>............................] - ETA: 22:08 - loss: 0.6981 - acc: 0.5270
 768/9333 [=>............................] - ETA: 21:57 - loss: 0.6984 - acc: 0.5247
 832/9333 [=>............................] - ETA: 21:42 - loss: 0.6980 - acc: 0.5204
 896/9333 [=>............................] - ETA: 21:27 - loss: 0.6975 - acc: 0.5190
 960/9333 [==>...........................] - ETA: 21:10 - loss: 0.6993 - acc: 0.5156
1024/9333 [==>...........................] - ETA: 21:00 - loss: 0.6995 - acc: 0.5137
1088/9333 [==>...........................] - ETA: 20:48 - loss: 0.7005 - acc: 0.5165
1152/9333 [==>...........................] - ETA: 20:36 - loss: 0.7006 - acc: 0.5165
1216/9333 [==>...........................] - ETA: 20:23 - loss: 0.7023 - acc: 0.5115
1280/9333 [===>..........................] - ETA: 20:11 - loss: 0.7032 - acc: 0.5078
1344/9333 [===>..........................] - ETA: 20:02 - loss: 0.7023 - acc: 0.5082
1408/9333 [===>..........................] - ETA: 19:53 - loss: 0.7014 - acc: 0.5071
1472/9333 [===>..........................] - ETA: 19:42 - loss: 0.6993 - acc: 0.5115
1536/9333 [===>..........................] - ETA: 19:31 - loss: 0.7007 - acc: 0.5117
1600/9333 [====>.........................] - ETA: 19:23 - loss: 0.7015 - acc: 0.5100
1664/9333 [====>.........................] - ETA: 19:13 - loss: 0.7015 - acc: 0.5114
1728/9333 [====>.........................] - ETA: 19:03 - loss: 0.7014 - acc: 0.5098
1792/9333 [====>.........................] - ETA: 18:54 - loss: 0.7004 - acc: 0.5128
1856/9333 [====>.........................] - ETA: 18:44 - loss: 0.7010 - acc: 0.5140
1920/9333 [=====>........................] - ETA: 18:37 - loss: 0.7008 - acc: 0.5167
1984/9333 [=====>........................] - ETA: 18:29 - loss: 0.7016 - acc: 0.5156
2048/9333 [=====>........................] - ETA: 18:21 - loss: 0.7026 - acc: 0.5146
2112/9333 [=====>........................] - ETA: 18:10 - loss: 0.7018 - acc: 0.5185
2176/9333 [=====>........................] - ETA: 17:59 - loss: 0.7017 - acc: 0.5170
2240/9333 [======>.......................] - ETA: 17:48 - loss: 0.7017 - acc: 0.5170
2304/9333 [======>.......................] - ETA: 17:37 - loss: 0.7007 - acc: 0.5174
2368/9333 [======>.......................] - ETA: 17:27 - loss: 0.7011 - acc: 0.5148
2432/9333 [======>.......................] - ETA: 17:17 - loss: 0.7004 - acc: 0.5164
2496/9333 [=======>......................] - ETA: 17:07 - loss: 0.6995 - acc: 0.5204
2560/9333 [=======>......................] - ETA: 16:58 - loss: 0.6993 - acc: 0.5211
2624/9333 [=======>......................] - ETA: 16:48 - loss: 0.6994 - acc: 0.5217
2688/9333 [=======>......................] - ETA: 16:38 - loss: 0.6993 - acc: 0.5205
2752/9333 [=======>......................] - ETA: 16:28 - loss: 0.6992 - acc: 0.5207
2816/9333 [========>.....................] - ETA: 16:18 - loss: 0.6991 - acc: 0.5210
2880/9333 [========>.....................] - ETA: 16:06 - loss: 0.6993 - acc: 0.5191
2944/9333 [========>.....................] - ETA: 15:56 - loss: 0.6989 - acc: 0.5190
3008/9333 [========>.....................] - ETA: 15:47 - loss: 0.6987 - acc: 0.5193
3072/9333 [========>.....................] - ETA: 15:36 - loss: 0.6985 - acc: 0.5202
3136/9333 [=========>....................] - ETA: 15:26 - loss: 0.6982 - acc: 0.5198
3200/9333 [=========>....................] - ETA: 15:16 - loss: 0.6985 - acc: 0.5184
3264/9333 [=========>....................] - ETA: 15:05 - loss: 0.6986 - acc: 0.5169
3328/9333 [=========>....................] - ETA: 14:55 - loss: 0.6984 - acc: 0.5177
3392/9333 [=========>....................] - ETA: 14:45 - loss: 0.6981 - acc: 0.5183
3456/9333 [==========>...................] - ETA: 14:35 - loss: 0.6980 - acc: 0.5185
3520/9333 [==========>...................] - ETA: 14:25 - loss: 0.6980 - acc: 0.5188
3584/9333 [==========>...................] - ETA: 14:14 - loss: 0.6984 - acc: 0.5165
3648/9333 [==========>...................] - ETA: 14:04 - loss: 0.6984 - acc: 0.5173
3712/9333 [==========>...................] - ETA: 13:54 - loss: 0.6982 - acc: 0.5175
3776/9333 [===========>..................] - ETA: 13:44 - loss: 0.6982 - acc: 0.5167
3840/9333 [===========>..................] - ETA: 13:35 - loss: 0.6980 - acc: 0.5172
3904/9333 [===========>..................] - ETA: 13:25 - loss: 0.6978 - acc: 0.5182
3968/9333 [===========>..................] - ETA: 13:15 - loss: 0.6978 - acc: 0.5171
4032/9333 [===========>..................] - ETA: 13:05 - loss: 0.6976 - acc: 0.5164
4096/9333 [============>.................] - ETA: 12:56 - loss: 0.6979 - acc: 0.5151
4160/9333 [============>.................] - ETA: 12:46 - loss: 0.6978 - acc: 0.5151
4224/9333 [============>.................] - ETA: 12:38 - loss: 0.6977 - acc: 0.5144
4288/9333 [============>.................] - ETA: 12:28 - loss: 0.6976 - acc: 0.5149
4352/9333 [============>.................] - ETA: 12:18 - loss: 0.6974 - acc: 0.5149
4416/9333 [=============>................] - ETA: 12:09 - loss: 0.6978 - acc: 0.5140
4480/9333 [=============>................] - ETA: 12:00 - loss: 0.6976 - acc: 0.5154
4544/9333 [=============>................] - ETA: 11:50 - loss: 0.6976 - acc: 0.5145
4608/9333 [=============>................] - ETA: 11:41 - loss: 0.6979 - acc: 0.5130
4672/9333 [==============>...............] - ETA: 11:31 - loss: 0.6977 - acc: 0.5150
4736/9333 [==============>...............] - ETA: 11:21 - loss: 0.6979 - acc: 0.5133
4800/9333 [==============>...............] - ETA: 11:12 - loss: 0.6985 - acc: 0.5117
4864/9333 [==============>...............] - ETA: 11:02 - loss: 0.6990 - acc: 0.5101
4928/9333 [==============>...............] - ETA: 10:53 - loss: 0.6987 - acc: 0.5114
4992/9333 [===============>..............] - ETA: 10:43 - loss: 0.6984 - acc: 0.5126
5056/9333 [===============>..............] - ETA: 10:34 - loss: 0.6986 - acc: 0.5115
5120/9333 [===============>..............] - ETA: 10:24 - loss: 0.6983 - acc: 0.5125
5184/9333 [===============>..............] - ETA: 10:14 - loss: 0.6984 - acc: 0.5122
5248/9333 [===============>..............] - ETA: 10:05 - loss: 0.6985 - acc: 0.5128
5312/9333 [================>.............] - ETA: 9:56 - loss: 0.6987 - acc: 0.5113 
5376/9333 [================>.............] - ETA: 9:47 - loss: 0.6986 - acc: 0.5115
5440/9333 [================>.............] - ETA: 9:37 - loss: 0.6988 - acc: 0.5107
5504/9333 [================>.............] - ETA: 9:28 - loss: 0.6986 - acc: 0.5113
5568/9333 [================>.............] - ETA: 9:19 - loss: 0.6985 - acc: 0.5108
5632/9333 [=================>............] - ETA: 9:09 - loss: 0.6985 - acc: 0.5107
5696/9333 [=================>............] - ETA: 8:59 - loss: 0.6982 - acc: 0.5118
5760/9333 [=================>............] - ETA: 8:49 - loss: 0.6983 - acc: 0.5108
5824/9333 [=================>............] - ETA: 8:40 - loss: 0.6984 - acc: 0.5105
5888/9333 [=================>............] - ETA: 8:30 - loss: 0.6985 - acc: 0.5102
5952/9333 [==================>...........] - ETA: 8:21 - loss: 0.6985 - acc: 0.5106
6016/9333 [==================>...........] - ETA: 8:11 - loss: 0.6984 - acc: 0.5105
6080/9333 [==================>...........] - ETA: 8:02 - loss: 0.6983 - acc: 0.5113
6144/9333 [==================>...........] - ETA: 7:52 - loss: 0.6983 - acc: 0.5114
6208/9333 [==================>...........] - ETA: 7:43 - loss: 0.6982 - acc: 0.5111
6272/9333 [===================>..........] - ETA: 7:33 - loss: 0.6982 - acc: 0.5116
6336/9333 [===================>..........] - ETA: 7:24 - loss: 0.6981 - acc: 0.5118
6400/9333 [===================>..........] - ETA: 7:15 - loss: 0.6979 - acc: 0.5122
6464/9333 [===================>..........] - ETA: 7:05 - loss: 0.6979 - acc: 0.5121
6528/9333 [===================>..........] - ETA: 6:56 - loss: 0.6979 - acc: 0.5123
6592/9333 [====================>.........] - ETA: 6:46 - loss: 0.6979 - acc: 0.5115
6656/9333 [====================>.........] - ETA: 6:36 - loss: 0.6979 - acc: 0.5113
6720/9333 [====================>.........] - ETA: 6:27 - loss: 0.6979 - acc: 0.5110
6784/9333 [====================>.........] - ETA: 6:17 - loss: 0.6979 - acc: 0.5112
6848/9333 [=====================>........] - ETA: 6:07 - loss: 0.6980 - acc: 0.5110
6912/9333 [=====================>........] - ETA: 5:58 - loss: 0.6980 - acc: 0.5104
6976/9333 [=====================>........] - ETA: 5:48 - loss: 0.6981 - acc: 0.5100
7040/9333 [=====================>........] - ETA: 5:39 - loss: 0.6981 - acc: 0.5098
7104/9333 [=====================>........] - ETA: 5:29 - loss: 0.6980 - acc: 0.5099
7168/9333 [======================>.......] - ETA: 5:20 - loss: 0.6980 - acc: 0.5099
7232/9333 [======================>.......] - ETA: 5:10 - loss: 0.6979 - acc: 0.5101
7296/9333 [======================>.......] - ETA: 5:01 - loss: 0.6980 - acc: 0.5099
7360/9333 [======================>.......] - ETA: 4:51 - loss: 0.6978 - acc: 0.5105
7424/9333 [======================>.......] - ETA: 4:42 - loss: 0.6979 - acc: 0.5094
7488/9333 [=======================>......] - ETA: 4:33 - loss: 0.6976 - acc: 0.5104
7552/9333 [=======================>......] - ETA: 4:23 - loss: 0.6976 - acc: 0.5110
7616/9333 [=======================>......] - ETA: 4:14 - loss: 0.6976 - acc: 0.5117
7680/9333 [=======================>......] - ETA: 4:04 - loss: 0.6978 - acc: 0.5108
7744/9333 [=======================>......] - ETA: 3:55 - loss: 0.6979 - acc: 0.5108
7808/9333 [========================>.....] - ETA: 3:45 - loss: 0.6978 - acc: 0.5110
7872/9333 [========================>.....] - ETA: 3:36 - loss: 0.6977 - acc: 0.5104
7936/9333 [========================>.....] - ETA: 3:26 - loss: 0.6979 - acc: 0.5096
8000/9333 [========================>.....] - ETA: 3:17 - loss: 0.6979 - acc: 0.5094
8064/9333 [========================>.....] - ETA: 3:07 - loss: 0.6979 - acc: 0.5091
8128/9333 [=========================>....] - ETA: 2:58 - loss: 0.6978 - acc: 0.5094
8192/9333 [=========================>....] - ETA: 2:48 - loss: 0.6977 - acc: 0.5104
8256/9333 [=========================>....] - ETA: 2:39 - loss: 0.6975 - acc: 0.5113
8320/9333 [=========================>....] - ETA: 2:30 - loss: 0.6973 - acc: 0.5121
8384/9333 [=========================>....] - ETA: 2:20 - loss: 0.6973 - acc: 0.5116
8448/9333 [==========================>...] - ETA: 2:11 - loss: 0.6973 - acc: 0.5115
8512/9333 [==========================>...] - ETA: 2:01 - loss: 0.6973 - acc: 0.5117
8576/9333 [==========================>...] - ETA: 1:52 - loss: 0.6972 - acc: 0.5115
8640/9333 [==========================>...] - ETA: 1:42 - loss: 0.6972 - acc: 0.5119
8704/9333 [==========================>...] - ETA: 1:33 - loss: 0.6971 - acc: 0.5122
8768/9333 [===========================>..] - ETA: 1:23 - loss: 0.6971 - acc: 0.5124
8832/9333 [===========================>..] - ETA: 1:14 - loss: 0.6971 - acc: 0.5126
8896/9333 [===========================>..] - ETA: 1:04 - loss: 0.6972 - acc: 0.5120
8960/9333 [===========================>..] - ETA: 55s - loss: 0.6972 - acc: 0.5116 
9024/9333 [============================>.] - ETA: 45s - loss: 0.6971 - acc: 0.5116
9088/9333 [============================>.] - ETA: 36s - loss: 0.6970 - acc: 0.5117
9152/9333 [============================>.] - ETA: 26s - loss: 0.6971 - acc: 0.5114
9216/9333 [============================>.] - ETA: 17s - loss: 0.6968 - acc: 0.5124
9280/9333 [============================>.] - ETA: 7s - loss: 0.6968 - acc: 0.5125 
9333/9333 [==============================] - 1426s 153ms/step - loss: 0.6968 - acc: 0.5121 - val_loss: 0.6943 - val_acc: 0.5082

Epoch 00002: val_acc did not improve from 0.52266
Epoch 3/10

  64/9333 [..............................] - ETA: 20:30 - loss: 0.6890 - acc: 0.5312
 128/9333 [..............................] - ETA: 21:29 - loss: 0.6885 - acc: 0.5312
 192/9333 [..............................] - ETA: 21:51 - loss: 0.6914 - acc: 0.5312
 256/9333 [..............................] - ETA: 21:49 - loss: 0.6956 - acc: 0.5039
 320/9333 [>.............................] - ETA: 21:38 - loss: 0.6977 - acc: 0.5031
 384/9333 [>.............................] - ETA: 21:35 - loss: 0.6925 - acc: 0.5182
 448/9333 [>.............................] - ETA: 21:19 - loss: 0.6957 - acc: 0.5089
 512/9333 [>.............................] - ETA: 21:10 - loss: 0.6951 - acc: 0.5098
 576/9333 [>.............................] - ETA: 20:42 - loss: 0.6975 - acc: 0.5035
 640/9333 [=>............................] - ETA: 20:35 - loss: 0.6954 - acc: 0.5141
 704/9333 [=>............................] - ETA: 20:29 - loss: 0.6948 - acc: 0.5142
 768/9333 [=>............................] - ETA: 20:14 - loss: 0.6933 - acc: 0.5182
 832/9333 [=>............................] - ETA: 20:01 - loss: 0.6927 - acc: 0.5252
 896/9333 [=>............................] - ETA: 19:55 - loss: 0.6936 - acc: 0.5246
 960/9333 [==>...........................] - ETA: 19:46 - loss: 0.6944 - acc: 0.5208
1024/9333 [==>...........................] - ETA: 19:37 - loss: 0.6927 - acc: 0.5186
1088/9333 [==>...........................] - ETA: 19:31 - loss: 0.6926 - acc: 0.5165
1152/9333 [==>...........................] - ETA: 19:22 - loss: 0.6926 - acc: 0.5174
1216/9333 [==>...........................] - ETA: 19:09 - loss: 0.6925 - acc: 0.5181
1280/9333 [===>..........................] - ETA: 19:02 - loss: 0.6955 - acc: 0.5109
1344/9333 [===>..........................] - ETA: 18:54 - loss: 0.6949 - acc: 0.5149
1408/9333 [===>..........................] - ETA: 18:49 - loss: 0.6952 - acc: 0.5135
1472/9333 [===>..........................] - ETA: 18:39 - loss: 0.6958 - acc: 0.5102
1536/9333 [===>..........................] - ETA: 18:30 - loss: 0.6952 - acc: 0.5143
1600/9333 [====>.........................] - ETA: 18:18 - loss: 0.6954 - acc: 0.5112
1664/9333 [====>.........................] - ETA: 18:10 - loss: 0.6959 - acc: 0.5108
1728/9333 [====>.........................] - ETA: 18:02 - loss: 0.6957 - acc: 0.5122
1792/9333 [====>.........................] - ETA: 17:52 - loss: 0.6963 - acc: 0.5117
1856/9333 [====>.........................] - ETA: 17:42 - loss: 0.6965 - acc: 0.5124
1920/9333 [=====>........................] - ETA: 17:33 - loss: 0.6966 - acc: 0.5099
1984/9333 [=====>........................] - ETA: 17:25 - loss: 0.6969 - acc: 0.5091
2048/9333 [=====>........................] - ETA: 17:17 - loss: 0.6966 - acc: 0.5098
2112/9333 [=====>........................] - ETA: 17:08 - loss: 0.6965 - acc: 0.5095
2176/9333 [=====>........................] - ETA: 17:00 - loss: 0.6966 - acc: 0.5074
2240/9333 [======>.......................] - ETA: 16:52 - loss: 0.6966 - acc: 0.5062
2304/9333 [======>.......................] - ETA: 16:43 - loss: 0.6964 - acc: 0.5065
2368/9333 [======>.......................] - ETA: 16:34 - loss: 0.6964 - acc: 0.5068
2432/9333 [======>.......................] - ETA: 16:23 - loss: 0.6967 - acc: 0.5062
2496/9333 [=======>......................] - ETA: 16:14 - loss: 0.6968 - acc: 0.5048
2560/9333 [=======>......................] - ETA: 16:05 - loss: 0.6967 - acc: 0.5047
2624/9333 [=======>......................] - ETA: 15:54 - loss: 0.6970 - acc: 0.5011
2688/9333 [=======>......................] - ETA: 15:45 - loss: 0.6969 - acc: 0.5019
2752/9333 [=======>......................] - ETA: 15:35 - loss: 0.6963 - acc: 0.5047
2816/9333 [========>.....................] - ETA: 15:24 - loss: 0.6964 - acc: 0.5032
2880/9333 [========>.....................] - ETA: 15:14 - loss: 0.6964 - acc: 0.5052
2944/9333 [========>.....................] - ETA: 15:06 - loss: 0.6965 - acc: 0.5061
3008/9333 [========>.....................] - ETA: 14:57 - loss: 0.6960 - acc: 0.5073
3072/9333 [========>.....................] - ETA: 14:50 - loss: 0.6958 - acc: 0.5091
3136/9333 [=========>....................] - ETA: 14:42 - loss: 0.6958 - acc: 0.5077
3200/9333 [=========>....................] - ETA: 14:32 - loss: 0.6955 - acc: 0.5091
3264/9333 [=========>....................] - ETA: 14:23 - loss: 0.6960 - acc: 0.5070
3328/9333 [=========>....................] - ETA: 14:14 - loss: 0.6960 - acc: 0.5072
3392/9333 [=========>....................] - ETA: 14:06 - loss: 0.6960 - acc: 0.5068
3456/9333 [==========>...................] - ETA: 13:57 - loss: 0.6956 - acc: 0.5095
3520/9333 [==========>...................] - ETA: 13:48 - loss: 0.6952 - acc: 0.5108
3584/9333 [==========>...................] - ETA: 13:39 - loss: 0.6953 - acc: 0.5117
3648/9333 [==========>...................] - ETA: 13:29 - loss: 0.6949 - acc: 0.5132
3712/9333 [==========>...................] - ETA: 13:20 - loss: 0.6948 - acc: 0.5143
3776/9333 [===========>..................] - ETA: 13:11 - loss: 0.6946 - acc: 0.5146
3840/9333 [===========>..................] - ETA: 13:02 - loss: 0.6945 - acc: 0.5156
3904/9333 [===========>..................] - ETA: 12:52 - loss: 0.6947 - acc: 0.5149
3968/9333 [===========>..................] - ETA: 12:44 - loss: 0.6948 - acc: 0.5156
4032/9333 [===========>..................] - ETA: 12:35 - loss: 0.6946 - acc: 0.5161
4096/9333 [============>.................] - ETA: 12:25 - loss: 0.6946 - acc: 0.5164
4160/9333 [============>.................] - ETA: 12:16 - loss: 0.6944 - acc: 0.5171
4224/9333 [============>.................] - ETA: 12:06 - loss: 0.6943 - acc: 0.5180
4288/9333 [============>.................] - ETA: 11:58 - loss: 0.6943 - acc: 0.5184
4352/9333 [============>.................] - ETA: 11:49 - loss: 0.6940 - acc: 0.5186
4416/9333 [=============>................] - ETA: 11:41 - loss: 0.6938 - acc: 0.5202
4480/9333 [=============>................] - ETA: 11:32 - loss: 0.6936 - acc: 0.5208
4544/9333 [=============>................] - ETA: 11:23 - loss: 0.6934 - acc: 0.5220
4608/9333 [=============>................] - ETA: 11:14 - loss: 0.6932 - acc: 0.5221
4672/9333 [==============>...............] - ETA: 11:05 - loss: 0.6934 - acc: 0.5220
4736/9333 [==============>...............] - ETA: 10:56 - loss: 0.6934 - acc: 0.5222
4800/9333 [==============>...............] - ETA: 10:47 - loss: 0.6932 - acc: 0.5231
4864/9333 [==============>...............] - ETA: 10:38 - loss: 0.6935 - acc: 0.5226
4928/9333 [==============>...............] - ETA: 10:29 - loss: 0.6933 - acc: 0.5227
4992/9333 [===============>..............] - ETA: 10:21 - loss: 0.6932 - acc: 0.5218
5056/9333 [===============>..............] - ETA: 10:11 - loss: 0.6932 - acc: 0.5222
5120/9333 [===============>..............] - ETA: 10:01 - loss: 0.6934 - acc: 0.5217
5184/9333 [===============>..............] - ETA: 9:52 - loss: 0.6933 - acc: 0.5214 
5248/9333 [===============>..............] - ETA: 9:43 - loss: 0.6932 - acc: 0.5217
5312/9333 [================>.............] - ETA: 9:33 - loss: 0.6935 - acc: 0.5203
5376/9333 [================>.............] - ETA: 9:24 - loss: 0.6934 - acc: 0.5214
5440/9333 [================>.............] - ETA: 9:15 - loss: 0.6934 - acc: 0.5215
5504/9333 [================>.............] - ETA: 9:06 - loss: 0.6936 - acc: 0.5207
5568/9333 [================>.............] - ETA: 8:57 - loss: 0.6936 - acc: 0.5212
5632/9333 [=================>............] - ETA: 8:47 - loss: 0.6939 - acc: 0.5201
5696/9333 [=================>............] - ETA: 8:39 - loss: 0.6941 - acc: 0.5190
5760/9333 [=================>............] - ETA: 8:30 - loss: 0.6942 - acc: 0.5186
5824/9333 [=================>............] - ETA: 8:21 - loss: 0.6942 - acc: 0.5187
5888/9333 [=================>............] - ETA: 8:12 - loss: 0.6941 - acc: 0.5190
5952/9333 [==================>...........] - ETA: 8:02 - loss: 0.6941 - acc: 0.5181
6016/9333 [==================>...........] - ETA: 7:53 - loss: 0.6942 - acc: 0.5180
6080/9333 [==================>...........] - ETA: 7:45 - loss: 0.6944 - acc: 0.5171
6144/9333 [==================>...........] - ETA: 7:35 - loss: 0.6945 - acc: 0.5171
6208/9333 [==================>...........] - ETA: 7:26 - loss: 0.6946 - acc: 0.5161
6272/9333 [===================>..........] - ETA: 7:17 - loss: 0.6946 - acc: 0.5159
6336/9333 [===================>..........] - ETA: 7:07 - loss: 0.6946 - acc: 0.5161
6400/9333 [===================>..........] - ETA: 6:58 - loss: 0.6945 - acc: 0.5158
6464/9333 [===================>..........] - ETA: 6:48 - loss: 0.6944 - acc: 0.5166
6528/9333 [===================>..........] - ETA: 6:39 - loss: 0.6945 - acc: 0.5158
6592/9333 [====================>.........] - ETA: 6:30 - loss: 0.6943 - acc: 0.5171
6656/9333 [====================>.........] - ETA: 6:20 - loss: 0.6944 - acc: 0.5165
6720/9333 [====================>.........] - ETA: 6:11 - loss: 0.6942 - acc: 0.5168
6784/9333 [====================>.........] - ETA: 6:02 - loss: 0.6942 - acc: 0.5167
6848/9333 [=====================>........] - ETA: 5:53 - loss: 0.6943 - acc: 0.5161
6912/9333 [=====================>........] - ETA: 5:44 - loss: 0.6943 - acc: 0.5158
6976/9333 [=====================>........] - ETA: 5:35 - loss: 0.6943 - acc: 0.5152
7040/9333 [=====================>........] - ETA: 5:25 - loss: 0.6944 - acc: 0.5151
7104/9333 [=====================>........] - ETA: 5:16 - loss: 0.6943 - acc: 0.5155
7168/9333 [======================>.......] - ETA: 5:07 - loss: 0.6944 - acc: 0.5151
7232/9333 [======================>.......] - ETA: 4:58 - loss: 0.6945 - acc: 0.5142
7296/9333 [======================>.......] - ETA: 4:49 - loss: 0.6947 - acc: 0.5138
7360/9333 [======================>.......] - ETA: 4:40 - loss: 0.6948 - acc: 0.5133
7424/9333 [======================>.......] - ETA: 4:31 - loss: 0.6948 - acc: 0.5129
7488/9333 [=======================>......] - ETA: 4:21 - loss: 0.6948 - acc: 0.5135
7552/9333 [=======================>......] - ETA: 4:12 - loss: 0.6947 - acc: 0.5139
7616/9333 [=======================>......] - ETA: 4:03 - loss: 0.6948 - acc: 0.5138
7680/9333 [=======================>......] - ETA: 3:54 - loss: 0.6948 - acc: 0.5138
7744/9333 [=======================>......] - ETA: 3:45 - loss: 0.6950 - acc: 0.5129
7808/9333 [========================>.....] - ETA: 3:36 - loss: 0.6950 - acc: 0.5129
7872/9333 [========================>.....] - ETA: 3:27 - loss: 0.6950 - acc: 0.5130
7936/9333 [========================>.....] - ETA: 3:18 - loss: 0.6950 - acc: 0.5130
8000/9333 [========================>.....] - ETA: 3:09 - loss: 0.6950 - acc: 0.5129
8064/9333 [========================>.....] - ETA: 2:59 - loss: 0.6948 - acc: 0.5134
8128/9333 [=========================>....] - ETA: 2:50 - loss: 0.6948 - acc: 0.5132
8192/9333 [=========================>....] - ETA: 2:41 - loss: 0.6948 - acc: 0.5133
8256/9333 [=========================>....] - ETA: 2:32 - loss: 0.6947 - acc: 0.5137
8320/9333 [=========================>....] - ETA: 2:23 - loss: 0.6946 - acc: 0.5136
8384/9333 [=========================>....] - ETA: 2:14 - loss: 0.6946 - acc: 0.5141
8448/9333 [==========================>...] - ETA: 2:05 - loss: 0.6945 - acc: 0.5144
8512/9333 [==========================>...] - ETA: 1:56 - loss: 0.6944 - acc: 0.5146
8576/9333 [==========================>...] - ETA: 1:47 - loss: 0.6943 - acc: 0.5150
8640/9333 [==========================>...] - ETA: 1:38 - loss: 0.6944 - acc: 0.5146
8704/9333 [==========================>...] - ETA: 1:29 - loss: 0.6944 - acc: 0.5145
8768/9333 [===========================>..] - ETA: 1:20 - loss: 0.6944 - acc: 0.5138
8832/9333 [===========================>..] - ETA: 1:11 - loss: 0.6945 - acc: 0.5134
8896/9333 [===========================>..] - ETA: 1:02 - loss: 0.6945 - acc: 0.5132
8960/9333 [===========================>..] - ETA: 52s - loss: 0.6942 - acc: 0.5143 
9024/9333 [============================>.] - ETA: 43s - loss: 0.6943 - acc: 0.5141
9088/9333 [============================>.] - ETA: 34s - loss: 0.6942 - acc: 0.5143
9152/9333 [============================>.] - ETA: 25s - loss: 0.6943 - acc: 0.5138
9216/9333 [============================>.] - ETA: 16s - loss: 0.6943 - acc: 0.5136
9280/9333 [============================>.] - ETA: 7s - loss: 0.6942 - acc: 0.5133 
9333/9333 [==============================] - 1376s 147ms/step - loss: 0.6942 - acc: 0.5137 - val_loss: 0.6925 - val_acc: 0.5246

Epoch 00003: val_acc improved from 0.52266 to 0.52459, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window20/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 4/10

  64/9333 [..............................] - ETA: 20:00 - loss: 0.7013 - acc: 0.4219
 128/9333 [..............................] - ETA: 21:47 - loss: 0.6873 - acc: 0.5156
 192/9333 [..............................] - ETA: 21:15 - loss: 0.6907 - acc: 0.5104
 256/9333 [..............................] - ETA: 20:53 - loss: 0.6919 - acc: 0.5195
 320/9333 [>.............................] - ETA: 21:07 - loss: 0.6891 - acc: 0.5188
 384/9333 [>.............................] - ETA: 20:55 - loss: 0.6872 - acc: 0.5286
 448/9333 [>.............................] - ETA: 20:57 - loss: 0.6903 - acc: 0.5156
 512/9333 [>.............................] - ETA: 20:48 - loss: 0.6897 - acc: 0.5137
 576/9333 [>.............................] - ETA: 20:39 - loss: 0.6920 - acc: 0.5000
 640/9333 [=>............................] - ETA: 20:36 - loss: 0.6913 - acc: 0.5047
 704/9333 [=>............................] - ETA: 20:27 - loss: 0.6928 - acc: 0.4986
 768/9333 [=>............................] - ETA: 20:22 - loss: 0.6932 - acc: 0.4896
 832/9333 [=>............................] - ETA: 20:15 - loss: 0.6922 - acc: 0.4976
 896/9333 [=>............................] - ETA: 20:10 - loss: 0.6912 - acc: 0.5033
 960/9333 [==>...........................] - ETA: 20:03 - loss: 0.6897 - acc: 0.5167
1024/9333 [==>...........................] - ETA: 19:56 - loss: 0.6915 - acc: 0.5098
1088/9333 [==>...........................] - ETA: 19:50 - loss: 0.6905 - acc: 0.5165
1152/9333 [==>...........................] - ETA: 19:39 - loss: 0.6905 - acc: 0.5148
1216/9333 [==>...........................] - ETA: 19:37 - loss: 0.6908 - acc: 0.5173
1280/9333 [===>..........................] - ETA: 19:28 - loss: 0.6920 - acc: 0.5133
1344/9333 [===>..........................] - ETA: 19:16 - loss: 0.6930 - acc: 0.5067
1408/9333 [===>..........................] - ETA: 19:08 - loss: 0.6929 - acc: 0.5085
1472/9333 [===>..........................] - ETA: 18:58 - loss: 0.6932 - acc: 0.5068
1536/9333 [===>..........................] - ETA: 18:51 - loss: 0.6923 - acc: 0.5098
1600/9333 [====>.........................] - ETA: 18:45 - loss: 0.6924 - acc: 0.5112
1664/9333 [====>.........................] - ETA: 18:40 - loss: 0.6924 - acc: 0.5138
1728/9333 [====>.........................] - ETA: 18:34 - loss: 0.6920 - acc: 0.5133
1792/9333 [====>.........................] - ETA: 18:30 - loss: 0.6920 - acc: 0.5134
1856/9333 [====>.........................] - ETA: 18:22 - loss: 0.6924 - acc: 0.5102
1920/9333 [=====>........................] - ETA: 18:16 - loss: 0.6919 - acc: 0.5125
1984/9333 [=====>........................] - ETA: 18:08 - loss: 0.6917 - acc: 0.5131
2048/9333 [=====>........................] - ETA: 18:01 - loss: 0.6920 - acc: 0.5103
2112/9333 [=====>........................] - ETA: 17:58 - loss: 0.6931 - acc: 0.5085
2176/9333 [=====>........................] - ETA: 17:49 - loss: 0.6928 - acc: 0.5092
2240/9333 [======>.......................] - ETA: 17:41 - loss: 0.6932 - acc: 0.5076
2304/9333 [======>.......................] - ETA: 17:34 - loss: 0.6936 - acc: 0.5061
2368/9333 [======>.......................] - ETA: 17:24 - loss: 0.6938 - acc: 0.5059
2432/9333 [======>.......................] - ETA: 17:16 - loss: 0.6939 - acc: 0.5066
2496/9333 [=======>......................] - ETA: 17:08 - loss: 0.6945 - acc: 0.5064
2560/9333 [=======>......................] - ETA: 17:01 - loss: 0.6946 - acc: 0.5059
2624/9333 [=======>......................] - ETA: 16:53 - loss: 0.6946 - acc: 0.5061
2688/9333 [=======>......................] - ETA: 16:47 - loss: 0.6948 - acc: 0.5041
2752/9333 [=======>......................] - ETA: 16:42 - loss: 0.6942 - acc: 0.5069
2816/9333 [========>.....................] - ETA: 16:34 - loss: 0.6940 - acc: 0.5071
2880/9333 [========>.....................] - ETA: 16:27 - loss: 0.6942 - acc: 0.5069
2944/9333 [========>.....................] - ETA: 16:19 - loss: 0.6937 - acc: 0.5099
3008/9333 [========>.....................] - ETA: 16:10 - loss: 0.6936 - acc: 0.5110
3072/9333 [========>.....................] - ETA: 16:02 - loss: 0.6937 - acc: 0.5104
3136/9333 [=========>....................] - ETA: 15:52 - loss: 0.6935 - acc: 0.5118
3200/9333 [=========>....................] - ETA: 15:42 - loss: 0.6935 - acc: 0.5116
3264/9333 [=========>....................] - ETA: 15:34 - loss: 0.6936 - acc: 0.5119
3328/9333 [=========>....................] - ETA: 15:24 - loss: 0.6935 - acc: 0.5138
3392/9333 [=========>....................] - ETA: 15:16 - loss: 0.6936 - acc: 0.5150
3456/9333 [==========>...................] - ETA: 15:08 - loss: 0.6934 - acc: 0.5153
3520/9333 [==========>...................] - ETA: 14:59 - loss: 0.6935 - acc: 0.5139
3584/9333 [==========>...................] - ETA: 14:51 - loss: 0.6938 - acc: 0.5128
3648/9333 [==========>...................] - ETA: 14:41 - loss: 0.6937 - acc: 0.5145
3712/9333 [==========>...................] - ETA: 14:31 - loss: 0.6938 - acc: 0.5151
3776/9333 [===========>..................] - ETA: 14:22 - loss: 0.6936 - acc: 0.5156
3840/9333 [===========>..................] - ETA: 14:13 - loss: 0.6939 - acc: 0.5146
3904/9333 [===========>..................] - ETA: 14:03 - loss: 0.6937 - acc: 0.5164
3968/9333 [===========>..................] - ETA: 13:54 - loss: 0.6934 - acc: 0.5181
4032/9333 [===========>..................] - ETA: 13:45 - loss: 0.6930 - acc: 0.5206
4096/9333 [============>.................] - ETA: 13:36 - loss: 0.6927 - acc: 0.5212
4160/9333 [============>.................] - ETA: 13:26 - loss: 0.6928 - acc: 0.5200
4224/9333 [============>.................] - ETA: 13:17 - loss: 0.6928 - acc: 0.5206
4288/9333 [============>.................] - ETA: 13:07 - loss: 0.6926 - acc: 0.5210
4352/9333 [============>.................] - ETA: 12:58 - loss: 0.6926 - acc: 0.5214
4416/9333 [=============>................] - ETA: 12:48 - loss: 0.6926 - acc: 0.5220
4480/9333 [=============>................] - ETA: 12:38 - loss: 0.6924 - acc: 0.5230
4544/9333 [=============>................] - ETA: 12:28 - loss: 0.6926 - acc: 0.5213
4608/9333 [=============>................] - ETA: 12:17 - loss: 0.6925 - acc: 0.5213
4672/9333 [==============>...............] - ETA: 12:07 - loss: 0.6928 - acc: 0.5214
4736/9333 [==============>...............] - ETA: 11:57 - loss: 0.6932 - acc: 0.5196
4800/9333 [==============>...............] - ETA: 11:48 - loss: 0.6933 - acc: 0.5194
4864/9333 [==============>...............] - ETA: 11:37 - loss: 0.6932 - acc: 0.5201
4928/9333 [==============>...............] - ETA: 11:28 - loss: 0.6932 - acc: 0.5207
4992/9333 [===============>..............] - ETA: 11:18 - loss: 0.6929 - acc: 0.5226
5056/9333 [===============>..............] - ETA: 11:08 - loss: 0.6930 - acc: 0.5218
5120/9333 [===============>..............] - ETA: 10:58 - loss: 0.6929 - acc: 0.5215
5184/9333 [===============>..............] - ETA: 10:48 - loss: 0.6929 - acc: 0.5212
5248/9333 [===============>..............] - ETA: 10:38 - loss: 0.6930 - acc: 0.5206
5312/9333 [================>.............] - ETA: 10:28 - loss: 0.6930 - acc: 0.5211
5376/9333 [================>.............] - ETA: 10:18 - loss: 0.6928 - acc: 0.5214
5440/9333 [================>.............] - ETA: 10:08 - loss: 0.6927 - acc: 0.5217
5504/9333 [================>.............] - ETA: 9:58 - loss: 0.6927 - acc: 0.5220 
5568/9333 [================>.............] - ETA: 9:48 - loss: 0.6929 - acc: 0.5205
5632/9333 [=================>............] - ETA: 9:38 - loss: 0.6929 - acc: 0.5201
5696/9333 [=================>............] - ETA: 9:28 - loss: 0.6928 - acc: 0.5197
5760/9333 [=================>............] - ETA: 9:18 - loss: 0.6928 - acc: 0.5200
5824/9333 [=================>............] - ETA: 9:08 - loss: 0.6929 - acc: 0.5194
5888/9333 [=================>............] - ETA: 8:58 - loss: 0.6928 - acc: 0.5200
5952/9333 [==================>...........] - ETA: 8:48 - loss: 0.6928 - acc: 0.5200
6016/9333 [==================>...........] - ETA: 8:38 - loss: 0.6928 - acc: 0.5201
6080/9333 [==================>...........] - ETA: 8:28 - loss: 0.6932 - acc: 0.5181
6144/9333 [==================>...........] - ETA: 8:18 - loss: 0.6932 - acc: 0.5173
6208/9333 [==================>...........] - ETA: 8:08 - loss: 0.6933 - acc: 0.5169
6272/9333 [===================>..........] - ETA: 7:58 - loss: 0.6933 - acc: 0.5167
6336/9333 [===================>..........] - ETA: 7:48 - loss: 0.6933 - acc: 0.5167
6400/9333 [===================>..........] - ETA: 7:38 - loss: 0.6931 - acc: 0.5181
6464/9333 [===================>..........] - ETA: 7:28 - loss: 0.6930 - acc: 0.5192
6528/9333 [===================>..........] - ETA: 7:18 - loss: 0.6931 - acc: 0.5190
6592/9333 [====================>.........] - ETA: 7:08 - loss: 0.6932 - acc: 0.5184
6656/9333 [====================>.........] - ETA: 6:58 - loss: 0.6932 - acc: 0.5183
6720/9333 [====================>.........] - ETA: 6:48 - loss: 0.6932 - acc: 0.5183
6784/9333 [====================>.........] - ETA: 6:37 - loss: 0.6932 - acc: 0.5186
6848/9333 [=====================>........] - ETA: 6:27 - loss: 0.6932 - acc: 0.5188
6912/9333 [=====================>........] - ETA: 6:17 - loss: 0.6929 - acc: 0.5201
6976/9333 [=====================>........] - ETA: 6:07 - loss: 0.6927 - acc: 0.5208
7040/9333 [=====================>........] - ETA: 5:57 - loss: 0.6926 - acc: 0.5214
7104/9333 [=====================>........] - ETA: 5:47 - loss: 0.6927 - acc: 0.5218
7168/9333 [======================>.......] - ETA: 5:37 - loss: 0.6928 - acc: 0.5212
7232/9333 [======================>.......] - ETA: 5:27 - loss: 0.6929 - acc: 0.5212
7296/9333 [======================>.......] - ETA: 5:17 - loss: 0.6930 - acc: 0.5206
7360/9333 [======================>.......] - ETA: 5:07 - loss: 0.6931 - acc: 0.5197
7424/9333 [======================>.......] - ETA: 4:57 - loss: 0.6931 - acc: 0.5195
7488/9333 [=======================>......] - ETA: 4:47 - loss: 0.6929 - acc: 0.5199
7552/9333 [=======================>......] - ETA: 4:37 - loss: 0.6930 - acc: 0.5197
7616/9333 [=======================>......] - ETA: 4:27 - loss: 0.6929 - acc: 0.5200
7680/9333 [=======================>......] - ETA: 4:17 - loss: 0.6927 - acc: 0.5204
7744/9333 [=======================>......] - ETA: 4:07 - loss: 0.6925 - acc: 0.5216
7808/9333 [========================>.....] - ETA: 3:57 - loss: 0.6926 - acc: 0.5215
7872/9333 [========================>.....] - ETA: 3:47 - loss: 0.6925 - acc: 0.5220
7936/9333 [========================>.....] - ETA: 3:37 - loss: 0.6926 - acc: 0.5218
8000/9333 [========================>.....] - ETA: 3:27 - loss: 0.6926 - acc: 0.5214
8064/9333 [========================>.....] - ETA: 3:17 - loss: 0.6925 - acc: 0.5213
8128/9333 [=========================>....] - ETA: 3:07 - loss: 0.6926 - acc: 0.5207
8192/9333 [=========================>....] - ETA: 2:57 - loss: 0.6926 - acc: 0.5210
8256/9333 [=========================>....] - ETA: 2:47 - loss: 0.6926 - acc: 0.5210
8320/9333 [=========================>....] - ETA: 2:37 - loss: 0.6925 - acc: 0.5207
8384/9333 [=========================>....] - ETA: 2:27 - loss: 0.6926 - acc: 0.5209
8448/9333 [==========================>...] - ETA: 2:17 - loss: 0.6926 - acc: 0.5205
8512/9333 [==========================>...] - ETA: 2:07 - loss: 0.6925 - acc: 0.5210
8576/9333 [==========================>...] - ETA: 1:57 - loss: 0.6925 - acc: 0.5208
8640/9333 [==========================>...] - ETA: 1:47 - loss: 0.6924 - acc: 0.5211
8704/9333 [==========================>...] - ETA: 1:37 - loss: 0.6924 - acc: 0.5207
8768/9333 [===========================>..] - ETA: 1:27 - loss: 0.6925 - acc: 0.5209
8832/9333 [===========================>..] - ETA: 1:17 - loss: 0.6925 - acc: 0.5212
8896/9333 [===========================>..] - ETA: 1:07 - loss: 0.6926 - acc: 0.5208
8960/9333 [===========================>..] - ETA: 57s - loss: 0.6924 - acc: 0.5214 
9024/9333 [============================>.] - ETA: 48s - loss: 0.6923 - acc: 0.5222
9088/9333 [============================>.] - ETA: 38s - loss: 0.6923 - acc: 0.5222
9152/9333 [============================>.] - ETA: 28s - loss: 0.6922 - acc: 0.5229
9216/9333 [============================>.] - ETA: 18s - loss: 0.6924 - acc: 0.5228
9280/9333 [============================>.] - ETA: 8s - loss: 0.6922 - acc: 0.5233 
9333/9333 [==============================] - 1500s 161ms/step - loss: 0.6922 - acc: 0.5237 - val_loss: 0.6976 - val_acc: 0.5043

Epoch 00004: val_acc did not improve from 0.52459
Epoch 5/10

  64/9333 [..............................] - ETA: 21:45 - loss: 0.6922 - acc: 0.5000
 128/9333 [..............................] - ETA: 22:15 - loss: 0.7003 - acc: 0.4844
 192/9333 [..............................] - ETA: 21:27 - loss: 0.6954 - acc: 0.5104
 256/9333 [..............................] - ETA: 21:24 - loss: 0.6923 - acc: 0.5234
 320/9333 [>.............................] - ETA: 21:11 - loss: 0.6899 - acc: 0.5281
 384/9333 [>.............................] - ETA: 21:06 - loss: 0.6888 - acc: 0.5391
 448/9333 [>.............................] - ETA: 21:04 - loss: 0.6929 - acc: 0.5201
 512/9333 [>.............................] - ETA: 21:00 - loss: 0.6973 - acc: 0.5059
 576/9333 [>.............................] - ETA: 20:46 - loss: 0.6974 - acc: 0.5122
 640/9333 [=>............................] - ETA: 20:36 - loss: 0.6964 - acc: 0.5172
 704/9333 [=>............................] - ETA: 20:30 - loss: 0.6947 - acc: 0.5213
 768/9333 [=>............................] - ETA: 20:22 - loss: 0.6946 - acc: 0.5234
 832/9333 [=>............................] - ETA: 20:16 - loss: 0.6933 - acc: 0.5288
 896/9333 [=>............................] - ETA: 20:10 - loss: 0.6932 - acc: 0.5301
 960/9333 [==>...........................] - ETA: 20:10 - loss: 0.6923 - acc: 0.5312
1024/9333 [==>...........................] - ETA: 19:59 - loss: 0.6937 - acc: 0.5254
1088/9333 [==>...........................] - ETA: 19:52 - loss: 0.6932 - acc: 0.5276
1152/9333 [==>...........................] - ETA: 19:52 - loss: 0.6931 - acc: 0.5286
1216/9333 [==>...........................] - ETA: 19:40 - loss: 0.6931 - acc: 0.5280
1280/9333 [===>..........................] - ETA: 19:34 - loss: 0.6919 - acc: 0.5312
1344/9333 [===>..........................] - ETA: 19:24 - loss: 0.6915 - acc: 0.5298
1408/9333 [===>..........................] - ETA: 19:15 - loss: 0.6917 - acc: 0.5284
1472/9333 [===>..........................] - ETA: 19:05 - loss: 0.6923 - acc: 0.5258
1536/9333 [===>..........................] - ETA: 18:54 - loss: 0.6920 - acc: 0.5280
1600/9333 [====>.........................] - ETA: 18:46 - loss: 0.6912 - acc: 0.5325
1664/9333 [====>.........................] - ETA: 18:38 - loss: 0.6915 - acc: 0.5337
1728/9333 [====>.........................] - ETA: 18:30 - loss: 0.6915 - acc: 0.5330
1792/9333 [====>.........................] - ETA: 18:21 - loss: 0.6914 - acc: 0.5340
1856/9333 [====>.........................] - ETA: 18:14 - loss: 0.6923 - acc: 0.5312
1920/9333 [=====>........................] - ETA: 18:09 - loss: 0.6923 - acc: 0.5302
1984/9333 [=====>........................] - ETA: 18:00 - loss: 0.6929 - acc: 0.5282
2048/9333 [=====>........................] - ETA: 17:53 - loss: 0.6935 - acc: 0.5254
2112/9333 [=====>........................] - ETA: 17:43 - loss: 0.6931 - acc: 0.5275
2176/9333 [=====>........................] - ETA: 17:34 - loss: 0.6924 - acc: 0.5294
2240/9333 [======>.......................] - ETA: 17:25 - loss: 0.6925 - acc: 0.5268
2304/9333 [======>.......................] - ETA: 17:15 - loss: 0.6921 - acc: 0.5273
2368/9333 [======>.......................] - ETA: 17:06 - loss: 0.6921 - acc: 0.5274
2432/9333 [======>.......................] - ETA: 16:57 - loss: 0.6916 - acc: 0.5292
2496/9333 [=======>......................] - ETA: 16:51 - loss: 0.6910 - acc: 0.5312
2560/9333 [=======>......................] - ETA: 16:42 - loss: 0.6905 - acc: 0.5332
2624/9333 [=======>......................] - ETA: 16:35 - loss: 0.6902 - acc: 0.5339
2688/9333 [=======>......................] - ETA: 16:26 - loss: 0.6910 - acc: 0.5294
2752/9333 [=======>......................] - ETA: 16:17 - loss: 0.6911 - acc: 0.5294
2816/9333 [========>.....................] - ETA: 16:08 - loss: 0.6910 - acc: 0.5302
2880/9333 [========>.....................] - ETA: 15:59 - loss: 0.6915 - acc: 0.5292
2944/9333 [========>.....................] - ETA: 15:52 - loss: 0.6913 - acc: 0.5292
3008/9333 [========>.....................] - ETA: 15:44 - loss: 0.6916 - acc: 0.5269
3072/9333 [========>.....................] - ETA: 15:35 - loss: 0.6914 - acc: 0.5283
3136/9333 [=========>....................] - ETA: 15:26 - loss: 0.6916 - acc: 0.5265
3200/9333 [=========>....................] - ETA: 15:18 - loss: 0.6911 - acc: 0.5284
3264/9333 [=========>....................] - ETA: 15:09 - loss: 0.6906 - acc: 0.5297
3328/9333 [=========>....................] - ETA: 15:00 - loss: 0.6906 - acc: 0.5288
3392/9333 [=========>....................] - ETA: 14:50 - loss: 0.6908 - acc: 0.5289
3456/9333 [==========>...................] - ETA: 14:41 - loss: 0.6908 - acc: 0.5286
3520/9333 [==========>...................] - ETA: 14:32 - loss: 0.6907 - acc: 0.5293
3584/9333 [==========>...................] - ETA: 14:23 - loss: 0.6906 - acc: 0.5287
3648/9333 [==========>...................] - ETA: 14:13 - loss: 0.6911 - acc: 0.5266
3712/9333 [==========>...................] - ETA: 14:05 - loss: 0.6911 - acc: 0.5264
3776/9333 [===========>..................] - ETA: 13:55 - loss: 0.6906 - acc: 0.5281
3840/9333 [===========>..................] - ETA: 13:45 - loss: 0.6905 - acc: 0.5279
3904/9333 [===========>..................] - ETA: 13:35 - loss: 0.6903 - acc: 0.5295
3968/9333 [===========>..................] - ETA: 13:26 - loss: 0.6901 - acc: 0.5305
4032/9333 [===========>..................] - ETA: 13:17 - loss: 0.6900 - acc: 0.5312
4096/9333 [============>.................] - ETA: 13:08 - loss: 0.6901 - acc: 0.5312
4160/9333 [============>.................] - ETA: 12:59 - loss: 0.6907 - acc: 0.5310
4224/9333 [============>.................] - ETA: 12:49 - loss: 0.6908 - acc: 0.5312
4288/9333 [============>.................] - ETA: 12:40 - loss: 0.6907 - acc: 0.5322
4352/9333 [============>.................] - ETA: 12:31 - loss: 0.6909 - acc: 0.5315
4416/9333 [=============>................] - ETA: 12:21 - loss: 0.6909 - acc: 0.5301
4480/9333 [=============>................] - ETA: 12:12 - loss: 0.6911 - acc: 0.5295
4544/9333 [=============>................] - ETA: 12:01 - loss: 0.6911 - acc: 0.5306
4608/9333 [=============>................] - ETA: 11:52 - loss: 0.6908 - acc: 0.5315
4672/9333 [==============>...............] - ETA: 11:42 - loss: 0.6909 - acc: 0.5319
4736/9333 [==============>...............] - ETA: 11:33 - loss: 0.6907 - acc: 0.5329
4800/9333 [==============>...............] - ETA: 11:24 - loss: 0.6910 - acc: 0.5310
4864/9333 [==============>...............] - ETA: 11:15 - loss: 0.6913 - acc: 0.5294
4928/9333 [==============>...............] - ETA: 11:06 - loss: 0.6916 - acc: 0.5284
4992/9333 [===============>..............] - ETA: 10:56 - loss: 0.6916 - acc: 0.5284
5056/9333 [===============>..............] - ETA: 10:47 - loss: 0.6917 - acc: 0.5285
5120/9333 [===============>..............] - ETA: 10:37 - loss: 0.6917 - acc: 0.5285
5184/9333 [===============>..............] - ETA: 10:28 - loss: 0.6917 - acc: 0.5282
5248/9333 [===============>..............] - ETA: 10:18 - loss: 0.6917 - acc: 0.5286
5312/9333 [================>.............] - ETA: 10:09 - loss: 0.6918 - acc: 0.5286
5376/9333 [================>.............] - ETA: 9:59 - loss: 0.6918 - acc: 0.5290 
5440/9333 [================>.............] - ETA: 9:49 - loss: 0.6918 - acc: 0.5300
5504/9333 [================>.............] - ETA: 9:40 - loss: 0.6918 - acc: 0.5300
5568/9333 [================>.............] - ETA: 9:30 - loss: 0.6918 - acc: 0.5304
5632/9333 [=================>............] - ETA: 9:20 - loss: 0.6917 - acc: 0.5309
5696/9333 [=================>............] - ETA: 9:10 - loss: 0.6916 - acc: 0.5320
5760/9333 [=================>............] - ETA: 9:00 - loss: 0.6917 - acc: 0.5306
5824/9333 [=================>............] - ETA: 8:51 - loss: 0.6917 - acc: 0.5311
5888/9333 [=================>............] - ETA: 8:41 - loss: 0.6918 - acc: 0.5306
5952/9333 [==================>...........] - ETA: 8:32 - loss: 0.6915 - acc: 0.5312
6016/9333 [==================>...........] - ETA: 8:22 - loss: 0.6916 - acc: 0.5306
6080/9333 [==================>...........] - ETA: 8:13 - loss: 0.6917 - acc: 0.5301
6144/9333 [==================>...........] - ETA: 8:03 - loss: 0.6916 - acc: 0.5303
6208/9333 [==================>...........] - ETA: 7:53 - loss: 0.6917 - acc: 0.5296
6272/9333 [===================>..........] - ETA: 7:43 - loss: 0.6917 - acc: 0.5303
6336/9333 [===================>..........] - ETA: 7:33 - loss: 0.6915 - acc: 0.5314
6400/9333 [===================>..........] - ETA: 7:24 - loss: 0.6915 - acc: 0.5316
6464/9333 [===================>..........] - ETA: 7:14 - loss: 0.6916 - acc: 0.5309
6528/9333 [===================>..........] - ETA: 7:04 - loss: 0.6914 - acc: 0.5322
6592/9333 [====================>.........] - ETA: 6:54 - loss: 0.6912 - acc: 0.5329
6656/9333 [====================>.........] - ETA: 6:45 - loss: 0.6910 - acc: 0.5341
6720/9333 [====================>.........] - ETA: 6:35 - loss: 0.6908 - acc: 0.5345
6784/9333 [====================>.........] - ETA: 6:25 - loss: 0.6907 - acc: 0.5343
6848/9333 [=====================>........] - ETA: 6:16 - loss: 0.6907 - acc: 0.5342
6912/9333 [=====================>........] - ETA: 6:06 - loss: 0.6907 - acc: 0.5339
6976/9333 [=====================>........] - ETA: 5:56 - loss: 0.6908 - acc: 0.5340
7040/9333 [=====================>........] - ETA: 5:47 - loss: 0.6907 - acc: 0.5351
7104/9333 [=====================>........] - ETA: 5:38 - loss: 0.6906 - acc: 0.5346
7168/9333 [======================>.......] - ETA: 5:28 - loss: 0.6905 - acc: 0.5343
7232/9333 [======================>.......] - ETA: 5:18 - loss: 0.6903 - acc: 0.5344
7296/9333 [======================>.......] - ETA: 5:09 - loss: 0.6901 - acc: 0.5347
7360/9333 [======================>.......] - ETA: 4:59 - loss: 0.6904 - acc: 0.5349
7424/9333 [======================>.......] - ETA: 4:49 - loss: 0.6903 - acc: 0.5346
7488/9333 [=======================>......] - ETA: 4:39 - loss: 0.6904 - acc: 0.5345
7552/9333 [=======================>......] - ETA: 4:29 - loss: 0.6905 - acc: 0.5343
7616/9333 [=======================>......] - ETA: 4:20 - loss: 0.6905 - acc: 0.5344
7680/9333 [=======================>......] - ETA: 4:10 - loss: 0.6904 - acc: 0.5353
7744/9333 [=======================>......] - ETA: 4:00 - loss: 0.6903 - acc: 0.5356
7808/9333 [========================>.....] - ETA: 3:50 - loss: 0.6905 - acc: 0.5355
7872/9333 [========================>.....] - ETA: 3:40 - loss: 0.6904 - acc: 0.5353
7936/9333 [========================>.....] - ETA: 3:31 - loss: 0.6903 - acc: 0.5358
8000/9333 [========================>.....] - ETA: 3:21 - loss: 0.6902 - acc: 0.5356
8064/9333 [========================>.....] - ETA: 3:11 - loss: 0.6904 - acc: 0.5355
8128/9333 [=========================>....] - ETA: 3:01 - loss: 0.6903 - acc: 0.5359
8192/9333 [=========================>....] - ETA: 2:52 - loss: 0.6903 - acc: 0.5361
8256/9333 [=========================>....] - ETA: 2:42 - loss: 0.6902 - acc: 0.5361
8320/9333 [=========================>....] - ETA: 2:32 - loss: 0.6902 - acc: 0.5367
8384/9333 [=========================>....] - ETA: 2:22 - loss: 0.6903 - acc: 0.5370
8448/9333 [==========================>...] - ETA: 2:13 - loss: 0.6905 - acc: 0.5366
8512/9333 [==========================>...] - ETA: 2:03 - loss: 0.6904 - acc: 0.5365
8576/9333 [==========================>...] - ETA: 1:53 - loss: 0.6905 - acc: 0.5360
8640/9333 [==========================>...] - ETA: 1:44 - loss: 0.6906 - acc: 0.5361
8704/9333 [==========================>...] - ETA: 1:34 - loss: 0.6908 - acc: 0.5357
8768/9333 [===========================>..] - ETA: 1:24 - loss: 0.6906 - acc: 0.5364
8832/9333 [===========================>..] - ETA: 1:15 - loss: 0.6905 - acc: 0.5368
8896/9333 [===========================>..] - ETA: 1:05 - loss: 0.6906 - acc: 0.5363
8960/9333 [===========================>..] - ETA: 56s - loss: 0.6906 - acc: 0.5362 
9024/9333 [============================>.] - ETA: 46s - loss: 0.6905 - acc: 0.5362
9088/9333 [============================>.] - ETA: 36s - loss: 0.6906 - acc: 0.5362
9152/9333 [============================>.] - ETA: 27s - loss: 0.6908 - acc: 0.5356
9216/9333 [============================>.] - ETA: 17s - loss: 0.6907 - acc: 0.5354
9280/9333 [============================>.] - ETA: 7s - loss: 0.6909 - acc: 0.5349 
9333/9333 [==============================] - 1448s 155ms/step - loss: 0.6910 - acc: 0.5346 - val_loss: 0.6919 - val_acc: 0.5178

Epoch 00005: val_acc did not improve from 0.52459
Epoch 6/10

  64/9333 [..............................] - ETA: 21:26 - loss: 0.7152 - acc: 0.4531
 128/9333 [..............................] - ETA: 22:01 - loss: 0.7087 - acc: 0.4453
 192/9333 [..............................] - ETA: 21:42 - loss: 0.7039 - acc: 0.4583
 256/9333 [..............................] - ETA: 21:41 - loss: 0.6959 - acc: 0.5000
 320/9333 [>.............................] - ETA: 22:06 - loss: 0.6941 - acc: 0.4969
 384/9333 [>.............................] - ETA: 22:02 - loss: 0.6908 - acc: 0.5182
 448/9333 [>.............................] - ETA: 21:49 - loss: 0.6908 - acc: 0.5223
 512/9333 [>.............................] - ETA: 21:39 - loss: 0.6882 - acc: 0.5312
 576/9333 [>.............................] - ETA: 21:32 - loss: 0.6861 - acc: 0.5434
 640/9333 [=>............................] - ETA: 21:24 - loss: 0.6865 - acc: 0.5359
 704/9333 [=>............................] - ETA: 21:22 - loss: 0.6863 - acc: 0.5384
 768/9333 [=>............................] - ETA: 21:07 - loss: 0.6887 - acc: 0.5273
 832/9333 [=>............................] - ETA: 20:59 - loss: 0.6871 - acc: 0.5349
 896/9333 [=>............................] - ETA: 20:47 - loss: 0.6875 - acc: 0.5312
 960/9333 [==>...........................] - ETA: 20:35 - loss: 0.6887 - acc: 0.5281
1024/9333 [==>...........................] - ETA: 20:27 - loss: 0.6872 - acc: 0.5352
1088/9333 [==>...........................] - ETA: 20:21 - loss: 0.6872 - acc: 0.5312
1152/9333 [==>...........................] - ETA: 20:14 - loss: 0.6865 - acc: 0.5339
1216/9333 [==>...........................] - ETA: 20:06 - loss: 0.6858 - acc: 0.5378
1280/9333 [===>..........................] - ETA: 19:56 - loss: 0.6864 - acc: 0.5375
1344/9333 [===>..........................] - ETA: 19:49 - loss: 0.6868 - acc: 0.5365
1408/9333 [===>..........................] - ETA: 19:43 - loss: 0.6865 - acc: 0.5362
1472/9333 [===>..........................] - ETA: 19:36 - loss: 0.6859 - acc: 0.5387
1536/9333 [===>..........................] - ETA: 19:28 - loss: 0.6868 - acc: 0.5371
1600/9333 [====>.........................] - ETA: 19:19 - loss: 0.6869 - acc: 0.5363
1664/9333 [====>.........................] - ETA: 19:08 - loss: 0.6874 - acc: 0.5349
1728/9333 [====>.........................] - ETA: 18:59 - loss: 0.6876 - acc: 0.5359
1792/9333 [====>.........................] - ETA: 18:51 - loss: 0.6883 - acc: 0.5363
1856/9333 [====>.........................] - ETA: 18:42 - loss: 0.6891 - acc: 0.5339
1920/9333 [=====>........................] - ETA: 18:35 - loss: 0.6892 - acc: 0.5349
1984/9333 [=====>........................] - ETA: 18:27 - loss: 0.6883 - acc: 0.5388
2048/9333 [=====>........................] - ETA: 18:19 - loss: 0.6881 - acc: 0.5396
2112/9333 [=====>........................] - ETA: 18:10 - loss: 0.6876 - acc: 0.5417
2176/9333 [=====>........................] - ETA: 18:02 - loss: 0.6886 - acc: 0.5409
2240/9333 [======>.......................] - ETA: 17:53 - loss: 0.6887 - acc: 0.5402
2304/9333 [======>.......................] - ETA: 17:42 - loss: 0.6901 - acc: 0.5373
2368/9333 [======>.......................] - ETA: 17:35 - loss: 0.6897 - acc: 0.5376
2432/9333 [======>.......................] - ETA: 17:25 - loss: 0.6903 - acc: 0.5354
2496/9333 [=======>......................] - ETA: 17:17 - loss: 0.6908 - acc: 0.5349
2560/9333 [=======>......................] - ETA: 17:09 - loss: 0.6917 - acc: 0.5332
2624/9333 [=======>......................] - ETA: 16:58 - loss: 0.6913 - acc: 0.5354
2688/9333 [=======>......................] - ETA: 16:50 - loss: 0.6913 - acc: 0.5361
2752/9333 [=======>......................] - ETA: 16:39 - loss: 0.6917 - acc: 0.5349
2816/9333 [========>.....................] - ETA: 16:31 - loss: 0.6918 - acc: 0.5337
2880/9333 [========>.....................] - ETA: 16:22 - loss: 0.6918 - acc: 0.5347
2944/9333 [========>.....................] - ETA: 16:13 - loss: 0.6916 - acc: 0.5343
3008/9333 [========>.....................] - ETA: 16:05 - loss: 0.6913 - acc: 0.5352
3072/9333 [========>.....................] - ETA: 15:56 - loss: 0.6913 - acc: 0.5342
3136/9333 [=========>....................] - ETA: 15:46 - loss: 0.6915 - acc: 0.5332
3200/9333 [=========>....................] - ETA: 15:38 - loss: 0.6921 - acc: 0.5316
3264/9333 [=========>....................] - ETA: 15:28 - loss: 0.6921 - acc: 0.5312
3328/9333 [=========>....................] - ETA: 15:20 - loss: 0.6923 - acc: 0.5312
3392/9333 [=========>....................] - ETA: 15:11 - loss: 0.6922 - acc: 0.5324
3456/9333 [==========>...................] - ETA: 15:04 - loss: 0.6921 - acc: 0.5333
3520/9333 [==========>...................] - ETA: 14:54 - loss: 0.6924 - acc: 0.5312
3584/9333 [==========>...................] - ETA: 14:45 - loss: 0.6922 - acc: 0.5332
3648/9333 [==========>...................] - ETA: 14:36 - loss: 0.6923 - acc: 0.5345
3712/9333 [==========>...................] - ETA: 14:27 - loss: 0.6923 - acc: 0.5350
3776/9333 [===========>..................] - ETA: 14:17 - loss: 0.6922 - acc: 0.5350
3840/9333 [===========>..................] - ETA: 14:07 - loss: 0.6924 - acc: 0.5336
3904/9333 [===========>..................] - ETA: 13:57 - loss: 0.6925 - acc: 0.5333
3968/9333 [===========>..................] - ETA: 13:48 - loss: 0.6926 - acc: 0.5330
4032/9333 [===========>..................] - ETA: 13:39 - loss: 0.6924 - acc: 0.5337
4096/9333 [============>.................] - ETA: 13:30 - loss: 0.6920 - acc: 0.5354
4160/9333 [============>.................] - ETA: 13:20 - loss: 0.6923 - acc: 0.5349
4224/9333 [============>.................] - ETA: 13:11 - loss: 0.6923 - acc: 0.5334
4288/9333 [============>.................] - ETA: 13:01 - loss: 0.6921 - acc: 0.5343
4352/9333 [============>.................] - ETA: 12:51 - loss: 0.6920 - acc: 0.5352
4416/9333 [=============>................] - ETA: 12:41 - loss: 0.6917 - acc: 0.5356
4480/9333 [=============>................] - ETA: 12:32 - loss: 0.6920 - acc: 0.5348
4544/9333 [=============>................] - ETA: 12:22 - loss: 0.6917 - acc: 0.5359
4608/9333 [=============>................] - ETA: 12:13 - loss: 0.6916 - acc: 0.5365
4672/9333 [==============>...............] - ETA: 12:02 - loss: 0.6917 - acc: 0.5368
4736/9333 [==============>...............] - ETA: 11:53 - loss: 0.6916 - acc: 0.5382
4800/9333 [==============>...............] - ETA: 11:42 - loss: 0.6917 - acc: 0.5383
4864/9333 [==============>...............] - ETA: 11:33 - loss: 0.6919 - acc: 0.5372
4928/9333 [==============>...............] - ETA: 11:23 - loss: 0.6920 - acc: 0.5363
4992/9333 [===============>..............] - ETA: 11:13 - loss: 0.6920 - acc: 0.5357
5056/9333 [===============>..............] - ETA: 11:03 - loss: 0.6922 - acc: 0.5342
5120/9333 [===============>..............] - ETA: 10:54 - loss: 0.6922 - acc: 0.5344
5184/9333 [===============>..............] - ETA: 10:44 - loss: 0.6924 - acc: 0.5343
5248/9333 [===============>..............] - ETA: 10:35 - loss: 0.6924 - acc: 0.5335
5312/9333 [================>.............] - ETA: 10:24 - loss: 0.6925 - acc: 0.5339
5376/9333 [================>.............] - ETA: 10:15 - loss: 0.6923 - acc: 0.5346
5440/9333 [================>.............] - ETA: 10:05 - loss: 0.6924 - acc: 0.5347
5504/9333 [================>.............] - ETA: 9:55 - loss: 0.6924 - acc: 0.5343 
5568/9333 [================>.............] - ETA: 9:45 - loss: 0.6924 - acc: 0.5338
5632/9333 [=================>............] - ETA: 9:36 - loss: 0.6924 - acc: 0.5339
5696/9333 [=================>............] - ETA: 9:26 - loss: 0.6925 - acc: 0.5337
5760/9333 [=================>............] - ETA: 9:16 - loss: 0.6925 - acc: 0.5330
5824/9333 [=================>............] - ETA: 9:06 - loss: 0.6924 - acc: 0.5335
5888/9333 [=================>............] - ETA: 8:57 - loss: 0.6922 - acc: 0.5341
5952/9333 [==================>...........] - ETA: 8:47 - loss: 0.6922 - acc: 0.5338
6016/9333 [==================>...........] - ETA: 8:37 - loss: 0.6920 - acc: 0.5344
6080/9333 [==================>...........] - ETA: 8:26 - loss: 0.6922 - acc: 0.5334
6144/9333 [==================>...........] - ETA: 8:16 - loss: 0.6922 - acc: 0.5334
6208/9333 [==================>...........] - ETA: 8:07 - loss: 0.6921 - acc: 0.5338
6272/9333 [===================>..........] - ETA: 7:57 - loss: 0.6920 - acc: 0.5344
6336/9333 [===================>..........] - ETA: 7:47 - loss: 0.6919 - acc: 0.5347
6400/9333 [===================>..........] - ETA: 7:37 - loss: 0.6917 - acc: 0.5353
6464/9333 [===================>..........] - ETA: 7:27 - loss: 0.6917 - acc: 0.5353
6528/9333 [===================>..........] - ETA: 7:17 - loss: 0.6916 - acc: 0.5355
6592/9333 [====================>.........] - ETA: 7:07 - loss: 0.6917 - acc: 0.5352
6656/9333 [====================>.........] - ETA: 6:58 - loss: 0.6921 - acc: 0.5347
6720/9333 [====================>.........] - ETA: 6:47 - loss: 0.6920 - acc: 0.5347
6784/9333 [====================>.........] - ETA: 6:37 - loss: 0.6924 - acc: 0.5336
6848/9333 [=====================>........] - ETA: 6:27 - loss: 0.6923 - acc: 0.5342
6912/9333 [=====================>........] - ETA: 6:17 - loss: 0.6924 - acc: 0.5337
6976/9333 [=====================>........] - ETA: 6:08 - loss: 0.6922 - acc: 0.5341
7040/9333 [=====================>........] - ETA: 5:58 - loss: 0.6921 - acc: 0.5342
7104/9333 [=====================>........] - ETA: 5:48 - loss: 0.6920 - acc: 0.5342
7168/9333 [======================>.......] - ETA: 5:37 - loss: 0.6922 - acc: 0.5333
7232/9333 [======================>.......] - ETA: 5:28 - loss: 0.6919 - acc: 0.5342
7296/9333 [======================>.......] - ETA: 5:18 - loss: 0.6917 - acc: 0.5347
7360/9333 [======================>.......] - ETA: 5:08 - loss: 0.6915 - acc: 0.5356
7424/9333 [======================>.......] - ETA: 4:58 - loss: 0.6915 - acc: 0.5354
7488/9333 [=======================>......] - ETA: 4:48 - loss: 0.6911 - acc: 0.5367
7552/9333 [=======================>......] - ETA: 4:37 - loss: 0.6910 - acc: 0.5367
7616/9333 [=======================>......] - ETA: 4:27 - loss: 0.6911 - acc: 0.5365
7680/9333 [=======================>......] - ETA: 4:17 - loss: 0.6912 - acc: 0.5359
7744/9333 [=======================>......] - ETA: 4:07 - loss: 0.6912 - acc: 0.5369
7808/9333 [========================>.....] - ETA: 3:57 - loss: 0.6913 - acc: 0.5366
7872/9333 [========================>.....] - ETA: 3:47 - loss: 0.6911 - acc: 0.5367
7936/9333 [========================>.....] - ETA: 3:37 - loss: 0.6912 - acc: 0.5360
8000/9333 [========================>.....] - ETA: 3:26 - loss: 0.6912 - acc: 0.5361
8064/9333 [========================>.....] - ETA: 3:16 - loss: 0.6911 - acc: 0.5361
8128/9333 [=========================>....] - ETA: 3:06 - loss: 0.6912 - acc: 0.5348
8192/9333 [=========================>....] - ETA: 2:56 - loss: 0.6914 - acc: 0.5344
8256/9333 [=========================>....] - ETA: 2:46 - loss: 0.6916 - acc: 0.5342
8320/9333 [=========================>....] - ETA: 2:36 - loss: 0.6916 - acc: 0.5340
8384/9333 [=========================>....] - ETA: 2:26 - loss: 0.6913 - acc: 0.5347
8448/9333 [==========================>...] - ETA: 2:16 - loss: 0.6914 - acc: 0.5344
8512/9333 [==========================>...] - ETA: 2:06 - loss: 0.6913 - acc: 0.5350
8576/9333 [==========================>...] - ETA: 1:57 - loss: 0.6910 - acc: 0.5364
8640/9333 [==========================>...] - ETA: 1:47 - loss: 0.6912 - acc: 0.5359
8704/9333 [==========================>...] - ETA: 1:37 - loss: 0.6911 - acc: 0.5361
8768/9333 [===========================>..] - ETA: 1:27 - loss: 0.6913 - acc: 0.5355
8832/9333 [===========================>..] - ETA: 1:17 - loss: 0.6914 - acc: 0.5358
8896/9333 [===========================>..] - ETA: 1:07 - loss: 0.6914 - acc: 0.5353
8960/9333 [===========================>..] - ETA: 57s - loss: 0.6914 - acc: 0.5352 
9024/9333 [============================>.] - ETA: 47s - loss: 0.6914 - acc: 0.5354
9088/9333 [============================>.] - ETA: 37s - loss: 0.6914 - acc: 0.5354
9152/9333 [============================>.] - ETA: 27s - loss: 0.6915 - acc: 0.5351
9216/9333 [============================>.] - ETA: 18s - loss: 0.6915 - acc: 0.5349
9280/9333 [============================>.] - ETA: 8s - loss: 0.6914 - acc: 0.5355 
9333/9333 [==============================] - 1495s 160ms/step - loss: 0.6914 - acc: 0.5359 - val_loss: 0.6906 - val_acc: 0.5284

Epoch 00006: val_acc improved from 0.52459 to 0.52845, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window20/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 7/10

  64/9333 [..............................] - ETA: 21:53 - loss: 0.6835 - acc: 0.5469
 128/9333 [..............................] - ETA: 22:15 - loss: 0.6937 - acc: 0.5156
 192/9333 [..............................] - ETA: 22:17 - loss: 0.6935 - acc: 0.5260
 256/9333 [..............................] - ETA: 22:18 - loss: 0.6924 - acc: 0.5195
 320/9333 [>.............................] - ETA: 22:11 - loss: 0.6888 - acc: 0.5312
 384/9333 [>.............................] - ETA: 22:08 - loss: 0.6886 - acc: 0.5339
 448/9333 [>.............................] - ETA: 22:11 - loss: 0.6886 - acc: 0.5469
 512/9333 [>.............................] - ETA: 22:01 - loss: 0.6873 - acc: 0.5488
 576/9333 [>.............................] - ETA: 21:54 - loss: 0.6886 - acc: 0.5434
 640/9333 [=>............................] - ETA: 21:46 - loss: 0.6907 - acc: 0.5375
 704/9333 [=>............................] - ETA: 21:37 - loss: 0.6889 - acc: 0.5440
 768/9333 [=>............................] - ETA: 21:24 - loss: 0.6886 - acc: 0.5482
 832/9333 [=>............................] - ETA: 21:12 - loss: 0.6855 - acc: 0.5553
 896/9333 [=>............................] - ETA: 21:02 - loss: 0.6857 - acc: 0.5536
 960/9333 [==>...........................] - ETA: 20:53 - loss: 0.6839 - acc: 0.5604
1024/9333 [==>...........................] - ETA: 20:45 - loss: 0.6851 - acc: 0.5576
1088/9333 [==>...........................] - ETA: 20:38 - loss: 0.6852 - acc: 0.5597
1152/9333 [==>...........................] - ETA: 20:26 - loss: 0.6859 - acc: 0.5590
1216/9333 [==>...........................] - ETA: 20:19 - loss: 0.6854 - acc: 0.5609
1280/9333 [===>..........................] - ETA: 20:08 - loss: 0.6867 - acc: 0.5563
1344/9333 [===>..........................] - ETA: 19:59 - loss: 0.6866 - acc: 0.5565
1408/9333 [===>..........................] - ETA: 19:53 - loss: 0.6870 - acc: 0.5568
1472/9333 [===>..........................] - ETA: 19:45 - loss: 0.6865 - acc: 0.5557
1536/9333 [===>..........................] - ETA: 19:35 - loss: 0.6869 - acc: 0.5527
1600/9333 [====>.........................] - ETA: 19:27 - loss: 0.6863 - acc: 0.5563
1664/9333 [====>.........................] - ETA: 19:18 - loss: 0.6860 - acc: 0.5559
1728/9333 [====>.........................] - ETA: 19:10 - loss: 0.6854 - acc: 0.5579
1792/9333 [====>.........................] - ETA: 18:59 - loss: 0.6855 - acc: 0.5592
1856/9333 [====>.........................] - ETA: 18:47 - loss: 0.6853 - acc: 0.5577
1920/9333 [=====>........................] - ETA: 18:35 - loss: 0.6864 - acc: 0.5557
1984/9333 [=====>........................] - ETA: 18:25 - loss: 0.6864 - acc: 0.5544
2048/9333 [=====>........................] - ETA: 18:15 - loss: 0.6867 - acc: 0.5537
2112/9333 [=====>........................] - ETA: 18:04 - loss: 0.6863 - acc: 0.5521
2176/9333 [=====>........................] - ETA: 17:53 - loss: 0.6865 - acc: 0.5501
2240/9333 [======>.......................] - ETA: 17:41 - loss: 0.6868 - acc: 0.5464
2304/9333 [======>.......................] - ETA: 17:29 - loss: 0.6868 - acc: 0.5486
2368/9333 [======>.......................] - ETA: 17:19 - loss: 0.6868 - acc: 0.5481
2432/9333 [======>.......................] - ETA: 17:08 - loss: 0.6869 - acc: 0.5473
2496/9333 [=======>......................] - ETA: 16:58 - loss: 0.6864 - acc: 0.5481
2560/9333 [=======>......................] - ETA: 16:49 - loss: 0.6863 - acc: 0.5488
2624/9333 [=======>......................] - ETA: 16:38 - loss: 0.6861 - acc: 0.5503
2688/9333 [=======>......................] - ETA: 16:29 - loss: 0.6867 - acc: 0.5491
2752/9333 [=======>......................] - ETA: 16:18 - loss: 0.6866 - acc: 0.5509
2816/9333 [========>.....................] - ETA: 16:08 - loss: 0.6868 - acc: 0.5490
2880/9333 [========>.....................] - ETA: 15:59 - loss: 0.6866 - acc: 0.5497
2944/9333 [========>.....................] - ETA: 15:50 - loss: 0.6865 - acc: 0.5496
3008/9333 [========>.....................] - ETA: 15:40 - loss: 0.6866 - acc: 0.5489
3072/9333 [========>.....................] - ETA: 15:30 - loss: 0.6877 - acc: 0.5439
3136/9333 [=========>....................] - ETA: 15:21 - loss: 0.6877 - acc: 0.5437
3200/9333 [=========>....................] - ETA: 15:11 - loss: 0.6878 - acc: 0.5431
3264/9333 [=========>....................] - ETA: 15:03 - loss: 0.6879 - acc: 0.5426
3328/9333 [=========>....................] - ETA: 14:54 - loss: 0.6875 - acc: 0.5448
3392/9333 [=========>....................] - ETA: 14:44 - loss: 0.6874 - acc: 0.5457
3456/9333 [==========>...................] - ETA: 14:35 - loss: 0.6876 - acc: 0.5457
3520/9333 [==========>...................] - ETA: 14:26 - loss: 0.6874 - acc: 0.5463
3584/9333 [==========>...................] - ETA: 14:16 - loss: 0.6873 - acc: 0.5452
3648/9333 [==========>...................] - ETA: 14:06 - loss: 0.6874 - acc: 0.5444
3712/9333 [==========>...................] - ETA: 13:57 - loss: 0.6874 - acc: 0.5447
3776/9333 [===========>..................] - ETA: 13:47 - loss: 0.6875 - acc: 0.5458
3840/9333 [===========>..................] - ETA: 13:38 - loss: 0.6875 - acc: 0.5453
3904/9333 [===========>..................] - ETA: 13:29 - loss: 0.6873 - acc: 0.5461
3968/9333 [===========>..................] - ETA: 13:19 - loss: 0.6874 - acc: 0.5451
4032/9333 [===========>..................] - ETA: 13:10 - loss: 0.6877 - acc: 0.5451
4096/9333 [============>.................] - ETA: 13:01 - loss: 0.6878 - acc: 0.5444
4160/9333 [============>.................] - ETA: 12:51 - loss: 0.6875 - acc: 0.5454
4224/9333 [============>.................] - ETA: 12:42 - loss: 0.6878 - acc: 0.5436
4288/9333 [============>.................] - ETA: 12:33 - loss: 0.6879 - acc: 0.5431
4352/9333 [============>.................] - ETA: 12:23 - loss: 0.6880 - acc: 0.5437
4416/9333 [=============>................] - ETA: 12:14 - loss: 0.6879 - acc: 0.5433
4480/9333 [=============>................] - ETA: 12:04 - loss: 0.6879 - acc: 0.5431
4544/9333 [=============>................] - ETA: 11:54 - loss: 0.6879 - acc: 0.5423
4608/9333 [=============>................] - ETA: 11:44 - loss: 0.6880 - acc: 0.5414
4672/9333 [==============>...............] - ETA: 11:35 - loss: 0.6879 - acc: 0.5424
4736/9333 [==============>...............] - ETA: 11:26 - loss: 0.6880 - acc: 0.5424
4800/9333 [==============>...............] - ETA: 11:16 - loss: 0.6880 - acc: 0.5425
4864/9333 [==============>...............] - ETA: 11:06 - loss: 0.6881 - acc: 0.5424
4928/9333 [==============>...............] - ETA: 10:57 - loss: 0.6883 - acc: 0.5412
4992/9333 [===============>..............] - ETA: 10:47 - loss: 0.6885 - acc: 0.5403
5056/9333 [===============>..............] - ETA: 10:38 - loss: 0.6886 - acc: 0.5403
5120/9333 [===============>..............] - ETA: 10:29 - loss: 0.6889 - acc: 0.5395
5184/9333 [===============>..............] - ETA: 10:19 - loss: 0.6888 - acc: 0.5397
5248/9333 [===============>..............] - ETA: 10:09 - loss: 0.6889 - acc: 0.5394
5312/9333 [================>.............] - ETA: 9:59 - loss: 0.6891 - acc: 0.5386 
5376/9333 [================>.............] - ETA: 9:50 - loss: 0.6893 - acc: 0.5381
5440/9333 [================>.............] - ETA: 9:41 - loss: 0.6893 - acc: 0.5379
5504/9333 [================>.............] - ETA: 9:31 - loss: 0.6892 - acc: 0.5382
5568/9333 [================>.............] - ETA: 9:21 - loss: 0.6891 - acc: 0.5384
5632/9333 [=================>............] - ETA: 9:12 - loss: 0.6893 - acc: 0.5380
5696/9333 [=================>............] - ETA: 9:02 - loss: 0.6894 - acc: 0.5376
5760/9333 [=================>............] - ETA: 8:53 - loss: 0.6893 - acc: 0.5378
5824/9333 [=================>............] - ETA: 8:44 - loss: 0.6893 - acc: 0.5381
5888/9333 [=================>............] - ETA: 8:34 - loss: 0.6895 - acc: 0.5374
5952/9333 [==================>...........] - ETA: 8:25 - loss: 0.6895 - acc: 0.5365
6016/9333 [==================>...........] - ETA: 8:15 - loss: 0.6895 - acc: 0.5367
6080/9333 [==================>...........] - ETA: 8:06 - loss: 0.6894 - acc: 0.5368
6144/9333 [==================>...........] - ETA: 7:56 - loss: 0.6893 - acc: 0.5378
6208/9333 [==================>...........] - ETA: 7:46 - loss: 0.6894 - acc: 0.5374
6272/9333 [===================>..........] - ETA: 7:37 - loss: 0.6894 - acc: 0.5376
6336/9333 [===================>..........] - ETA: 7:27 - loss: 0.6895 - acc: 0.5382
6400/9333 [===================>..........] - ETA: 7:18 - loss: 0.6895 - acc: 0.5383
6464/9333 [===================>..........] - ETA: 7:08 - loss: 0.6892 - acc: 0.5396
6528/9333 [===================>..........] - ETA: 6:59 - loss: 0.6893 - acc: 0.5395
6592/9333 [====================>.........] - ETA: 6:49 - loss: 0.6892 - acc: 0.5393
6656/9333 [====================>.........] - ETA: 6:39 - loss: 0.6890 - acc: 0.5404
6720/9333 [====================>.........] - ETA: 6:30 - loss: 0.6889 - acc: 0.5409
6784/9333 [====================>.........] - ETA: 6:21 - loss: 0.6888 - acc: 0.5419
6848/9333 [=====================>........] - ETA: 6:11 - loss: 0.6889 - acc: 0.5416
6912/9333 [=====================>........] - ETA: 6:02 - loss: 0.6890 - acc: 0.5407
6976/9333 [=====================>........] - ETA: 5:52 - loss: 0.6890 - acc: 0.5404
7040/9333 [=====================>........] - ETA: 5:42 - loss: 0.6890 - acc: 0.5406
7104/9333 [=====================>........] - ETA: 5:33 - loss: 0.6889 - acc: 0.5411
7168/9333 [======================>.......] - ETA: 5:23 - loss: 0.6888 - acc: 0.5414
7232/9333 [======================>.......] - ETA: 5:13 - loss: 0.6889 - acc: 0.5411
7296/9333 [======================>.......] - ETA: 5:04 - loss: 0.6887 - acc: 0.5415
7360/9333 [======================>.......] - ETA: 4:54 - loss: 0.6890 - acc: 0.5409
7424/9333 [======================>.......] - ETA: 4:45 - loss: 0.6891 - acc: 0.5409
7488/9333 [=======================>......] - ETA: 4:35 - loss: 0.6891 - acc: 0.5413
7552/9333 [=======================>......] - ETA: 4:25 - loss: 0.6890 - acc: 0.5417
7616/9333 [=======================>......] - ETA: 4:16 - loss: 0.6890 - acc: 0.5414
7680/9333 [=======================>......] - ETA: 4:06 - loss: 0.6889 - acc: 0.5418
7744/9333 [=======================>......] - ETA: 3:56 - loss: 0.6889 - acc: 0.5415
7808/9333 [========================>.....] - ETA: 3:47 - loss: 0.6891 - acc: 0.5409
7872/9333 [========================>.....] - ETA: 3:37 - loss: 0.6892 - acc: 0.5403
7936/9333 [========================>.....] - ETA: 3:28 - loss: 0.6889 - acc: 0.5412
8000/9333 [========================>.....] - ETA: 3:18 - loss: 0.6887 - acc: 0.5417
8064/9333 [========================>.....] - ETA: 3:08 - loss: 0.6888 - acc: 0.5412
8128/9333 [=========================>....] - ETA: 2:59 - loss: 0.6885 - acc: 0.5418
8192/9333 [=========================>....] - ETA: 2:49 - loss: 0.6884 - acc: 0.5413
8256/9333 [=========================>....] - ETA: 2:40 - loss: 0.6883 - acc: 0.5419
8320/9333 [=========================>....] - ETA: 2:30 - loss: 0.6884 - acc: 0.5412
8384/9333 [=========================>....] - ETA: 2:21 - loss: 0.6885 - acc: 0.5410
8448/9333 [==========================>...] - ETA: 2:11 - loss: 0.6886 - acc: 0.5408
8512/9333 [==========================>...] - ETA: 2:02 - loss: 0.6885 - acc: 0.5408
8576/9333 [==========================>...] - ETA: 1:52 - loss: 0.6886 - acc: 0.5409
8640/9333 [==========================>...] - ETA: 1:43 - loss: 0.6888 - acc: 0.5400
8704/9333 [==========================>...] - ETA: 1:33 - loss: 0.6888 - acc: 0.5400
8768/9333 [===========================>..] - ETA: 1:24 - loss: 0.6890 - acc: 0.5399
8832/9333 [===========================>..] - ETA: 1:14 - loss: 0.6889 - acc: 0.5403
8896/9333 [===========================>..] - ETA: 1:05 - loss: 0.6891 - acc: 0.5399
8960/9333 [===========================>..] - ETA: 55s - loss: 0.6891 - acc: 0.5403 
9024/9333 [============================>.] - ETA: 45s - loss: 0.6891 - acc: 0.5408
9088/9333 [============================>.] - ETA: 36s - loss: 0.6892 - acc: 0.5408
9152/9333 [============================>.] - ETA: 26s - loss: 0.6893 - acc: 0.5409
9216/9333 [============================>.] - ETA: 17s - loss: 0.6894 - acc: 0.5407
9280/9333 [============================>.] - ETA: 7s - loss: 0.6893 - acc: 0.5409 
9333/9333 [==============================] - 1443s 155ms/step - loss: 0.6893 - acc: 0.5409 - val_loss: 0.6928 - val_acc: 0.5381

Epoch 00007: val_acc improved from 0.52845 to 0.53809, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window20/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 8/10

  64/9333 [..............................] - ETA: 23:12 - loss: 0.7007 - acc: 0.5312
 128/9333 [..............................] - ETA: 23:14 - loss: 0.6897 - acc: 0.5469
 192/9333 [..............................] - ETA: 23:06 - loss: 0.6863 - acc: 0.5260
 256/9333 [..............................] - ETA: 22:56 - loss: 0.6897 - acc: 0.5195
 320/9333 [>.............................] - ETA: 22:41 - loss: 0.6926 - acc: 0.5062
 384/9333 [>.............................] - ETA: 22:27 - loss: 0.6909 - acc: 0.5156
 448/9333 [>.............................] - ETA: 22:17 - loss: 0.6922 - acc: 0.5089
 512/9333 [>.............................] - ETA: 22:04 - loss: 0.6918 - acc: 0.5078
 576/9333 [>.............................] - ETA: 21:52 - loss: 0.6871 - acc: 0.5278
 640/9333 [=>............................] - ETA: 21:49 - loss: 0.6870 - acc: 0.5375
 704/9333 [=>............................] - ETA: 21:44 - loss: 0.6837 - acc: 0.5511
 768/9333 [=>............................] - ETA: 21:31 - loss: 0.6853 - acc: 0.5495
 832/9333 [=>............................] - ETA: 21:24 - loss: 0.6872 - acc: 0.5445
 896/9333 [=>............................] - ETA: 21:08 - loss: 0.6861 - acc: 0.5513
 960/9333 [==>...........................] - ETA: 20:56 - loss: 0.6841 - acc: 0.5594
1024/9333 [==>...........................] - ETA: 20:42 - loss: 0.6835 - acc: 0.5625
1088/9333 [==>...........................] - ETA: 20:32 - loss: 0.6842 - acc: 0.5625
1152/9333 [==>...........................] - ETA: 20:23 - loss: 0.6840 - acc: 0.5625
1216/9333 [==>...........................] - ETA: 20:19 - loss: 0.6842 - acc: 0.5609
1280/9333 [===>..........................] - ETA: 20:08 - loss: 0.6840 - acc: 0.5609
1344/9333 [===>..........................] - ETA: 19:59 - loss: 0.6835 - acc: 0.5625
1408/9333 [===>..........................] - ETA: 19:52 - loss: 0.6839 - acc: 0.5604
1472/9333 [===>..........................] - ETA: 19:44 - loss: 0.6843 - acc: 0.5591
1536/9333 [===>..........................] - ETA: 19:36 - loss: 0.6832 - acc: 0.5605
1600/9333 [====>.........................] - ETA: 19:31 - loss: 0.6829 - acc: 0.5644
1664/9333 [====>.........................] - ETA: 19:21 - loss: 0.6837 - acc: 0.5631
1728/9333 [====>.........................] - ETA: 19:10 - loss: 0.6843 - acc: 0.5619
1792/9333 [====>.........................] - ETA: 18:59 - loss: 0.6837 - acc: 0.5658
1856/9333 [====>.........................] - ETA: 18:50 - loss: 0.6850 - acc: 0.5625
1920/9333 [=====>........................] - ETA: 18:40 - loss: 0.6853 - acc: 0.5620
1984/9333 [=====>........................] - ETA: 18:31 - loss: 0.6850 - acc: 0.5625
2048/9333 [=====>........................] - ETA: 18:22 - loss: 0.6856 - acc: 0.5649
2112/9333 [=====>........................] - ETA: 18:11 - loss: 0.6866 - acc: 0.5620
2176/9333 [=====>........................] - ETA: 18:02 - loss: 0.6866 - acc: 0.5611
2240/9333 [======>.......................] - ETA: 17:52 - loss: 0.6858 - acc: 0.5616
2304/9333 [======>.......................] - ETA: 17:42 - loss: 0.6857 - acc: 0.5603
2368/9333 [======>.......................] - ETA: 17:33 - loss: 0.6852 - acc: 0.5604
2432/9333 [======>.......................] - ETA: 17:22 - loss: 0.6857 - acc: 0.5584
2496/9333 [=======>......................] - ETA: 17:14 - loss: 0.6862 - acc: 0.5553
2560/9333 [=======>......................] - ETA: 17:03 - loss: 0.6860 - acc: 0.5570
2624/9333 [=======>......................] - ETA: 16:54 - loss: 0.6861 - acc: 0.5549
2688/9333 [=======>......................] - ETA: 16:43 - loss: 0.6858 - acc: 0.5565
2752/9333 [=======>......................] - ETA: 16:36 - loss: 0.6856 - acc: 0.5560
2816/9333 [========>.....................] - ETA: 16:27 - loss: 0.6851 - acc: 0.5575
2880/9333 [========>.....................] - ETA: 16:18 - loss: 0.6851 - acc: 0.5563
2944/9333 [========>.....................] - ETA: 16:06 - loss: 0.6854 - acc: 0.5540
3008/9333 [========>.....................] - ETA: 15:55 - loss: 0.6855 - acc: 0.5529
3072/9333 [========>.....................] - ETA: 15:45 - loss: 0.6856 - acc: 0.5511
3136/9333 [=========>....................] - ETA: 15:34 - loss: 0.6860 - acc: 0.5507
3200/9333 [=========>....................] - ETA: 15:24 - loss: 0.6863 - acc: 0.5494
3264/9333 [=========>....................] - ETA: 15:15 - loss: 0.6870 - acc: 0.5463
3328/9333 [=========>....................] - ETA: 15:05 - loss: 0.6867 - acc: 0.5475
3392/9333 [=========>....................] - ETA: 14:56 - loss: 0.6868 - acc: 0.5472
3456/9333 [==========>...................] - ETA: 14:46 - loss: 0.6867 - acc: 0.5463
3520/9333 [==========>...................] - ETA: 14:37 - loss: 0.6866 - acc: 0.5466
3584/9333 [==========>...................] - ETA: 14:28 - loss: 0.6865 - acc: 0.5477
3648/9333 [==========>...................] - ETA: 14:19 - loss: 0.6863 - acc: 0.5482
3712/9333 [==========>...................] - ETA: 14:09 - loss: 0.6865 - acc: 0.5477
3776/9333 [===========>..................] - ETA: 14:00 - loss: 0.6865 - acc: 0.5471
3840/9333 [===========>..................] - ETA: 13:49 - loss: 0.6863 - acc: 0.5484
3904/9333 [===========>..................] - ETA: 13:40 - loss: 0.6859 - acc: 0.5492
3968/9333 [===========>..................] - ETA: 13:30 - loss: 0.6856 - acc: 0.5504
4032/9333 [===========>..................] - ETA: 13:21 - loss: 0.6856 - acc: 0.5508
4096/9333 [============>.................] - ETA: 13:10 - loss: 0.6855 - acc: 0.5510
4160/9333 [============>.................] - ETA: 13:01 - loss: 0.6854 - acc: 0.5502
4224/9333 [============>.................] - ETA: 12:52 - loss: 0.6851 - acc: 0.5504
4288/9333 [============>.................] - ETA: 12:42 - loss: 0.6848 - acc: 0.5508
4352/9333 [============>.................] - ETA: 12:32 - loss: 0.6856 - acc: 0.5499
4416/9333 [=============>................] - ETA: 12:22 - loss: 0.6861 - acc: 0.5491
4480/9333 [=============>................] - ETA: 12:12 - loss: 0.6861 - acc: 0.5487
4544/9333 [=============>................] - ETA: 12:03 - loss: 0.6863 - acc: 0.5469
4608/9333 [=============>................] - ETA: 11:53 - loss: 0.6861 - acc: 0.5471
4672/9333 [==============>...............] - ETA: 11:44 - loss: 0.6864 - acc: 0.5464
4736/9333 [==============>...............] - ETA: 11:34 - loss: 0.6864 - acc: 0.5454
4800/9333 [==============>...............] - ETA: 11:24 - loss: 0.6862 - acc: 0.5463
4864/9333 [==============>...............] - ETA: 11:15 - loss: 0.6863 - acc: 0.5456
4928/9333 [==============>...............] - ETA: 11:06 - loss: 0.6863 - acc: 0.5461
4992/9333 [===============>..............] - ETA: 10:56 - loss: 0.6862 - acc: 0.5463
5056/9333 [===============>..............] - ETA: 10:47 - loss: 0.6865 - acc: 0.5453
5120/9333 [===============>..............] - ETA: 10:37 - loss: 0.6864 - acc: 0.5449
5184/9333 [===============>..............] - ETA: 10:27 - loss: 0.6865 - acc: 0.5455
5248/9333 [===============>..............] - ETA: 10:17 - loss: 0.6864 - acc: 0.5444
5312/9333 [================>.............] - ETA: 10:08 - loss: 0.6865 - acc: 0.5442
5376/9333 [================>.............] - ETA: 9:58 - loss: 0.6867 - acc: 0.5445 
5440/9333 [================>.............] - ETA: 9:49 - loss: 0.6864 - acc: 0.5452
5504/9333 [================>.............] - ETA: 9:39 - loss: 0.6865 - acc: 0.5451
5568/9333 [================>.............] - ETA: 9:30 - loss: 0.6865 - acc: 0.5451
5632/9333 [=================>............] - ETA: 9:20 - loss: 0.6863 - acc: 0.5456
5696/9333 [=================>............] - ETA: 9:10 - loss: 0.6864 - acc: 0.5451
5760/9333 [=================>............] - ETA: 9:00 - loss: 0.6861 - acc: 0.5458
5824/9333 [=================>............] - ETA: 8:50 - loss: 0.6861 - acc: 0.5458
5888/9333 [=================>............] - ETA: 8:40 - loss: 0.6860 - acc: 0.5462
5952/9333 [==================>...........] - ETA: 8:30 - loss: 0.6862 - acc: 0.5457
6016/9333 [==================>...........] - ETA: 8:20 - loss: 0.6862 - acc: 0.5459
6080/9333 [==================>...........] - ETA: 8:11 - loss: 0.6862 - acc: 0.5457
6144/9333 [==================>...........] - ETA: 8:01 - loss: 0.6864 - acc: 0.5451
6208/9333 [==================>...........] - ETA: 7:52 - loss: 0.6864 - acc: 0.5445
6272/9333 [===================>..........] - ETA: 7:42 - loss: 0.6865 - acc: 0.5440
6336/9333 [===================>..........] - ETA: 7:32 - loss: 0.6869 - acc: 0.5428
6400/9333 [===================>..........] - ETA: 7:22 - loss: 0.6868 - acc: 0.5433
6464/9333 [===================>..........] - ETA: 7:13 - loss: 0.6867 - acc: 0.5436
6528/9333 [===================>..........] - ETA: 7:03 - loss: 0.6871 - acc: 0.5423
6592/9333 [====================>.........] - ETA: 6:53 - loss: 0.6870 - acc: 0.5426
6656/9333 [====================>.........] - ETA: 6:43 - loss: 0.6873 - acc: 0.5413
6720/9333 [====================>.........] - ETA: 6:33 - loss: 0.6876 - acc: 0.5411
6784/9333 [====================>.........] - ETA: 6:23 - loss: 0.6875 - acc: 0.5414
6848/9333 [=====================>........] - ETA: 6:14 - loss: 0.6875 - acc: 0.5412
6912/9333 [=====================>........] - ETA: 6:04 - loss: 0.6874 - acc: 0.5411
6976/9333 [=====================>........] - ETA: 5:54 - loss: 0.6874 - acc: 0.5417
7040/9333 [=====================>........] - ETA: 5:44 - loss: 0.6876 - acc: 0.5416
7104/9333 [=====================>........] - ETA: 5:35 - loss: 0.6875 - acc: 0.5415
7168/9333 [======================>.......] - ETA: 5:25 - loss: 0.6876 - acc: 0.5417
7232/9333 [======================>.......] - ETA: 5:15 - loss: 0.6875 - acc: 0.5412
7296/9333 [======================>.......] - ETA: 5:05 - loss: 0.6876 - acc: 0.5403
7360/9333 [======================>.......] - ETA: 4:56 - loss: 0.6878 - acc: 0.5404
7424/9333 [======================>.......] - ETA: 4:46 - loss: 0.6878 - acc: 0.5396
7488/9333 [=======================>......] - ETA: 4:36 - loss: 0.6880 - acc: 0.5390
7552/9333 [=======================>......] - ETA: 4:26 - loss: 0.6880 - acc: 0.5393
7616/9333 [=======================>......] - ETA: 4:17 - loss: 0.6879 - acc: 0.5403
7680/9333 [=======================>......] - ETA: 4:07 - loss: 0.6881 - acc: 0.5397
7744/9333 [=======================>......] - ETA: 3:58 - loss: 0.6882 - acc: 0.5395
7808/9333 [========================>.....] - ETA: 3:48 - loss: 0.6883 - acc: 0.5392
7872/9333 [========================>.....] - ETA: 3:38 - loss: 0.6882 - acc: 0.5391
7936/9333 [========================>.....] - ETA: 3:29 - loss: 0.6882 - acc: 0.5396
8000/9333 [========================>.....] - ETA: 3:19 - loss: 0.6881 - acc: 0.5401
8064/9333 [========================>.....] - ETA: 3:10 - loss: 0.6881 - acc: 0.5403
8128/9333 [=========================>....] - ETA: 3:00 - loss: 0.6880 - acc: 0.5405
8192/9333 [=========================>....] - ETA: 2:51 - loss: 0.6880 - acc: 0.5403
8256/9333 [=========================>....] - ETA: 2:41 - loss: 0.6881 - acc: 0.5407
8320/9333 [=========================>....] - ETA: 2:32 - loss: 0.6880 - acc: 0.5405
8384/9333 [=========================>....] - ETA: 2:22 - loss: 0.6880 - acc: 0.5406
8448/9333 [==========================>...] - ETA: 2:12 - loss: 0.6883 - acc: 0.5397
8512/9333 [==========================>...] - ETA: 2:03 - loss: 0.6883 - acc: 0.5396
8576/9333 [==========================>...] - ETA: 1:53 - loss: 0.6884 - acc: 0.5396
8640/9333 [==========================>...] - ETA: 1:44 - loss: 0.6884 - acc: 0.5396
8704/9333 [==========================>...] - ETA: 1:34 - loss: 0.6883 - acc: 0.5399
8768/9333 [===========================>..] - ETA: 1:24 - loss: 0.6885 - acc: 0.5397
8832/9333 [===========================>..] - ETA: 1:15 - loss: 0.6886 - acc: 0.5396
8896/9333 [===========================>..] - ETA: 1:05 - loss: 0.6885 - acc: 0.5397
8960/9333 [===========================>..] - ETA: 56s - loss: 0.6885 - acc: 0.5396 
9024/9333 [============================>.] - ETA: 46s - loss: 0.6884 - acc: 0.5393
9088/9333 [============================>.] - ETA: 36s - loss: 0.6884 - acc: 0.5395
9152/9333 [============================>.] - ETA: 27s - loss: 0.6883 - acc: 0.5396
9216/9333 [============================>.] - ETA: 17s - loss: 0.6882 - acc: 0.5401
9280/9333 [============================>.] - ETA: 7s - loss: 0.6882 - acc: 0.5400 
9333/9333 [==============================] - 1457s 156ms/step - loss: 0.6882 - acc: 0.5395 - val_loss: 0.6910 - val_acc: 0.5410

Epoch 00008: val_acc improved from 0.53809 to 0.54098, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window20/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 9/10

  64/9333 [..............................] - ETA: 23:56 - loss: 0.7048 - acc: 0.4531
 128/9333 [..............................] - ETA: 23:01 - loss: 0.6925 - acc: 0.5312
 192/9333 [..............................] - ETA: 23:11 - loss: 0.6875 - acc: 0.5573
 256/9333 [..............................] - ETA: 22:52 - loss: 0.6845 - acc: 0.5664
 320/9333 [>.............................] - ETA: 22:51 - loss: 0.6883 - acc: 0.5312
 384/9333 [>.............................] - ETA: 22:46 - loss: 0.6873 - acc: 0.5417
 448/9333 [>.............................] - ETA: 22:42 - loss: 0.6861 - acc: 0.5424
 512/9333 [>.............................] - ETA: 22:27 - loss: 0.6893 - acc: 0.5312
 576/9333 [>.............................] - ETA: 22:17 - loss: 0.6885 - acc: 0.5382
 640/9333 [=>............................] - ETA: 22:03 - loss: 0.6864 - acc: 0.5453
 704/9333 [=>............................] - ETA: 21:53 - loss: 0.6856 - acc: 0.5582
 768/9333 [=>............................] - ETA: 21:46 - loss: 0.6855 - acc: 0.5573
 832/9333 [=>............................] - ETA: 21:34 - loss: 0.6860 - acc: 0.5517
 896/9333 [=>............................] - ETA: 21:23 - loss: 0.6855 - acc: 0.5525
 960/9333 [==>...........................] - ETA: 21:16 - loss: 0.6836 - acc: 0.5573
1024/9333 [==>...........................] - ETA: 21:04 - loss: 0.6830 - acc: 0.5576
1088/9333 [==>...........................] - ETA: 20:52 - loss: 0.6829 - acc: 0.5579
1152/9333 [==>...........................] - ETA: 20:46 - loss: 0.6841 - acc: 0.5556
1216/9333 [==>...........................] - ETA: 20:38 - loss: 0.6848 - acc: 0.5526
1280/9333 [===>..........................] - ETA: 20:28 - loss: 0.6853 - acc: 0.5484
1344/9333 [===>..........................] - ETA: 20:14 - loss: 0.6861 - acc: 0.5439
1408/9333 [===>..........................] - ETA: 20:04 - loss: 0.6865 - acc: 0.5412
1472/9333 [===>..........................] - ETA: 19:54 - loss: 0.6853 - acc: 0.5442
1536/9333 [===>..........................] - ETA: 19:44 - loss: 0.6857 - acc: 0.5443
1600/9333 [====>.........................] - ETA: 19:35 - loss: 0.6860 - acc: 0.5419
1664/9333 [====>.........................] - ETA: 19:28 - loss: 0.6860 - acc: 0.5439
1728/9333 [====>.........................] - ETA: 19:18 - loss: 0.6865 - acc: 0.5411
1792/9333 [====>.........................] - ETA: 19:07 - loss: 0.6865 - acc: 0.5424
1856/9333 [====>.........................] - ETA: 19:00 - loss: 0.6873 - acc: 0.5399
1920/9333 [=====>........................] - ETA: 18:51 - loss: 0.6865 - acc: 0.5437
1984/9333 [=====>........................] - ETA: 18:39 - loss: 0.6865 - acc: 0.5439
2048/9333 [=====>........................] - ETA: 18:30 - loss: 0.6862 - acc: 0.5449
2112/9333 [=====>........................] - ETA: 18:21 - loss: 0.6861 - acc: 0.5450
2176/9333 [=====>........................] - ETA: 18:11 - loss: 0.6854 - acc: 0.5478
2240/9333 [======>.......................] - ETA: 18:03 - loss: 0.6851 - acc: 0.5513
2304/9333 [======>.......................] - ETA: 17:54 - loss: 0.6855 - acc: 0.5503
2368/9333 [======>.......................] - ETA: 17:44 - loss: 0.6861 - acc: 0.5481
2432/9333 [======>.......................] - ETA: 17:34 - loss: 0.6854 - acc: 0.5506
2496/9333 [=======>......................] - ETA: 17:25 - loss: 0.6854 - acc: 0.5525
2560/9333 [=======>......................] - ETA: 17:15 - loss: 0.6854 - acc: 0.5523
2624/9333 [=======>......................] - ETA: 17:04 - loss: 0.6851 - acc: 0.5534
2688/9333 [=======>......................] - ETA: 16:53 - loss: 0.6851 - acc: 0.5532
2752/9333 [=======>......................] - ETA: 16:43 - loss: 0.6855 - acc: 0.5520
2816/9333 [========>.....................] - ETA: 16:33 - loss: 0.6857 - acc: 0.5504
2880/9333 [========>.....................] - ETA: 16:24 - loss: 0.6851 - acc: 0.5521
2944/9333 [========>.....................] - ETA: 16:13 - loss: 0.6857 - acc: 0.5506
3008/9333 [========>.....................] - ETA: 16:02 - loss: 0.6853 - acc: 0.5509
3072/9333 [========>.....................] - ETA: 15:52 - loss: 0.6859 - acc: 0.5495
3136/9333 [=========>....................] - ETA: 15:41 - loss: 0.6855 - acc: 0.5504
3200/9333 [=========>....................] - ETA: 15:31 - loss: 0.6856 - acc: 0.5494
3264/9333 [=========>....................] - ETA: 15:20 - loss: 0.6857 - acc: 0.5499
3328/9333 [=========>....................] - ETA: 15:10 - loss: 0.6854 - acc: 0.5520
3392/9333 [=========>....................] - ETA: 14:59 - loss: 0.6857 - acc: 0.5507
3456/9333 [==========>...................] - ETA: 14:49 - loss: 0.6863 - acc: 0.5489
3520/9333 [==========>...................] - ETA: 14:38 - loss: 0.6859 - acc: 0.5506
3584/9333 [==========>...................] - ETA: 14:28 - loss: 0.6861 - acc: 0.5497
3648/9333 [==========>...................] - ETA: 14:18 - loss: 0.6862 - acc: 0.5491
3712/9333 [==========>...................] - ETA: 14:07 - loss: 0.6860 - acc: 0.5493
3776/9333 [===========>..................] - ETA: 13:58 - loss: 0.6859 - acc: 0.5501
3840/9333 [===========>..................] - ETA: 13:48 - loss: 0.6857 - acc: 0.5508
3904/9333 [===========>..................] - ETA: 13:38 - loss: 0.6858 - acc: 0.5499
3968/9333 [===========>..................] - ETA: 13:28 - loss: 0.6856 - acc: 0.5514
4032/9333 [===========>..................] - ETA: 13:18 - loss: 0.6855 - acc: 0.5523
4096/9333 [============>.................] - ETA: 13:08 - loss: 0.6854 - acc: 0.5540
4160/9333 [============>.................] - ETA: 12:59 - loss: 0.6851 - acc: 0.5550
4224/9333 [============>.................] - ETA: 12:48 - loss: 0.6850 - acc: 0.5559
4288/9333 [============>.................] - ETA: 12:38 - loss: 0.6854 - acc: 0.5546
4352/9333 [============>.................] - ETA: 12:28 - loss: 0.6853 - acc: 0.5547
4416/9333 [=============>................] - ETA: 12:18 - loss: 0.6854 - acc: 0.5548
4480/9333 [=============>................] - ETA: 12:08 - loss: 0.6855 - acc: 0.5556
4544/9333 [=============>................] - ETA: 11:58 - loss: 0.6856 - acc: 0.5559
4608/9333 [=============>................] - ETA: 11:48 - loss: 0.6858 - acc: 0.5547
4672/9333 [==============>...............] - ETA: 11:38 - loss: 0.6857 - acc: 0.5552
4736/9333 [==============>...............] - ETA: 11:28 - loss: 0.6857 - acc: 0.5553
4800/9333 [==============>...............] - ETA: 11:18 - loss: 0.6861 - acc: 0.5544
4864/9333 [==============>...............] - ETA: 11:08 - loss: 0.6861 - acc: 0.5535
4928/9333 [==============>...............] - ETA: 10:58 - loss: 0.6859 - acc: 0.5544
4992/9333 [===============>..............] - ETA: 10:48 - loss: 0.6860 - acc: 0.5537
5056/9333 [===============>..............] - ETA: 10:39 - loss: 0.6858 - acc: 0.5534
5120/9333 [===============>..............] - ETA: 10:29 - loss: 0.6858 - acc: 0.5533
5184/9333 [===============>..............] - ETA: 10:19 - loss: 0.6858 - acc: 0.5529
5248/9333 [===============>..............] - ETA: 10:09 - loss: 0.6860 - acc: 0.5524
5312/9333 [================>.............] - ETA: 9:58 - loss: 0.6859 - acc: 0.5523 
5376/9333 [================>.............] - ETA: 9:48 - loss: 0.6862 - acc: 0.5517
5440/9333 [================>.............] - ETA: 9:38 - loss: 0.6858 - acc: 0.5528
5504/9333 [================>.............] - ETA: 9:29 - loss: 0.6856 - acc: 0.5532
5568/9333 [================>.............] - ETA: 9:19 - loss: 0.6858 - acc: 0.5532
5632/9333 [=================>............] - ETA: 9:09 - loss: 0.6859 - acc: 0.5518
5696/9333 [=================>............] - ETA: 8:59 - loss: 0.6861 - acc: 0.5513
5760/9333 [=================>............] - ETA: 8:49 - loss: 0.6861 - acc: 0.5516
5824/9333 [=================>............] - ETA: 8:39 - loss: 0.6862 - acc: 0.5517
5888/9333 [=================>............] - ETA: 8:29 - loss: 0.6860 - acc: 0.5521
5952/9333 [==================>...........] - ETA: 8:20 - loss: 0.6863 - acc: 0.5509
6016/9333 [==================>...........] - ETA: 8:10 - loss: 0.6864 - acc: 0.5509
6080/9333 [==================>...........] - ETA: 8:00 - loss: 0.6864 - acc: 0.5507
6144/9333 [==================>...........] - ETA: 7:50 - loss: 0.6866 - acc: 0.5498
6208/9333 [==================>...........] - ETA: 7:41 - loss: 0.6867 - acc: 0.5483
6272/9333 [===================>..........] - ETA: 7:31 - loss: 0.6872 - acc: 0.5470
6336/9333 [===================>..........] - ETA: 7:22 - loss: 0.6875 - acc: 0.5455
6400/9333 [===================>..........] - ETA: 7:12 - loss: 0.6875 - acc: 0.5459
6464/9333 [===================>..........] - ETA: 7:02 - loss: 0.6874 - acc: 0.5463
6528/9333 [===================>..........] - ETA: 6:53 - loss: 0.6872 - acc: 0.5467
6592/9333 [====================>.........] - ETA: 6:43 - loss: 0.6871 - acc: 0.5467
6656/9333 [====================>.........] - ETA: 6:33 - loss: 0.6869 - acc: 0.5472
6720/9333 [====================>.........] - ETA: 6:24 - loss: 0.6872 - acc: 0.5458
6784/9333 [====================>.........] - ETA: 6:14 - loss: 0.6872 - acc: 0.5460
6848/9333 [=====================>........] - ETA: 6:05 - loss: 0.6873 - acc: 0.5466
6912/9333 [=====================>........] - ETA: 5:55 - loss: 0.6874 - acc: 0.5463
6976/9333 [=====================>........] - ETA: 5:46 - loss: 0.6876 - acc: 0.5452
7040/9333 [=====================>........] - ETA: 5:36 - loss: 0.6877 - acc: 0.5439
7104/9333 [=====================>........] - ETA: 5:27 - loss: 0.6876 - acc: 0.5443
7168/9333 [======================>.......] - ETA: 5:17 - loss: 0.6875 - acc: 0.5449
7232/9333 [======================>.......] - ETA: 5:08 - loss: 0.6875 - acc: 0.5452
7296/9333 [======================>.......] - ETA: 4:58 - loss: 0.6874 - acc: 0.5459
7360/9333 [======================>.......] - ETA: 4:48 - loss: 0.6875 - acc: 0.5459
7424/9333 [======================>.......] - ETA: 4:39 - loss: 0.6877 - acc: 0.5457
7488/9333 [=======================>......] - ETA: 4:29 - loss: 0.6878 - acc: 0.5454
7552/9333 [=======================>......] - ETA: 4:20 - loss: 0.6877 - acc: 0.5461
7616/9333 [=======================>......] - ETA: 4:11 - loss: 0.6878 - acc: 0.5460
7680/9333 [=======================>......] - ETA: 4:01 - loss: 0.6882 - acc: 0.5447
7744/9333 [=======================>......] - ETA: 3:52 - loss: 0.6881 - acc: 0.5446
7808/9333 [========================>.....] - ETA: 3:42 - loss: 0.6881 - acc: 0.5446
7872/9333 [========================>.....] - ETA: 3:33 - loss: 0.6879 - acc: 0.5451
7936/9333 [========================>.....] - ETA: 3:24 - loss: 0.6879 - acc: 0.5454
8000/9333 [========================>.....] - ETA: 3:14 - loss: 0.6878 - acc: 0.5456
8064/9333 [========================>.....] - ETA: 3:05 - loss: 0.6877 - acc: 0.5463
8128/9333 [=========================>....] - ETA: 2:55 - loss: 0.6875 - acc: 0.5472
8192/9333 [=========================>....] - ETA: 2:46 - loss: 0.6873 - acc: 0.5482
8256/9333 [=========================>....] - ETA: 2:37 - loss: 0.6873 - acc: 0.5482
8320/9333 [=========================>....] - ETA: 2:27 - loss: 0.6875 - acc: 0.5472
8384/9333 [=========================>....] - ETA: 2:18 - loss: 0.6873 - acc: 0.5482
8448/9333 [==========================>...] - ETA: 2:09 - loss: 0.6873 - acc: 0.5482
8512/9333 [==========================>...] - ETA: 1:59 - loss: 0.6875 - acc: 0.5479
8576/9333 [==========================>...] - ETA: 1:50 - loss: 0.6874 - acc: 0.5487
8640/9333 [==========================>...] - ETA: 1:41 - loss: 0.6873 - acc: 0.5488
8704/9333 [==========================>...] - ETA: 1:31 - loss: 0.6873 - acc: 0.5485
8768/9333 [===========================>..] - ETA: 1:22 - loss: 0.6875 - acc: 0.5479
8832/9333 [===========================>..] - ETA: 1:13 - loss: 0.6875 - acc: 0.5477
8896/9333 [===========================>..] - ETA: 1:03 - loss: 0.6875 - acc: 0.5478
8960/9333 [===========================>..] - ETA: 54s - loss: 0.6875 - acc: 0.5478 
9024/9333 [============================>.] - ETA: 45s - loss: 0.6874 - acc: 0.5482
9088/9333 [============================>.] - ETA: 35s - loss: 0.6877 - acc: 0.5470
9152/9333 [============================>.] - ETA: 26s - loss: 0.6878 - acc: 0.5469
9216/9333 [============================>.] - ETA: 17s - loss: 0.6878 - acc: 0.5467
9280/9333 [============================>.] - ETA: 7s - loss: 0.6878 - acc: 0.5471 
9333/9333 [==============================] - 1408s 151ms/step - loss: 0.6878 - acc: 0.5470 - val_loss: 0.6912 - val_acc: 0.5198

Epoch 00009: val_acc did not improve from 0.54098
Epoch 10/10

  64/9333 [..............................] - ETA: 24:02 - loss: 0.6899 - acc: 0.5312
 128/9333 [..............................] - ETA: 23:08 - loss: 0.6769 - acc: 0.5547
 192/9333 [..............................] - ETA: 22:34 - loss: 0.6844 - acc: 0.5312
 256/9333 [..............................] - ETA: 22:10 - loss: 0.6811 - acc: 0.5508
 320/9333 [>.............................] - ETA: 21:59 - loss: 0.6857 - acc: 0.5406
 384/9333 [>.............................] - ETA: 21:48 - loss: 0.6885 - acc: 0.5339
 448/9333 [>.............................] - ETA: 21:42 - loss: 0.6912 - acc: 0.5179
 512/9333 [>.............................] - ETA: 21:28 - loss: 0.6929 - acc: 0.5098
 576/9333 [>.............................] - ETA: 21:15 - loss: 0.6923 - acc: 0.5104
 640/9333 [=>............................] - ETA: 20:59 - loss: 0.6926 - acc: 0.5172
 704/9333 [=>............................] - ETA: 20:47 - loss: 0.6907 - acc: 0.5270
 768/9333 [=>............................] - ETA: 20:34 - loss: 0.6885 - acc: 0.5365
 832/9333 [=>............................] - ETA: 20:27 - loss: 0.6886 - acc: 0.5385
 896/9333 [=>............................] - ETA: 20:17 - loss: 0.6881 - acc: 0.5424
 960/9333 [==>...........................] - ETA: 20:06 - loss: 0.6896 - acc: 0.5344
1024/9333 [==>...........................] - ETA: 19:56 - loss: 0.6886 - acc: 0.5371
1088/9333 [==>...........................] - ETA: 19:48 - loss: 0.6882 - acc: 0.5377
1152/9333 [==>...........................] - ETA: 19:34 - loss: 0.6872 - acc: 0.5443
1216/9333 [==>...........................] - ETA: 19:27 - loss: 0.6880 - acc: 0.5419
1280/9333 [===>..........................] - ETA: 19:17 - loss: 0.6885 - acc: 0.5422
1344/9333 [===>..........................] - ETA: 19:05 - loss: 0.6886 - acc: 0.5402
1408/9333 [===>..........................] - ETA: 18:56 - loss: 0.6878 - acc: 0.5433
1472/9333 [===>..........................] - ETA: 18:52 - loss: 0.6879 - acc: 0.5462
1536/9333 [===>..........................] - ETA: 18:41 - loss: 0.6879 - acc: 0.5449
1600/9333 [====>.........................] - ETA: 18:32 - loss: 0.6879 - acc: 0.5431
1664/9333 [====>.........................] - ETA: 18:26 - loss: 0.6878 - acc: 0.5433
1728/9333 [====>.........................] - ETA: 18:16 - loss: 0.6880 - acc: 0.5428
1792/9333 [====>.........................] - ETA: 18:09 - loss: 0.6877 - acc: 0.5430
1856/9333 [====>.........................] - ETA: 17:57 - loss: 0.6871 - acc: 0.5447
1920/9333 [=====>........................] - ETA: 17:46 - loss: 0.6867 - acc: 0.5448
1984/9333 [=====>........................] - ETA: 17:36 - loss: 0.6858 - acc: 0.5489
2048/9333 [=====>........................] - ETA: 17:27 - loss: 0.6868 - acc: 0.5454
2112/9333 [=====>........................] - ETA: 17:17 - loss: 0.6869 - acc: 0.5459
2176/9333 [=====>........................] - ETA: 17:09 - loss: 0.6870 - acc: 0.5441
2240/9333 [======>.......................] - ETA: 17:00 - loss: 0.6859 - acc: 0.5487
2304/9333 [======>.......................] - ETA: 16:49 - loss: 0.6857 - acc: 0.5508
2368/9333 [======>.......................] - ETA: 16:40 - loss: 0.6858 - acc: 0.5515
2432/9333 [======>.......................] - ETA: 16:30 - loss: 0.6862 - acc: 0.5485
2496/9333 [=======>......................] - ETA: 16:20 - loss: 0.6858 - acc: 0.5493
2560/9333 [=======>......................] - ETA: 16:11 - loss: 0.6863 - acc: 0.5488
2624/9333 [=======>......................] - ETA: 16:01 - loss: 0.6859 - acc: 0.5495
2688/9333 [=======>......................] - ETA: 15:52 - loss: 0.6861 - acc: 0.5495
2752/9333 [=======>......................] - ETA: 15:43 - loss: 0.6864 - acc: 0.5487
2816/9333 [========>.....................] - ETA: 15:34 - loss: 0.6867 - acc: 0.5476
2880/9333 [========>.....................] - ETA: 15:25 - loss: 0.6872 - acc: 0.5455
2944/9333 [========>.....................] - ETA: 15:14 - loss: 0.6869 - acc: 0.5465
3008/9333 [========>.....................] - ETA: 15:06 - loss: 0.6863 - acc: 0.5465
3072/9333 [========>.....................] - ETA: 14:57 - loss: 0.6865 - acc: 0.5459
3136/9333 [=========>....................] - ETA: 14:47 - loss: 0.6868 - acc: 0.5453
3200/9333 [=========>....................] - ETA: 14:37 - loss: 0.6866 - acc: 0.5453
3264/9333 [=========>....................] - ETA: 14:28 - loss: 0.6864 - acc: 0.5450
3328/9333 [=========>....................] - ETA: 14:18 - loss: 0.6864 - acc: 0.5463
3392/9333 [=========>....................] - ETA: 14:08 - loss: 0.6871 - acc: 0.5436
3456/9333 [==========>...................] - ETA: 13:59 - loss: 0.6872 - acc: 0.5431
3520/9333 [==========>...................] - ETA: 13:49 - loss: 0.6874 - acc: 0.5420
3584/9333 [==========>...................] - ETA: 13:41 - loss: 0.6870 - acc: 0.5430
3648/9333 [==========>...................] - ETA: 13:31 - loss: 0.6868 - acc: 0.5436
3712/9333 [==========>...................] - ETA: 13:22 - loss: 0.6872 - acc: 0.5415
3776/9333 [===========>..................] - ETA: 13:12 - loss: 0.6869 - acc: 0.5437
3840/9333 [===========>..................] - ETA: 13:03 - loss: 0.6869 - acc: 0.5440
3904/9333 [===========>..................] - ETA: 12:54 - loss: 0.6872 - acc: 0.5423
3968/9333 [===========>..................] - ETA: 12:44 - loss: 0.6873 - acc: 0.5421
4032/9333 [===========>..................] - ETA: 12:35 - loss: 0.6874 - acc: 0.5417
4096/9333 [============>.................] - ETA: 12:25 - loss: 0.6874 - acc: 0.5415
4160/9333 [============>.................] - ETA: 12:16 - loss: 0.6874 - acc: 0.5411
4224/9333 [============>.................] - ETA: 12:07 - loss: 0.6875 - acc: 0.5410
4288/9333 [============>.................] - ETA: 11:58 - loss: 0.6867 - acc: 0.5441
4352/9333 [============>.................] - ETA: 11:48 - loss: 0.6869 - acc: 0.5434
4416/9333 [=============>................] - ETA: 11:39 - loss: 0.6873 - acc: 0.5428
4480/9333 [=============>................] - ETA: 11:29 - loss: 0.6872 - acc: 0.5442
4544/9333 [=============>................] - ETA: 11:20 - loss: 0.6873 - acc: 0.5445
4608/9333 [=============>................] - ETA: 11:10 - loss: 0.6873 - acc: 0.5449
4672/9333 [==============>...............] - ETA: 11:01 - loss: 0.6875 - acc: 0.5439
4736/9333 [==============>...............] - ETA: 10:52 - loss: 0.6875 - acc: 0.5439
4800/9333 [==============>...............] - ETA: 10:42 - loss: 0.6878 - acc: 0.5419
4864/9333 [==============>...............] - ETA: 10:33 - loss: 0.6878 - acc: 0.5413
4928/9333 [==============>...............] - ETA: 10:24 - loss: 0.6878 - acc: 0.5412
4992/9333 [===============>..............] - ETA: 10:15 - loss: 0.6877 - acc: 0.5417
5056/9333 [===============>..............] - ETA: 10:05 - loss: 0.6874 - acc: 0.5433
5120/9333 [===============>..............] - ETA: 9:56 - loss: 0.6876 - acc: 0.5432 
5184/9333 [===============>..............] - ETA: 9:47 - loss: 0.6878 - acc: 0.5426
5248/9333 [===============>..............] - ETA: 9:38 - loss: 0.6880 - acc: 0.5423
5312/9333 [================>.............] - ETA: 9:29 - loss: 0.6879 - acc: 0.5433
5376/9333 [================>.............] - ETA: 9:20 - loss: 0.6876 - acc: 0.5448
5440/9333 [================>.............] - ETA: 9:11 - loss: 0.6877 - acc: 0.5443
5504/9333 [================>.............] - ETA: 9:02 - loss: 0.6875 - acc: 0.5452
5568/9333 [================>.............] - ETA: 8:53 - loss: 0.6876 - acc: 0.5447
5632/9333 [=================>............] - ETA: 8:44 - loss: 0.6876 - acc: 0.5446
5696/9333 [=================>............] - ETA: 8:34 - loss: 0.6872 - acc: 0.5456
5760/9333 [=================>............] - ETA: 8:25 - loss: 0.6871 - acc: 0.5460
5824/9333 [=================>............] - ETA: 8:16 - loss: 0.6871 - acc: 0.5457
5888/9333 [=================>............] - ETA: 8:07 - loss: 0.6872 - acc: 0.5455
5952/9333 [==================>...........] - ETA: 7:57 - loss: 0.6871 - acc: 0.5459
6016/9333 [==================>...........] - ETA: 7:48 - loss: 0.6866 - acc: 0.5474
6080/9333 [==================>...........] - ETA: 7:39 - loss: 0.6866 - acc: 0.5472
6144/9333 [==================>...........] - ETA: 7:29 - loss: 0.6867 - acc: 0.5475
6208/9333 [==================>...........] - ETA: 7:20 - loss: 0.6867 - acc: 0.5475
6272/9333 [===================>..........] - ETA: 7:11 - loss: 0.6868 - acc: 0.5472
6336/9333 [===================>..........] - ETA: 7:02 - loss: 0.6867 - acc: 0.5473
6400/9333 [===================>..........] - ETA: 6:52 - loss: 0.6867 - acc: 0.5480
6464/9333 [===================>..........] - ETA: 6:43 - loss: 0.6865 - acc: 0.5481
6528/9333 [===================>..........] - ETA: 6:34 - loss: 0.6864 - acc: 0.5481
6592/9333 [====================>.........] - ETA: 6:25 - loss: 0.6867 - acc: 0.5467
6656/9333 [====================>.........] - ETA: 6:16 - loss: 0.6867 - acc: 0.5469
6720/9333 [====================>.........] - ETA: 6:07 - loss: 0.6864 - acc: 0.5472
6784/9333 [====================>.........] - ETA: 5:57 - loss: 0.6865 - acc: 0.5467
6848/9333 [=====================>........] - ETA: 5:48 - loss: 0.6861 - acc: 0.5479
6912/9333 [=====================>........] - ETA: 5:40 - loss: 0.6860 - acc: 0.5483
6976/9333 [=====================>........] - ETA: 5:31 - loss: 0.6862 - acc: 0.5480
7040/9333 [=====================>........] - ETA: 5:22 - loss: 0.6862 - acc: 0.5477
7104/9333 [=====================>........] - ETA: 5:13 - loss: 0.6863 - acc: 0.5474
7168/9333 [======================>.......] - ETA: 5:04 - loss: 0.6864 - acc: 0.5472
7232/9333 [======================>.......] - ETA: 4:55 - loss: 0.6864 - acc: 0.5474
7296/9333 [======================>.......] - ETA: 4:46 - loss: 0.6863 - acc: 0.5481
7360/9333 [======================>.......] - ETA: 4:37 - loss: 0.6861 - acc: 0.5482
7424/9333 [======================>.......] - ETA: 4:28 - loss: 0.6860 - acc: 0.5480
7488/9333 [=======================>......] - ETA: 4:18 - loss: 0.6861 - acc: 0.5481
7552/9333 [=======================>......] - ETA: 4:09 - loss: 0.6859 - acc: 0.5494
7616/9333 [=======================>......] - ETA: 4:00 - loss: 0.6863 - acc: 0.5482
7680/9333 [=======================>......] - ETA: 3:51 - loss: 0.6863 - acc: 0.5482
7744/9333 [=======================>......] - ETA: 3:42 - loss: 0.6863 - acc: 0.5479
7808/9333 [========================>.....] - ETA: 3:33 - loss: 0.6863 - acc: 0.5479
7872/9333 [========================>.....] - ETA: 3:24 - loss: 0.6864 - acc: 0.5471
7936/9333 [========================>.....] - ETA: 3:15 - loss: 0.6863 - acc: 0.5473
8000/9333 [========================>.....] - ETA: 3:06 - loss: 0.6863 - acc: 0.5474
8064/9333 [========================>.....] - ETA: 2:57 - loss: 0.6861 - acc: 0.5477
8128/9333 [=========================>....] - ETA: 2:48 - loss: 0.6862 - acc: 0.5475
8192/9333 [=========================>....] - ETA: 2:39 - loss: 0.6864 - acc: 0.5470
8256/9333 [=========================>....] - ETA: 2:30 - loss: 0.6862 - acc: 0.5472
8320/9333 [=========================>....] - ETA: 2:21 - loss: 0.6861 - acc: 0.5474
8384/9333 [=========================>....] - ETA: 2:13 - loss: 0.6862 - acc: 0.5470
8448/9333 [==========================>...] - ETA: 2:04 - loss: 0.6861 - acc: 0.5472
8512/9333 [==========================>...] - ETA: 1:55 - loss: 0.6861 - acc: 0.5468
8576/9333 [==========================>...] - ETA: 1:46 - loss: 0.6860 - acc: 0.5465
8640/9333 [==========================>...] - ETA: 1:37 - loss: 0.6862 - acc: 0.5458
8704/9333 [==========================>...] - ETA: 1:28 - loss: 0.6862 - acc: 0.5461
8768/9333 [===========================>..] - ETA: 1:19 - loss: 0.6862 - acc: 0.5462
8832/9333 [===========================>..] - ETA: 1:10 - loss: 0.6862 - acc: 0.5464
8896/9333 [===========================>..] - ETA: 1:01 - loss: 0.6864 - acc: 0.5462
8960/9333 [===========================>..] - ETA: 52s - loss: 0.6866 - acc: 0.5458 
9024/9333 [============================>.] - ETA: 43s - loss: 0.6866 - acc: 0.5460
9088/9333 [============================>.] - ETA: 34s - loss: 0.6867 - acc: 0.5459
9152/9333 [============================>.] - ETA: 25s - loss: 0.6866 - acc: 0.5458
9216/9333 [============================>.] - ETA: 16s - loss: 0.6865 - acc: 0.5461
9280/9333 [============================>.] - ETA: 7s - loss: 0.6864 - acc: 0.5467 
9333/9333 [==============================] - 1353s 145ms/step - loss: 0.6863 - acc: 0.5472 - val_loss: 0.6924 - val_acc: 0.5178

Epoch 00010: val_acc did not improve from 0.54098
样本个数 1296
样本个数 2592
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f817ea80050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f817ea80050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7ca0337690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7ca0337690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c6073a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c6073a390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81c37c8490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81c37c8490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f817623d890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f817623d890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a1816c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81a1816c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c60117f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c60117f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a485b2ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a485b2ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8175f4e490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8175f4e490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81762b0b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81762b0b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a7449d810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a7449d810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8175f4e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8175f4e1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a744a92d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a744a92d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8175c2c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8175c2c390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8175d87450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8175d87450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8175e39c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8175e39c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8175c2ccd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8175c2ccd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c78445cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c78445cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f815dacd090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f815dacd090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f815da1f650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f815da1f650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815db48a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815db48a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f815dacd610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f815dacd610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815da47190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815da47190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f815db5e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f815db5e8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f815d7f0d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f815d7f0d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815d8076d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815d8076d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f815d899b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f815d899b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815d590510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815d590510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f815d582f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f815d582f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f815d47fcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f815d47fcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815d5f87d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815d5f87d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f815d487290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f815d487290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81552b4090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81552b4090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f815d3b2510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f815d3b2510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f815517b710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f815517b710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154f51410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154f51410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81551d01d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81551d01d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154f40e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154f40e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8155122650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8155122650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8154d82b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8154d82b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81550a0090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81550a0090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8154fc0290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8154fc0290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154e3bd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154e3bd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8154c78b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8154c78b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8154b6ee50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8154b6ee50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154b90150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154b90150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8154b5d250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8154b5d250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81549795d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81549795d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8154af2bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8154af2bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f815479d790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f815479d790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154b6ee50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154b6ee50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8154af2490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8154af2490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154d4a650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154d4a650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8154787e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8154787e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8154560ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8154560ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815450d850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815450d850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8154634290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8154634290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815450d190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f815450d190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81544c4490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81544c4490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8154156350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8154156350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81541d3510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81541d3510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8154434b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8154434b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154156c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8154156c90>>: AttributeError: module 'gast' has no attribute 'Str'
window_select17-21.py:152: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/2592 [..............................] - ETA: 6:28
 128/2592 [>.............................] - ETA: 4:08
 192/2592 [=>............................] - ETA: 3:16
 256/2592 [=>............................] - ETA: 2:49
 320/2592 [==>...........................] - ETA: 2:32
 384/2592 [===>..........................] - ETA: 2:20
 448/2592 [====>.........................] - ETA: 2:12
 512/2592 [====>.........................] - ETA: 2:04
 576/2592 [=====>........................] - ETA: 1:57
 640/2592 [======>.......................] - ETA: 1:50
 704/2592 [=======>......................] - ETA: 1:44
 768/2592 [=======>......................] - ETA: 1:39
 832/2592 [========>.....................] - ETA: 1:34
 896/2592 [=========>....................] - ETA: 1:29
 960/2592 [==========>...................] - ETA: 1:25
1024/2592 [==========>...................] - ETA: 1:21
1088/2592 [===========>..................] - ETA: 1:17
1152/2592 [============>.................] - ETA: 1:13
1216/2592 [=============>................] - ETA: 1:09
1280/2592 [=============>................] - ETA: 1:06
1344/2592 [==============>...............] - ETA: 1:02
1408/2592 [===============>..............] - ETA: 59s 
1472/2592 [================>.............] - ETA: 55s
1536/2592 [================>.............] - ETA: 52s
1600/2592 [=================>............] - ETA: 49s
1664/2592 [==================>...........] - ETA: 45s
1728/2592 [===================>..........] - ETA: 42s
1792/2592 [===================>..........] - ETA: 39s
1856/2592 [====================>.........] - ETA: 35s
1920/2592 [=====================>........] - ETA: 32s
1984/2592 [=====================>........] - ETA: 29s
2048/2592 [======================>.......] - ETA: 26s
2112/2592 [=======================>......] - ETA: 22s
2176/2592 [========================>.....] - ETA: 19s
2240/2592 [========================>.....] - ETA: 16s
2304/2592 [=========================>....] - ETA: 13s
2368/2592 [==========================>...] - ETA: 10s
2432/2592 [===========================>..] - ETA: 7s 
2496/2592 [===========================>..] - ETA: 4s
2560/2592 [============================>.] - ETA: 1s
2592/2592 [==============================] - 122s 47ms/step
loss: 0.6853369842340917
acc: 0.5590277777777778
样本个数 5185
样本个数 10370
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7a4856dcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7a4856dcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7a48535e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7a48535e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cc1f04d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81cc1f04d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8176316510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8176316510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f817e5b9a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f817e5b9a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817e4b65d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817e4b65d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f812a70f190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f812a70f190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f798c741fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f798c741fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8176364c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8176364c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f817e80b2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f817e80b2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817e7ff510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817e7ff510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8176364450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8176364450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817e98b250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817e98b250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f817e7d3d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f817e7d3d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f817e69e5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f817e69e5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817e501110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817e501110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f817e8b4a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f817e8b4a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79bc1d7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79bc1d7290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a48307e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a48307e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f817e42f650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f817e42f650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f812a3e8b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f812a3e8b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f79bc1f8bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f79bc1f8bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a4831e090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a4831e090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a107bedd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a107bedd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7a1077c9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7a1077c9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a1067c410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a1067c410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a107be2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a107be2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a106b2f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a106b2f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a104ae290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a104ae290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7a10352190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7a10352190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a10481f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a10481f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a4820aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a4820aa90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a10389a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a10389a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a101b69d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a101b69d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7a1038dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7a1038dfd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a10167810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a10167810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a104b6c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a104b6c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a10073210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a10073210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a0864d790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a0864d790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7a084d7dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7a084d7dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a08601090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a08601090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a0864dd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a0864dd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a08512890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a08512890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a086366d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a086366d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7a081c2210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7a081c2210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a080ecd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a080ecd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a103a8490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a103a8490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a080ecf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7a080ecf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a080a5dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7a080a5dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f79e87814d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f79e87814d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e87b5b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e87b5b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a080e8290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7a080e8290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e87185d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e87185d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f79e8781dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f79e8781dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f79e8450450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f79e8450450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e87355d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e87355d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f79e8550150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f79e8550150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e8340690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e8340690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f79e83d1610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f79e83d1610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f79e813e950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f79e813e950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e818d3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e818d3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f79e83d1350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f79e83d1350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e805c790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f79e805c790>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 9333 samples, validate on 1037 samples
Epoch 1/10

  64/9333 [..............................] - ETA: 1:45:37 - loss: 0.7043 - acc: 0.5312
 128/9333 [..............................] - ETA: 1:04:49 - loss: 0.7353 - acc: 0.5078
 192/9333 [..............................] - ETA: 50:34 - loss: 0.7377 - acc: 0.4896  
 256/9333 [..............................] - ETA: 43:20 - loss: 0.7563 - acc: 0.4805
 320/9333 [>.............................] - ETA: 39:00 - loss: 0.7579 - acc: 0.4938
 384/9333 [>.............................] - ETA: 36:22 - loss: 0.7492 - acc: 0.4922
 448/9333 [>.............................] - ETA: 34:08 - loss: 0.7537 - acc: 0.4978
 512/9333 [>.............................] - ETA: 32:31 - loss: 0.7521 - acc: 0.4980
 576/9333 [>.............................] - ETA: 31:09 - loss: 0.7534 - acc: 0.5052
 640/9333 [=>............................] - ETA: 30:02 - loss: 0.7477 - acc: 0.5031
 704/9333 [=>............................] - ETA: 28:59 - loss: 0.7441 - acc: 0.5071
 768/9333 [=>............................] - ETA: 28:11 - loss: 0.7422 - acc: 0.5104
 832/9333 [=>............................] - ETA: 27:22 - loss: 0.7469 - acc: 0.5036
 896/9333 [=>............................] - ETA: 26:51 - loss: 0.7524 - acc: 0.4967
 960/9333 [==>...........................] - ETA: 26:15 - loss: 0.7525 - acc: 0.4948
1024/9333 [==>...........................] - ETA: 25:43 - loss: 0.7513 - acc: 0.4971
1088/9333 [==>...........................] - ETA: 25:12 - loss: 0.7543 - acc: 0.4926
1152/9333 [==>...........................] - ETA: 24:48 - loss: 0.7471 - acc: 0.4991
1216/9333 [==>...........................] - ETA: 24:25 - loss: 0.7480 - acc: 0.4984
1280/9333 [===>..........................] - ETA: 24:03 - loss: 0.7449 - acc: 0.5016
1344/9333 [===>..........................] - ETA: 23:44 - loss: 0.7433 - acc: 0.5030
1408/9333 [===>..........................] - ETA: 23:26 - loss: 0.7431 - acc: 0.5007
1472/9333 [===>..........................] - ETA: 23:11 - loss: 0.7426 - acc: 0.5014
1536/9333 [===>..........................] - ETA: 22:52 - loss: 0.7435 - acc: 0.4980
1600/9333 [====>.........................] - ETA: 22:36 - loss: 0.7453 - acc: 0.4969
1664/9333 [====>.........................] - ETA: 22:18 - loss: 0.7463 - acc: 0.4952
1728/9333 [====>.........................] - ETA: 22:00 - loss: 0.7445 - acc: 0.4988
1792/9333 [====>.........................] - ETA: 21:45 - loss: 0.7431 - acc: 0.5011
1856/9333 [====>.........................] - ETA: 21:27 - loss: 0.7445 - acc: 0.4968
1920/9333 [=====>........................] - ETA: 21:10 - loss: 0.7438 - acc: 0.4969
1984/9333 [=====>........................] - ETA: 20:53 - loss: 0.7427 - acc: 0.4970
2048/9333 [=====>........................] - ETA: 20:39 - loss: 0.7434 - acc: 0.4956
2112/9333 [=====>........................] - ETA: 20:27 - loss: 0.7431 - acc: 0.4967
2176/9333 [=====>........................] - ETA: 20:13 - loss: 0.7427 - acc: 0.4954
2240/9333 [======>.......................] - ETA: 20:01 - loss: 0.7434 - acc: 0.4933
2304/9333 [======>.......................] - ETA: 19:46 - loss: 0.7427 - acc: 0.4935
2368/9333 [======>.......................] - ETA: 19:33 - loss: 0.7415 - acc: 0.4945
2432/9333 [======>.......................] - ETA: 19:21 - loss: 0.7413 - acc: 0.4951
2496/9333 [=======>......................] - ETA: 19:07 - loss: 0.7413 - acc: 0.4948
2560/9333 [=======>......................] - ETA: 18:52 - loss: 0.7415 - acc: 0.4930
2624/9333 [=======>......................] - ETA: 18:39 - loss: 0.7396 - acc: 0.4950
2688/9333 [=======>......................] - ETA: 18:28 - loss: 0.7391 - acc: 0.4952
2752/9333 [=======>......................] - ETA: 18:15 - loss: 0.7388 - acc: 0.4953
2816/9333 [========>.....................] - ETA: 18:03 - loss: 0.7392 - acc: 0.4950
2880/9333 [========>.....................] - ETA: 17:51 - loss: 0.7380 - acc: 0.4976
2944/9333 [========>.....................] - ETA: 17:37 - loss: 0.7377 - acc: 0.4973
3008/9333 [========>.....................] - ETA: 17:24 - loss: 0.7363 - acc: 0.4997
3072/9333 [========>.....................] - ETA: 17:13 - loss: 0.7360 - acc: 0.4984
3136/9333 [=========>....................] - ETA: 17:01 - loss: 0.7351 - acc: 0.4990
3200/9333 [=========>....................] - ETA: 16:49 - loss: 0.7341 - acc: 0.4991
3264/9333 [=========>....................] - ETA: 16:36 - loss: 0.7333 - acc: 0.5006
3328/9333 [=========>....................] - ETA: 16:23 - loss: 0.7330 - acc: 0.5018
3392/9333 [=========>....................] - ETA: 16:11 - loss: 0.7331 - acc: 0.5012
3456/9333 [==========>...................] - ETA: 16:00 - loss: 0.7331 - acc: 0.5006
3520/9333 [==========>...................] - ETA: 15:49 - loss: 0.7320 - acc: 0.5017
3584/9333 [==========>...................] - ETA: 15:37 - loss: 0.7312 - acc: 0.5031
3648/9333 [==========>...................] - ETA: 15:26 - loss: 0.7306 - acc: 0.5036
3712/9333 [==========>...................] - ETA: 15:15 - loss: 0.7295 - acc: 0.5048
3776/9333 [===========>..................] - ETA: 15:06 - loss: 0.7289 - acc: 0.5050
3840/9333 [===========>..................] - ETA: 14:55 - loss: 0.7297 - acc: 0.5036
3904/9333 [===========>..................] - ETA: 14:44 - loss: 0.7307 - acc: 0.5023
3968/9333 [===========>..................] - ETA: 14:33 - loss: 0.7305 - acc: 0.5035
4032/9333 [===========>..................] - ETA: 14:21 - loss: 0.7296 - acc: 0.5050
4096/9333 [============>.................] - ETA: 14:09 - loss: 0.7286 - acc: 0.5051
4160/9333 [============>.................] - ETA: 13:57 - loss: 0.7285 - acc: 0.5048
4224/9333 [============>.................] - ETA: 13:45 - loss: 0.7283 - acc: 0.5050
4288/9333 [============>.................] - ETA: 13:34 - loss: 0.7283 - acc: 0.5051
4352/9333 [============>.................] - ETA: 13:23 - loss: 0.7279 - acc: 0.5062
4416/9333 [=============>................] - ETA: 13:11 - loss: 0.7274 - acc: 0.5079
4480/9333 [=============>................] - ETA: 13:00 - loss: 0.7267 - acc: 0.5085
4544/9333 [=============>................] - ETA: 12:49 - loss: 0.7267 - acc: 0.5081
4608/9333 [=============>................] - ETA: 12:38 - loss: 0.7268 - acc: 0.5069
4672/9333 [==============>...............] - ETA: 12:25 - loss: 0.7263 - acc: 0.5086
4736/9333 [==============>...............] - ETA: 12:13 - loss: 0.7266 - acc: 0.5072
4800/9333 [==============>...............] - ETA: 12:04 - loss: 0.7266 - acc: 0.5067
4864/9333 [==============>...............] - ETA: 11:52 - loss: 0.7263 - acc: 0.5074
4928/9333 [==============>...............] - ETA: 11:42 - loss: 0.7259 - acc: 0.5071
4992/9333 [===============>..............] - ETA: 11:30 - loss: 0.7262 - acc: 0.5068
5056/9333 [===============>..............] - ETA: 11:20 - loss: 0.7255 - acc: 0.5075
5120/9333 [===============>..............] - ETA: 11:08 - loss: 0.7254 - acc: 0.5074
5184/9333 [===============>..............] - ETA: 10:58 - loss: 0.7252 - acc: 0.5077
5248/9333 [===============>..............] - ETA: 10:47 - loss: 0.7250 - acc: 0.5080
5312/9333 [================>.............] - ETA: 10:36 - loss: 0.7248 - acc: 0.5081
5376/9333 [================>.............] - ETA: 10:26 - loss: 0.7240 - acc: 0.5093
5440/9333 [================>.............] - ETA: 10:15 - loss: 0.7234 - acc: 0.5097
5504/9333 [================>.............] - ETA: 10:04 - loss: 0.7228 - acc: 0.5096
5568/9333 [================>.............] - ETA: 9:53 - loss: 0.7230 - acc: 0.5092 
5632/9333 [=================>............] - ETA: 9:43 - loss: 0.7225 - acc: 0.5107
5696/9333 [=================>............] - ETA: 9:32 - loss: 0.7226 - acc: 0.5102
5760/9333 [=================>............] - ETA: 9:22 - loss: 0.7225 - acc: 0.5101
5824/9333 [=================>............] - ETA: 9:12 - loss: 0.7220 - acc: 0.5108
5888/9333 [=================>............] - ETA: 9:01 - loss: 0.7222 - acc: 0.5100
5952/9333 [==================>...........] - ETA: 8:51 - loss: 0.7221 - acc: 0.5097
6016/9333 [==================>...........] - ETA: 8:40 - loss: 0.7214 - acc: 0.5106
6080/9333 [==================>...........] - ETA: 8:30 - loss: 0.7209 - acc: 0.5107
6144/9333 [==================>...........] - ETA: 8:19 - loss: 0.7205 - acc: 0.5124
6208/9333 [==================>...........] - ETA: 8:09 - loss: 0.7206 - acc: 0.5113
6272/9333 [===================>..........] - ETA: 7:58 - loss: 0.7200 - acc: 0.5118
6336/9333 [===================>..........] - ETA: 7:48 - loss: 0.7203 - acc: 0.5110
6400/9333 [===================>..........] - ETA: 7:37 - loss: 0.7204 - acc: 0.5105
6464/9333 [===================>..........] - ETA: 7:27 - loss: 0.7200 - acc: 0.5110
6528/9333 [===================>..........] - ETA: 7:17 - loss: 0.7198 - acc: 0.5112
6592/9333 [====================>.........] - ETA: 7:06 - loss: 0.7197 - acc: 0.5108
6656/9333 [====================>.........] - ETA: 6:56 - loss: 0.7199 - acc: 0.5099
6720/9333 [====================>.........] - ETA: 6:46 - loss: 0.7197 - acc: 0.5101
6784/9333 [====================>.........] - ETA: 6:36 - loss: 0.7194 - acc: 0.5100
6848/9333 [=====================>........] - ETA: 6:26 - loss: 0.7193 - acc: 0.5105
6912/9333 [=====================>........] - ETA: 6:16 - loss: 0.7193 - acc: 0.5100
6976/9333 [=====================>........] - ETA: 6:05 - loss: 0.7189 - acc: 0.5112
7040/9333 [=====================>........] - ETA: 5:55 - loss: 0.7190 - acc: 0.5102
7104/9333 [=====================>........] - ETA: 5:45 - loss: 0.7189 - acc: 0.5101
7168/9333 [======================>.......] - ETA: 5:35 - loss: 0.7190 - acc: 0.5098
7232/9333 [======================>.......] - ETA: 5:24 - loss: 0.7188 - acc: 0.5102
7296/9333 [======================>.......] - ETA: 5:14 - loss: 0.7181 - acc: 0.5111
7360/9333 [======================>.......] - ETA: 5:04 - loss: 0.7180 - acc: 0.5113
7424/9333 [======================>.......] - ETA: 4:54 - loss: 0.7178 - acc: 0.5113
7488/9333 [=======================>......] - ETA: 4:44 - loss: 0.7176 - acc: 0.5118
7552/9333 [=======================>......] - ETA: 4:34 - loss: 0.7173 - acc: 0.5123
7616/9333 [=======================>......] - ETA: 4:24 - loss: 0.7171 - acc: 0.5129
7680/9333 [=======================>......] - ETA: 4:14 - loss: 0.7169 - acc: 0.5125
7744/9333 [=======================>......] - ETA: 4:04 - loss: 0.7172 - acc: 0.5121
7808/9333 [========================>.....] - ETA: 3:54 - loss: 0.7173 - acc: 0.5113
7872/9333 [========================>.....] - ETA: 3:44 - loss: 0.7170 - acc: 0.5113
7936/9333 [========================>.....] - ETA: 3:34 - loss: 0.7169 - acc: 0.5115
8000/9333 [========================>.....] - ETA: 3:25 - loss: 0.7169 - acc: 0.5111
8064/9333 [========================>.....] - ETA: 3:14 - loss: 0.7167 - acc: 0.5109
8128/9333 [=========================>....] - ETA: 3:05 - loss: 0.7164 - acc: 0.5113
8192/9333 [=========================>....] - ETA: 2:55 - loss: 0.7164 - acc: 0.5114
8256/9333 [=========================>....] - ETA: 2:45 - loss: 0.7164 - acc: 0.5117
8320/9333 [=========================>....] - ETA: 2:35 - loss: 0.7164 - acc: 0.5117
8384/9333 [=========================>....] - ETA: 2:25 - loss: 0.7165 - acc: 0.5117
8448/9333 [==========================>...] - ETA: 2:15 - loss: 0.7166 - acc: 0.5112
8512/9333 [==========================>...] - ETA: 2:05 - loss: 0.7165 - acc: 0.5114
8576/9333 [==========================>...] - ETA: 1:55 - loss: 0.7164 - acc: 0.5112
8640/9333 [==========================>...] - ETA: 1:45 - loss: 0.7163 - acc: 0.5111
8704/9333 [==========================>...] - ETA: 1:36 - loss: 0.7162 - acc: 0.5113
8768/9333 [===========================>..] - ETA: 1:26 - loss: 0.7159 - acc: 0.5119
8832/9333 [===========================>..] - ETA: 1:16 - loss: 0.7157 - acc: 0.5120
8896/9333 [===========================>..] - ETA: 1:06 - loss: 0.7159 - acc: 0.5115
8960/9333 [===========================>..] - ETA: 56s - loss: 0.7161 - acc: 0.5109 
9024/9333 [============================>.] - ETA: 47s - loss: 0.7162 - acc: 0.5106
9088/9333 [============================>.] - ETA: 37s - loss: 0.7159 - acc: 0.5105
9152/9333 [============================>.] - ETA: 27s - loss: 0.7158 - acc: 0.5103
9216/9333 [============================>.] - ETA: 17s - loss: 0.7159 - acc: 0.5101
9280/9333 [============================>.] - ETA: 8s - loss: 0.7160 - acc: 0.5097 
9333/9333 [==============================] - 1472s 158ms/step - loss: 0.7160 - acc: 0.5095 - val_loss: 0.6910 - val_acc: 0.5362

Epoch 00001: val_acc improved from -inf to 0.53616, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window21/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 2/10

  64/9333 [..............................] - ETA: 22:25 - loss: 0.6499 - acc: 0.6094
 128/9333 [..............................] - ETA: 23:17 - loss: 0.6915 - acc: 0.4922
 192/9333 [..............................] - ETA: 22:35 - loss: 0.6978 - acc: 0.4948
 256/9333 [..............................] - ETA: 22:22 - loss: 0.6913 - acc: 0.5117
 320/9333 [>.............................] - ETA: 21:54 - loss: 0.6839 - acc: 0.5375
 384/9333 [>.............................] - ETA: 21:43 - loss: 0.6845 - acc: 0.5443
 448/9333 [>.............................] - ETA: 21:17 - loss: 0.6904 - acc: 0.5312
 512/9333 [>.............................] - ETA: 21:01 - loss: 0.6892 - acc: 0.5312
 576/9333 [>.............................] - ETA: 20:52 - loss: 0.6934 - acc: 0.5208
 640/9333 [=>............................] - ETA: 20:42 - loss: 0.6921 - acc: 0.5234
 704/9333 [=>............................] - ETA: 20:25 - loss: 0.6937 - acc: 0.5241
 768/9333 [=>............................] - ETA: 20:17 - loss: 0.6961 - acc: 0.5221
 832/9333 [=>............................] - ETA: 20:04 - loss: 0.6969 - acc: 0.5240
 896/9333 [=>............................] - ETA: 19:55 - loss: 0.6962 - acc: 0.5223
 960/9333 [==>...........................] - ETA: 19:46 - loss: 0.6961 - acc: 0.5208
1024/9333 [==>...........................] - ETA: 19:43 - loss: 0.6956 - acc: 0.5205
1088/9333 [==>...........................] - ETA: 19:34 - loss: 0.6926 - acc: 0.5267
1152/9333 [==>...........................] - ETA: 19:26 - loss: 0.6946 - acc: 0.5234
1216/9333 [==>...........................] - ETA: 19:13 - loss: 0.6993 - acc: 0.5197
1280/9333 [===>..........................] - ETA: 19:02 - loss: 0.7011 - acc: 0.5164
1344/9333 [===>..........................] - ETA: 18:54 - loss: 0.7006 - acc: 0.5201
1408/9333 [===>..........................] - ETA: 18:46 - loss: 0.7014 - acc: 0.5178
1472/9333 [===>..........................] - ETA: 18:38 - loss: 0.7037 - acc: 0.5136
1536/9333 [===>..........................] - ETA: 18:31 - loss: 0.7031 - acc: 0.5150
1600/9333 [====>.........................] - ETA: 18:19 - loss: 0.7023 - acc: 0.5194
1664/9333 [====>.........................] - ETA: 18:14 - loss: 0.7024 - acc: 0.5180
1728/9333 [====>.........................] - ETA: 18:04 - loss: 0.7023 - acc: 0.5162
1792/9333 [====>.........................] - ETA: 17:57 - loss: 0.7022 - acc: 0.5167
1856/9333 [====>.........................] - ETA: 17:47 - loss: 0.7021 - acc: 0.5140
1920/9333 [=====>........................] - ETA: 17:42 - loss: 0.7023 - acc: 0.5125
1984/9333 [=====>........................] - ETA: 17:33 - loss: 0.7020 - acc: 0.5111
2048/9333 [=====>........................] - ETA: 17:28 - loss: 0.7020 - acc: 0.5107
2112/9333 [=====>........................] - ETA: 17:17 - loss: 0.7023 - acc: 0.5099
2176/9333 [=====>........................] - ETA: 17:09 - loss: 0.7015 - acc: 0.5101
2240/9333 [======>.......................] - ETA: 16:57 - loss: 0.7016 - acc: 0.5094
2304/9333 [======>.......................] - ETA: 16:47 - loss: 0.7014 - acc: 0.5100
2368/9333 [======>.......................] - ETA: 16:39 - loss: 0.7013 - acc: 0.5122
2432/9333 [======>.......................] - ETA: 16:30 - loss: 0.7008 - acc: 0.5144
2496/9333 [=======>......................] - ETA: 16:22 - loss: 0.7006 - acc: 0.5144
2560/9333 [=======>......................] - ETA: 16:12 - loss: 0.7004 - acc: 0.5141
2624/9333 [=======>......................] - ETA: 16:04 - loss: 0.7000 - acc: 0.5156
2688/9333 [=======>......................] - ETA: 15:53 - loss: 0.6996 - acc: 0.5164
2752/9333 [=======>......................] - ETA: 15:43 - loss: 0.6999 - acc: 0.5160
2816/9333 [========>.....................] - ETA: 15:34 - loss: 0.7000 - acc: 0.5156
2880/9333 [========>.....................] - ETA: 15:25 - loss: 0.7009 - acc: 0.5135
2944/9333 [========>.....................] - ETA: 15:19 - loss: 0.7016 - acc: 0.5119
3008/9333 [========>.....................] - ETA: 15:09 - loss: 0.7016 - acc: 0.5113
3072/9333 [========>.....................] - ETA: 15:00 - loss: 0.7023 - acc: 0.5107
3136/9333 [=========>....................] - ETA: 14:51 - loss: 0.7030 - acc: 0.5092
3200/9333 [=========>....................] - ETA: 14:41 - loss: 0.7028 - acc: 0.5094
3264/9333 [=========>....................] - ETA: 14:32 - loss: 0.7026 - acc: 0.5110
3328/9333 [=========>....................] - ETA: 14:23 - loss: 0.7019 - acc: 0.5129
3392/9333 [=========>....................] - ETA: 14:13 - loss: 0.7017 - acc: 0.5139
3456/9333 [==========>...................] - ETA: 14:03 - loss: 0.7012 - acc: 0.5156
3520/9333 [==========>...................] - ETA: 13:55 - loss: 0.7008 - acc: 0.5165
3584/9333 [==========>...................] - ETA: 13:45 - loss: 0.7006 - acc: 0.5162
3648/9333 [==========>...................] - ETA: 13:36 - loss: 0.7004 - acc: 0.5148
3712/9333 [==========>...................] - ETA: 13:27 - loss: 0.7003 - acc: 0.5148
3776/9333 [===========>..................] - ETA: 13:19 - loss: 0.7003 - acc: 0.5159
3840/9333 [===========>..................] - ETA: 13:09 - loss: 0.7003 - acc: 0.5164
3904/9333 [===========>..................] - ETA: 13:00 - loss: 0.7003 - acc: 0.5166
3968/9333 [===========>..................] - ETA: 12:51 - loss: 0.7007 - acc: 0.5156
4032/9333 [===========>..................] - ETA: 12:41 - loss: 0.7003 - acc: 0.5164
4096/9333 [============>.................] - ETA: 12:32 - loss: 0.6997 - acc: 0.5183
4160/9333 [============>.................] - ETA: 12:23 - loss: 0.6994 - acc: 0.5192
4224/9333 [============>.................] - ETA: 12:14 - loss: 0.6997 - acc: 0.5189
4288/9333 [============>.................] - ETA: 12:04 - loss: 0.6999 - acc: 0.5191
4352/9333 [============>.................] - ETA: 11:55 - loss: 0.6993 - acc: 0.5198
4416/9333 [=============>................] - ETA: 11:46 - loss: 0.6993 - acc: 0.5192
4480/9333 [=============>................] - ETA: 11:38 - loss: 0.6990 - acc: 0.5190
4544/9333 [=============>................] - ETA: 11:29 - loss: 0.6992 - acc: 0.5191
4608/9333 [=============>................] - ETA: 11:20 - loss: 0.6992 - acc: 0.5191
4672/9333 [==============>...............] - ETA: 11:12 - loss: 0.6991 - acc: 0.5199
4736/9333 [==============>...............] - ETA: 11:03 - loss: 0.6994 - acc: 0.5188
4800/9333 [==============>...............] - ETA: 10:54 - loss: 0.6988 - acc: 0.5202
4864/9333 [==============>...............] - ETA: 10:45 - loss: 0.6988 - acc: 0.5201
4928/9333 [==============>...............] - ETA: 10:36 - loss: 0.6982 - acc: 0.5225
4992/9333 [===============>..............] - ETA: 10:27 - loss: 0.6981 - acc: 0.5224
5056/9333 [===============>..............] - ETA: 10:18 - loss: 0.6985 - acc: 0.5218
5120/9333 [===============>..............] - ETA: 10:10 - loss: 0.6986 - acc: 0.5205
5184/9333 [===============>..............] - ETA: 10:01 - loss: 0.6990 - acc: 0.5195
5248/9333 [===============>..............] - ETA: 9:52 - loss: 0.6992 - acc: 0.5185 
5312/9333 [================>.............] - ETA: 9:43 - loss: 0.6988 - acc: 0.5194
5376/9333 [================>.............] - ETA: 9:34 - loss: 0.6988 - acc: 0.5197
5440/9333 [================>.............] - ETA: 9:25 - loss: 0.6991 - acc: 0.5188
5504/9333 [================>.............] - ETA: 9:17 - loss: 0.6991 - acc: 0.5185
5568/9333 [================>.............] - ETA: 9:09 - loss: 0.6990 - acc: 0.5180
5632/9333 [=================>............] - ETA: 9:01 - loss: 0.6989 - acc: 0.5186
5696/9333 [=================>............] - ETA: 8:51 - loss: 0.6990 - acc: 0.5177
5760/9333 [=================>............] - ETA: 8:43 - loss: 0.6987 - acc: 0.5179
5824/9333 [=================>............] - ETA: 8:33 - loss: 0.6985 - acc: 0.5184
5888/9333 [=================>............] - ETA: 8:25 - loss: 0.6987 - acc: 0.5177
5952/9333 [==================>...........] - ETA: 8:15 - loss: 0.6988 - acc: 0.5171
6016/9333 [==================>...........] - ETA: 8:06 - loss: 0.6989 - acc: 0.5175
6080/9333 [==================>...........] - ETA: 7:57 - loss: 0.6991 - acc: 0.5169
6144/9333 [==================>...........] - ETA: 7:47 - loss: 0.6992 - acc: 0.5164
6208/9333 [==================>...........] - ETA: 7:39 - loss: 0.6995 - acc: 0.5148
6272/9333 [===================>..........] - ETA: 7:30 - loss: 0.6993 - acc: 0.5153
6336/9333 [===================>..........] - ETA: 7:21 - loss: 0.6995 - acc: 0.5144
6400/9333 [===================>..........] - ETA: 7:12 - loss: 0.6996 - acc: 0.5134
6464/9333 [===================>..........] - ETA: 7:03 - loss: 0.6996 - acc: 0.5128
6528/9333 [===================>..........] - ETA: 6:54 - loss: 0.6994 - acc: 0.5136
6592/9333 [====================>.........] - ETA: 6:45 - loss: 0.6994 - acc: 0.5135
6656/9333 [====================>.........] - ETA: 6:36 - loss: 0.6995 - acc: 0.5128
6720/9333 [====================>.........] - ETA: 6:27 - loss: 0.6994 - acc: 0.5129
6784/9333 [====================>.........] - ETA: 6:17 - loss: 0.6994 - acc: 0.5136
6848/9333 [=====================>........] - ETA: 6:08 - loss: 0.6992 - acc: 0.5139
6912/9333 [=====================>........] - ETA: 5:59 - loss: 0.6993 - acc: 0.5135
6976/9333 [=====================>........] - ETA: 5:49 - loss: 0.6993 - acc: 0.5136
7040/9333 [=====================>........] - ETA: 5:40 - loss: 0.6992 - acc: 0.5139
7104/9333 [=====================>........] - ETA: 5:31 - loss: 0.6991 - acc: 0.5141
7168/9333 [======================>.......] - ETA: 5:22 - loss: 0.6991 - acc: 0.5133
7232/9333 [======================>.......] - ETA: 5:13 - loss: 0.6989 - acc: 0.5140
7296/9333 [======================>.......] - ETA: 5:03 - loss: 0.6989 - acc: 0.5143
7360/9333 [======================>.......] - ETA: 4:54 - loss: 0.6990 - acc: 0.5139
7424/9333 [======================>.......] - ETA: 4:45 - loss: 0.6991 - acc: 0.5140
7488/9333 [=======================>......] - ETA: 4:35 - loss: 0.6991 - acc: 0.5136
7552/9333 [=======================>......] - ETA: 4:26 - loss: 0.6990 - acc: 0.5140
7616/9333 [=======================>......] - ETA: 4:16 - loss: 0.6991 - acc: 0.5142
7680/9333 [=======================>......] - ETA: 4:07 - loss: 0.6988 - acc: 0.5145
7744/9333 [=======================>......] - ETA: 3:58 - loss: 0.6988 - acc: 0.5150
7808/9333 [========================>.....] - ETA: 3:48 - loss: 0.6989 - acc: 0.5146
7872/9333 [========================>.....] - ETA: 3:39 - loss: 0.6990 - acc: 0.5140
7936/9333 [========================>.....] - ETA: 3:29 - loss: 0.6989 - acc: 0.5142
8000/9333 [========================>.....] - ETA: 3:20 - loss: 0.6989 - acc: 0.5141
8064/9333 [========================>.....] - ETA: 3:10 - loss: 0.6987 - acc: 0.5146
8128/9333 [=========================>....] - ETA: 3:01 - loss: 0.6986 - acc: 0.5149
8192/9333 [=========================>....] - ETA: 2:52 - loss: 0.6988 - acc: 0.5148
8256/9333 [=========================>....] - ETA: 2:42 - loss: 0.6988 - acc: 0.5147
8320/9333 [=========================>....] - ETA: 2:33 - loss: 0.6990 - acc: 0.5139
8384/9333 [=========================>....] - ETA: 2:23 - loss: 0.6989 - acc: 0.5144
8448/9333 [==========================>...] - ETA: 2:13 - loss: 0.6989 - acc: 0.5147
8512/9333 [==========================>...] - ETA: 2:04 - loss: 0.6988 - acc: 0.5142
8576/9333 [==========================>...] - ETA: 1:54 - loss: 0.6987 - acc: 0.5146
8640/9333 [==========================>...] - ETA: 1:45 - loss: 0.6988 - acc: 0.5142
8704/9333 [==========================>...] - ETA: 1:35 - loss: 0.6988 - acc: 0.5140
8768/9333 [===========================>..] - ETA: 1:25 - loss: 0.6988 - acc: 0.5143
8832/9333 [===========================>..] - ETA: 1:16 - loss: 0.6990 - acc: 0.5144
8896/9333 [===========================>..] - ETA: 1:06 - loss: 0.6991 - acc: 0.5138
8960/9333 [===========================>..] - ETA: 56s - loss: 0.6991 - acc: 0.5134 
9024/9333 [============================>.] - ETA: 47s - loss: 0.6990 - acc: 0.5130
9088/9333 [============================>.] - ETA: 37s - loss: 0.6989 - acc: 0.5136
9152/9333 [============================>.] - ETA: 27s - loss: 0.6989 - acc: 0.5131
9216/9333 [============================>.] - ETA: 17s - loss: 0.6990 - acc: 0.5125
9280/9333 [============================>.] - ETA: 8s - loss: 0.6990 - acc: 0.5124 
9333/9333 [==============================] - 1484s 159ms/step - loss: 0.6990 - acc: 0.5123 - val_loss: 0.6918 - val_acc: 0.5082

Epoch 00002: val_acc did not improve from 0.53616
Epoch 3/10

  64/9333 [..............................] - ETA: 27:42 - loss: 0.6892 - acc: 0.5625
 128/9333 [..............................] - ETA: 26:05 - loss: 0.6836 - acc: 0.5703
 192/9333 [..............................] - ETA: 25:34 - loss: 0.6890 - acc: 0.5573
 256/9333 [..............................] - ETA: 25:03 - loss: 0.6812 - acc: 0.5781
 320/9333 [>.............................] - ETA: 24:56 - loss: 0.6799 - acc: 0.5906
 384/9333 [>.............................] - ETA: 24:43 - loss: 0.6857 - acc: 0.5755
 448/9333 [>.............................] - ETA: 24:38 - loss: 0.6846 - acc: 0.5670
 512/9333 [>.............................] - ETA: 24:23 - loss: 0.6873 - acc: 0.5586
 576/9333 [>.............................] - ETA: 24:13 - loss: 0.6861 - acc: 0.5590
 640/9333 [=>............................] - ETA: 23:59 - loss: 0.6879 - acc: 0.5437
 704/9333 [=>............................] - ETA: 23:56 - loss: 0.6881 - acc: 0.5369
 768/9333 [=>............................] - ETA: 23:42 - loss: 0.6880 - acc: 0.5326
 832/9333 [=>............................] - ETA: 23:36 - loss: 0.6883 - acc: 0.5288
 896/9333 [=>............................] - ETA: 23:16 - loss: 0.6878 - acc: 0.5268
 960/9333 [==>...........................] - ETA: 23:00 - loss: 0.6872 - acc: 0.5292
1024/9333 [==>...........................] - ETA: 22:48 - loss: 0.6893 - acc: 0.5215
1088/9333 [==>...........................] - ETA: 22:36 - loss: 0.6896 - acc: 0.5202
1152/9333 [==>...........................] - ETA: 22:25 - loss: 0.6900 - acc: 0.5234
1216/9333 [==>...........................] - ETA: 22:13 - loss: 0.6912 - acc: 0.5197
1280/9333 [===>..........................] - ETA: 22:08 - loss: 0.6914 - acc: 0.5234
1344/9333 [===>..........................] - ETA: 21:56 - loss: 0.6925 - acc: 0.5193
1408/9333 [===>..........................] - ETA: 21:46 - loss: 0.6925 - acc: 0.5192
1472/9333 [===>..........................] - ETA: 21:36 - loss: 0.6928 - acc: 0.5170
1536/9333 [===>..........................] - ETA: 21:28 - loss: 0.6913 - acc: 0.5202
1600/9333 [====>.........................] - ETA: 21:18 - loss: 0.6909 - acc: 0.5219
1664/9333 [====>.........................] - ETA: 21:06 - loss: 0.6917 - acc: 0.5204
1728/9333 [====>.........................] - ETA: 20:57 - loss: 0.6930 - acc: 0.5174
1792/9333 [====>.........................] - ETA: 20:46 - loss: 0.6930 - acc: 0.5184
1856/9333 [====>.........................] - ETA: 20:37 - loss: 0.6931 - acc: 0.5183
1920/9333 [=====>........................] - ETA: 20:26 - loss: 0.6940 - acc: 0.5156
1984/9333 [=====>........................] - ETA: 20:16 - loss: 0.6949 - acc: 0.5126
2048/9333 [=====>........................] - ETA: 20:06 - loss: 0.6941 - acc: 0.5156
2112/9333 [=====>........................] - ETA: 19:59 - loss: 0.6941 - acc: 0.5161
2176/9333 [=====>........................] - ETA: 19:48 - loss: 0.6940 - acc: 0.5152
2240/9333 [======>.......................] - ETA: 19:37 - loss: 0.6943 - acc: 0.5152
2304/9333 [======>.......................] - ETA: 19:25 - loss: 0.6944 - acc: 0.5139
2368/9333 [======>.......................] - ETA: 19:14 - loss: 0.6946 - acc: 0.5135
2432/9333 [======>.......................] - ETA: 19:04 - loss: 0.6945 - acc: 0.5136
2496/9333 [=======>......................] - ETA: 18:52 - loss: 0.6947 - acc: 0.5116
2560/9333 [=======>......................] - ETA: 18:42 - loss: 0.6949 - acc: 0.5109
2624/9333 [=======>......................] - ETA: 18:30 - loss: 0.6949 - acc: 0.5114
2688/9333 [=======>......................] - ETA: 18:19 - loss: 0.6947 - acc: 0.5141
2752/9333 [=======>......................] - ETA: 18:07 - loss: 0.6952 - acc: 0.5131
2816/9333 [========>.....................] - ETA: 17:57 - loss: 0.6947 - acc: 0.5174
2880/9333 [========>.....................] - ETA: 17:48 - loss: 0.6943 - acc: 0.5181
2944/9333 [========>.....................] - ETA: 17:36 - loss: 0.6941 - acc: 0.5187
3008/9333 [========>.....................] - ETA: 17:24 - loss: 0.6943 - acc: 0.5193
3072/9333 [========>.....................] - ETA: 17:13 - loss: 0.6937 - acc: 0.5212
3136/9333 [=========>....................] - ETA: 17:04 - loss: 0.6935 - acc: 0.5220
3200/9333 [=========>....................] - ETA: 16:52 - loss: 0.6933 - acc: 0.5228
3264/9333 [=========>....................] - ETA: 16:42 - loss: 0.6935 - acc: 0.5227
3328/9333 [=========>....................] - ETA: 16:32 - loss: 0.6932 - acc: 0.5237
3392/9333 [=========>....................] - ETA: 16:21 - loss: 0.6933 - acc: 0.5224
3456/9333 [==========>...................] - ETA: 16:11 - loss: 0.6930 - acc: 0.5231
3520/9333 [==========>...................] - ETA: 16:01 - loss: 0.6934 - acc: 0.5219
3584/9333 [==========>...................] - ETA: 15:51 - loss: 0.6938 - acc: 0.5212
3648/9333 [==========>...................] - ETA: 15:40 - loss: 0.6934 - acc: 0.5244
3712/9333 [==========>...................] - ETA: 15:29 - loss: 0.6931 - acc: 0.5248
3776/9333 [===========>..................] - ETA: 15:18 - loss: 0.6925 - acc: 0.5257
3840/9333 [===========>..................] - ETA: 15:06 - loss: 0.6926 - acc: 0.5253
3904/9333 [===========>..................] - ETA: 14:55 - loss: 0.6930 - acc: 0.5236
3968/9333 [===========>..................] - ETA: 14:44 - loss: 0.6933 - acc: 0.5227
4032/9333 [===========>..................] - ETA: 14:34 - loss: 0.6946 - acc: 0.5201
4096/9333 [============>.................] - ETA: 14:23 - loss: 0.6943 - acc: 0.5208
4160/9333 [============>.................] - ETA: 14:13 - loss: 0.6943 - acc: 0.5214
4224/9333 [============>.................] - ETA: 14:01 - loss: 0.6948 - acc: 0.5199
4288/9333 [============>.................] - ETA: 13:51 - loss: 0.6945 - acc: 0.5203
4352/9333 [============>.................] - ETA: 13:40 - loss: 0.6947 - acc: 0.5186
4416/9333 [=============>................] - ETA: 13:28 - loss: 0.6946 - acc: 0.5186
4480/9333 [=============>................] - ETA: 13:17 - loss: 0.6941 - acc: 0.5201
4544/9333 [=============>................] - ETA: 13:06 - loss: 0.6944 - acc: 0.5189
4608/9333 [=============>................] - ETA: 12:56 - loss: 0.6942 - acc: 0.5191
4672/9333 [==============>...............] - ETA: 12:45 - loss: 0.6940 - acc: 0.5199
4736/9333 [==============>...............] - ETA: 12:35 - loss: 0.6938 - acc: 0.5205
4800/9333 [==============>...............] - ETA: 12:24 - loss: 0.6942 - acc: 0.5196
4864/9333 [==============>...............] - ETA: 12:14 - loss: 0.6940 - acc: 0.5193
4928/9333 [==============>...............] - ETA: 12:04 - loss: 0.6943 - acc: 0.5181
4992/9333 [===============>..............] - ETA: 11:54 - loss: 0.6942 - acc: 0.5182
5056/9333 [===============>..............] - ETA: 11:43 - loss: 0.6945 - acc: 0.5170
5120/9333 [===============>..............] - ETA: 11:33 - loss: 0.6946 - acc: 0.5158
5184/9333 [===============>..............] - ETA: 11:22 - loss: 0.6947 - acc: 0.5149
5248/9333 [===============>..............] - ETA: 11:13 - loss: 0.6948 - acc: 0.5143
5312/9333 [================>.............] - ETA: 11:02 - loss: 0.6950 - acc: 0.5143
5376/9333 [================>.............] - ETA: 10:51 - loss: 0.6951 - acc: 0.5138
5440/9333 [================>.............] - ETA: 10:40 - loss: 0.6949 - acc: 0.5147
5504/9333 [================>.............] - ETA: 10:30 - loss: 0.6952 - acc: 0.5134
5568/9333 [================>.............] - ETA: 10:19 - loss: 0.6949 - acc: 0.5145
5632/9333 [=================>............] - ETA: 10:09 - loss: 0.6950 - acc: 0.5137
5696/9333 [=================>............] - ETA: 9:58 - loss: 0.6952 - acc: 0.5128 
5760/9333 [=================>............] - ETA: 9:48 - loss: 0.6952 - acc: 0.5127
5824/9333 [=================>............] - ETA: 9:37 - loss: 0.6954 - acc: 0.5122
5888/9333 [=================>............] - ETA: 9:26 - loss: 0.6954 - acc: 0.5126
5952/9333 [==================>...........] - ETA: 9:16 - loss: 0.6955 - acc: 0.5126
6016/9333 [==================>...........] - ETA: 9:05 - loss: 0.6955 - acc: 0.5121
6080/9333 [==================>...........] - ETA: 8:55 - loss: 0.6954 - acc: 0.5128
6144/9333 [==================>...........] - ETA: 8:44 - loss: 0.6954 - acc: 0.5132
6208/9333 [==================>...........] - ETA: 8:33 - loss: 0.6954 - acc: 0.5129
6272/9333 [===================>..........] - ETA: 8:23 - loss: 0.6954 - acc: 0.5131
6336/9333 [===================>..........] - ETA: 8:12 - loss: 0.6952 - acc: 0.5133
6400/9333 [===================>..........] - ETA: 8:02 - loss: 0.6949 - acc: 0.5139
6464/9333 [===================>..........] - ETA: 7:51 - loss: 0.6949 - acc: 0.5136
6528/9333 [===================>..........] - ETA: 7:41 - loss: 0.6948 - acc: 0.5139
6592/9333 [====================>.........] - ETA: 7:30 - loss: 0.6951 - acc: 0.5135
6656/9333 [====================>.........] - ETA: 7:20 - loss: 0.6949 - acc: 0.5138
6720/9333 [====================>.........] - ETA: 7:10 - loss: 0.6949 - acc: 0.5138
6784/9333 [====================>.........] - ETA: 6:59 - loss: 0.6949 - acc: 0.5136
6848/9333 [=====================>........] - ETA: 6:49 - loss: 0.6948 - acc: 0.5139
6912/9333 [=====================>........] - ETA: 6:39 - loss: 0.6948 - acc: 0.5140
6976/9333 [=====================>........] - ETA: 6:28 - loss: 0.6948 - acc: 0.5136
7040/9333 [=====================>........] - ETA: 6:17 - loss: 0.6950 - acc: 0.5131
7104/9333 [=====================>........] - ETA: 6:07 - loss: 0.6950 - acc: 0.5125
7168/9333 [======================>.......] - ETA: 5:56 - loss: 0.6948 - acc: 0.5137
7232/9333 [======================>.......] - ETA: 5:46 - loss: 0.6949 - acc: 0.5133
7296/9333 [======================>.......] - ETA: 5:35 - loss: 0.6947 - acc: 0.5141
7360/9333 [======================>.......] - ETA: 5:25 - loss: 0.6949 - acc: 0.5130
7424/9333 [======================>.......] - ETA: 5:14 - loss: 0.6949 - acc: 0.5129
7488/9333 [=======================>......] - ETA: 5:04 - loss: 0.6950 - acc: 0.5127
7552/9333 [=======================>......] - ETA: 4:53 - loss: 0.6950 - acc: 0.5126
7616/9333 [=======================>......] - ETA: 4:43 - loss: 0.6950 - acc: 0.5127
7680/9333 [=======================>......] - ETA: 4:32 - loss: 0.6950 - acc: 0.5132
7744/9333 [=======================>......] - ETA: 4:22 - loss: 0.6950 - acc: 0.5128
7808/9333 [========================>.....] - ETA: 4:12 - loss: 0.6949 - acc: 0.5127
7872/9333 [========================>.....] - ETA: 4:01 - loss: 0.6949 - acc: 0.5130
7936/9333 [========================>.....] - ETA: 3:51 - loss: 0.6949 - acc: 0.5130
8000/9333 [========================>.....] - ETA: 3:40 - loss: 0.6948 - acc: 0.5125
8064/9333 [========================>.....] - ETA: 3:29 - loss: 0.6947 - acc: 0.5131
8128/9333 [=========================>....] - ETA: 3:19 - loss: 0.6946 - acc: 0.5134
8192/9333 [=========================>....] - ETA: 3:08 - loss: 0.6949 - acc: 0.5129
8256/9333 [=========================>....] - ETA: 2:58 - loss: 0.6949 - acc: 0.5132
8320/9333 [=========================>....] - ETA: 2:47 - loss: 0.6948 - acc: 0.5138
8384/9333 [=========================>....] - ETA: 2:36 - loss: 0.6949 - acc: 0.5135
8448/9333 [==========================>...] - ETA: 2:26 - loss: 0.6949 - acc: 0.5133
8512/9333 [==========================>...] - ETA: 2:15 - loss: 0.6948 - acc: 0.5137
8576/9333 [==========================>...] - ETA: 2:05 - loss: 0.6948 - acc: 0.5139
8640/9333 [==========================>...] - ETA: 1:54 - loss: 0.6947 - acc: 0.5145
8704/9333 [==========================>...] - ETA: 1:43 - loss: 0.6949 - acc: 0.5137
8768/9333 [===========================>..] - ETA: 1:33 - loss: 0.6949 - acc: 0.5133
8832/9333 [===========================>..] - ETA: 1:22 - loss: 0.6949 - acc: 0.5132
8896/9333 [===========================>..] - ETA: 1:12 - loss: 0.6949 - acc: 0.5130
8960/9333 [===========================>..] - ETA: 1:01 - loss: 0.6948 - acc: 0.5134
9024/9333 [============================>.] - ETA: 50s - loss: 0.6949 - acc: 0.5133 
9088/9333 [============================>.] - ETA: 40s - loss: 0.6950 - acc: 0.5130
9152/9333 [============================>.] - ETA: 29s - loss: 0.6949 - acc: 0.5133
9216/9333 [============================>.] - ETA: 19s - loss: 0.6948 - acc: 0.5141
9280/9333 [============================>.] - ETA: 8s - loss: 0.6947 - acc: 0.5142 
9333/9333 [==============================] - 1586s 170ms/step - loss: 0.6948 - acc: 0.5131 - val_loss: 0.6908 - val_acc: 0.5304

Epoch 00003: val_acc did not improve from 0.53616
Epoch 4/10

  64/9333 [..............................] - ETA: 23:27 - loss: 0.6996 - acc: 0.4531
 128/9333 [..............................] - ETA: 23:22 - loss: 0.7063 - acc: 0.4297
 192/9333 [..............................] - ETA: 22:53 - loss: 0.7056 - acc: 0.4427
 256/9333 [..............................] - ETA: 23:01 - loss: 0.7040 - acc: 0.4531
 320/9333 [>.............................] - ETA: 22:59 - loss: 0.7021 - acc: 0.4656
 384/9333 [>.............................] - ETA: 23:10 - loss: 0.7005 - acc: 0.4844
 448/9333 [>.............................] - ETA: 23:00 - loss: 0.7002 - acc: 0.4866
 512/9333 [>.............................] - ETA: 23:06 - loss: 0.6988 - acc: 0.4941
 576/9333 [>.............................] - ETA: 22:47 - loss: 0.6977 - acc: 0.4931
 640/9333 [=>............................] - ETA: 22:53 - loss: 0.6987 - acc: 0.4875
 704/9333 [=>............................] - ETA: 22:38 - loss: 0.6982 - acc: 0.4886
 768/9333 [=>............................] - ETA: 22:34 - loss: 0.6966 - acc: 0.4896
 832/9333 [=>............................] - ETA: 22:16 - loss: 0.6978 - acc: 0.4844
 896/9333 [=>............................] - ETA: 22:13 - loss: 0.6969 - acc: 0.4911
 960/9333 [==>...........................] - ETA: 22:02 - loss: 0.6966 - acc: 0.4927
1024/9333 [==>...........................] - ETA: 21:58 - loss: 0.6966 - acc: 0.4932
1088/9333 [==>...........................] - ETA: 21:42 - loss: 0.6960 - acc: 0.4963
1152/9333 [==>...........................] - ETA: 21:34 - loss: 0.6959 - acc: 0.4974
1216/9333 [==>...........................] - ETA: 21:16 - loss: 0.6952 - acc: 0.4975
1280/9333 [===>..........................] - ETA: 21:07 - loss: 0.6954 - acc: 0.5008
1344/9333 [===>..........................] - ETA: 20:56 - loss: 0.6952 - acc: 0.5000
1408/9333 [===>..........................] - ETA: 20:43 - loss: 0.6948 - acc: 0.5000
1472/9333 [===>..........................] - ETA: 20:32 - loss: 0.6949 - acc: 0.5027
1536/9333 [===>..........................] - ETA: 20:18 - loss: 0.6946 - acc: 0.5020
1600/9333 [====>.........................] - ETA: 20:11 - loss: 0.6938 - acc: 0.5075
1664/9333 [====>.........................] - ETA: 19:58 - loss: 0.6933 - acc: 0.5102
1728/9333 [====>.........................] - ETA: 19:51 - loss: 0.6933 - acc: 0.5116
1792/9333 [====>.........................] - ETA: 19:40 - loss: 0.6937 - acc: 0.5100
1856/9333 [====>.........................] - ETA: 19:31 - loss: 0.6935 - acc: 0.5119
1920/9333 [=====>........................] - ETA: 19:17 - loss: 0.6932 - acc: 0.5146
1984/9333 [=====>........................] - ETA: 19:09 - loss: 0.6934 - acc: 0.5131
2048/9333 [=====>........................] - ETA: 18:56 - loss: 0.6930 - acc: 0.5142
2112/9333 [=====>........................] - ETA: 18:44 - loss: 0.6932 - acc: 0.5156
2176/9333 [=====>........................] - ETA: 18:30 - loss: 0.6936 - acc: 0.5138
2240/9333 [======>.......................] - ETA: 18:20 - loss: 0.6935 - acc: 0.5156
2304/9333 [======>.......................] - ETA: 18:09 - loss: 0.6936 - acc: 0.5156
2368/9333 [======>.......................] - ETA: 17:57 - loss: 0.6938 - acc: 0.5152
2432/9333 [======>.......................] - ETA: 17:47 - loss: 0.6940 - acc: 0.5127
2496/9333 [=======>......................] - ETA: 17:35 - loss: 0.6942 - acc: 0.5104
2560/9333 [=======>......................] - ETA: 17:26 - loss: 0.6943 - acc: 0.5098
2624/9333 [=======>......................] - ETA: 17:16 - loss: 0.6942 - acc: 0.5095
2688/9333 [=======>......................] - ETA: 17:07 - loss: 0.6942 - acc: 0.5097
2752/9333 [=======>......................] - ETA: 16:57 - loss: 0.6945 - acc: 0.5091
2816/9333 [========>.....................] - ETA: 16:48 - loss: 0.6947 - acc: 0.5099
2880/9333 [========>.....................] - ETA: 16:37 - loss: 0.6947 - acc: 0.5118
2944/9333 [========>.....................] - ETA: 16:28 - loss: 0.6943 - acc: 0.5126
3008/9333 [========>.....................] - ETA: 16:17 - loss: 0.6945 - acc: 0.5133
3072/9333 [========>.....................] - ETA: 16:06 - loss: 0.6944 - acc: 0.5120
3136/9333 [=========>....................] - ETA: 15:57 - loss: 0.6943 - acc: 0.5112
3200/9333 [=========>....................] - ETA: 15:47 - loss: 0.6941 - acc: 0.5119
3264/9333 [=========>....................] - ETA: 15:36 - loss: 0.6940 - acc: 0.5126
3328/9333 [=========>....................] - ETA: 15:27 - loss: 0.6943 - acc: 0.5120
3392/9333 [=========>....................] - ETA: 15:15 - loss: 0.6939 - acc: 0.5147
3456/9333 [==========>...................] - ETA: 15:06 - loss: 0.6937 - acc: 0.5150
3520/9333 [==========>...................] - ETA: 14:55 - loss: 0.6942 - acc: 0.5128
3584/9333 [==========>...................] - ETA: 14:44 - loss: 0.6938 - acc: 0.5142
3648/9333 [==========>...................] - ETA: 14:35 - loss: 0.6940 - acc: 0.5137
3712/9333 [==========>...................] - ETA: 14:25 - loss: 0.6940 - acc: 0.5135
3776/9333 [===========>..................] - ETA: 14:15 - loss: 0.6943 - acc: 0.5122
3840/9333 [===========>..................] - ETA: 14:05 - loss: 0.6948 - acc: 0.5112
3904/9333 [===========>..................] - ETA: 13:56 - loss: 0.6947 - acc: 0.5110
3968/9333 [===========>..................] - ETA: 13:46 - loss: 0.6949 - acc: 0.5106
4032/9333 [===========>..................] - ETA: 13:36 - loss: 0.6945 - acc: 0.5117
4096/9333 [============>.................] - ETA: 13:27 - loss: 0.6946 - acc: 0.5112
4160/9333 [============>.................] - ETA: 13:17 - loss: 0.6946 - acc: 0.5101
4224/9333 [============>.................] - ETA: 13:06 - loss: 0.6947 - acc: 0.5092
4288/9333 [============>.................] - ETA: 12:55 - loss: 0.6947 - acc: 0.5089
4352/9333 [============>.................] - ETA: 12:45 - loss: 0.6950 - acc: 0.5076
4416/9333 [=============>................] - ETA: 12:35 - loss: 0.6951 - acc: 0.5082
4480/9333 [=============>................] - ETA: 12:25 - loss: 0.6948 - acc: 0.5087
4544/9333 [=============>................] - ETA: 12:15 - loss: 0.6944 - acc: 0.5099
4608/9333 [=============>................] - ETA: 12:05 - loss: 0.6939 - acc: 0.5122
4672/9333 [==============>...............] - ETA: 11:55 - loss: 0.6938 - acc: 0.5124
4736/9333 [==============>...............] - ETA: 11:45 - loss: 0.6938 - acc: 0.5129
4800/9333 [==============>...............] - ETA: 11:36 - loss: 0.6933 - acc: 0.5135
4864/9333 [==============>...............] - ETA: 11:26 - loss: 0.6931 - acc: 0.5142
4928/9333 [==============>...............] - ETA: 11:15 - loss: 0.6931 - acc: 0.5144
4992/9333 [===============>..............] - ETA: 11:05 - loss: 0.6928 - acc: 0.5164
5056/9333 [===============>..............] - ETA: 10:55 - loss: 0.6931 - acc: 0.5160
5120/9333 [===============>..............] - ETA: 10:45 - loss: 0.6932 - acc: 0.5162
5184/9333 [===============>..............] - ETA: 10:34 - loss: 0.6935 - acc: 0.5150
5248/9333 [===============>..............] - ETA: 10:23 - loss: 0.6938 - acc: 0.5145
5312/9333 [================>.............] - ETA: 10:14 - loss: 0.6940 - acc: 0.5139
5376/9333 [================>.............] - ETA: 10:03 - loss: 0.6939 - acc: 0.5138
5440/9333 [================>.............] - ETA: 9:54 - loss: 0.6940 - acc: 0.5138 
5504/9333 [================>.............] - ETA: 9:44 - loss: 0.6942 - acc: 0.5133
5568/9333 [================>.............] - ETA: 9:34 - loss: 0.6947 - acc: 0.5120
5632/9333 [=================>............] - ETA: 9:23 - loss: 0.6944 - acc: 0.5130
5696/9333 [=================>............] - ETA: 9:13 - loss: 0.6945 - acc: 0.5128
5760/9333 [=================>............] - ETA: 9:03 - loss: 0.6942 - acc: 0.5141
5824/9333 [=================>............] - ETA: 8:53 - loss: 0.6944 - acc: 0.5134
5888/9333 [=================>............] - ETA: 8:44 - loss: 0.6944 - acc: 0.5143
5952/9333 [==================>...........] - ETA: 8:34 - loss: 0.6942 - acc: 0.5151
6016/9333 [==================>...........] - ETA: 8:24 - loss: 0.6942 - acc: 0.5151
6080/9333 [==================>...........] - ETA: 8:14 - loss: 0.6944 - acc: 0.5145
6144/9333 [==================>...........] - ETA: 8:03 - loss: 0.6942 - acc: 0.5148
6208/9333 [==================>...........] - ETA: 7:54 - loss: 0.6940 - acc: 0.5153
6272/9333 [===================>..........] - ETA: 7:44 - loss: 0.6938 - acc: 0.5158
6336/9333 [===================>..........] - ETA: 7:34 - loss: 0.6938 - acc: 0.5158
6400/9333 [===================>..........] - ETA: 7:24 - loss: 0.6939 - acc: 0.5155
6464/9333 [===================>..........] - ETA: 7:15 - loss: 0.6940 - acc: 0.5155
6528/9333 [===================>..........] - ETA: 7:05 - loss: 0.6938 - acc: 0.5169
6592/9333 [====================>.........] - ETA: 6:55 - loss: 0.6937 - acc: 0.5170
6656/9333 [====================>.........] - ETA: 6:46 - loss: 0.6939 - acc: 0.5167
6720/9333 [====================>.........] - ETA: 6:36 - loss: 0.6940 - acc: 0.5162
6784/9333 [====================>.........] - ETA: 6:26 - loss: 0.6943 - acc: 0.5150
6848/9333 [=====================>........] - ETA: 6:16 - loss: 0.6940 - acc: 0.5162
6912/9333 [=====================>........] - ETA: 6:06 - loss: 0.6940 - acc: 0.5162
6976/9333 [=====================>........] - ETA: 5:57 - loss: 0.6937 - acc: 0.5172
7040/9333 [=====================>........] - ETA: 5:47 - loss: 0.6936 - acc: 0.5176
7104/9333 [=====================>........] - ETA: 5:37 - loss: 0.6937 - acc: 0.5176
7168/9333 [======================>.......] - ETA: 5:27 - loss: 0.6935 - acc: 0.5177
7232/9333 [======================>.......] - ETA: 5:17 - loss: 0.6938 - acc: 0.5167
7296/9333 [======================>.......] - ETA: 5:08 - loss: 0.6937 - acc: 0.5173
7360/9333 [======================>.......] - ETA: 4:58 - loss: 0.6937 - acc: 0.5174
7424/9333 [======================>.......] - ETA: 4:48 - loss: 0.6936 - acc: 0.5178
7488/9333 [=======================>......] - ETA: 4:39 - loss: 0.6936 - acc: 0.5179
7552/9333 [=======================>......] - ETA: 4:29 - loss: 0.6937 - acc: 0.5179
7616/9333 [=======================>......] - ETA: 4:19 - loss: 0.6938 - acc: 0.5175
7680/9333 [=======================>......] - ETA: 4:09 - loss: 0.6938 - acc: 0.5180
7744/9333 [=======================>......] - ETA: 4:00 - loss: 0.6939 - acc: 0.5170
7808/9333 [========================>.....] - ETA: 3:50 - loss: 0.6938 - acc: 0.5173
7872/9333 [========================>.....] - ETA: 3:40 - loss: 0.6938 - acc: 0.5177
7936/9333 [========================>.....] - ETA: 3:31 - loss: 0.6939 - acc: 0.5185
8000/9333 [========================>.....] - ETA: 3:21 - loss: 0.6941 - acc: 0.5182
8064/9333 [========================>.....] - ETA: 3:11 - loss: 0.6940 - acc: 0.5185
8128/9333 [=========================>....] - ETA: 3:02 - loss: 0.6939 - acc: 0.5196
8192/9333 [=========================>....] - ETA: 2:52 - loss: 0.6939 - acc: 0.5200
8256/9333 [=========================>....] - ETA: 2:42 - loss: 0.6939 - acc: 0.5200
8320/9333 [=========================>....] - ETA: 2:33 - loss: 0.6940 - acc: 0.5198
8384/9333 [=========================>....] - ETA: 2:23 - loss: 0.6937 - acc: 0.5209
8448/9333 [==========================>...] - ETA: 2:13 - loss: 0.6937 - acc: 0.5208
8512/9333 [==========================>...] - ETA: 2:03 - loss: 0.6938 - acc: 0.5209
8576/9333 [==========================>...] - ETA: 1:54 - loss: 0.6938 - acc: 0.5212
8640/9333 [==========================>...] - ETA: 1:44 - loss: 0.6938 - acc: 0.5208
8704/9333 [==========================>...] - ETA: 1:34 - loss: 0.6938 - acc: 0.5206
8768/9333 [===========================>..] - ETA: 1:25 - loss: 0.6940 - acc: 0.5204
8832/9333 [===========================>..] - ETA: 1:15 - loss: 0.6940 - acc: 0.5207
8896/9333 [===========================>..] - ETA: 1:05 - loss: 0.6941 - acc: 0.5205
8960/9333 [===========================>..] - ETA: 56s - loss: 0.6940 - acc: 0.5209 
9024/9333 [============================>.] - ETA: 46s - loss: 0.6938 - acc: 0.5216
9088/9333 [============================>.] - ETA: 36s - loss: 0.6938 - acc: 0.5218
9152/9333 [============================>.] - ETA: 27s - loss: 0.6939 - acc: 0.5212
9216/9333 [============================>.] - ETA: 17s - loss: 0.6940 - acc: 0.5216
9280/9333 [============================>.] - ETA: 7s - loss: 0.6939 - acc: 0.5212 
9333/9333 [==============================] - 1458s 156ms/step - loss: 0.6938 - acc: 0.5214 - val_loss: 0.6924 - val_acc: 0.5198

Epoch 00004: val_acc did not improve from 0.53616
Epoch 5/10

  64/9333 [..............................] - ETA: 21:30 - loss: 0.6889 - acc: 0.5469
 128/9333 [..............................] - ETA: 21:18 - loss: 0.6824 - acc: 0.5625
 192/9333 [..............................] - ETA: 22:00 - loss: 0.6873 - acc: 0.5156
 256/9333 [..............................] - ETA: 22:15 - loss: 0.6914 - acc: 0.5039
 320/9333 [>.............................] - ETA: 22:12 - loss: 0.6916 - acc: 0.5062
 384/9333 [>.............................] - ETA: 21:59 - loss: 0.6973 - acc: 0.4896
 448/9333 [>.............................] - ETA: 22:01 - loss: 0.6980 - acc: 0.4911
 512/9333 [>.............................] - ETA: 21:37 - loss: 0.7000 - acc: 0.4785
 576/9333 [>.............................] - ETA: 21:30 - loss: 0.7009 - acc: 0.4792
 640/9333 [=>............................] - ETA: 21:36 - loss: 0.7000 - acc: 0.4797
 704/9333 [=>............................] - ETA: 21:22 - loss: 0.6994 - acc: 0.4844
 768/9333 [=>............................] - ETA: 21:20 - loss: 0.6970 - acc: 0.4948
 832/9333 [=>............................] - ETA: 21:05 - loss: 0.6985 - acc: 0.4832
 896/9333 [=>............................] - ETA: 21:09 - loss: 0.6974 - acc: 0.4888
 960/9333 [==>...........................] - ETA: 20:52 - loss: 0.6972 - acc: 0.4906
1024/9333 [==>...........................] - ETA: 20:44 - loss: 0.6976 - acc: 0.4893
1088/9333 [==>...........................] - ETA: 20:33 - loss: 0.6978 - acc: 0.4890
1152/9333 [==>...........................] - ETA: 20:24 - loss: 0.6972 - acc: 0.4939
1216/9333 [==>...........................] - ETA: 20:15 - loss: 0.6961 - acc: 0.5000
1280/9333 [===>..........................] - ETA: 20:02 - loss: 0.6963 - acc: 0.4992
1344/9333 [===>..........................] - ETA: 19:56 - loss: 0.6952 - acc: 0.5052
1408/9333 [===>..........................] - ETA: 19:45 - loss: 0.6943 - acc: 0.5085
1472/9333 [===>..........................] - ETA: 19:32 - loss: 0.6937 - acc: 0.5088
1536/9333 [===>..........................] - ETA: 19:19 - loss: 0.6933 - acc: 0.5072
1600/9333 [====>.........................] - ETA: 19:07 - loss: 0.6931 - acc: 0.5081
1664/9333 [====>.........................] - ETA: 18:58 - loss: 0.6928 - acc: 0.5102
1728/9333 [====>.........................] - ETA: 18:49 - loss: 0.6926 - acc: 0.5098
1792/9333 [====>.........................] - ETA: 18:38 - loss: 0.6933 - acc: 0.5100
1856/9333 [====>.........................] - ETA: 18:29 - loss: 0.6930 - acc: 0.5124
1920/9333 [=====>........................] - ETA: 18:18 - loss: 0.6925 - acc: 0.5135
1984/9333 [=====>........................] - ETA: 18:11 - loss: 0.6933 - acc: 0.5121
2048/9333 [=====>........................] - ETA: 17:59 - loss: 0.6936 - acc: 0.5122
2112/9333 [=====>........................] - ETA: 17:47 - loss: 0.6925 - acc: 0.5166
2176/9333 [=====>........................] - ETA: 17:37 - loss: 0.6924 - acc: 0.5175
2240/9333 [======>.......................] - ETA: 17:25 - loss: 0.6925 - acc: 0.5174
2304/9333 [======>.......................] - ETA: 17:14 - loss: 0.6928 - acc: 0.5174
2368/9333 [======>.......................] - ETA: 17:03 - loss: 0.6933 - acc: 0.5148
2432/9333 [======>.......................] - ETA: 16:52 - loss: 0.6929 - acc: 0.5169
2496/9333 [=======>......................] - ETA: 16:43 - loss: 0.6932 - acc: 0.5148
2560/9333 [=======>......................] - ETA: 16:32 - loss: 0.6930 - acc: 0.5160
2624/9333 [=======>......................] - ETA: 16:22 - loss: 0.6927 - acc: 0.5160
2688/9333 [=======>......................] - ETA: 16:12 - loss: 0.6930 - acc: 0.5160
2752/9333 [=======>......................] - ETA: 16:02 - loss: 0.6931 - acc: 0.5153
2816/9333 [========>.....................] - ETA: 15:52 - loss: 0.6935 - acc: 0.5149
2880/9333 [========>.....................] - ETA: 15:43 - loss: 0.6926 - acc: 0.5188
2944/9333 [========>.....................] - ETA: 15:32 - loss: 0.6923 - acc: 0.5211
3008/9333 [========>.....................] - ETA: 15:23 - loss: 0.6926 - acc: 0.5203
3072/9333 [========>.....................] - ETA: 15:14 - loss: 0.6929 - acc: 0.5189
3136/9333 [=========>....................] - ETA: 15:04 - loss: 0.6932 - acc: 0.5172
3200/9333 [=========>....................] - ETA: 14:54 - loss: 0.6928 - acc: 0.5184
3264/9333 [=========>....................] - ETA: 14:43 - loss: 0.6929 - acc: 0.5172
3328/9333 [=========>....................] - ETA: 14:35 - loss: 0.6930 - acc: 0.5186
3392/9333 [=========>....................] - ETA: 14:25 - loss: 0.6927 - acc: 0.5203
3456/9333 [==========>...................] - ETA: 14:15 - loss: 0.6926 - acc: 0.5214
3520/9333 [==========>...................] - ETA: 14:04 - loss: 0.6927 - acc: 0.5202
3584/9333 [==========>...................] - ETA: 13:53 - loss: 0.6924 - acc: 0.5206
3648/9333 [==========>...................] - ETA: 13:44 - loss: 0.6926 - acc: 0.5200
3712/9333 [==========>...................] - ETA: 13:35 - loss: 0.6930 - acc: 0.5194
3776/9333 [===========>..................] - ETA: 13:25 - loss: 0.6929 - acc: 0.5193
3840/9333 [===========>..................] - ETA: 13:16 - loss: 0.6930 - acc: 0.5177
3904/9333 [===========>..................] - ETA: 13:06 - loss: 0.6929 - acc: 0.5182
3968/9333 [===========>..................] - ETA: 12:57 - loss: 0.6929 - acc: 0.5189
4032/9333 [===========>..................] - ETA: 12:47 - loss: 0.6927 - acc: 0.5196
4096/9333 [============>.................] - ETA: 12:38 - loss: 0.6930 - acc: 0.5193
4160/9333 [============>.................] - ETA: 12:28 - loss: 0.6928 - acc: 0.5204
4224/9333 [============>.................] - ETA: 12:18 - loss: 0.6928 - acc: 0.5208
4288/9333 [============>.................] - ETA: 12:09 - loss: 0.6929 - acc: 0.5212
4352/9333 [============>.................] - ETA: 11:59 - loss: 0.6930 - acc: 0.5209
4416/9333 [=============>................] - ETA: 11:51 - loss: 0.6930 - acc: 0.5213
4480/9333 [=============>................] - ETA: 11:41 - loss: 0.6932 - acc: 0.5205
4544/9333 [=============>................] - ETA: 11:32 - loss: 0.6933 - acc: 0.5200
4608/9333 [=============>................] - ETA: 11:22 - loss: 0.6935 - acc: 0.5195
4672/9333 [==============>...............] - ETA: 11:13 - loss: 0.6931 - acc: 0.5214
4736/9333 [==============>...............] - ETA: 11:04 - loss: 0.6933 - acc: 0.5211
4800/9333 [==============>...............] - ETA: 10:55 - loss: 0.6931 - acc: 0.5229
4864/9333 [==============>...............] - ETA: 10:45 - loss: 0.6928 - acc: 0.5238
4928/9333 [==============>...............] - ETA: 10:35 - loss: 0.6925 - acc: 0.5241
4992/9333 [===============>..............] - ETA: 10:25 - loss: 0.6926 - acc: 0.5238
5056/9333 [===============>..............] - ETA: 10:15 - loss: 0.6926 - acc: 0.5245
5120/9333 [===============>..............] - ETA: 10:05 - loss: 0.6922 - acc: 0.5262
5184/9333 [===============>..............] - ETA: 9:55 - loss: 0.6922 - acc: 0.5262 
5248/9333 [===============>..............] - ETA: 9:45 - loss: 0.6922 - acc: 0.5263
5312/9333 [================>.............] - ETA: 9:36 - loss: 0.6920 - acc: 0.5273
5376/9333 [================>.............] - ETA: 9:27 - loss: 0.6922 - acc: 0.5262
5440/9333 [================>.............] - ETA: 9:17 - loss: 0.6918 - acc: 0.5278
5504/9333 [================>.............] - ETA: 9:07 - loss: 0.6920 - acc: 0.5274
5568/9333 [================>.............] - ETA: 8:58 - loss: 0.6917 - acc: 0.5284
5632/9333 [=================>............] - ETA: 8:49 - loss: 0.6919 - acc: 0.5277
5696/9333 [=================>............] - ETA: 8:40 - loss: 0.6920 - acc: 0.5279
5760/9333 [=================>............] - ETA: 8:30 - loss: 0.6921 - acc: 0.5274
5824/9333 [=================>............] - ETA: 8:21 - loss: 0.6926 - acc: 0.5261
5888/9333 [=================>............] - ETA: 8:12 - loss: 0.6927 - acc: 0.5265
5952/9333 [==================>...........] - ETA: 8:03 - loss: 0.6929 - acc: 0.5265
6016/9333 [==================>...........] - ETA: 7:53 - loss: 0.6930 - acc: 0.5259
6080/9333 [==================>...........] - ETA: 7:44 - loss: 0.6930 - acc: 0.5265
6144/9333 [==================>...........] - ETA: 7:34 - loss: 0.6929 - acc: 0.5273
6208/9333 [==================>...........] - ETA: 7:25 - loss: 0.6927 - acc: 0.5280
6272/9333 [===================>..........] - ETA: 7:16 - loss: 0.6926 - acc: 0.5277
6336/9333 [===================>..........] - ETA: 7:07 - loss: 0.6927 - acc: 0.5276
6400/9333 [===================>..........] - ETA: 6:57 - loss: 0.6926 - acc: 0.5270
6464/9333 [===================>..........] - ETA: 6:48 - loss: 0.6926 - acc: 0.5265
6528/9333 [===================>..........] - ETA: 6:38 - loss: 0.6926 - acc: 0.5273
6592/9333 [====================>.........] - ETA: 6:29 - loss: 0.6924 - acc: 0.5279
6656/9333 [====================>.........] - ETA: 6:20 - loss: 0.6922 - acc: 0.5276
6720/9333 [====================>.........] - ETA: 6:10 - loss: 0.6920 - acc: 0.5287
6784/9333 [====================>.........] - ETA: 6:01 - loss: 0.6919 - acc: 0.5282
6848/9333 [=====================>........] - ETA: 5:52 - loss: 0.6918 - acc: 0.5286
6912/9333 [=====================>........] - ETA: 5:43 - loss: 0.6916 - acc: 0.5295
6976/9333 [=====================>........] - ETA: 5:33 - loss: 0.6916 - acc: 0.5288
7040/9333 [=====================>........] - ETA: 5:24 - loss: 0.6916 - acc: 0.5287
7104/9333 [=====================>........] - ETA: 5:15 - loss: 0.6916 - acc: 0.5294
7168/9333 [======================>.......] - ETA: 5:06 - loss: 0.6915 - acc: 0.5301
7232/9333 [======================>.......] - ETA: 4:57 - loss: 0.6914 - acc: 0.5301
7296/9333 [======================>.......] - ETA: 4:48 - loss: 0.6914 - acc: 0.5296
7360/9333 [======================>.......] - ETA: 4:39 - loss: 0.6916 - acc: 0.5291
7424/9333 [======================>.......] - ETA: 4:29 - loss: 0.6915 - acc: 0.5292
7488/9333 [=======================>......] - ETA: 4:20 - loss: 0.6918 - acc: 0.5284
7552/9333 [=======================>......] - ETA: 4:11 - loss: 0.6919 - acc: 0.5281
7616/9333 [=======================>......] - ETA: 4:02 - loss: 0.6920 - acc: 0.5278
7680/9333 [=======================>......] - ETA: 3:53 - loss: 0.6920 - acc: 0.5275
7744/9333 [=======================>......] - ETA: 3:44 - loss: 0.6917 - acc: 0.5284
7808/9333 [========================>.....] - ETA: 3:35 - loss: 0.6917 - acc: 0.5283
7872/9333 [========================>.....] - ETA: 3:25 - loss: 0.6915 - acc: 0.5297
7936/9333 [========================>.....] - ETA: 3:16 - loss: 0.6915 - acc: 0.5295
8000/9333 [========================>.....] - ETA: 3:07 - loss: 0.6913 - acc: 0.5298
8064/9333 [========================>.....] - ETA: 2:58 - loss: 0.6912 - acc: 0.5304
8128/9333 [=========================>....] - ETA: 2:49 - loss: 0.6913 - acc: 0.5301
8192/9333 [=========================>....] - ETA: 2:40 - loss: 0.6913 - acc: 0.5305
8256/9333 [=========================>....] - ETA: 2:31 - loss: 0.6912 - acc: 0.5309
8320/9333 [=========================>....] - ETA: 2:22 - loss: 0.6911 - acc: 0.5312
8384/9333 [=========================>....] - ETA: 2:13 - loss: 0.6911 - acc: 0.5311
8448/9333 [==========================>...] - ETA: 2:04 - loss: 0.6911 - acc: 0.5312
8512/9333 [==========================>...] - ETA: 1:54 - loss: 0.6914 - acc: 0.5305
8576/9333 [==========================>...] - ETA: 1:46 - loss: 0.6916 - acc: 0.5301
8640/9333 [==========================>...] - ETA: 1:36 - loss: 0.6916 - acc: 0.5301
8704/9333 [==========================>...] - ETA: 1:27 - loss: 0.6915 - acc: 0.5308
8768/9333 [===========================>..] - ETA: 1:19 - loss: 0.6917 - acc: 0.5305
8832/9333 [===========================>..] - ETA: 1:10 - loss: 0.6916 - acc: 0.5305
8896/9333 [===========================>..] - ETA: 1:01 - loss: 0.6916 - acc: 0.5302
8960/9333 [===========================>..] - ETA: 52s - loss: 0.6915 - acc: 0.5307 
9024/9333 [============================>.] - ETA: 43s - loss: 0.6915 - acc: 0.5307
9088/9333 [============================>.] - ETA: 34s - loss: 0.6918 - acc: 0.5300
9152/9333 [============================>.] - ETA: 25s - loss: 0.6917 - acc: 0.5306
9216/9333 [============================>.] - ETA: 16s - loss: 0.6917 - acc: 0.5304
9280/9333 [============================>.] - ETA: 7s - loss: 0.6916 - acc: 0.5308 
9333/9333 [==============================] - 1348s 144ms/step - loss: 0.6915 - acc: 0.5317 - val_loss: 0.6912 - val_acc: 0.5140

Epoch 00005: val_acc did not improve from 0.53616
Epoch 6/10

  64/9333 [..............................] - ETA: 26:00 - loss: 0.6813 - acc: 0.5625
 128/9333 [..............................] - ETA: 23:24 - loss: 0.6849 - acc: 0.5391
 192/9333 [..............................] - ETA: 22:58 - loss: 0.6873 - acc: 0.5312
 256/9333 [..............................] - ETA: 22:20 - loss: 0.6885 - acc: 0.5312
 320/9333 [>.............................] - ETA: 21:58 - loss: 0.6918 - acc: 0.5125
 384/9333 [>.............................] - ETA: 21:44 - loss: 0.6924 - acc: 0.5078
 448/9333 [>.............................] - ETA: 21:28 - loss: 0.6923 - acc: 0.5290
 512/9333 [>.............................] - ETA: 21:19 - loss: 0.6910 - acc: 0.5410
 576/9333 [>.............................] - ETA: 21:05 - loss: 0.6921 - acc: 0.5347
 640/9333 [=>............................] - ETA: 20:49 - loss: 0.6922 - acc: 0.5359
 704/9333 [=>............................] - ETA: 20:37 - loss: 0.6916 - acc: 0.5327
 768/9333 [=>............................] - ETA: 20:34 - loss: 0.6916 - acc: 0.5312
 832/9333 [=>............................] - ETA: 20:23 - loss: 0.6911 - acc: 0.5349
 896/9333 [=>............................] - ETA: 20:12 - loss: 0.6910 - acc: 0.5335
 960/9333 [==>...........................] - ETA: 20:01 - loss: 0.6918 - acc: 0.5302
1024/9333 [==>...........................] - ETA: 19:48 - loss: 0.6927 - acc: 0.5264
1088/9333 [==>...........................] - ETA: 19:39 - loss: 0.6923 - acc: 0.5312
1152/9333 [==>...........................] - ETA: 19:27 - loss: 0.6927 - acc: 0.5295
1216/9333 [==>...........................] - ETA: 19:20 - loss: 0.6919 - acc: 0.5312
1280/9333 [===>..........................] - ETA: 19:12 - loss: 0.6914 - acc: 0.5328
1344/9333 [===>..........................] - ETA: 19:00 - loss: 0.6910 - acc: 0.5335
1408/9333 [===>..........................] - ETA: 18:54 - loss: 0.6906 - acc: 0.5341
1472/9333 [===>..........................] - ETA: 18:46 - loss: 0.6904 - acc: 0.5346
1536/9333 [===>..........................] - ETA: 18:36 - loss: 0.6901 - acc: 0.5352
1600/9333 [====>.........................] - ETA: 18:27 - loss: 0.6896 - acc: 0.5356
1664/9333 [====>.........................] - ETA: 18:21 - loss: 0.6898 - acc: 0.5373
1728/9333 [====>.........................] - ETA: 18:14 - loss: 0.6900 - acc: 0.5359
1792/9333 [====>.........................] - ETA: 18:04 - loss: 0.6891 - acc: 0.5391
1856/9333 [====>.........................] - ETA: 17:56 - loss: 0.6894 - acc: 0.5366
1920/9333 [=====>........................] - ETA: 17:46 - loss: 0.6897 - acc: 0.5344
1984/9333 [=====>........................] - ETA: 17:40 - loss: 0.6899 - acc: 0.5388
2048/9333 [=====>........................] - ETA: 17:30 - loss: 0.6892 - acc: 0.5396
2112/9333 [=====>........................] - ETA: 17:20 - loss: 0.6897 - acc: 0.5379
2176/9333 [=====>........................] - ETA: 17:09 - loss: 0.6893 - acc: 0.5386
2240/9333 [======>.......................] - ETA: 17:01 - loss: 0.6887 - acc: 0.5406
2304/9333 [======>.......................] - ETA: 16:52 - loss: 0.6894 - acc: 0.5391
2368/9333 [======>.......................] - ETA: 16:43 - loss: 0.6898 - acc: 0.5380
2432/9333 [======>.......................] - ETA: 16:34 - loss: 0.6903 - acc: 0.5370
2496/9333 [=======>......................] - ETA: 16:26 - loss: 0.6900 - acc: 0.5357
2560/9333 [=======>......................] - ETA: 16:16 - loss: 0.6897 - acc: 0.5391
2624/9333 [=======>......................] - ETA: 16:08 - loss: 0.6886 - acc: 0.5415
2688/9333 [=======>......................] - ETA: 15:58 - loss: 0.6891 - acc: 0.5394
2752/9333 [=======>......................] - ETA: 15:49 - loss: 0.6893 - acc: 0.5389
2816/9333 [========>.....................] - ETA: 15:39 - loss: 0.6892 - acc: 0.5384
2880/9333 [========>.....................] - ETA: 15:30 - loss: 0.6891 - acc: 0.5382
2944/9333 [========>.....................] - ETA: 15:20 - loss: 0.6893 - acc: 0.5370
3008/9333 [========>.....................] - ETA: 15:10 - loss: 0.6895 - acc: 0.5366
3072/9333 [========>.....................] - ETA: 15:00 - loss: 0.6897 - acc: 0.5365
3136/9333 [=========>....................] - ETA: 14:51 - loss: 0.6895 - acc: 0.5373
3200/9333 [=========>....................] - ETA: 14:42 - loss: 0.6898 - acc: 0.5363
3264/9333 [=========>....................] - ETA: 14:33 - loss: 0.6896 - acc: 0.5368
3328/9333 [=========>....................] - ETA: 14:23 - loss: 0.6892 - acc: 0.5385
3392/9333 [=========>....................] - ETA: 14:15 - loss: 0.6891 - acc: 0.5389
3456/9333 [==========>...................] - ETA: 14:06 - loss: 0.6890 - acc: 0.5394
3520/9333 [==========>...................] - ETA: 13:58 - loss: 0.6890 - acc: 0.5398
3584/9333 [==========>...................] - ETA: 13:48 - loss: 0.6889 - acc: 0.5396
3648/9333 [==========>...................] - ETA: 13:39 - loss: 0.6891 - acc: 0.5395
3712/9333 [==========>...................] - ETA: 13:30 - loss: 0.6893 - acc: 0.5391
3776/9333 [===========>..................] - ETA: 13:20 - loss: 0.6893 - acc: 0.5387
3840/9333 [===========>..................] - ETA: 13:10 - loss: 0.6892 - acc: 0.5393
3904/9333 [===========>..................] - ETA: 13:01 - loss: 0.6897 - acc: 0.5371
3968/9333 [===========>..................] - ETA: 12:52 - loss: 0.6898 - acc: 0.5373
4032/9333 [===========>..................] - ETA: 12:42 - loss: 0.6900 - acc: 0.5372
4096/9333 [============>.................] - ETA: 12:34 - loss: 0.6900 - acc: 0.5356
4160/9333 [============>.................] - ETA: 12:24 - loss: 0.6900 - acc: 0.5346
4224/9333 [============>.................] - ETA: 12:16 - loss: 0.6899 - acc: 0.5348
4288/9333 [============>.................] - ETA: 12:06 - loss: 0.6901 - acc: 0.5340
4352/9333 [============>.................] - ETA: 11:57 - loss: 0.6902 - acc: 0.5338
4416/9333 [=============>................] - ETA: 11:48 - loss: 0.6903 - acc: 0.5333
4480/9333 [=============>................] - ETA: 11:39 - loss: 0.6903 - acc: 0.5328
4544/9333 [=============>................] - ETA: 11:31 - loss: 0.6904 - acc: 0.5330
4608/9333 [=============>................] - ETA: 11:22 - loss: 0.6905 - acc: 0.5328
4672/9333 [==============>...............] - ETA: 11:13 - loss: 0.6905 - acc: 0.5330
4736/9333 [==============>...............] - ETA: 11:03 - loss: 0.6902 - acc: 0.5336
4800/9333 [==============>...............] - ETA: 10:54 - loss: 0.6903 - acc: 0.5333
4864/9333 [==============>...............] - ETA: 10:45 - loss: 0.6904 - acc: 0.5323
4928/9333 [==============>...............] - ETA: 10:36 - loss: 0.6904 - acc: 0.5321
4992/9333 [===============>..............] - ETA: 10:26 - loss: 0.6905 - acc: 0.5317
5056/9333 [===============>..............] - ETA: 10:17 - loss: 0.6903 - acc: 0.5328
5120/9333 [===============>..............] - ETA: 10:08 - loss: 0.6902 - acc: 0.5326
5184/9333 [===============>..............] - ETA: 9:58 - loss: 0.6900 - acc: 0.5326 
5248/9333 [===============>..............] - ETA: 9:50 - loss: 0.6900 - acc: 0.5328
5312/9333 [================>.............] - ETA: 9:40 - loss: 0.6902 - acc: 0.5320
5376/9333 [================>.............] - ETA: 9:31 - loss: 0.6902 - acc: 0.5314
5440/9333 [================>.............] - ETA: 9:22 - loss: 0.6903 - acc: 0.5320
5504/9333 [================>.............] - ETA: 9:13 - loss: 0.6905 - acc: 0.5312
5568/9333 [================>.............] - ETA: 9:04 - loss: 0.6905 - acc: 0.5311
5632/9333 [=================>............] - ETA: 8:55 - loss: 0.6904 - acc: 0.5314
5696/9333 [=================>............] - ETA: 8:46 - loss: 0.6903 - acc: 0.5314
5760/9333 [=================>............] - ETA: 8:36 - loss: 0.6901 - acc: 0.5326
5824/9333 [=================>............] - ETA: 8:27 - loss: 0.6899 - acc: 0.5335
5888/9333 [=================>............] - ETA: 8:18 - loss: 0.6902 - acc: 0.5329
5952/9333 [==================>...........] - ETA: 8:08 - loss: 0.6902 - acc: 0.5323
6016/9333 [==================>...........] - ETA: 8:00 - loss: 0.6903 - acc: 0.5317
6080/9333 [==================>...........] - ETA: 7:50 - loss: 0.6904 - acc: 0.5308
6144/9333 [==================>...........] - ETA: 7:41 - loss: 0.6902 - acc: 0.5314
6208/9333 [==================>...........] - ETA: 7:32 - loss: 0.6903 - acc: 0.5314
6272/9333 [===================>..........] - ETA: 7:23 - loss: 0.6906 - acc: 0.5301
6336/9333 [===================>..........] - ETA: 7:13 - loss: 0.6906 - acc: 0.5300
6400/9333 [===================>..........] - ETA: 7:04 - loss: 0.6906 - acc: 0.5298
6464/9333 [===================>..........] - ETA: 6:55 - loss: 0.6906 - acc: 0.5302
6528/9333 [===================>..........] - ETA: 6:46 - loss: 0.6907 - acc: 0.5299
6592/9333 [====================>.........] - ETA: 6:36 - loss: 0.6907 - acc: 0.5300
6656/9333 [====================>.........] - ETA: 6:27 - loss: 0.6905 - acc: 0.5312
6720/9333 [====================>.........] - ETA: 6:18 - loss: 0.6903 - acc: 0.5315
6784/9333 [====================>.........] - ETA: 6:09 - loss: 0.6905 - acc: 0.5315
6848/9333 [=====================>........] - ETA: 6:00 - loss: 0.6904 - acc: 0.5323
6912/9333 [=====================>........] - ETA: 5:51 - loss: 0.6907 - acc: 0.5315
6976/9333 [=====================>........] - ETA: 5:42 - loss: 0.6907 - acc: 0.5314
7040/9333 [=====================>........] - ETA: 5:33 - loss: 0.6905 - acc: 0.5320
7104/9333 [=====================>........] - ETA: 5:23 - loss: 0.6904 - acc: 0.5324
7168/9333 [======================>.......] - ETA: 5:14 - loss: 0.6906 - acc: 0.5317
7232/9333 [======================>.......] - ETA: 5:04 - loss: 0.6907 - acc: 0.5315
7296/9333 [======================>.......] - ETA: 4:55 - loss: 0.6908 - acc: 0.5315
7360/9333 [======================>.......] - ETA: 4:46 - loss: 0.6909 - acc: 0.5310
7424/9333 [======================>.......] - ETA: 4:37 - loss: 0.6909 - acc: 0.5310
7488/9333 [=======================>......] - ETA: 4:27 - loss: 0.6909 - acc: 0.5308
7552/9333 [=======================>......] - ETA: 4:18 - loss: 0.6911 - acc: 0.5306
7616/9333 [=======================>......] - ETA: 4:09 - loss: 0.6910 - acc: 0.5314
7680/9333 [=======================>......] - ETA: 3:59 - loss: 0.6907 - acc: 0.5322
7744/9333 [=======================>......] - ETA: 3:50 - loss: 0.6904 - acc: 0.5336
7808/9333 [========================>.....] - ETA: 3:41 - loss: 0.6903 - acc: 0.5343
7872/9333 [========================>.....] - ETA: 3:31 - loss: 0.6900 - acc: 0.5354
7936/9333 [========================>.....] - ETA: 3:22 - loss: 0.6899 - acc: 0.5360
8000/9333 [========================>.....] - ETA: 3:13 - loss: 0.6900 - acc: 0.5356
8064/9333 [========================>.....] - ETA: 3:04 - loss: 0.6902 - acc: 0.5353
8128/9333 [=========================>....] - ETA: 2:54 - loss: 0.6901 - acc: 0.5354
8192/9333 [=========================>....] - ETA: 2:45 - loss: 0.6900 - acc: 0.5356
8256/9333 [=========================>....] - ETA: 2:36 - loss: 0.6899 - acc: 0.5362
8320/9333 [=========================>....] - ETA: 2:27 - loss: 0.6898 - acc: 0.5369
8384/9333 [=========================>....] - ETA: 2:17 - loss: 0.6897 - acc: 0.5366
8448/9333 [==========================>...] - ETA: 2:08 - loss: 0.6896 - acc: 0.5367
8512/9333 [==========================>...] - ETA: 1:59 - loss: 0.6898 - acc: 0.5367
8576/9333 [==========================>...] - ETA: 1:49 - loss: 0.6896 - acc: 0.5373
8640/9333 [==========================>...] - ETA: 1:40 - loss: 0.6897 - acc: 0.5370
8704/9333 [==========================>...] - ETA: 1:31 - loss: 0.6899 - acc: 0.5358
8768/9333 [===========================>..] - ETA: 1:22 - loss: 0.6900 - acc: 0.5352
8832/9333 [===========================>..] - ETA: 1:12 - loss: 0.6896 - acc: 0.5365
8896/9333 [===========================>..] - ETA: 1:03 - loss: 0.6897 - acc: 0.5364
8960/9333 [===========================>..] - ETA: 54s - loss: 0.6898 - acc: 0.5362 
9024/9333 [============================>.] - ETA: 44s - loss: 0.6898 - acc: 0.5358
9088/9333 [============================>.] - ETA: 35s - loss: 0.6899 - acc: 0.5360
9152/9333 [============================>.] - ETA: 26s - loss: 0.6897 - acc: 0.5368
9216/9333 [============================>.] - ETA: 17s - loss: 0.6896 - acc: 0.5369
9280/9333 [============================>.] - ETA: 7s - loss: 0.6898 - acc: 0.5364 
9333/9333 [==============================] - 1408s 151ms/step - loss: 0.6898 - acc: 0.5366 - val_loss: 0.6977 - val_acc: 0.5246

Epoch 00006: val_acc did not improve from 0.53616
Epoch 7/10

  64/9333 [..............................] - ETA: 26:51 - loss: 0.6849 - acc: 0.5625
 128/9333 [..............................] - ETA: 24:18 - loss: 0.6980 - acc: 0.5078
 192/9333 [..............................] - ETA: 23:44 - loss: 0.6973 - acc: 0.5208
 256/9333 [..............................] - ETA: 23:04 - loss: 0.6959 - acc: 0.5195
 320/9333 [>.............................] - ETA: 22:38 - loss: 0.6926 - acc: 0.5281
 384/9333 [>.............................] - ETA: 22:17 - loss: 0.6961 - acc: 0.5156
 448/9333 [>.............................] - ETA: 22:11 - loss: 0.6940 - acc: 0.5201
 512/9333 [>.............................] - ETA: 21:57 - loss: 0.6944 - acc: 0.5137
 576/9333 [>.............................] - ETA: 21:31 - loss: 0.6937 - acc: 0.5156
 640/9333 [=>............................] - ETA: 21:29 - loss: 0.6954 - acc: 0.5203
 704/9333 [=>............................] - ETA: 21:25 - loss: 0.6945 - acc: 0.5185
 768/9333 [=>............................] - ETA: 21:20 - loss: 0.6930 - acc: 0.5247
 832/9333 [=>............................] - ETA: 21:00 - loss: 0.6918 - acc: 0.5240
 896/9333 [=>............................] - ETA: 20:54 - loss: 0.6906 - acc: 0.5301
 960/9333 [==>...........................] - ETA: 20:43 - loss: 0.6910 - acc: 0.5302
1024/9333 [==>...........................] - ETA: 20:35 - loss: 0.6925 - acc: 0.5244
1088/9333 [==>...........................] - ETA: 20:24 - loss: 0.6915 - acc: 0.5257
1152/9333 [==>...........................] - ETA: 20:10 - loss: 0.6924 - acc: 0.5234
1216/9333 [==>...........................] - ETA: 20:06 - loss: 0.6933 - acc: 0.5206
1280/9333 [===>..........................] - ETA: 19:54 - loss: 0.6934 - acc: 0.5219
1344/9333 [===>..........................] - ETA: 19:45 - loss: 0.6916 - acc: 0.5290
1408/9333 [===>..........................] - ETA: 19:35 - loss: 0.6928 - acc: 0.5277
1472/9333 [===>..........................] - ETA: 19:27 - loss: 0.6922 - acc: 0.5306
1536/9333 [===>..........................] - ETA: 19:17 - loss: 0.6916 - acc: 0.5326
1600/9333 [====>.........................] - ETA: 19:06 - loss: 0.6905 - acc: 0.5363
1664/9333 [====>.........................] - ETA: 18:54 - loss: 0.6897 - acc: 0.5379
1728/9333 [====>.........................] - ETA: 18:44 - loss: 0.6893 - acc: 0.5422
1792/9333 [====>.........................] - ETA: 18:31 - loss: 0.6898 - acc: 0.5396
1856/9333 [====>.........................] - ETA: 18:24 - loss: 0.6913 - acc: 0.5345
1920/9333 [=====>........................] - ETA: 18:13 - loss: 0.6908 - acc: 0.5370
1984/9333 [=====>........................] - ETA: 18:02 - loss: 0.6906 - acc: 0.5343
2048/9333 [=====>........................] - ETA: 17:51 - loss: 0.6912 - acc: 0.5317
2112/9333 [=====>........................] - ETA: 17:42 - loss: 0.6906 - acc: 0.5341
2176/9333 [=====>........................] - ETA: 17:30 - loss: 0.6903 - acc: 0.5345
2240/9333 [======>.......................] - ETA: 17:21 - loss: 0.6900 - acc: 0.5335
2304/9333 [======>.......................] - ETA: 17:08 - loss: 0.6907 - acc: 0.5321
2368/9333 [======>.......................] - ETA: 16:57 - loss: 0.6901 - acc: 0.5338
2432/9333 [======>.......................] - ETA: 16:46 - loss: 0.6898 - acc: 0.5354
2496/9333 [=======>......................] - ETA: 16:34 - loss: 0.6898 - acc: 0.5357
2560/9333 [=======>......................] - ETA: 16:25 - loss: 0.6897 - acc: 0.5363
2624/9333 [=======>......................] - ETA: 16:16 - loss: 0.6895 - acc: 0.5354
2688/9333 [=======>......................] - ETA: 16:05 - loss: 0.6896 - acc: 0.5331
2752/9333 [=======>......................] - ETA: 15:54 - loss: 0.6894 - acc: 0.5334
2816/9333 [========>.....................] - ETA: 15:44 - loss: 0.6901 - acc: 0.5309
2880/9333 [========>.....................] - ETA: 15:35 - loss: 0.6900 - acc: 0.5312
2944/9333 [========>.....................] - ETA: 15:24 - loss: 0.6900 - acc: 0.5309
3008/9333 [========>.....................] - ETA: 15:13 - loss: 0.6890 - acc: 0.5352
3072/9333 [========>.....................] - ETA: 15:05 - loss: 0.6892 - acc: 0.5339
3136/9333 [=========>....................] - ETA: 14:53 - loss: 0.6886 - acc: 0.5373
3200/9333 [=========>....................] - ETA: 14:45 - loss: 0.6888 - acc: 0.5363
3264/9333 [=========>....................] - ETA: 14:37 - loss: 0.6888 - acc: 0.5371
3328/9333 [=========>....................] - ETA: 14:27 - loss: 0.6888 - acc: 0.5373
3392/9333 [=========>....................] - ETA: 14:18 - loss: 0.6891 - acc: 0.5360
3456/9333 [==========>...................] - ETA: 14:09 - loss: 0.6884 - acc: 0.5385
3520/9333 [==========>...................] - ETA: 14:00 - loss: 0.6883 - acc: 0.5378
3584/9333 [==========>...................] - ETA: 13:50 - loss: 0.6883 - acc: 0.5379
3648/9333 [==========>...................] - ETA: 13:42 - loss: 0.6890 - acc: 0.5362
3712/9333 [==========>...................] - ETA: 13:34 - loss: 0.6891 - acc: 0.5358
3776/9333 [===========>..................] - ETA: 13:24 - loss: 0.6890 - acc: 0.5360
3840/9333 [===========>..................] - ETA: 13:16 - loss: 0.6894 - acc: 0.5346
3904/9333 [===========>..................] - ETA: 13:07 - loss: 0.6894 - acc: 0.5341
3968/9333 [===========>..................] - ETA: 12:57 - loss: 0.6892 - acc: 0.5353
4032/9333 [===========>..................] - ETA: 12:48 - loss: 0.6889 - acc: 0.5357
4096/9333 [============>.................] - ETA: 12:39 - loss: 0.6890 - acc: 0.5369
4160/9333 [============>.................] - ETA: 12:30 - loss: 0.6892 - acc: 0.5368
4224/9333 [============>.................] - ETA: 12:21 - loss: 0.6892 - acc: 0.5369
4288/9333 [============>.................] - ETA: 12:12 - loss: 0.6892 - acc: 0.5364
4352/9333 [============>.................] - ETA: 12:03 - loss: 0.6899 - acc: 0.5324
4416/9333 [=============>................] - ETA: 11:54 - loss: 0.6900 - acc: 0.5315
4480/9333 [=============>................] - ETA: 11:45 - loss: 0.6899 - acc: 0.5319
4544/9333 [=============>................] - ETA: 11:35 - loss: 0.6898 - acc: 0.5328
4608/9333 [=============>................] - ETA: 11:26 - loss: 0.6896 - acc: 0.5332
4672/9333 [==============>...............] - ETA: 11:17 - loss: 0.6894 - acc: 0.5345
4736/9333 [==============>...............] - ETA: 11:08 - loss: 0.6897 - acc: 0.5319
4800/9333 [==============>...............] - ETA: 10:58 - loss: 0.6896 - acc: 0.5312
4864/9333 [==============>...............] - ETA: 10:50 - loss: 0.6894 - acc: 0.5312
4928/9333 [==============>...............] - ETA: 10:40 - loss: 0.6895 - acc: 0.5310
4992/9333 [===============>..............] - ETA: 10:30 - loss: 0.6894 - acc: 0.5317
5056/9333 [===============>..............] - ETA: 10:22 - loss: 0.6894 - acc: 0.5309
5120/9333 [===============>..............] - ETA: 10:13 - loss: 0.6893 - acc: 0.5314
5184/9333 [===============>..............] - ETA: 10:03 - loss: 0.6891 - acc: 0.5328
5248/9333 [===============>..............] - ETA: 9:54 - loss: 0.6891 - acc: 0.5330 
5312/9333 [================>.............] - ETA: 9:45 - loss: 0.6889 - acc: 0.5346
5376/9333 [================>.............] - ETA: 9:35 - loss: 0.6888 - acc: 0.5350
5440/9333 [================>.............] - ETA: 9:26 - loss: 0.6887 - acc: 0.5351
5504/9333 [================>.............] - ETA: 9:16 - loss: 0.6888 - acc: 0.5343
5568/9333 [================>.............] - ETA: 9:07 - loss: 0.6889 - acc: 0.5334
5632/9333 [=================>............] - ETA: 8:58 - loss: 0.6891 - acc: 0.5325
5696/9333 [=================>............] - ETA: 8:48 - loss: 0.6890 - acc: 0.5328
5760/9333 [=================>............] - ETA: 8:39 - loss: 0.6889 - acc: 0.5328
5824/9333 [=================>............] - ETA: 8:30 - loss: 0.6890 - acc: 0.5325
5888/9333 [=================>............] - ETA: 8:21 - loss: 0.6892 - acc: 0.5321
5952/9333 [==================>...........] - ETA: 8:11 - loss: 0.6893 - acc: 0.5316
6016/9333 [==================>...........] - ETA: 8:03 - loss: 0.6895 - acc: 0.5306
6080/9333 [==================>...........] - ETA: 7:53 - loss: 0.6893 - acc: 0.5316
6144/9333 [==================>...........] - ETA: 7:44 - loss: 0.6891 - acc: 0.5329
6208/9333 [==================>...........] - ETA: 7:35 - loss: 0.6891 - acc: 0.5329
6272/9333 [===================>..........] - ETA: 7:26 - loss: 0.6891 - acc: 0.5332
6336/9333 [===================>..........] - ETA: 7:16 - loss: 0.6893 - acc: 0.5327
6400/9333 [===================>..........] - ETA: 7:07 - loss: 0.6893 - acc: 0.5322
6464/9333 [===================>..........] - ETA: 6:58 - loss: 0.6894 - acc: 0.5319
6528/9333 [===================>..........] - ETA: 6:48 - loss: 0.6894 - acc: 0.5320
6592/9333 [====================>.........] - ETA: 6:38 - loss: 0.6893 - acc: 0.5326
6656/9333 [====================>.........] - ETA: 6:29 - loss: 0.6891 - acc: 0.5337
6720/9333 [====================>.........] - ETA: 6:20 - loss: 0.6890 - acc: 0.5342
6784/9333 [====================>.........] - ETA: 6:11 - loss: 0.6891 - acc: 0.5339
6848/9333 [=====================>........] - ETA: 6:01 - loss: 0.6888 - acc: 0.5350
6912/9333 [=====================>........] - ETA: 5:52 - loss: 0.6887 - acc: 0.5350
6976/9333 [=====================>........] - ETA: 5:42 - loss: 0.6886 - acc: 0.5353
7040/9333 [=====================>........] - ETA: 5:33 - loss: 0.6885 - acc: 0.5361
7104/9333 [=====================>........] - ETA: 5:23 - loss: 0.6886 - acc: 0.5358
7168/9333 [======================>.......] - ETA: 5:14 - loss: 0.6887 - acc: 0.5357
7232/9333 [======================>.......] - ETA: 5:04 - loss: 0.6886 - acc: 0.5366
7296/9333 [======================>.......] - ETA: 4:55 - loss: 0.6888 - acc: 0.5358
7360/9333 [======================>.......] - ETA: 4:45 - loss: 0.6888 - acc: 0.5353
7424/9333 [======================>.......] - ETA: 4:36 - loss: 0.6886 - acc: 0.5366
7488/9333 [=======================>......] - ETA: 4:27 - loss: 0.6887 - acc: 0.5367
7552/9333 [=======================>......] - ETA: 4:17 - loss: 0.6886 - acc: 0.5372
7616/9333 [=======================>......] - ETA: 4:08 - loss: 0.6886 - acc: 0.5378
7680/9333 [=======================>......] - ETA: 3:59 - loss: 0.6886 - acc: 0.5379
7744/9333 [=======================>......] - ETA: 3:50 - loss: 0.6883 - acc: 0.5391
7808/9333 [========================>.....] - ETA: 3:40 - loss: 0.6881 - acc: 0.5401
7872/9333 [========================>.....] - ETA: 3:31 - loss: 0.6879 - acc: 0.5408
7936/9333 [========================>.....] - ETA: 3:22 - loss: 0.6878 - acc: 0.5411
8000/9333 [========================>.....] - ETA: 3:12 - loss: 0.6878 - acc: 0.5410
8064/9333 [========================>.....] - ETA: 3:03 - loss: 0.6880 - acc: 0.5407
8128/9333 [=========================>....] - ETA: 2:54 - loss: 0.6879 - acc: 0.5412
8192/9333 [=========================>....] - ETA: 2:44 - loss: 0.6880 - acc: 0.5411
8256/9333 [=========================>....] - ETA: 2:35 - loss: 0.6880 - acc: 0.5409
8320/9333 [=========================>....] - ETA: 2:26 - loss: 0.6878 - acc: 0.5422
8384/9333 [=========================>....] - ETA: 2:17 - loss: 0.6878 - acc: 0.5422
8448/9333 [==========================>...] - ETA: 2:07 - loss: 0.6880 - acc: 0.5421
8512/9333 [==========================>...] - ETA: 1:58 - loss: 0.6884 - acc: 0.5410
8576/9333 [==========================>...] - ETA: 1:49 - loss: 0.6883 - acc: 0.5413
8640/9333 [==========================>...] - ETA: 1:40 - loss: 0.6882 - acc: 0.5417
8704/9333 [==========================>...] - ETA: 1:30 - loss: 0.6881 - acc: 0.5419
8768/9333 [===========================>..] - ETA: 1:21 - loss: 0.6879 - acc: 0.5424
8832/9333 [===========================>..] - ETA: 1:12 - loss: 0.6882 - acc: 0.5416
8896/9333 [===========================>..] - ETA: 1:03 - loss: 0.6881 - acc: 0.5414
8960/9333 [===========================>..] - ETA: 53s - loss: 0.6880 - acc: 0.5415 
9024/9333 [============================>.] - ETA: 44s - loss: 0.6880 - acc: 0.5420
9088/9333 [============================>.] - ETA: 35s - loss: 0.6880 - acc: 0.5421
9152/9333 [============================>.] - ETA: 26s - loss: 0.6882 - acc: 0.5416
9216/9333 [============================>.] - ETA: 16s - loss: 0.6881 - acc: 0.5418
9280/9333 [============================>.] - ETA: 7s - loss: 0.6882 - acc: 0.5417 
9333/9333 [==============================] - 1394s 149ms/step - loss: 0.6882 - acc: 0.5416 - val_loss: 0.6948 - val_acc: 0.5246

Epoch 00007: val_acc did not improve from 0.53616
Epoch 8/10

  64/9333 [..............................] - ETA: 21:35 - loss: 0.6819 - acc: 0.5625
 128/9333 [..............................] - ETA: 20:44 - loss: 0.6942 - acc: 0.5625
 192/9333 [..............................] - ETA: 21:05 - loss: 0.6834 - acc: 0.5885
 256/9333 [..............................] - ETA: 21:00 - loss: 0.6903 - acc: 0.5625
 320/9333 [>.............................] - ETA: 20:50 - loss: 0.6882 - acc: 0.5531
 384/9333 [>.............................] - ETA: 20:40 - loss: 0.6937 - acc: 0.5339
 448/9333 [>.............................] - ETA: 20:31 - loss: 0.6928 - acc: 0.5335
 512/9333 [>.............................] - ETA: 20:28 - loss: 0.6942 - acc: 0.5254
 576/9333 [>.............................] - ETA: 20:20 - loss: 0.6978 - acc: 0.5226
 640/9333 [=>............................] - ETA: 20:12 - loss: 0.6972 - acc: 0.5234
 704/9333 [=>............................] - ETA: 20:00 - loss: 0.6954 - acc: 0.5284
 768/9333 [=>............................] - ETA: 19:44 - loss: 0.6938 - acc: 0.5260
 832/9333 [=>............................] - ETA: 19:36 - loss: 0.6917 - acc: 0.5312
 896/9333 [=>............................] - ETA: 19:30 - loss: 0.6934 - acc: 0.5268
 960/9333 [==>...........................] - ETA: 19:14 - loss: 0.6920 - acc: 0.5292
1024/9333 [==>...........................] - ETA: 19:08 - loss: 0.6922 - acc: 0.5303
1088/9333 [==>...........................] - ETA: 19:01 - loss: 0.6918 - acc: 0.5285
1152/9333 [==>...........................] - ETA: 18:53 - loss: 0.6898 - acc: 0.5356
1216/9333 [==>...........................] - ETA: 18:45 - loss: 0.6906 - acc: 0.5345
1280/9333 [===>..........................] - ETA: 18:37 - loss: 0.6911 - acc: 0.5336
1344/9333 [===>..........................] - ETA: 18:30 - loss: 0.6912 - acc: 0.5312
1408/9333 [===>..........................] - ETA: 18:21 - loss: 0.6904 - acc: 0.5320
1472/9333 [===>..........................] - ETA: 18:10 - loss: 0.6894 - acc: 0.5353
1536/9333 [===>..........................] - ETA: 18:05 - loss: 0.6882 - acc: 0.5397
1600/9333 [====>.........................] - ETA: 17:59 - loss: 0.6885 - acc: 0.5400
1664/9333 [====>.........................] - ETA: 17:50 - loss: 0.6885 - acc: 0.5397
1728/9333 [====>.........................] - ETA: 17:41 - loss: 0.6889 - acc: 0.5394
1792/9333 [====>.........................] - ETA: 17:34 - loss: 0.6888 - acc: 0.5396
1856/9333 [====>.........................] - ETA: 17:24 - loss: 0.6890 - acc: 0.5404
1920/9333 [=====>........................] - ETA: 17:16 - loss: 0.6889 - acc: 0.5406
1984/9333 [=====>........................] - ETA: 17:07 - loss: 0.6896 - acc: 0.5378
2048/9333 [=====>........................] - ETA: 17:02 - loss: 0.6889 - acc: 0.5420
2112/9333 [=====>........................] - ETA: 16:53 - loss: 0.6893 - acc: 0.5412
2176/9333 [=====>........................] - ETA: 16:43 - loss: 0.6885 - acc: 0.5423
2240/9333 [======>.......................] - ETA: 16:33 - loss: 0.6882 - acc: 0.5433
2304/9333 [======>.......................] - ETA: 16:23 - loss: 0.6881 - acc: 0.5430
2368/9333 [======>.......................] - ETA: 16:16 - loss: 0.6876 - acc: 0.5439
2432/9333 [======>.......................] - ETA: 16:06 - loss: 0.6875 - acc: 0.5452
2496/9333 [=======>......................] - ETA: 15:57 - loss: 0.6877 - acc: 0.5433
2560/9333 [=======>......................] - ETA: 15:50 - loss: 0.6877 - acc: 0.5434
2624/9333 [=======>......................] - ETA: 15:41 - loss: 0.6874 - acc: 0.5446
2688/9333 [=======>......................] - ETA: 15:34 - loss: 0.6879 - acc: 0.5417
2752/9333 [=======>......................] - ETA: 15:26 - loss: 0.6879 - acc: 0.5422
2816/9333 [========>.....................] - ETA: 15:16 - loss: 0.6882 - acc: 0.5408
2880/9333 [========>.....................] - ETA: 15:07 - loss: 0.6888 - acc: 0.5389
2944/9333 [========>.....................] - ETA: 14:59 - loss: 0.6894 - acc: 0.5370
3008/9333 [========>.....................] - ETA: 14:50 - loss: 0.6893 - acc: 0.5386
3072/9333 [========>.....................] - ETA: 14:41 - loss: 0.6896 - acc: 0.5381
3136/9333 [=========>....................] - ETA: 14:33 - loss: 0.6893 - acc: 0.5392
3200/9333 [=========>....................] - ETA: 14:22 - loss: 0.6896 - acc: 0.5378
3264/9333 [=========>....................] - ETA: 14:13 - loss: 0.6896 - acc: 0.5383
3328/9333 [=========>....................] - ETA: 14:04 - loss: 0.6893 - acc: 0.5394
3392/9333 [=========>....................] - ETA: 13:54 - loss: 0.6890 - acc: 0.5398
3456/9333 [==========>...................] - ETA: 13:44 - loss: 0.6888 - acc: 0.5396
3520/9333 [==========>...................] - ETA: 13:34 - loss: 0.6888 - acc: 0.5395
3584/9333 [==========>...................] - ETA: 13:25 - loss: 0.6887 - acc: 0.5405
3648/9333 [==========>...................] - ETA: 13:15 - loss: 0.6885 - acc: 0.5400
3712/9333 [==========>...................] - ETA: 13:06 - loss: 0.6882 - acc: 0.5423
3776/9333 [===========>..................] - ETA: 12:58 - loss: 0.6883 - acc: 0.5424
3840/9333 [===========>..................] - ETA: 12:48 - loss: 0.6889 - acc: 0.5404
3904/9333 [===========>..................] - ETA: 12:40 - loss: 0.6889 - acc: 0.5397
3968/9333 [===========>..................] - ETA: 12:31 - loss: 0.6888 - acc: 0.5393
4032/9333 [===========>..................] - ETA: 12:22 - loss: 0.6882 - acc: 0.5414
4096/9333 [============>.................] - ETA: 12:13 - loss: 0.6884 - acc: 0.5413
4160/9333 [============>.................] - ETA: 12:05 - loss: 0.6880 - acc: 0.5423
4224/9333 [============>.................] - ETA: 11:56 - loss: 0.6883 - acc: 0.5421
4288/9333 [============>.................] - ETA: 11:48 - loss: 0.6882 - acc: 0.5434
4352/9333 [============>.................] - ETA: 11:39 - loss: 0.6885 - acc: 0.5430
4416/9333 [=============>................] - ETA: 11:30 - loss: 0.6884 - acc: 0.5430
4480/9333 [=============>................] - ETA: 11:22 - loss: 0.6885 - acc: 0.5431
4544/9333 [=============>................] - ETA: 11:13 - loss: 0.6886 - acc: 0.5429
4608/9333 [=============>................] - ETA: 11:05 - loss: 0.6887 - acc: 0.5428
4672/9333 [==============>...............] - ETA: 10:56 - loss: 0.6885 - acc: 0.5441
4736/9333 [==============>...............] - ETA: 10:46 - loss: 0.6884 - acc: 0.5443
4800/9333 [==============>...............] - ETA: 10:37 - loss: 0.6883 - acc: 0.5444
4864/9333 [==============>...............] - ETA: 10:28 - loss: 0.6883 - acc: 0.5446
4928/9333 [==============>...............] - ETA: 10:20 - loss: 0.6882 - acc: 0.5455
4992/9333 [===============>..............] - ETA: 10:11 - loss: 0.6880 - acc: 0.5459
5056/9333 [===============>..............] - ETA: 10:03 - loss: 0.6881 - acc: 0.5449
5120/9333 [===============>..............] - ETA: 9:54 - loss: 0.6879 - acc: 0.5453 
5184/9333 [===============>..............] - ETA: 9:44 - loss: 0.6881 - acc: 0.5453
5248/9333 [===============>..............] - ETA: 9:35 - loss: 0.6880 - acc: 0.5459
5312/9333 [================>.............] - ETA: 9:27 - loss: 0.6881 - acc: 0.5456
5376/9333 [================>.............] - ETA: 9:18 - loss: 0.6879 - acc: 0.5463
5440/9333 [================>.............] - ETA: 9:09 - loss: 0.6880 - acc: 0.5469
5504/9333 [================>.............] - ETA: 8:59 - loss: 0.6875 - acc: 0.5480
5568/9333 [================>.............] - ETA: 8:50 - loss: 0.6878 - acc: 0.5476
5632/9333 [=================>............] - ETA: 8:41 - loss: 0.6881 - acc: 0.5474
5696/9333 [=================>............] - ETA: 8:31 - loss: 0.6887 - acc: 0.5463
5760/9333 [=================>............] - ETA: 8:22 - loss: 0.6887 - acc: 0.5464
5824/9333 [=================>............] - ETA: 8:13 - loss: 0.6889 - acc: 0.5457
5888/9333 [=================>............] - ETA: 8:04 - loss: 0.6886 - acc: 0.5465
5952/9333 [==================>...........] - ETA: 7:56 - loss: 0.6885 - acc: 0.5467
6016/9333 [==================>...........] - ETA: 7:47 - loss: 0.6888 - acc: 0.5460
6080/9333 [==================>...........] - ETA: 7:37 - loss: 0.6888 - acc: 0.5467
6144/9333 [==================>...........] - ETA: 7:28 - loss: 0.6891 - acc: 0.5461
6208/9333 [==================>...........] - ETA: 7:18 - loss: 0.6891 - acc: 0.5459
6272/9333 [===================>..........] - ETA: 7:09 - loss: 0.6892 - acc: 0.5461
6336/9333 [===================>..........] - ETA: 7:00 - loss: 0.6894 - acc: 0.5458
6400/9333 [===================>..........] - ETA: 6:51 - loss: 0.6894 - acc: 0.5456
6464/9333 [===================>..........] - ETA: 6:42 - loss: 0.6896 - acc: 0.5452
6528/9333 [===================>..........] - ETA: 6:33 - loss: 0.6895 - acc: 0.5452
6592/9333 [====================>.........] - ETA: 6:24 - loss: 0.6895 - acc: 0.5455
6656/9333 [====================>.........] - ETA: 6:15 - loss: 0.6892 - acc: 0.5467
6720/9333 [====================>.........] - ETA: 6:06 - loss: 0.6893 - acc: 0.5452
6784/9333 [====================>.........] - ETA: 5:57 - loss: 0.6895 - acc: 0.5439
6848/9333 [=====================>........] - ETA: 5:48 - loss: 0.6895 - acc: 0.5437
6912/9333 [=====================>........] - ETA: 5:39 - loss: 0.6893 - acc: 0.5444
6976/9333 [=====================>........] - ETA: 5:30 - loss: 0.6891 - acc: 0.5452
7040/9333 [=====================>........] - ETA: 5:21 - loss: 0.6892 - acc: 0.5445
7104/9333 [=====================>........] - ETA: 5:12 - loss: 0.6893 - acc: 0.5435
7168/9333 [======================>.......] - ETA: 5:03 - loss: 0.6892 - acc: 0.5435
7232/9333 [======================>.......] - ETA: 4:54 - loss: 0.6892 - acc: 0.5440
7296/9333 [======================>.......] - ETA: 4:45 - loss: 0.6892 - acc: 0.5437
7360/9333 [======================>.......] - ETA: 4:36 - loss: 0.6890 - acc: 0.5443
7424/9333 [======================>.......] - ETA: 4:27 - loss: 0.6891 - acc: 0.5440
7488/9333 [=======================>......] - ETA: 4:19 - loss: 0.6891 - acc: 0.5441
7552/9333 [=======================>......] - ETA: 4:10 - loss: 0.6891 - acc: 0.5440
7616/9333 [=======================>......] - ETA: 4:01 - loss: 0.6893 - acc: 0.5427
7680/9333 [=======================>......] - ETA: 3:52 - loss: 0.6891 - acc: 0.5431
7744/9333 [=======================>......] - ETA: 3:43 - loss: 0.6893 - acc: 0.5431
7808/9333 [========================>.....] - ETA: 3:34 - loss: 0.6892 - acc: 0.5435
7872/9333 [========================>.....] - ETA: 3:25 - loss: 0.6892 - acc: 0.5440
7936/9333 [========================>.....] - ETA: 3:16 - loss: 0.6892 - acc: 0.5435
8000/9333 [========================>.....] - ETA: 3:07 - loss: 0.6892 - acc: 0.5435
8064/9333 [========================>.....] - ETA: 2:58 - loss: 0.6892 - acc: 0.5434
8128/9333 [=========================>....] - ETA: 2:49 - loss: 0.6892 - acc: 0.5434
8192/9333 [=========================>....] - ETA: 2:40 - loss: 0.6892 - acc: 0.5435
8256/9333 [=========================>....] - ETA: 2:31 - loss: 0.6890 - acc: 0.5442
8320/9333 [=========================>....] - ETA: 2:22 - loss: 0.6890 - acc: 0.5440
8384/9333 [=========================>....] - ETA: 2:13 - loss: 0.6889 - acc: 0.5444
8448/9333 [==========================>...] - ETA: 2:04 - loss: 0.6890 - acc: 0.5439
8512/9333 [==========================>...] - ETA: 1:55 - loss: 0.6890 - acc: 0.5442
8576/9333 [==========================>...] - ETA: 1:46 - loss: 0.6889 - acc: 0.5440
8640/9333 [==========================>...] - ETA: 1:37 - loss: 0.6889 - acc: 0.5442
8704/9333 [==========================>...] - ETA: 1:28 - loss: 0.6888 - acc: 0.5445
8768/9333 [===========================>..] - ETA: 1:19 - loss: 0.6888 - acc: 0.5445
8832/9333 [===========================>..] - ETA: 1:10 - loss: 0.6889 - acc: 0.5443
8896/9333 [===========================>..] - ETA: 1:01 - loss: 0.6887 - acc: 0.5445
8960/9333 [===========================>..] - ETA: 52s - loss: 0.6887 - acc: 0.5445 
9024/9333 [============================>.] - ETA: 43s - loss: 0.6887 - acc: 0.5444
9088/9333 [============================>.] - ETA: 34s - loss: 0.6887 - acc: 0.5449
9152/9333 [============================>.] - ETA: 25s - loss: 0.6887 - acc: 0.5448
9216/9333 [============================>.] - ETA: 16s - loss: 0.6885 - acc: 0.5456
9280/9333 [============================>.] - ETA: 7s - loss: 0.6886 - acc: 0.5452 
9333/9333 [==============================] - 1359s 146ms/step - loss: 0.6886 - acc: 0.5449 - val_loss: 0.6961 - val_acc: 0.5207

Epoch 00008: val_acc did not improve from 0.53616
Epoch 9/10

  64/9333 [..............................] - ETA: 20:46 - loss: 0.6658 - acc: 0.6719
 128/9333 [..............................] - ETA: 20:30 - loss: 0.6809 - acc: 0.5781
 192/9333 [..............................] - ETA: 20:55 - loss: 0.6934 - acc: 0.5052
 256/9333 [..............................] - ETA: 21:04 - loss: 0.6929 - acc: 0.4922
 320/9333 [>.............................] - ETA: 20:51 - loss: 0.6908 - acc: 0.4938
 384/9333 [>.............................] - ETA: 20:30 - loss: 0.6941 - acc: 0.5026
 448/9333 [>.............................] - ETA: 20:30 - loss: 0.6955 - acc: 0.5045
 512/9333 [>.............................] - ETA: 20:25 - loss: 0.6979 - acc: 0.5000
 576/9333 [>.............................] - ETA: 20:21 - loss: 0.6972 - acc: 0.5000
 640/9333 [=>............................] - ETA: 20:15 - loss: 0.6974 - acc: 0.5078
 704/9333 [=>............................] - ETA: 20:05 - loss: 0.6950 - acc: 0.5142
 768/9333 [=>............................] - ETA: 19:59 - loss: 0.6926 - acc: 0.5208
 832/9333 [=>............................] - ETA: 19:46 - loss: 0.6927 - acc: 0.5192
 896/9333 [=>............................] - ETA: 19:40 - loss: 0.6896 - acc: 0.5279
 960/9333 [==>...........................] - ETA: 19:27 - loss: 0.6910 - acc: 0.5250
1024/9333 [==>...........................] - ETA: 19:16 - loss: 0.6909 - acc: 0.5244
1088/9333 [==>...........................] - ETA: 19:02 - loss: 0.6913 - acc: 0.5230
1152/9333 [==>...........................] - ETA: 18:49 - loss: 0.6909 - acc: 0.5252
1216/9333 [==>...........................] - ETA: 18:39 - loss: 0.6905 - acc: 0.5238
1280/9333 [===>..........................] - ETA: 18:32 - loss: 0.6903 - acc: 0.5242
1344/9333 [===>..........................] - ETA: 18:22 - loss: 0.6910 - acc: 0.5246
1408/9333 [===>..........................] - ETA: 18:15 - loss: 0.6910 - acc: 0.5249
1472/9333 [===>..........................] - ETA: 18:04 - loss: 0.6910 - acc: 0.5265
1536/9333 [===>..........................] - ETA: 17:55 - loss: 0.6907 - acc: 0.5280
1600/9333 [====>.........................] - ETA: 17:49 - loss: 0.6910 - acc: 0.5244
1664/9333 [====>.........................] - ETA: 17:40 - loss: 0.6914 - acc: 0.5234
1728/9333 [====>.........................] - ETA: 17:35 - loss: 0.6909 - acc: 0.5249
1792/9333 [====>.........................] - ETA: 17:28 - loss: 0.6909 - acc: 0.5234
1856/9333 [====>.........................] - ETA: 17:22 - loss: 0.6903 - acc: 0.5253
1920/9333 [=====>........................] - ETA: 17:12 - loss: 0.6905 - acc: 0.5260
1984/9333 [=====>........................] - ETA: 17:05 - loss: 0.6904 - acc: 0.5252
2048/9333 [=====>........................] - ETA: 16:57 - loss: 0.6902 - acc: 0.5264
2112/9333 [=====>........................] - ETA: 16:49 - loss: 0.6900 - acc: 0.5260
2176/9333 [=====>........................] - ETA: 16:39 - loss: 0.6894 - acc: 0.5271
2240/9333 [======>.......................] - ETA: 16:28 - loss: 0.6892 - acc: 0.5254
2304/9333 [======>.......................] - ETA: 16:23 - loss: 0.6893 - acc: 0.5252
2368/9333 [======>.......................] - ETA: 16:13 - loss: 0.6884 - acc: 0.5283
2432/9333 [======>.......................] - ETA: 16:06 - loss: 0.6885 - acc: 0.5280
2496/9333 [=======>......................] - ETA: 15:58 - loss: 0.6889 - acc: 0.5264
2560/9333 [=======>......................] - ETA: 15:49 - loss: 0.6893 - acc: 0.5270
2624/9333 [=======>......................] - ETA: 15:39 - loss: 0.6890 - acc: 0.5278
2688/9333 [=======>......................] - ETA: 15:30 - loss: 0.6886 - acc: 0.5283
2752/9333 [=======>......................] - ETA: 15:23 - loss: 0.6890 - acc: 0.5265
2816/9333 [========>.....................] - ETA: 15:13 - loss: 0.6893 - acc: 0.5259
2880/9333 [========>.....................] - ETA: 15:04 - loss: 0.6884 - acc: 0.5295
2944/9333 [========>.....................] - ETA: 14:55 - loss: 0.6886 - acc: 0.5289
3008/9333 [========>.....................] - ETA: 14:46 - loss: 0.6884 - acc: 0.5309
3072/9333 [========>.....................] - ETA: 14:37 - loss: 0.6883 - acc: 0.5319
3136/9333 [=========>....................] - ETA: 14:27 - loss: 0.6882 - acc: 0.5338
3200/9333 [=========>....................] - ETA: 14:18 - loss: 0.6890 - acc: 0.5322
3264/9333 [=========>....................] - ETA: 14:08 - loss: 0.6887 - acc: 0.5340
3328/9333 [=========>....................] - ETA: 13:59 - loss: 0.6881 - acc: 0.5352
3392/9333 [=========>....................] - ETA: 13:48 - loss: 0.6876 - acc: 0.5374
3456/9333 [==========>...................] - ETA: 13:38 - loss: 0.6874 - acc: 0.5376
3520/9333 [==========>...................] - ETA: 13:29 - loss: 0.6872 - acc: 0.5378
3584/9333 [==========>...................] - ETA: 13:20 - loss: 0.6877 - acc: 0.5360
3648/9333 [==========>...................] - ETA: 13:12 - loss: 0.6878 - acc: 0.5365
3712/9333 [==========>...................] - ETA: 13:03 - loss: 0.6880 - acc: 0.5366
3776/9333 [===========>..................] - ETA: 12:54 - loss: 0.6878 - acc: 0.5363
3840/9333 [===========>..................] - ETA: 12:44 - loss: 0.6878 - acc: 0.5362
3904/9333 [===========>..................] - ETA: 12:35 - loss: 0.6878 - acc: 0.5364
3968/9333 [===========>..................] - ETA: 12:27 - loss: 0.6874 - acc: 0.5370
4032/9333 [===========>..................] - ETA: 12:18 - loss: 0.6874 - acc: 0.5377
4096/9333 [============>.................] - ETA: 12:10 - loss: 0.6873 - acc: 0.5386
4160/9333 [============>.................] - ETA: 12:01 - loss: 0.6873 - acc: 0.5387
4224/9333 [============>.................] - ETA: 11:52 - loss: 0.6873 - acc: 0.5400
4288/9333 [============>.................] - ETA: 11:43 - loss: 0.6876 - acc: 0.5399
4352/9333 [============>.................] - ETA: 11:34 - loss: 0.6877 - acc: 0.5393
4416/9333 [=============>................] - ETA: 11:25 - loss: 0.6875 - acc: 0.5399
4480/9333 [=============>................] - ETA: 11:16 - loss: 0.6875 - acc: 0.5391
4544/9333 [=============>................] - ETA: 11:08 - loss: 0.6875 - acc: 0.5385
4608/9333 [=============>................] - ETA: 10:58 - loss: 0.6875 - acc: 0.5386
4672/9333 [==============>...............] - ETA: 10:48 - loss: 0.6877 - acc: 0.5383
4736/9333 [==============>...............] - ETA: 10:39 - loss: 0.6876 - acc: 0.5395
4800/9333 [==============>...............] - ETA: 10:30 - loss: 0.6879 - acc: 0.5381
4864/9333 [==============>...............] - ETA: 10:21 - loss: 0.6878 - acc: 0.5378
4928/9333 [==============>...............] - ETA: 10:13 - loss: 0.6874 - acc: 0.5398
4992/9333 [===============>..............] - ETA: 10:04 - loss: 0.6873 - acc: 0.5409
5056/9333 [===============>..............] - ETA: 9:55 - loss: 0.6874 - acc: 0.5411 
5120/9333 [===============>..............] - ETA: 9:47 - loss: 0.6872 - acc: 0.5420
5184/9333 [===============>..............] - ETA: 9:37 - loss: 0.6875 - acc: 0.5405
5248/9333 [===============>..............] - ETA: 9:30 - loss: 0.6875 - acc: 0.5406
5312/9333 [================>.............] - ETA: 9:21 - loss: 0.6878 - acc: 0.5399
5376/9333 [================>.............] - ETA: 9:13 - loss: 0.6880 - acc: 0.5391
5440/9333 [================>.............] - ETA: 9:04 - loss: 0.6881 - acc: 0.5392
5504/9333 [================>.............] - ETA: 8:55 - loss: 0.6882 - acc: 0.5392
5568/9333 [================>.............] - ETA: 8:46 - loss: 0.6883 - acc: 0.5390
5632/9333 [=================>............] - ETA: 8:37 - loss: 0.6882 - acc: 0.5392
5696/9333 [=================>............] - ETA: 8:29 - loss: 0.6882 - acc: 0.5400
5760/9333 [=================>............] - ETA: 8:20 - loss: 0.6882 - acc: 0.5406
5824/9333 [=================>............] - ETA: 8:11 - loss: 0.6882 - acc: 0.5410
5888/9333 [=================>............] - ETA: 8:02 - loss: 0.6881 - acc: 0.5416
5952/9333 [==================>...........] - ETA: 7:53 - loss: 0.6881 - acc: 0.5418
6016/9333 [==================>...........] - ETA: 7:44 - loss: 0.6879 - acc: 0.5429
6080/9333 [==================>...........] - ETA: 7:35 - loss: 0.6879 - acc: 0.5431
6144/9333 [==================>...........] - ETA: 7:26 - loss: 0.6882 - acc: 0.5423
6208/9333 [==================>...........] - ETA: 7:17 - loss: 0.6882 - acc: 0.5427
6272/9333 [===================>..........] - ETA: 7:08 - loss: 0.6882 - acc: 0.5429
6336/9333 [===================>..........] - ETA: 6:59 - loss: 0.6880 - acc: 0.5440
6400/9333 [===================>..........] - ETA: 6:50 - loss: 0.6878 - acc: 0.5444
6464/9333 [===================>..........] - ETA: 6:41 - loss: 0.6878 - acc: 0.5442
6528/9333 [===================>..........] - ETA: 6:32 - loss: 0.6878 - acc: 0.5441
6592/9333 [====================>.........] - ETA: 6:23 - loss: 0.6878 - acc: 0.5444
6656/9333 [====================>.........] - ETA: 6:14 - loss: 0.6878 - acc: 0.5448
6720/9333 [====================>.........] - ETA: 6:05 - loss: 0.6875 - acc: 0.5457
6784/9333 [====================>.........] - ETA: 5:56 - loss: 0.6875 - acc: 0.5454
6848/9333 [=====================>........] - ETA: 5:47 - loss: 0.6875 - acc: 0.5453
6912/9333 [=====================>........] - ETA: 5:38 - loss: 0.6876 - acc: 0.5451
6976/9333 [=====================>........] - ETA: 5:29 - loss: 0.6876 - acc: 0.5450
7040/9333 [=====================>........] - ETA: 5:20 - loss: 0.6876 - acc: 0.5450
7104/9333 [=====================>........] - ETA: 5:11 - loss: 0.6875 - acc: 0.5449
7168/9333 [======================>.......] - ETA: 5:02 - loss: 0.6876 - acc: 0.5449
7232/9333 [======================>.......] - ETA: 4:53 - loss: 0.6875 - acc: 0.5452
7296/9333 [======================>.......] - ETA: 4:44 - loss: 0.6873 - acc: 0.5458
7360/9333 [======================>.......] - ETA: 4:35 - loss: 0.6874 - acc: 0.5451
7424/9333 [======================>.......] - ETA: 4:26 - loss: 0.6872 - acc: 0.5462
7488/9333 [=======================>......] - ETA: 4:17 - loss: 0.6871 - acc: 0.5463
7552/9333 [=======================>......] - ETA: 4:08 - loss: 0.6872 - acc: 0.5457
7616/9333 [=======================>......] - ETA: 4:00 - loss: 0.6873 - acc: 0.5453
7680/9333 [=======================>......] - ETA: 3:51 - loss: 0.6872 - acc: 0.5458
7744/9333 [=======================>......] - ETA: 3:42 - loss: 0.6872 - acc: 0.5456
7808/9333 [========================>.....] - ETA: 3:33 - loss: 0.6873 - acc: 0.5448
7872/9333 [========================>.....] - ETA: 3:24 - loss: 0.6875 - acc: 0.5450
7936/9333 [========================>.....] - ETA: 3:15 - loss: 0.6875 - acc: 0.5446
8000/9333 [========================>.....] - ETA: 3:06 - loss: 0.6874 - acc: 0.5447
8064/9333 [========================>.....] - ETA: 2:57 - loss: 0.6873 - acc: 0.5445
8128/9333 [=========================>....] - ETA: 2:48 - loss: 0.6871 - acc: 0.5456
8192/9333 [=========================>....] - ETA: 2:39 - loss: 0.6871 - acc: 0.5457
8256/9333 [=========================>....] - ETA: 2:30 - loss: 0.6872 - acc: 0.5448
8320/9333 [=========================>....] - ETA: 2:21 - loss: 0.6873 - acc: 0.5444
8384/9333 [=========================>....] - ETA: 2:12 - loss: 0.6872 - acc: 0.5443
8448/9333 [==========================>...] - ETA: 2:03 - loss: 0.6874 - acc: 0.5437
8512/9333 [==========================>...] - ETA: 1:54 - loss: 0.6874 - acc: 0.5435
8576/9333 [==========================>...] - ETA: 1:45 - loss: 0.6874 - acc: 0.5442
8640/9333 [==========================>...] - ETA: 1:36 - loss: 0.6874 - acc: 0.5442
8704/9333 [==========================>...] - ETA: 1:27 - loss: 0.6874 - acc: 0.5440
8768/9333 [===========================>..] - ETA: 1:19 - loss: 0.6876 - acc: 0.5433
8832/9333 [===========================>..] - ETA: 1:10 - loss: 0.6877 - acc: 0.5431
8896/9333 [===========================>..] - ETA: 1:01 - loss: 0.6876 - acc: 0.5437
8960/9333 [===========================>..] - ETA: 52s - loss: 0.6877 - acc: 0.5434 
9024/9333 [============================>.] - ETA: 43s - loss: 0.6873 - acc: 0.5448
9088/9333 [============================>.] - ETA: 34s - loss: 0.6873 - acc: 0.5445
9152/9333 [============================>.] - ETA: 25s - loss: 0.6874 - acc: 0.5439
9216/9333 [============================>.] - ETA: 16s - loss: 0.6874 - acc: 0.5435
9280/9333 [============================>.] - ETA: 7s - loss: 0.6875 - acc: 0.5435 
9333/9333 [==============================] - 1355s 145ms/step - loss: 0.6875 - acc: 0.5432 - val_loss: 0.6931 - val_acc: 0.5217

Epoch 00009: val_acc did not improve from 0.53616
Epoch 10/10

  64/9333 [..............................] - ETA: 21:37 - loss: 0.6709 - acc: 0.5625
 128/9333 [..............................] - ETA: 21:17 - loss: 0.6839 - acc: 0.5156
 192/9333 [..............................] - ETA: 21:40 - loss: 0.6896 - acc: 0.5000
 256/9333 [..............................] - ETA: 21:14 - loss: 0.6882 - acc: 0.5234
 320/9333 [>.............................] - ETA: 21:38 - loss: 0.6890 - acc: 0.5250
 384/9333 [>.............................] - ETA: 21:31 - loss: 0.6891 - acc: 0.5260
 448/9333 [>.............................] - ETA: 21:30 - loss: 0.6867 - acc: 0.5312
 512/9333 [>.............................] - ETA: 21:13 - loss: 0.6870 - acc: 0.5332
 576/9333 [>.............................] - ETA: 21:18 - loss: 0.6860 - acc: 0.5382
 640/9333 [=>............................] - ETA: 21:11 - loss: 0.6841 - acc: 0.5500
 704/9333 [=>............................] - ETA: 21:00 - loss: 0.6876 - acc: 0.5384
 768/9333 [=>............................] - ETA: 20:54 - loss: 0.6918 - acc: 0.5221
 832/9333 [=>............................] - ETA: 20:45 - loss: 0.6930 - acc: 0.5216
 896/9333 [=>............................] - ETA: 20:35 - loss: 0.6933 - acc: 0.5212
 960/9333 [==>...........................] - ETA: 20:28 - loss: 0.6926 - acc: 0.5219
1024/9333 [==>...........................] - ETA: 20:18 - loss: 0.6927 - acc: 0.5176
1088/9333 [==>...........................] - ETA: 20:04 - loss: 0.6931 - acc: 0.5147
1152/9333 [==>...........................] - ETA: 19:56 - loss: 0.6925 - acc: 0.5165
1216/9333 [==>...........................] - ETA: 19:42 - loss: 0.6920 - acc: 0.5173
1280/9333 [===>..........................] - ETA: 19:39 - loss: 0.6931 - acc: 0.5133
1344/9333 [===>..........................] - ETA: 19:29 - loss: 0.6939 - acc: 0.5112
1408/9333 [===>..........................] - ETA: 19:17 - loss: 0.6931 - acc: 0.5178
1472/9333 [===>..........................] - ETA: 19:09 - loss: 0.6939 - acc: 0.5136
1536/9333 [===>..........................] - ETA: 18:59 - loss: 0.6935 - acc: 0.5150
1600/9333 [====>.........................] - ETA: 18:55 - loss: 0.6928 - acc: 0.5188
1664/9333 [====>.........................] - ETA: 18:45 - loss: 0.6926 - acc: 0.5198
1728/9333 [====>.........................] - ETA: 18:34 - loss: 0.6928 - acc: 0.5179
1792/9333 [====>.........................] - ETA: 18:24 - loss: 0.6919 - acc: 0.5234
1856/9333 [====>.........................] - ETA: 18:13 - loss: 0.6912 - acc: 0.5253
1920/9333 [=====>........................] - ETA: 18:03 - loss: 0.6910 - acc: 0.5260
1984/9333 [=====>........................] - ETA: 17:53 - loss: 0.6909 - acc: 0.5267
2048/9333 [=====>........................] - ETA: 17:44 - loss: 0.6902 - acc: 0.5298
2112/9333 [=====>........................] - ETA: 17:34 - loss: 0.6899 - acc: 0.5312
2176/9333 [=====>........................] - ETA: 17:23 - loss: 0.6901 - acc: 0.5322
2240/9333 [======>.......................] - ETA: 17:13 - loss: 0.6893 - acc: 0.5348
2304/9333 [======>.......................] - ETA: 17:01 - loss: 0.6886 - acc: 0.5373
2368/9333 [======>.......................] - ETA: 16:50 - loss: 0.6887 - acc: 0.5384
2432/9333 [======>.......................] - ETA: 16:41 - loss: 0.6884 - acc: 0.5399
2496/9333 [=======>......................] - ETA: 16:32 - loss: 0.6885 - acc: 0.5401
2560/9333 [=======>......................] - ETA: 16:21 - loss: 0.6893 - acc: 0.5375
2624/9333 [=======>......................] - ETA: 16:10 - loss: 0.6894 - acc: 0.5358
2688/9333 [=======>......................] - ETA: 16:01 - loss: 0.6894 - acc: 0.5368
2752/9333 [=======>......................] - ETA: 15:52 - loss: 0.6893 - acc: 0.5363
2816/9333 [========>.....................] - ETA: 15:43 - loss: 0.6891 - acc: 0.5362
2880/9333 [========>.....................] - ETA: 15:32 - loss: 0.6888 - acc: 0.5361
2944/9333 [========>.....................] - ETA: 15:23 - loss: 0.6890 - acc: 0.5367
3008/9333 [========>.....................] - ETA: 15:13 - loss: 0.6881 - acc: 0.5389
3072/9333 [========>.....................] - ETA: 15:06 - loss: 0.6879 - acc: 0.5400
3136/9333 [=========>....................] - ETA: 14:56 - loss: 0.6879 - acc: 0.5392
3200/9333 [=========>....................] - ETA: 14:46 - loss: 0.6885 - acc: 0.5372
3264/9333 [=========>....................] - ETA: 14:37 - loss: 0.6884 - acc: 0.5374
3328/9333 [=========>....................] - ETA: 14:27 - loss: 0.6886 - acc: 0.5364
3392/9333 [=========>....................] - ETA: 14:18 - loss: 0.6881 - acc: 0.5389
3456/9333 [==========>...................] - ETA: 14:07 - loss: 0.6882 - acc: 0.5391
3520/9333 [==========>...................] - ETA: 13:58 - loss: 0.6884 - acc: 0.5389
3584/9333 [==========>...................] - ETA: 13:49 - loss: 0.6884 - acc: 0.5399
3648/9333 [==========>...................] - ETA: 13:39 - loss: 0.6884 - acc: 0.5406
3712/9333 [==========>...................] - ETA: 13:29 - loss: 0.6886 - acc: 0.5396
3776/9333 [===========>..................] - ETA: 13:20 - loss: 0.6882 - acc: 0.5405
3840/9333 [===========>..................] - ETA: 13:11 - loss: 0.6883 - acc: 0.5406
3904/9333 [===========>..................] - ETA: 13:02 - loss: 0.6881 - acc: 0.5410
3968/9333 [===========>..................] - ETA: 12:53 - loss: 0.6881 - acc: 0.5416
4032/9333 [===========>..................] - ETA: 12:43 - loss: 0.6878 - acc: 0.5424
4096/9333 [============>.................] - ETA: 12:33 - loss: 0.6877 - acc: 0.5432
4160/9333 [============>.................] - ETA: 12:24 - loss: 0.6878 - acc: 0.5428
4224/9333 [============>.................] - ETA: 12:15 - loss: 0.6881 - acc: 0.5419
4288/9333 [============>.................] - ETA: 12:06 - loss: 0.6882 - acc: 0.5408
4352/9333 [============>.................] - ETA: 11:57 - loss: 0.6878 - acc: 0.5418
4416/9333 [=============>................] - ETA: 11:48 - loss: 0.6881 - acc: 0.5412
4480/9333 [=============>................] - ETA: 11:38 - loss: 0.6880 - acc: 0.5413
4544/9333 [=============>................] - ETA: 11:29 - loss: 0.6877 - acc: 0.5414
4608/9333 [=============>................] - ETA: 11:20 - loss: 0.6873 - acc: 0.5419
4672/9333 [==============>...............] - ETA: 11:11 - loss: 0.6874 - acc: 0.5417
4736/9333 [==============>...............] - ETA: 11:02 - loss: 0.6868 - acc: 0.5433
4800/9333 [==============>...............] - ETA: 10:52 - loss: 0.6869 - acc: 0.5427
4864/9333 [==============>...............] - ETA: 10:43 - loss: 0.6871 - acc: 0.5419
4928/9333 [==============>...............] - ETA: 10:34 - loss: 0.6873 - acc: 0.5414
4992/9333 [===============>..............] - ETA: 10:25 - loss: 0.6875 - acc: 0.5419
5056/9333 [===============>..............] - ETA: 10:15 - loss: 0.6877 - acc: 0.5419
5120/9333 [===============>..............] - ETA: 10:06 - loss: 0.6880 - acc: 0.5416
5184/9333 [===============>..............] - ETA: 9:57 - loss: 0.6880 - acc: 0.5430 
5248/9333 [===============>..............] - ETA: 9:48 - loss: 0.6880 - acc: 0.5440
5312/9333 [================>.............] - ETA: 9:39 - loss: 0.6880 - acc: 0.5442
5376/9333 [================>.............] - ETA: 9:30 - loss: 0.6882 - acc: 0.5439
5440/9333 [================>.............] - ETA: 9:21 - loss: 0.6883 - acc: 0.5439
5504/9333 [================>.............] - ETA: 9:11 - loss: 0.6882 - acc: 0.5445
5568/9333 [================>.............] - ETA: 9:02 - loss: 0.6883 - acc: 0.5440
5632/9333 [=================>............] - ETA: 8:53 - loss: 0.6883 - acc: 0.5437
5696/9333 [=================>............] - ETA: 8:44 - loss: 0.6881 - acc: 0.5446
5760/9333 [=================>............] - ETA: 8:35 - loss: 0.6880 - acc: 0.5448
5824/9333 [=================>............] - ETA: 8:26 - loss: 0.6878 - acc: 0.5455
5888/9333 [=================>............] - ETA: 8:17 - loss: 0.6879 - acc: 0.5450
5952/9333 [==================>...........] - ETA: 8:08 - loss: 0.6878 - acc: 0.5452
6016/9333 [==================>...........] - ETA: 7:59 - loss: 0.6879 - acc: 0.5455
6080/9333 [==================>...........] - ETA: 7:50 - loss: 0.6878 - acc: 0.5461
6144/9333 [==================>...........] - ETA: 7:41 - loss: 0.6878 - acc: 0.5467
6208/9333 [==================>...........] - ETA: 7:32 - loss: 0.6876 - acc: 0.5472
6272/9333 [===================>..........] - ETA: 7:22 - loss: 0.6874 - acc: 0.5474
6336/9333 [===================>..........] - ETA: 7:13 - loss: 0.6874 - acc: 0.5472
6400/9333 [===================>..........] - ETA: 7:04 - loss: 0.6875 - acc: 0.5472
6464/9333 [===================>..........] - ETA: 6:55 - loss: 0.6879 - acc: 0.5464
6528/9333 [===================>..........] - ETA: 6:45 - loss: 0.6878 - acc: 0.5463
6592/9333 [====================>.........] - ETA: 6:36 - loss: 0.6877 - acc: 0.5460
6656/9333 [====================>.........] - ETA: 6:27 - loss: 0.6878 - acc: 0.5458
6720/9333 [====================>.........] - ETA: 6:18 - loss: 0.6874 - acc: 0.5475
6784/9333 [====================>.........] - ETA: 6:08 - loss: 0.6876 - acc: 0.5476
6848/9333 [=====================>........] - ETA: 5:59 - loss: 0.6876 - acc: 0.5472
6912/9333 [=====================>........] - ETA: 5:50 - loss: 0.6876 - acc: 0.5473
6976/9333 [=====================>........] - ETA: 5:40 - loss: 0.6875 - acc: 0.5479
7040/9333 [=====================>........] - ETA: 5:31 - loss: 0.6877 - acc: 0.5473
7104/9333 [=====================>........] - ETA: 5:22 - loss: 0.6880 - acc: 0.5467
7168/9333 [======================>.......] - ETA: 5:13 - loss: 0.6879 - acc: 0.5469
7232/9333 [======================>.......] - ETA: 5:03 - loss: 0.6878 - acc: 0.5476
7296/9333 [======================>.......] - ETA: 4:54 - loss: 0.6878 - acc: 0.5469
7360/9333 [======================>.......] - ETA: 4:45 - loss: 0.6876 - acc: 0.5474
7424/9333 [======================>.......] - ETA: 4:36 - loss: 0.6876 - acc: 0.5474
7488/9333 [=======================>......] - ETA: 4:26 - loss: 0.6877 - acc: 0.5475
7552/9333 [=======================>......] - ETA: 4:17 - loss: 0.6878 - acc: 0.5479
7616/9333 [=======================>......] - ETA: 4:08 - loss: 0.6875 - acc: 0.5485
7680/9333 [=======================>......] - ETA: 3:58 - loss: 0.6874 - acc: 0.5484
7744/9333 [=======================>......] - ETA: 3:49 - loss: 0.6872 - acc: 0.5492
7808/9333 [========================>.....] - ETA: 3:40 - loss: 0.6870 - acc: 0.5498
7872/9333 [========================>.....] - ETA: 3:31 - loss: 0.6871 - acc: 0.5494
7936/9333 [========================>.....] - ETA: 3:22 - loss: 0.6872 - acc: 0.5491
8000/9333 [========================>.....] - ETA: 3:12 - loss: 0.6870 - acc: 0.5491
8064/9333 [========================>.....] - ETA: 3:03 - loss: 0.6870 - acc: 0.5491
8128/9333 [=========================>....] - ETA: 2:54 - loss: 0.6871 - acc: 0.5491
8192/9333 [=========================>....] - ETA: 2:45 - loss: 0.6870 - acc: 0.5496
8256/9333 [=========================>....] - ETA: 2:35 - loss: 0.6868 - acc: 0.5498
8320/9333 [=========================>....] - ETA: 2:26 - loss: 0.6870 - acc: 0.5494
8384/9333 [=========================>....] - ETA: 2:17 - loss: 0.6869 - acc: 0.5494
8448/9333 [==========================>...] - ETA: 2:08 - loss: 0.6868 - acc: 0.5491
8512/9333 [==========================>...] - ETA: 1:58 - loss: 0.6866 - acc: 0.5499
8576/9333 [==========================>...] - ETA: 1:49 - loss: 0.6863 - acc: 0.5507
8640/9333 [==========================>...] - ETA: 1:40 - loss: 0.6864 - acc: 0.5507
8704/9333 [==========================>...] - ETA: 1:31 - loss: 0.6864 - acc: 0.5508
8768/9333 [===========================>..] - ETA: 1:21 - loss: 0.6867 - acc: 0.5501
8832/9333 [===========================>..] - ETA: 1:12 - loss: 0.6867 - acc: 0.5498
8896/9333 [===========================>..] - ETA: 1:03 - loss: 0.6868 - acc: 0.5502
8960/9333 [===========================>..] - ETA: 54s - loss: 0.6868 - acc: 0.5506 
9024/9333 [============================>.] - ETA: 44s - loss: 0.6868 - acc: 0.5506
9088/9333 [============================>.] - ETA: 35s - loss: 0.6867 - acc: 0.5507
9152/9333 [============================>.] - ETA: 26s - loss: 0.6865 - acc: 0.5514
9216/9333 [============================>.] - ETA: 16s - loss: 0.6866 - acc: 0.5510
9280/9333 [============================>.] - ETA: 7s - loss: 0.6868 - acc: 0.5503 
9333/9333 [==============================] - 1397s 150ms/step - loss: 0.6867 - acc: 0.5502 - val_loss: 0.6927 - val_acc: 0.5333

Epoch 00010: val_acc did not improve from 0.53616
样本个数 1296
样本个数 2592
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f812ab56d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f812ab56d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f812ab75d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f812ab75d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817ea48250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f817ea48250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f812a3901d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f812a3901d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f812aaa3250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f812aaa3250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81222eee50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81222eee50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f812a390150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f812a390150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81221d5c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81221d5c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8122322810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8122322810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81221a3d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81221a3d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81222f2cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81222f2cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8122322510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8122322510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81222f2c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81222f2c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8121eeac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8121eeac50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8121e75650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8121e75650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8121def4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8121def4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8121ffb1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8121ffb1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f812210cc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f812210cc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f812217ca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f812217ca90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8119b1b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8119b1b8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8121f7ac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8121f7ac90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8121bb8910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8121bb8910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f811997ae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f811997ae50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81199136d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81199136d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8119a88810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8119a88810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81197b7450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81197b7450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f79e872bbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f79e872bbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81198ae410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81198ae410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8119769350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8119769350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8119641610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8119641610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f811958e790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f811958e790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8119769850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8119769850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8119571c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8119571c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81194e3650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81194e3650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f810128e9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f810128e9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f810101a3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f810101a3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81194dbcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81194dbcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100ff3ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100ff3ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8100f79bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8100f79bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8100ff3fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8100ff3fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100e120d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100e120d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8100f79190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8100f79190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100dce050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100dce050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8100c35510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8100c35510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8100c20750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8100c20750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81009efb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81009efb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8100c35c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8100c35c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100b50410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100b50410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8100974510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8100974510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81007ead90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f81007ead90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100880690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100880690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8100b4cb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8100b4cb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81006ee950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f81006ee950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81007cce50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81007cce50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8100444c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8100444c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f810047f710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f810047f710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81007c7210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81007c7210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100397e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8100397e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81003a3ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f81003a3ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f8231b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80f8231b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f827add0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f827add0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81005e58d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f81005e58d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f828a890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80f828a890>>: AttributeError: module 'gast' has no attribute 'Str'
Traceback (most recent call last):
  File "window_select17-21.py", line 272, in <module>
    X_seq_test = [extract_single_features(tokenizer, x['sequence']) for x in sequenceList]
  File "window_select17-21.py", line 272, in <listcomp>
    X_seq_test = [extract_single_features(tokenizer, x['sequence']) for x in sequenceList]
  File "window_select17-21.py", line 186, in extract_single_features
    indices, segments = tokenizer.encode(first=text, max_len=max_len)
  File "/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras_bert/tokenizer.py", line 73, in encode
    first_tokens = self._tokenize(first)
  File "window_select17-21.py", line 176, in _tokenize
    elif self._is_space(tmp):
  File "/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras_bert/tokenizer.py", line 165, in _is_space
    unicodedata.category(ch) == 'Zs'
TypeError: category() argument must be a unicode character, not str
nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 5185
样本个数 10370
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fac250d6d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fac250d6d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fad01643710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fad01643710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad018e6f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad018e6f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad01376550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad01376550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad01273f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad01273f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad012709d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad012709d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad01271190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad01271190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0135aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0135aa90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24ee1c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24ee1c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad0136e890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad0136e890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac25050390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac25050390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad0136da10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad0136da10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24e43110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24e43110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24c12990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24c12990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24e69d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24e69d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24e47cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24e47cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac24c52810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac24c52810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24af3c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24af3c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24946710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac24946710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24b5f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24b5f1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2490f7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2490f7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac2480fed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac2480fed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2488c450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2488c450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac246d9150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac246d9150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24516410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac24516410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac245b75d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac245b75d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac248831d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac248831d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2453d450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac2453d450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac245252d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac245252d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac242dc250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac242dc250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0128fa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0128fa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac24525090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac24525090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24229e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac24229e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac2417b490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac2417b490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac1bf88250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac1bf88250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac241decd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac241decd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac242ae310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac242ae310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1be5e9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1be5e9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac1bd5ba10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac1bd5ba10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac1be64990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac1be64990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1bc0a850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1bc0a850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac1beec410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac1beec410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1bb75510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1bb75510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac1bb6ba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac1bb6ba90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac13821890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac13821890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac138974d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac138974d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac1b933190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac1b933190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1374fd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1374fd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac13623350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac13623350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac247bb150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac247bb150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac13892fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac13892fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac136f94d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac136f94d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1384f290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1384f290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac132ed090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fac132ed090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac13275490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fac13275490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1326b810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1326b810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac135b66d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac135b66d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1320c9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1320c9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fabfb025110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fabfb025110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fabfaf809d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fabfaf809d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fabfb027cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fabfb027cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac132e8310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fac132e8310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fabfaefbf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fabfaefbf50>>: AttributeError: module 'gast' has no attribute 'Str'
2022-12-05 10:28:33.876093: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-12-05 10:28:33.999741: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-12-05 10:28:34.135693: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f0fbc2da50 executing computations on platform Host. Devices:
2022-12-05 10:28:34.135836: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-12-05 10:28:35.020108: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window_select17-21.py:152: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 9333 samples, validate on 1037 samples
Epoch 1/10

  64/9333 [..............................] - ETA: 49:19 - loss: 0.6903 - acc: 0.5781
 128/9333 [..............................] - ETA: 39:12 - loss: 0.7985 - acc: 0.5469
 192/9333 [..............................] - ETA: 32:26 - loss: 0.7856 - acc: 0.5312
 256/9333 [..............................] - ETA: 28:51 - loss: 0.7605 - acc: 0.5430
 320/9333 [>.............................] - ETA: 26:15 - loss: 0.7554 - acc: 0.5281
 384/9333 [>.............................] - ETA: 24:35 - loss: 0.7545 - acc: 0.5156
 448/9333 [>.............................] - ETA: 23:31 - loss: 0.7378 - acc: 0.5379
 512/9333 [>.............................] - ETA: 22:35 - loss: 0.7381 - acc: 0.5312
 576/9333 [>.............................] - ETA: 21:46 - loss: 0.7339 - acc: 0.5312
 640/9333 [=>............................] - ETA: 21:02 - loss: 0.7291 - acc: 0.5406
 704/9333 [=>............................] - ETA: 20:30 - loss: 0.7320 - acc: 0.5284
 768/9333 [=>............................] - ETA: 20:00 - loss: 0.7308 - acc: 0.5365
 832/9333 [=>............................] - ETA: 19:48 - loss: 0.7275 - acc: 0.5409
 896/9333 [=>............................] - ETA: 19:50 - loss: 0.7309 - acc: 0.5335
 960/9333 [==>...........................] - ETA: 19:35 - loss: 0.7249 - acc: 0.5385
1024/9333 [==>...........................] - ETA: 19:17 - loss: 0.7254 - acc: 0.5381
1088/9333 [==>...........................] - ETA: 19:35 - loss: 0.7260 - acc: 0.5331
1152/9333 [==>...........................] - ETA: 19:28 - loss: 0.7249 - acc: 0.5312
1216/9333 [==>...........................] - ETA: 19:10 - loss: 0.7252 - acc: 0.5271
1280/9333 [===>..........................] - ETA: 18:50 - loss: 0.7276 - acc: 0.5188
1344/9333 [===>..........................] - ETA: 18:36 - loss: 0.7293 - acc: 0.5119
1408/9333 [===>..........................] - ETA: 18:20 - loss: 0.7284 - acc: 0.5149
1472/9333 [===>..........................] - ETA: 18:05 - loss: 0.7301 - acc: 0.5095
1536/9333 [===>..........................] - ETA: 17:48 - loss: 0.7301 - acc: 0.5085
1600/9333 [====>.........................] - ETA: 17:32 - loss: 0.7307 - acc: 0.5088
1664/9333 [====>.........................] - ETA: 17:17 - loss: 0.7321 - acc: 0.5042
1728/9333 [====>.........................] - ETA: 17:03 - loss: 0.7323 - acc: 0.5023
1792/9333 [====>.........................] - ETA: 16:47 - loss: 0.7330 - acc: 0.5006
1856/9333 [====>.........................] - ETA: 16:34 - loss: 0.7339 - acc: 0.4989
1920/9333 [=====>........................] - ETA: 16:21 - loss: 0.7338 - acc: 0.5005
1984/9333 [=====>........................] - ETA: 16:10 - loss: 0.7322 - acc: 0.5030
2048/9333 [=====>........................] - ETA: 15:58 - loss: 0.7311 - acc: 0.5020
2112/9333 [=====>........................] - ETA: 15:44 - loss: 0.7285 - acc: 0.5057
2176/9333 [=====>........................] - ETA: 15:32 - loss: 0.7274 - acc: 0.5083
2240/9333 [======>.......................] - ETA: 15:19 - loss: 0.7277 - acc: 0.5062
2304/9333 [======>.......................] - ETA: 15:08 - loss: 0.7281 - acc: 0.5043
2368/9333 [======>.......................] - ETA: 14:55 - loss: 0.7289 - acc: 0.5046
2432/9333 [======>.......................] - ETA: 14:43 - loss: 0.7279 - acc: 0.5037
2496/9333 [=======>......................] - ETA: 14:32 - loss: 0.7265 - acc: 0.5076
2560/9333 [=======>......................] - ETA: 14:20 - loss: 0.7260 - acc: 0.5066
2624/9333 [=======>......................] - ETA: 14:10 - loss: 0.7252 - acc: 0.5072
2688/9333 [=======>......................] - ETA: 13:59 - loss: 0.7252 - acc: 0.5086
2752/9333 [=======>......................] - ETA: 13:51 - loss: 0.7242 - acc: 0.5105
2816/9333 [========>.....................] - ETA: 13:40 - loss: 0.7235 - acc: 0.5117
2880/9333 [========>.....................] - ETA: 13:30 - loss: 0.7236 - acc: 0.5097
2944/9333 [========>.....................] - ETA: 13:19 - loss: 0.7232 - acc: 0.5092
3008/9333 [========>.....................] - ETA: 13:10 - loss: 0.7210 - acc: 0.5126
3072/9333 [========>.....................] - ETA: 13:00 - loss: 0.7206 - acc: 0.5137
3136/9333 [=========>....................] - ETA: 12:52 - loss: 0.7206 - acc: 0.5118
3200/9333 [=========>....................] - ETA: 12:42 - loss: 0.7209 - acc: 0.5106
3264/9333 [=========>....................] - ETA: 12:33 - loss: 0.7204 - acc: 0.5101
3328/9333 [=========>....................] - ETA: 12:24 - loss: 0.7197 - acc: 0.5114
3392/9333 [=========>....................] - ETA: 12:14 - loss: 0.7199 - acc: 0.5115
3456/9333 [==========>...................] - ETA: 12:05 - loss: 0.7201 - acc: 0.5095
3520/9333 [==========>...................] - ETA: 11:55 - loss: 0.7198 - acc: 0.5105
3584/9333 [==========>...................] - ETA: 11:47 - loss: 0.7201 - acc: 0.5089
3648/9333 [==========>...................] - ETA: 11:38 - loss: 0.7199 - acc: 0.5077
3712/9333 [==========>...................] - ETA: 11:28 - loss: 0.7195 - acc: 0.5081
3776/9333 [===========>..................] - ETA: 11:19 - loss: 0.7197 - acc: 0.5074
3840/9333 [===========>..................] - ETA: 11:10 - loss: 0.7188 - acc: 0.5086
3904/9333 [===========>..................] - ETA: 11:01 - loss: 0.7190 - acc: 0.5074
3968/9333 [===========>..................] - ETA: 10:52 - loss: 0.7195 - acc: 0.5058
4032/9333 [===========>..................] - ETA: 10:43 - loss: 0.7197 - acc: 0.5045
4096/9333 [============>.................] - ETA: 10:34 - loss: 0.7193 - acc: 0.5042
4160/9333 [============>.................] - ETA: 10:25 - loss: 0.7186 - acc: 0.5055
4224/9333 [============>.................] - ETA: 10:16 - loss: 0.7177 - acc: 0.5076
4288/9333 [============>.................] - ETA: 10:06 - loss: 0.7178 - acc: 0.5068
4352/9333 [============>.................] - ETA: 9:58 - loss: 0.7177 - acc: 0.5062 
4416/9333 [=============>................] - ETA: 9:49 - loss: 0.7176 - acc: 0.5061
4480/9333 [=============>................] - ETA: 9:41 - loss: 0.7181 - acc: 0.5051
4544/9333 [=============>................] - ETA: 9:32 - loss: 0.7184 - acc: 0.5046
4608/9333 [=============>................] - ETA: 9:24 - loss: 0.7178 - acc: 0.5052
4672/9333 [==============>...............] - ETA: 9:15 - loss: 0.7181 - acc: 0.5051
4736/9333 [==============>...............] - ETA: 9:07 - loss: 0.7181 - acc: 0.5059
4800/9333 [==============>...............] - ETA: 8:59 - loss: 0.7186 - acc: 0.5040
4864/9333 [==============>...............] - ETA: 8:51 - loss: 0.7188 - acc: 0.5037
4928/9333 [==============>...............] - ETA: 8:42 - loss: 0.7186 - acc: 0.5032
4992/9333 [===============>..............] - ETA: 8:34 - loss: 0.7186 - acc: 0.5030
5056/9333 [===============>..............] - ETA: 8:26 - loss: 0.7181 - acc: 0.5036
5120/9333 [===============>..............] - ETA: 8:18 - loss: 0.7181 - acc: 0.5039
5184/9333 [===============>..............] - ETA: 8:10 - loss: 0.7188 - acc: 0.5023
5248/9333 [===============>..............] - ETA: 8:02 - loss: 0.7189 - acc: 0.5015
5312/9333 [================>.............] - ETA: 7:54 - loss: 0.7189 - acc: 0.5017
5376/9333 [================>.............] - ETA: 7:46 - loss: 0.7186 - acc: 0.5020
5440/9333 [================>.............] - ETA: 7:38 - loss: 0.7184 - acc: 0.5011
5504/9333 [================>.............] - ETA: 7:30 - loss: 0.7184 - acc: 0.5002
5568/9333 [================>.............] - ETA: 7:22 - loss: 0.7180 - acc: 0.5004
5632/9333 [=================>............] - ETA: 7:14 - loss: 0.7181 - acc: 0.5004
5696/9333 [=================>............] - ETA: 7:06 - loss: 0.7181 - acc: 0.5002
5760/9333 [=================>............] - ETA: 6:58 - loss: 0.7178 - acc: 0.5005
5824/9333 [=================>............] - ETA: 6:50 - loss: 0.7174 - acc: 0.5010
5888/9333 [=================>............] - ETA: 6:42 - loss: 0.7174 - acc: 0.5007
5952/9333 [==================>...........] - ETA: 6:35 - loss: 0.7173 - acc: 0.5002
6016/9333 [==================>...........] - ETA: 6:27 - loss: 0.7173 - acc: 0.4998
6080/9333 [==================>...........] - ETA: 6:19 - loss: 0.7172 - acc: 0.5003
6144/9333 [==================>...........] - ETA: 6:11 - loss: 0.7169 - acc: 0.5008
6208/9333 [==================>...........] - ETA: 6:03 - loss: 0.7167 - acc: 0.5008
6272/9333 [===================>..........] - ETA: 5:56 - loss: 0.7164 - acc: 0.5014
6336/9333 [===================>..........] - ETA: 5:48 - loss: 0.7161 - acc: 0.5013
6400/9333 [===================>..........] - ETA: 5:40 - loss: 0.7161 - acc: 0.5006
6464/9333 [===================>..........] - ETA: 5:33 - loss: 0.7162 - acc: 0.5002
6528/9333 [===================>..........] - ETA: 5:25 - loss: 0.7159 - acc: 0.5008
6592/9333 [====================>.........] - ETA: 5:18 - loss: 0.7156 - acc: 0.5018
6656/9333 [====================>.........] - ETA: 5:10 - loss: 0.7154 - acc: 0.5023
6720/9333 [====================>.........] - ETA: 5:02 - loss: 0.7153 - acc: 0.5024
6784/9333 [====================>.........] - ETA: 4:55 - loss: 0.7152 - acc: 0.5018
6848/9333 [=====================>........] - ETA: 4:47 - loss: 0.7152 - acc: 0.5010
6912/9333 [=====================>........] - ETA: 4:39 - loss: 0.7155 - acc: 0.5004
6976/9333 [=====================>........] - ETA: 4:32 - loss: 0.7155 - acc: 0.5009
7040/9333 [=====================>........] - ETA: 4:24 - loss: 0.7158 - acc: 0.4999
7104/9333 [=====================>........] - ETA: 4:17 - loss: 0.7156 - acc: 0.5000
7168/9333 [======================>.......] - ETA: 4:09 - loss: 0.7153 - acc: 0.5008
7232/9333 [======================>.......] - ETA: 4:02 - loss: 0.7151 - acc: 0.5012
7296/9333 [======================>.......] - ETA: 3:54 - loss: 0.7149 - acc: 0.5014
7360/9333 [======================>.......] - ETA: 3:46 - loss: 0.7148 - acc: 0.5018
7424/9333 [======================>.......] - ETA: 3:39 - loss: 0.7149 - acc: 0.5022
7488/9333 [=======================>......] - ETA: 3:31 - loss: 0.7144 - acc: 0.5036
7552/9333 [=======================>......] - ETA: 3:24 - loss: 0.7143 - acc: 0.5034
7616/9333 [=======================>......] - ETA: 3:16 - loss: 0.7142 - acc: 0.5034
7680/9333 [=======================>......] - ETA: 3:09 - loss: 0.7142 - acc: 0.5030
7744/9333 [=======================>......] - ETA: 3:02 - loss: 0.7142 - acc: 0.5032
7808/9333 [========================>.....] - ETA: 2:54 - loss: 0.7143 - acc: 0.5028
7872/9333 [========================>.....] - ETA: 2:47 - loss: 0.7144 - acc: 0.5023
7936/9333 [========================>.....] - ETA: 2:39 - loss: 0.7146 - acc: 0.5014
8000/9333 [========================>.....] - ETA: 2:32 - loss: 0.7146 - acc: 0.5015
8064/9333 [========================>.....] - ETA: 2:24 - loss: 0.7142 - acc: 0.5022
8128/9333 [=========================>....] - ETA: 2:17 - loss: 0.7140 - acc: 0.5027
8192/9333 [=========================>....] - ETA: 2:10 - loss: 0.7138 - acc: 0.5032
8256/9333 [=========================>....] - ETA: 2:02 - loss: 0.7138 - acc: 0.5030
8320/9333 [=========================>....] - ETA: 1:55 - loss: 0.7138 - acc: 0.5030
8384/9333 [=========================>....] - ETA: 1:48 - loss: 0.7136 - acc: 0.5030
8448/9333 [==========================>...] - ETA: 1:40 - loss: 0.7136 - acc: 0.5027
8512/9333 [==========================>...] - ETA: 1:33 - loss: 0.7136 - acc: 0.5027
8576/9333 [==========================>...] - ETA: 1:25 - loss: 0.7134 - acc: 0.5028
8640/9333 [==========================>...] - ETA: 1:18 - loss: 0.7135 - acc: 0.5022
8704/9333 [==========================>...] - ETA: 1:11 - loss: 0.7136 - acc: 0.5022
8768/9333 [===========================>..] - ETA: 1:04 - loss: 0.7138 - acc: 0.5015
8832/9333 [===========================>..] - ETA: 56s - loss: 0.7139 - acc: 0.5007 
8896/9333 [===========================>..] - ETA: 49s - loss: 0.7137 - acc: 0.5010
8960/9333 [===========================>..] - ETA: 42s - loss: 0.7134 - acc: 0.5015
9024/9333 [============================>.] - ETA: 34s - loss: 0.7133 - acc: 0.5012
9088/9333 [============================>.] - ETA: 27s - loss: 0.7133 - acc: 0.5007
9152/9333 [============================>.] - ETA: 20s - loss: 0.7131 - acc: 0.5008
9216/9333 [============================>.] - ETA: 13s - loss: 0.7131 - acc: 0.5000
9280/9333 [============================>.] - ETA: 5s - loss: 0.7130 - acc: 0.5000 
9333/9333 [==============================] - 1090s 117ms/step - loss: 0.7128 - acc: 0.5008 - val_loss: 0.6930 - val_acc: 0.5053

Epoch 00001: val_acc improved from -inf to 0.50530, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window21/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 2/10

  64/9333 [..............................] - ETA: 16:37 - loss: 0.6839 - acc: 0.5312
 128/9333 [..............................] - ETA: 16:42 - loss: 0.6897 - acc: 0.5078
 192/9333 [..............................] - ETA: 16:21 - loss: 0.6855 - acc: 0.5521
 256/9333 [..............................] - ETA: 16:23 - loss: 0.6896 - acc: 0.5391
 320/9333 [>.............................] - ETA: 16:23 - loss: 0.6949 - acc: 0.5156
 384/9333 [>.............................] - ETA: 16:14 - loss: 0.6970 - acc: 0.5026
 448/9333 [>.............................] - ETA: 16:09 - loss: 0.6985 - acc: 0.5089
 512/9333 [>.............................] - ETA: 15:57 - loss: 0.6965 - acc: 0.5195
 576/9333 [>.............................] - ETA: 15:55 - loss: 0.6987 - acc: 0.5156
 640/9333 [=>............................] - ETA: 15:44 - loss: 0.6976 - acc: 0.5109
 704/9333 [=>............................] - ETA: 15:35 - loss: 0.6953 - acc: 0.5156
 768/9333 [=>............................] - ETA: 15:22 - loss: 0.6960 - acc: 0.5143
 832/9333 [=>............................] - ETA: 15:16 - loss: 0.6967 - acc: 0.5084
 896/9333 [=>............................] - ETA: 15:05 - loss: 0.6980 - acc: 0.5078
 960/9333 [==>...........................] - ETA: 15:00 - loss: 0.6988 - acc: 0.5042
1024/9333 [==>...........................] - ETA: 14:56 - loss: 0.6984 - acc: 0.5029
1088/9333 [==>...........................] - ETA: 14:47 - loss: 0.6973 - acc: 0.5064
1152/9333 [==>...........................] - ETA: 14:41 - loss: 0.6973 - acc: 0.5043
1216/9333 [==>...........................] - ETA: 14:32 - loss: 0.6967 - acc: 0.5090
1280/9333 [===>..........................] - ETA: 14:23 - loss: 0.6946 - acc: 0.5141
1344/9333 [===>..........................] - ETA: 14:16 - loss: 0.6928 - acc: 0.5193
1408/9333 [===>..........................] - ETA: 14:08 - loss: 0.6930 - acc: 0.5178
1472/9333 [===>..........................] - ETA: 14:03 - loss: 0.6929 - acc: 0.5177
1536/9333 [===>..........................] - ETA: 13:57 - loss: 0.6937 - acc: 0.5150
1600/9333 [====>.........................] - ETA: 13:49 - loss: 0.6934 - acc: 0.5188
1664/9333 [====>.........................] - ETA: 13:41 - loss: 0.6935 - acc: 0.5174
1728/9333 [====>.........................] - ETA: 13:33 - loss: 0.6938 - acc: 0.5168
1792/9333 [====>.........................] - ETA: 13:24 - loss: 0.6939 - acc: 0.5179
1856/9333 [====>.........................] - ETA: 13:18 - loss: 0.6931 - acc: 0.5194
1920/9333 [=====>........................] - ETA: 13:11 - loss: 0.6931 - acc: 0.5203
1984/9333 [=====>........................] - ETA: 13:04 - loss: 0.6940 - acc: 0.5192
2048/9333 [=====>........................] - ETA: 12:57 - loss: 0.6941 - acc: 0.5186
2112/9333 [=====>........................] - ETA: 12:50 - loss: 0.6941 - acc: 0.5185
2176/9333 [=====>........................] - ETA: 12:42 - loss: 0.6936 - acc: 0.5188
2240/9333 [======>.......................] - ETA: 12:36 - loss: 0.6933 - acc: 0.5188
2304/9333 [======>.......................] - ETA: 12:28 - loss: 0.6932 - acc: 0.5195
2368/9333 [======>.......................] - ETA: 12:20 - loss: 0.6932 - acc: 0.5186
2432/9333 [======>.......................] - ETA: 12:14 - loss: 0.6932 - acc: 0.5181
2496/9333 [=======>......................] - ETA: 12:05 - loss: 0.6939 - acc: 0.5164
2560/9333 [=======>......................] - ETA: 11:59 - loss: 0.6947 - acc: 0.5145
2624/9333 [=======>......................] - ETA: 11:51 - loss: 0.6951 - acc: 0.5137
2688/9333 [=======>......................] - ETA: 11:43 - loss: 0.6946 - acc: 0.5167
2752/9333 [=======>......................] - ETA: 11:38 - loss: 0.6949 - acc: 0.5164
2816/9333 [========>.....................] - ETA: 11:31 - loss: 0.6950 - acc: 0.5146
2880/9333 [========>.....................] - ETA: 11:23 - loss: 0.6948 - acc: 0.5153
2944/9333 [========>.....................] - ETA: 11:16 - loss: 0.6953 - acc: 0.5129
3008/9333 [========>.....................] - ETA: 11:10 - loss: 0.6955 - acc: 0.5116
3072/9333 [========>.....................] - ETA: 11:03 - loss: 0.6955 - acc: 0.5114
3136/9333 [=========>....................] - ETA: 10:56 - loss: 0.6956 - acc: 0.5118
3200/9333 [=========>....................] - ETA: 10:49 - loss: 0.6956 - acc: 0.5125
3264/9333 [=========>....................] - ETA: 10:41 - loss: 0.6956 - acc: 0.5123
3328/9333 [=========>....................] - ETA: 10:34 - loss: 0.6958 - acc: 0.5120
3392/9333 [=========>....................] - ETA: 10:27 - loss: 0.6957 - acc: 0.5121
3456/9333 [==========>...................] - ETA: 10:20 - loss: 0.6955 - acc: 0.5133
3520/9333 [==========>...................] - ETA: 10:13 - loss: 0.6958 - acc: 0.5125
3584/9333 [==========>...................] - ETA: 10:06 - loss: 0.6957 - acc: 0.5137
3648/9333 [==========>...................] - ETA: 9:59 - loss: 0.6953 - acc: 0.5156 
3712/9333 [==========>...................] - ETA: 9:53 - loss: 0.6955 - acc: 0.5156
3776/9333 [===========>..................] - ETA: 9:46 - loss: 0.6952 - acc: 0.5154
3840/9333 [===========>..................] - ETA: 9:39 - loss: 0.6953 - acc: 0.5154
3904/9333 [===========>..................] - ETA: 9:32 - loss: 0.6953 - acc: 0.5156
3968/9333 [===========>..................] - ETA: 9:25 - loss: 0.6954 - acc: 0.5159
4032/9333 [===========>..................] - ETA: 9:18 - loss: 0.6953 - acc: 0.5164
4096/9333 [============>.................] - ETA: 9:11 - loss: 0.6957 - acc: 0.5142
4160/9333 [============>.................] - ETA: 9:04 - loss: 0.6957 - acc: 0.5154
4224/9333 [============>.................] - ETA: 8:57 - loss: 0.6958 - acc: 0.5149
4288/9333 [============>.................] - ETA: 8:51 - loss: 0.6961 - acc: 0.5142
4352/9333 [============>.................] - ETA: 8:44 - loss: 0.6959 - acc: 0.5142
4416/9333 [=============>................] - ETA: 8:37 - loss: 0.6957 - acc: 0.5147
4480/9333 [=============>................] - ETA: 8:30 - loss: 0.6954 - acc: 0.5158
4544/9333 [=============>................] - ETA: 8:23 - loss: 0.6951 - acc: 0.5167
4608/9333 [=============>................] - ETA: 8:16 - loss: 0.6951 - acc: 0.5163
4672/9333 [==============>...............] - ETA: 8:09 - loss: 0.6947 - acc: 0.5176
4736/9333 [==============>...............] - ETA: 8:02 - loss: 0.6946 - acc: 0.5190
4800/9333 [==============>...............] - ETA: 7:55 - loss: 0.6946 - acc: 0.5190
4864/9333 [==============>...............] - ETA: 7:49 - loss: 0.6947 - acc: 0.5177
4928/9333 [==============>...............] - ETA: 7:42 - loss: 0.6946 - acc: 0.5170
4992/9333 [===============>..............] - ETA: 7:35 - loss: 0.6948 - acc: 0.5164
5056/9333 [===============>..............] - ETA: 7:28 - loss: 0.6948 - acc: 0.5168
5120/9333 [===============>..............] - ETA: 7:22 - loss: 0.6949 - acc: 0.5166
5184/9333 [===============>..............] - ETA: 7:15 - loss: 0.6947 - acc: 0.5176
5248/9333 [===============>..............] - ETA: 7:08 - loss: 0.6947 - acc: 0.5185
5312/9333 [================>.............] - ETA: 7:01 - loss: 0.6948 - acc: 0.5177
5376/9333 [================>.............] - ETA: 6:54 - loss: 0.6949 - acc: 0.5166
5440/9333 [================>.............] - ETA: 6:48 - loss: 0.6949 - acc: 0.5164
5504/9333 [================>.............] - ETA: 6:41 - loss: 0.6950 - acc: 0.5158
5568/9333 [================>.............] - ETA: 6:34 - loss: 0.6952 - acc: 0.5151
5632/9333 [=================>............] - ETA: 6:27 - loss: 0.6952 - acc: 0.5153
5696/9333 [=================>............] - ETA: 6:21 - loss: 0.6954 - acc: 0.5146
5760/9333 [=================>............] - ETA: 6:14 - loss: 0.6953 - acc: 0.5155
5824/9333 [=================>............] - ETA: 6:07 - loss: 0.6951 - acc: 0.5167
5888/9333 [=================>............] - ETA: 6:00 - loss: 0.6951 - acc: 0.5160
5952/9333 [==================>...........] - ETA: 5:54 - loss: 0.6951 - acc: 0.5156
6016/9333 [==================>...........] - ETA: 5:47 - loss: 0.6951 - acc: 0.5155
6080/9333 [==================>...........] - ETA: 5:40 - loss: 0.6951 - acc: 0.5155
6144/9333 [==================>...........] - ETA: 5:33 - loss: 0.6953 - acc: 0.5138
6208/9333 [==================>...........] - ETA: 5:27 - loss: 0.6951 - acc: 0.5143
6272/9333 [===================>..........] - ETA: 5:20 - loss: 0.6952 - acc: 0.5139
6336/9333 [===================>..........] - ETA: 5:13 - loss: 0.6951 - acc: 0.5134
6400/9333 [===================>..........] - ETA: 5:06 - loss: 0.6952 - acc: 0.5130
6464/9333 [===================>..........] - ETA: 4:59 - loss: 0.6952 - acc: 0.5130
6528/9333 [===================>..........] - ETA: 4:53 - loss: 0.6952 - acc: 0.5127
6592/9333 [====================>.........] - ETA: 4:46 - loss: 0.6952 - acc: 0.5129
6656/9333 [====================>.........] - ETA: 4:39 - loss: 0.6950 - acc: 0.5138
6720/9333 [====================>.........] - ETA: 4:32 - loss: 0.6949 - acc: 0.5144
6784/9333 [====================>.........] - ETA: 4:25 - loss: 0.6947 - acc: 0.5147
6848/9333 [=====================>........] - ETA: 4:19 - loss: 0.6946 - acc: 0.5145
6912/9333 [=====================>........] - ETA: 4:12 - loss: 0.6949 - acc: 0.5145
6976/9333 [=====================>........] - ETA: 4:05 - loss: 0.6948 - acc: 0.5151
7040/9333 [=====================>........] - ETA: 3:59 - loss: 0.6948 - acc: 0.5148
7104/9333 [=====================>........] - ETA: 3:52 - loss: 0.6948 - acc: 0.5153
7168/9333 [======================>.......] - ETA: 3:45 - loss: 0.6947 - acc: 0.5158
7232/9333 [======================>.......] - ETA: 3:38 - loss: 0.6948 - acc: 0.5148
7296/9333 [======================>.......] - ETA: 3:32 - loss: 0.6949 - acc: 0.5136
7360/9333 [======================>.......] - ETA: 3:25 - loss: 0.6949 - acc: 0.5140
7424/9333 [======================>.......] - ETA: 3:18 - loss: 0.6949 - acc: 0.5136
7488/9333 [=======================>......] - ETA: 3:11 - loss: 0.6949 - acc: 0.5144
7552/9333 [=======================>......] - ETA: 3:05 - loss: 0.6949 - acc: 0.5143
7616/9333 [=======================>......] - ETA: 2:58 - loss: 0.6948 - acc: 0.5148
7680/9333 [=======================>......] - ETA: 2:51 - loss: 0.6949 - acc: 0.5138
7744/9333 [=======================>......] - ETA: 2:45 - loss: 0.6948 - acc: 0.5141
7808/9333 [========================>.....] - ETA: 2:38 - loss: 0.6946 - acc: 0.5143
7872/9333 [========================>.....] - ETA: 2:31 - loss: 0.6944 - acc: 0.5159
7936/9333 [========================>.....] - ETA: 2:25 - loss: 0.6945 - acc: 0.5150
8000/9333 [========================>.....] - ETA: 2:18 - loss: 0.6944 - acc: 0.5155
8064/9333 [========================>.....] - ETA: 2:11 - loss: 0.6947 - acc: 0.5150
8128/9333 [=========================>....] - ETA: 2:05 - loss: 0.6948 - acc: 0.5150
8192/9333 [=========================>....] - ETA: 1:58 - loss: 0.6949 - acc: 0.5145
8256/9333 [=========================>....] - ETA: 1:52 - loss: 0.6950 - acc: 0.5143
8320/9333 [=========================>....] - ETA: 1:45 - loss: 0.6951 - acc: 0.5135
8384/9333 [=========================>....] - ETA: 1:38 - loss: 0.6951 - acc: 0.5132
8448/9333 [==========================>...] - ETA: 1:32 - loss: 0.6950 - acc: 0.5144
8512/9333 [==========================>...] - ETA: 1:25 - loss: 0.6950 - acc: 0.5136
8576/9333 [==========================>...] - ETA: 1:18 - loss: 0.6951 - acc: 0.5128
8640/9333 [==========================>...] - ETA: 1:12 - loss: 0.6952 - acc: 0.5133
8704/9333 [==========================>...] - ETA: 1:05 - loss: 0.6953 - acc: 0.5138
8768/9333 [===========================>..] - ETA: 58s - loss: 0.6952 - acc: 0.5138 
8832/9333 [===========================>..] - ETA: 52s - loss: 0.6950 - acc: 0.5142
8896/9333 [===========================>..] - ETA: 45s - loss: 0.6951 - acc: 0.5139
8960/9333 [===========================>..] - ETA: 38s - loss: 0.6951 - acc: 0.5135
9024/9333 [============================>.] - ETA: 32s - loss: 0.6951 - acc: 0.5136
9088/9333 [============================>.] - ETA: 25s - loss: 0.6951 - acc: 0.5138
9152/9333 [============================>.] - ETA: 18s - loss: 0.6951 - acc: 0.5141
9216/9333 [============================>.] - ETA: 12s - loss: 0.6951 - acc: 0.5146
9280/9333 [============================>.] - ETA: 5s - loss: 0.6950 - acc: 0.5150 
9333/9333 [==============================] - 1008s 108ms/step - loss: 0.6949 - acc: 0.5149 - val_loss: 0.6926 - val_acc: 0.5149

Epoch 00002: val_acc improved from 0.50530 to 0.51495, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window21/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 3/10

  64/9333 [..............................] - ETA: 17:15 - loss: 0.6882 - acc: 0.5312
 128/9333 [..............................] - ETA: 16:38 - loss: 0.7040 - acc: 0.4844
 192/9333 [..............................] - ETA: 16:53 - loss: 0.6958 - acc: 0.5104
 256/9333 [..............................] - ETA: 16:24 - loss: 0.6962 - acc: 0.5156
 320/9333 [>.............................] - ETA: 16:25 - loss: 0.6987 - acc: 0.5031
 384/9333 [>.............................] - ETA: 16:17 - loss: 0.6971 - acc: 0.5104
 448/9333 [>.............................] - ETA: 16:06 - loss: 0.6955 - acc: 0.5201
 512/9333 [>.............................] - ETA: 16:01 - loss: 0.6971 - acc: 0.5137
 576/9333 [>.............................] - ETA: 15:47 - loss: 0.6972 - acc: 0.5174
 640/9333 [=>............................] - ETA: 15:35 - loss: 0.6969 - acc: 0.5172
 704/9333 [=>............................] - ETA: 15:34 - loss: 0.6958 - acc: 0.5213
 768/9333 [=>............................] - ETA: 15:24 - loss: 0.6968 - acc: 0.5130
 832/9333 [=>............................] - ETA: 15:14 - loss: 0.6984 - acc: 0.5120
 896/9333 [=>............................] - ETA: 15:05 - loss: 0.6999 - acc: 0.5033
 960/9333 [==>...........................] - ETA: 14:54 - loss: 0.6991 - acc: 0.5083
1024/9333 [==>...........................] - ETA: 14:45 - loss: 0.6998 - acc: 0.5078
1088/9333 [==>...........................] - ETA: 14:36 - loss: 0.6998 - acc: 0.5092
1152/9333 [==>...........................] - ETA: 14:29 - loss: 0.7005 - acc: 0.5061
1216/9333 [==>...........................] - ETA: 14:19 - loss: 0.6999 - acc: 0.5090
1280/9333 [===>..........................] - ETA: 14:11 - loss: 0.6990 - acc: 0.5156
1344/9333 [===>..........................] - ETA: 14:05 - loss: 0.6980 - acc: 0.5171
1408/9333 [===>..........................] - ETA: 13:59 - loss: 0.6977 - acc: 0.5178
1472/9333 [===>..........................] - ETA: 13:53 - loss: 0.6975 - acc: 0.5183
1536/9333 [===>..........................] - ETA: 13:45 - loss: 0.6977 - acc: 0.5189
1600/9333 [====>.........................] - ETA: 13:34 - loss: 0.6973 - acc: 0.5194
1664/9333 [====>.........................] - ETA: 13:24 - loss: 0.6972 - acc: 0.5186
1728/9333 [====>.........................] - ETA: 13:19 - loss: 0.6971 - acc: 0.5203
1792/9333 [====>.........................] - ETA: 13:11 - loss: 0.6960 - acc: 0.5229
1856/9333 [====>.........................] - ETA: 13:04 - loss: 0.6954 - acc: 0.5253
1920/9333 [=====>........................] - ETA: 12:57 - loss: 0.6957 - acc: 0.5224
1984/9333 [=====>........................] - ETA: 12:50 - loss: 0.6956 - acc: 0.5227
2048/9333 [=====>........................] - ETA: 12:43 - loss: 0.6957 - acc: 0.5215
2112/9333 [=====>........................] - ETA: 12:37 - loss: 0.6959 - acc: 0.5185
2176/9333 [=====>........................] - ETA: 12:29 - loss: 0.6959 - acc: 0.5175
2240/9333 [======>.......................] - ETA: 12:23 - loss: 0.6958 - acc: 0.5179
2304/9333 [======>.......................] - ETA: 12:16 - loss: 0.6957 - acc: 0.5182
2368/9333 [======>.......................] - ETA: 12:11 - loss: 0.6956 - acc: 0.5194
2432/9333 [======>.......................] - ETA: 12:02 - loss: 0.6955 - acc: 0.5193
2496/9333 [=======>......................] - ETA: 11:56 - loss: 0.6952 - acc: 0.5200
2560/9333 [=======>......................] - ETA: 11:49 - loss: 0.6951 - acc: 0.5207
2624/9333 [=======>......................] - ETA: 11:42 - loss: 0.6949 - acc: 0.5221
2688/9333 [=======>......................] - ETA: 11:35 - loss: 0.6950 - acc: 0.5219
2752/9333 [=======>......................] - ETA: 11:27 - loss: 0.6946 - acc: 0.5236
2816/9333 [========>.....................] - ETA: 11:21 - loss: 0.6942 - acc: 0.5249
2880/9333 [========>.....................] - ETA: 11:14 - loss: 0.6940 - acc: 0.5253
2944/9333 [========>.....................] - ETA: 11:07 - loss: 0.6939 - acc: 0.5258
3008/9333 [========>.....................] - ETA: 11:00 - loss: 0.6940 - acc: 0.5256
3072/9333 [========>.....................] - ETA: 10:54 - loss: 0.6943 - acc: 0.5247
3136/9333 [=========>....................] - ETA: 10:46 - loss: 0.6942 - acc: 0.5258
3200/9333 [=========>....................] - ETA: 10:39 - loss: 0.6945 - acc: 0.5247
3264/9333 [=========>....................] - ETA: 10:33 - loss: 0.6954 - acc: 0.5224
3328/9333 [=========>....................] - ETA: 10:26 - loss: 0.6953 - acc: 0.5228
3392/9333 [=========>....................] - ETA: 10:19 - loss: 0.6952 - acc: 0.5224
3456/9333 [==========>...................] - ETA: 10:11 - loss: 0.6957 - acc: 0.5205
3520/9333 [==========>...................] - ETA: 10:04 - loss: 0.6956 - acc: 0.5216
3584/9333 [==========>...................] - ETA: 9:57 - loss: 0.6956 - acc: 0.5201 
3648/9333 [==========>...................] - ETA: 9:51 - loss: 0.6957 - acc: 0.5195
3712/9333 [==========>...................] - ETA: 9:43 - loss: 0.6952 - acc: 0.5213
3776/9333 [===========>..................] - ETA: 9:36 - loss: 0.6951 - acc: 0.5207
3840/9333 [===========>..................] - ETA: 9:30 - loss: 0.6952 - acc: 0.5208
3904/9333 [===========>..................] - ETA: 9:23 - loss: 0.6954 - acc: 0.5197
3968/9333 [===========>..................] - ETA: 9:16 - loss: 0.6955 - acc: 0.5199
4032/9333 [===========>..................] - ETA: 9:09 - loss: 0.6956 - acc: 0.5193
4096/9333 [============>.................] - ETA: 9:03 - loss: 0.6953 - acc: 0.5203
4160/9333 [============>.................] - ETA: 8:56 - loss: 0.6953 - acc: 0.5200
4224/9333 [============>.................] - ETA: 8:50 - loss: 0.6951 - acc: 0.5196
4288/9333 [============>.................] - ETA: 8:43 - loss: 0.6949 - acc: 0.5203
4352/9333 [============>.................] - ETA: 8:37 - loss: 0.6947 - acc: 0.5209
4416/9333 [=============>................] - ETA: 8:30 - loss: 0.6947 - acc: 0.5202
4480/9333 [=============>................] - ETA: 8:24 - loss: 0.6946 - acc: 0.5203
4544/9333 [=============>................] - ETA: 8:17 - loss: 0.6947 - acc: 0.5207
4608/9333 [=============>................] - ETA: 8:10 - loss: 0.6943 - acc: 0.5226
4672/9333 [==============>...............] - ETA: 8:03 - loss: 0.6947 - acc: 0.5216
4736/9333 [==============>...............] - ETA: 7:56 - loss: 0.6947 - acc: 0.5215
4800/9333 [==============>...............] - ETA: 7:49 - loss: 0.6944 - acc: 0.5221
4864/9333 [==============>...............] - ETA: 7:42 - loss: 0.6941 - acc: 0.5222
4928/9333 [==============>...............] - ETA: 7:35 - loss: 0.6940 - acc: 0.5219
4992/9333 [===============>..............] - ETA: 7:28 - loss: 0.6936 - acc: 0.5240
5056/9333 [===============>..............] - ETA: 7:21 - loss: 0.6934 - acc: 0.5241
5120/9333 [===============>..............] - ETA: 7:14 - loss: 0.6938 - acc: 0.5225
5184/9333 [===============>..............] - ETA: 7:08 - loss: 0.6940 - acc: 0.5228
5248/9333 [===============>..............] - ETA: 7:01 - loss: 0.6938 - acc: 0.5231
5312/9333 [================>.............] - ETA: 6:54 - loss: 0.6934 - acc: 0.5241
5376/9333 [================>.............] - ETA: 6:48 - loss: 0.6939 - acc: 0.5236
5440/9333 [================>.............] - ETA: 6:41 - loss: 0.6944 - acc: 0.5228
5504/9333 [================>.............] - ETA: 6:34 - loss: 0.6944 - acc: 0.5231
5568/9333 [================>.............] - ETA: 6:27 - loss: 0.6941 - acc: 0.5241
5632/9333 [=================>............] - ETA: 6:21 - loss: 0.6942 - acc: 0.5243
5696/9333 [=================>............] - ETA: 6:14 - loss: 0.6945 - acc: 0.5235
5760/9333 [=================>............] - ETA: 6:07 - loss: 0.6943 - acc: 0.5243
5824/9333 [=================>............] - ETA: 6:00 - loss: 0.6943 - acc: 0.5239
5888/9333 [=================>............] - ETA: 5:54 - loss: 0.6946 - acc: 0.5229
5952/9333 [==================>...........] - ETA: 5:47 - loss: 0.6944 - acc: 0.5235
6016/9333 [==================>...........] - ETA: 5:40 - loss: 0.6943 - acc: 0.5241
6080/9333 [==================>...........] - ETA: 5:33 - loss: 0.6942 - acc: 0.5240
6144/9333 [==================>...........] - ETA: 5:26 - loss: 0.6943 - acc: 0.5239
6208/9333 [==================>...........] - ETA: 5:20 - loss: 0.6943 - acc: 0.5237
6272/9333 [===================>..........] - ETA: 5:13 - loss: 0.6942 - acc: 0.5241
6336/9333 [===================>..........] - ETA: 5:06 - loss: 0.6943 - acc: 0.5230
6400/9333 [===================>..........] - ETA: 5:00 - loss: 0.6943 - acc: 0.5222
6464/9333 [===================>..........] - ETA: 4:53 - loss: 0.6944 - acc: 0.5223
6528/9333 [===================>..........] - ETA: 4:47 - loss: 0.6943 - acc: 0.5218
6592/9333 [====================>.........] - ETA: 4:40 - loss: 0.6943 - acc: 0.5212
6656/9333 [====================>.........] - ETA: 4:33 - loss: 0.6943 - acc: 0.5212
6720/9333 [====================>.........] - ETA: 4:27 - loss: 0.6943 - acc: 0.5219
6784/9333 [====================>.........] - ETA: 4:20 - loss: 0.6942 - acc: 0.5220
6848/9333 [=====================>........] - ETA: 4:14 - loss: 0.6944 - acc: 0.5213
6912/9333 [=====================>........] - ETA: 4:08 - loss: 0.6943 - acc: 0.5216
6976/9333 [=====================>........] - ETA: 4:01 - loss: 0.6943 - acc: 0.5218
7040/9333 [=====================>........] - ETA: 3:54 - loss: 0.6944 - acc: 0.5214
7104/9333 [=====================>........] - ETA: 3:48 - loss: 0.6945 - acc: 0.5206
7168/9333 [======================>.......] - ETA: 3:41 - loss: 0.6943 - acc: 0.5209
7232/9333 [======================>.......] - ETA: 3:35 - loss: 0.6944 - acc: 0.5203
7296/9333 [======================>.......] - ETA: 3:28 - loss: 0.6944 - acc: 0.5206
7360/9333 [======================>.......] - ETA: 3:22 - loss: 0.6945 - acc: 0.5200
7424/9333 [======================>.......] - ETA: 3:15 - loss: 0.6943 - acc: 0.5207
7488/9333 [=======================>......] - ETA: 3:08 - loss: 0.6943 - acc: 0.5203
7552/9333 [=======================>......] - ETA: 3:02 - loss: 0.6945 - acc: 0.5191
7616/9333 [=======================>......] - ETA: 2:55 - loss: 0.6944 - acc: 0.5189
7680/9333 [=======================>......] - ETA: 2:49 - loss: 0.6944 - acc: 0.5188
7744/9333 [=======================>......] - ETA: 2:42 - loss: 0.6943 - acc: 0.5191
7808/9333 [========================>.....] - ETA: 2:35 - loss: 0.6942 - acc: 0.5196
7872/9333 [========================>.....] - ETA: 2:29 - loss: 0.6941 - acc: 0.5202
7936/9333 [========================>.....] - ETA: 2:22 - loss: 0.6943 - acc: 0.5194
8000/9333 [========================>.....] - ETA: 2:16 - loss: 0.6944 - acc: 0.5190
8064/9333 [========================>.....] - ETA: 2:09 - loss: 0.6943 - acc: 0.5197
8128/9333 [=========================>....] - ETA: 2:03 - loss: 0.6941 - acc: 0.5204
8192/9333 [=========================>....] - ETA: 1:56 - loss: 0.6942 - acc: 0.5208
8256/9333 [=========================>....] - ETA: 1:49 - loss: 0.6941 - acc: 0.5218
8320/9333 [=========================>....] - ETA: 1:43 - loss: 0.6939 - acc: 0.5225
8384/9333 [=========================>....] - ETA: 1:36 - loss: 0.6939 - acc: 0.5219
8448/9333 [==========================>...] - ETA: 1:30 - loss: 0.6940 - acc: 0.5221
8512/9333 [==========================>...] - ETA: 1:23 - loss: 0.6939 - acc: 0.5226
8576/9333 [==========================>...] - ETA: 1:17 - loss: 0.6940 - acc: 0.5224
8640/9333 [==========================>...] - ETA: 1:10 - loss: 0.6939 - acc: 0.5228
8704/9333 [==========================>...] - ETA: 1:04 - loss: 0.6939 - acc: 0.5227
8768/9333 [===========================>..] - ETA: 57s - loss: 0.6939 - acc: 0.5224 
8832/9333 [===========================>..] - ETA: 50s - loss: 0.6939 - acc: 0.5221
8896/9333 [===========================>..] - ETA: 44s - loss: 0.6939 - acc: 0.5219
8960/9333 [===========================>..] - ETA: 37s - loss: 0.6939 - acc: 0.5218
9024/9333 [============================>.] - ETA: 31s - loss: 0.6940 - acc: 0.5213
9088/9333 [============================>.] - ETA: 24s - loss: 0.6941 - acc: 0.5207
9152/9333 [============================>.] - ETA: 18s - loss: 0.6940 - acc: 0.5211
9216/9333 [============================>.] - ETA: 11s - loss: 0.6939 - acc: 0.5215
9280/9333 [============================>.] - ETA: 5s - loss: 0.6939 - acc: 0.5216 
9333/9333 [==============================] - 985s 106ms/step - loss: 0.6940 - acc: 0.5214 - val_loss: 0.6909 - val_acc: 0.5284

Epoch 00003: val_acc improved from 0.51495 to 0.52845, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window21/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 4/10

  64/9333 [..............................] - ETA: 16:06 - loss: 0.6850 - acc: 0.5312
 128/9333 [..............................] - ETA: 15:44 - loss: 0.6751 - acc: 0.5859
 192/9333 [..............................] - ETA: 15:11 - loss: 0.6861 - acc: 0.5469
 256/9333 [..............................] - ETA: 14:54 - loss: 0.6888 - acc: 0.5430
 320/9333 [>.............................] - ETA: 14:55 - loss: 0.6912 - acc: 0.5375
 384/9333 [>.............................] - ETA: 14:50 - loss: 0.6924 - acc: 0.5312
 448/9333 [>.............................] - ETA: 14:51 - loss: 0.6922 - acc: 0.5290
 512/9333 [>.............................] - ETA: 14:47 - loss: 0.6952 - acc: 0.5234
 576/9333 [>.............................] - ETA: 14:41 - loss: 0.6942 - acc: 0.5191
 640/9333 [=>............................] - ETA: 14:38 - loss: 0.6919 - acc: 0.5266
 704/9333 [=>............................] - ETA: 14:29 - loss: 0.6921 - acc: 0.5241
 768/9333 [=>............................] - ETA: 14:20 - loss: 0.6919 - acc: 0.5286
 832/9333 [=>............................] - ETA: 14:11 - loss: 0.6932 - acc: 0.5228
 896/9333 [=>............................] - ETA: 14:04 - loss: 0.6929 - acc: 0.5290
 960/9333 [==>...........................] - ETA: 13:58 - loss: 0.6921 - acc: 0.5333
1024/9333 [==>...........................] - ETA: 13:56 - loss: 0.6923 - acc: 0.5332
1088/9333 [==>...........................] - ETA: 13:49 - loss: 0.6937 - acc: 0.5276
1152/9333 [==>...........................] - ETA: 13:43 - loss: 0.6944 - acc: 0.5278
1216/9333 [==>...........................] - ETA: 13:37 - loss: 0.6949 - acc: 0.5280
1280/9333 [===>..........................] - ETA: 13:33 - loss: 0.6950 - acc: 0.5281
1344/9333 [===>..........................] - ETA: 13:26 - loss: 0.6932 - acc: 0.5320
1408/9333 [===>..........................] - ETA: 13:20 - loss: 0.6931 - acc: 0.5320
1472/9333 [===>..........................] - ETA: 13:13 - loss: 0.6939 - acc: 0.5292
1536/9333 [===>..........................] - ETA: 13:08 - loss: 0.6935 - acc: 0.5312
1600/9333 [====>.........................] - ETA: 13:01 - loss: 0.6933 - acc: 0.5312
1664/9333 [====>.........................] - ETA: 12:54 - loss: 0.6935 - acc: 0.5288
1728/9333 [====>.........................] - ETA: 12:48 - loss: 0.6932 - acc: 0.5301
1792/9333 [====>.........................] - ETA: 12:40 - loss: 0.6927 - acc: 0.5318
1856/9333 [====>.........................] - ETA: 12:33 - loss: 0.6928 - acc: 0.5312
1920/9333 [=====>........................] - ETA: 12:25 - loss: 0.6925 - acc: 0.5318
1984/9333 [=====>........................] - ETA: 12:18 - loss: 0.6924 - acc: 0.5318
2048/9333 [=====>........................] - ETA: 12:12 - loss: 0.6924 - acc: 0.5322
2112/9333 [=====>........................] - ETA: 12:03 - loss: 0.6927 - acc: 0.5303
2176/9333 [=====>........................] - ETA: 11:59 - loss: 0.6931 - acc: 0.5303
2240/9333 [======>.......................] - ETA: 11:52 - loss: 0.6928 - acc: 0.5308
2304/9333 [======>.......................] - ETA: 11:45 - loss: 0.6923 - acc: 0.5330
2368/9333 [======>.......................] - ETA: 11:40 - loss: 0.6928 - acc: 0.5312
2432/9333 [======>.......................] - ETA: 11:33 - loss: 0.6930 - acc: 0.5296
2496/9333 [=======>......................] - ETA: 11:28 - loss: 0.6934 - acc: 0.5276
2560/9333 [=======>......................] - ETA: 11:22 - loss: 0.6934 - acc: 0.5277
2624/9333 [=======>......................] - ETA: 11:16 - loss: 0.6934 - acc: 0.5263
2688/9333 [=======>......................] - ETA: 11:09 - loss: 0.6933 - acc: 0.5260
2752/9333 [=======>......................] - ETA: 11:01 - loss: 0.6934 - acc: 0.5258
2816/9333 [========>.....................] - ETA: 10:55 - loss: 0.6935 - acc: 0.5249
2880/9333 [========>.....................] - ETA: 10:47 - loss: 0.6934 - acc: 0.5250
2944/9333 [========>.....................] - ETA: 10:41 - loss: 0.6935 - acc: 0.5245
3008/9333 [========>.....................] - ETA: 10:35 - loss: 0.6934 - acc: 0.5269
3072/9333 [========>.....................] - ETA: 10:29 - loss: 0.6932 - acc: 0.5286
3136/9333 [=========>....................] - ETA: 10:23 - loss: 0.6929 - acc: 0.5306
3200/9333 [=========>....................] - ETA: 10:16 - loss: 0.6927 - acc: 0.5312
3264/9333 [=========>....................] - ETA: 10:10 - loss: 0.6926 - acc: 0.5306
3328/9333 [=========>....................] - ETA: 10:04 - loss: 0.6925 - acc: 0.5306
3392/9333 [=========>....................] - ETA: 9:59 - loss: 0.6924 - acc: 0.5292 
3456/9333 [==========>...................] - ETA: 9:52 - loss: 0.6925 - acc: 0.5284
3520/9333 [==========>...................] - ETA: 9:46 - loss: 0.6921 - acc: 0.5295
3584/9333 [==========>...................] - ETA: 9:40 - loss: 0.6919 - acc: 0.5301
3648/9333 [==========>...................] - ETA: 9:34 - loss: 0.6919 - acc: 0.5302
3712/9333 [==========>...................] - ETA: 9:28 - loss: 0.6920 - acc: 0.5286
3776/9333 [===========>..................] - ETA: 9:21 - loss: 0.6922 - acc: 0.5273
3840/9333 [===========>..................] - ETA: 9:14 - loss: 0.6922 - acc: 0.5276
3904/9333 [===========>..................] - ETA: 9:08 - loss: 0.6922 - acc: 0.5277
3968/9333 [===========>..................] - ETA: 9:01 - loss: 0.6923 - acc: 0.5267
4032/9333 [===========>..................] - ETA: 8:54 - loss: 0.6920 - acc: 0.5285
4096/9333 [============>.................] - ETA: 8:48 - loss: 0.6918 - acc: 0.5291
4160/9333 [============>.................] - ETA: 8:42 - loss: 0.6918 - acc: 0.5286
4224/9333 [============>.................] - ETA: 8:35 - loss: 0.6917 - acc: 0.5305
4288/9333 [============>.................] - ETA: 8:29 - loss: 0.6919 - acc: 0.5299
4352/9333 [============>.................] - ETA: 8:23 - loss: 0.6921 - acc: 0.5299
4416/9333 [=============>................] - ETA: 8:16 - loss: 0.6923 - acc: 0.5288
4480/9333 [=============>................] - ETA: 8:10 - loss: 0.6924 - acc: 0.5277
4544/9333 [=============>................] - ETA: 8:03 - loss: 0.6927 - acc: 0.5260
4608/9333 [=============>................] - ETA: 7:57 - loss: 0.6928 - acc: 0.5263
4672/9333 [==============>...............] - ETA: 7:51 - loss: 0.6926 - acc: 0.5270
4736/9333 [==============>...............] - ETA: 7:44 - loss: 0.6928 - acc: 0.5249
4800/9333 [==============>...............] - ETA: 7:38 - loss: 0.6929 - acc: 0.5246
4864/9333 [==============>...............] - ETA: 7:31 - loss: 0.6931 - acc: 0.5241
4928/9333 [==============>...............] - ETA: 7:25 - loss: 0.6931 - acc: 0.5241
4992/9333 [===============>..............] - ETA: 7:18 - loss: 0.6930 - acc: 0.5244
5056/9333 [===============>..............] - ETA: 7:11 - loss: 0.6928 - acc: 0.5253
5120/9333 [===============>..............] - ETA: 7:05 - loss: 0.6928 - acc: 0.5244
5184/9333 [===============>..............] - ETA: 6:58 - loss: 0.6926 - acc: 0.5241
5248/9333 [===============>..............] - ETA: 6:52 - loss: 0.6926 - acc: 0.5240
5312/9333 [================>.............] - ETA: 6:45 - loss: 0.6926 - acc: 0.5239
5376/9333 [================>.............] - ETA: 6:38 - loss: 0.6927 - acc: 0.5229
5440/9333 [================>.............] - ETA: 6:32 - loss: 0.6924 - acc: 0.5243
5504/9333 [================>.............] - ETA: 6:25 - loss: 0.6926 - acc: 0.5233
5568/9333 [================>.............] - ETA: 6:18 - loss: 0.6926 - acc: 0.5226
5632/9333 [=================>............] - ETA: 6:12 - loss: 0.6925 - acc: 0.5229
5696/9333 [=================>............] - ETA: 6:06 - loss: 0.6924 - acc: 0.5235
5760/9333 [=================>............] - ETA: 6:00 - loss: 0.6923 - acc: 0.5241
5824/9333 [=================>............] - ETA: 5:53 - loss: 0.6924 - acc: 0.5246
5888/9333 [=================>............] - ETA: 5:47 - loss: 0.6922 - acc: 0.5255
5952/9333 [==================>...........] - ETA: 5:40 - loss: 0.6923 - acc: 0.5250
6016/9333 [==================>...........] - ETA: 5:34 - loss: 0.6922 - acc: 0.5246
6080/9333 [==================>...........] - ETA: 5:28 - loss: 0.6922 - acc: 0.5248
6144/9333 [==================>...........] - ETA: 5:21 - loss: 0.6923 - acc: 0.5246
6208/9333 [==================>...........] - ETA: 5:14 - loss: 0.6921 - acc: 0.5255
6272/9333 [===================>..........] - ETA: 5:08 - loss: 0.6919 - acc: 0.5263
6336/9333 [===================>..........] - ETA: 5:02 - loss: 0.6917 - acc: 0.5271
6400/9333 [===================>..........] - ETA: 4:55 - loss: 0.6918 - acc: 0.5267
6464/9333 [===================>..........] - ETA: 4:49 - loss: 0.6917 - acc: 0.5274
6528/9333 [===================>..........] - ETA: 4:42 - loss: 0.6916 - acc: 0.5271
6592/9333 [====================>.........] - ETA: 4:36 - loss: 0.6913 - acc: 0.5276
6656/9333 [====================>.........] - ETA: 4:30 - loss: 0.6912 - acc: 0.5282
6720/9333 [====================>.........] - ETA: 4:23 - loss: 0.6912 - acc: 0.5284
6784/9333 [====================>.........] - ETA: 4:17 - loss: 0.6914 - acc: 0.5282
6848/9333 [=====================>........] - ETA: 4:10 - loss: 0.6914 - acc: 0.5280
6912/9333 [=====================>........] - ETA: 4:04 - loss: 0.6915 - acc: 0.5278
6976/9333 [=====================>........] - ETA: 3:57 - loss: 0.6912 - acc: 0.5294
7040/9333 [=====================>........] - ETA: 3:51 - loss: 0.6913 - acc: 0.5290
7104/9333 [=====================>........] - ETA: 3:44 - loss: 0.6914 - acc: 0.5290
7168/9333 [======================>.......] - ETA: 3:38 - loss: 0.6914 - acc: 0.5290
7232/9333 [======================>.......] - ETA: 3:31 - loss: 0.6914 - acc: 0.5285
7296/9333 [======================>.......] - ETA: 3:25 - loss: 0.6913 - acc: 0.5286
7360/9333 [======================>.......] - ETA: 3:18 - loss: 0.6911 - acc: 0.5288
7424/9333 [======================>.......] - ETA: 3:12 - loss: 0.6910 - acc: 0.5292
7488/9333 [=======================>......] - ETA: 3:05 - loss: 0.6909 - acc: 0.5290
7552/9333 [=======================>......] - ETA: 2:59 - loss: 0.6910 - acc: 0.5286
7616/9333 [=======================>......] - ETA: 2:53 - loss: 0.6911 - acc: 0.5282
7680/9333 [=======================>......] - ETA: 2:46 - loss: 0.6910 - acc: 0.5286
7744/9333 [=======================>......] - ETA: 2:39 - loss: 0.6910 - acc: 0.5287
7808/9333 [========================>.....] - ETA: 2:33 - loss: 0.6913 - acc: 0.5278
7872/9333 [========================>.....] - ETA: 2:27 - loss: 0.6914 - acc: 0.5279
7936/9333 [========================>.....] - ETA: 2:20 - loss: 0.6915 - acc: 0.5277
8000/9333 [========================>.....] - ETA: 2:14 - loss: 0.6914 - acc: 0.5279
8064/9333 [========================>.....] - ETA: 2:07 - loss: 0.6911 - acc: 0.5296
8128/9333 [=========================>....] - ETA: 2:01 - loss: 0.6910 - acc: 0.5304
8192/9333 [=========================>....] - ETA: 1:54 - loss: 0.6911 - acc: 0.5302
8256/9333 [=========================>....] - ETA: 1:48 - loss: 0.6911 - acc: 0.5298
8320/9333 [=========================>....] - ETA: 1:41 - loss: 0.6911 - acc: 0.5293
8384/9333 [=========================>....] - ETA: 1:35 - loss: 0.6910 - acc: 0.5301
8448/9333 [==========================>...] - ETA: 1:28 - loss: 0.6909 - acc: 0.5302
8512/9333 [==========================>...] - ETA: 1:22 - loss: 0.6909 - acc: 0.5298
8576/9333 [==========================>...] - ETA: 1:15 - loss: 0.6909 - acc: 0.5297
8640/9333 [==========================>...] - ETA: 1:09 - loss: 0.6908 - acc: 0.5299
8704/9333 [==========================>...] - ETA: 1:03 - loss: 0.6908 - acc: 0.5294
8768/9333 [===========================>..] - ETA: 56s - loss: 0.6908 - acc: 0.5293 
8832/9333 [===========================>..] - ETA: 50s - loss: 0.6910 - acc: 0.5289
8896/9333 [===========================>..] - ETA: 43s - loss: 0.6911 - acc: 0.5278
8960/9333 [===========================>..] - ETA: 37s - loss: 0.6912 - acc: 0.5273
9024/9333 [============================>.] - ETA: 30s - loss: 0.6911 - acc: 0.5277
9088/9333 [============================>.] - ETA: 24s - loss: 0.6911 - acc: 0.5276
9152/9333 [============================>.] - ETA: 18s - loss: 0.6911 - acc: 0.5273
9216/9333 [============================>.] - ETA: 11s - loss: 0.6911 - acc: 0.5277
9280/9333 [============================>.] - ETA: 5s - loss: 0.6909 - acc: 0.5279 
9333/9333 [==============================] - 973s 104ms/step - loss: 0.6910 - acc: 0.5282 - val_loss: 0.6952 - val_acc: 0.5014

Epoch 00004: val_acc did not improve from 0.52845
Epoch 5/10

  64/9333 [..............................] - ETA: 16:33 - loss: 0.6876 - acc: 0.5469
 128/9333 [..............................] - ETA: 15:48 - loss: 0.6988 - acc: 0.5000
 192/9333 [..............................] - ETA: 15:31 - loss: 0.7026 - acc: 0.4896
 256/9333 [..............................] - ETA: 15:06 - loss: 0.7064 - acc: 0.4805
 320/9333 [>.............................] - ETA: 15:25 - loss: 0.7019 - acc: 0.4969
 384/9333 [>.............................] - ETA: 15:16 - loss: 0.7018 - acc: 0.5000
 448/9333 [>.............................] - ETA: 15:20 - loss: 0.7010 - acc: 0.5000
 512/9333 [>.............................] - ETA: 15:16 - loss: 0.7022 - acc: 0.5078
 576/9333 [>.............................] - ETA: 15:11 - loss: 0.7005 - acc: 0.5069
 640/9333 [=>............................] - ETA: 15:07 - loss: 0.6983 - acc: 0.5188
 704/9333 [=>............................] - ETA: 14:56 - loss: 0.6986 - acc: 0.5270
 768/9333 [=>............................] - ETA: 14:47 - loss: 0.6980 - acc: 0.5286
 832/9333 [=>............................] - ETA: 14:37 - loss: 0.6966 - acc: 0.5325
 896/9333 [=>............................] - ETA: 14:25 - loss: 0.6973 - acc: 0.5290
 960/9333 [==>...........................] - ETA: 14:15 - loss: 0.6965 - acc: 0.5365
1024/9333 [==>...........................] - ETA: 14:06 - loss: 0.6968 - acc: 0.5381
1088/9333 [==>...........................] - ETA: 14:00 - loss: 0.6971 - acc: 0.5358
1152/9333 [==>...........................] - ETA: 13:50 - loss: 0.6974 - acc: 0.5339
1216/9333 [==>...........................] - ETA: 13:45 - loss: 0.6978 - acc: 0.5345
1280/9333 [===>..........................] - ETA: 13:37 - loss: 0.6975 - acc: 0.5359
1344/9333 [===>..........................] - ETA: 13:29 - loss: 0.6980 - acc: 0.5298
1408/9333 [===>..........................] - ETA: 13:23 - loss: 0.6976 - acc: 0.5284
1472/9333 [===>..........................] - ETA: 13:17 - loss: 0.6972 - acc: 0.5312
1536/9333 [===>..........................] - ETA: 13:10 - loss: 0.6973 - acc: 0.5299
1600/9333 [====>.........................] - ETA: 13:04 - loss: 0.6968 - acc: 0.5331
1664/9333 [====>.........................] - ETA: 12:59 - loss: 0.6974 - acc: 0.5282
1728/9333 [====>.........................] - ETA: 12:51 - loss: 0.6981 - acc: 0.5255
1792/9333 [====>.........................] - ETA: 12:44 - loss: 0.6987 - acc: 0.5218
1856/9333 [====>.........................] - ETA: 12:37 - loss: 0.6981 - acc: 0.5269
1920/9333 [=====>........................] - ETA: 12:33 - loss: 0.6977 - acc: 0.5271
1984/9333 [=====>........................] - ETA: 12:28 - loss: 0.6981 - acc: 0.5252
2048/9333 [=====>........................] - ETA: 12:21 - loss: 0.6978 - acc: 0.5259
2112/9333 [=====>........................] - ETA: 12:13 - loss: 0.6973 - acc: 0.5279
2176/9333 [=====>........................] - ETA: 12:06 - loss: 0.6971 - acc: 0.5280
2240/9333 [======>.......................] - ETA: 11:59 - loss: 0.6973 - acc: 0.5277
2304/9333 [======>.......................] - ETA: 11:52 - loss: 0.6971 - acc: 0.5282
2368/9333 [======>.......................] - ETA: 11:44 - loss: 0.6970 - acc: 0.5262
2432/9333 [======>.......................] - ETA: 11:38 - loss: 0.6972 - acc: 0.5243
2496/9333 [=======>......................] - ETA: 11:31 - loss: 0.6972 - acc: 0.5240
2560/9333 [=======>......................] - ETA: 11:24 - loss: 0.6974 - acc: 0.5215
2624/9333 [=======>......................] - ETA: 11:17 - loss: 0.6973 - acc: 0.5210
2688/9333 [=======>......................] - ETA: 11:11 - loss: 0.6973 - acc: 0.5219
2752/9333 [=======>......................] - ETA: 11:06 - loss: 0.6978 - acc: 0.5211
2816/9333 [========>.....................] - ETA: 10:59 - loss: 0.6973 - acc: 0.5217
2880/9333 [========>.....................] - ETA: 10:53 - loss: 0.6978 - acc: 0.5194
2944/9333 [========>.....................] - ETA: 10:46 - loss: 0.6977 - acc: 0.5197
3008/9333 [========>.....................] - ETA: 10:41 - loss: 0.6977 - acc: 0.5186
3072/9333 [========>.....................] - ETA: 10:36 - loss: 0.6980 - acc: 0.5179
3136/9333 [=========>....................] - ETA: 10:29 - loss: 0.6977 - acc: 0.5179
3200/9333 [=========>....................] - ETA: 10:24 - loss: 0.6976 - acc: 0.5178
3264/9333 [=========>....................] - ETA: 10:17 - loss: 0.6973 - acc: 0.5187
3328/9333 [=========>....................] - ETA: 10:10 - loss: 0.6973 - acc: 0.5180
3392/9333 [=========>....................] - ETA: 10:04 - loss: 0.6974 - acc: 0.5153
3456/9333 [==========>...................] - ETA: 9:57 - loss: 0.6970 - acc: 0.5177 
3520/9333 [==========>...................] - ETA: 9:51 - loss: 0.6969 - acc: 0.5176
3584/9333 [==========>...................] - ETA: 9:44 - loss: 0.6967 - acc: 0.5184
3648/9333 [==========>...................] - ETA: 9:38 - loss: 0.6967 - acc: 0.5186
3712/9333 [==========>...................] - ETA: 9:31 - loss: 0.6968 - acc: 0.5175
3776/9333 [===========>..................] - ETA: 9:25 - loss: 0.6963 - acc: 0.5185
3840/9333 [===========>..................] - ETA: 9:19 - loss: 0.6964 - acc: 0.5195
3904/9333 [===========>..................] - ETA: 9:12 - loss: 0.6962 - acc: 0.5202
3968/9333 [===========>..................] - ETA: 9:05 - loss: 0.6959 - acc: 0.5229
4032/9333 [===========>..................] - ETA: 8:59 - loss: 0.6959 - acc: 0.5226
4096/9333 [============>.................] - ETA: 8:53 - loss: 0.6959 - acc: 0.5222
4160/9333 [============>.................] - ETA: 8:47 - loss: 0.6960 - acc: 0.5221
4224/9333 [============>.................] - ETA: 8:40 - loss: 0.6962 - acc: 0.5215
4288/9333 [============>.................] - ETA: 8:34 - loss: 0.6957 - acc: 0.5226
4352/9333 [============>.................] - ETA: 8:28 - loss: 0.6956 - acc: 0.5227
4416/9333 [=============>................] - ETA: 8:22 - loss: 0.6956 - acc: 0.5231
4480/9333 [=============>................] - ETA: 8:15 - loss: 0.6954 - acc: 0.5234
4544/9333 [=============>................] - ETA: 8:08 - loss: 0.6951 - acc: 0.5242
4608/9333 [=============>................] - ETA: 8:02 - loss: 0.6950 - acc: 0.5241
4672/9333 [==============>...............] - ETA: 7:55 - loss: 0.6949 - acc: 0.5253
4736/9333 [==============>...............] - ETA: 7:49 - loss: 0.6945 - acc: 0.5268
4800/9333 [==============>...............] - ETA: 7:42 - loss: 0.6942 - acc: 0.5285
4864/9333 [==============>...............] - ETA: 7:36 - loss: 0.6940 - acc: 0.5294
4928/9333 [==============>...............] - ETA: 7:29 - loss: 0.6939 - acc: 0.5304
4992/9333 [===============>..............] - ETA: 7:23 - loss: 0.6940 - acc: 0.5298
5056/9333 [===============>..............] - ETA: 7:16 - loss: 0.6942 - acc: 0.5291
5120/9333 [===============>..............] - ETA: 7:10 - loss: 0.6938 - acc: 0.5309
5184/9333 [===============>..............] - ETA: 7:03 - loss: 0.6939 - acc: 0.5293
5248/9333 [===============>..............] - ETA: 6:57 - loss: 0.6939 - acc: 0.5297
5312/9333 [================>.............] - ETA: 6:51 - loss: 0.6941 - acc: 0.5290
5376/9333 [================>.............] - ETA: 6:44 - loss: 0.6942 - acc: 0.5285
5440/9333 [================>.............] - ETA: 6:38 - loss: 0.6938 - acc: 0.5298
5504/9333 [================>.............] - ETA: 6:31 - loss: 0.6940 - acc: 0.5300
5568/9333 [================>.............] - ETA: 6:25 - loss: 0.6942 - acc: 0.5291
5632/9333 [=================>............] - ETA: 6:18 - loss: 0.6939 - acc: 0.5304
5696/9333 [=================>............] - ETA: 6:12 - loss: 0.6942 - acc: 0.5291
5760/9333 [=================>............] - ETA: 6:05 - loss: 0.6941 - acc: 0.5283
5824/9333 [=================>............] - ETA: 5:59 - loss: 0.6941 - acc: 0.5287
5888/9333 [=================>............] - ETA: 5:52 - loss: 0.6944 - acc: 0.5265
5952/9333 [==================>...........] - ETA: 5:46 - loss: 0.6943 - acc: 0.5260
6016/9333 [==================>...........] - ETA: 5:39 - loss: 0.6942 - acc: 0.5261
6080/9333 [==================>...........] - ETA: 5:32 - loss: 0.6941 - acc: 0.5266
6144/9333 [==================>...........] - ETA: 5:26 - loss: 0.6941 - acc: 0.5265
6208/9333 [==================>...........] - ETA: 5:20 - loss: 0.6939 - acc: 0.5271
6272/9333 [===================>..........] - ETA: 5:13 - loss: 0.6939 - acc: 0.5268
6336/9333 [===================>..........] - ETA: 5:07 - loss: 0.6939 - acc: 0.5267
6400/9333 [===================>..........] - ETA: 5:00 - loss: 0.6936 - acc: 0.5280
6464/9333 [===================>..........] - ETA: 4:54 - loss: 0.6935 - acc: 0.5283
6528/9333 [===================>..........] - ETA: 4:47 - loss: 0.6936 - acc: 0.5279
6592/9333 [====================>.........] - ETA: 4:41 - loss: 0.6936 - acc: 0.5282
6656/9333 [====================>.........] - ETA: 4:34 - loss: 0.6936 - acc: 0.5279
6720/9333 [====================>.........] - ETA: 4:27 - loss: 0.6934 - acc: 0.5283
6784/9333 [====================>.........] - ETA: 4:21 - loss: 0.6934 - acc: 0.5282
6848/9333 [=====================>........] - ETA: 4:14 - loss: 0.6933 - acc: 0.5285
6912/9333 [=====================>........] - ETA: 4:08 - loss: 0.6934 - acc: 0.5279
6976/9333 [=====================>........] - ETA: 4:01 - loss: 0.6932 - acc: 0.5285
7040/9333 [=====================>........] - ETA: 3:55 - loss: 0.6933 - acc: 0.5281
7104/9333 [=====================>........] - ETA: 3:48 - loss: 0.6932 - acc: 0.5283
7168/9333 [======================>.......] - ETA: 3:41 - loss: 0.6933 - acc: 0.5275
7232/9333 [======================>.......] - ETA: 3:35 - loss: 0.6930 - acc: 0.5282
7296/9333 [======================>.......] - ETA: 3:28 - loss: 0.6930 - acc: 0.5284
7360/9333 [======================>.......] - ETA: 3:22 - loss: 0.6928 - acc: 0.5293
7424/9333 [======================>.......] - ETA: 3:15 - loss: 0.6928 - acc: 0.5290
7488/9333 [=======================>......] - ETA: 3:09 - loss: 0.6928 - acc: 0.5287
7552/9333 [=======================>......] - ETA: 3:02 - loss: 0.6925 - acc: 0.5305
7616/9333 [=======================>......] - ETA: 2:56 - loss: 0.6924 - acc: 0.5303
7680/9333 [=======================>......] - ETA: 2:49 - loss: 0.6922 - acc: 0.5309
7744/9333 [=======================>......] - ETA: 2:43 - loss: 0.6920 - acc: 0.5314
7808/9333 [========================>.....] - ETA: 2:36 - loss: 0.6920 - acc: 0.5310
7872/9333 [========================>.....] - ETA: 2:29 - loss: 0.6920 - acc: 0.5311
7936/9333 [========================>.....] - ETA: 2:23 - loss: 0.6920 - acc: 0.5307
8000/9333 [========================>.....] - ETA: 2:16 - loss: 0.6922 - acc: 0.5305
8064/9333 [========================>.....] - ETA: 2:10 - loss: 0.6921 - acc: 0.5311
8128/9333 [=========================>....] - ETA: 2:03 - loss: 0.6919 - acc: 0.5314
8192/9333 [=========================>....] - ETA: 1:57 - loss: 0.6918 - acc: 0.5320
8256/9333 [=========================>....] - ETA: 1:50 - loss: 0.6918 - acc: 0.5320
8320/9333 [=========================>....] - ETA: 1:43 - loss: 0.6918 - acc: 0.5317
8384/9333 [=========================>....] - ETA: 1:37 - loss: 0.6919 - acc: 0.5315
8448/9333 [==========================>...] - ETA: 1:30 - loss: 0.6918 - acc: 0.5316
8512/9333 [==========================>...] - ETA: 1:24 - loss: 0.6918 - acc: 0.5320
8576/9333 [==========================>...] - ETA: 1:17 - loss: 0.6916 - acc: 0.5326
8640/9333 [==========================>...] - ETA: 1:11 - loss: 0.6917 - acc: 0.5324
8704/9333 [==========================>...] - ETA: 1:04 - loss: 0.6916 - acc: 0.5326
8768/9333 [===========================>..] - ETA: 58s - loss: 0.6916 - acc: 0.5325 
8832/9333 [===========================>..] - ETA: 51s - loss: 0.6913 - acc: 0.5335
8896/9333 [===========================>..] - ETA: 45s - loss: 0.6913 - acc: 0.5337
8960/9333 [===========================>..] - ETA: 38s - loss: 0.6912 - acc: 0.5339
9024/9333 [============================>.] - ETA: 31s - loss: 0.6911 - acc: 0.5346
9088/9333 [============================>.] - ETA: 25s - loss: 0.6912 - acc: 0.5343
9152/9333 [============================>.] - ETA: 18s - loss: 0.6915 - acc: 0.5338
9216/9333 [============================>.] - ETA: 12s - loss: 0.6915 - acc: 0.5333
9280/9333 [============================>.] - ETA: 5s - loss: 0.6916 - acc: 0.5330 
9333/9333 [==============================] - 998s 107ms/step - loss: 0.6915 - acc: 0.5332 - val_loss: 0.6952 - val_acc: 0.5333

Epoch 00005: val_acc improved from 0.52845 to 0.53327, saving model to /data/lyli/Lung/Lung-model/Seq_feature_exact/window21/checkpoints/final_seq_model/lung_seq_model_2_mer.hdf5
Epoch 6/10

  64/9333 [..............................] - ETA: 17:00 - loss: 0.6969 - acc: 0.5781
 128/9333 [..............................] - ETA: 16:54 - loss: 0.6932 - acc: 0.5781
 192/9333 [..............................] - ETA: 16:40 - loss: 0.6817 - acc: 0.6042
 256/9333 [..............................] - ETA: 16:40 - loss: 0.6917 - acc: 0.5820
 320/9333 [>.............................] - ETA: 16:17 - loss: 0.6931 - acc: 0.5625
 384/9333 [>.............................] - ETA: 16:02 - loss: 0.6879 - acc: 0.5625
 448/9333 [>.............................] - ETA: 15:51 - loss: 0.6873 - acc: 0.5603
 512/9333 [>.............................] - ETA: 15:32 - loss: 0.6849 - acc: 0.5605
 576/9333 [>.............................] - ETA: 15:24 - loss: 0.6869 - acc: 0.5538
 640/9333 [=>............................] - ETA: 15:17 - loss: 0.6871 - acc: 0.5531
 704/9333 [=>............................] - ETA: 15:07 - loss: 0.6908 - acc: 0.5412
 768/9333 [=>............................] - ETA: 14:55 - loss: 0.6901 - acc: 0.5456
 832/9333 [=>............................] - ETA: 14:46 - loss: 0.6899 - acc: 0.5505
 896/9333 [=>............................] - ETA: 14:32 - loss: 0.6909 - acc: 0.5491
 960/9333 [==>...........................] - ETA: 14:22 - loss: 0.6914 - acc: 0.5437
1024/9333 [==>...........................] - ETA: 14:11 - loss: 0.6922 - acc: 0.5449
1088/9333 [==>...........................] - ETA: 13:59 - loss: 0.6937 - acc: 0.5368
1152/9333 [==>...........................] - ETA: 13:54 - loss: 0.6928 - acc: 0.5365
1216/9333 [==>...........................] - ETA: 13:46 - loss: 0.6917 - acc: 0.5411
1280/9333 [===>..........................] - ETA: 13:41 - loss: 0.6928 - acc: 0.5367
1344/9333 [===>..........................] - ETA: 13:32 - loss: 0.6937 - acc: 0.5357
1408/9333 [===>..........................] - ETA: 13:25 - loss: 0.6949 - acc: 0.5305
1472/9333 [===>..........................] - ETA: 13:19 - loss: 0.6940 - acc: 0.5353
1536/9333 [===>..........................] - ETA: 13:12 - loss: 0.6945 - acc: 0.5312
1600/9333 [====>.........................] - ETA: 13:02 - loss: 0.6938 - acc: 0.5344
1664/9333 [====>.........................] - ETA: 12:55 - loss: 0.6934 - acc: 0.5349
1728/9333 [====>.........................] - ETA: 12:47 - loss: 0.6928 - acc: 0.5388
1792/9333 [====>.........................] - ETA: 12:40 - loss: 0.6932 - acc: 0.5385
1856/9333 [====>.........................] - ETA: 12:32 - loss: 0.6925 - acc: 0.5415
1920/9333 [=====>........................] - ETA: 12:26 - loss: 0.6926 - acc: 0.5385
1984/9333 [=====>........................] - ETA: 12:21 - loss: 0.6925 - acc: 0.5378
2048/9333 [=====>........................] - ETA: 12:14 - loss: 0.6927 - acc: 0.5361
2112/9333 [=====>........................] - ETA: 12:09 - loss: 0.6925 - acc: 0.5369
2176/9333 [=====>........................] - ETA: 12:02 - loss: 0.6923 - acc: 0.5368
2240/9333 [======>.......................] - ETA: 11:57 - loss: 0.6920 - acc: 0.5362
2304/9333 [======>.......................] - ETA: 11:50 - loss: 0.6919 - acc: 0.5365
2368/9333 [======>.......................] - ETA: 11:44 - loss: 0.6919 - acc: 0.5359
2432/9333 [======>.......................] - ETA: 11:39 - loss: 0.6925 - acc: 0.5341
2496/9333 [=======>......................] - ETA: 11:32 - loss: 0.6926 - acc: 0.5345
2560/9333 [=======>......................] - ETA: 11:24 - loss: 0.6925 - acc: 0.5359
2624/9333 [=======>......................] - ETA: 11:17 - loss: 0.6926 - acc: 0.5354
2688/9333 [=======>......................] - ETA: 11:10 - loss: 0.6922 - acc: 0.5372
2752/9333 [=======>......................] - ETA: 11:05 - loss: 0.6924 - acc: 0.5367
2816/9333 [========>.....................] - ETA: 10:59 - loss: 0.6926 - acc: 0.5352
2880/9333 [========>.....................] - ETA: 10:52 - loss: 0.6922 - acc: 0.5361
2944/9333 [========>.....................] - ETA: 10:45 - loss: 0.6922 - acc: 0.5357
3008/9333 [========>.....................] - ETA: 10:39 - loss: 0.6921 - acc: 0.5369
3072/9333 [========>.....................] - ETA: 10:32 - loss: 0.6919 - acc: 0.5368
3136/9333 [=========>....................] - ETA: 10:26 - loss: 0.6921 - acc: 0.5357
3200/9333 [=========>....................] - ETA: 10:19 - loss: 0.6920 - acc: 0.5350
3264/9333 [=========>....................] - ETA: 10:13 - loss: 0.6914 - acc: 0.5355
3328/9333 [=========>....................] - ETA: 10:06 - loss: 0.6912 - acc: 0.5367
3392/9333 [=========>....................] - ETA: 9:59 - loss: 0.6910 - acc: 0.5366 
3456/9333 [==========>...................] - ETA: 9:53 - loss: 0.6908 - acc: 0.5356
3520/9333 [==========>...................] - ETA: 9:46 - loss: 0.6908 - acc: 0.5358
3584/9333 [==========>...................] - ETA: 9:39 - loss: 0.6905 - acc: 0.5363
3648/9333 [==========>...................] - ETA: 9:33 - loss: 0.6903 - acc: 0.5365
3712/9333 [==========>...................] - ETA: 9:26 - loss: 0.6906 - acc: 0.5348
3776/9333 [===========>..................] - ETA: 9:20 - loss: 0.6906 - acc: 0.5350
3840/9333 [===========>..................] - ETA: 9:13 - loss: 0.6904 - acc: 0.5357
3904/9333 [===========>..................] - ETA: 9:07 - loss: 0.6900 - acc: 0.5369
3968/9333 [===========>..................] - ETA: 9:01 - loss: 0.6896 - acc: 0.5386
4032/9333 [===========>..................] - ETA: 8:54 - loss: 0.6898 - acc: 0.5375
4096/9333 [============>.................] - ETA: 8:47 - loss: 0.6899 - acc: 0.5378
4160/9333 [============>.................] - ETA: 8:41 - loss: 0.6897 - acc: 0.5392
4224/9333 [============>.................] - ETA: 8:35 - loss: 0.6895 - acc: 0.5402
4288/9333 [============>.................] - ETA: 8:28 - loss: 0.6894 - acc: 0.5399
4352/9333 [============>.................] - ETA: 8:22 - loss: 0.6894 - acc: 0.5395
4416/9333 [=============>................] - ETA: 8:15 - loss: 0.6892 - acc: 0.5405
4480/9333 [=============>................] - ETA: 8:09 - loss: 0.6892 - acc: 0.5406
4544/9333 [=============>................] - ETA: 8:02 - loss: 0.6890 - acc: 0.5403
4608/9333 [=============>................] - ETA: 7:56 - loss: 0.6885 - acc: 0.5417
4672/9333 [==============>...............] - ETA: 7:49 - loss: 0.6887 - acc: 0.5415
4736/9333 [==============>...............] - ETA: 7:42 - loss: 0.6887 - acc: 0.5418
4800/9333 [==============>...............] - ETA: 7:36 - loss: 0.6885 - acc: 0.5419
4864/9333 [==============>...............] - ETA: 7:29 - loss: 0.6884 - acc: 0.5430
4928/9333 [==============>...............] - ETA: 7:23 - loss: 0.6890 - acc: 0.5424
4992/9333 [===============>..............] - ETA: 7:17 - loss: 0.6896 - acc: 0.5417
5056/9333 [===============>..............] - ETA: 7:10 - loss: 0.6898 - acc: 0.5415
5120/9333 [===============>..............] - ETA: 7:03 - loss: 0.6897 - acc: 0.5418
5184/9333 [===============>..............] - ETA: 6:57 - loss: 0.6895 - acc: 0.5424
5248/9333 [===============>..............] - ETA: 6:50 - loss: 0.6895 - acc: 0.5421
5312/9333 [================>.............] - ETA: 6:44 - loss: 0.6892 - acc: 0.5433
5376/9333 [================>.............] - ETA: 6:38 - loss: 0.6893 - acc: 0.5433
5440/9333 [================>.............] - ETA: 6:32 - loss: 0.6894 - acc: 0.5432
5504/9333 [================>.............] - ETA: 6:25 - loss: 0.6894 - acc: 0.5438
5568/9333 [================>.............] - ETA: 6:18 - loss: 0.6893 - acc: 0.5438
5632/9333 [=================>............] - ETA: 6:12 - loss: 0.6893 - acc: 0.5446
5696/9333 [=================>............] - ETA: 6:05 - loss: 0.6893 - acc: 0.5446
5760/9333 [=================>............] - ETA: 5:59 - loss: 0.6896 - acc: 0.5444
5824/9333 [=================>............] - ETA: 5:52 - loss: 0.6896 - acc: 0.5438
5888/9333 [=================>............] - ETA: 5:46 - loss: 0.6893 - acc: 0.5445
5952/9333 [==================>...........] - ETA: 5:39 - loss: 0.6895 - acc: 0.5428
6016/9333 [==================>...........] - ETA: 5:33 - loss: 0.6895 - acc: 0.5429
6080/9333 [==================>...........] - ETA: 5:27 - loss: 0.6894 - acc: 0.5437
6144/9333 [==================>...........] - ETA: 5:20 - loss: 0.6894 - acc: 0.5439
6208/9333 [==================>...........] - ETA: 5:14 - loss: 0.6893 - acc: 0.5440
6272/9333 [===================>..........] - ETA: 5:07 - loss: 0.6893 - acc: 0.5442
6336/9333 [===================>..........] - ETA: 5:00 - loss: 0.6890 - acc: 0.5450
6400/9333 [===================>..........] - ETA: 4:54 - loss: 0.6889 - acc: 0.5456
6464/9333 [===================>..........] - ETA: 4:47 - loss: 0.6887 - acc: 0.5456
6528/9333 [===================>..........] - ETA: 4:41 - loss: 0.6887 - acc: 0.5456
6592/9333 [====================>.........] - ETA: 4:34 - loss: 0.6888 - acc: 0.5449
6656/9333 [====================>.........] - ETA: 4:28 - loss: 0.6886 - acc: 0.5451
6720/9333 [====================>.........] - ETA: 4:21 - loss: 0.6884 - acc: 0.5449
6784/9333 [====================>.........] - ETA: 4:15 - loss: 0.6884 - acc: 0.5451
6848/9333 [=====================>........] - ETA: 4:09 - loss: 0.6884 - acc: 0.5450
6912/9333 [=====================>........] - ETA: 4:02 - loss: 0.6882 - acc: 0.5448
6976/9333 [=====================>........] - ETA: 3:56 - loss: 0.6883 - acc: 0.5446
7040/9333 [=====================>........] - ETA: 3:49 - loss: 0.6883 - acc: 0.5445
7104/9333 [=====================>........] - ETA: 3:43 - loss: 0.6882 - acc: 0.5449
7168/9333 [======================>.......] - ETA: 3:36 - loss: 0.6882 - acc: 0.5452
7232/9333 [======================>.......] - ETA: 3:30 - loss: 0.6882 - acc: 0.5454
7296/9333 [======================>.......] - ETA: 3:24 - loss: 0.6880 - acc: 0.5458
7360/9333 [======================>.......] - ETA: 3:17 - loss: 0.6881 - acc: 0.5459
7424/9333 [======================>.......] - ETA: 3:11 - loss: 0.6882 - acc: 0.5461
7488/9333 [=======================>......] - ETA: 3:05 - loss: 0.6884 - acc: 0.5455
7552/9333 [=======================>......] - ETA: 2:58 - loss: 0.6885 - acc: 0.5456
7616/9333 [=======================>......] - ETA: 2:52 - loss: 0.6889 - acc: 0.5448
7680/9333 [=======================>......] - ETA: 2:46 - loss: 0.6888 - acc: 0.5447
7744/9333 [=======================>......] - ETA: 2:39 - loss: 0.6888 - acc: 0.5448
7808/9333 [========================>.....] - ETA: 2:33 - loss: 0.6888 - acc: 0.5447
7872/9333 [========================>.....] - ETA: 2:26 - loss: 0.6884 - acc: 0.5460
7936/9333 [========================>.....] - ETA: 2:20 - loss: 0.6885 - acc: 0.5457
8000/9333 [========================>.....] - ETA: 2:13 - loss: 0.6885 - acc: 0.5457
8064/9333 [========================>.....] - ETA: 2:07 - loss: 0.6887 - acc: 0.5456
8128/9333 [=========================>....] - ETA: 2:01 - loss: 0.6883 - acc: 0.5464
8192/9333 [=========================>....] - ETA: 1:54 - loss: 0.6884 - acc: 0.5461
8256/9333 [=========================>....] - ETA: 1:48 - loss: 0.6883 - acc: 0.5463
8320/9333 [=========================>....] - ETA: 1:41 - loss: 0.6884 - acc: 0.5454
8384/9333 [=========================>....] - ETA: 1:35 - loss: 0.6883 - acc: 0.5460
8448/9333 [==========================>...] - ETA: 1:28 - loss: 0.6882 - acc: 0.5460
8512/9333 [==========================>...] - ETA: 1:22 - loss: 0.6883 - acc: 0.5462
8576/9333 [==========================>...] - ETA: 1:16 - loss: 0.6883 - acc: 0.5458
8640/9333 [==========================>...] - ETA: 1:09 - loss: 0.6881 - acc: 0.5463
8704/9333 [==========================>...] - ETA: 1:03 - loss: 0.6882 - acc: 0.5460
8768/9333 [===========================>..] - ETA: 56s - loss: 0.6882 - acc: 0.5463 
8832/9333 [===========================>..] - ETA: 50s - loss: 0.6881 - acc: 0.5468
8896/9333 [===========================>..] - ETA: 43s - loss: 0.6882 - acc: 0.5468
8960/9333 [===========================>..] - ETA: 37s - loss: 0.6884 - acc: 0.5459
9024/9333 [============================>.] - ETA: 31s - loss: 0.6884 - acc: 0.5463
9088/9333 [============================>.] - ETA: 24s - loss: 0.6884 - acc: 0.5461
9152/9333 [============================>.] - ETA: 18s - loss: 0.6884 - acc: 0.5465
9216/9333 [============================>.] - ETA: 11s - loss: 0.6886 - acc: 0.5464
9280/9333 [============================>.] - ETA: 5s - loss: 0.6887 - acc: 0.5459 
9333/9333 [==============================] - 975s 104ms/step - loss: 0.6886 - acc: 0.5462 - val_loss: 0.6966 - val_acc: 0.5130

Epoch 00006: val_acc did not improve from 0.53327
Epoch 7/10

  64/9333 [..............................] - ETA: 16:13 - loss: 0.7130 - acc: 0.4844
 128/9333 [..............................] - ETA: 15:40 - loss: 0.6955 - acc: 0.4922
 192/9333 [..............................] - ETA: 15:23 - loss: 0.6923 - acc: 0.5156
 256/9333 [..............................] - ETA: 14:56 - loss: 0.6944 - acc: 0.5000
 320/9333 [>.............................] - ETA: 14:43 - loss: 0.6932 - acc: 0.5156
 384/9333 [>.............................] - ETA: 14:53 - loss: 0.6908 - acc: 0.5260
 448/9333 [>.............................] - ETA: 15:04 - loss: 0.6912 - acc: 0.5223
 512/9333 [>.............................] - ETA: 14:54 - loss: 0.6891 - acc: 0.5254
 576/9333 [>.............................] - ETA: 14:47 - loss: 0.6888 - acc: 0.5295
 640/9333 [=>............................] - ETA: 14:39 - loss: 0.6902 - acc: 0.5234
 704/9333 [=>............................] - ETA: 14:32 - loss: 0.6901 - acc: 0.5270
 768/9333 [=>............................] - ETA: 14:31 - loss: 0.6922 - acc: 0.5195
 832/9333 [=>............................] - ETA: 14:25 - loss: 0.6928 - acc: 0.5204
 896/9333 [=>............................] - ETA: 14:16 - loss: 0.6925 - acc: 0.5234
 960/9333 [==>...........................] - ETA: 14:08 - loss: 0.6923 - acc: 0.5260
1024/9333 [==>...........................] - ETA: 13:58 - loss: 0.6934 - acc: 0.5225
1088/9333 [==>...........................] - ETA: 13:51 - loss: 0.6926 - acc: 0.5285
1152/9333 [==>...........................] - ETA: 13:44 - loss: 0.6924 - acc: 0.5304
1216/9333 [==>...........................] - ETA: 13:38 - loss: 0.6930 - acc: 0.5271
1280/9333 [===>..........................] - ETA: 13:31 - loss: 0.6905 - acc: 0.5359
1344/9333 [===>..........................] - ETA: 13:26 - loss: 0.6899 - acc: 0.5379
1408/9333 [===>..........................] - ETA: 13:18 - loss: 0.6900 - acc: 0.5376
1472/9333 [===>..........................] - ETA: 13:16 - loss: 0.6888 - acc: 0.5414
1536/9333 [===>..........................] - ETA: 13:10 - loss: 0.6898 - acc: 0.5384
1600/9333 [====>.........................] - ETA: 13:01 - loss: 0.6884 - acc: 0.5406
1664/9333 [====>.........................] - ETA: 12:55 - loss: 0.6895 - acc: 0.5391
1728/9333 [====>.........................] - ETA: 12:49 - loss: 0.6888 - acc: 0.5440
1792/9333 [====>.........................] - ETA: 12:43 - loss: 0.6901 - acc: 0.5391
1856/9333 [====>.........................] - ETA: 12:39 - loss: 0.6895 - acc: 0.5415
1920/9333 [=====>........................] - ETA: 12:34 - loss: 0.6890 - acc: 0.5417
1984/9333 [=====>........................] - ETA: 12:27 - loss: 0.6903 - acc: 0.5383
2048/9333 [=====>........................] - ETA: 12:18 - loss: 0.6905 - acc: 0.5371
2112/9333 [=====>........................] - ETA: 12:10 - loss: 0.6912 - acc: 0.5360
2176/9333 [=====>........................] - ETA: 12:03 - loss: 0.6911 - acc: 0.5363
2240/9333 [======>.......................] - ETA: 11:55 - loss: 0.6918 - acc: 0.5326
2304/9333 [======>.......................] - ETA: 11:48 - loss: 0.6920 - acc: 0.5317
2368/9333 [======>.......................] - ETA: 11:44 - loss: 0.6916 - acc: 0.5312
2432/9333 [======>.......................] - ETA: 11:34 - loss: 0.6917 - acc: 0.5317
2496/9333 [=======>......................] - ETA: 11:29 - loss: 0.6917 - acc: 0.5317
2560/9333 [=======>......................] - ETA: 11:21 - loss: 0.6916 - acc: 0.5316
2624/9333 [=======>......................] - ETA: 11:15 - loss: 0.6910 - acc: 0.5351
2688/9333 [=======>......................] - ETA: 11:07 - loss: 0.6914 - acc: 0.5331
2752/9333 [=======>......................] - ETA: 11:00 - loss: 0.6913 - acc: 0.5331
2816/9333 [========>.....................] - ETA: 10:53 - loss: 0.6917 - acc: 0.5312
2880/9333 [========>.....................] - ETA: 10:46 - loss: 0.6910 - acc: 0.5337
2944/9333 [========>.....................] - ETA: 10:40 - loss: 0.6907 - acc: 0.5353
3008/9333 [========>.....................] - ETA: 10:33 - loss: 0.6911 - acc: 0.5336
3072/9333 [========>.....................] - ETA: 10:26 - loss: 0.6909 - acc: 0.5335
3136/9333 [=========>....................] - ETA: 10:20 - loss: 0.6908 - acc: 0.5338
3200/9333 [=========>....................] - ETA: 10:13 - loss: 0.6907 - acc: 0.5325
3264/9333 [=========>....................] - ETA: 10:07 - loss: 0.6908 - acc: 0.5319
3328/9333 [=========>....................] - ETA: 10:00 - loss: 0.6910 - acc: 0.5309
3392/9333 [=========>....................] - ETA: 9:54 - loss: 0.6906 - acc: 0.5324 
3456/9333 [==========>...................] - ETA: 9:48 - loss: 0.6901 - acc: 0.5353
3520/9333 [==========>...................] - ETA: 9:41 - loss: 0.6907 - acc: 0.5335
3584/9333 [==========>...................] - ETA: 9:34 - loss: 0.6910 - acc: 0.5329
3648/9333 [==========>...................] - ETA: 9:27 - loss: 0.6908 - acc: 0.5332
3712/9333 [==========>...................] - ETA: 9:21 - loss: 0.6907 - acc: 0.5331
3776/9333 [===========>..................] - ETA: 9:14 - loss: 0.6907 - acc: 0.5323
3840/9333 [===========>..................] - ETA: 9:07 - loss: 0.6905 - acc: 0.5349
3904/9333 [===========>..................] - ETA: 9:01 - loss: 0.6904 - acc: 0.5356
3968/9333 [===========>..................] - ETA: 8:55 - loss: 0.6906 - acc: 0.5345
4032/9333 [===========>..................] - ETA: 8:49 - loss: 0.6912 - acc: 0.5325
4096/9333 [============>.................] - ETA: 8:42 - loss: 0.6915 - acc: 0.5308
4160/9333 [============>.................] - ETA: 8:36 - loss: 0.6917 - acc: 0.5303
4224/9333 [============>.................] - ETA: 8:30 - loss: 0.6918 - acc: 0.5301
4288/9333 [============>.................] - ETA: 8:23 - loss: 0.6915 - acc: 0.5315
4352/9333 [============>.................] - ETA: 8:17 - loss: 0.6918 - acc: 0.5310
4416/9333 [=============>................] - ETA: 8:11 - loss: 0.6918 - acc: 0.5306
4480/9333 [=============>................] - ETA: 8:05 - loss: 0.6917 - acc: 0.5321
4544/9333 [=============>................] - ETA: 7:59 - loss: 0.6921 - acc: 0.5310
4608/9333 [=============>................] - ETA: 7:52 - loss: 0.6921 - acc: 0.5319
4672/9333 [==============>...............] - ETA: 7:46 - loss: 0.6920 - acc: 0.5312
4736/9333 [==============>...............] - ETA: 7:40 - loss: 0.6920 - acc: 0.5312
4800/9333 [==============>...............] - ETA: 7:33 - loss: 0.6921 - acc: 0.5315
4864/9333 [==============>...............] - ETA: 7:26 - loss: 0.6922 - acc: 0.5304
4928/9333 [==============>...............] - ETA: 7:20 - loss: 0.6920 - acc: 0.5308
4992/9333 [===============>..............] - ETA: 7:13 - loss: 0.6919 - acc: 0.5317
5056/9333 [===============>..............] - ETA: 7:07 - loss: 0.6920 - acc: 0.5314
5120/9333 [===============>..............] - ETA: 7:01 - loss: 0.6922 - acc: 0.5305
5184/9333 [===============>..............] - ETA: 6:54 - loss: 0.6922 - acc: 0.5299
5248/9333 [===============>..............] - ETA: 6:48 - loss: 0.6922 - acc: 0.5299
5312/9333 [================>.............] - ETA: 6:42 - loss: 0.6922 - acc: 0.5303
5376/9333 [================>.............] - ETA: 6:35 - loss: 0.6926 - acc: 0.5290
5440/9333 [================>.............] - ETA: 6:29 - loss: 0.6926 - acc: 0.5294
5504/9333 [================>.............] - ETA: 6:22 - loss: 0.6926 - acc: 0.5296
5568/9333 [================>.............] - ETA: 6:15 - loss: 0.6925 - acc: 0.5295
5632/9333 [=================>............] - ETA: 6:09 - loss: 0.6923 - acc: 0.5305
5696/9333 [=================>............] - ETA: 6:03 - loss: 0.6925 - acc: 0.5298
5760/9333 [=================>............] - ETA: 5:57 - loss: 0.6927 - acc: 0.5290
5824/9333 [=================>............] - ETA: 5:50 - loss: 0.6923 - acc: 0.5302
5888/9333 [=================>............] - ETA: 5:44 - loss: 0.6923 - acc: 0.5304
5952/9333 [==================>...........] - ETA: 5:37 - loss: 0.6923 - acc: 0.5291
6016/9333 [==================>...........] - ETA: 5:31 - loss: 0.6924 - acc: 0.5284
6080/9333 [==================>...........] - ETA: 5:24 - loss: 0.6922 - acc: 0.5286
6144/9333 [==================>...........] - ETA: 5:17 - loss: 0.6923 - acc: 0.5283
6208/9333 [==================>...........] - ETA: 5:11 - loss: 0.6923 - acc: 0.5288
6272/9333 [===================>..........] - ETA: 5:05 - loss: 0.6919 - acc: 0.5297
6336/9333 [===================>..........] - ETA: 4:58 - loss: 0.6921 - acc: 0.5289
6400/9333 [===================>..........] - ETA: 4:52 - loss: 0.6919 - acc: 0.5295
6464/9333 [===================>..........] - ETA: 4:45 - loss: 0.6919 - acc: 0.5286
6528/9333 [===================>..........] - ETA: 4:39 - loss: 0.6919 - acc: 0.5288
6592/9333 [====================>.........] - ETA: 4:32 - loss: 0.6919 - acc: 0.5287
6656/9333 [====================>.........] - ETA: 4:26 - loss: 0.6918 - acc: 0.5294
6720/9333 [====================>.........] - ETA: 4:20 - loss: 0.6918 - acc: 0.5290
6784/9333 [====================>.........] - ETA: 4:13 - loss: 0.6917 - acc: 0.5295
6848/9333 [=====================>........] - ETA: 4:07 - loss: 0.6916 - acc: 0.5298
6912/9333 [=====================>........] - ETA: 4:00 - loss: 0.6915 - acc: 0.5304
6976/9333 [=====================>........] - ETA: 3:54 - loss: 0.6913 - acc: 0.5310
7040/9333 [=====================>........] - ETA: 3:47 - loss: 0.6912 - acc: 0.5317
7104/9333 [=====================>........] - ETA: 3:41 - loss: 0.6912 - acc: 0.5312
7168/9333 [======================>.......] - ETA: 3:35 - loss: 0.6912 - acc: 0.5304
7232/9333 [======================>.......] - ETA: 3:28 - loss: 0.6911 - acc: 0.5306
7296/9333 [======================>.......] - ETA: 3:22 - loss: 0.6912 - acc: 0.5306
7360/9333 [======================>.......] - ETA: 3:16 - loss: 0.6912 - acc: 0.5310
7424/9333 [======================>.......] - ETA: 3:10 - loss: 0.6911 - acc: 0.5314
7488/9333 [=======================>......] - ETA: 3:03 - loss: 0.6910 - acc: 0.5322
7552/9333 [=======================>......] - ETA: 2:57 - loss: 0.6910 - acc: 0.5324
7616/9333 [=======================>......] - ETA: 2:51 - loss: 0.6910 - acc: 0.5322
7680/9333 [=======================>......] - ETA: 2:45 - loss: 0.6911 - acc: 0.5319
7744/9333 [=======================>......] - ETA: 2:38 - loss: 0.6912 - acc: 0.5311
7808/9333 [========================>.....] - ETA: 2:32 - loss: 0.6911 - acc: 0.5314
7872/9333 [========================>.....] - ETA: 2:26 - loss: 0.6913 - acc: 0.5301
7936/9333 [========================>.....] - ETA: 2:19 - loss: 0.6914 - acc: 0.5299
8000/9333 [========================>.....] - ETA: 2:13 - loss: 0.6913 - acc: 0.5302
8064/9333 [========================>.....] - ETA: 2:07 - loss: 0.6910 - acc: 0.5312
8128/9333 [=========================>....] - ETA: 2:00 - loss: 0.6910 - acc: 0.5310
8192/9333 [=========================>....] - ETA: 1:54 - loss: 0.6909 - acc: 0.5311
8256/9333 [=========================>....] - ETA: 1:47 - loss: 0.6910 - acc: 0.5303
8320/9333 [=========================>....] - ETA: 1:41 - loss: 0.6909 - acc: 0.5306
8384/9333 [=========================>....] - ETA: 1:35 - loss: 0.6909 - acc: 0.5303
8448/9333 [==========================>...] - ETA: 1:28 - loss: 0.6909 - acc: 0.5305
8512/9333 [==========================>...] - ETA: 1:22 - loss: 0.6908 - acc: 0.5309
8576/9333 [==========================>...] - ETA: 1:15 - loss: 0.6906 - acc: 0.5317
8640/9333 [==========================>...] - ETA: 1:09 - loss: 0.6908 - acc: 0.5309
8704/9333 [==========================>...] - ETA: 1:02 - loss: 0.6908 - acc: 0.5304
8768/9333 [===========================>..] - ETA: 56s - loss: 0.6908 - acc: 0.5307 
8832/9333 [===========================>..] - ETA: 50s - loss: 0.6907 - acc: 0.5309
8896/9333 [===========================>..] - ETA: 43s - loss: 0.6908 - acc: 0.5308
8960/9333 [===========================>..] - ETA: 37s - loss: 0.6908 - acc: 0.5306
9024/9333 [============================>.] - ETA: 30s - loss: 0.6909 - acc: 0.5303
9088/9333 [============================>.] - ETA: 24s - loss: 0.6908 - acc: 0.5300
9152/9333 [============================>.] - ETA: 18s - loss: 0.6908 - acc: 0.5300
9216/9333 [============================>.] - ETA: 11s - loss: 0.6906 - acc: 0.5308
9280/9333 [============================>.] - ETA: 5s - loss: 0.6907 - acc: 0.5310 
9333/9333 [==============================] - 971s 104ms/step - loss: 0.6906 - acc: 0.5313 - val_loss: 0.6974 - val_acc: 0.5188

Epoch 00007: val_acc did not improve from 0.53327
Epoch 8/10

  64/9333 [..............................] - ETA: 16:11 - loss: 0.7295 - acc: 0.3906
 128/9333 [..............................] - ETA: 16:26 - loss: 0.7015 - acc: 0.4844
 192/9333 [..............................] - ETA: 15:36 - loss: 0.7017 - acc: 0.4948
 256/9333 [..............................] - ETA: 15:16 - loss: 0.6986 - acc: 0.5156
 320/9333 [>.............................] - ETA: 15:09 - loss: 0.6953 - acc: 0.5281
 384/9333 [>.............................] - ETA: 14:41 - loss: 0.6953 - acc: 0.5234
 448/9333 [>.............................] - ETA: 14:39 - loss: 0.6944 - acc: 0.5312
 512/9333 [>.............................] - ETA: 14:30 - loss: 0.6926 - acc: 0.5332
 576/9333 [>.............................] - ETA: 14:19 - loss: 0.6922 - acc: 0.5382
 640/9333 [=>............................] - ETA: 14:13 - loss: 0.6930 - acc: 0.5281
 704/9333 [=>............................] - ETA: 14:11 - loss: 0.6915 - acc: 0.5312
 768/9333 [=>............................] - ETA: 14:08 - loss: 0.6928 - acc: 0.5247
 832/9333 [=>............................] - ETA: 14:05 - loss: 0.6929 - acc: 0.5228
 896/9333 [=>............................] - ETA: 13:58 - loss: 0.6924 - acc: 0.5234
 960/9333 [==>...........................] - ETA: 13:46 - loss: 0.6916 - acc: 0.5292
1024/9333 [==>...........................] - ETA: 13:41 - loss: 0.6904 - acc: 0.5332
1088/9333 [==>...........................] - ETA: 13:36 - loss: 0.6905 - acc: 0.5294
1152/9333 [==>...........................] - ETA: 13:29 - loss: 0.6898 - acc: 0.5312
1216/9333 [==>...........................] - ETA: 13:23 - loss: 0.6893 - acc: 0.5354
1280/9333 [===>..........................] - ETA: 13:13 - loss: 0.6915 - acc: 0.5266
1344/9333 [===>..........................] - ETA: 13:07 - loss: 0.6910 - acc: 0.5320
1408/9333 [===>..........................] - ETA: 13:00 - loss: 0.6913 - acc: 0.5348
1472/9333 [===>..........................] - ETA: 12:55 - loss: 0.6902 - acc: 0.5387
1536/9333 [===>..........................] - ETA: 12:46 - loss: 0.6901 - acc: 0.5404
1600/9333 [====>.........................] - ETA: 12:42 - loss: 0.6901 - acc: 0.5400
1664/9333 [====>.........................] - ETA: 12:36 - loss: 0.6894 - acc: 0.5421
1728/9333 [====>.........................] - ETA: 12:29 - loss: 0.6891 - acc: 0.5434
1792/9333 [====>.........................] - ETA: 12:21 - loss: 0.6889 - acc: 0.5452
1856/9333 [====>.........................] - ETA: 12:14 - loss: 0.6902 - acc: 0.5409
1920/9333 [=====>........................] - ETA: 12:08 - loss: 0.6896 - acc: 0.5417
1984/9333 [=====>........................] - ETA: 12:00 - loss: 0.6894 - acc: 0.5413
2048/9333 [=====>........................] - ETA: 11:53 - loss: 0.6888 - acc: 0.5425
2112/9333 [=====>........................] - ETA: 11:46 - loss: 0.6891 - acc: 0.5417
2176/9333 [=====>........................] - ETA: 11:40 - loss: 0.6884 - acc: 0.5437
2240/9333 [======>.......................] - ETA: 11:32 - loss: 0.6886 - acc: 0.5433
2304/9333 [======>.......................] - ETA: 11:24 - loss: 0.6877 - acc: 0.5469
2368/9333 [======>.......................] - ETA: 11:18 - loss: 0.6874 - acc: 0.5477
2432/9333 [======>.......................] - ETA: 11:11 - loss: 0.6873 - acc: 0.5481
2496/9333 [=======>......................] - ETA: 11:05 - loss: 0.6878 - acc: 0.5465
2560/9333 [=======>......................] - ETA: 10:58 - loss: 0.6874 - acc: 0.5484
2624/9333 [=======>......................] - ETA: 10:51 - loss: 0.6880 - acc: 0.5457
2688/9333 [=======>......................] - ETA: 10:44 - loss: 0.6878 - acc: 0.5443
2752/9333 [=======>......................] - ETA: 10:39 - loss: 0.6884 - acc: 0.5443
2816/9333 [========>.....................] - ETA: 10:32 - loss: 0.6875 - acc: 0.5458
2880/9333 [========>.....................] - ETA: 10:25 - loss: 0.6875 - acc: 0.5462
2944/9333 [========>.....................] - ETA: 10:20 - loss: 0.6874 - acc: 0.5472
3008/9333 [========>.....................] - ETA: 10:13 - loss: 0.6873 - acc: 0.5479
3072/9333 [========>.....................] - ETA: 10:06 - loss: 0.6873 - acc: 0.5488
3136/9333 [=========>....................] - ETA: 10:01 - loss: 0.6872 - acc: 0.5488
3200/9333 [=========>....................] - ETA: 9:55 - loss: 0.6874 - acc: 0.5481 
3264/9333 [=========>....................] - ETA: 9:49 - loss: 0.6874 - acc: 0.5490
3328/9333 [=========>....................] - ETA: 9:43 - loss: 0.6870 - acc: 0.5502
3392/9333 [=========>....................] - ETA: 9:38 - loss: 0.6867 - acc: 0.5525
3456/9333 [==========>...................] - ETA: 9:32 - loss: 0.6861 - acc: 0.5541
3520/9333 [==========>...................] - ETA: 9:26 - loss: 0.6860 - acc: 0.5543
3584/9333 [==========>...................] - ETA: 9:21 - loss: 0.6861 - acc: 0.5533
3648/9333 [==========>...................] - ETA: 9:14 - loss: 0.6863 - acc: 0.5529
3712/9333 [==========>...................] - ETA: 9:08 - loss: 0.6859 - acc: 0.5525
3776/9333 [===========>..................] - ETA: 9:01 - loss: 0.6853 - acc: 0.5527
3840/9333 [===========>..................] - ETA: 8:55 - loss: 0.6851 - acc: 0.5544
3904/9333 [===========>..................] - ETA: 8:49 - loss: 0.6853 - acc: 0.5533
3968/9333 [===========>..................] - ETA: 8:43 - loss: 0.6863 - acc: 0.5519
4032/9333 [===========>..................] - ETA: 8:36 - loss: 0.6863 - acc: 0.5518
4096/9333 [============>.................] - ETA: 8:29 - loss: 0.6866 - acc: 0.5508
4160/9333 [============>.................] - ETA: 8:23 - loss: 0.6865 - acc: 0.5510
4224/9333 [============>.................] - ETA: 8:17 - loss: 0.6864 - acc: 0.5509
4288/9333 [============>.................] - ETA: 8:10 - loss: 0.6864 - acc: 0.5513
4352/9333 [============>.................] - ETA: 8:04 - loss: 0.6866 - acc: 0.5512
4416/9333 [=============>................] - ETA: 7:58 - loss: 0.6864 - acc: 0.5525
4480/9333 [=============>................] - ETA: 7:52 - loss: 0.6866 - acc: 0.5531
4544/9333 [=============>................] - ETA: 7:45 - loss: 0.6862 - acc: 0.5541
4608/9333 [=============>................] - ETA: 7:39 - loss: 0.6865 - acc: 0.5534
4672/9333 [==============>...............] - ETA: 7:33 - loss: 0.6869 - acc: 0.5524
4736/9333 [==============>...............] - ETA: 7:27 - loss: 0.6868 - acc: 0.5532
4800/9333 [==============>...............] - ETA: 7:21 - loss: 0.6867 - acc: 0.5531
4864/9333 [==============>...............] - ETA: 7:14 - loss: 0.6867 - acc: 0.5528
4928/9333 [==============>...............] - ETA: 7:08 - loss: 0.6869 - acc: 0.5528
4992/9333 [===============>..............] - ETA: 7:02 - loss: 0.6870 - acc: 0.5519
5056/9333 [===============>..............] - ETA: 6:57 - loss: 0.6869 - acc: 0.5522
5120/9333 [===============>..............] - ETA: 6:51 - loss: 0.6871 - acc: 0.5516
5184/9333 [===============>..............] - ETA: 6:44 - loss: 0.6869 - acc: 0.5519
5248/9333 [===============>..............] - ETA: 6:38 - loss: 0.6872 - acc: 0.5511
5312/9333 [================>.............] - ETA: 6:32 - loss: 0.6874 - acc: 0.5508
5376/9333 [================>.............] - ETA: 6:26 - loss: 0.6878 - acc: 0.5506
5440/9333 [================>.............] - ETA: 6:20 - loss: 0.6879 - acc: 0.5506
5504/9333 [================>.............] - ETA: 6:13 - loss: 0.6880 - acc: 0.5496
5568/9333 [================>.............] - ETA: 6:07 - loss: 0.6881 - acc: 0.5492
5632/9333 [=================>............] - ETA: 6:00 - loss: 0.6885 - acc: 0.5479
5696/9333 [=================>............] - ETA: 5:54 - loss: 0.6887 - acc: 0.5474
5760/9333 [=================>............] - ETA: 5:48 - loss: 0.6888 - acc: 0.5469
5824/9333 [=================>............] - ETA: 5:41 - loss: 0.6889 - acc: 0.5462
5888/9333 [=================>............] - ETA: 5:35 - loss: 0.6892 - acc: 0.5452
5952/9333 [==================>...........] - ETA: 5:29 - loss: 0.6894 - acc: 0.5445
6016/9333 [==================>...........] - ETA: 5:23 - loss: 0.6894 - acc: 0.5447
6080/9333 [==================>...........] - ETA: 5:17 - loss: 0.6896 - acc: 0.5441
6144/9333 [==================>...........] - ETA: 5:11 - loss: 0.6895 - acc: 0.5441
6208/9333 [==================>...........] - ETA: 5:04 - loss: 0.6895 - acc: 0.5435
6272/9333 [===================>..........] - ETA: 4:58 - loss: 0.6894 - acc: 0.5440
6336/9333 [===================>..........] - ETA: 4:52 - loss: 0.6895 - acc: 0.5440
6400/9333 [===================>..........] - ETA: 4:45 - loss: 0.6895 - acc: 0.5441
6464/9333 [===================>..........] - ETA: 4:39 - loss: 0.6895 - acc: 0.5439
6528/9333 [===================>..........] - ETA: 4:33 - loss: 0.6895 - acc: 0.5440
6592/9333 [====================>.........] - ETA: 4:27 - loss: 0.6895 - acc: 0.5435
6656/9333 [====================>.........] - ETA: 4:21 - loss: 0.6893 - acc: 0.5439
6720/9333 [====================>.........] - ETA: 4:15 - loss: 0.6892 - acc: 0.5440
6784/9333 [====================>.........] - ETA: 4:08 - loss: 0.6891 - acc: 0.5445
6848/9333 [=====================>........] - ETA: 4:02 - loss: 0.6890 - acc: 0.5448
6912/9333 [=====================>........] - ETA: 3:56 - loss: 0.6890 - acc: 0.5443
6976/9333 [=====================>........] - ETA: 3:50 - loss: 0.6890 - acc: 0.5442
7040/9333 [=====================>........] - ETA: 3:43 - loss: 0.6893 - acc: 0.5433
7104/9333 [=====================>........] - ETA: 3:37 - loss: 0.6894 - acc: 0.5434
7168/9333 [======================>.......] - ETA: 3:30 - loss: 0.6893 - acc: 0.5424
7232/9333 [======================>.......] - ETA: 3:24 - loss: 0.6894 - acc: 0.5422
7296/9333 [======================>.......] - ETA: 3:18 - loss: 0.6893 - acc: 0.5419
7360/9333 [======================>.......] - ETA: 3:12 - loss: 0.6894 - acc: 0.5416
7424/9333 [======================>.......] - ETA: 3:06 - loss: 0.6894 - acc: 0.5414
7488/9333 [=======================>......] - ETA: 2:59 - loss: 0.6895 - acc: 0.5417
7552/9333 [=======================>......] - ETA: 2:53 - loss: 0.6895 - acc: 0.5414
7616/9333 [=======================>......] - ETA: 2:47 - loss: 0.6895 - acc: 0.5415
7680/9333 [=======================>......] - ETA: 2:41 - loss: 0.6894 - acc: 0.5422
7744/9333 [=======================>......] - ETA: 2:34 - loss: 0.6893 - acc: 0.5427
7808/9333 [========================>.....] - ETA: 2:28 - loss: 0.6894 - acc: 0.5421
7872/9333 [========================>.....] - ETA: 2:22 - loss: 0.6895 - acc: 0.5419
7936/9333 [========================>.....] - ETA: 2:16 - loss: 0.6894 - acc: 0.5417
8000/9333 [========================>.....] - ETA: 2:09 - loss: 0.6894 - acc: 0.5414
8064/9333 [========================>.....] - ETA: 2:03 - loss: 0.6893 - acc: 0.5417
8128/9333 [=========================>....] - ETA: 1:57 - loss: 0.6892 - acc: 0.5424
8192/9333 [=========================>....] - ETA: 1:51 - loss: 0.6892 - acc: 0.5428
8256/9333 [=========================>....] - ETA: 1:45 - loss: 0.6892 - acc: 0.5425
8320/9333 [=========================>....] - ETA: 1:38 - loss: 0.6892 - acc: 0.5425
8384/9333 [=========================>....] - ETA: 1:32 - loss: 0.6891 - acc: 0.5425
8448/9333 [==========================>...] - ETA: 1:26 - loss: 0.6892 - acc: 0.5421
8512/9333 [==========================>...] - ETA: 1:20 - loss: 0.6893 - acc: 0.5415
8576/9333 [==========================>...] - ETA: 1:13 - loss: 0.6893 - acc: 0.5416
8640/9333 [==========================>...] - ETA: 1:07 - loss: 0.6893 - acc: 0.5417
8704/9333 [==========================>...] - ETA: 1:01 - loss: 0.6893 - acc: 0.5416
8768/9333 [===========================>..] - ETA: 55s - loss: 0.6893 - acc: 0.5413 
8832/9333 [===========================>..] - ETA: 48s - loss: 0.6893 - acc: 0.5412
8896/9333 [===========================>..] - ETA: 42s - loss: 0.6891 - acc: 0.5419
8960/9333 [===========================>..] - ETA: 36s - loss: 0.6891 - acc: 0.5421
9024/9333 [============================>.] - ETA: 30s - loss: 0.6890 - acc: 0.5418
9088/9333 [============================>.] - ETA: 23s - loss: 0.6889 - acc: 0.5424
9152/9333 [============================>.] - ETA: 17s - loss: 0.6888 - acc: 0.5428
9216/9333 [============================>.] - ETA: 11s - loss: 0.6889 - acc: 0.5428
9280/9333 [============================>.] - ETA: 5s - loss: 0.6887 - acc: 0.5430 
9333/9333 [==============================] - 952s 102ms/step - loss: 0.6888 - acc: 0.5428 - val_loss: 0.6934 - val_acc: 0.5198

Epoch 00008: val_acc did not improve from 0.53327
Epoch 9/10

  64/9333 [..............................] - ETA: 16:09 - loss: 0.6757 - acc: 0.6250
 128/9333 [..............................] - ETA: 15:35 - loss: 0.6888 - acc: 0.5625
 192/9333 [..............................] - ETA: 15:46 - loss: 0.6885 - acc: 0.5677
 256/9333 [..............................] - ETA: 15:31 - loss: 0.6912 - acc: 0.5391
 320/9333 [>.............................] - ETA: 15:17 - loss: 0.6940 - acc: 0.5062
 384/9333 [>.............................] - ETA: 15:16 - loss: 0.6933 - acc: 0.5182
 448/9333 [>.............................] - ETA: 15:04 - loss: 0.6945 - acc: 0.5201
 512/9333 [>.............................] - ETA: 15:03 - loss: 0.6936 - acc: 0.5273
 576/9333 [>.............................] - ETA: 14:48 - loss: 0.6944 - acc: 0.5260
 640/9333 [=>............................] - ETA: 14:48 - loss: 0.6916 - acc: 0.5328
 704/9333 [=>............................] - ETA: 14:42 - loss: 0.6897 - acc: 0.5355
 768/9333 [=>............................] - ETA: 14:38 - loss: 0.6892 - acc: 0.5365
 832/9333 [=>............................] - ETA: 14:29 - loss: 0.6891 - acc: 0.5373
 896/9333 [=>............................] - ETA: 14:19 - loss: 0.6895 - acc: 0.5368
 960/9333 [==>...........................] - ETA: 14:09 - loss: 0.6899 - acc: 0.5323
1024/9333 [==>...........................] - ETA: 14:01 - loss: 0.6886 - acc: 0.5391
1088/9333 [==>...........................] - ETA: 13:50 - loss: 0.6877 - acc: 0.5423
1152/9333 [==>...........................] - ETA: 13:46 - loss: 0.6874 - acc: 0.5434
1216/9333 [==>...........................] - ETA: 13:40 - loss: 0.6876 - acc: 0.5411
1280/9333 [===>..........................] - ETA: 13:34 - loss: 0.6880 - acc: 0.5406
1344/9333 [===>..........................] - ETA: 13:26 - loss: 0.6879 - acc: 0.5424
1408/9333 [===>..........................] - ETA: 13:19 - loss: 0.6872 - acc: 0.5462
1472/9333 [===>..........................] - ETA: 13:13 - loss: 0.6879 - acc: 0.5462
1536/9333 [===>..........................] - ETA: 13:06 - loss: 0.6881 - acc: 0.5430
1600/9333 [====>.........................] - ETA: 12:59 - loss: 0.6887 - acc: 0.5375
1664/9333 [====>.........................] - ETA: 12:55 - loss: 0.6884 - acc: 0.5385
1728/9333 [====>.........................] - ETA: 12:48 - loss: 0.6880 - acc: 0.5405
1792/9333 [====>.........................] - ETA: 12:41 - loss: 0.6881 - acc: 0.5413
1856/9333 [====>.........................] - ETA: 12:34 - loss: 0.6884 - acc: 0.5404
1920/9333 [=====>........................] - ETA: 12:27 - loss: 0.6890 - acc: 0.5375
1984/9333 [=====>........................] - ETA: 12:20 - loss: 0.6888 - acc: 0.5398
2048/9333 [=====>........................] - ETA: 12:14 - loss: 0.6880 - acc: 0.5415
2112/9333 [=====>........................] - ETA: 12:06 - loss: 0.6877 - acc: 0.5407
2176/9333 [=====>........................] - ETA: 12:01 - loss: 0.6877 - acc: 0.5400
2240/9333 [======>.......................] - ETA: 11:54 - loss: 0.6877 - acc: 0.5406
2304/9333 [======>.......................] - ETA: 11:47 - loss: 0.6874 - acc: 0.5421
2368/9333 [======>.......................] - ETA: 11:40 - loss: 0.6874 - acc: 0.5414
2432/9333 [======>.......................] - ETA: 11:33 - loss: 0.6875 - acc: 0.5382
2496/9333 [=======>......................] - ETA: 11:28 - loss: 0.6876 - acc: 0.5373
2560/9333 [=======>......................] - ETA: 11:22 - loss: 0.6877 - acc: 0.5363
2624/9333 [=======>......................] - ETA: 11:15 - loss: 0.6883 - acc: 0.5362
2688/9333 [=======>......................] - ETA: 11:08 - loss: 0.6881 - acc: 0.5361
2752/9333 [=======>......................] - ETA: 11:03 - loss: 0.6882 - acc: 0.5363
2816/9333 [========>.....................] - ETA: 10:56 - loss: 0.6878 - acc: 0.5373
2880/9333 [========>.....................] - ETA: 10:51 - loss: 0.6886 - acc: 0.5375
2944/9333 [========>.....................] - ETA: 10:44 - loss: 0.6881 - acc: 0.5401
3008/9333 [========>.....................] - ETA: 10:37 - loss: 0.6878 - acc: 0.5402
3072/9333 [========>.....................] - ETA: 10:30 - loss: 0.6875 - acc: 0.5394
3136/9333 [=========>....................] - ETA: 10:22 - loss: 0.6871 - acc: 0.5399
3200/9333 [=========>....................] - ETA: 10:15 - loss: 0.6868 - acc: 0.5419
3264/9333 [=========>....................] - ETA: 10:09 - loss: 0.6865 - acc: 0.5429
3328/9333 [=========>....................] - ETA: 10:01 - loss: 0.6864 - acc: 0.5430
3392/9333 [=========>....................] - ETA: 9:55 - loss: 0.6862 - acc: 0.5425 
3456/9333 [==========>...................] - ETA: 9:48 - loss: 0.6860 - acc: 0.5425
3520/9333 [==========>...................] - ETA: 9:42 - loss: 0.6864 - acc: 0.5426
3584/9333 [==========>...................] - ETA: 9:36 - loss: 0.6869 - acc: 0.5427
3648/9333 [==========>...................] - ETA: 9:29 - loss: 0.6868 - acc: 0.5422
3712/9333 [==========>...................] - ETA: 9:24 - loss: 0.6869 - acc: 0.5423
3776/9333 [===========>..................] - ETA: 9:17 - loss: 0.6876 - acc: 0.5405
3840/9333 [===========>..................] - ETA: 9:11 - loss: 0.6876 - acc: 0.5417
3904/9333 [===========>..................] - ETA: 9:04 - loss: 0.6873 - acc: 0.5420
3968/9333 [===========>..................] - ETA: 8:58 - loss: 0.6870 - acc: 0.5423
4032/9333 [===========>..................] - ETA: 8:51 - loss: 0.6870 - acc: 0.5427
4096/9333 [============>.................] - ETA: 8:44 - loss: 0.6875 - acc: 0.5403
4160/9333 [============>.................] - ETA: 8:37 - loss: 0.6874 - acc: 0.5399
4224/9333 [============>.................] - ETA: 8:31 - loss: 0.6878 - acc: 0.5391
4288/9333 [============>.................] - ETA: 8:25 - loss: 0.6874 - acc: 0.5396
4352/9333 [============>.................] - ETA: 8:19 - loss: 0.6872 - acc: 0.5402
4416/9333 [=============>................] - ETA: 8:12 - loss: 0.6873 - acc: 0.5399
4480/9333 [=============>................] - ETA: 8:06 - loss: 0.6870 - acc: 0.5406
4544/9333 [=============>................] - ETA: 7:59 - loss: 0.6872 - acc: 0.5401
4608/9333 [=============>................] - ETA: 7:54 - loss: 0.6872 - acc: 0.5401
4672/9333 [==============>...............] - ETA: 7:48 - loss: 0.6870 - acc: 0.5411
4736/9333 [==============>...............] - ETA: 7:42 - loss: 0.6873 - acc: 0.5408
4800/9333 [==============>...............] - ETA: 7:36 - loss: 0.6870 - acc: 0.5427
4864/9333 [==============>...............] - ETA: 7:29 - loss: 0.6869 - acc: 0.5440
4928/9333 [==============>...............] - ETA: 7:23 - loss: 0.6870 - acc: 0.5438
4992/9333 [===============>..............] - ETA: 7:16 - loss: 0.6875 - acc: 0.5435
5056/9333 [===============>..............] - ETA: 7:10 - loss: 0.6883 - acc: 0.5409
5120/9333 [===============>..............] - ETA: 7:04 - loss: 0.6881 - acc: 0.5412
5184/9333 [===============>..............] - ETA: 6:57 - loss: 0.6881 - acc: 0.5415
5248/9333 [===============>..............] - ETA: 6:51 - loss: 0.6882 - acc: 0.5412
5312/9333 [================>.............] - ETA: 6:45 - loss: 0.6880 - acc: 0.5418
5376/9333 [================>.............] - ETA: 6:38 - loss: 0.6881 - acc: 0.5419
5440/9333 [================>.............] - ETA: 6:32 - loss: 0.6880 - acc: 0.5425
5504/9333 [================>.............] - ETA: 6:25 - loss: 0.6879 - acc: 0.5420
5568/9333 [================>.............] - ETA: 6:18 - loss: 0.6878 - acc: 0.5422
5632/9333 [=================>............] - ETA: 6:12 - loss: 0.6878 - acc: 0.5423
5696/9333 [=================>............] - ETA: 6:05 - loss: 0.6879 - acc: 0.5420
5760/9333 [=================>............] - ETA: 5:59 - loss: 0.6876 - acc: 0.5427
5824/9333 [=================>............] - ETA: 5:52 - loss: 0.6875 - acc: 0.5431
5888/9333 [=================>............] - ETA: 5:46 - loss: 0.6876 - acc: 0.5428
5952/9333 [==================>...........] - ETA: 5:39 - loss: 0.6875 - acc: 0.5435
6016/9333 [==================>...........] - ETA: 5:33 - loss: 0.6874 - acc: 0.5434
6080/9333 [==================>...........] - ETA: 5:26 - loss: 0.6876 - acc: 0.5431
6144/9333 [==================>...........] - ETA: 5:20 - loss: 0.6874 - acc: 0.5439
6208/9333 [==================>...........] - ETA: 5:13 - loss: 0.6875 - acc: 0.5437
6272/9333 [===================>..........] - ETA: 5:06 - loss: 0.6879 - acc: 0.5434
6336/9333 [===================>..........] - ETA: 5:00 - loss: 0.6882 - acc: 0.5428
6400/9333 [===================>..........] - ETA: 4:54 - loss: 0.6881 - acc: 0.5430
6464/9333 [===================>..........] - ETA: 4:47 - loss: 0.6878 - acc: 0.5438
6528/9333 [===================>..........] - ETA: 4:41 - loss: 0.6878 - acc: 0.5440
6592/9333 [====================>.........] - ETA: 4:34 - loss: 0.6880 - acc: 0.5429
6656/9333 [====================>.........] - ETA: 4:28 - loss: 0.6880 - acc: 0.5433
6720/9333 [====================>.........] - ETA: 4:21 - loss: 0.6880 - acc: 0.5432
6784/9333 [====================>.........] - ETA: 4:15 - loss: 0.6880 - acc: 0.5432
6848/9333 [=====================>........] - ETA: 4:08 - loss: 0.6877 - acc: 0.5438
6912/9333 [=====================>........] - ETA: 4:02 - loss: 0.6879 - acc: 0.5434
6976/9333 [=====================>........] - ETA: 3:55 - loss: 0.6877 - acc: 0.5443
7040/9333 [=====================>........] - ETA: 3:49 - loss: 0.6878 - acc: 0.5439
7104/9333 [=====================>........] - ETA: 3:42 - loss: 0.6877 - acc: 0.5441
7168/9333 [======================>.......] - ETA: 3:36 - loss: 0.6878 - acc: 0.5435
7232/9333 [======================>.......] - ETA: 3:29 - loss: 0.6880 - acc: 0.5427
7296/9333 [======================>.......] - ETA: 3:22 - loss: 0.6880 - acc: 0.5432
7360/9333 [======================>.......] - ETA: 3:16 - loss: 0.6881 - acc: 0.5428
7424/9333 [======================>.......] - ETA: 3:10 - loss: 0.6879 - acc: 0.5426
7488/9333 [=======================>......] - ETA: 3:03 - loss: 0.6878 - acc: 0.5430
7552/9333 [=======================>......] - ETA: 2:57 - loss: 0.6880 - acc: 0.5425
7616/9333 [=======================>......] - ETA: 2:50 - loss: 0.6879 - acc: 0.5433
7680/9333 [=======================>......] - ETA: 2:44 - loss: 0.6878 - acc: 0.5435
7744/9333 [=======================>......] - ETA: 2:38 - loss: 0.6878 - acc: 0.5435
7808/9333 [========================>.....] - ETA: 2:31 - loss: 0.6876 - acc: 0.5442
7872/9333 [========================>.....] - ETA: 2:25 - loss: 0.6873 - acc: 0.5446
7936/9333 [========================>.....] - ETA: 2:19 - loss: 0.6873 - acc: 0.5446
8000/9333 [========================>.....] - ETA: 2:12 - loss: 0.6872 - acc: 0.5449
8064/9333 [========================>.....] - ETA: 2:06 - loss: 0.6872 - acc: 0.5446
8128/9333 [=========================>....] - ETA: 1:59 - loss: 0.6873 - acc: 0.5443
8192/9333 [=========================>....] - ETA: 1:53 - loss: 0.6871 - acc: 0.5450
8256/9333 [=========================>....] - ETA: 1:47 - loss: 0.6871 - acc: 0.5449
8320/9333 [=========================>....] - ETA: 1:40 - loss: 0.6870 - acc: 0.5456
8384/9333 [=========================>....] - ETA: 1:34 - loss: 0.6873 - acc: 0.5443
8448/9333 [==========================>...] - ETA: 1:27 - loss: 0.6873 - acc: 0.5443
8512/9333 [==========================>...] - ETA: 1:21 - loss: 0.6874 - acc: 0.5443
8576/9333 [==========================>...] - ETA: 1:15 - loss: 0.6873 - acc: 0.5441
8640/9333 [==========================>...] - ETA: 1:08 - loss: 0.6874 - acc: 0.5443
8704/9333 [==========================>...] - ETA: 1:02 - loss: 0.6875 - acc: 0.5442
8768/9333 [===========================>..] - ETA: 56s - loss: 0.6874 - acc: 0.5446 
8832/9333 [===========================>..] - ETA: 49s - loss: 0.6874 - acc: 0.5452
8896/9333 [===========================>..] - ETA: 43s - loss: 0.6873 - acc: 0.5455
8960/9333 [===========================>..] - ETA: 37s - loss: 0.6876 - acc: 0.5452
9024/9333 [============================>.] - ETA: 30s - loss: 0.6875 - acc: 0.5452
9088/9333 [============================>.] - ETA: 24s - loss: 0.6876 - acc: 0.5450
9152/9333 [============================>.] - ETA: 17s - loss: 0.6878 - acc: 0.5444
9216/9333 [============================>.] - ETA: 11s - loss: 0.6878 - acc: 0.5442
9280/9333 [============================>.] - ETA: 5s - loss: 0.6878 - acc: 0.5443 
9333/9333 [==============================] - 961s 103ms/step - loss: 0.6878 - acc: 0.5449 - val_loss: 0.6959 - val_acc: 0.5198

Epoch 00009: val_acc did not improve from 0.53327
Epoch 10/10

  64/9333 [..............................] - ETA: 15:05 - loss: 0.6628 - acc: 0.6562
 128/9333 [..............................] - ETA: 14:41 - loss: 0.6781 - acc: 0.5625
 192/9333 [..............................] - ETA: 14:50 - loss: 0.6846 - acc: 0.5573
 256/9333 [..............................] - ETA: 15:01 - loss: 0.6880 - acc: 0.5469
 320/9333 [>.............................] - ETA: 14:46 - loss: 0.6854 - acc: 0.5469
 384/9333 [>.............................] - ETA: 14:46 - loss: 0.6861 - acc: 0.5417
 448/9333 [>.............................] - ETA: 14:41 - loss: 0.6864 - acc: 0.5357
 512/9333 [>.............................] - ETA: 14:39 - loss: 0.6861 - acc: 0.5371
 576/9333 [>.............................] - ETA: 14:32 - loss: 0.6879 - acc: 0.5260
 640/9333 [=>............................] - ETA: 14:28 - loss: 0.6864 - acc: 0.5359
 704/9333 [=>............................] - ETA: 14:26 - loss: 0.6849 - acc: 0.5455
 768/9333 [=>............................] - ETA: 14:15 - loss: 0.6844 - acc: 0.5534
 832/9333 [=>............................] - ETA: 14:07 - loss: 0.6849 - acc: 0.5565
 896/9333 [=>............................] - ETA: 14:07 - loss: 0.6832 - acc: 0.5592
 960/9333 [==>...........................] - ETA: 14:02 - loss: 0.6827 - acc: 0.5594
1024/9333 [==>...........................] - ETA: 13:55 - loss: 0.6811 - acc: 0.5625
1088/9333 [==>...........................] - ETA: 13:51 - loss: 0.6818 - acc: 0.5634
1152/9333 [==>...........................] - ETA: 13:45 - loss: 0.6816 - acc: 0.5634
1216/9333 [==>...........................] - ETA: 13:38 - loss: 0.6819 - acc: 0.5641
1280/9333 [===>..........................] - ETA: 13:29 - loss: 0.6829 - acc: 0.5609
1344/9333 [===>..........................] - ETA: 13:25 - loss: 0.6829 - acc: 0.5580
1408/9333 [===>..........................] - ETA: 13:19 - loss: 0.6827 - acc: 0.5561
1472/9333 [===>..........................] - ETA: 13:14 - loss: 0.6823 - acc: 0.5571
1536/9333 [===>..........................] - ETA: 13:06 - loss: 0.6819 - acc: 0.5586
1600/9333 [====>.........................] - ETA: 12:59 - loss: 0.6820 - acc: 0.5581
1664/9333 [====>.........................] - ETA: 12:52 - loss: 0.6823 - acc: 0.5565
1728/9333 [====>.........................] - ETA: 12:46 - loss: 0.6815 - acc: 0.5573
1792/9333 [====>.........................] - ETA: 12:39 - loss: 0.6816 - acc: 0.5558
1856/9333 [====>.........................] - ETA: 12:33 - loss: 0.6807 - acc: 0.5598
1920/9333 [=====>........................] - ETA: 12:26 - loss: 0.6805 - acc: 0.5604
1984/9333 [=====>........................] - ETA: 12:19 - loss: 0.6812 - acc: 0.5580
2048/9333 [=====>........................] - ETA: 12:12 - loss: 0.6806 - acc: 0.5591
2112/9333 [=====>........................] - ETA: 12:05 - loss: 0.6807 - acc: 0.5592
2176/9333 [=====>........................] - ETA: 11:58 - loss: 0.6806 - acc: 0.5588
2240/9333 [======>.......................] - ETA: 11:52 - loss: 0.6819 - acc: 0.5558
2304/9333 [======>.......................] - ETA: 11:46 - loss: 0.6824 - acc: 0.5525
2368/9333 [======>.......................] - ETA: 11:38 - loss: 0.6831 - acc: 0.5507
2432/9333 [======>.......................] - ETA: 11:30 - loss: 0.6827 - acc: 0.5510
2496/9333 [=======>......................] - ETA: 11:23 - loss: 0.6826 - acc: 0.5505
2560/9333 [=======>......................] - ETA: 11:17 - loss: 0.6830 - acc: 0.5488
2624/9333 [=======>......................] - ETA: 11:11 - loss: 0.6832 - acc: 0.5476
2688/9333 [=======>......................] - ETA: 11:04 - loss: 0.6833 - acc: 0.5465
2752/9333 [=======>......................] - ETA: 10:58 - loss: 0.6837 - acc: 0.5451
2816/9333 [========>.....................] - ETA: 10:51 - loss: 0.6838 - acc: 0.5451
2880/9333 [========>.....................] - ETA: 10:45 - loss: 0.6834 - acc: 0.5462
2944/9333 [========>.....................] - ETA: 10:39 - loss: 0.6833 - acc: 0.5462
3008/9333 [========>.....................] - ETA: 10:32 - loss: 0.6834 - acc: 0.5459
3072/9333 [========>.....................] - ETA: 10:25 - loss: 0.6834 - acc: 0.5469
3136/9333 [=========>....................] - ETA: 10:19 - loss: 0.6833 - acc: 0.5472
3200/9333 [=========>....................] - ETA: 10:12 - loss: 0.6834 - acc: 0.5472
3264/9333 [=========>....................] - ETA: 10:06 - loss: 0.6836 - acc: 0.5469
3328/9333 [=========>....................] - ETA: 9:58 - loss: 0.6841 - acc: 0.5460 
3392/9333 [=========>....................] - ETA: 9:52 - loss: 0.6838 - acc: 0.5472
3456/9333 [==========>...................] - ETA: 9:46 - loss: 0.6839 - acc: 0.5475
3520/9333 [==========>...................] - ETA: 9:39 - loss: 0.6840 - acc: 0.5474
3584/9333 [==========>...................] - ETA: 9:33 - loss: 0.6839 - acc: 0.5480
3648/9333 [==========>...................] - ETA: 9:26 - loss: 0.6839 - acc: 0.5480
3712/9333 [==========>...................] - ETA: 9:20 - loss: 0.6841 - acc: 0.5469
3776/9333 [===========>..................] - ETA: 9:14 - loss: 0.6840 - acc: 0.5474
3840/9333 [===========>..................] - ETA: 9:08 - loss: 0.6842 - acc: 0.5474
3904/9333 [===========>..................] - ETA: 9:02 - loss: 0.6847 - acc: 0.5461
3968/9333 [===========>..................] - ETA: 8:55 - loss: 0.6851 - acc: 0.5449
4032/9333 [===========>..................] - ETA: 8:49 - loss: 0.6849 - acc: 0.5444
4096/9333 [============>.................] - ETA: 8:42 - loss: 0.6849 - acc: 0.5444
4160/9333 [============>.................] - ETA: 8:36 - loss: 0.6852 - acc: 0.5428
4224/9333 [============>.................] - ETA: 8:30 - loss: 0.6854 - acc: 0.5421
4288/9333 [============>.................] - ETA: 8:24 - loss: 0.6849 - acc: 0.5441
4352/9333 [============>.................] - ETA: 8:17 - loss: 0.6847 - acc: 0.5443
4416/9333 [=============>................] - ETA: 8:11 - loss: 0.6844 - acc: 0.5448
4480/9333 [=============>................] - ETA: 8:05 - loss: 0.6843 - acc: 0.5455
4544/9333 [=============>................] - ETA: 7:58 - loss: 0.6841 - acc: 0.5464
4608/9333 [=============>................] - ETA: 7:52 - loss: 0.6846 - acc: 0.5447
4672/9333 [==============>...............] - ETA: 7:45 - loss: 0.6846 - acc: 0.5449
4736/9333 [==============>...............] - ETA: 7:39 - loss: 0.6849 - acc: 0.5450
4800/9333 [==============>...............] - ETA: 7:32 - loss: 0.6847 - acc: 0.5456
4864/9333 [==============>...............] - ETA: 7:26 - loss: 0.6846 - acc: 0.5465
4928/9333 [==============>...............] - ETA: 7:20 - loss: 0.6846 - acc: 0.5459
4992/9333 [===============>..............] - ETA: 7:13 - loss: 0.6851 - acc: 0.5439
5056/9333 [===============>..............] - ETA: 7:07 - loss: 0.6851 - acc: 0.5443
5120/9333 [===============>..............] - ETA: 7:01 - loss: 0.6856 - acc: 0.5432
5184/9333 [===============>..............] - ETA: 6:54 - loss: 0.6854 - acc: 0.5432
5248/9333 [===============>..............] - ETA: 6:47 - loss: 0.6853 - acc: 0.5434
5312/9333 [================>.............] - ETA: 6:41 - loss: 0.6852 - acc: 0.5435
5376/9333 [================>.............] - ETA: 6:35 - loss: 0.6854 - acc: 0.5435
5440/9333 [================>.............] - ETA: 6:29 - loss: 0.6857 - acc: 0.5421
5504/9333 [================>.............] - ETA: 6:22 - loss: 0.6860 - acc: 0.5416
5568/9333 [================>.............] - ETA: 6:16 - loss: 0.6860 - acc: 0.5422
5632/9333 [=================>............] - ETA: 6:10 - loss: 0.6861 - acc: 0.5421
5696/9333 [=================>............] - ETA: 6:03 - loss: 0.6863 - acc: 0.5414
5760/9333 [=================>............] - ETA: 5:57 - loss: 0.6864 - acc: 0.5411
5824/9333 [=================>............] - ETA: 5:51 - loss: 0.6865 - acc: 0.5405
5888/9333 [=================>............] - ETA: 5:45 - loss: 0.6863 - acc: 0.5408
5952/9333 [==================>...........] - ETA: 5:38 - loss: 0.6861 - acc: 0.5415
6016/9333 [==================>...........] - ETA: 5:32 - loss: 0.6860 - acc: 0.5417
6080/9333 [==================>...........] - ETA: 5:25 - loss: 0.6859 - acc: 0.5424
6144/9333 [==================>...........] - ETA: 5:19 - loss: 0.6862 - acc: 0.5413
6208/9333 [==================>...........] - ETA: 5:13 - loss: 0.6861 - acc: 0.5416
6272/9333 [===================>..........] - ETA: 5:06 - loss: 0.6861 - acc: 0.5415
6336/9333 [===================>..........] - ETA: 5:00 - loss: 0.6862 - acc: 0.5421
6400/9333 [===================>..........] - ETA: 4:54 - loss: 0.6860 - acc: 0.5427
6464/9333 [===================>..........] - ETA: 4:47 - loss: 0.6863 - acc: 0.5424
6528/9333 [===================>..........] - ETA: 4:41 - loss: 0.6864 - acc: 0.5421
6592/9333 [====================>.........] - ETA: 4:35 - loss: 0.6862 - acc: 0.5425
6656/9333 [====================>.........] - ETA: 4:28 - loss: 0.6864 - acc: 0.5415
6720/9333 [====================>.........] - ETA: 4:22 - loss: 0.6865 - acc: 0.5412
6784/9333 [====================>.........] - ETA: 4:15 - loss: 0.6865 - acc: 0.5410
6848/9333 [=====================>........] - ETA: 4:09 - loss: 0.6866 - acc: 0.5409
6912/9333 [=====================>........] - ETA: 4:02 - loss: 0.6867 - acc: 0.5411
6976/9333 [=====================>........] - ETA: 3:56 - loss: 0.6866 - acc: 0.5417
7040/9333 [=====================>........] - ETA: 3:49 - loss: 0.6866 - acc: 0.5409
7104/9333 [=====================>........] - ETA: 3:43 - loss: 0.6867 - acc: 0.5405
7168/9333 [======================>.......] - ETA: 3:37 - loss: 0.6867 - acc: 0.5405
7232/9333 [======================>.......] - ETA: 3:30 - loss: 0.6869 - acc: 0.5398
7296/9333 [======================>.......] - ETA: 3:24 - loss: 0.6869 - acc: 0.5399
7360/9333 [======================>.......] - ETA: 3:17 - loss: 0.6868 - acc: 0.5398
7424/9333 [======================>.......] - ETA: 3:11 - loss: 0.6869 - acc: 0.5397
7488/9333 [=======================>......] - ETA: 3:04 - loss: 0.6869 - acc: 0.5391
7552/9333 [=======================>......] - ETA: 2:58 - loss: 0.6868 - acc: 0.5393
7616/9333 [=======================>......] - ETA: 2:52 - loss: 0.6868 - acc: 0.5397
7680/9333 [=======================>......] - ETA: 2:45 - loss: 0.6868 - acc: 0.5402
7744/9333 [=======================>......] - ETA: 2:39 - loss: 0.6871 - acc: 0.5396
7808/9333 [========================>.....] - ETA: 2:32 - loss: 0.6870 - acc: 0.5398
7872/9333 [========================>.....] - ETA: 2:26 - loss: 0.6869 - acc: 0.5404
7936/9333 [========================>.....] - ETA: 2:20 - loss: 0.6869 - acc: 0.5411
8000/9333 [========================>.....] - ETA: 2:13 - loss: 0.6869 - acc: 0.5410
8064/9333 [========================>.....] - ETA: 2:07 - loss: 0.6869 - acc: 0.5414
8128/9333 [=========================>....] - ETA: 2:01 - loss: 0.6869 - acc: 0.5416
8192/9333 [=========================>....] - ETA: 1:54 - loss: 0.6865 - acc: 0.5430
8256/9333 [=========================>....] - ETA: 1:48 - loss: 0.6865 - acc: 0.5432
8320/9333 [=========================>....] - ETA: 1:41 - loss: 0.6866 - acc: 0.5427
8384/9333 [=========================>....] - ETA: 1:35 - loss: 0.6868 - acc: 0.5417
8448/9333 [==========================>...] - ETA: 1:29 - loss: 0.6868 - acc: 0.5421
8512/9333 [==========================>...] - ETA: 1:22 - loss: 0.6868 - acc: 0.5418
8576/9333 [==========================>...] - ETA: 1:16 - loss: 0.6867 - acc: 0.5423
8640/9333 [==========================>...] - ETA: 1:09 - loss: 0.6869 - acc: 0.5416
8704/9333 [==========================>...] - ETA: 1:03 - loss: 0.6868 - acc: 0.5417
8768/9333 [===========================>..] - ETA: 56s - loss: 0.6868 - acc: 0.5409 
8832/9333 [===========================>..] - ETA: 50s - loss: 0.6870 - acc: 0.5406
8896/9333 [===========================>..] - ETA: 44s - loss: 0.6869 - acc: 0.5414
8960/9333 [===========================>..] - ETA: 37s - loss: 0.6868 - acc: 0.5420
9024/9333 [============================>.] - ETA: 31s - loss: 0.6868 - acc: 0.5420
9088/9333 [============================>.] - ETA: 24s - loss: 0.6867 - acc: 0.5423
9152/9333 [============================>.] - ETA: 18s - loss: 0.6867 - acc: 0.5425
9216/9333 [============================>.] - ETA: 11s - loss: 0.6867 - acc: 0.5424
9280/9333 [============================>.] - ETA: 5s - loss: 0.6865 - acc: 0.5430 
9333/9333 [==============================] - 979s 105ms/step - loss: 0.6865 - acc: 0.5427 - val_loss: 0.6949 - val_acc: 0.5313

Epoch 00010: val_acc did not improve from 0.53327
样本个数 1296
样本个数 2592
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fad09a98410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fad09a98410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fad09a70710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fad09a70710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad017eb450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad017eb450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad0109b150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad0109b150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad00e4f8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad00e4f8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0109b4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0109b4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00eb0b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00eb0b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0109f890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0109f890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad00f8cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad00f8cf90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad01033810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad01033810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad00e71090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad00e71090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00fc3bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00fc3bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad00e6dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad00e6dfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa700169750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa700169750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad00badd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad00badd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa70015db10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa70015db10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00f0b2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00f0b2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad00bee6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad00bee6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad00a44810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad00a44810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad00851f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad00851f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0088a990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0088a990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00bc5bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00bc5bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad008987d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad008987d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad006a8a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad006a8a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad0063fa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad0063fa50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1365af50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fac1365af50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00441d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00441d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad005ac610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad005ac610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad0034d150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad0034d150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad0020cb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad0020cb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad00369b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad00369b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00455050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad00455050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0033a890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad0033a890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad005680d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fad005680d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad0010f910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fad0010f910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facf7fd0690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facf7fd0690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad009c9250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fad009c9250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facf7f295d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facf7f295d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7facf7ef6c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7facf7ef6c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7facf7d2e710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7facf7d2e710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facf7d55890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facf7d55890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7facf7e233d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7facf7e233d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facf7c91810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facf7c91810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7facf7d11390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7facf7d11390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7facf7ca08d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7facf7ca08d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad00c99350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fad00c99350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7facf7cf7d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7facf7cf7d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef7ee410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef7ee410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7facef6e5b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7facef6e5b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7facef596fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7facef596fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef6e4090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef6e4090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7facef6e5d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7facef6e5d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef656b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef656b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7facef5e3e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7facef5e3e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7facef3a43d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7facef3a43d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef27f550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef27f550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7facef5f9890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7facef5f9890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef13cf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef13cf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7facef08bb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7facef08bb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7faceefbd550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7faceefbd550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef319cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7facef319cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7facef08b590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7facef08b590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7faceee36b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7faceee36b90>>: AttributeError: module 'gast' has no attribute 'Str'
window_select17-21.py:152: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/2592 [..............................] - ETA: 4:03
 128/2592 [>.............................] - ETA: 2:39
 192/2592 [=>............................] - ETA: 2:08
 256/2592 [=>............................] - ETA: 1:52
 320/2592 [==>...........................] - ETA: 1:41
 384/2592 [===>..........................] - ETA: 1:32
 448/2592 [====>.........................] - ETA: 1:27
 512/2592 [====>.........................] - ETA: 1:22
 576/2592 [=====>........................] - ETA: 1:17
 640/2592 [======>.......................] - ETA: 1:13
 704/2592 [=======>......................] - ETA: 1:09
 768/2592 [=======>......................] - ETA: 1:06
 832/2592 [========>.....................] - ETA: 1:03
 896/2592 [=========>....................] - ETA: 1:00
 960/2592 [==========>...................] - ETA: 57s 
1024/2592 [==========>...................] - ETA: 54s
1088/2592 [===========>..................] - ETA: 52s
1152/2592 [============>.................] - ETA: 50s
1216/2592 [=============>................] - ETA: 48s
1280/2592 [=============>................] - ETA: 45s
1344/2592 [==============>...............] - ETA: 43s
1408/2592 [===============>..............] - ETA: 40s
1472/2592 [================>.............] - ETA: 38s
1536/2592 [================>.............] - ETA: 36s
1600/2592 [=================>............] - ETA: 34s
1664/2592 [==================>...........] - ETA: 31s
1728/2592 [===================>..........] - ETA: 29s
1792/2592 [===================>..........] - ETA: 27s
1856/2592 [====================>.........] - ETA: 24s
1920/2592 [=====================>........] - ETA: 22s
1984/2592 [=====================>........] - ETA: 20s
2048/2592 [======================>.......] - ETA: 18s
2112/2592 [=======================>......] - ETA: 16s
2176/2592 [========================>.....] - ETA: 13s
2240/2592 [========================>.....] - ETA: 11s
2304/2592 [=========================>....] - ETA: 9s 
2368/2592 [==========================>...] - ETA: 7s
2432/2592 [===========================>..] - ETA: 5s
2496/2592 [===========================>..] - ETA: 3s
2560/2592 [============================>.] - ETA: 1s
2592/2592 [==============================] - 87s 34ms/step
loss: 0.6878975960943434
acc: 0.5439814814814815
