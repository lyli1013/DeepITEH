nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2694
样本个数 5388
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f320f18cd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f320f18cd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f3285698550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f3285698550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f328565e990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f328565e990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f328565e650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f328565e650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3285652610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3285652610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32856988d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32856988d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f328565efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f328565efd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f12ae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f12ae50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f320f111790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f320f111790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31feeca310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31feeca310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31ff056e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31ff056e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31ff05b490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31ff05b490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3285867d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3285867d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31fedae350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31fedae350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31feb6f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31feb6f1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fea62450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fea62450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31feb682d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31feb682d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31feeee110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31feeee110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31fea824d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31fea824d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31fe730850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31fe730850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe62afd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe62afd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31fe814ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31fe814ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe77acd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe77acd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31fe529350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31fe529350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31fe58b5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31fe58b5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe517e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe517e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f320f12c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f320f12c1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe34f410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe34f410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31fe1d4790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31fe1d4790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31fe21ac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31fe21ac90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe2c3650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe2c3650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31fe4addd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31fe4addd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe1d7210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe1d7210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31f5ee1f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31f5ee1f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31fe0eac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31fe0eac90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31f5fe6510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31f5fe6510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31f5ee18d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31f5ee18d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe122dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31fe122dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31f5ddcad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31f5ddcad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31f5aa7d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31f5aa7d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31f59a5bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31f59a5bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31fec14890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31fec14890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31f5af4e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31f5af4e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31f5ad9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31f5ad9e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31f58a3450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31f58a3450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31ed7bf450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31ed7bf450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31f5b6f810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31f5b6f810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31ed6ac2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31ed6ac2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31ed551050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31ed551050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31ed477e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31ed477e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31f5893c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31f5893c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31ed7957d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31ed7957d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31f5b98b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31f5b98b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31ed53e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31ed53e1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31ed150e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31ed150e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31ed4a3a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31ed4a3a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31fefb7990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31fefb7990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31ed2be410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31ed2be410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31e4f75d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31e4f75d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31e4f8e110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31e4f8e110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31e507e590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31e507e590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31e4f758d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31e4f758d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31e4e1a110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31e4e1a110>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-26 19:02:26.220513: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-26 19:02:26.303917: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-26 19:02:26.372440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55639fe57250 executing computations on platform Host. Devices:
2022-11-26 19:02:26.372541: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-26 19:02:27.943715: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 18:57 - loss: 0.7726 - acc: 0.4219
 128/4849 [..............................] - ETA: 13:08 - loss: 0.8834 - acc: 0.4297
 192/4849 [>.............................] - ETA: 10:59 - loss: 0.8484 - acc: 0.4479
 256/4849 [>.............................] - ETA: 9:54 - loss: 0.8161 - acc: 0.4805 
 320/4849 [>.............................] - ETA: 9:14 - loss: 0.8043 - acc: 0.4906
 384/4849 [=>............................] - ETA: 8:49 - loss: 0.7915 - acc: 0.4844
 448/4849 [=>............................] - ETA: 8:28 - loss: 0.7810 - acc: 0.4955
 512/4849 [==>...........................] - ETA: 8:17 - loss: 0.7772 - acc: 0.4902
 576/4849 [==>...........................] - ETA: 8:03 - loss: 0.7699 - acc: 0.4983
 640/4849 [==>...........................] - ETA: 7:44 - loss: 0.7626 - acc: 0.5000
 704/4849 [===>..........................] - ETA: 7:36 - loss: 0.7567 - acc: 0.5000
 768/4849 [===>..........................] - ETA: 7:25 - loss: 0.7498 - acc: 0.5091
 832/4849 [====>.........................] - ETA: 7:12 - loss: 0.7460 - acc: 0.5132
 896/4849 [====>.........................] - ETA: 6:58 - loss: 0.7436 - acc: 0.5156
 960/4849 [====>.........................] - ETA: 6:47 - loss: 0.7391 - acc: 0.5177
1024/4849 [=====>........................] - ETA: 6:36 - loss: 0.7364 - acc: 0.5215
1088/4849 [=====>........................] - ETA: 6:26 - loss: 0.7357 - acc: 0.5211
1152/4849 [======>.......................] - ETA: 6:17 - loss: 0.7390 - acc: 0.5191
1216/4849 [======>.......................] - ETA: 6:08 - loss: 0.7337 - acc: 0.5255
1280/4849 [======>.......................] - ETA: 5:58 - loss: 0.7315 - acc: 0.5297
1344/4849 [=======>......................] - ETA: 5:49 - loss: 0.7278 - acc: 0.5342
1408/4849 [=======>......................] - ETA: 5:43 - loss: 0.7307 - acc: 0.5334
1472/4849 [========>.....................] - ETA: 5:35 - loss: 0.7314 - acc: 0.5333
1536/4849 [========>.....................] - ETA: 5:27 - loss: 0.7277 - acc: 0.5352
1600/4849 [========>.....................] - ETA: 5:20 - loss: 0.7264 - acc: 0.5375
1664/4849 [=========>....................] - ETA: 5:13 - loss: 0.7265 - acc: 0.5373
1728/4849 [=========>....................] - ETA: 5:06 - loss: 0.7285 - acc: 0.5336
1792/4849 [==========>...................] - ETA: 4:58 - loss: 0.7292 - acc: 0.5301
1856/4849 [==========>...................] - ETA: 4:51 - loss: 0.7306 - acc: 0.5264
1920/4849 [==========>...................] - ETA: 4:44 - loss: 0.7307 - acc: 0.5245
1984/4849 [===========>..................] - ETA: 4:37 - loss: 0.7305 - acc: 0.5247
2048/4849 [===========>..................] - ETA: 4:30 - loss: 0.7306 - acc: 0.5239
2112/4849 [============>.................] - ETA: 4:24 - loss: 0.7289 - acc: 0.5265
2176/4849 [============>.................] - ETA: 4:16 - loss: 0.7282 - acc: 0.5262
2240/4849 [============>.................] - ETA: 4:10 - loss: 0.7270 - acc: 0.5259
2304/4849 [=============>................] - ETA: 4:04 - loss: 0.7270 - acc: 0.5252
2368/4849 [=============>................] - ETA: 3:57 - loss: 0.7270 - acc: 0.5224
2432/4849 [==============>...............] - ETA: 3:50 - loss: 0.7254 - acc: 0.5230
2496/4849 [==============>...............] - ETA: 3:44 - loss: 0.7248 - acc: 0.5232
2560/4849 [==============>...............] - ETA: 3:37 - loss: 0.7251 - acc: 0.5199
2624/4849 [===============>..............] - ETA: 3:31 - loss: 0.7247 - acc: 0.5194
2688/4849 [===============>..............] - ETA: 3:24 - loss: 0.7254 - acc: 0.5186
2752/4849 [================>.............] - ETA: 3:18 - loss: 0.7239 - acc: 0.5196
2816/4849 [================>.............] - ETA: 3:11 - loss: 0.7232 - acc: 0.5181
2880/4849 [================>.............] - ETA: 3:05 - loss: 0.7235 - acc: 0.5170
2944/4849 [=================>............] - ETA: 2:59 - loss: 0.7230 - acc: 0.5183
3008/4849 [=================>............] - ETA: 2:53 - loss: 0.7231 - acc: 0.5186
3072/4849 [==================>...........] - ETA: 2:46 - loss: 0.7232 - acc: 0.5176
3136/4849 [==================>...........] - ETA: 2:40 - loss: 0.7221 - acc: 0.5201
3200/4849 [==================>...........] - ETA: 2:33 - loss: 0.7226 - acc: 0.5178
3264/4849 [===================>..........] - ETA: 2:27 - loss: 0.7229 - acc: 0.5169
3328/4849 [===================>..........] - ETA: 2:21 - loss: 0.7222 - acc: 0.5183
3392/4849 [===================>..........] - ETA: 2:15 - loss: 0.7220 - acc: 0.5186
3456/4849 [====================>.........] - ETA: 2:09 - loss: 0.7211 - acc: 0.5203
3520/4849 [====================>.........] - ETA: 2:03 - loss: 0.7201 - acc: 0.5227
3584/4849 [=====================>........] - ETA: 1:57 - loss: 0.7209 - acc: 0.5215
3648/4849 [=====================>........] - ETA: 1:51 - loss: 0.7202 - acc: 0.5222
3712/4849 [=====================>........] - ETA: 1:45 - loss: 0.7196 - acc: 0.5226
3776/4849 [======================>.......] - ETA: 1:39 - loss: 0.7187 - acc: 0.5233
3840/4849 [======================>.......] - ETA: 1:33 - loss: 0.7180 - acc: 0.5253
3904/4849 [=======================>......] - ETA: 1:27 - loss: 0.7184 - acc: 0.5243
3968/4849 [=======================>......] - ETA: 1:21 - loss: 0.7191 - acc: 0.5217
4032/4849 [=======================>......] - ETA: 1:15 - loss: 0.7188 - acc: 0.5208
4096/4849 [========================>.....] - ETA: 1:09 - loss: 0.7191 - acc: 0.5200
4160/4849 [========================>.....] - ETA: 1:03 - loss: 0.7193 - acc: 0.5192
4224/4849 [=========================>....] - ETA: 57s - loss: 0.7191 - acc: 0.5194 
4288/4849 [=========================>....] - ETA: 51s - loss: 0.7189 - acc: 0.5194
4352/4849 [=========================>....] - ETA: 45s - loss: 0.7185 - acc: 0.5198
4416/4849 [==========================>...] - ETA: 39s - loss: 0.7188 - acc: 0.5195
4480/4849 [==========================>...] - ETA: 33s - loss: 0.7193 - acc: 0.5185
4544/4849 [===========================>..] - ETA: 27s - loss: 0.7183 - acc: 0.5198
4608/4849 [===========================>..] - ETA: 22s - loss: 0.7180 - acc: 0.5191
4672/4849 [===========================>..] - ETA: 16s - loss: 0.7173 - acc: 0.5205
4736/4849 [============================>.] - ETA: 10s - loss: 0.7175 - acc: 0.5198
4800/4849 [============================>.] - ETA: 4s - loss: 0.7169 - acc: 0.5212 
4849/4849 [==============================] - 461s 95ms/step - loss: 0.7165 - acc: 0.5216 - val_loss: 0.6912 - val_acc: 0.5009

Epoch 00001: val_acc improved from -inf to 0.50093, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window01/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 7:40 - loss: 0.6697 - acc: 0.5625
 128/4849 [..............................] - ETA: 7:11 - loss: 0.6711 - acc: 0.5781
 192/4849 [>.............................] - ETA: 6:54 - loss: 0.6810 - acc: 0.5625
 256/4849 [>.............................] - ETA: 6:51 - loss: 0.6842 - acc: 0.5625
 320/4849 [>.............................] - ETA: 6:47 - loss: 0.6785 - acc: 0.5687
 384/4849 [=>............................] - ETA: 6:39 - loss: 0.6729 - acc: 0.5807
 448/4849 [=>............................] - ETA: 6:31 - loss: 0.6731 - acc: 0.5893
 512/4849 [==>...........................] - ETA: 6:21 - loss: 0.6745 - acc: 0.5898
 576/4849 [==>...........................] - ETA: 6:13 - loss: 0.6773 - acc: 0.5764
 640/4849 [==>...........................] - ETA: 6:08 - loss: 0.6818 - acc: 0.5625
 704/4849 [===>..........................] - ETA: 6:03 - loss: 0.6808 - acc: 0.5668
 768/4849 [===>..........................] - ETA: 5:55 - loss: 0.6817 - acc: 0.5612
 832/4849 [====>.........................] - ETA: 5:47 - loss: 0.6852 - acc: 0.5565
 896/4849 [====>.........................] - ETA: 5:46 - loss: 0.6878 - acc: 0.5502
 960/4849 [====>.........................] - ETA: 5:40 - loss: 0.6886 - acc: 0.5521
1024/4849 [=====>........................] - ETA: 5:38 - loss: 0.6883 - acc: 0.5527
1088/4849 [=====>........................] - ETA: 5:35 - loss: 0.6896 - acc: 0.5515
1152/4849 [======>.......................] - ETA: 5:26 - loss: 0.6896 - acc: 0.5477
1216/4849 [======>.......................] - ETA: 5:20 - loss: 0.6921 - acc: 0.5428
1280/4849 [======>.......................] - ETA: 5:14 - loss: 0.6937 - acc: 0.5391
1344/4849 [=======>......................] - ETA: 5:09 - loss: 0.6950 - acc: 0.5372
1408/4849 [=======>......................] - ETA: 5:02 - loss: 0.6951 - acc: 0.5369
1472/4849 [========>.....................] - ETA: 4:57 - loss: 0.6959 - acc: 0.5374
1536/4849 [========>.....................] - ETA: 4:50 - loss: 0.6963 - acc: 0.5332
1600/4849 [========>.....................] - ETA: 4:45 - loss: 0.6959 - acc: 0.5319
1664/4849 [=========>....................] - ETA: 4:39 - loss: 0.6959 - acc: 0.5319
1728/4849 [=========>....................] - ETA: 4:32 - loss: 0.6948 - acc: 0.5336
1792/4849 [==========>...................] - ETA: 4:26 - loss: 0.6936 - acc: 0.5363
1856/4849 [==========>...................] - ETA: 4:21 - loss: 0.6941 - acc: 0.5329
1920/4849 [==========>...................] - ETA: 4:16 - loss: 0.6946 - acc: 0.5297
1984/4849 [===========>..................] - ETA: 4:13 - loss: 0.6947 - acc: 0.5292
2048/4849 [===========>..................] - ETA: 4:08 - loss: 0.6944 - acc: 0.5308
2112/4849 [============>.................] - ETA: 4:04 - loss: 0.6957 - acc: 0.5279
2176/4849 [============>.................] - ETA: 3:58 - loss: 0.6975 - acc: 0.5262
2240/4849 [============>.................] - ETA: 3:52 - loss: 0.6972 - acc: 0.5295
2304/4849 [=============>................] - ETA: 3:46 - loss: 0.6956 - acc: 0.5321
2368/4849 [=============>................] - ETA: 3:40 - loss: 0.6952 - acc: 0.5321
2432/4849 [==============>...............] - ETA: 3:35 - loss: 0.6957 - acc: 0.5317
2496/4849 [==============>...............] - ETA: 3:29 - loss: 0.6955 - acc: 0.5308
2560/4849 [==============>...............] - ETA: 3:24 - loss: 0.6955 - acc: 0.5301
2624/4849 [===============>..............] - ETA: 3:18 - loss: 0.6946 - acc: 0.5320
2688/4849 [===============>..............] - ETA: 3:12 - loss: 0.6960 - acc: 0.5283
2752/4849 [================>.............] - ETA: 3:07 - loss: 0.6966 - acc: 0.5269
2816/4849 [================>.............] - ETA: 3:01 - loss: 0.6972 - acc: 0.5245
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6969 - acc: 0.5240
2944/4849 [=================>............] - ETA: 2:50 - loss: 0.6969 - acc: 0.5241
3008/4849 [=================>............] - ETA: 2:44 - loss: 0.6975 - acc: 0.5236
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6978 - acc: 0.5225
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6968 - acc: 0.5242
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6964 - acc: 0.5241
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6959 - acc: 0.5267
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6954 - acc: 0.5276
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6946 - acc: 0.5295
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6945 - acc: 0.5301
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6944 - acc: 0.5304
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6939 - acc: 0.5321
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6935 - acc: 0.5332
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6935 - acc: 0.5329
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6934 - acc: 0.5328
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6927 - acc: 0.5341
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6927 - acc: 0.5346
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6929 - acc: 0.5350
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6929 - acc: 0.5347
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6931 - acc: 0.5342
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6929 - acc: 0.5346
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6936 - acc: 0.5331 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6936 - acc: 0.5329
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6935 - acc: 0.5333
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6938 - acc: 0.5326
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6938 - acc: 0.5326
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6940 - acc: 0.5330
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6940 - acc: 0.5332
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6935 - acc: 0.5345
4736/4849 [============================>.] - ETA: 10s - loss: 0.6932 - acc: 0.5351
4800/4849 [============================>.] - ETA: 4s - loss: 0.6927 - acc: 0.5360 
4849/4849 [==============================] - 452s 93ms/step - loss: 0.6925 - acc: 0.5358 - val_loss: 0.6864 - val_acc: 0.5492

Epoch 00002: val_acc improved from 0.50093 to 0.54917, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window01/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 3/10

  64/4849 [..............................] - ETA: 7:24 - loss: 0.7079 - acc: 0.5000
 128/4849 [..............................] - ETA: 7:06 - loss: 0.6983 - acc: 0.5312
 192/4849 [>.............................] - ETA: 6:59 - loss: 0.7156 - acc: 0.4740
 256/4849 [>.............................] - ETA: 6:47 - loss: 0.7066 - acc: 0.4844
 320/4849 [>.............................] - ETA: 6:38 - loss: 0.7040 - acc: 0.4969
 384/4849 [=>............................] - ETA: 6:32 - loss: 0.7094 - acc: 0.4844
 448/4849 [=>............................] - ETA: 6:28 - loss: 0.7094 - acc: 0.4866
 512/4849 [==>...........................] - ETA: 6:16 - loss: 0.7029 - acc: 0.5000
 576/4849 [==>...........................] - ETA: 6:13 - loss: 0.7006 - acc: 0.5052
 640/4849 [==>...........................] - ETA: 6:09 - loss: 0.6972 - acc: 0.5109
 704/4849 [===>..........................] - ETA: 6:01 - loss: 0.6960 - acc: 0.5170
 768/4849 [===>..........................] - ETA: 5:55 - loss: 0.6931 - acc: 0.5143
 832/4849 [====>.........................] - ETA: 5:50 - loss: 0.6932 - acc: 0.5156
 896/4849 [====>.........................] - ETA: 5:44 - loss: 0.6937 - acc: 0.5145
 960/4849 [====>.........................] - ETA: 5:39 - loss: 0.6904 - acc: 0.5250
1024/4849 [=====>........................] - ETA: 5:33 - loss: 0.6916 - acc: 0.5225
1088/4849 [=====>........................] - ETA: 5:26 - loss: 0.6907 - acc: 0.5230
1152/4849 [======>.......................] - ETA: 5:20 - loss: 0.6909 - acc: 0.5234
1216/4849 [======>.......................] - ETA: 5:14 - loss: 0.6917 - acc: 0.5230
1280/4849 [======>.......................] - ETA: 5:07 - loss: 0.6910 - acc: 0.5219
1344/4849 [=======>......................] - ETA: 5:02 - loss: 0.6927 - acc: 0.5223
1408/4849 [=======>......................] - ETA: 4:58 - loss: 0.6931 - acc: 0.5220
1472/4849 [========>.....................] - ETA: 4:51 - loss: 0.6936 - acc: 0.5204
1536/4849 [========>.....................] - ETA: 4:45 - loss: 0.6948 - acc: 0.5189
1600/4849 [========>.....................] - ETA: 4:42 - loss: 0.6944 - acc: 0.5188
1664/4849 [=========>....................] - ETA: 4:36 - loss: 0.6946 - acc: 0.5186
1728/4849 [=========>....................] - ETA: 4:30 - loss: 0.6948 - acc: 0.5191
1792/4849 [==========>...................] - ETA: 4:25 - loss: 0.6947 - acc: 0.5201
1856/4849 [==========>...................] - ETA: 4:19 - loss: 0.6943 - acc: 0.5216
1920/4849 [==========>...................] - ETA: 4:13 - loss: 0.6938 - acc: 0.5229
1984/4849 [===========>..................] - ETA: 4:08 - loss: 0.6925 - acc: 0.5257
2048/4849 [===========>..................] - ETA: 4:04 - loss: 0.6914 - acc: 0.5283
2112/4849 [============>.................] - ETA: 3:58 - loss: 0.6912 - acc: 0.5303
2176/4849 [============>.................] - ETA: 3:52 - loss: 0.6907 - acc: 0.5317
2240/4849 [============>.................] - ETA: 3:46 - loss: 0.6912 - acc: 0.5321
2304/4849 [=============>................] - ETA: 3:40 - loss: 0.6912 - acc: 0.5343
2368/4849 [=============>................] - ETA: 3:35 - loss: 0.6924 - acc: 0.5346
2432/4849 [==============>...............] - ETA: 3:29 - loss: 0.6909 - acc: 0.5362
2496/4849 [==============>...............] - ETA: 3:24 - loss: 0.6906 - acc: 0.5369
2560/4849 [==============>...............] - ETA: 3:18 - loss: 0.6915 - acc: 0.5367
2624/4849 [===============>..............] - ETA: 3:12 - loss: 0.6921 - acc: 0.5354
2688/4849 [===============>..............] - ETA: 3:07 - loss: 0.6914 - acc: 0.5368
2752/4849 [================>.............] - ETA: 3:01 - loss: 0.6915 - acc: 0.5371
2816/4849 [================>.............] - ETA: 2:56 - loss: 0.6907 - acc: 0.5391
2880/4849 [================>.............] - ETA: 2:50 - loss: 0.6907 - acc: 0.5385
2944/4849 [=================>............] - ETA: 2:44 - loss: 0.6908 - acc: 0.5384
3008/4849 [=================>............] - ETA: 2:39 - loss: 0.6912 - acc: 0.5376
3072/4849 [==================>...........] - ETA: 2:33 - loss: 0.6910 - acc: 0.5387
3136/4849 [==================>...........] - ETA: 2:28 - loss: 0.6914 - acc: 0.5392
3200/4849 [==================>...........] - ETA: 2:22 - loss: 0.6918 - acc: 0.5381
3264/4849 [===================>..........] - ETA: 2:17 - loss: 0.6920 - acc: 0.5377
3328/4849 [===================>..........] - ETA: 2:11 - loss: 0.6915 - acc: 0.5397
3392/4849 [===================>..........] - ETA: 2:06 - loss: 0.6914 - acc: 0.5404
3456/4849 [====================>.........] - ETA: 2:00 - loss: 0.6898 - acc: 0.5425
3520/4849 [====================>.........] - ETA: 1:55 - loss: 0.6906 - acc: 0.5398
3584/4849 [=====================>........] - ETA: 1:49 - loss: 0.6895 - acc: 0.5419
3648/4849 [=====================>........] - ETA: 1:44 - loss: 0.6894 - acc: 0.5430
3712/4849 [=====================>........] - ETA: 1:38 - loss: 0.6886 - acc: 0.5450
3776/4849 [======================>.......] - ETA: 1:33 - loss: 0.6879 - acc: 0.5469
3840/4849 [======================>.......] - ETA: 1:27 - loss: 0.6871 - acc: 0.5482
3904/4849 [=======================>......] - ETA: 1:22 - loss: 0.6866 - acc: 0.5494
3968/4849 [=======================>......] - ETA: 1:16 - loss: 0.6876 - acc: 0.5479
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6875 - acc: 0.5489
4096/4849 [========================>.....] - ETA: 1:05 - loss: 0.6874 - acc: 0.5488
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6868 - acc: 0.5495
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6871 - acc: 0.5495 
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6860 - acc: 0.5515
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6861 - acc: 0.5517
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6865 - acc: 0.5514
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6864 - acc: 0.5520
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6858 - acc: 0.5533
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6869 - acc: 0.5525
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6864 - acc: 0.5527
4736/4849 [============================>.] - ETA: 9s - loss: 0.6864 - acc: 0.5536 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6869 - acc: 0.5531
4849/4849 [==============================] - 441s 91ms/step - loss: 0.6874 - acc: 0.5529 - val_loss: 0.6952 - val_acc: 0.5788

Epoch 00003: val_acc improved from 0.54917 to 0.57885, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window01/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 4/10

  64/4849 [..............................] - ETA: 7:01 - loss: 0.6686 - acc: 0.5938
 128/4849 [..............................] - ETA: 7:12 - loss: 0.6769 - acc: 0.5469
 192/4849 [>.............................] - ETA: 7:11 - loss: 0.6923 - acc: 0.5208
 256/4849 [>.............................] - ETA: 7:12 - loss: 0.6945 - acc: 0.5117
 320/4849 [>.............................] - ETA: 6:53 - loss: 0.6941 - acc: 0.5188
 384/4849 [=>............................] - ETA: 6:42 - loss: 0.6965 - acc: 0.5208
 448/4849 [=>............................] - ETA: 6:40 - loss: 0.6876 - acc: 0.5357
 512/4849 [==>...........................] - ETA: 6:33 - loss: 0.6864 - acc: 0.5430
 576/4849 [==>...........................] - ETA: 6:27 - loss: 0.6836 - acc: 0.5538
 640/4849 [==>...........................] - ETA: 6:20 - loss: 0.6833 - acc: 0.5547
 704/4849 [===>..........................] - ETA: 6:13 - loss: 0.6817 - acc: 0.5611
 768/4849 [===>..........................] - ETA: 6:07 - loss: 0.6867 - acc: 0.5482
 832/4849 [====>.........................] - ETA: 6:03 - loss: 0.6856 - acc: 0.5529
 896/4849 [====>.........................] - ETA: 5:57 - loss: 0.6840 - acc: 0.5525
 960/4849 [====>.........................] - ETA: 5:50 - loss: 0.6816 - acc: 0.5573
1024/4849 [=====>........................] - ETA: 5:43 - loss: 0.6787 - acc: 0.5645
1088/4849 [=====>........................] - ETA: 5:37 - loss: 0.6799 - acc: 0.5662
1152/4849 [======>.......................] - ETA: 5:31 - loss: 0.6802 - acc: 0.5668
1216/4849 [======>.......................] - ETA: 5:25 - loss: 0.6784 - acc: 0.5699
1280/4849 [======>.......................] - ETA: 5:19 - loss: 0.6779 - acc: 0.5711
1344/4849 [=======>......................] - ETA: 5:13 - loss: 0.6796 - acc: 0.5692
1408/4849 [=======>......................] - ETA: 5:06 - loss: 0.6800 - acc: 0.5682
1472/4849 [========>.....................] - ETA: 5:02 - loss: 0.6805 - acc: 0.5659
1536/4849 [========>.....................] - ETA: 4:55 - loss: 0.6803 - acc: 0.5645
1600/4849 [========>.....................] - ETA: 4:49 - loss: 0.6784 - acc: 0.5694
1664/4849 [=========>....................] - ETA: 4:44 - loss: 0.6785 - acc: 0.5673
1728/4849 [=========>....................] - ETA: 4:38 - loss: 0.6786 - acc: 0.5689
1792/4849 [==========>...................] - ETA: 4:33 - loss: 0.6787 - acc: 0.5698
1856/4849 [==========>...................] - ETA: 4:26 - loss: 0.6777 - acc: 0.5754
1920/4849 [==========>...................] - ETA: 4:21 - loss: 0.6789 - acc: 0.5729
1984/4849 [===========>..................] - ETA: 4:15 - loss: 0.6780 - acc: 0.5741
2048/4849 [===========>..................] - ETA: 4:09 - loss: 0.6776 - acc: 0.5752
2112/4849 [============>.................] - ETA: 4:04 - loss: 0.6780 - acc: 0.5753
2176/4849 [============>.................] - ETA: 3:58 - loss: 0.6775 - acc: 0.5758
2240/4849 [============>.................] - ETA: 3:52 - loss: 0.6775 - acc: 0.5763
2304/4849 [=============>................] - ETA: 3:47 - loss: 0.6781 - acc: 0.5751
2368/4849 [=============>................] - ETA: 3:41 - loss: 0.6763 - acc: 0.5785
2432/4849 [==============>...............] - ETA: 3:35 - loss: 0.6764 - acc: 0.5785
2496/4849 [==============>...............] - ETA: 3:29 - loss: 0.6773 - acc: 0.5777
2560/4849 [==============>...............] - ETA: 3:23 - loss: 0.6766 - acc: 0.5789
2624/4849 [===============>..............] - ETA: 3:18 - loss: 0.6750 - acc: 0.5812
2688/4849 [===============>..............] - ETA: 3:13 - loss: 0.6747 - acc: 0.5804
2752/4849 [================>.............] - ETA: 3:07 - loss: 0.6754 - acc: 0.5789
2816/4849 [================>.............] - ETA: 3:01 - loss: 0.6761 - acc: 0.5771
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6755 - acc: 0.5792
2944/4849 [=================>............] - ETA: 2:50 - loss: 0.6754 - acc: 0.5802
3008/4849 [=================>............] - ETA: 2:44 - loss: 0.6758 - acc: 0.5808
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6754 - acc: 0.5807
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6765 - acc: 0.5791
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6763 - acc: 0.5787
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6760 - acc: 0.5794
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6766 - acc: 0.5790
3392/4849 [===================>..........] - ETA: 2:08 - loss: 0.6778 - acc: 0.5784
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6777 - acc: 0.5796
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6779 - acc: 0.5795
3584/4849 [=====================>........] - ETA: 1:51 - loss: 0.6784 - acc: 0.5790
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6786 - acc: 0.5787
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6786 - acc: 0.5779
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6790 - acc: 0.5771
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6790 - acc: 0.5773
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6795 - acc: 0.5756
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6801 - acc: 0.5743
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6807 - acc: 0.5729
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6805 - acc: 0.5730
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6809 - acc: 0.5709
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6805 - acc: 0.5715 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6809 - acc: 0.5690
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6813 - acc: 0.5666
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6813 - acc: 0.5668
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6819 - acc: 0.5656
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6823 - acc: 0.5649
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6822 - acc: 0.5660
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6824 - acc: 0.5653
4736/4849 [============================>.] - ETA: 9s - loss: 0.6823 - acc: 0.5652 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6827 - acc: 0.5646
4849/4849 [==============================] - 444s 92ms/step - loss: 0.6827 - acc: 0.5642 - val_loss: 0.6866 - val_acc: 0.5659

Epoch 00004: val_acc did not improve from 0.57885
Epoch 5/10

  64/4849 [..............................] - ETA: 7:31 - loss: 0.6891 - acc: 0.5781
 128/4849 [..............................] - ETA: 6:55 - loss: 0.6855 - acc: 0.5703
 192/4849 [>.............................] - ETA: 7:10 - loss: 0.6855 - acc: 0.5938
 256/4849 [>.............................] - ETA: 6:59 - loss: 0.6759 - acc: 0.6016
 320/4849 [>.............................] - ETA: 6:47 - loss: 0.6740 - acc: 0.6094
 384/4849 [=>............................] - ETA: 6:37 - loss: 0.6796 - acc: 0.5833
 448/4849 [=>............................] - ETA: 6:30 - loss: 0.6839 - acc: 0.5714
 512/4849 [==>...........................] - ETA: 6:24 - loss: 0.6855 - acc: 0.5625
 576/4849 [==>...........................] - ETA: 6:20 - loss: 0.6875 - acc: 0.5573
 640/4849 [==>...........................] - ETA: 6:16 - loss: 0.6890 - acc: 0.5578
 704/4849 [===>..........................] - ETA: 6:12 - loss: 0.6845 - acc: 0.5639
 768/4849 [===>..........................] - ETA: 6:05 - loss: 0.6839 - acc: 0.5664
 832/4849 [====>.........................] - ETA: 6:00 - loss: 0.6842 - acc: 0.5685
 896/4849 [====>.........................] - ETA: 5:51 - loss: 0.6801 - acc: 0.5770
 960/4849 [====>.........................] - ETA: 5:46 - loss: 0.6796 - acc: 0.5823
1024/4849 [=====>........................] - ETA: 5:40 - loss: 0.6793 - acc: 0.5781
1088/4849 [=====>........................] - ETA: 5:34 - loss: 0.6791 - acc: 0.5800
1152/4849 [======>.......................] - ETA: 5:28 - loss: 0.6772 - acc: 0.5825
1216/4849 [======>.......................] - ETA: 5:23 - loss: 0.6781 - acc: 0.5773
1280/4849 [======>.......................] - ETA: 5:16 - loss: 0.6770 - acc: 0.5750
1344/4849 [=======>......................] - ETA: 5:10 - loss: 0.6766 - acc: 0.5737
1408/4849 [=======>......................] - ETA: 5:04 - loss: 0.6773 - acc: 0.5732
1472/4849 [========>.....................] - ETA: 4:58 - loss: 0.6770 - acc: 0.5727
1536/4849 [========>.....................] - ETA: 4:52 - loss: 0.6762 - acc: 0.5729
1600/4849 [========>.....................] - ETA: 4:48 - loss: 0.6742 - acc: 0.5769
1664/4849 [=========>....................] - ETA: 4:43 - loss: 0.6751 - acc: 0.5763
1728/4849 [=========>....................] - ETA: 4:36 - loss: 0.6752 - acc: 0.5741
1792/4849 [==========>...................] - ETA: 4:31 - loss: 0.6731 - acc: 0.5781
1856/4849 [==========>...................] - ETA: 4:25 - loss: 0.6739 - acc: 0.5776
1920/4849 [==========>...................] - ETA: 4:19 - loss: 0.6741 - acc: 0.5771
1984/4849 [===========>..................] - ETA: 4:13 - loss: 0.6750 - acc: 0.5756
2048/4849 [===========>..................] - ETA: 4:08 - loss: 0.6757 - acc: 0.5742
2112/4849 [============>.................] - ETA: 4:02 - loss: 0.6744 - acc: 0.5772
2176/4849 [============>.................] - ETA: 3:56 - loss: 0.6735 - acc: 0.5777
2240/4849 [============>.................] - ETA: 3:51 - loss: 0.6738 - acc: 0.5768
2304/4849 [=============>................] - ETA: 3:45 - loss: 0.6743 - acc: 0.5764
2368/4849 [=============>................] - ETA: 3:40 - loss: 0.6731 - acc: 0.5790
2432/4849 [==============>...............] - ETA: 3:34 - loss: 0.6734 - acc: 0.5785
2496/4849 [==============>...............] - ETA: 3:28 - loss: 0.6733 - acc: 0.5801
2560/4849 [==============>...............] - ETA: 3:23 - loss: 0.6732 - acc: 0.5813
2624/4849 [===============>..............] - ETA: 3:17 - loss: 0.6731 - acc: 0.5800
2688/4849 [===============>..............] - ETA: 3:12 - loss: 0.6734 - acc: 0.5789
2752/4849 [================>.............] - ETA: 3:06 - loss: 0.6740 - acc: 0.5767
2816/4849 [================>.............] - ETA: 3:00 - loss: 0.6750 - acc: 0.5767
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6748 - acc: 0.5764
2944/4849 [=================>............] - ETA: 2:49 - loss: 0.6752 - acc: 0.5754
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6749 - acc: 0.5751
3072/4849 [==================>...........] - ETA: 2:37 - loss: 0.6748 - acc: 0.5755
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6752 - acc: 0.5737
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6762 - acc: 0.5725
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6773 - acc: 0.5714
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6774 - acc: 0.5718
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6780 - acc: 0.5716
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6776 - acc: 0.5732
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6780 - acc: 0.5730
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6780 - acc: 0.5734
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6778 - acc: 0.5746
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6776 - acc: 0.5746
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6765 - acc: 0.5776
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6764 - acc: 0.5771
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6767 - acc: 0.5766
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6768 - acc: 0.5764
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6772 - acc: 0.5754
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6775 - acc: 0.5740
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6776 - acc: 0.5736
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6775 - acc: 0.5746 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6777 - acc: 0.5739
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6775 - acc: 0.5740
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6778 - acc: 0.5731
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6776 - acc: 0.5743
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6782 - acc: 0.5735
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6781 - acc: 0.5742
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6778 - acc: 0.5749
4736/4849 [============================>.] - ETA: 10s - loss: 0.6778 - acc: 0.5750
4800/4849 [============================>.] - ETA: 4s - loss: 0.6778 - acc: 0.5756 
4849/4849 [==============================] - 451s 93ms/step - loss: 0.6782 - acc: 0.5750 - val_loss: 0.7030 - val_acc: 0.5195

Epoch 00005: val_acc did not improve from 0.57885
Epoch 6/10

  64/4849 [..............................] - ETA: 7:19 - loss: 0.6775 - acc: 0.5469
 128/4849 [..............................] - ETA: 7:21 - loss: 0.6975 - acc: 0.5391
 192/4849 [>.............................] - ETA: 7:05 - loss: 0.7097 - acc: 0.5365
 256/4849 [>.............................] - ETA: 6:55 - loss: 0.6953 - acc: 0.5664
 320/4849 [>.............................] - ETA: 6:56 - loss: 0.6891 - acc: 0.5687
 384/4849 [=>............................] - ETA: 6:43 - loss: 0.6852 - acc: 0.5755
 448/4849 [=>............................] - ETA: 6:35 - loss: 0.6853 - acc: 0.5804
 512/4849 [==>...........................] - ETA: 6:29 - loss: 0.6852 - acc: 0.5801
 576/4849 [==>...........................] - ETA: 6:23 - loss: 0.6814 - acc: 0.5885
 640/4849 [==>...........................] - ETA: 6:17 - loss: 0.6814 - acc: 0.5891
 704/4849 [===>..........................] - ETA: 6:13 - loss: 0.6828 - acc: 0.5866
 768/4849 [===>..........................] - ETA: 6:08 - loss: 0.6823 - acc: 0.5846
 832/4849 [====>.........................] - ETA: 6:03 - loss: 0.6807 - acc: 0.5913
 896/4849 [====>.........................] - ETA: 5:56 - loss: 0.6803 - acc: 0.5915
 960/4849 [====>.........................] - ETA: 5:50 - loss: 0.6801 - acc: 0.5917
1024/4849 [=====>........................] - ETA: 5:45 - loss: 0.6795 - acc: 0.5918
1088/4849 [=====>........................] - ETA: 5:39 - loss: 0.6799 - acc: 0.5901
1152/4849 [======>.......................] - ETA: 5:32 - loss: 0.6830 - acc: 0.5816
1216/4849 [======>.......................] - ETA: 5:26 - loss: 0.6804 - acc: 0.5814
1280/4849 [======>.......................] - ETA: 5:20 - loss: 0.6771 - acc: 0.5852
1344/4849 [=======>......................] - ETA: 5:14 - loss: 0.6764 - acc: 0.5833
1408/4849 [=======>......................] - ETA: 5:08 - loss: 0.6770 - acc: 0.5824
1472/4849 [========>.....................] - ETA: 5:02 - loss: 0.6762 - acc: 0.5842
1536/4849 [========>.....................] - ETA: 4:56 - loss: 0.6762 - acc: 0.5820
1600/4849 [========>.....................] - ETA: 4:50 - loss: 0.6758 - acc: 0.5831
1664/4849 [=========>....................] - ETA: 4:43 - loss: 0.6775 - acc: 0.5811
1728/4849 [=========>....................] - ETA: 4:37 - loss: 0.6768 - acc: 0.5816
1792/4849 [==========>...................] - ETA: 4:32 - loss: 0.6757 - acc: 0.5820
1856/4849 [==========>...................] - ETA: 4:26 - loss: 0.6746 - acc: 0.5841
1920/4849 [==========>...................] - ETA: 4:20 - loss: 0.6783 - acc: 0.5781
1984/4849 [===========>..................] - ETA: 4:13 - loss: 0.6793 - acc: 0.5766
2048/4849 [===========>..................] - ETA: 4:08 - loss: 0.6803 - acc: 0.5771
2112/4849 [============>.................] - ETA: 4:02 - loss: 0.6827 - acc: 0.5720
2176/4849 [============>.................] - ETA: 3:56 - loss: 0.6817 - acc: 0.5744
2240/4849 [============>.................] - ETA: 3:51 - loss: 0.6821 - acc: 0.5714
2304/4849 [=============>................] - ETA: 3:46 - loss: 0.6834 - acc: 0.5694
2368/4849 [=============>................] - ETA: 3:40 - loss: 0.6837 - acc: 0.5697
2432/4849 [==============>...............] - ETA: 3:34 - loss: 0.6830 - acc: 0.5724
2496/4849 [==============>...............] - ETA: 3:29 - loss: 0.6840 - acc: 0.5677
2560/4849 [==============>...............] - ETA: 3:24 - loss: 0.6831 - acc: 0.5695
2624/4849 [===============>..............] - ETA: 3:18 - loss: 0.6829 - acc: 0.5682
2688/4849 [===============>..............] - ETA: 3:13 - loss: 0.6825 - acc: 0.5677
2752/4849 [================>.............] - ETA: 3:07 - loss: 0.6820 - acc: 0.5687
2816/4849 [================>.............] - ETA: 3:01 - loss: 0.6821 - acc: 0.5682
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6813 - acc: 0.5705
2944/4849 [=================>............] - ETA: 2:50 - loss: 0.6811 - acc: 0.5707
3008/4849 [=================>............] - ETA: 2:44 - loss: 0.6811 - acc: 0.5711
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6806 - acc: 0.5716
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6799 - acc: 0.5724
3200/4849 [==================>...........] - ETA: 2:27 - loss: 0.6800 - acc: 0.5709
3264/4849 [===================>..........] - ETA: 2:21 - loss: 0.6791 - acc: 0.5708
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6789 - acc: 0.5712
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6782 - acc: 0.5710
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6779 - acc: 0.5720
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6769 - acc: 0.5733
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6768 - acc: 0.5737
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6765 - acc: 0.5743
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6763 - acc: 0.5744
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6767 - acc: 0.5736
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6770 - acc: 0.5727
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6762 - acc: 0.5740
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6757 - acc: 0.5754
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6755 - acc: 0.5756
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6759 - acc: 0.5750
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6761 - acc: 0.5745
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6767 - acc: 0.5734 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6769 - acc: 0.5730
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6774 - acc: 0.5719
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6773 - acc: 0.5720
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6775 - acc: 0.5723
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6773 - acc: 0.5726
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6771 - acc: 0.5729
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6768 - acc: 0.5734
4736/4849 [============================>.] - ETA: 9s - loss: 0.6768 - acc: 0.5735 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6765 - acc: 0.5737
4849/4849 [==============================] - 444s 92ms/step - loss: 0.6763 - acc: 0.5743 - val_loss: 0.6981 - val_acc: 0.5473

Epoch 00006: val_acc did not improve from 0.57885
Epoch 7/10

  64/4849 [..............................] - ETA: 6:34 - loss: 0.6851 - acc: 0.5312
 128/4849 [..............................] - ETA: 6:41 - loss: 0.6781 - acc: 0.5547
 192/4849 [>.............................] - ETA: 6:38 - loss: 0.6742 - acc: 0.5833
 256/4849 [>.............................] - ETA: 6:26 - loss: 0.6980 - acc: 0.5391
 320/4849 [>.............................] - ETA: 6:31 - loss: 0.7038 - acc: 0.5375
 384/4849 [=>............................] - ETA: 6:24 - loss: 0.6965 - acc: 0.5547
 448/4849 [=>............................] - ETA: 6:19 - loss: 0.6942 - acc: 0.5625
 512/4849 [==>...........................] - ETA: 6:16 - loss: 0.6993 - acc: 0.5508
 576/4849 [==>...........................] - ETA: 6:16 - loss: 0.6964 - acc: 0.5556
 640/4849 [==>...........................] - ETA: 6:12 - loss: 0.6930 - acc: 0.5625
 704/4849 [===>..........................] - ETA: 6:07 - loss: 0.6879 - acc: 0.5653
 768/4849 [===>..........................] - ETA: 6:04 - loss: 0.6829 - acc: 0.5742
 832/4849 [====>.........................] - ETA: 5:58 - loss: 0.6839 - acc: 0.5685
 896/4849 [====>.........................] - ETA: 5:53 - loss: 0.6835 - acc: 0.5658
 960/4849 [====>.........................] - ETA: 5:46 - loss: 0.6823 - acc: 0.5708
1024/4849 [=====>........................] - ETA: 5:40 - loss: 0.6792 - acc: 0.5752
1088/4849 [=====>........................] - ETA: 5:35 - loss: 0.6758 - acc: 0.5809
1152/4849 [======>.......................] - ETA: 5:30 - loss: 0.6779 - acc: 0.5781
1216/4849 [======>.......................] - ETA: 5:23 - loss: 0.6762 - acc: 0.5781
1280/4849 [======>.......................] - ETA: 5:18 - loss: 0.6792 - acc: 0.5703
1344/4849 [=======>......................] - ETA: 5:12 - loss: 0.6814 - acc: 0.5662
1408/4849 [=======>......................] - ETA: 5:06 - loss: 0.6793 - acc: 0.5717
1472/4849 [========>.....................] - ETA: 5:01 - loss: 0.6801 - acc: 0.5713
1536/4849 [========>.....................] - ETA: 4:54 - loss: 0.6773 - acc: 0.5781
1600/4849 [========>.....................] - ETA: 4:49 - loss: 0.6770 - acc: 0.5781
1664/4849 [=========>....................] - ETA: 4:44 - loss: 0.6774 - acc: 0.5775
1728/4849 [=========>....................] - ETA: 4:38 - loss: 0.6757 - acc: 0.5793
1792/4849 [==========>...................] - ETA: 4:32 - loss: 0.6751 - acc: 0.5781
1856/4849 [==========>...................] - ETA: 4:26 - loss: 0.6747 - acc: 0.5792
1920/4849 [==========>...................] - ETA: 4:19 - loss: 0.6746 - acc: 0.5781
1984/4849 [===========>..................] - ETA: 4:14 - loss: 0.6746 - acc: 0.5791
2048/4849 [===========>..................] - ETA: 4:09 - loss: 0.6753 - acc: 0.5786
2112/4849 [============>.................] - ETA: 4:03 - loss: 0.6751 - acc: 0.5791
2176/4849 [============>.................] - ETA: 3:57 - loss: 0.6751 - acc: 0.5790
2240/4849 [============>.................] - ETA: 3:52 - loss: 0.6762 - acc: 0.5768
2304/4849 [=============>................] - ETA: 3:46 - loss: 0.6768 - acc: 0.5760
2368/4849 [=============>................] - ETA: 3:41 - loss: 0.6780 - acc: 0.5735
2432/4849 [==============>...............] - ETA: 3:35 - loss: 0.6782 - acc: 0.5744
2496/4849 [==============>...............] - ETA: 3:29 - loss: 0.6784 - acc: 0.5749
2560/4849 [==============>...............] - ETA: 3:24 - loss: 0.6771 - acc: 0.5785
2624/4849 [===============>..............] - ETA: 3:19 - loss: 0.6769 - acc: 0.5789
2688/4849 [===============>..............] - ETA: 3:13 - loss: 0.6771 - acc: 0.5792
2752/4849 [================>.............] - ETA: 3:07 - loss: 0.6776 - acc: 0.5781
2816/4849 [================>.............] - ETA: 3:02 - loss: 0.6785 - acc: 0.5781
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6787 - acc: 0.5774
2944/4849 [=================>............] - ETA: 2:49 - loss: 0.6777 - acc: 0.5791
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6786 - acc: 0.5778
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6794 - acc: 0.5762
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6792 - acc: 0.5765
3200/4849 [==================>...........] - ETA: 2:27 - loss: 0.6790 - acc: 0.5772
3264/4849 [===================>..........] - ETA: 2:21 - loss: 0.6788 - acc: 0.5769
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6791 - acc: 0.5775
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6796 - acc: 0.5755
3456/4849 [====================>.........] - ETA: 2:04 - loss: 0.6794 - acc: 0.5764
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6790 - acc: 0.5778
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6792 - acc: 0.5776
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6791 - acc: 0.5779
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6792 - acc: 0.5773
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6793 - acc: 0.5765
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6796 - acc: 0.5760
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6794 - acc: 0.5763
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6789 - acc: 0.5781
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6785 - acc: 0.5789
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6790 - acc: 0.5781
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6791 - acc: 0.5779
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6792 - acc: 0.5779 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6789 - acc: 0.5779
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6792 - acc: 0.5765
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6798 - acc: 0.5750
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6801 - acc: 0.5739
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6800 - acc: 0.5735
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6805 - acc: 0.5727
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6802 - acc: 0.5719
4736/4849 [============================>.] - ETA: 10s - loss: 0.6797 - acc: 0.5728
4800/4849 [============================>.] - ETA: 4s - loss: 0.6798 - acc: 0.5721 
4849/4849 [==============================] - 447s 92ms/step - loss: 0.6798 - acc: 0.5717 - val_loss: 0.6938 - val_acc: 0.5566

Epoch 00007: val_acc did not improve from 0.57885
Epoch 8/10

  64/4849 [..............................] - ETA: 6:54 - loss: 0.6635 - acc: 0.6250
 128/4849 [..............................] - ETA: 6:40 - loss: 0.6541 - acc: 0.6406
 192/4849 [>.............................] - ETA: 6:34 - loss: 0.6521 - acc: 0.6302
 256/4849 [>.............................] - ETA: 6:37 - loss: 0.6627 - acc: 0.6016
 320/4849 [>.............................] - ETA: 6:30 - loss: 0.6676 - acc: 0.6000
 384/4849 [=>............................] - ETA: 6:20 - loss: 0.6662 - acc: 0.5990
 448/4849 [=>............................] - ETA: 6:14 - loss: 0.6705 - acc: 0.5915
 512/4849 [==>...........................] - ETA: 6:11 - loss: 0.6682 - acc: 0.6035
 576/4849 [==>...........................] - ETA: 6:08 - loss: 0.6721 - acc: 0.5868
 640/4849 [==>...........................] - ETA: 6:03 - loss: 0.6756 - acc: 0.5828
 704/4849 [===>..........................] - ETA: 5:56 - loss: 0.6760 - acc: 0.5838
 768/4849 [===>..........................] - ETA: 5:46 - loss: 0.6766 - acc: 0.5872
 832/4849 [====>.........................] - ETA: 5:43 - loss: 0.6792 - acc: 0.5781
 896/4849 [====>.........................] - ETA: 5:35 - loss: 0.6766 - acc: 0.5781
 960/4849 [====>.........................] - ETA: 5:33 - loss: 0.6757 - acc: 0.5833
1024/4849 [=====>........................] - ETA: 5:29 - loss: 0.6781 - acc: 0.5742
1088/4849 [=====>........................] - ETA: 5:23 - loss: 0.6785 - acc: 0.5763
1152/4849 [======>.......................] - ETA: 5:16 - loss: 0.6763 - acc: 0.5816
1216/4849 [======>.......................] - ETA: 5:11 - loss: 0.6764 - acc: 0.5814
1280/4849 [======>.......................] - ETA: 5:05 - loss: 0.6775 - acc: 0.5813
1344/4849 [=======>......................] - ETA: 4:58 - loss: 0.6777 - acc: 0.5804
1408/4849 [=======>......................] - ETA: 4:55 - loss: 0.6803 - acc: 0.5753
1472/4849 [========>.....................] - ETA: 4:50 - loss: 0.6797 - acc: 0.5768
1536/4849 [========>.....................] - ETA: 4:44 - loss: 0.6804 - acc: 0.5742
1600/4849 [========>.....................] - ETA: 4:39 - loss: 0.6828 - acc: 0.5694
1664/4849 [=========>....................] - ETA: 4:33 - loss: 0.6811 - acc: 0.5733
1728/4849 [=========>....................] - ETA: 4:28 - loss: 0.6806 - acc: 0.5729
1792/4849 [==========>...................] - ETA: 4:23 - loss: 0.6806 - acc: 0.5720
1856/4849 [==========>...................] - ETA: 4:17 - loss: 0.6816 - acc: 0.5700
1920/4849 [==========>...................] - ETA: 4:11 - loss: 0.6798 - acc: 0.5745
1984/4849 [===========>..................] - ETA: 4:06 - loss: 0.6804 - acc: 0.5726
2048/4849 [===========>..................] - ETA: 4:01 - loss: 0.6798 - acc: 0.5752
2112/4849 [============>.................] - ETA: 3:54 - loss: 0.6802 - acc: 0.5724
2176/4849 [============>.................] - ETA: 3:49 - loss: 0.6797 - acc: 0.5722
2240/4849 [============>.................] - ETA: 3:42 - loss: 0.6793 - acc: 0.5705
2304/4849 [=============>................] - ETA: 3:37 - loss: 0.6798 - acc: 0.5690
2368/4849 [=============>................] - ETA: 3:32 - loss: 0.6794 - acc: 0.5705
2432/4849 [==============>...............] - ETA: 3:27 - loss: 0.6788 - acc: 0.5728
2496/4849 [==============>...............] - ETA: 3:22 - loss: 0.6777 - acc: 0.5757
2560/4849 [==============>...............] - ETA: 3:16 - loss: 0.6774 - acc: 0.5754
2624/4849 [===============>..............] - ETA: 3:11 - loss: 0.6778 - acc: 0.5732
2688/4849 [===============>..............] - ETA: 3:05 - loss: 0.6777 - acc: 0.5733
2752/4849 [================>.............] - ETA: 3:00 - loss: 0.6795 - acc: 0.5709
2816/4849 [================>.............] - ETA: 2:55 - loss: 0.6792 - acc: 0.5714
2880/4849 [================>.............] - ETA: 2:49 - loss: 0.6796 - acc: 0.5708
2944/4849 [=================>............] - ETA: 2:44 - loss: 0.6802 - acc: 0.5707
3008/4849 [=================>............] - ETA: 2:38 - loss: 0.6799 - acc: 0.5711
3072/4849 [==================>...........] - ETA: 2:32 - loss: 0.6791 - acc: 0.5729
3136/4849 [==================>...........] - ETA: 2:27 - loss: 0.6784 - acc: 0.5740
3200/4849 [==================>...........] - ETA: 2:21 - loss: 0.6779 - acc: 0.5750
3264/4849 [===================>..........] - ETA: 2:16 - loss: 0.6778 - acc: 0.5748
3328/4849 [===================>..........] - ETA: 2:10 - loss: 0.6778 - acc: 0.5739
3392/4849 [===================>..........] - ETA: 2:04 - loss: 0.6773 - acc: 0.5758
3456/4849 [====================>.........] - ETA: 1:59 - loss: 0.6778 - acc: 0.5747
3520/4849 [====================>.........] - ETA: 1:53 - loss: 0.6777 - acc: 0.5756
3584/4849 [=====================>........] - ETA: 1:48 - loss: 0.6767 - acc: 0.5759
3648/4849 [=====================>........] - ETA: 1:43 - loss: 0.6763 - acc: 0.5770
3712/4849 [=====================>........] - ETA: 1:37 - loss: 0.6755 - acc: 0.5781
3776/4849 [======================>.......] - ETA: 1:32 - loss: 0.6754 - acc: 0.5779
3840/4849 [======================>.......] - ETA: 1:26 - loss: 0.6757 - acc: 0.5768
3904/4849 [=======================>......] - ETA: 1:21 - loss: 0.6756 - acc: 0.5771
3968/4849 [=======================>......] - ETA: 1:15 - loss: 0.6753 - acc: 0.5781
4032/4849 [=======================>......] - ETA: 1:10 - loss: 0.6755 - acc: 0.5784
4096/4849 [========================>.....] - ETA: 1:04 - loss: 0.6755 - acc: 0.5779
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6749 - acc: 0.5788 
4224/4849 [=========================>....] - ETA: 53s - loss: 0.6753 - acc: 0.5781
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6745 - acc: 0.5795
4352/4849 [=========================>....] - ETA: 42s - loss: 0.6747 - acc: 0.5790
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6747 - acc: 0.5795
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6747 - acc: 0.5797
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6740 - acc: 0.5805
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6737 - acc: 0.5818
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6737 - acc: 0.5826
4736/4849 [============================>.] - ETA: 9s - loss: 0.6737 - acc: 0.5819 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6737 - acc: 0.5821
4849/4849 [==============================] - 435s 90ms/step - loss: 0.6737 - acc: 0.5826 - val_loss: 0.6972 - val_acc: 0.5677

Epoch 00008: val_acc did not improve from 0.57885
Epoch 9/10

  64/4849 [..............................] - ETA: 6:41 - loss: 0.6345 - acc: 0.6562
 128/4849 [..............................] - ETA: 6:45 - loss: 0.6426 - acc: 0.6484
 192/4849 [>.............................] - ETA: 6:35 - loss: 0.6427 - acc: 0.6406
 256/4849 [>.............................] - ETA: 6:31 - loss: 0.6466 - acc: 0.6328
 320/4849 [>.............................] - ETA: 6:34 - loss: 0.6603 - acc: 0.6062
 384/4849 [=>............................] - ETA: 6:32 - loss: 0.6550 - acc: 0.6146
 448/4849 [=>............................] - ETA: 6:25 - loss: 0.6591 - acc: 0.6071
 512/4849 [==>...........................] - ETA: 6:17 - loss: 0.6671 - acc: 0.5898
 576/4849 [==>...........................] - ETA: 6:12 - loss: 0.6737 - acc: 0.5764
 640/4849 [==>...........................] - ETA: 6:07 - loss: 0.6708 - acc: 0.5781
 704/4849 [===>..........................] - ETA: 6:00 - loss: 0.6701 - acc: 0.5824
 768/4849 [===>..........................] - ETA: 5:55 - loss: 0.6713 - acc: 0.5794
 832/4849 [====>.........................] - ETA: 5:47 - loss: 0.6715 - acc: 0.5805
 896/4849 [====>.........................] - ETA: 5:40 - loss: 0.6720 - acc: 0.5837
 960/4849 [====>.........................] - ETA: 5:33 - loss: 0.6745 - acc: 0.5813
1024/4849 [=====>........................] - ETA: 5:26 - loss: 0.6717 - acc: 0.5869
1088/4849 [=====>........................] - ETA: 5:21 - loss: 0.6723 - acc: 0.5873
1152/4849 [======>.......................] - ETA: 5:16 - loss: 0.6740 - acc: 0.5825
1216/4849 [======>.......................] - ETA: 5:10 - loss: 0.6769 - acc: 0.5781
1280/4849 [======>.......................] - ETA: 5:04 - loss: 0.6783 - acc: 0.5766
1344/4849 [=======>......................] - ETA: 5:00 - loss: 0.6772 - acc: 0.5789
1408/4849 [=======>......................] - ETA: 4:53 - loss: 0.6759 - acc: 0.5810
1472/4849 [========>.....................] - ETA: 4:48 - loss: 0.6770 - acc: 0.5815
1536/4849 [========>.....................] - ETA: 4:44 - loss: 0.6743 - acc: 0.5859
1600/4849 [========>.....................] - ETA: 4:38 - loss: 0.6757 - acc: 0.5794
1664/4849 [=========>....................] - ETA: 4:33 - loss: 0.6736 - acc: 0.5823
1728/4849 [=========>....................] - ETA: 4:27 - loss: 0.6733 - acc: 0.5851
1792/4849 [==========>...................] - ETA: 4:22 - loss: 0.6729 - acc: 0.5871
1856/4849 [==========>...................] - ETA: 4:18 - loss: 0.6715 - acc: 0.5873
1920/4849 [==========>...................] - ETA: 4:12 - loss: 0.6725 - acc: 0.5844
1984/4849 [===========>..................] - ETA: 4:06 - loss: 0.6735 - acc: 0.5837
2048/4849 [===========>..................] - ETA: 4:01 - loss: 0.6734 - acc: 0.5815
2112/4849 [============>.................] - ETA: 3:56 - loss: 0.6752 - acc: 0.5795
2176/4849 [============>.................] - ETA: 3:49 - loss: 0.6743 - acc: 0.5809
2240/4849 [============>.................] - ETA: 3:44 - loss: 0.6749 - acc: 0.5790
2304/4849 [=============>................] - ETA: 3:38 - loss: 0.6749 - acc: 0.5807
2368/4849 [=============>................] - ETA: 3:32 - loss: 0.6745 - acc: 0.5807
2432/4849 [==============>...............] - ETA: 3:26 - loss: 0.6751 - acc: 0.5789
2496/4849 [==============>...............] - ETA: 3:20 - loss: 0.6756 - acc: 0.5793
2560/4849 [==============>...............] - ETA: 3:15 - loss: 0.6753 - acc: 0.5773
2624/4849 [===============>..............] - ETA: 3:10 - loss: 0.6752 - acc: 0.5774
2688/4849 [===============>..............] - ETA: 3:04 - loss: 0.6743 - acc: 0.5789
2752/4849 [================>.............] - ETA: 2:58 - loss: 0.6734 - acc: 0.5818
2816/4849 [================>.............] - ETA: 2:53 - loss: 0.6725 - acc: 0.5842
2880/4849 [================>.............] - ETA: 2:47 - loss: 0.6720 - acc: 0.5847
2944/4849 [=================>............] - ETA: 2:42 - loss: 0.6722 - acc: 0.5846
3008/4849 [=================>............] - ETA: 2:36 - loss: 0.6731 - acc: 0.5841
3072/4849 [==================>...........] - ETA: 2:31 - loss: 0.6734 - acc: 0.5827
3136/4849 [==================>...........] - ETA: 2:25 - loss: 0.6734 - acc: 0.5823
3200/4849 [==================>...........] - ETA: 2:20 - loss: 0.6729 - acc: 0.5822
3264/4849 [===================>..........] - ETA: 2:14 - loss: 0.6725 - acc: 0.5827
3328/4849 [===================>..........] - ETA: 2:09 - loss: 0.6736 - acc: 0.5811
3392/4849 [===================>..........] - ETA: 2:03 - loss: 0.6737 - acc: 0.5802
3456/4849 [====================>.........] - ETA: 1:58 - loss: 0.6745 - acc: 0.5799
3520/4849 [====================>.........] - ETA: 1:52 - loss: 0.6742 - acc: 0.5810
3584/4849 [=====================>........] - ETA: 1:46 - loss: 0.6740 - acc: 0.5820
3648/4849 [=====================>........] - ETA: 1:41 - loss: 0.6735 - acc: 0.5828
3712/4849 [=====================>........] - ETA: 1:36 - loss: 0.6733 - acc: 0.5838
3776/4849 [======================>.......] - ETA: 1:30 - loss: 0.6735 - acc: 0.5824
3840/4849 [======================>.......] - ETA: 1:25 - loss: 0.6732 - acc: 0.5826
3904/4849 [=======================>......] - ETA: 1:19 - loss: 0.6727 - acc: 0.5832
3968/4849 [=======================>......] - ETA: 1:14 - loss: 0.6727 - acc: 0.5837
4032/4849 [=======================>......] - ETA: 1:09 - loss: 0.6725 - acc: 0.5851
4096/4849 [========================>.....] - ETA: 1:03 - loss: 0.6717 - acc: 0.5869
4160/4849 [========================>.....] - ETA: 58s - loss: 0.6726 - acc: 0.5853 
4224/4849 [=========================>....] - ETA: 52s - loss: 0.6721 - acc: 0.5859
4288/4849 [=========================>....] - ETA: 47s - loss: 0.6717 - acc: 0.5868
4352/4849 [=========================>....] - ETA: 42s - loss: 0.6714 - acc: 0.5869
4416/4849 [==========================>...] - ETA: 36s - loss: 0.6709 - acc: 0.5879
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6704 - acc: 0.5884
4544/4849 [===========================>..] - ETA: 25s - loss: 0.6703 - acc: 0.5882
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6697 - acc: 0.5888
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6695 - acc: 0.5901
4736/4849 [============================>.] - ETA: 9s - loss: 0.6701 - acc: 0.5889 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6702 - acc: 0.5887
4849/4849 [==============================] - 429s 88ms/step - loss: 0.6699 - acc: 0.5896 - val_loss: 0.7149 - val_acc: 0.5492

Epoch 00009: val_acc did not improve from 0.57885
Epoch 10/10

  64/4849 [..............................] - ETA: 7:23 - loss: 0.6425 - acc: 0.6562
 128/4849 [..............................] - ETA: 7:04 - loss: 0.6500 - acc: 0.6016
 192/4849 [>.............................] - ETA: 6:49 - loss: 0.6782 - acc: 0.5625
 256/4849 [>.............................] - ETA: 6:46 - loss: 0.6673 - acc: 0.5859
 320/4849 [>.............................] - ETA: 6:38 - loss: 0.6689 - acc: 0.5938
 384/4849 [=>............................] - ETA: 6:37 - loss: 0.6633 - acc: 0.5964
 448/4849 [=>............................] - ETA: 6:36 - loss: 0.6611 - acc: 0.6049
 512/4849 [==>...........................] - ETA: 6:28 - loss: 0.6635 - acc: 0.5957
 576/4849 [==>...........................] - ETA: 6:27 - loss: 0.6630 - acc: 0.5972
 640/4849 [==>...........................] - ETA: 6:24 - loss: 0.6627 - acc: 0.6031
 704/4849 [===>..........................] - ETA: 6:22 - loss: 0.6593 - acc: 0.6122
 768/4849 [===>..........................] - ETA: 6:20 - loss: 0.6576 - acc: 0.6120
 832/4849 [====>.........................] - ETA: 6:14 - loss: 0.6590 - acc: 0.6130
 896/4849 [====>.........................] - ETA: 6:13 - loss: 0.6602 - acc: 0.6127
 960/4849 [====>.........................] - ETA: 6:13 - loss: 0.6645 - acc: 0.6031
1024/4849 [=====>........................] - ETA: 6:11 - loss: 0.6635 - acc: 0.6016
1088/4849 [=====>........................] - ETA: 6:09 - loss: 0.6662 - acc: 0.5965
1152/4849 [======>.......................] - ETA: 6:05 - loss: 0.6664 - acc: 0.5972
1216/4849 [======>.......................] - ETA: 6:01 - loss: 0.6666 - acc: 0.5962
1280/4849 [======>.......................] - ETA: 5:56 - loss: 0.6639 - acc: 0.6008
1344/4849 [=======>......................] - ETA: 5:52 - loss: 0.6615 - acc: 0.6034
1408/4849 [=======>......................] - ETA: 5:47 - loss: 0.6625 - acc: 0.6016
1472/4849 [========>.....................] - ETA: 5:43 - loss: 0.6658 - acc: 0.5971
1536/4849 [========>.....................] - ETA: 5:38 - loss: 0.6656 - acc: 0.5964
1600/4849 [========>.....................] - ETA: 5:33 - loss: 0.6638 - acc: 0.6006
1664/4849 [=========>....................] - ETA: 5:28 - loss: 0.6636 - acc: 0.5998
1728/4849 [=========>....................] - ETA: 5:22 - loss: 0.6639 - acc: 0.6001
1792/4849 [==========>...................] - ETA: 5:16 - loss: 0.6664 - acc: 0.5988
1856/4849 [==========>...................] - ETA: 5:10 - loss: 0.6660 - acc: 0.6013
1920/4849 [==========>...................] - ETA: 5:05 - loss: 0.6677 - acc: 0.5984
1984/4849 [===========>..................] - ETA: 4:59 - loss: 0.6665 - acc: 0.6013
2048/4849 [===========>..................] - ETA: 4:54 - loss: 0.6658 - acc: 0.6016
2112/4849 [============>.................] - ETA: 4:48 - loss: 0.6664 - acc: 0.6009
2176/4849 [============>.................] - ETA: 4:41 - loss: 0.6656 - acc: 0.6034
2240/4849 [============>.................] - ETA: 4:35 - loss: 0.6650 - acc: 0.6049
2304/4849 [=============>................] - ETA: 4:30 - loss: 0.6648 - acc: 0.6050
2368/4849 [=============>................] - ETA: 4:23 - loss: 0.6641 - acc: 0.6060
2432/4849 [==============>...............] - ETA: 4:17 - loss: 0.6653 - acc: 0.6024
2496/4849 [==============>...............] - ETA: 4:11 - loss: 0.6649 - acc: 0.6030
2560/4849 [==============>...............] - ETA: 4:05 - loss: 0.6658 - acc: 0.6012
2624/4849 [===============>..............] - ETA: 3:58 - loss: 0.6653 - acc: 0.6033
2688/4849 [===============>..............] - ETA: 3:52 - loss: 0.6647 - acc: 0.6045
2752/4849 [================>.............] - ETA: 3:45 - loss: 0.6649 - acc: 0.6032
2816/4849 [================>.............] - ETA: 3:38 - loss: 0.6648 - acc: 0.6037
2880/4849 [================>.............] - ETA: 3:31 - loss: 0.6647 - acc: 0.6028
2944/4849 [=================>............] - ETA: 3:25 - loss: 0.6644 - acc: 0.6033
3008/4849 [=================>............] - ETA: 3:18 - loss: 0.6642 - acc: 0.6034
3072/4849 [==================>...........] - ETA: 3:11 - loss: 0.6652 - acc: 0.6016
3136/4849 [==================>...........] - ETA: 3:04 - loss: 0.6655 - acc: 0.6001
3200/4849 [==================>...........] - ETA: 2:58 - loss: 0.6657 - acc: 0.6003
3264/4849 [===================>..........] - ETA: 2:51 - loss: 0.6659 - acc: 0.5996
3328/4849 [===================>..........] - ETA: 2:44 - loss: 0.6661 - acc: 0.5983
3392/4849 [===================>..........] - ETA: 2:37 - loss: 0.6668 - acc: 0.5967
3456/4849 [====================>.........] - ETA: 2:31 - loss: 0.6663 - acc: 0.5972
3520/4849 [====================>.........] - ETA: 2:24 - loss: 0.6658 - acc: 0.5986
3584/4849 [=====================>........] - ETA: 2:17 - loss: 0.6660 - acc: 0.5977
3648/4849 [=====================>........] - ETA: 2:10 - loss: 0.6660 - acc: 0.5973
3712/4849 [=====================>........] - ETA: 2:03 - loss: 0.6661 - acc: 0.5975
3776/4849 [======================>.......] - ETA: 1:56 - loss: 0.6656 - acc: 0.5983
3840/4849 [======================>.......] - ETA: 1:49 - loss: 0.6656 - acc: 0.5984
3904/4849 [=======================>......] - ETA: 1:43 - loss: 0.6654 - acc: 0.5994
3968/4849 [=======================>......] - ETA: 1:36 - loss: 0.6654 - acc: 0.5990
4032/4849 [=======================>......] - ETA: 1:29 - loss: 0.6659 - acc: 0.5982
4096/4849 [========================>.....] - ETA: 1:22 - loss: 0.6650 - acc: 0.6001
4160/4849 [========================>.....] - ETA: 1:15 - loss: 0.6649 - acc: 0.6014
4224/4849 [=========================>....] - ETA: 1:08 - loss: 0.6648 - acc: 0.6032
4288/4849 [=========================>....] - ETA: 1:01 - loss: 0.6650 - acc: 0.6035
4352/4849 [=========================>....] - ETA: 54s - loss: 0.6655 - acc: 0.6027 
4416/4849 [==========================>...] - ETA: 47s - loss: 0.6653 - acc: 0.6030
4480/4849 [==========================>...] - ETA: 40s - loss: 0.6653 - acc: 0.6022
4544/4849 [===========================>..] - ETA: 33s - loss: 0.6650 - acc: 0.6026
4608/4849 [===========================>..] - ETA: 26s - loss: 0.6653 - acc: 0.6013
4672/4849 [===========================>..] - ETA: 19s - loss: 0.6662 - acc: 0.5995
4736/4849 [============================>.] - ETA: 12s - loss: 0.6665 - acc: 0.5980
4800/4849 [============================>.] - ETA: 5s - loss: 0.6665 - acc: 0.5985 
4849/4849 [==============================] - 554s 114ms/step - loss: 0.6676 - acc: 0.5960 - val_loss: 0.6979 - val_acc: 0.5659

Epoch 00010: val_acc did not improve from 0.57885
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f328da43550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f328da43550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f328557f0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f328557f0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3285834150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3285834150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f328d9d4f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f328d9d4f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3285597650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3285597650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f328538a450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f328538a450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f32855764d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f32855764d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3285598c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3285598c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f320f1ed450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f320f1ed450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3285285890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3285285890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32852bebd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32852bebd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f320f208310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f320f208310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f328530a810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f328530a810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f328530a990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f328530a990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f328513b6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f328513b6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32850fce90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32850fce90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2c2c083690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2c2c083690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c2c0e3910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c2c0e3910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2c2c08a950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2c2c08a950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3274cb8610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3274cb8610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3274af3690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3274af3690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2c2c1325d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2c2c1325d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3274cc6a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3274cc6a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3274a06510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3274a06510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3274cd4210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3274cd4210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3274bf22d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3274bf22d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3274c37fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3274c37fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32748ee410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32748ee410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f326c7dcd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f326c7dcd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f326c6eb390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f326c6eb390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31e4ef3d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f31e4ef3d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f32748e7f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f32748e7f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f326c60a2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f326c60a2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f326c608110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f326c608110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f326c2af4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f326c2af4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f326c2a55d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f326c2a55d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f326c608450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f326c608450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f326c1a1ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f326c1a1ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f326c12aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f326c12aed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f32854a3650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f32854a3650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f326c2db510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f326c2db510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f326c12ae90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f326c12ae90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f326c2af2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f326c2af2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3263da3190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3263da3190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3263c1f150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3263c1f150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3263f7f450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3263f7f450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3263da3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3263da3c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3263c70ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3263c70ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3263c6a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3263c6a550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3263a0de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3263a0de10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3253842110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3253842110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3263c6ad10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3263c6ad10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3263979c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3263979c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f325389b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f325389b8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f325374bf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f325374bf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32536b7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32536b7290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3263995810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3263995810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32536bb190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32536bb190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3253757310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3253757310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f325344f350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f325344f350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3253476150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3253476150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3253711e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3253711e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3253481cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3253481cd0>>: AttributeError: module 'gast' has no attribute 'Str'
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 1:51
 128/1348 [=>............................] - ETA: 1:13
 192/1348 [===>..........................] - ETA: 58s 
 256/1348 [====>.........................] - ETA: 53s
 320/1348 [======>.......................] - ETA: 47s
 384/1348 [=======>......................] - ETA: 42s
 448/1348 [========>.....................] - ETA: 38s
 512/1348 [==========>...................] - ETA: 34s
 576/1348 [===========>..................] - ETA: 31s
 640/1348 [=============>................] - ETA: 28s
 704/1348 [==============>...............] - ETA: 25s
 768/1348 [================>.............] - ETA: 22s
 832/1348 [=================>............] - ETA: 19s
 896/1348 [==================>...........] - ETA: 17s
 960/1348 [====================>.........] - ETA: 14s
1024/1348 [=====================>........] - ETA: 12s
1088/1348 [=======================>......] - ETA: 9s 
1152/1348 [========================>.....] - ETA: 7s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 51s 38ms/step
loss: 0.683755952809614
acc: 0.5675074183976261
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f2d041a8810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f2d041a8810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f32858348d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f32858348d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32421beb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32421beb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3242270790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3242270790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f328da71510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f328da71510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a68127190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a68127190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3242270110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3242270110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f328d9980d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f328d9980d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f32856bf790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f32856bf790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2c087cc690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2c087cc690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c08652590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c08652590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f328da531d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f328da531d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c087cc6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c087cc6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f328d8da190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f328d8da190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2c08673d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2c08673d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c085c6fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c085c6fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f328d8da290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f328d8da290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32856b65d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32856b65d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2c0824fcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2c0824fcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2c08256090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2c08256090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c081d3110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c081d3110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2c0824f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2c0824f590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c0813ba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c0813ba90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2be874f410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2be874f410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2be8731610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2be8731610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c08141050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2c08141050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2be874f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2be874f590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2be861a690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2be861a690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2be867b890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2be867b890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2be834ab50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2be834ab50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2be83bfa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2be83bfa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2be8613c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2be8613c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2be81f0510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2be81f0510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2be80da050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2be80da050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2be8059cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2be8059cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2be83e3290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2be83e3290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2be80dabd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2be80dabd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc8783110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc8783110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2bc8798410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2bc8798410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2bc8468f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2bc8468f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc84c4450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc84c4450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2bc86acad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2bc86acad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc8465050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc8465050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2bc8262c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2bc8262c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2bc815b190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2bc815b190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc81ef310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc81ef310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2bc8262490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2bc8262490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc81fb850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc81fb850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2a48793a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2a48793a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2a48647f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2a48647f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a486d9410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a486d9410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2bc81e5750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2bc81e5750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a4872a650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a4872a650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2a483ea110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2a483ea110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2a48346a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2a48346a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a48628910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a48628910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2a484b1450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2a484b1450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a48367ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a48367ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2a4808de50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2a4808de50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2a48286110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2a48286110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a2c7a2610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a2c7a2610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2a4808d210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2a4808d210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a2c65d150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a2c65d150>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 26:58 - loss: 0.6238 - acc: 0.6875
 128/4849 [..............................] - ETA: 16:37 - loss: 0.7189 - acc: 0.6016
 192/4849 [>.............................] - ETA: 12:59 - loss: 0.7159 - acc: 0.5885
 256/4849 [>.............................] - ETA: 11:05 - loss: 0.7256 - acc: 0.5625
 320/4849 [>.............................] - ETA: 9:56 - loss: 0.7474 - acc: 0.5375 
 384/4849 [=>............................] - ETA: 9:11 - loss: 0.7572 - acc: 0.5260
 448/4849 [=>............................] - ETA: 8:37 - loss: 0.7664 - acc: 0.5134
 512/4849 [==>...........................] - ETA: 8:12 - loss: 0.7661 - acc: 0.5039
 576/4849 [==>...........................] - ETA: 7:51 - loss: 0.7652 - acc: 0.5035
 640/4849 [==>...........................] - ETA: 7:29 - loss: 0.7572 - acc: 0.5141
 704/4849 [===>..........................] - ETA: 7:12 - loss: 0.7636 - acc: 0.5014
 768/4849 [===>..........................] - ETA: 6:55 - loss: 0.7588 - acc: 0.5065
 832/4849 [====>.........................] - ETA: 6:46 - loss: 0.7547 - acc: 0.5084
 896/4849 [====>.........................] - ETA: 6:36 - loss: 0.7528 - acc: 0.5045
 960/4849 [====>.........................] - ETA: 6:24 - loss: 0.7498 - acc: 0.5073
1024/4849 [=====>........................] - ETA: 6:16 - loss: 0.7485 - acc: 0.5059
1088/4849 [=====>........................] - ETA: 6:07 - loss: 0.7535 - acc: 0.4963
1152/4849 [======>.......................] - ETA: 5:58 - loss: 0.7525 - acc: 0.4939
1216/4849 [======>.......................] - ETA: 5:49 - loss: 0.7503 - acc: 0.4992
1280/4849 [======>.......................] - ETA: 5:38 - loss: 0.7487 - acc: 0.5000
1344/4849 [=======>......................] - ETA: 5:30 - loss: 0.7463 - acc: 0.5037
1408/4849 [=======>......................] - ETA: 5:21 - loss: 0.7438 - acc: 0.5043
1472/4849 [========>.....................] - ETA: 5:13 - loss: 0.7416 - acc: 0.5048
1536/4849 [========>.....................] - ETA: 5:05 - loss: 0.7420 - acc: 0.5072
1600/4849 [========>.....................] - ETA: 4:59 - loss: 0.7424 - acc: 0.5044
1664/4849 [=========>....................] - ETA: 4:52 - loss: 0.7427 - acc: 0.5000
1728/4849 [=========>....................] - ETA: 4:47 - loss: 0.7417 - acc: 0.5012
1792/4849 [==========>...................] - ETA: 4:40 - loss: 0.7396 - acc: 0.5061
1856/4849 [==========>...................] - ETA: 4:32 - loss: 0.7386 - acc: 0.5070
1920/4849 [==========>...................] - ETA: 4:26 - loss: 0.7364 - acc: 0.5089
1984/4849 [===========>..................] - ETA: 4:19 - loss: 0.7363 - acc: 0.5111
2048/4849 [===========>..................] - ETA: 4:12 - loss: 0.7361 - acc: 0.5142
2112/4849 [============>.................] - ETA: 4:05 - loss: 0.7343 - acc: 0.5175
2176/4849 [============>.................] - ETA: 3:58 - loss: 0.7345 - acc: 0.5161
2240/4849 [============>.................] - ETA: 3:52 - loss: 0.7342 - acc: 0.5165
2304/4849 [=============>................] - ETA: 3:45 - loss: 0.7327 - acc: 0.5191
2368/4849 [=============>................] - ETA: 3:38 - loss: 0.7339 - acc: 0.5156
2432/4849 [==============>...............] - ETA: 3:32 - loss: 0.7339 - acc: 0.5123
2496/4849 [==============>...............] - ETA: 3:26 - loss: 0.7338 - acc: 0.5132
2560/4849 [==============>...............] - ETA: 3:20 - loss: 0.7339 - acc: 0.5129
2624/4849 [===============>..............] - ETA: 3:13 - loss: 0.7331 - acc: 0.5133
2688/4849 [===============>..............] - ETA: 3:08 - loss: 0.7313 - acc: 0.5153
2752/4849 [================>.............] - ETA: 3:02 - loss: 0.7302 - acc: 0.5167
2816/4849 [================>.............] - ETA: 2:56 - loss: 0.7300 - acc: 0.5170
2880/4849 [================>.............] - ETA: 2:50 - loss: 0.7302 - acc: 0.5153
2944/4849 [=================>............] - ETA: 2:44 - loss: 0.7296 - acc: 0.5156
3008/4849 [=================>............] - ETA: 2:38 - loss: 0.7299 - acc: 0.5140
3072/4849 [==================>...........] - ETA: 2:32 - loss: 0.7297 - acc: 0.5146
3136/4849 [==================>...........] - ETA: 2:27 - loss: 0.7287 - acc: 0.5156
3200/4849 [==================>...........] - ETA: 2:21 - loss: 0.7280 - acc: 0.5153
3264/4849 [===================>..........] - ETA: 2:15 - loss: 0.7270 - acc: 0.5150
3328/4849 [===================>..........] - ETA: 2:09 - loss: 0.7269 - acc: 0.5138
3392/4849 [===================>..........] - ETA: 2:04 - loss: 0.7266 - acc: 0.5139
3456/4849 [====================>.........] - ETA: 1:58 - loss: 0.7260 - acc: 0.5133
3520/4849 [====================>.........] - ETA: 1:53 - loss: 0.7253 - acc: 0.5156
3584/4849 [=====================>........] - ETA: 1:47 - loss: 0.7259 - acc: 0.5159
3648/4849 [=====================>........] - ETA: 1:41 - loss: 0.7256 - acc: 0.5167
3712/4849 [=====================>........] - ETA: 1:36 - loss: 0.7255 - acc: 0.5159
3776/4849 [======================>.......] - ETA: 1:30 - loss: 0.7251 - acc: 0.5169
3840/4849 [======================>.......] - ETA: 1:25 - loss: 0.7248 - acc: 0.5154
3904/4849 [=======================>......] - ETA: 1:19 - loss: 0.7242 - acc: 0.5164
3968/4849 [=======================>......] - ETA: 1:14 - loss: 0.7238 - acc: 0.5164
4032/4849 [=======================>......] - ETA: 1:08 - loss: 0.7234 - acc: 0.5161
4096/4849 [========================>.....] - ETA: 1:03 - loss: 0.7229 - acc: 0.5168
4160/4849 [========================>.....] - ETA: 57s - loss: 0.7229 - acc: 0.5156 
4224/4849 [=========================>....] - ETA: 52s - loss: 0.7219 - acc: 0.5166
4288/4849 [=========================>....] - ETA: 47s - loss: 0.7218 - acc: 0.5159
4352/4849 [=========================>....] - ETA: 41s - loss: 0.7221 - acc: 0.5138
4416/4849 [==========================>...] - ETA: 36s - loss: 0.7219 - acc: 0.5138
4480/4849 [==========================>...] - ETA: 30s - loss: 0.7214 - acc: 0.5147
4544/4849 [===========================>..] - ETA: 25s - loss: 0.7214 - acc: 0.5152
4608/4849 [===========================>..] - ETA: 20s - loss: 0.7216 - acc: 0.5143
4672/4849 [===========================>..] - ETA: 14s - loss: 0.7209 - acc: 0.5161
4736/4849 [============================>.] - ETA: 9s - loss: 0.7203 - acc: 0.5160 
4800/4849 [============================>.] - ETA: 4s - loss: 0.7200 - acc: 0.5156
4849/4849 [==============================] - 424s 87ms/step - loss: 0.7199 - acc: 0.5156 - val_loss: 0.6848 - val_acc: 0.5788

Epoch 00001: val_acc improved from -inf to 0.57885, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window02/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 4:27 - loss: 0.6451 - acc: 0.6250
 128/4849 [..............................] - ETA: 4:42 - loss: 0.6993 - acc: 0.5312
 192/4849 [>.............................] - ETA: 4:38 - loss: 0.6936 - acc: 0.5312
 256/4849 [>.............................] - ETA: 4:31 - loss: 0.6996 - acc: 0.5156
 320/4849 [>.............................] - ETA: 4:34 - loss: 0.7099 - acc: 0.4969
 384/4849 [=>............................] - ETA: 4:32 - loss: 0.7095 - acc: 0.5026
 448/4849 [=>............................] - ETA: 4:27 - loss: 0.7071 - acc: 0.5112
 512/4849 [==>...........................] - ETA: 4:22 - loss: 0.7037 - acc: 0.5137
 576/4849 [==>...........................] - ETA: 4:14 - loss: 0.7021 - acc: 0.5243
 640/4849 [==>...........................] - ETA: 4:16 - loss: 0.7019 - acc: 0.5281
 704/4849 [===>..........................] - ETA: 4:15 - loss: 0.6967 - acc: 0.5398
 768/4849 [===>..........................] - ETA: 4:13 - loss: 0.6977 - acc: 0.5352
 832/4849 [====>.........................] - ETA: 4:08 - loss: 0.7005 - acc: 0.5288
 896/4849 [====>.........................] - ETA: 4:06 - loss: 0.7011 - acc: 0.5335
 960/4849 [====>.........................] - ETA: 4:07 - loss: 0.7007 - acc: 0.5365
1024/4849 [=====>........................] - ETA: 4:04 - loss: 0.6998 - acc: 0.5332
1088/4849 [=====>........................] - ETA: 4:02 - loss: 0.7002 - acc: 0.5349
1152/4849 [======>.......................] - ETA: 3:59 - loss: 0.7000 - acc: 0.5347
1216/4849 [======>.......................] - ETA: 3:56 - loss: 0.7020 - acc: 0.5329
1280/4849 [======>.......................] - ETA: 3:52 - loss: 0.7004 - acc: 0.5344
1344/4849 [=======>......................] - ETA: 3:48 - loss: 0.6999 - acc: 0.5372
1408/4849 [=======>......................] - ETA: 3:43 - loss: 0.6977 - acc: 0.5405
1472/4849 [========>.....................] - ETA: 3:39 - loss: 0.6952 - acc: 0.5442
1536/4849 [========>.....................] - ETA: 3:35 - loss: 0.6964 - acc: 0.5436
1600/4849 [========>.....................] - ETA: 3:33 - loss: 0.6980 - acc: 0.5413
1664/4849 [=========>....................] - ETA: 3:28 - loss: 0.6983 - acc: 0.5379
1728/4849 [=========>....................] - ETA: 3:24 - loss: 0.6998 - acc: 0.5347
1792/4849 [==========>...................] - ETA: 3:20 - loss: 0.7012 - acc: 0.5318
1856/4849 [==========>...................] - ETA: 3:16 - loss: 0.7011 - acc: 0.5318
1920/4849 [==========>...................] - ETA: 3:12 - loss: 0.6997 - acc: 0.5339
1984/4849 [===========>..................] - ETA: 3:06 - loss: 0.6993 - acc: 0.5328
2048/4849 [===========>..................] - ETA: 3:03 - loss: 0.6997 - acc: 0.5317
2112/4849 [============>.................] - ETA: 2:59 - loss: 0.7011 - acc: 0.5298
2176/4849 [============>.................] - ETA: 2:54 - loss: 0.7007 - acc: 0.5308
2240/4849 [============>.................] - ETA: 2:50 - loss: 0.6998 - acc: 0.5321
2304/4849 [=============>................] - ETA: 2:46 - loss: 0.7008 - acc: 0.5304
2368/4849 [=============>................] - ETA: 2:42 - loss: 0.7008 - acc: 0.5291
2432/4849 [==============>...............] - ETA: 2:37 - loss: 0.6997 - acc: 0.5304
2496/4849 [==============>...............] - ETA: 2:33 - loss: 0.7002 - acc: 0.5312
2560/4849 [==============>...............] - ETA: 2:30 - loss: 0.7001 - acc: 0.5301
2624/4849 [===============>..............] - ETA: 2:25 - loss: 0.7007 - acc: 0.5282
2688/4849 [===============>..............] - ETA: 2:21 - loss: 0.7010 - acc: 0.5283
2752/4849 [================>.............] - ETA: 2:16 - loss: 0.7005 - acc: 0.5287
2816/4849 [================>.............] - ETA: 2:12 - loss: 0.7005 - acc: 0.5291
2880/4849 [================>.............] - ETA: 2:08 - loss: 0.6997 - acc: 0.5299
2944/4849 [=================>............] - ETA: 2:04 - loss: 0.7006 - acc: 0.5279
3008/4849 [=================>............] - ETA: 2:00 - loss: 0.7006 - acc: 0.5279
3072/4849 [==================>...........] - ETA: 1:56 - loss: 0.7004 - acc: 0.5283
3136/4849 [==================>...........] - ETA: 1:51 - loss: 0.7002 - acc: 0.5293
3200/4849 [==================>...........] - ETA: 1:47 - loss: 0.7004 - acc: 0.5291
3264/4849 [===================>..........] - ETA: 1:43 - loss: 0.6997 - acc: 0.5306
3328/4849 [===================>..........] - ETA: 1:38 - loss: 0.6992 - acc: 0.5312
3392/4849 [===================>..........] - ETA: 1:34 - loss: 0.6989 - acc: 0.5330
3456/4849 [====================>.........] - ETA: 1:30 - loss: 0.6988 - acc: 0.5321
3520/4849 [====================>.........] - ETA: 1:26 - loss: 0.6986 - acc: 0.5307
3584/4849 [=====================>........] - ETA: 1:22 - loss: 0.6982 - acc: 0.5310
3648/4849 [=====================>........] - ETA: 1:18 - loss: 0.6977 - acc: 0.5321
3712/4849 [=====================>........] - ETA: 1:13 - loss: 0.6984 - acc: 0.5315
3776/4849 [======================>.......] - ETA: 1:09 - loss: 0.6980 - acc: 0.5318
3840/4849 [======================>.......] - ETA: 1:05 - loss: 0.6984 - acc: 0.5312
3904/4849 [=======================>......] - ETA: 1:01 - loss: 0.6977 - acc: 0.5325
3968/4849 [=======================>......] - ETA: 57s - loss: 0.6969 - acc: 0.5340 
4032/4849 [=======================>......] - ETA: 53s - loss: 0.6967 - acc: 0.5345
4096/4849 [========================>.....] - ETA: 48s - loss: 0.6966 - acc: 0.5349
4160/4849 [========================>.....] - ETA: 44s - loss: 0.6966 - acc: 0.5356
4224/4849 [=========================>....] - ETA: 40s - loss: 0.6959 - acc: 0.5372
4288/4849 [=========================>....] - ETA: 36s - loss: 0.6957 - acc: 0.5373
4352/4849 [=========================>....] - ETA: 32s - loss: 0.6955 - acc: 0.5377
4416/4849 [==========================>...] - ETA: 28s - loss: 0.6954 - acc: 0.5371
4480/4849 [==========================>...] - ETA: 23s - loss: 0.6953 - acc: 0.5362
4544/4849 [===========================>..] - ETA: 19s - loss: 0.6952 - acc: 0.5368
4608/4849 [===========================>..] - ETA: 15s - loss: 0.6948 - acc: 0.5378
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6946 - acc: 0.5385
4736/4849 [============================>.] - ETA: 7s - loss: 0.6947 - acc: 0.5382 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6948 - acc: 0.5379
4849/4849 [==============================] - 329s 68ms/step - loss: 0.6949 - acc: 0.5370 - val_loss: 0.6871 - val_acc: 0.5566

Epoch 00002: val_acc did not improve from 0.57885
Epoch 3/10

  64/4849 [..............................] - ETA: 5:24 - loss: 0.6757 - acc: 0.6250
 128/4849 [..............................] - ETA: 5:12 - loss: 0.6830 - acc: 0.5938
 192/4849 [>.............................] - ETA: 5:20 - loss: 0.6957 - acc: 0.5833
 256/4849 [>.............................] - ETA: 5:10 - loss: 0.7047 - acc: 0.5469
 320/4849 [>.............................] - ETA: 4:58 - loss: 0.6971 - acc: 0.5531
 384/4849 [=>............................] - ETA: 4:50 - loss: 0.6931 - acc: 0.5625
 448/4849 [=>............................] - ETA: 4:45 - loss: 0.6926 - acc: 0.5580
 512/4849 [==>...........................] - ETA: 4:40 - loss: 0.6944 - acc: 0.5527
 576/4849 [==>...........................] - ETA: 4:33 - loss: 0.6939 - acc: 0.5538
 640/4849 [==>...........................] - ETA: 4:26 - loss: 0.6993 - acc: 0.5359
 704/4849 [===>..........................] - ETA: 4:26 - loss: 0.6983 - acc: 0.5341
 768/4849 [===>..........................] - ETA: 4:24 - loss: 0.6975 - acc: 0.5352
 832/4849 [====>.........................] - ETA: 4:21 - loss: 0.6971 - acc: 0.5361
 896/4849 [====>.........................] - ETA: 4:14 - loss: 0.6988 - acc: 0.5279
 960/4849 [====>.........................] - ETA: 4:08 - loss: 0.6974 - acc: 0.5281
1024/4849 [=====>........................] - ETA: 4:02 - loss: 0.6959 - acc: 0.5322
1088/4849 [=====>........................] - ETA: 3:58 - loss: 0.6970 - acc: 0.5294
1152/4849 [======>.......................] - ETA: 3:53 - loss: 0.6950 - acc: 0.5304
1216/4849 [======>.......................] - ETA: 3:48 - loss: 0.6961 - acc: 0.5263
1280/4849 [======>.......................] - ETA: 3:45 - loss: 0.6958 - acc: 0.5258
1344/4849 [=======>......................] - ETA: 3:41 - loss: 0.6948 - acc: 0.5275
1408/4849 [=======>......................] - ETA: 3:36 - loss: 0.6932 - acc: 0.5277
1472/4849 [========>.....................] - ETA: 3:31 - loss: 0.6917 - acc: 0.5333
1536/4849 [========>.....................] - ETA: 3:27 - loss: 0.6924 - acc: 0.5352
1600/4849 [========>.....................] - ETA: 3:24 - loss: 0.6916 - acc: 0.5356
1664/4849 [=========>....................] - ETA: 3:21 - loss: 0.6911 - acc: 0.5367
1728/4849 [=========>....................] - ETA: 3:17 - loss: 0.6899 - acc: 0.5382
1792/4849 [==========>...................] - ETA: 3:13 - loss: 0.6881 - acc: 0.5407
1856/4849 [==========>...................] - ETA: 3:10 - loss: 0.6873 - acc: 0.5420
1920/4849 [==========>...................] - ETA: 3:05 - loss: 0.6879 - acc: 0.5411
1984/4849 [===========>..................] - ETA: 3:00 - loss: 0.6877 - acc: 0.5428
2048/4849 [===========>..................] - ETA: 2:56 - loss: 0.6875 - acc: 0.5430
2112/4849 [============>.................] - ETA: 2:53 - loss: 0.6871 - acc: 0.5445
2176/4849 [============>.................] - ETA: 2:49 - loss: 0.6868 - acc: 0.5464
2240/4849 [============>.................] - ETA: 2:45 - loss: 0.6875 - acc: 0.5451
2304/4849 [=============>................] - ETA: 2:40 - loss: 0.6871 - acc: 0.5460
2368/4849 [=============>................] - ETA: 2:36 - loss: 0.6874 - acc: 0.5448
2432/4849 [==============>...............] - ETA: 2:32 - loss: 0.6864 - acc: 0.5477
2496/4849 [==============>...............] - ETA: 2:28 - loss: 0.6857 - acc: 0.5493
2560/4849 [==============>...............] - ETA: 2:23 - loss: 0.6857 - acc: 0.5492
2624/4849 [===============>..............] - ETA: 2:19 - loss: 0.6849 - acc: 0.5495
2688/4849 [===============>..............] - ETA: 2:15 - loss: 0.6841 - acc: 0.5506
2752/4849 [================>.............] - ETA: 2:11 - loss: 0.6841 - acc: 0.5494
2816/4849 [================>.............] - ETA: 2:07 - loss: 0.6854 - acc: 0.5476
2880/4849 [================>.............] - ETA: 2:03 - loss: 0.6857 - acc: 0.5479
2944/4849 [=================>............] - ETA: 1:59 - loss: 0.6859 - acc: 0.5482
3008/4849 [=================>............] - ETA: 1:55 - loss: 0.6864 - acc: 0.5472
3072/4849 [==================>...........] - ETA: 1:51 - loss: 0.6862 - acc: 0.5492
3136/4849 [==================>...........] - ETA: 1:47 - loss: 0.6855 - acc: 0.5504
3200/4849 [==================>...........] - ETA: 1:43 - loss: 0.6847 - acc: 0.5506
3264/4849 [===================>..........] - ETA: 1:39 - loss: 0.6853 - acc: 0.5493
3328/4849 [===================>..........] - ETA: 1:35 - loss: 0.6848 - acc: 0.5511
3392/4849 [===================>..........] - ETA: 1:32 - loss: 0.6857 - acc: 0.5513
3456/4849 [====================>.........] - ETA: 1:28 - loss: 0.6858 - acc: 0.5498
3520/4849 [====================>.........] - ETA: 1:24 - loss: 0.6861 - acc: 0.5489
3584/4849 [=====================>........] - ETA: 1:20 - loss: 0.6869 - acc: 0.5472
3648/4849 [=====================>........] - ETA: 1:16 - loss: 0.6865 - acc: 0.5474
3712/4849 [=====================>........] - ETA: 1:12 - loss: 0.6866 - acc: 0.5474
3776/4849 [======================>.......] - ETA: 1:07 - loss: 0.6862 - acc: 0.5487
3840/4849 [======================>.......] - ETA: 1:03 - loss: 0.6862 - acc: 0.5495
3904/4849 [=======================>......] - ETA: 59s - loss: 0.6871 - acc: 0.5482 
3968/4849 [=======================>......] - ETA: 55s - loss: 0.6873 - acc: 0.5486
4032/4849 [=======================>......] - ETA: 51s - loss: 0.6874 - acc: 0.5479
4096/4849 [========================>.....] - ETA: 47s - loss: 0.6875 - acc: 0.5483
4160/4849 [========================>.....] - ETA: 43s - loss: 0.6876 - acc: 0.5476
4224/4849 [=========================>....] - ETA: 39s - loss: 0.6873 - acc: 0.5481
4288/4849 [=========================>....] - ETA: 35s - loss: 0.6873 - acc: 0.5478
4352/4849 [=========================>....] - ETA: 31s - loss: 0.6878 - acc: 0.5478
4416/4849 [==========================>...] - ETA: 27s - loss: 0.6881 - acc: 0.5476
4480/4849 [==========================>...] - ETA: 23s - loss: 0.6886 - acc: 0.5469
4544/4849 [===========================>..] - ETA: 19s - loss: 0.6893 - acc: 0.5451
4608/4849 [===========================>..] - ETA: 15s - loss: 0.6890 - acc: 0.5454
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6893 - acc: 0.5456
4736/4849 [============================>.] - ETA: 7s - loss: 0.6895 - acc: 0.5448 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6898 - acc: 0.5437
4849/4849 [==============================] - 318s 66ms/step - loss: 0.6895 - acc: 0.5446 - val_loss: 0.6889 - val_acc: 0.5622

Epoch 00003: val_acc did not improve from 0.57885
Epoch 4/10

  64/4849 [..............................] - ETA: 4:41 - loss: 0.6952 - acc: 0.5312
 128/4849 [..............................] - ETA: 4:49 - loss: 0.6851 - acc: 0.5625
 192/4849 [>.............................] - ETA: 4:39 - loss: 0.6792 - acc: 0.5729
 256/4849 [>.............................] - ETA: 5:00 - loss: 0.6764 - acc: 0.5898
 320/4849 [>.............................] - ETA: 4:55 - loss: 0.6775 - acc: 0.5938
 384/4849 [=>............................] - ETA: 4:51 - loss: 0.6817 - acc: 0.5859
 448/4849 [=>............................] - ETA: 4:50 - loss: 0.6820 - acc: 0.5759
 512/4849 [==>...........................] - ETA: 4:50 - loss: 0.6780 - acc: 0.5820
 576/4849 [==>...........................] - ETA: 4:50 - loss: 0.6785 - acc: 0.5816
 640/4849 [==>...........................] - ETA: 4:44 - loss: 0.6769 - acc: 0.5828
 704/4849 [===>..........................] - ETA: 4:37 - loss: 0.6771 - acc: 0.5838
 768/4849 [===>..........................] - ETA: 4:35 - loss: 0.6782 - acc: 0.5807
 832/4849 [====>.........................] - ETA: 4:33 - loss: 0.6765 - acc: 0.5865
 896/4849 [====>.........................] - ETA: 4:27 - loss: 0.6794 - acc: 0.5792
 960/4849 [====>.........................] - ETA: 4:24 - loss: 0.6782 - acc: 0.5823
1024/4849 [=====>........................] - ETA: 4:21 - loss: 0.6793 - acc: 0.5840
1088/4849 [=====>........................] - ETA: 4:15 - loss: 0.6786 - acc: 0.5855
1152/4849 [======>.......................] - ETA: 4:09 - loss: 0.6788 - acc: 0.5868
1216/4849 [======>.......................] - ETA: 4:05 - loss: 0.6791 - acc: 0.5839
1280/4849 [======>.......................] - ETA: 4:01 - loss: 0.6802 - acc: 0.5813
1344/4849 [=======>......................] - ETA: 3:56 - loss: 0.6813 - acc: 0.5774
1408/4849 [=======>......................] - ETA: 3:52 - loss: 0.6793 - acc: 0.5824
1472/4849 [========>.....................] - ETA: 3:47 - loss: 0.6807 - acc: 0.5795
1536/4849 [========>.....................] - ETA: 3:41 - loss: 0.6781 - acc: 0.5872
1600/4849 [========>.....................] - ETA: 3:36 - loss: 0.6793 - acc: 0.5819
1664/4849 [=========>....................] - ETA: 3:34 - loss: 0.6798 - acc: 0.5817
1728/4849 [=========>....................] - ETA: 3:29 - loss: 0.6798 - acc: 0.5816
1792/4849 [==========>...................] - ETA: 3:25 - loss: 0.6810 - acc: 0.5792
1856/4849 [==========>...................] - ETA: 3:20 - loss: 0.6806 - acc: 0.5808
1920/4849 [==========>...................] - ETA: 3:15 - loss: 0.6811 - acc: 0.5802
1984/4849 [===========>..................] - ETA: 3:10 - loss: 0.6804 - acc: 0.5811
2048/4849 [===========>..................] - ETA: 3:06 - loss: 0.6805 - acc: 0.5820
2112/4849 [============>.................] - ETA: 3:02 - loss: 0.6810 - acc: 0.5824
2176/4849 [============>.................] - ETA: 2:58 - loss: 0.6803 - acc: 0.5818
2240/4849 [============>.................] - ETA: 2:54 - loss: 0.6809 - acc: 0.5804
2304/4849 [=============>................] - ETA: 2:50 - loss: 0.6814 - acc: 0.5799
2368/4849 [=============>................] - ETA: 2:45 - loss: 0.6810 - acc: 0.5794
2432/4849 [==============>...............] - ETA: 2:40 - loss: 0.6817 - acc: 0.5789
2496/4849 [==============>...............] - ETA: 2:35 - loss: 0.6820 - acc: 0.5785
2560/4849 [==============>...............] - ETA: 2:31 - loss: 0.6823 - acc: 0.5766
2624/4849 [===============>..............] - ETA: 2:26 - loss: 0.6834 - acc: 0.5755
2688/4849 [===============>..............] - ETA: 2:22 - loss: 0.6821 - acc: 0.5778
2752/4849 [================>.............] - ETA: 2:18 - loss: 0.6805 - acc: 0.5803
2816/4849 [================>.............] - ETA: 2:13 - loss: 0.6803 - acc: 0.5813
2880/4849 [================>.............] - ETA: 2:08 - loss: 0.6804 - acc: 0.5802
2944/4849 [=================>............] - ETA: 2:04 - loss: 0.6811 - acc: 0.5768
3008/4849 [=================>............] - ETA: 2:00 - loss: 0.6816 - acc: 0.5765
3072/4849 [==================>...........] - ETA: 1:55 - loss: 0.6819 - acc: 0.5729
3136/4849 [==================>...........] - ETA: 1:51 - loss: 0.6817 - acc: 0.5724
3200/4849 [==================>...........] - ETA: 1:47 - loss: 0.6816 - acc: 0.5713
3264/4849 [===================>..........] - ETA: 1:43 - loss: 0.6809 - acc: 0.5723
3328/4849 [===================>..........] - ETA: 1:39 - loss: 0.6807 - acc: 0.5718
3392/4849 [===================>..........] - ETA: 1:35 - loss: 0.6817 - acc: 0.5705
3456/4849 [====================>.........] - ETA: 1:31 - loss: 0.6818 - acc: 0.5709
3520/4849 [====================>.........] - ETA: 1:26 - loss: 0.6824 - acc: 0.5710
3584/4849 [=====================>........] - ETA: 1:22 - loss: 0.6824 - acc: 0.5709
3648/4849 [=====================>........] - ETA: 1:18 - loss: 0.6827 - acc: 0.5713
3712/4849 [=====================>........] - ETA: 1:14 - loss: 0.6828 - acc: 0.5719
3776/4849 [======================>.......] - ETA: 1:09 - loss: 0.6833 - acc: 0.5715
3840/4849 [======================>.......] - ETA: 1:05 - loss: 0.6841 - acc: 0.5701
3904/4849 [=======================>......] - ETA: 1:01 - loss: 0.6836 - acc: 0.5704
3968/4849 [=======================>......] - ETA: 57s - loss: 0.6838 - acc: 0.5706 
4032/4849 [=======================>......] - ETA: 53s - loss: 0.6838 - acc: 0.5704
4096/4849 [========================>.....] - ETA: 49s - loss: 0.6839 - acc: 0.5693
4160/4849 [========================>.....] - ETA: 44s - loss: 0.6837 - acc: 0.5685
4224/4849 [=========================>....] - ETA: 40s - loss: 0.6838 - acc: 0.5670
4288/4849 [=========================>....] - ETA: 36s - loss: 0.6838 - acc: 0.5667
4352/4849 [=========================>....] - ETA: 32s - loss: 0.6841 - acc: 0.5657
4416/4849 [==========================>...] - ETA: 28s - loss: 0.6837 - acc: 0.5673
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6836 - acc: 0.5670
4544/4849 [===========================>..] - ETA: 19s - loss: 0.6836 - acc: 0.5673
4608/4849 [===========================>..] - ETA: 15s - loss: 0.6835 - acc: 0.5671
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6834 - acc: 0.5659
4736/4849 [============================>.] - ETA: 7s - loss: 0.6835 - acc: 0.5655 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6830 - acc: 0.5658
4849/4849 [==============================] - 329s 68ms/step - loss: 0.6830 - acc: 0.5659 - val_loss: 0.6871 - val_acc: 0.5306

Epoch 00004: val_acc did not improve from 0.57885
Epoch 5/10

  64/4849 [..............................] - ETA: 4:38 - loss: 0.6772 - acc: 0.6250
 128/4849 [..............................] - ETA: 4:47 - loss: 0.6646 - acc: 0.6094
 192/4849 [>.............................] - ETA: 4:45 - loss: 0.6749 - acc: 0.5625
 256/4849 [>.............................] - ETA: 4:36 - loss: 0.6670 - acc: 0.5898
 320/4849 [>.............................] - ETA: 4:33 - loss: 0.6640 - acc: 0.5969
 384/4849 [=>............................] - ETA: 4:31 - loss: 0.6718 - acc: 0.5677
 448/4849 [=>............................] - ETA: 4:25 - loss: 0.6771 - acc: 0.5558
 512/4849 [==>...........................] - ETA: 4:24 - loss: 0.6745 - acc: 0.5586
 576/4849 [==>...........................] - ETA: 4:20 - loss: 0.6786 - acc: 0.5556
 640/4849 [==>...........................] - ETA: 4:16 - loss: 0.6795 - acc: 0.5500
 704/4849 [===>..........................] - ETA: 4:13 - loss: 0.6795 - acc: 0.5526
 768/4849 [===>..........................] - ETA: 4:06 - loss: 0.6828 - acc: 0.5534
 832/4849 [====>.........................] - ETA: 4:01 - loss: 0.6820 - acc: 0.5613
 896/4849 [====>.........................] - ETA: 3:57 - loss: 0.6826 - acc: 0.5580
 960/4849 [====>.........................] - ETA: 3:54 - loss: 0.6814 - acc: 0.5615
1024/4849 [=====>........................] - ETA: 3:49 - loss: 0.6809 - acc: 0.5635
1088/4849 [=====>........................] - ETA: 3:46 - loss: 0.6804 - acc: 0.5643
1152/4849 [======>.......................] - ETA: 3:42 - loss: 0.6817 - acc: 0.5625
1216/4849 [======>.......................] - ETA: 3:38 - loss: 0.6804 - acc: 0.5674
1280/4849 [======>.......................] - ETA: 3:34 - loss: 0.6804 - acc: 0.5648
1344/4849 [=======>......................] - ETA: 3:30 - loss: 0.6799 - acc: 0.5655
1408/4849 [=======>......................] - ETA: 3:26 - loss: 0.6771 - acc: 0.5724
1472/4849 [========>.....................] - ETA: 3:22 - loss: 0.6783 - acc: 0.5707
1536/4849 [========>.....................] - ETA: 3:19 - loss: 0.6787 - acc: 0.5697
1600/4849 [========>.....................] - ETA: 3:14 - loss: 0.6791 - acc: 0.5725
1664/4849 [=========>....................] - ETA: 3:10 - loss: 0.6785 - acc: 0.5733
1728/4849 [=========>....................] - ETA: 3:07 - loss: 0.6795 - acc: 0.5712
1792/4849 [==========>...................] - ETA: 3:04 - loss: 0.6791 - acc: 0.5714
1856/4849 [==========>...................] - ETA: 2:59 - loss: 0.6795 - acc: 0.5684
1920/4849 [==========>...................] - ETA: 2:56 - loss: 0.6793 - acc: 0.5677
1984/4849 [===========>..................] - ETA: 2:52 - loss: 0.6802 - acc: 0.5640
2048/4849 [===========>..................] - ETA: 2:48 - loss: 0.6805 - acc: 0.5635
2112/4849 [============>.................] - ETA: 2:45 - loss: 0.6810 - acc: 0.5630
2176/4849 [============>.................] - ETA: 2:41 - loss: 0.6791 - acc: 0.5662
2240/4849 [============>.................] - ETA: 2:37 - loss: 0.6781 - acc: 0.5687
2304/4849 [=============>................] - ETA: 2:34 - loss: 0.6775 - acc: 0.5699
2368/4849 [=============>................] - ETA: 2:30 - loss: 0.6778 - acc: 0.5680
2432/4849 [==============>...............] - ETA: 2:26 - loss: 0.6767 - acc: 0.5711
2496/4849 [==============>...............] - ETA: 2:22 - loss: 0.6777 - acc: 0.5701
2560/4849 [==============>...............] - ETA: 2:18 - loss: 0.6784 - acc: 0.5699
2624/4849 [===============>..............] - ETA: 2:15 - loss: 0.6777 - acc: 0.5720
2688/4849 [===============>..............] - ETA: 2:11 - loss: 0.6774 - acc: 0.5733
2752/4849 [================>.............] - ETA: 2:06 - loss: 0.6772 - acc: 0.5734
2816/4849 [================>.............] - ETA: 2:02 - loss: 0.6785 - acc: 0.5717
2880/4849 [================>.............] - ETA: 1:59 - loss: 0.6789 - acc: 0.5719
2944/4849 [=================>............] - ETA: 1:55 - loss: 0.6793 - acc: 0.5696
3008/4849 [=================>............] - ETA: 1:51 - loss: 0.6788 - acc: 0.5715
3072/4849 [==================>...........] - ETA: 1:47 - loss: 0.6796 - acc: 0.5710
3136/4849 [==================>...........] - ETA: 1:43 - loss: 0.6797 - acc: 0.5714
3200/4849 [==================>...........] - ETA: 1:39 - loss: 0.6804 - acc: 0.5703
3264/4849 [===================>..........] - ETA: 1:35 - loss: 0.6811 - acc: 0.5686
3328/4849 [===================>..........] - ETA: 1:31 - loss: 0.6818 - acc: 0.5667
3392/4849 [===================>..........] - ETA: 1:28 - loss: 0.6820 - acc: 0.5663
3456/4849 [====================>.........] - ETA: 1:24 - loss: 0.6808 - acc: 0.5683
3520/4849 [====================>.........] - ETA: 1:20 - loss: 0.6823 - acc: 0.5656
3584/4849 [=====================>........] - ETA: 1:16 - loss: 0.6817 - acc: 0.5667
3648/4849 [=====================>........] - ETA: 1:12 - loss: 0.6824 - acc: 0.5652
3712/4849 [=====================>........] - ETA: 1:08 - loss: 0.6821 - acc: 0.5657
3776/4849 [======================>.......] - ETA: 1:04 - loss: 0.6825 - acc: 0.5657
3840/4849 [======================>.......] - ETA: 1:00 - loss: 0.6833 - acc: 0.5643
3904/4849 [=======================>......] - ETA: 57s - loss: 0.6834 - acc: 0.5648 
3968/4849 [=======================>......] - ETA: 53s - loss: 0.6834 - acc: 0.5650
4032/4849 [=======================>......] - ETA: 49s - loss: 0.6837 - acc: 0.5642
4096/4849 [========================>.....] - ETA: 45s - loss: 0.6844 - acc: 0.5635
4160/4849 [========================>.....] - ETA: 41s - loss: 0.6848 - acc: 0.5627
4224/4849 [=========================>....] - ETA: 37s - loss: 0.6849 - acc: 0.5630
4288/4849 [=========================>....] - ETA: 34s - loss: 0.6846 - acc: 0.5623
4352/4849 [=========================>....] - ETA: 30s - loss: 0.6843 - acc: 0.5618
4416/4849 [==========================>...] - ETA: 26s - loss: 0.6841 - acc: 0.5625
4480/4849 [==========================>...] - ETA: 22s - loss: 0.6847 - acc: 0.5607
4544/4849 [===========================>..] - ETA: 18s - loss: 0.6841 - acc: 0.5616
4608/4849 [===========================>..] - ETA: 14s - loss: 0.6839 - acc: 0.5625
4672/4849 [===========================>..] - ETA: 10s - loss: 0.6842 - acc: 0.5621
4736/4849 [============================>.] - ETA: 6s - loss: 0.6839 - acc: 0.5619 
4800/4849 [============================>.] - ETA: 2s - loss: 0.6840 - acc: 0.5617
4849/4849 [==============================] - 307s 63ms/step - loss: 0.6839 - acc: 0.5611 - val_loss: 0.6880 - val_acc: 0.5603

Epoch 00005: val_acc did not improve from 0.57885
Epoch 6/10

  64/4849 [..............................] - ETA: 4:48 - loss: 0.7134 - acc: 0.5625
 128/4849 [..............................] - ETA: 4:45 - loss: 0.7065 - acc: 0.5391
 192/4849 [>.............................] - ETA: 4:37 - loss: 0.6994 - acc: 0.5573
 256/4849 [>.............................] - ETA: 4:44 - loss: 0.7102 - acc: 0.5195
 320/4849 [>.............................] - ETA: 4:36 - loss: 0.7126 - acc: 0.5062
 384/4849 [=>............................] - ETA: 4:27 - loss: 0.7056 - acc: 0.5339
 448/4849 [=>............................] - ETA: 4:19 - loss: 0.6974 - acc: 0.5446
 512/4849 [==>...........................] - ETA: 4:12 - loss: 0.6949 - acc: 0.5430
 576/4849 [==>...........................] - ETA: 4:10 - loss: 0.6880 - acc: 0.5486
 640/4849 [==>...........................] - ETA: 4:07 - loss: 0.6858 - acc: 0.5578
 704/4849 [===>..........................] - ETA: 4:05 - loss: 0.6840 - acc: 0.5653
 768/4849 [===>..........................] - ETA: 4:05 - loss: 0.6860 - acc: 0.5612
 832/4849 [====>.........................] - ETA: 4:01 - loss: 0.6870 - acc: 0.5577
 896/4849 [====>.........................] - ETA: 3:56 - loss: 0.6872 - acc: 0.5547
 960/4849 [====>.........................] - ETA: 3:52 - loss: 0.6866 - acc: 0.5583
1024/4849 [=====>........................] - ETA: 3:48 - loss: 0.6861 - acc: 0.5625
1088/4849 [=====>........................] - ETA: 3:45 - loss: 0.6877 - acc: 0.5579
1152/4849 [======>.......................] - ETA: 3:40 - loss: 0.6875 - acc: 0.5582
1216/4849 [======>.......................] - ETA: 3:38 - loss: 0.6881 - acc: 0.5551
1280/4849 [======>.......................] - ETA: 3:36 - loss: 0.6870 - acc: 0.5555
1344/4849 [=======>......................] - ETA: 3:32 - loss: 0.6873 - acc: 0.5536
1408/4849 [=======>......................] - ETA: 3:27 - loss: 0.6860 - acc: 0.5575
1472/4849 [========>.....................] - ETA: 3:23 - loss: 0.6860 - acc: 0.5550
1536/4849 [========>.....................] - ETA: 3:19 - loss: 0.6859 - acc: 0.5553
1600/4849 [========>.....................] - ETA: 3:14 - loss: 0.6845 - acc: 0.5594
1664/4849 [=========>....................] - ETA: 3:09 - loss: 0.6854 - acc: 0.5577
1728/4849 [=========>....................] - ETA: 3:05 - loss: 0.6847 - acc: 0.5579
1792/4849 [==========>...................] - ETA: 3:02 - loss: 0.6842 - acc: 0.5586
1856/4849 [==========>...................] - ETA: 2:57 - loss: 0.6828 - acc: 0.5609
1920/4849 [==========>...................] - ETA: 2:53 - loss: 0.6832 - acc: 0.5620
1984/4849 [===========>..................] - ETA: 2:49 - loss: 0.6831 - acc: 0.5620
2048/4849 [===========>..................] - ETA: 2:45 - loss: 0.6826 - acc: 0.5635
2112/4849 [============>.................] - ETA: 2:41 - loss: 0.6814 - acc: 0.5668
2176/4849 [============>.................] - ETA: 2:38 - loss: 0.6824 - acc: 0.5648
2240/4849 [============>.................] - ETA: 2:34 - loss: 0.6818 - acc: 0.5683
2304/4849 [=============>................] - ETA: 2:31 - loss: 0.6810 - acc: 0.5690
2368/4849 [=============>................] - ETA: 2:27 - loss: 0.6817 - acc: 0.5650
2432/4849 [==============>...............] - ETA: 2:23 - loss: 0.6810 - acc: 0.5662
2496/4849 [==============>...............] - ETA: 2:19 - loss: 0.6800 - acc: 0.5681
2560/4849 [==============>...............] - ETA: 2:15 - loss: 0.6804 - acc: 0.5664
2624/4849 [===============>..............] - ETA: 2:11 - loss: 0.6799 - acc: 0.5663
2688/4849 [===============>..............] - ETA: 2:07 - loss: 0.6795 - acc: 0.5662
2752/4849 [================>.............] - ETA: 2:03 - loss: 0.6790 - acc: 0.5672
2816/4849 [================>.............] - ETA: 1:59 - loss: 0.6786 - acc: 0.5675
2880/4849 [================>.............] - ETA: 1:55 - loss: 0.6784 - acc: 0.5663
2944/4849 [=================>............] - ETA: 1:51 - loss: 0.6789 - acc: 0.5659
3008/4849 [=================>............] - ETA: 1:47 - loss: 0.6784 - acc: 0.5672
3072/4849 [==================>...........] - ETA: 1:43 - loss: 0.6782 - acc: 0.5677
3136/4849 [==================>...........] - ETA: 1:39 - loss: 0.6781 - acc: 0.5679
3200/4849 [==================>...........] - ETA: 1:35 - loss: 0.6786 - acc: 0.5681
3264/4849 [===================>..........] - ETA: 1:31 - loss: 0.6782 - acc: 0.5686
3328/4849 [===================>..........] - ETA: 1:27 - loss: 0.6782 - acc: 0.5697
3392/4849 [===================>..........] - ETA: 1:23 - loss: 0.6786 - acc: 0.5690
3456/4849 [====================>.........] - ETA: 1:19 - loss: 0.6783 - acc: 0.5686
3520/4849 [====================>.........] - ETA: 1:16 - loss: 0.6800 - acc: 0.5645
3584/4849 [=====================>........] - ETA: 1:12 - loss: 0.6795 - acc: 0.5656
3648/4849 [=====================>........] - ETA: 1:08 - loss: 0.6791 - acc: 0.5674
3712/4849 [=====================>........] - ETA: 1:04 - loss: 0.6795 - acc: 0.5665
3776/4849 [======================>.......] - ETA: 1:00 - loss: 0.6790 - acc: 0.5681
3840/4849 [======================>.......] - ETA: 56s - loss: 0.6793 - acc: 0.5680 
3904/4849 [=======================>......] - ETA: 53s - loss: 0.6793 - acc: 0.5692
3968/4849 [=======================>......] - ETA: 49s - loss: 0.6795 - acc: 0.5691
4032/4849 [=======================>......] - ETA: 46s - loss: 0.6793 - acc: 0.5692
4096/4849 [========================>.....] - ETA: 42s - loss: 0.6796 - acc: 0.5698
4160/4849 [========================>.....] - ETA: 38s - loss: 0.6794 - acc: 0.5702
4224/4849 [=========================>....] - ETA: 35s - loss: 0.6794 - acc: 0.5715
4288/4849 [=========================>....] - ETA: 31s - loss: 0.6795 - acc: 0.5707
4352/4849 [=========================>....] - ETA: 27s - loss: 0.6791 - acc: 0.5715
4416/4849 [==========================>...] - ETA: 24s - loss: 0.6790 - acc: 0.5716
4480/4849 [==========================>...] - ETA: 20s - loss: 0.6797 - acc: 0.5701
4544/4849 [===========================>..] - ETA: 17s - loss: 0.6797 - acc: 0.5709
4608/4849 [===========================>..] - ETA: 13s - loss: 0.6794 - acc: 0.5710
4672/4849 [===========================>..] - ETA: 9s - loss: 0.6791 - acc: 0.5711 
4736/4849 [============================>.] - ETA: 6s - loss: 0.6792 - acc: 0.5712
4800/4849 [============================>.] - ETA: 2s - loss: 0.6787 - acc: 0.5721
4849/4849 [==============================] - 280s 58ms/step - loss: 0.6782 - acc: 0.5727 - val_loss: 0.6795 - val_acc: 0.6104

Epoch 00006: val_acc improved from 0.57885 to 0.61039, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window02/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 7/10

  64/4849 [..............................] - ETA: 5:21 - loss: 0.7215 - acc: 0.4688
 128/4849 [..............................] - ETA: 5:11 - loss: 0.6960 - acc: 0.4922
 192/4849 [>.............................] - ETA: 5:03 - loss: 0.7002 - acc: 0.5104
 256/4849 [>.............................] - ETA: 5:01 - loss: 0.6888 - acc: 0.5430
 320/4849 [>.............................] - ETA: 4:58 - loss: 0.6828 - acc: 0.5625
 384/4849 [=>............................] - ETA: 4:51 - loss: 0.6890 - acc: 0.5547
 448/4849 [=>............................] - ETA: 4:49 - loss: 0.6873 - acc: 0.5491
 512/4849 [==>...........................] - ETA: 4:45 - loss: 0.6848 - acc: 0.5625
 576/4849 [==>...........................] - ETA: 4:42 - loss: 0.6805 - acc: 0.5729
 640/4849 [==>...........................] - ETA: 4:36 - loss: 0.6807 - acc: 0.5687
 704/4849 [===>..........................] - ETA: 4:30 - loss: 0.6801 - acc: 0.5696
 768/4849 [===>..........................] - ETA: 4:26 - loss: 0.6802 - acc: 0.5677
 832/4849 [====>.........................] - ETA: 4:22 - loss: 0.6795 - acc: 0.5685
 896/4849 [====>.........................] - ETA: 4:20 - loss: 0.6800 - acc: 0.5692
 960/4849 [====>.........................] - ETA: 4:16 - loss: 0.6782 - acc: 0.5708
1024/4849 [=====>........................] - ETA: 4:12 - loss: 0.6795 - acc: 0.5684
1088/4849 [=====>........................] - ETA: 4:06 - loss: 0.6786 - acc: 0.5699
1152/4849 [======>.......................] - ETA: 4:01 - loss: 0.6789 - acc: 0.5694
1216/4849 [======>.......................] - ETA: 3:57 - loss: 0.6783 - acc: 0.5715
1280/4849 [======>.......................] - ETA: 3:52 - loss: 0.6781 - acc: 0.5727
1344/4849 [=======>......................] - ETA: 3:48 - loss: 0.6771 - acc: 0.5737
1408/4849 [=======>......................] - ETA: 3:45 - loss: 0.6771 - acc: 0.5739
1472/4849 [========>.....................] - ETA: 3:41 - loss: 0.6765 - acc: 0.5747
1536/4849 [========>.....................] - ETA: 3:37 - loss: 0.6760 - acc: 0.5749
1600/4849 [========>.....................] - ETA: 3:32 - loss: 0.6769 - acc: 0.5737
1664/4849 [=========>....................] - ETA: 3:28 - loss: 0.6760 - acc: 0.5739
1728/4849 [=========>....................] - ETA: 3:24 - loss: 0.6755 - acc: 0.5775
1792/4849 [==========>...................] - ETA: 3:22 - loss: 0.6743 - acc: 0.5831
1856/4849 [==========>...................] - ETA: 3:19 - loss: 0.6755 - acc: 0.5830
1920/4849 [==========>...................] - ETA: 3:18 - loss: 0.6766 - acc: 0.5813
1984/4849 [===========>..................] - ETA: 3:15 - loss: 0.6764 - acc: 0.5837
2048/4849 [===========>..................] - ETA: 3:11 - loss: 0.6756 - acc: 0.5840
2112/4849 [============>.................] - ETA: 3:08 - loss: 0.6743 - acc: 0.5866
2176/4849 [============>.................] - ETA: 3:05 - loss: 0.6741 - acc: 0.5855
2240/4849 [============>.................] - ETA: 3:02 - loss: 0.6741 - acc: 0.5862
2304/4849 [=============>................] - ETA: 2:59 - loss: 0.6746 - acc: 0.5851
2368/4849 [=============>................] - ETA: 2:55 - loss: 0.6738 - acc: 0.5870
2432/4849 [==============>...............] - ETA: 2:51 - loss: 0.6733 - acc: 0.5880
2496/4849 [==============>...............] - ETA: 2:48 - loss: 0.6735 - acc: 0.5881
2560/4849 [==============>...............] - ETA: 2:44 - loss: 0.6739 - acc: 0.5887
2624/4849 [===============>..............] - ETA: 2:40 - loss: 0.6740 - acc: 0.5877
2688/4849 [===============>..............] - ETA: 2:36 - loss: 0.6755 - acc: 0.5841
2752/4849 [================>.............] - ETA: 2:32 - loss: 0.6766 - acc: 0.5825
2816/4849 [================>.............] - ETA: 2:28 - loss: 0.6763 - acc: 0.5817
2880/4849 [================>.............] - ETA: 2:24 - loss: 0.6774 - acc: 0.5795
2944/4849 [=================>............] - ETA: 2:19 - loss: 0.6768 - acc: 0.5798
3008/4849 [=================>............] - ETA: 2:15 - loss: 0.6772 - acc: 0.5778
3072/4849 [==================>...........] - ETA: 2:11 - loss: 0.6769 - acc: 0.5794
3136/4849 [==================>...........] - ETA: 2:06 - loss: 0.6774 - acc: 0.5797
3200/4849 [==================>...........] - ETA: 2:01 - loss: 0.6765 - acc: 0.5813
3264/4849 [===================>..........] - ETA: 1:57 - loss: 0.6766 - acc: 0.5803
3328/4849 [===================>..........] - ETA: 1:53 - loss: 0.6763 - acc: 0.5796
3392/4849 [===================>..........] - ETA: 1:48 - loss: 0.6759 - acc: 0.5799
3456/4849 [====================>.........] - ETA: 1:44 - loss: 0.6764 - acc: 0.5796
3520/4849 [====================>.........] - ETA: 1:39 - loss: 0.6765 - acc: 0.5790
3584/4849 [=====================>........] - ETA: 1:34 - loss: 0.6762 - acc: 0.5790
3648/4849 [=====================>........] - ETA: 1:30 - loss: 0.6761 - acc: 0.5795
3712/4849 [=====================>........] - ETA: 1:25 - loss: 0.6752 - acc: 0.5811
3776/4849 [======================>.......] - ETA: 1:21 - loss: 0.6755 - acc: 0.5813
3840/4849 [======================>.......] - ETA: 1:16 - loss: 0.6754 - acc: 0.5807
3904/4849 [=======================>......] - ETA: 1:11 - loss: 0.6748 - acc: 0.5825
3968/4849 [=======================>......] - ETA: 1:07 - loss: 0.6752 - acc: 0.5809
4032/4849 [=======================>......] - ETA: 1:02 - loss: 0.6754 - acc: 0.5806
4096/4849 [========================>.....] - ETA: 57s - loss: 0.6761 - acc: 0.5798 
4160/4849 [========================>.....] - ETA: 52s - loss: 0.6765 - acc: 0.5786
4224/4849 [=========================>....] - ETA: 47s - loss: 0.6769 - acc: 0.5784
4288/4849 [=========================>....] - ETA: 42s - loss: 0.6773 - acc: 0.5772
4352/4849 [=========================>....] - ETA: 38s - loss: 0.6786 - acc: 0.5747
4416/4849 [==========================>...] - ETA: 33s - loss: 0.6778 - acc: 0.5763
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6780 - acc: 0.5754
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6777 - acc: 0.5766
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6779 - acc: 0.5757
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6782 - acc: 0.5749
4736/4849 [============================>.] - ETA: 8s - loss: 0.6779 - acc: 0.5758 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6779 - acc: 0.5765
4849/4849 [==============================] - 389s 80ms/step - loss: 0.6773 - acc: 0.5776 - val_loss: 0.6937 - val_acc: 0.5306

Epoch 00007: val_acc did not improve from 0.61039
Epoch 8/10

  64/4849 [..............................] - ETA: 6:48 - loss: 0.6658 - acc: 0.5938
 128/4849 [..............................] - ETA: 7:04 - loss: 0.6552 - acc: 0.6250
 192/4849 [>.............................] - ETA: 6:41 - loss: 0.6628 - acc: 0.5833
 256/4849 [>.............................] - ETA: 6:24 - loss: 0.6554 - acc: 0.5938
 320/4849 [>.............................] - ETA: 6:19 - loss: 0.6620 - acc: 0.5813
 384/4849 [=>............................] - ETA: 6:12 - loss: 0.6695 - acc: 0.5599
 448/4849 [=>............................] - ETA: 6:12 - loss: 0.6678 - acc: 0.5647
 512/4849 [==>...........................] - ETA: 6:08 - loss: 0.6718 - acc: 0.5566
 576/4849 [==>...........................] - ETA: 6:01 - loss: 0.6765 - acc: 0.5556
 640/4849 [==>...........................] - ETA: 5:52 - loss: 0.6765 - acc: 0.5547
 704/4849 [===>..........................] - ETA: 5:48 - loss: 0.6833 - acc: 0.5483
 768/4849 [===>..........................] - ETA: 5:44 - loss: 0.6820 - acc: 0.5534
 832/4849 [====>.........................] - ETA: 5:37 - loss: 0.6823 - acc: 0.5553
 896/4849 [====>.........................] - ETA: 5:32 - loss: 0.6832 - acc: 0.5513
 960/4849 [====>.........................] - ETA: 5:26 - loss: 0.6825 - acc: 0.5552
1024/4849 [=====>........................] - ETA: 5:19 - loss: 0.6809 - acc: 0.5645
1088/4849 [=====>........................] - ETA: 5:12 - loss: 0.6789 - acc: 0.5671
1152/4849 [======>.......................] - ETA: 5:07 - loss: 0.6780 - acc: 0.5712
1216/4849 [======>.......................] - ETA: 5:02 - loss: 0.6781 - acc: 0.5748
1280/4849 [======>.......................] - ETA: 4:57 - loss: 0.6785 - acc: 0.5742
1344/4849 [=======>......................] - ETA: 4:52 - loss: 0.6794 - acc: 0.5737
1408/4849 [=======>......................] - ETA: 4:46 - loss: 0.6796 - acc: 0.5710
1472/4849 [========>.....................] - ETA: 4:41 - loss: 0.6785 - acc: 0.5740
1536/4849 [========>.....................] - ETA: 4:36 - loss: 0.6793 - acc: 0.5723
1600/4849 [========>.....................] - ETA: 4:29 - loss: 0.6792 - acc: 0.5719
1664/4849 [=========>....................] - ETA: 4:24 - loss: 0.6775 - acc: 0.5757
1728/4849 [=========>....................] - ETA: 4:19 - loss: 0.6772 - acc: 0.5741
1792/4849 [==========>...................] - ETA: 4:13 - loss: 0.6761 - acc: 0.5765
1856/4849 [==========>...................] - ETA: 4:09 - loss: 0.6755 - acc: 0.5754
1920/4849 [==========>...................] - ETA: 4:04 - loss: 0.6754 - acc: 0.5771
1984/4849 [===========>..................] - ETA: 3:58 - loss: 0.6745 - acc: 0.5786
2048/4849 [===========>..................] - ETA: 3:53 - loss: 0.6743 - acc: 0.5781
2112/4849 [============>.................] - ETA: 3:48 - loss: 0.6749 - acc: 0.5767
2176/4849 [============>.................] - ETA: 3:42 - loss: 0.6759 - acc: 0.5740
2240/4849 [============>.................] - ETA: 3:37 - loss: 0.6748 - acc: 0.5777
2304/4849 [=============>................] - ETA: 3:32 - loss: 0.6748 - acc: 0.5786
2368/4849 [=============>................] - ETA: 3:26 - loss: 0.6750 - acc: 0.5798
2432/4849 [==============>...............] - ETA: 3:21 - loss: 0.6748 - acc: 0.5798
2496/4849 [==============>...............] - ETA: 3:15 - loss: 0.6746 - acc: 0.5797
2560/4849 [==============>...............] - ETA: 3:10 - loss: 0.6746 - acc: 0.5785
2624/4849 [===============>..............] - ETA: 3:04 - loss: 0.6738 - acc: 0.5800
2688/4849 [===============>..............] - ETA: 2:59 - loss: 0.6741 - acc: 0.5785
2752/4849 [================>.............] - ETA: 2:54 - loss: 0.6738 - acc: 0.5781
2816/4849 [================>.............] - ETA: 2:48 - loss: 0.6726 - acc: 0.5799
2880/4849 [================>.............] - ETA: 2:43 - loss: 0.6711 - acc: 0.5819
2944/4849 [=================>............] - ETA: 2:38 - loss: 0.6712 - acc: 0.5815
3008/4849 [=================>............] - ETA: 2:32 - loss: 0.6719 - acc: 0.5811
3072/4849 [==================>...........] - ETA: 2:27 - loss: 0.6727 - acc: 0.5801
3136/4849 [==================>...........] - ETA: 2:21 - loss: 0.6737 - acc: 0.5784
3200/4849 [==================>...........] - ETA: 2:16 - loss: 0.6731 - acc: 0.5800
3264/4849 [===================>..........] - ETA: 2:11 - loss: 0.6732 - acc: 0.5797
3328/4849 [===================>..........] - ETA: 2:05 - loss: 0.6739 - acc: 0.5784
3392/4849 [===================>..........] - ETA: 2:00 - loss: 0.6742 - acc: 0.5772
3456/4849 [====================>.........] - ETA: 1:55 - loss: 0.6737 - acc: 0.5775
3520/4849 [====================>.........] - ETA: 1:49 - loss: 0.6743 - acc: 0.5776
3584/4849 [=====================>........] - ETA: 1:44 - loss: 0.6751 - acc: 0.5767
3648/4849 [=====================>........] - ETA: 1:39 - loss: 0.6742 - acc: 0.5779
3712/4849 [=====================>........] - ETA: 1:33 - loss: 0.6745 - acc: 0.5781
3776/4849 [======================>.......] - ETA: 1:28 - loss: 0.6753 - acc: 0.5763
3840/4849 [======================>.......] - ETA: 1:22 - loss: 0.6744 - acc: 0.5781
3904/4849 [=======================>......] - ETA: 1:17 - loss: 0.6745 - acc: 0.5784
3968/4849 [=======================>......] - ETA: 1:11 - loss: 0.6747 - acc: 0.5784
4032/4849 [=======================>......] - ETA: 1:06 - loss: 0.6738 - acc: 0.5804
4096/4849 [========================>.....] - ETA: 1:00 - loss: 0.6734 - acc: 0.5808
4160/4849 [========================>.....] - ETA: 55s - loss: 0.6738 - acc: 0.5791 
4224/4849 [=========================>....] - ETA: 50s - loss: 0.6745 - acc: 0.5786
4288/4849 [=========================>....] - ETA: 45s - loss: 0.6747 - acc: 0.5788
4352/4849 [=========================>....] - ETA: 39s - loss: 0.6752 - acc: 0.5779
4416/4849 [==========================>...] - ETA: 34s - loss: 0.6746 - acc: 0.5788
4480/4849 [==========================>...] - ETA: 29s - loss: 0.6749 - acc: 0.5783
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6748 - acc: 0.5790
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6752 - acc: 0.5792
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6758 - acc: 0.5790
4736/4849 [============================>.] - ETA: 8s - loss: 0.6757 - acc: 0.5794 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6754 - acc: 0.5813
4849/4849 [==============================] - 393s 81ms/step - loss: 0.6751 - acc: 0.5812 - val_loss: 0.7438 - val_acc: 0.4972

Epoch 00008: val_acc did not improve from 0.61039
Epoch 9/10

  64/4849 [..............................] - ETA: 5:17 - loss: 0.6383 - acc: 0.6094
 128/4849 [..............................] - ETA: 5:28 - loss: 0.6477 - acc: 0.5938
 192/4849 [>.............................] - ETA: 5:11 - loss: 0.6601 - acc: 0.5573
 256/4849 [>.............................] - ETA: 5:03 - loss: 0.6636 - acc: 0.5703
 320/4849 [>.............................] - ETA: 5:00 - loss: 0.6563 - acc: 0.5813
 384/4849 [=>............................] - ETA: 4:54 - loss: 0.6676 - acc: 0.5625
 448/4849 [=>............................] - ETA: 4:50 - loss: 0.6724 - acc: 0.5580
 512/4849 [==>...........................] - ETA: 4:44 - loss: 0.6777 - acc: 0.5586
 576/4849 [==>...........................] - ETA: 4:44 - loss: 0.6739 - acc: 0.5694
 640/4849 [==>...........................] - ETA: 4:39 - loss: 0.6710 - acc: 0.5828
 704/4849 [===>..........................] - ETA: 4:36 - loss: 0.6667 - acc: 0.5966
 768/4849 [===>..........................] - ETA: 4:30 - loss: 0.6655 - acc: 0.5964
 832/4849 [====>.........................] - ETA: 4:27 - loss: 0.6703 - acc: 0.5889
 896/4849 [====>.........................] - ETA: 4:21 - loss: 0.6713 - acc: 0.5837
 960/4849 [====>.........................] - ETA: 4:17 - loss: 0.6705 - acc: 0.5854
1024/4849 [=====>........................] - ETA: 4:11 - loss: 0.6714 - acc: 0.5840
1088/4849 [=====>........................] - ETA: 4:07 - loss: 0.6737 - acc: 0.5772
1152/4849 [======>.......................] - ETA: 4:02 - loss: 0.6737 - acc: 0.5764
1216/4849 [======>.......................] - ETA: 3:58 - loss: 0.6724 - acc: 0.5798
1280/4849 [======>.......................] - ETA: 3:54 - loss: 0.6728 - acc: 0.5758
1344/4849 [=======>......................] - ETA: 3:50 - loss: 0.6724 - acc: 0.5759
1408/4849 [=======>......................] - ETA: 3:45 - loss: 0.6736 - acc: 0.5746
1472/4849 [========>.....................] - ETA: 3:41 - loss: 0.6732 - acc: 0.5761
1536/4849 [========>.....................] - ETA: 3:36 - loss: 0.6722 - acc: 0.5762
1600/4849 [========>.....................] - ETA: 3:31 - loss: 0.6731 - acc: 0.5731
1664/4849 [=========>....................] - ETA: 3:27 - loss: 0.6735 - acc: 0.5703
1728/4849 [=========>....................] - ETA: 3:22 - loss: 0.6737 - acc: 0.5689
1792/4849 [==========>...................] - ETA: 3:18 - loss: 0.6758 - acc: 0.5631
1856/4849 [==========>...................] - ETA: 3:14 - loss: 0.6746 - acc: 0.5657
1920/4849 [==========>...................] - ETA: 3:09 - loss: 0.6766 - acc: 0.5630
1984/4849 [===========>..................] - ETA: 3:05 - loss: 0.6766 - acc: 0.5630
2048/4849 [===========>..................] - ETA: 3:01 - loss: 0.6775 - acc: 0.5630
2112/4849 [============>.................] - ETA: 2:57 - loss: 0.6781 - acc: 0.5606
2176/4849 [============>.................] - ETA: 2:54 - loss: 0.6781 - acc: 0.5611
2240/4849 [============>.................] - ETA: 2:49 - loss: 0.6774 - acc: 0.5621
2304/4849 [=============>................] - ETA: 2:45 - loss: 0.6778 - acc: 0.5612
2368/4849 [=============>................] - ETA: 2:41 - loss: 0.6782 - acc: 0.5625
2432/4849 [==============>...............] - ETA: 2:37 - loss: 0.6771 - acc: 0.5654
2496/4849 [==============>...............] - ETA: 2:32 - loss: 0.6771 - acc: 0.5645
2560/4849 [==============>...............] - ETA: 2:28 - loss: 0.6765 - acc: 0.5664
2624/4849 [===============>..............] - ETA: 2:24 - loss: 0.6760 - acc: 0.5663
2688/4849 [===============>..............] - ETA: 2:20 - loss: 0.6759 - acc: 0.5670
2752/4849 [================>.............] - ETA: 2:17 - loss: 0.6755 - acc: 0.5687
2816/4849 [================>.............] - ETA: 2:12 - loss: 0.6768 - acc: 0.5653
2880/4849 [================>.............] - ETA: 2:08 - loss: 0.6771 - acc: 0.5642
2944/4849 [=================>............] - ETA: 2:04 - loss: 0.6773 - acc: 0.5632
3008/4849 [=================>............] - ETA: 2:00 - loss: 0.6772 - acc: 0.5652
3072/4849 [==================>...........] - ETA: 1:56 - loss: 0.6767 - acc: 0.5664
3136/4849 [==================>...........] - ETA: 1:52 - loss: 0.6759 - acc: 0.5686
3200/4849 [==================>...........] - ETA: 1:48 - loss: 0.6770 - acc: 0.5669
3264/4849 [===================>..........] - ETA: 1:43 - loss: 0.6769 - acc: 0.5674
3328/4849 [===================>..........] - ETA: 1:39 - loss: 0.6772 - acc: 0.5673
3392/4849 [===================>..........] - ETA: 1:35 - loss: 0.6770 - acc: 0.5678
3456/4849 [====================>.........] - ETA: 1:31 - loss: 0.6780 - acc: 0.5651
3520/4849 [====================>.........] - ETA: 1:27 - loss: 0.6781 - acc: 0.5659
3584/4849 [=====================>........] - ETA: 1:23 - loss: 0.6785 - acc: 0.5650
3648/4849 [=====================>........] - ETA: 1:18 - loss: 0.6776 - acc: 0.5666
3712/4849 [=====================>........] - ETA: 1:14 - loss: 0.6776 - acc: 0.5665
3776/4849 [======================>.......] - ETA: 1:10 - loss: 0.6772 - acc: 0.5673
3840/4849 [======================>.......] - ETA: 1:06 - loss: 0.6770 - acc: 0.5685
3904/4849 [=======================>......] - ETA: 1:01 - loss: 0.6764 - acc: 0.5697
3968/4849 [=======================>......] - ETA: 57s - loss: 0.6761 - acc: 0.5698 
4032/4849 [=======================>......] - ETA: 53s - loss: 0.6761 - acc: 0.5699
4096/4849 [========================>.....] - ETA: 49s - loss: 0.6767 - acc: 0.5696
4160/4849 [========================>.....] - ETA: 45s - loss: 0.6769 - acc: 0.5690
4224/4849 [=========================>....] - ETA: 41s - loss: 0.6769 - acc: 0.5698
4288/4849 [=========================>....] - ETA: 36s - loss: 0.6761 - acc: 0.5711
4352/4849 [=========================>....] - ETA: 32s - loss: 0.6765 - acc: 0.5699
4416/4849 [==========================>...] - ETA: 28s - loss: 0.6767 - acc: 0.5693
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6767 - acc: 0.5690
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6765 - acc: 0.5700
4608/4849 [===========================>..] - ETA: 15s - loss: 0.6760 - acc: 0.5703
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6758 - acc: 0.5713
4736/4849 [============================>.] - ETA: 7s - loss: 0.6755 - acc: 0.5718 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6760 - acc: 0.5723
4849/4849 [==============================] - 333s 69ms/step - loss: 0.6763 - acc: 0.5725 - val_loss: 0.6936 - val_acc: 0.5529

Epoch 00009: val_acc did not improve from 0.61039
Epoch 10/10

  64/4849 [..............................] - ETA: 4:44 - loss: 0.6493 - acc: 0.6250
 128/4849 [..............................] - ETA: 5:07 - loss: 0.6482 - acc: 0.6406
 192/4849 [>.............................] - ETA: 5:04 - loss: 0.6645 - acc: 0.6094
 256/4849 [>.............................] - ETA: 4:57 - loss: 0.6699 - acc: 0.5977
 320/4849 [>.............................] - ETA: 5:04 - loss: 0.6663 - acc: 0.5906
 384/4849 [=>............................] - ETA: 4:58 - loss: 0.6656 - acc: 0.5859
 448/4849 [=>............................] - ETA: 4:56 - loss: 0.6643 - acc: 0.5826
 512/4849 [==>...........................] - ETA: 4:46 - loss: 0.6667 - acc: 0.5938
 576/4849 [==>...........................] - ETA: 4:44 - loss: 0.6716 - acc: 0.5833
 640/4849 [==>...........................] - ETA: 4:35 - loss: 0.6694 - acc: 0.5891
 704/4849 [===>..........................] - ETA: 4:32 - loss: 0.6694 - acc: 0.5923
 768/4849 [===>..........................] - ETA: 4:28 - loss: 0.6691 - acc: 0.5911
 832/4849 [====>.........................] - ETA: 4:24 - loss: 0.6713 - acc: 0.5853
 896/4849 [====>.........................] - ETA: 4:18 - loss: 0.6709 - acc: 0.5859
 960/4849 [====>.........................] - ETA: 4:15 - loss: 0.6704 - acc: 0.5854
1024/4849 [=====>........................] - ETA: 4:11 - loss: 0.6706 - acc: 0.5840
1088/4849 [=====>........................] - ETA: 4:08 - loss: 0.6709 - acc: 0.5846
1152/4849 [======>.......................] - ETA: 4:03 - loss: 0.6699 - acc: 0.5842
1216/4849 [======>.......................] - ETA: 3:59 - loss: 0.6695 - acc: 0.5839
1280/4849 [======>.......................] - ETA: 3:52 - loss: 0.6704 - acc: 0.5844
1344/4849 [=======>......................] - ETA: 3:49 - loss: 0.6696 - acc: 0.5856
1408/4849 [=======>......................] - ETA: 3:43 - loss: 0.6681 - acc: 0.5902
1472/4849 [========>.....................] - ETA: 3:39 - loss: 0.6668 - acc: 0.5938
1536/4849 [========>.....................] - ETA: 3:35 - loss: 0.6712 - acc: 0.5872
1600/4849 [========>.....................] - ETA: 3:31 - loss: 0.6710 - acc: 0.5869
1664/4849 [=========>....................] - ETA: 3:27 - loss: 0.6732 - acc: 0.5817
1728/4849 [=========>....................] - ETA: 3:22 - loss: 0.6733 - acc: 0.5804
1792/4849 [==========>...................] - ETA: 3:18 - loss: 0.6733 - acc: 0.5809
1856/4849 [==========>...................] - ETA: 3:14 - loss: 0.6730 - acc: 0.5792
1920/4849 [==========>...................] - ETA: 3:09 - loss: 0.6729 - acc: 0.5792
1984/4849 [===========>..................] - ETA: 3:04 - loss: 0.6730 - acc: 0.5786
2048/4849 [===========>..................] - ETA: 3:00 - loss: 0.6724 - acc: 0.5791
2112/4849 [============>.................] - ETA: 2:56 - loss: 0.6722 - acc: 0.5800
2176/4849 [============>.................] - ETA: 2:52 - loss: 0.6726 - acc: 0.5795
2240/4849 [============>.................] - ETA: 2:48 - loss: 0.6742 - acc: 0.5772
2304/4849 [=============>................] - ETA: 2:44 - loss: 0.6733 - acc: 0.5790
2368/4849 [=============>................] - ETA: 2:40 - loss: 0.6725 - acc: 0.5794
2432/4849 [==============>...............] - ETA: 2:36 - loss: 0.6727 - acc: 0.5785
2496/4849 [==============>...............] - ETA: 2:32 - loss: 0.6730 - acc: 0.5773
2560/4849 [==============>...............] - ETA: 2:28 - loss: 0.6721 - acc: 0.5789
2624/4849 [===============>..............] - ETA: 2:23 - loss: 0.6738 - acc: 0.5774
2688/4849 [===============>..............] - ETA: 2:19 - loss: 0.6737 - acc: 0.5763
2752/4849 [================>.............] - ETA: 2:15 - loss: 0.6733 - acc: 0.5770
2816/4849 [================>.............] - ETA: 2:11 - loss: 0.6729 - acc: 0.5767
2880/4849 [================>.............] - ETA: 2:07 - loss: 0.6715 - acc: 0.5799
2944/4849 [=================>............] - ETA: 2:03 - loss: 0.6715 - acc: 0.5791
3008/4849 [=================>............] - ETA: 1:59 - loss: 0.6721 - acc: 0.5775
3072/4849 [==================>...........] - ETA: 1:54 - loss: 0.6717 - acc: 0.5788
3136/4849 [==================>...........] - ETA: 1:50 - loss: 0.6709 - acc: 0.5807
3200/4849 [==================>...........] - ETA: 1:46 - loss: 0.6708 - acc: 0.5809
3264/4849 [===================>..........] - ETA: 1:42 - loss: 0.6705 - acc: 0.5821
3328/4849 [===================>..........] - ETA: 1:38 - loss: 0.6708 - acc: 0.5817
3392/4849 [===================>..........] - ETA: 1:33 - loss: 0.6699 - acc: 0.5834
3456/4849 [====================>.........] - ETA: 1:29 - loss: 0.6699 - acc: 0.5845
3520/4849 [====================>.........] - ETA: 1:25 - loss: 0.6697 - acc: 0.5858
3584/4849 [=====================>........] - ETA: 1:21 - loss: 0.6706 - acc: 0.5848
3648/4849 [=====================>........] - ETA: 1:17 - loss: 0.6701 - acc: 0.5861
3712/4849 [=====================>........] - ETA: 1:13 - loss: 0.6704 - acc: 0.5867
3776/4849 [======================>.......] - ETA: 1:09 - loss: 0.6697 - acc: 0.5877
3840/4849 [======================>.......] - ETA: 1:05 - loss: 0.6705 - acc: 0.5852
3904/4849 [=======================>......] - ETA: 1:00 - loss: 0.6708 - acc: 0.5843
3968/4849 [=======================>......] - ETA: 56s - loss: 0.6707 - acc: 0.5849 
4032/4849 [=======================>......] - ETA: 52s - loss: 0.6701 - acc: 0.5861
4096/4849 [========================>.....] - ETA: 48s - loss: 0.6711 - acc: 0.5852
4160/4849 [========================>.....] - ETA: 44s - loss: 0.6710 - acc: 0.5858
4224/4849 [=========================>....] - ETA: 40s - loss: 0.6706 - acc: 0.5855
4288/4849 [=========================>....] - ETA: 36s - loss: 0.6710 - acc: 0.5849
4352/4849 [=========================>....] - ETA: 31s - loss: 0.6711 - acc: 0.5859
4416/4849 [==========================>...] - ETA: 27s - loss: 0.6712 - acc: 0.5858
4480/4849 [==========================>...] - ETA: 23s - loss: 0.6712 - acc: 0.5859
4544/4849 [===========================>..] - ETA: 19s - loss: 0.6707 - acc: 0.5869
4608/4849 [===========================>..] - ETA: 15s - loss: 0.6702 - acc: 0.5881
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6702 - acc: 0.5882
4736/4849 [============================>.] - ETA: 7s - loss: 0.6701 - acc: 0.5883 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6703 - acc: 0.5879
4849/4849 [==============================] - 326s 67ms/step - loss: 0.6705 - acc: 0.5873 - val_loss: 0.7119 - val_acc: 0.5158

Epoch 00010: val_acc did not improve from 0.61039
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f3242349f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f3242349f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f32422e5d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f32422e5d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3285834110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3285834110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3217f039d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3217f039d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3217f49cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3217f49cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217d88150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217d88150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3217f034d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3217f034d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217ecae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217ecae50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3217e3bed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3217e3bed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3217bf0f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3217bf0f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217e76d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217e76d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3217d87990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f3217d87990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217c42690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217c42690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3217d2bb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f3217d2bb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3217c35850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f3217c35850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217a1be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217a1be10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29d022e450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29d022e450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217a41250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217a41250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f320f788b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f320f788b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f320f780210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f320f780210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f74a850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f74a850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f320f7886d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f320f7886d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f4eb390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f4eb390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f320f6d9710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f320f6d9710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f320f428e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f320f428e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f2b3390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f2b3390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f320f423990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f320f423990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f636d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f636d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31cb0e2e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f31cb0e2e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31cb0ff550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f31cb0ff550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f3f88d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f3f88d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31cb0e6410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f31cb0e6410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc807ffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bc807ffd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2bac5fe450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2bac5fe450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2bac55fcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2bac55fcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bac63abd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bac63abd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2bac5fe910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2bac5fe910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bac5e1110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bac5e1110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2bac63a890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2bac63a890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2bac371c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2bac371c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bac203c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bac203c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2bac692650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2bac692650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bac200cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2bac200cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2bac225a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2bac225a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b886a3d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b886a3d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b886d8950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b886d8950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2e6c1f0350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2e6c1f0350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b88775a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b88775a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b884e69d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b884e69d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b8833de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b8833de90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f65d250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f320f65d250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b884e6210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b884e6210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b882a15d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b882a15d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b883b30d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b883b30d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b881b26d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b881b26d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b8810e690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b8810e690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b88298110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b88298110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b686bd050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b686bd050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b68629790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b68629790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b8805aa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b8805aa10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b68619d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b68619d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b881db310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b881db310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b8809a150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b8809a150>>: AttributeError: module 'gast' has no attribute 'Str'
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 1:35
 128/1348 [=>............................] - ETA: 58s 
 192/1348 [===>..........................] - ETA: 45s
 256/1348 [====>.........................] - ETA: 38s
 320/1348 [======>.......................] - ETA: 33s
 384/1348 [=======>......................] - ETA: 30s
 448/1348 [========>.....................] - ETA: 26s
 512/1348 [==========>...................] - ETA: 24s
 576/1348 [===========>..................] - ETA: 21s
 640/1348 [=============>................] - ETA: 19s
 704/1348 [==============>...............] - ETA: 18s
 768/1348 [================>.............] - ETA: 16s
 832/1348 [=================>............] - ETA: 14s
 896/1348 [==================>...........] - ETA: 12s
 960/1348 [====================>.........] - ETA: 10s
1024/1348 [=====================>........] - ETA: 8s 
1088/1348 [=======================>......] - ETA: 7s
1152/1348 [========================>.....] - ETA: 5s
1216/1348 [==========================>...] - ETA: 3s
1280/1348 [===========================>..] - ETA: 1s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 38s 28ms/step
loss: 0.6790663246231192
acc: 0.577893175074184
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f29d01d8750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f29d01d8750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f29d0194f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f29d0194f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f296c1d8290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f296c1d8290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b284cd050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b284cd050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f324209a910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f324209a910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29d01a6dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29d01a6dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b284cd2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b284cd2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b28611b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b28611b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2a2c4e7890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2a2c4e7890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f29d008cc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f29d008cc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32421845d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f32421845d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f32420f3210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f32420f3210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c874b510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c874b510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29d0089890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29d0089890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f29c85c6d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f29c85c6d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c8463410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c8463410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29d0089510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29d0089510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c859fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c859fc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29c85661d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29c85661d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f29d00d2350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f29d00d2350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c81708d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c81708d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29c833a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29c833a1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c82ea110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c82ea110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29c8071310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29c8071310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f299c727e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f299c727e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c80a7dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29c80a7dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29c8071a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29c8071a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f299c57d590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f299c57d590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f299c50d1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f299c50d1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f299c4e3350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f299c4e3350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b685a35d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b685a35d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f299c50d690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f299c50d690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f299c2f0c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f299c2f0c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f299c187e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f299c187e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f299c1cf3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f299c1cf3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f299c0a0050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f299c0a0050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f299c187390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f299c187390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f299c0ec490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f299c0ec490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f296c65c290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f296c65c290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f296c645050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f296c645050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f296c67ee50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f296c67ee50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f296c65c690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f296c65c690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f296c422450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f296c422450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f296c398d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f296c398d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f296c27d090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f296c27d090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f296c397210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f296c397210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f299c21db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f299c21db90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f296c26ad10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f296c26ad10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2888160090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2888160090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2888189310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2888189310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2888278bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2888278bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2888160e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2888160e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28686b9750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28686b9750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f286861ed10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f286861ed10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f286861e410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f286861e410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f288803a610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f288803a610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f286861e350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f286861e350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f286852a810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f286852a810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f286852a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f286852a550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28682fc910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28682fc910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2868260b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2868260b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28684dd410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28684dd410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28682d0d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28682d0d50>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 20:42 - loss: 0.7624 - acc: 0.4375
 128/4849 [..............................] - ETA: 12:42 - loss: 0.7653 - acc: 0.4688
 192/4849 [>.............................] - ETA: 10:05 - loss: 0.8025 - acc: 0.4635
 256/4849 [>.............................] - ETA: 9:06 - loss: 0.7858 - acc: 0.4844 
 320/4849 [>.............................] - ETA: 8:16 - loss: 0.7767 - acc: 0.4906
 384/4849 [=>............................] - ETA: 7:45 - loss: 0.7778 - acc: 0.4870
 448/4849 [=>............................] - ETA: 7:19 - loss: 0.7671 - acc: 0.4978
 512/4849 [==>...........................] - ETA: 6:56 - loss: 0.7561 - acc: 0.5098
 576/4849 [==>...........................] - ETA: 6:35 - loss: 0.7463 - acc: 0.5208
 640/4849 [==>...........................] - ETA: 6:23 - loss: 0.7442 - acc: 0.5188
 704/4849 [===>..........................] - ETA: 6:08 - loss: 0.7396 - acc: 0.5213
 768/4849 [===>..........................] - ETA: 5:57 - loss: 0.7380 - acc: 0.5234
 832/4849 [====>.........................] - ETA: 5:46 - loss: 0.7317 - acc: 0.5312
 896/4849 [====>.........................] - ETA: 5:37 - loss: 0.7349 - acc: 0.5223
 960/4849 [====>.........................] - ETA: 5:28 - loss: 0.7332 - acc: 0.5219
1024/4849 [=====>........................] - ETA: 5:20 - loss: 0.7321 - acc: 0.5215
1088/4849 [=====>........................] - ETA: 5:11 - loss: 0.7331 - acc: 0.5184
1152/4849 [======>.......................] - ETA: 5:04 - loss: 0.7332 - acc: 0.5165
1216/4849 [======>.......................] - ETA: 4:57 - loss: 0.7328 - acc: 0.5173
1280/4849 [======>.......................] - ETA: 4:52 - loss: 0.7308 - acc: 0.5195
1344/4849 [=======>......................] - ETA: 4:46 - loss: 0.7285 - acc: 0.5231
1408/4849 [=======>......................] - ETA: 4:38 - loss: 0.7275 - acc: 0.5213
1472/4849 [========>.....................] - ETA: 4:32 - loss: 0.7260 - acc: 0.5238
1536/4849 [========>.....................] - ETA: 4:25 - loss: 0.7272 - acc: 0.5228
1600/4849 [========>.....................] - ETA: 4:18 - loss: 0.7275 - acc: 0.5225
1664/4849 [=========>....................] - ETA: 4:11 - loss: 0.7244 - acc: 0.5288
1728/4849 [=========>....................] - ETA: 4:06 - loss: 0.7240 - acc: 0.5284
1792/4849 [==========>...................] - ETA: 4:00 - loss: 0.7245 - acc: 0.5279
1856/4849 [==========>...................] - ETA: 3:54 - loss: 0.7243 - acc: 0.5269
1920/4849 [==========>...................] - ETA: 3:49 - loss: 0.7248 - acc: 0.5260
1984/4849 [===========>..................] - ETA: 3:44 - loss: 0.7248 - acc: 0.5267
2048/4849 [===========>..................] - ETA: 3:38 - loss: 0.7242 - acc: 0.5259
2112/4849 [============>.................] - ETA: 3:32 - loss: 0.7233 - acc: 0.5270
2176/4849 [============>.................] - ETA: 3:28 - loss: 0.7234 - acc: 0.5262
2240/4849 [============>.................] - ETA: 3:23 - loss: 0.7230 - acc: 0.5254
2304/4849 [=============>................] - ETA: 3:17 - loss: 0.7222 - acc: 0.5269
2368/4849 [=============>................] - ETA: 3:12 - loss: 0.7233 - acc: 0.5236
2432/4849 [==============>...............] - ETA: 3:07 - loss: 0.7215 - acc: 0.5271
2496/4849 [==============>...............] - ETA: 3:02 - loss: 0.7222 - acc: 0.5268
2560/4849 [==============>...............] - ETA: 2:56 - loss: 0.7218 - acc: 0.5285
2624/4849 [===============>..............] - ETA: 2:51 - loss: 0.7210 - acc: 0.5305
2688/4849 [===============>..............] - ETA: 2:46 - loss: 0.7203 - acc: 0.5305
2752/4849 [================>.............] - ETA: 2:40 - loss: 0.7220 - acc: 0.5276
2816/4849 [================>.............] - ETA: 2:36 - loss: 0.7236 - acc: 0.5252
2880/4849 [================>.............] - ETA: 2:30 - loss: 0.7241 - acc: 0.5260
2944/4849 [=================>............] - ETA: 2:25 - loss: 0.7240 - acc: 0.5258
3008/4849 [=================>............] - ETA: 2:20 - loss: 0.7241 - acc: 0.5249
3072/4849 [==================>...........] - ETA: 2:15 - loss: 0.7228 - acc: 0.5273
3136/4849 [==================>...........] - ETA: 2:10 - loss: 0.7228 - acc: 0.5261
3200/4849 [==================>...........] - ETA: 2:05 - loss: 0.7226 - acc: 0.5259
3264/4849 [===================>..........] - ETA: 2:00 - loss: 0.7220 - acc: 0.5273
3328/4849 [===================>..........] - ETA: 1:55 - loss: 0.7218 - acc: 0.5267
3392/4849 [===================>..........] - ETA: 1:50 - loss: 0.7217 - acc: 0.5265
3456/4849 [====================>.........] - ETA: 1:45 - loss: 0.7213 - acc: 0.5275
3520/4849 [====================>.........] - ETA: 1:40 - loss: 0.7220 - acc: 0.5256
3584/4849 [=====================>........] - ETA: 1:35 - loss: 0.7216 - acc: 0.5251
3648/4849 [=====================>........] - ETA: 1:30 - loss: 0.7209 - acc: 0.5258
3712/4849 [=====================>........] - ETA: 1:25 - loss: 0.7211 - acc: 0.5240
3776/4849 [======================>.......] - ETA: 1:20 - loss: 0.7211 - acc: 0.5222
3840/4849 [======================>.......] - ETA: 1:16 - loss: 0.7203 - acc: 0.5234
3904/4849 [=======================>......] - ETA: 1:11 - loss: 0.7202 - acc: 0.5231
3968/4849 [=======================>......] - ETA: 1:06 - loss: 0.7202 - acc: 0.5229
4032/4849 [=======================>......] - ETA: 1:01 - loss: 0.7199 - acc: 0.5218
4096/4849 [========================>.....] - ETA: 56s - loss: 0.7199 - acc: 0.5212 
4160/4849 [========================>.....] - ETA: 51s - loss: 0.7190 - acc: 0.5224
4224/4849 [=========================>....] - ETA: 47s - loss: 0.7190 - acc: 0.5220
4288/4849 [=========================>....] - ETA: 42s - loss: 0.7180 - acc: 0.5229
4352/4849 [=========================>....] - ETA: 37s - loss: 0.7179 - acc: 0.5232
4416/4849 [==========================>...] - ETA: 32s - loss: 0.7177 - acc: 0.5238
4480/4849 [==========================>...] - ETA: 27s - loss: 0.7176 - acc: 0.5239
4544/4849 [===========================>..] - ETA: 22s - loss: 0.7174 - acc: 0.5238
4608/4849 [===========================>..] - ETA: 18s - loss: 0.7172 - acc: 0.5241
4672/4849 [===========================>..] - ETA: 13s - loss: 0.7171 - acc: 0.5246
4736/4849 [============================>.] - ETA: 8s - loss: 0.7163 - acc: 0.5255 
4800/4849 [============================>.] - ETA: 3s - loss: 0.7155 - acc: 0.5271
4849/4849 [==============================] - 381s 78ms/step - loss: 0.7156 - acc: 0.5271 - val_loss: 0.6840 - val_acc: 0.5584

Epoch 00001: val_acc improved from -inf to 0.55844, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window03/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 6:05 - loss: 0.6703 - acc: 0.6094
 128/4849 [..............................] - ETA: 5:54 - loss: 0.6800 - acc: 0.5781
 192/4849 [>.............................] - ETA: 5:55 - loss: 0.6954 - acc: 0.5573
 256/4849 [>.............................] - ETA: 5:51 - loss: 0.7028 - acc: 0.5469
 320/4849 [>.............................] - ETA: 5:48 - loss: 0.7120 - acc: 0.5281
 384/4849 [=>............................] - ETA: 5:39 - loss: 0.7134 - acc: 0.5104
 448/4849 [=>............................] - ETA: 5:37 - loss: 0.7050 - acc: 0.5223
 512/4849 [==>...........................] - ETA: 5:30 - loss: 0.7070 - acc: 0.5195
 576/4849 [==>...........................] - ETA: 5:25 - loss: 0.7069 - acc: 0.5122
 640/4849 [==>...........................] - ETA: 5:20 - loss: 0.7068 - acc: 0.5156
 704/4849 [===>..........................] - ETA: 5:15 - loss: 0.7043 - acc: 0.5199
 768/4849 [===>..........................] - ETA: 5:10 - loss: 0.7017 - acc: 0.5169
 832/4849 [====>.........................] - ETA: 5:03 - loss: 0.7017 - acc: 0.5192
 896/4849 [====>.........................] - ETA: 4:57 - loss: 0.7030 - acc: 0.5167
 960/4849 [====>.........................] - ETA: 4:52 - loss: 0.7023 - acc: 0.5167
1024/4849 [=====>........................] - ETA: 4:48 - loss: 0.7017 - acc: 0.5186
1088/4849 [=====>........................] - ETA: 4:42 - loss: 0.7021 - acc: 0.5175
1152/4849 [======>.......................] - ETA: 4:38 - loss: 0.6994 - acc: 0.5217
1216/4849 [======>.......................] - ETA: 4:31 - loss: 0.6981 - acc: 0.5263
1280/4849 [======>.......................] - ETA: 4:26 - loss: 0.6991 - acc: 0.5258
1344/4849 [=======>......................] - ETA: 4:20 - loss: 0.6988 - acc: 0.5246
1408/4849 [=======>......................] - ETA: 4:17 - loss: 0.6994 - acc: 0.5227
1472/4849 [========>.....................] - ETA: 4:11 - loss: 0.7005 - acc: 0.5197
1536/4849 [========>.....................] - ETA: 4:07 - loss: 0.7003 - acc: 0.5202
1600/4849 [========>.....................] - ETA: 4:01 - loss: 0.6989 - acc: 0.5219
1664/4849 [=========>....................] - ETA: 3:57 - loss: 0.6990 - acc: 0.5222
1728/4849 [=========>....................] - ETA: 3:51 - loss: 0.7002 - acc: 0.5174
1792/4849 [==========>...................] - ETA: 3:46 - loss: 0.7005 - acc: 0.5128
1856/4849 [==========>...................] - ETA: 3:41 - loss: 0.7011 - acc: 0.5124
1920/4849 [==========>...................] - ETA: 3:36 - loss: 0.7013 - acc: 0.5130
1984/4849 [===========>..................] - ETA: 3:32 - loss: 0.7009 - acc: 0.5146
2048/4849 [===========>..................] - ETA: 3:26 - loss: 0.7005 - acc: 0.5156
2112/4849 [============>.................] - ETA: 3:22 - loss: 0.6997 - acc: 0.5185
2176/4849 [============>.................] - ETA: 3:17 - loss: 0.6987 - acc: 0.5198
2240/4849 [============>.................] - ETA: 3:12 - loss: 0.6985 - acc: 0.5219
2304/4849 [=============>................] - ETA: 3:07 - loss: 0.6992 - acc: 0.5191
2368/4849 [=============>................] - ETA: 3:03 - loss: 0.6993 - acc: 0.5186
2432/4849 [==============>...............] - ETA: 2:58 - loss: 0.6994 - acc: 0.5185
2496/4849 [==============>...............] - ETA: 2:53 - loss: 0.6990 - acc: 0.5192
2560/4849 [==============>...............] - ETA: 2:48 - loss: 0.6993 - acc: 0.5199
2624/4849 [===============>..............] - ETA: 2:44 - loss: 0.6990 - acc: 0.5202
2688/4849 [===============>..............] - ETA: 2:39 - loss: 0.6988 - acc: 0.5201
2752/4849 [================>.............] - ETA: 2:34 - loss: 0.6990 - acc: 0.5189
2816/4849 [================>.............] - ETA: 2:29 - loss: 0.6992 - acc: 0.5174
2880/4849 [================>.............] - ETA: 2:25 - loss: 0.6989 - acc: 0.5184
2944/4849 [=================>............] - ETA: 2:20 - loss: 0.6987 - acc: 0.5197
3008/4849 [=================>............] - ETA: 2:15 - loss: 0.6989 - acc: 0.5193
3072/4849 [==================>...........] - ETA: 2:11 - loss: 0.6995 - acc: 0.5189
3136/4849 [==================>...........] - ETA: 2:06 - loss: 0.6995 - acc: 0.5198
3200/4849 [==================>...........] - ETA: 2:01 - loss: 0.6991 - acc: 0.5212
3264/4849 [===================>..........] - ETA: 1:57 - loss: 0.6997 - acc: 0.5214
3328/4849 [===================>..........] - ETA: 1:52 - loss: 0.6997 - acc: 0.5204
3392/4849 [===================>..........] - ETA: 1:48 - loss: 0.6990 - acc: 0.5215
3456/4849 [====================>.........] - ETA: 1:43 - loss: 0.6983 - acc: 0.5226
3520/4849 [====================>.........] - ETA: 1:38 - loss: 0.6986 - acc: 0.5224
3584/4849 [=====================>........] - ETA: 1:33 - loss: 0.6990 - acc: 0.5223
3648/4849 [=====================>........] - ETA: 1:29 - loss: 0.6993 - acc: 0.5219
3712/4849 [=====================>........] - ETA: 1:24 - loss: 0.6996 - acc: 0.5210
3776/4849 [======================>.......] - ETA: 1:19 - loss: 0.7003 - acc: 0.5193
3840/4849 [======================>.......] - ETA: 1:14 - loss: 0.7001 - acc: 0.5195
3904/4849 [=======================>......] - ETA: 1:10 - loss: 0.6998 - acc: 0.5207
3968/4849 [=======================>......] - ETA: 1:05 - loss: 0.6994 - acc: 0.5214
4032/4849 [=======================>......] - ETA: 1:00 - loss: 0.6996 - acc: 0.5206
4096/4849 [========================>.....] - ETA: 55s - loss: 0.6992 - acc: 0.5210 
4160/4849 [========================>.....] - ETA: 51s - loss: 0.6993 - acc: 0.5202
4224/4849 [=========================>....] - ETA: 46s - loss: 0.6989 - acc: 0.5215
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6992 - acc: 0.5210
4352/4849 [=========================>....] - ETA: 36s - loss: 0.6988 - acc: 0.5223
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6987 - acc: 0.5208
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6985 - acc: 0.5203
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6985 - acc: 0.5202
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6985 - acc: 0.5200
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6991 - acc: 0.5193
4736/4849 [============================>.] - ETA: 8s - loss: 0.6989 - acc: 0.5194 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6989 - acc: 0.5190
4849/4849 [==============================] - 375s 77ms/step - loss: 0.6992 - acc: 0.5178 - val_loss: 0.6880 - val_acc: 0.5288

Epoch 00002: val_acc did not improve from 0.55844
Epoch 3/10

  64/4849 [..............................] - ETA: 5:44 - loss: 0.6709 - acc: 0.5625
 128/4849 [..............................] - ETA: 5:58 - loss: 0.6823 - acc: 0.5703
 192/4849 [>.............................] - ETA: 5:55 - loss: 0.6942 - acc: 0.5573
 256/4849 [>.............................] - ETA: 5:49 - loss: 0.6969 - acc: 0.5430
 320/4849 [>.............................] - ETA: 5:47 - loss: 0.6946 - acc: 0.5375
 384/4849 [=>............................] - ETA: 5:47 - loss: 0.6884 - acc: 0.5495
 448/4849 [=>............................] - ETA: 5:39 - loss: 0.6896 - acc: 0.5513
 512/4849 [==>...........................] - ETA: 5:30 - loss: 0.6970 - acc: 0.5352
 576/4849 [==>...........................] - ETA: 5:25 - loss: 0.6959 - acc: 0.5330
 640/4849 [==>...........................] - ETA: 5:19 - loss: 0.6965 - acc: 0.5297
 704/4849 [===>..........................] - ETA: 5:12 - loss: 0.6940 - acc: 0.5384
 768/4849 [===>..........................] - ETA: 5:09 - loss: 0.6943 - acc: 0.5326
 832/4849 [====>.........................] - ETA: 5:02 - loss: 0.6939 - acc: 0.5337
 896/4849 [====>.........................] - ETA: 4:58 - loss: 0.6952 - acc: 0.5290
 960/4849 [====>.........................] - ETA: 4:53 - loss: 0.6952 - acc: 0.5302
1024/4849 [=====>........................] - ETA: 4:47 - loss: 0.6939 - acc: 0.5332
1088/4849 [=====>........................] - ETA: 4:40 - loss: 0.6942 - acc: 0.5294
1152/4849 [======>.......................] - ETA: 4:36 - loss: 0.6928 - acc: 0.5356
1216/4849 [======>.......................] - ETA: 4:30 - loss: 0.6939 - acc: 0.5329
1280/4849 [======>.......................] - ETA: 4:25 - loss: 0.6929 - acc: 0.5344
1344/4849 [=======>......................] - ETA: 4:20 - loss: 0.6920 - acc: 0.5357
1408/4849 [=======>......................] - ETA: 4:15 - loss: 0.6928 - acc: 0.5341
1472/4849 [========>.....................] - ETA: 4:10 - loss: 0.6908 - acc: 0.5374
1536/4849 [========>.....................] - ETA: 4:05 - loss: 0.6911 - acc: 0.5345
1600/4849 [========>.....................] - ETA: 4:01 - loss: 0.6908 - acc: 0.5350
1664/4849 [=========>....................] - ETA: 3:56 - loss: 0.6902 - acc: 0.5367
1728/4849 [=========>....................] - ETA: 3:51 - loss: 0.6902 - acc: 0.5376
1792/4849 [==========>...................] - ETA: 3:47 - loss: 0.6887 - acc: 0.5424
1856/4849 [==========>...................] - ETA: 3:42 - loss: 0.6885 - acc: 0.5442
1920/4849 [==========>...................] - ETA: 3:36 - loss: 0.6912 - acc: 0.5380
1984/4849 [===========>..................] - ETA: 3:33 - loss: 0.6909 - acc: 0.5373
2048/4849 [===========>..................] - ETA: 3:27 - loss: 0.6904 - acc: 0.5405
2112/4849 [============>.................] - ETA: 3:22 - loss: 0.6886 - acc: 0.5450
2176/4849 [============>.................] - ETA: 3:18 - loss: 0.6880 - acc: 0.5455
2240/4849 [============>.................] - ETA: 3:13 - loss: 0.6880 - acc: 0.5451
2304/4849 [=============>................] - ETA: 3:08 - loss: 0.6881 - acc: 0.5438
2368/4849 [=============>................] - ETA: 3:04 - loss: 0.6883 - acc: 0.5435
2432/4849 [==============>...............] - ETA: 2:59 - loss: 0.6880 - acc: 0.5436
2496/4849 [==============>...............] - ETA: 2:54 - loss: 0.6883 - acc: 0.5425
2560/4849 [==============>...............] - ETA: 2:49 - loss: 0.6883 - acc: 0.5410
2624/4849 [===============>..............] - ETA: 2:44 - loss: 0.6884 - acc: 0.5400
2688/4849 [===============>..............] - ETA: 2:39 - loss: 0.6884 - acc: 0.5402
2752/4849 [================>.............] - ETA: 2:34 - loss: 0.6879 - acc: 0.5425
2816/4849 [================>.............] - ETA: 2:30 - loss: 0.6884 - acc: 0.5405
2880/4849 [================>.............] - ETA: 2:25 - loss: 0.6887 - acc: 0.5392
2944/4849 [=================>............] - ETA: 2:20 - loss: 0.6887 - acc: 0.5408
3008/4849 [=================>............] - ETA: 2:15 - loss: 0.6883 - acc: 0.5422
3072/4849 [==================>...........] - ETA: 2:11 - loss: 0.6888 - acc: 0.5410
3136/4849 [==================>...........] - ETA: 2:06 - loss: 0.6897 - acc: 0.5399
3200/4849 [==================>...........] - ETA: 2:01 - loss: 0.6898 - acc: 0.5391
3264/4849 [===================>..........] - ETA: 1:57 - loss: 0.6897 - acc: 0.5386
3328/4849 [===================>..........] - ETA: 1:52 - loss: 0.6896 - acc: 0.5388
3392/4849 [===================>..........] - ETA: 1:47 - loss: 0.6907 - acc: 0.5354
3456/4849 [====================>.........] - ETA: 1:42 - loss: 0.6906 - acc: 0.5359
3520/4849 [====================>.........] - ETA: 1:37 - loss: 0.6905 - acc: 0.5358
3584/4849 [=====================>........] - ETA: 1:32 - loss: 0.6908 - acc: 0.5349
3648/4849 [=====================>........] - ETA: 1:28 - loss: 0.6905 - acc: 0.5359
3712/4849 [=====================>........] - ETA: 1:23 - loss: 0.6903 - acc: 0.5377
3776/4849 [======================>.......] - ETA: 1:18 - loss: 0.6899 - acc: 0.5395
3840/4849 [======================>.......] - ETA: 1:13 - loss: 0.6898 - acc: 0.5396
3904/4849 [=======================>......] - ETA: 1:09 - loss: 0.6892 - acc: 0.5400
3968/4849 [=======================>......] - ETA: 1:04 - loss: 0.6899 - acc: 0.5383
4032/4849 [=======================>......] - ETA: 59s - loss: 0.6898 - acc: 0.5382 
4096/4849 [========================>.....] - ETA: 55s - loss: 0.6898 - acc: 0.5386
4160/4849 [========================>.....] - ETA: 50s - loss: 0.6893 - acc: 0.5399
4224/4849 [=========================>....] - ETA: 45s - loss: 0.6890 - acc: 0.5410
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6895 - acc: 0.5403
4352/4849 [=========================>....] - ETA: 36s - loss: 0.6897 - acc: 0.5411
4416/4849 [==========================>...] - ETA: 31s - loss: 0.6893 - acc: 0.5414
4480/4849 [==========================>...] - ETA: 26s - loss: 0.6895 - acc: 0.5404
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6892 - acc: 0.5412
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6887 - acc: 0.5428
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6888 - acc: 0.5432
4736/4849 [============================>.] - ETA: 8s - loss: 0.6888 - acc: 0.5431 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6896 - acc: 0.5423
4849/4849 [==============================] - 368s 76ms/step - loss: 0.6899 - acc: 0.5420 - val_loss: 0.7203 - val_acc: 0.5028

Epoch 00003: val_acc did not improve from 0.55844
Epoch 4/10

  64/4849 [..............................] - ETA: 5:49 - loss: 0.7017 - acc: 0.5000
 128/4849 [..............................] - ETA: 6:01 - loss: 0.6835 - acc: 0.5391
 192/4849 [>.............................] - ETA: 6:01 - loss: 0.6858 - acc: 0.5312
 256/4849 [>.............................] - ETA: 5:42 - loss: 0.7034 - acc: 0.4922
 320/4849 [>.............................] - ETA: 5:40 - loss: 0.6998 - acc: 0.5000
 384/4849 [=>............................] - ETA: 5:33 - loss: 0.6962 - acc: 0.5078
 448/4849 [=>............................] - ETA: 5:26 - loss: 0.6941 - acc: 0.5112
 512/4849 [==>...........................] - ETA: 5:18 - loss: 0.6913 - acc: 0.5195
 576/4849 [==>...........................] - ETA: 5:11 - loss: 0.6900 - acc: 0.5347
 640/4849 [==>...........................] - ETA: 5:07 - loss: 0.6884 - acc: 0.5375
 704/4849 [===>..........................] - ETA: 4:59 - loss: 0.6866 - acc: 0.5412
 768/4849 [===>..........................] - ETA: 4:55 - loss: 0.6863 - acc: 0.5456
 832/4849 [====>.........................] - ETA: 4:49 - loss: 0.6878 - acc: 0.5421
 896/4849 [====>.........................] - ETA: 4:45 - loss: 0.6890 - acc: 0.5368
 960/4849 [====>.........................] - ETA: 4:40 - loss: 0.6884 - acc: 0.5365
1024/4849 [=====>........................] - ETA: 4:37 - loss: 0.6905 - acc: 0.5322
1088/4849 [=====>........................] - ETA: 4:31 - loss: 0.6873 - acc: 0.5414
1152/4849 [======>.......................] - ETA: 4:26 - loss: 0.6896 - acc: 0.5365
1216/4849 [======>.......................] - ETA: 4:23 - loss: 0.6889 - acc: 0.5345
1280/4849 [======>.......................] - ETA: 4:19 - loss: 0.6884 - acc: 0.5359
1344/4849 [=======>......................] - ETA: 4:14 - loss: 0.6871 - acc: 0.5402
1408/4849 [=======>......................] - ETA: 4:08 - loss: 0.6879 - acc: 0.5384
1472/4849 [========>.....................] - ETA: 4:05 - loss: 0.6876 - acc: 0.5367
1536/4849 [========>.....................] - ETA: 4:01 - loss: 0.6870 - acc: 0.5410
1600/4849 [========>.....................] - ETA: 3:57 - loss: 0.6860 - acc: 0.5450
1664/4849 [=========>....................] - ETA: 3:53 - loss: 0.6865 - acc: 0.5445
1728/4849 [=========>....................] - ETA: 3:48 - loss: 0.6862 - acc: 0.5451
1792/4849 [==========>...................] - ETA: 3:43 - loss: 0.6869 - acc: 0.5446
1856/4849 [==========>...................] - ETA: 3:39 - loss: 0.6878 - acc: 0.5426
1920/4849 [==========>...................] - ETA: 3:34 - loss: 0.6871 - acc: 0.5453
1984/4849 [===========>..................] - ETA: 3:30 - loss: 0.6878 - acc: 0.5444
2048/4849 [===========>..................] - ETA: 3:25 - loss: 0.6870 - acc: 0.5479
2112/4849 [============>.................] - ETA: 3:21 - loss: 0.6872 - acc: 0.5483
2176/4849 [============>.................] - ETA: 3:16 - loss: 0.6882 - acc: 0.5441
2240/4849 [============>.................] - ETA: 3:11 - loss: 0.6885 - acc: 0.5433
2304/4849 [=============>................] - ETA: 3:06 - loss: 0.6881 - acc: 0.5443
2368/4849 [=============>................] - ETA: 3:02 - loss: 0.6878 - acc: 0.5456
2432/4849 [==============>...............] - ETA: 2:57 - loss: 0.6872 - acc: 0.5473
2496/4849 [==============>...............] - ETA: 2:52 - loss: 0.6864 - acc: 0.5513
2560/4849 [==============>...............] - ETA: 2:47 - loss: 0.6863 - acc: 0.5512
2624/4849 [===============>..............] - ETA: 2:43 - loss: 0.6859 - acc: 0.5537
2688/4849 [===============>..............] - ETA: 2:38 - loss: 0.6865 - acc: 0.5521
2752/4849 [================>.............] - ETA: 2:34 - loss: 0.6866 - acc: 0.5531
2816/4849 [================>.............] - ETA: 2:29 - loss: 0.6872 - acc: 0.5518
2880/4849 [================>.............] - ETA: 2:24 - loss: 0.6862 - acc: 0.5542
2944/4849 [=================>............] - ETA: 2:20 - loss: 0.6862 - acc: 0.5543
3008/4849 [=================>............] - ETA: 2:15 - loss: 0.6863 - acc: 0.5525
3072/4849 [==================>...........] - ETA: 2:10 - loss: 0.6860 - acc: 0.5540
3136/4849 [==================>...........] - ETA: 2:06 - loss: 0.6863 - acc: 0.5536
3200/4849 [==================>...........] - ETA: 2:01 - loss: 0.6863 - acc: 0.5528
3264/4849 [===================>..........] - ETA: 1:56 - loss: 0.6860 - acc: 0.5530
3328/4849 [===================>..........] - ETA: 1:52 - loss: 0.6860 - acc: 0.5517
3392/4849 [===================>..........] - ETA: 1:47 - loss: 0.6861 - acc: 0.5513
3456/4849 [====================>.........] - ETA: 1:43 - loss: 0.6862 - acc: 0.5509
3520/4849 [====================>.........] - ETA: 1:38 - loss: 0.6853 - acc: 0.5517
3584/4849 [=====================>........] - ETA: 1:33 - loss: 0.6846 - acc: 0.5541
3648/4849 [=====================>........] - ETA: 1:29 - loss: 0.6848 - acc: 0.5535
3712/4849 [=====================>........] - ETA: 1:24 - loss: 0.6854 - acc: 0.5533
3776/4849 [======================>.......] - ETA: 1:19 - loss: 0.6848 - acc: 0.5551
3840/4849 [======================>.......] - ETA: 1:14 - loss: 0.6845 - acc: 0.5555
3904/4849 [=======================>......] - ETA: 1:10 - loss: 0.6847 - acc: 0.5553
3968/4849 [=======================>......] - ETA: 1:05 - loss: 0.6850 - acc: 0.5552
4032/4849 [=======================>......] - ETA: 1:00 - loss: 0.6848 - acc: 0.5558
4096/4849 [========================>.....] - ETA: 56s - loss: 0.6844 - acc: 0.5562 
4160/4849 [========================>.....] - ETA: 51s - loss: 0.6843 - acc: 0.5567
4224/4849 [=========================>....] - ETA: 46s - loss: 0.6851 - acc: 0.5556
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6848 - acc: 0.5553
4352/4849 [=========================>....] - ETA: 36s - loss: 0.6848 - acc: 0.5558
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6851 - acc: 0.5555
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6858 - acc: 0.5538
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6858 - acc: 0.5533
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6861 - acc: 0.5536
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6862 - acc: 0.5535
4736/4849 [============================>.] - ETA: 8s - loss: 0.6856 - acc: 0.5545 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6851 - acc: 0.5558
4849/4849 [==============================] - 367s 76ms/step - loss: 0.6849 - acc: 0.5562 - val_loss: 0.6804 - val_acc: 0.5788

Epoch 00004: val_acc improved from 0.55844 to 0.57885, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window03/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 5/10

  64/4849 [..............................] - ETA: 5:26 - loss: 0.7147 - acc: 0.4219
 128/4849 [..............................] - ETA: 5:45 - loss: 0.6986 - acc: 0.4844
 192/4849 [>.............................] - ETA: 5:39 - loss: 0.6976 - acc: 0.4844
 256/4849 [>.............................] - ETA: 5:34 - loss: 0.6986 - acc: 0.5039
 320/4849 [>.............................] - ETA: 5:24 - loss: 0.6941 - acc: 0.5312
 384/4849 [=>............................] - ETA: 5:26 - loss: 0.6916 - acc: 0.5339
 448/4849 [=>............................] - ETA: 5:11 - loss: 0.6917 - acc: 0.5290
 512/4849 [==>...........................] - ETA: 5:12 - loss: 0.6906 - acc: 0.5234
 576/4849 [==>...........................] - ETA: 5:06 - loss: 0.6905 - acc: 0.5243
 640/4849 [==>...........................] - ETA: 5:00 - loss: 0.6871 - acc: 0.5312
 704/4849 [===>..........................] - ETA: 4:56 - loss: 0.6857 - acc: 0.5355
 768/4849 [===>..........................] - ETA: 4:53 - loss: 0.6883 - acc: 0.5286
 832/4849 [====>.........................] - ETA: 4:48 - loss: 0.6921 - acc: 0.5276
 896/4849 [====>.........................] - ETA: 4:41 - loss: 0.6869 - acc: 0.5379
 960/4849 [====>.........................] - ETA: 4:36 - loss: 0.6838 - acc: 0.5406
1024/4849 [=====>........................] - ETA: 4:30 - loss: 0.6818 - acc: 0.5459
1088/4849 [=====>........................] - ETA: 4:24 - loss: 0.6826 - acc: 0.5450
1152/4849 [======>.......................] - ETA: 4:20 - loss: 0.6806 - acc: 0.5530
1216/4849 [======>.......................] - ETA: 4:15 - loss: 0.6791 - acc: 0.5567
1280/4849 [======>.......................] - ETA: 4:09 - loss: 0.6809 - acc: 0.5555
1344/4849 [=======>......................] - ETA: 4:05 - loss: 0.6805 - acc: 0.5573
1408/4849 [=======>......................] - ETA: 4:01 - loss: 0.6819 - acc: 0.5561
1472/4849 [========>.....................] - ETA: 3:56 - loss: 0.6801 - acc: 0.5591
1536/4849 [========>.....................] - ETA: 3:52 - loss: 0.6810 - acc: 0.5566
1600/4849 [========>.....................] - ETA: 3:47 - loss: 0.6813 - acc: 0.5575
1664/4849 [=========>....................] - ETA: 3:43 - loss: 0.6819 - acc: 0.5559
1728/4849 [=========>....................] - ETA: 3:37 - loss: 0.6828 - acc: 0.5544
1792/4849 [==========>...................] - ETA: 3:33 - loss: 0.6825 - acc: 0.5564
1856/4849 [==========>...................] - ETA: 3:28 - loss: 0.6811 - acc: 0.5593
1920/4849 [==========>...................] - ETA: 3:24 - loss: 0.6809 - acc: 0.5599
1984/4849 [===========>..................] - ETA: 3:20 - loss: 0.6796 - acc: 0.5630
2048/4849 [===========>..................] - ETA: 3:15 - loss: 0.6801 - acc: 0.5620
2112/4849 [============>.................] - ETA: 3:10 - loss: 0.6804 - acc: 0.5601
2176/4849 [============>.................] - ETA: 3:05 - loss: 0.6806 - acc: 0.5579
2240/4849 [============>.................] - ETA: 3:01 - loss: 0.6797 - acc: 0.5598
2304/4849 [=============>................] - ETA: 2:57 - loss: 0.6799 - acc: 0.5595
2368/4849 [=============>................] - ETA: 2:53 - loss: 0.6805 - acc: 0.5587
2432/4849 [==============>...............] - ETA: 2:48 - loss: 0.6799 - acc: 0.5596
2496/4849 [==============>...............] - ETA: 2:44 - loss: 0.6800 - acc: 0.5577
2560/4849 [==============>...............] - ETA: 2:39 - loss: 0.6804 - acc: 0.5582
2624/4849 [===============>..............] - ETA: 2:35 - loss: 0.6808 - acc: 0.5564
2688/4849 [===============>..............] - ETA: 2:31 - loss: 0.6812 - acc: 0.5569
2752/4849 [================>.............] - ETA: 2:26 - loss: 0.6806 - acc: 0.5574
2816/4849 [================>.............] - ETA: 2:23 - loss: 0.6803 - acc: 0.5579
2880/4849 [================>.............] - ETA: 2:18 - loss: 0.6800 - acc: 0.5587
2944/4849 [=================>............] - ETA: 2:14 - loss: 0.6803 - acc: 0.5581
3008/4849 [=================>............] - ETA: 2:09 - loss: 0.6804 - acc: 0.5575
3072/4849 [==================>...........] - ETA: 2:05 - loss: 0.6815 - acc: 0.5553
3136/4849 [==================>...........] - ETA: 2:00 - loss: 0.6815 - acc: 0.5555
3200/4849 [==================>...........] - ETA: 1:55 - loss: 0.6818 - acc: 0.5556
3264/4849 [===================>..........] - ETA: 1:51 - loss: 0.6822 - acc: 0.5551
3328/4849 [===================>..........] - ETA: 1:46 - loss: 0.6822 - acc: 0.5556
3392/4849 [===================>..........] - ETA: 1:42 - loss: 0.6833 - acc: 0.5531
3456/4849 [====================>.........] - ETA: 1:37 - loss: 0.6829 - acc: 0.5530
3520/4849 [====================>.........] - ETA: 1:33 - loss: 0.6836 - acc: 0.5534
3584/4849 [=====================>........] - ETA: 1:28 - loss: 0.6832 - acc: 0.5547
3648/4849 [=====================>........] - ETA: 1:24 - loss: 0.6837 - acc: 0.5551
3712/4849 [=====================>........] - ETA: 1:19 - loss: 0.6836 - acc: 0.5541
3776/4849 [======================>.......] - ETA: 1:15 - loss: 0.6832 - acc: 0.5551
3840/4849 [======================>.......] - ETA: 1:10 - loss: 0.6839 - acc: 0.5544
3904/4849 [=======================>......] - ETA: 1:05 - loss: 0.6832 - acc: 0.5548
3968/4849 [=======================>......] - ETA: 1:01 - loss: 0.6831 - acc: 0.5542
4032/4849 [=======================>......] - ETA: 56s - loss: 0.6836 - acc: 0.5526 
4096/4849 [========================>.....] - ETA: 52s - loss: 0.6841 - acc: 0.5522
4160/4849 [========================>.....] - ETA: 47s - loss: 0.6844 - acc: 0.5519
4224/4849 [=========================>....] - ETA: 43s - loss: 0.6847 - acc: 0.5514
4288/4849 [=========================>....] - ETA: 39s - loss: 0.6842 - acc: 0.5532
4352/4849 [=========================>....] - ETA: 34s - loss: 0.6837 - acc: 0.5533
4416/4849 [==========================>...] - ETA: 30s - loss: 0.6836 - acc: 0.5532
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6836 - acc: 0.5538
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6835 - acc: 0.5541
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6834 - acc: 0.5549
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6834 - acc: 0.5554
4736/4849 [============================>.] - ETA: 7s - loss: 0.6834 - acc: 0.5560 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6834 - acc: 0.5569
4849/4849 [==============================] - 350s 72ms/step - loss: 0.6835 - acc: 0.5570 - val_loss: 0.6813 - val_acc: 0.5640

Epoch 00005: val_acc did not improve from 0.57885
Epoch 6/10

  64/4849 [..............................] - ETA: 5:59 - loss: 0.7024 - acc: 0.5156
 128/4849 [..............................] - ETA: 5:15 - loss: 0.7024 - acc: 0.5078
 192/4849 [>.............................] - ETA: 5:27 - loss: 0.6947 - acc: 0.5260
 256/4849 [>.............................] - ETA: 5:33 - loss: 0.6958 - acc: 0.5273
 320/4849 [>.............................] - ETA: 5:20 - loss: 0.6911 - acc: 0.5406
 384/4849 [=>............................] - ETA: 5:20 - loss: 0.6890 - acc: 0.5495
 448/4849 [=>............................] - ETA: 5:06 - loss: 0.6859 - acc: 0.5536
 512/4849 [==>...........................] - ETA: 4:59 - loss: 0.6870 - acc: 0.5527
 576/4849 [==>...........................] - ETA: 4:48 - loss: 0.6811 - acc: 0.5694
 640/4849 [==>...........................] - ETA: 4:47 - loss: 0.6806 - acc: 0.5719
 704/4849 [===>..........................] - ETA: 4:41 - loss: 0.6822 - acc: 0.5682
 768/4849 [===>..........................] - ETA: 4:37 - loss: 0.6802 - acc: 0.5742
 832/4849 [====>.........................] - ETA: 4:33 - loss: 0.6818 - acc: 0.5685
 896/4849 [====>.........................] - ETA: 4:26 - loss: 0.6830 - acc: 0.5670
 960/4849 [====>.........................] - ETA: 4:23 - loss: 0.6819 - acc: 0.5708
1024/4849 [=====>........................] - ETA: 4:17 - loss: 0.6819 - acc: 0.5723
1088/4849 [=====>........................] - ETA: 4:14 - loss: 0.6798 - acc: 0.5772
1152/4849 [======>.......................] - ETA: 4:10 - loss: 0.6802 - acc: 0.5790
1216/4849 [======>.......................] - ETA: 4:06 - loss: 0.6806 - acc: 0.5748
1280/4849 [======>.......................] - ETA: 4:04 - loss: 0.6834 - acc: 0.5687
1344/4849 [=======>......................] - ETA: 3:59 - loss: 0.6804 - acc: 0.5737
1408/4849 [=======>......................] - ETA: 3:55 - loss: 0.6810 - acc: 0.5732
1472/4849 [========>.....................] - ETA: 3:50 - loss: 0.6822 - acc: 0.5659
1536/4849 [========>.....................] - ETA: 3:46 - loss: 0.6811 - acc: 0.5697
1600/4849 [========>.....................] - ETA: 3:42 - loss: 0.6805 - acc: 0.5719
1664/4849 [=========>....................] - ETA: 3:37 - loss: 0.6803 - acc: 0.5715
1728/4849 [=========>....................] - ETA: 3:32 - loss: 0.6812 - acc: 0.5689
1792/4849 [==========>...................] - ETA: 3:26 - loss: 0.6798 - acc: 0.5714
1856/4849 [==========>...................] - ETA: 3:22 - loss: 0.6787 - acc: 0.5717
1920/4849 [==========>...................] - ETA: 3:18 - loss: 0.6784 - acc: 0.5708
1984/4849 [===========>..................] - ETA: 3:14 - loss: 0.6778 - acc: 0.5726
2048/4849 [===========>..................] - ETA: 3:09 - loss: 0.6788 - acc: 0.5728
2112/4849 [============>.................] - ETA: 3:04 - loss: 0.6793 - acc: 0.5729
2176/4849 [============>.................] - ETA: 3:00 - loss: 0.6791 - acc: 0.5708
2240/4849 [============>.................] - ETA: 2:55 - loss: 0.6788 - acc: 0.5714
2304/4849 [=============>................] - ETA: 2:51 - loss: 0.6782 - acc: 0.5720
2368/4849 [=============>................] - ETA: 2:47 - loss: 0.6773 - acc: 0.5731
2432/4849 [==============>...............] - ETA: 2:43 - loss: 0.6778 - acc: 0.5724
2496/4849 [==============>...............] - ETA: 2:38 - loss: 0.6778 - acc: 0.5733
2560/4849 [==============>...............] - ETA: 2:34 - loss: 0.6778 - acc: 0.5719
2624/4849 [===============>..............] - ETA: 2:29 - loss: 0.6770 - acc: 0.5724
2688/4849 [===============>..............] - ETA: 2:25 - loss: 0.6767 - acc: 0.5740
2752/4849 [================>.............] - ETA: 2:20 - loss: 0.6764 - acc: 0.5738
2816/4849 [================>.............] - ETA: 2:16 - loss: 0.6772 - acc: 0.5717
2880/4849 [================>.............] - ETA: 2:12 - loss: 0.6767 - acc: 0.5708
2944/4849 [=================>............] - ETA: 2:07 - loss: 0.6769 - acc: 0.5710
3008/4849 [=================>............] - ETA: 2:03 - loss: 0.6766 - acc: 0.5711
3072/4849 [==================>...........] - ETA: 1:59 - loss: 0.6771 - acc: 0.5710
3136/4849 [==================>...........] - ETA: 1:54 - loss: 0.6761 - acc: 0.5724
3200/4849 [==================>...........] - ETA: 1:50 - loss: 0.6765 - acc: 0.5731
3264/4849 [===================>..........] - ETA: 1:46 - loss: 0.6771 - acc: 0.5729
3328/4849 [===================>..........] - ETA: 1:41 - loss: 0.6772 - acc: 0.5736
3392/4849 [===================>..........] - ETA: 1:37 - loss: 0.6775 - acc: 0.5743
3456/4849 [====================>.........] - ETA: 1:33 - loss: 0.6773 - acc: 0.5747
3520/4849 [====================>.........] - ETA: 1:28 - loss: 0.6769 - acc: 0.5756
3584/4849 [=====================>........] - ETA: 1:24 - loss: 0.6763 - acc: 0.5765
3648/4849 [=====================>........] - ETA: 1:20 - loss: 0.6767 - acc: 0.5762
3712/4849 [=====================>........] - ETA: 1:16 - loss: 0.6767 - acc: 0.5760
3776/4849 [======================>.......] - ETA: 1:11 - loss: 0.6772 - acc: 0.5755
3840/4849 [======================>.......] - ETA: 1:07 - loss: 0.6771 - acc: 0.5755
3904/4849 [=======================>......] - ETA: 1:03 - loss: 0.6772 - acc: 0.5761
3968/4849 [=======================>......] - ETA: 58s - loss: 0.6774 - acc: 0.5761 
4032/4849 [=======================>......] - ETA: 54s - loss: 0.6779 - acc: 0.5754
4096/4849 [========================>.....] - ETA: 50s - loss: 0.6786 - acc: 0.5735
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6790 - acc: 0.5724
4224/4849 [=========================>....] - ETA: 41s - loss: 0.6788 - acc: 0.5729
4288/4849 [=========================>....] - ETA: 37s - loss: 0.6789 - acc: 0.5721
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6794 - acc: 0.5705
4416/4849 [==========================>...] - ETA: 28s - loss: 0.6795 - acc: 0.5711
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6793 - acc: 0.5701
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6794 - acc: 0.5695
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6795 - acc: 0.5692
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6800 - acc: 0.5685
4736/4849 [============================>.] - ETA: 7s - loss: 0.6796 - acc: 0.5688 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6796 - acc: 0.5685
4849/4849 [==============================] - 338s 70ms/step - loss: 0.6795 - acc: 0.5684 - val_loss: 0.6754 - val_acc: 0.5826

Epoch 00006: val_acc improved from 0.57885 to 0.58256, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window03/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 7/10

  64/4849 [..............................] - ETA: 4:42 - loss: 0.6869 - acc: 0.5938
 128/4849 [..............................] - ETA: 5:11 - loss: 0.6899 - acc: 0.5625
 192/4849 [>.............................] - ETA: 5:12 - loss: 0.6974 - acc: 0.5521
 256/4849 [>.............................] - ETA: 5:12 - loss: 0.6977 - acc: 0.5664
 320/4849 [>.............................] - ETA: 5:01 - loss: 0.6912 - acc: 0.5750
 384/4849 [=>............................] - ETA: 4:58 - loss: 0.6941 - acc: 0.5625
 448/4849 [=>............................] - ETA: 4:59 - loss: 0.6921 - acc: 0.5536
 512/4849 [==>...........................] - ETA: 4:49 - loss: 0.6905 - acc: 0.5586
 576/4849 [==>...........................] - ETA: 4:47 - loss: 0.6933 - acc: 0.5469
 640/4849 [==>...........................] - ETA: 4:38 - loss: 0.6921 - acc: 0.5469
 704/4849 [===>..........................] - ETA: 4:33 - loss: 0.6936 - acc: 0.5455
 768/4849 [===>..........................] - ETA: 4:28 - loss: 0.6952 - acc: 0.5456
 832/4849 [====>.........................] - ETA: 4:23 - loss: 0.6938 - acc: 0.5493
 896/4849 [====>.........................] - ETA: 4:24 - loss: 0.6923 - acc: 0.5525
 960/4849 [====>.........................] - ETA: 4:22 - loss: 0.6908 - acc: 0.5552
1024/4849 [=====>........................] - ETA: 4:21 - loss: 0.6906 - acc: 0.5547
1088/4849 [=====>........................] - ETA: 4:19 - loss: 0.6883 - acc: 0.5634
1152/4849 [======>.......................] - ETA: 4:15 - loss: 0.6872 - acc: 0.5651
1216/4849 [======>.......................] - ETA: 4:13 - loss: 0.6883 - acc: 0.5617
1280/4849 [======>.......................] - ETA: 4:11 - loss: 0.6870 - acc: 0.5625
1344/4849 [=======>......................] - ETA: 4:07 - loss: 0.6860 - acc: 0.5640
1408/4849 [=======>......................] - ETA: 4:05 - loss: 0.6854 - acc: 0.5661
1472/4849 [========>.....................] - ETA: 4:01 - loss: 0.6864 - acc: 0.5645
1536/4849 [========>.....................] - ETA: 3:58 - loss: 0.6869 - acc: 0.5645
1600/4849 [========>.....................] - ETA: 3:54 - loss: 0.6880 - acc: 0.5619
1664/4849 [=========>....................] - ETA: 3:50 - loss: 0.6856 - acc: 0.5679
1728/4849 [=========>....................] - ETA: 3:46 - loss: 0.6861 - acc: 0.5666
1792/4849 [==========>...................] - ETA: 3:42 - loss: 0.6865 - acc: 0.5658
1856/4849 [==========>...................] - ETA: 3:38 - loss: 0.6857 - acc: 0.5663
1920/4849 [==========>...................] - ETA: 3:34 - loss: 0.6862 - acc: 0.5646
1984/4849 [===========>..................] - ETA: 3:29 - loss: 0.6850 - acc: 0.5660
2048/4849 [===========>..................] - ETA: 3:26 - loss: 0.6836 - acc: 0.5669
2112/4849 [============>.................] - ETA: 3:22 - loss: 0.6838 - acc: 0.5658
2176/4849 [============>.................] - ETA: 3:17 - loss: 0.6841 - acc: 0.5657
2240/4849 [============>.................] - ETA: 3:12 - loss: 0.6837 - acc: 0.5652
2304/4849 [=============>................] - ETA: 3:08 - loss: 0.6831 - acc: 0.5660
2368/4849 [=============>................] - ETA: 3:03 - loss: 0.6832 - acc: 0.5663
2432/4849 [==============>...............] - ETA: 2:59 - loss: 0.6826 - acc: 0.5658
2496/4849 [==============>...............] - ETA: 2:54 - loss: 0.6832 - acc: 0.5633
2560/4849 [==============>...............] - ETA: 2:51 - loss: 0.6823 - acc: 0.5652
2624/4849 [===============>..............] - ETA: 2:46 - loss: 0.6815 - acc: 0.5678
2688/4849 [===============>..............] - ETA: 2:41 - loss: 0.6818 - acc: 0.5658
2752/4849 [================>.............] - ETA: 2:36 - loss: 0.6816 - acc: 0.5658
2816/4849 [================>.............] - ETA: 2:32 - loss: 0.6802 - acc: 0.5685
2880/4849 [================>.............] - ETA: 2:27 - loss: 0.6798 - acc: 0.5694
2944/4849 [=================>............] - ETA: 2:22 - loss: 0.6800 - acc: 0.5686
3008/4849 [=================>............] - ETA: 2:18 - loss: 0.6807 - acc: 0.5668
3072/4849 [==================>...........] - ETA: 2:13 - loss: 0.6801 - acc: 0.5680
3136/4849 [==================>...........] - ETA: 2:08 - loss: 0.6807 - acc: 0.5682
3200/4849 [==================>...........] - ETA: 2:04 - loss: 0.6810 - acc: 0.5669
3264/4849 [===================>..........] - ETA: 1:59 - loss: 0.6806 - acc: 0.5689
3328/4849 [===================>..........] - ETA: 1:54 - loss: 0.6800 - acc: 0.5712
3392/4849 [===================>..........] - ETA: 1:50 - loss: 0.6787 - acc: 0.5725
3456/4849 [====================>.........] - ETA: 1:45 - loss: 0.6788 - acc: 0.5723
3520/4849 [====================>.........] - ETA: 1:40 - loss: 0.6778 - acc: 0.5744
3584/4849 [=====================>........] - ETA: 1:35 - loss: 0.6778 - acc: 0.5734
3648/4849 [=====================>........] - ETA: 1:30 - loss: 0.6774 - acc: 0.5740
3712/4849 [=====================>........] - ETA: 1:26 - loss: 0.6770 - acc: 0.5757
3776/4849 [======================>.......] - ETA: 1:21 - loss: 0.6775 - acc: 0.5744
3840/4849 [======================>.......] - ETA: 1:16 - loss: 0.6768 - acc: 0.5760
3904/4849 [=======================>......] - ETA: 1:11 - loss: 0.6772 - acc: 0.5753
3968/4849 [=======================>......] - ETA: 1:06 - loss: 0.6777 - acc: 0.5751
4032/4849 [=======================>......] - ETA: 1:01 - loss: 0.6774 - acc: 0.5759
4096/4849 [========================>.....] - ETA: 57s - loss: 0.6779 - acc: 0.5747 
4160/4849 [========================>.....] - ETA: 52s - loss: 0.6774 - acc: 0.5755
4224/4849 [=========================>....] - ETA: 47s - loss: 0.6773 - acc: 0.5750
4288/4849 [=========================>....] - ETA: 42s - loss: 0.6776 - acc: 0.5742
4352/4849 [=========================>....] - ETA: 37s - loss: 0.6775 - acc: 0.5747
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6778 - acc: 0.5738
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6776 - acc: 0.5743
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6772 - acc: 0.5753
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6768 - acc: 0.5757
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6768 - acc: 0.5753
4736/4849 [============================>.] - ETA: 8s - loss: 0.6772 - acc: 0.5741 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6776 - acc: 0.5744
4849/4849 [==============================] - 384s 79ms/step - loss: 0.6772 - acc: 0.5746 - val_loss: 0.6973 - val_acc: 0.5362

Epoch 00007: val_acc did not improve from 0.58256
Epoch 8/10

  64/4849 [..............................] - ETA: 5:21 - loss: 0.6913 - acc: 0.5469
 128/4849 [..............................] - ETA: 5:50 - loss: 0.6837 - acc: 0.5703
 192/4849 [>.............................] - ETA: 5:41 - loss: 0.6809 - acc: 0.5833
 256/4849 [>.............................] - ETA: 5:44 - loss: 0.6744 - acc: 0.5859
 320/4849 [>.............................] - ETA: 5:33 - loss: 0.6842 - acc: 0.5687
 384/4849 [=>............................] - ETA: 5:29 - loss: 0.6843 - acc: 0.5651
 448/4849 [=>............................] - ETA: 5:25 - loss: 0.6768 - acc: 0.5759
 512/4849 [==>...........................] - ETA: 5:20 - loss: 0.6813 - acc: 0.5645
 576/4849 [==>...........................] - ETA: 5:12 - loss: 0.6857 - acc: 0.5608
 640/4849 [==>...........................] - ETA: 5:11 - loss: 0.6862 - acc: 0.5625
 704/4849 [===>..........................] - ETA: 5:07 - loss: 0.6831 - acc: 0.5710
 768/4849 [===>..........................] - ETA: 5:01 - loss: 0.6829 - acc: 0.5664
 832/4849 [====>.........................] - ETA: 4:59 - loss: 0.6817 - acc: 0.5673
 896/4849 [====>.........................] - ETA: 4:55 - loss: 0.6822 - acc: 0.5692
 960/4849 [====>.........................] - ETA: 4:49 - loss: 0.6817 - acc: 0.5740
1024/4849 [=====>........................] - ETA: 4:45 - loss: 0.6824 - acc: 0.5723
1088/4849 [=====>........................] - ETA: 4:42 - loss: 0.6821 - acc: 0.5708
1152/4849 [======>.......................] - ETA: 4:36 - loss: 0.6834 - acc: 0.5677
1216/4849 [======>.......................] - ETA: 4:33 - loss: 0.6845 - acc: 0.5658
1280/4849 [======>.......................] - ETA: 4:29 - loss: 0.6857 - acc: 0.5648
1344/4849 [=======>......................] - ETA: 4:23 - loss: 0.6851 - acc: 0.5662
1408/4849 [=======>......................] - ETA: 4:18 - loss: 0.6847 - acc: 0.5646
1472/4849 [========>.....................] - ETA: 4:14 - loss: 0.6844 - acc: 0.5652
1536/4849 [========>.....................] - ETA: 4:09 - loss: 0.6843 - acc: 0.5671
1600/4849 [========>.....................] - ETA: 4:05 - loss: 0.6827 - acc: 0.5713
1664/4849 [=========>....................] - ETA: 4:00 - loss: 0.6812 - acc: 0.5739
1728/4849 [=========>....................] - ETA: 3:56 - loss: 0.6801 - acc: 0.5747
1792/4849 [==========>...................] - ETA: 3:51 - loss: 0.6800 - acc: 0.5731
1856/4849 [==========>...................] - ETA: 3:46 - loss: 0.6792 - acc: 0.5749
1920/4849 [==========>...................] - ETA: 3:41 - loss: 0.6803 - acc: 0.5729
1984/4849 [===========>..................] - ETA: 3:36 - loss: 0.6803 - acc: 0.5741
2048/4849 [===========>..................] - ETA: 3:31 - loss: 0.6819 - acc: 0.5698
2112/4849 [============>.................] - ETA: 3:26 - loss: 0.6812 - acc: 0.5710
2176/4849 [============>.................] - ETA: 3:21 - loss: 0.6825 - acc: 0.5676
2240/4849 [============>.................] - ETA: 3:17 - loss: 0.6821 - acc: 0.5679
2304/4849 [=============>................] - ETA: 3:12 - loss: 0.6815 - acc: 0.5686
2368/4849 [=============>................] - ETA: 3:07 - loss: 0.6803 - acc: 0.5718
2432/4849 [==============>...............] - ETA: 3:02 - loss: 0.6800 - acc: 0.5715
2496/4849 [==============>...............] - ETA: 2:58 - loss: 0.6794 - acc: 0.5737
2560/4849 [==============>...............] - ETA: 2:52 - loss: 0.6789 - acc: 0.5766
2624/4849 [===============>..............] - ETA: 2:48 - loss: 0.6784 - acc: 0.5777
2688/4849 [===============>..............] - ETA: 2:43 - loss: 0.6784 - acc: 0.5785
2752/4849 [================>.............] - ETA: 2:38 - loss: 0.6785 - acc: 0.5789
2816/4849 [================>.............] - ETA: 2:33 - loss: 0.6779 - acc: 0.5788
2880/4849 [================>.............] - ETA: 2:29 - loss: 0.6783 - acc: 0.5788
2944/4849 [=================>............] - ETA: 2:24 - loss: 0.6787 - acc: 0.5788
3008/4849 [=================>............] - ETA: 2:19 - loss: 0.6783 - acc: 0.5791
3072/4849 [==================>...........] - ETA: 2:14 - loss: 0.6794 - acc: 0.5771
3136/4849 [==================>...........] - ETA: 2:09 - loss: 0.6784 - acc: 0.5788
3200/4849 [==================>...........] - ETA: 2:04 - loss: 0.6772 - acc: 0.5809
3264/4849 [===================>..........] - ETA: 2:00 - loss: 0.6786 - acc: 0.5790
3328/4849 [===================>..........] - ETA: 1:55 - loss: 0.6785 - acc: 0.5793
3392/4849 [===================>..........] - ETA: 1:50 - loss: 0.6790 - acc: 0.5775
3456/4849 [====================>.........] - ETA: 1:45 - loss: 0.6798 - acc: 0.5758
3520/4849 [====================>.........] - ETA: 1:40 - loss: 0.6794 - acc: 0.5773
3584/4849 [=====================>........] - ETA: 1:35 - loss: 0.6805 - acc: 0.5762
3648/4849 [=====================>........] - ETA: 1:30 - loss: 0.6804 - acc: 0.5765
3712/4849 [=====================>........] - ETA: 1:26 - loss: 0.6803 - acc: 0.5765
3776/4849 [======================>.......] - ETA: 1:21 - loss: 0.6796 - acc: 0.5771
3840/4849 [======================>.......] - ETA: 1:16 - loss: 0.6801 - acc: 0.5763
3904/4849 [=======================>......] - ETA: 1:11 - loss: 0.6802 - acc: 0.5763
3968/4849 [=======================>......] - ETA: 1:06 - loss: 0.6799 - acc: 0.5764
4032/4849 [=======================>......] - ETA: 1:01 - loss: 0.6801 - acc: 0.5749
4096/4849 [========================>.....] - ETA: 56s - loss: 0.6798 - acc: 0.5764 
4160/4849 [========================>.....] - ETA: 52s - loss: 0.6796 - acc: 0.5769
4224/4849 [=========================>....] - ETA: 47s - loss: 0.6795 - acc: 0.5767
4288/4849 [=========================>....] - ETA: 42s - loss: 0.6796 - acc: 0.5770
4352/4849 [=========================>....] - ETA: 37s - loss: 0.6793 - acc: 0.5777
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6792 - acc: 0.5774
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6791 - acc: 0.5783
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6788 - acc: 0.5794
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6781 - acc: 0.5807
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6784 - acc: 0.5803
4736/4849 [============================>.] - ETA: 8s - loss: 0.6778 - acc: 0.5815 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6783 - acc: 0.5810
4849/4849 [==============================] - 381s 79ms/step - loss: 0.6785 - acc: 0.5803 - val_loss: 0.6840 - val_acc: 0.5770

Epoch 00008: val_acc did not improve from 0.58256
Epoch 9/10

  64/4849 [..............................] - ETA: 5:31 - loss: 0.7056 - acc: 0.5625
 128/4849 [..............................] - ETA: 5:53 - loss: 0.6998 - acc: 0.5625
 192/4849 [>.............................] - ETA: 5:51 - loss: 0.7100 - acc: 0.5729
 256/4849 [>.............................] - ETA: 5:49 - loss: 0.6959 - acc: 0.5664
 320/4849 [>.............................] - ETA: 5:44 - loss: 0.6940 - acc: 0.5656
 384/4849 [=>............................] - ETA: 5:43 - loss: 0.6911 - acc: 0.5677
 448/4849 [=>............................] - ETA: 5:38 - loss: 0.6882 - acc: 0.5670
 512/4849 [==>...........................] - ETA: 5:33 - loss: 0.6905 - acc: 0.5547
 576/4849 [==>...........................] - ETA: 5:29 - loss: 0.6927 - acc: 0.5573
 640/4849 [==>...........................] - ETA: 5:23 - loss: 0.6878 - acc: 0.5672
 704/4849 [===>..........................] - ETA: 5:18 - loss: 0.6861 - acc: 0.5653
 768/4849 [===>..........................] - ETA: 5:14 - loss: 0.6875 - acc: 0.5638
 832/4849 [====>.........................] - ETA: 5:08 - loss: 0.6900 - acc: 0.5613
 896/4849 [====>.........................] - ETA: 5:04 - loss: 0.6893 - acc: 0.5614
 960/4849 [====>.........................] - ETA: 4:57 - loss: 0.6857 - acc: 0.5677
1024/4849 [=====>........................] - ETA: 4:51 - loss: 0.6842 - acc: 0.5713
1088/4849 [=====>........................] - ETA: 4:46 - loss: 0.6837 - acc: 0.5680
1152/4849 [======>.......................] - ETA: 4:41 - loss: 0.6835 - acc: 0.5703
1216/4849 [======>.......................] - ETA: 4:36 - loss: 0.6815 - acc: 0.5740
1280/4849 [======>.......................] - ETA: 4:32 - loss: 0.6790 - acc: 0.5789
1344/4849 [=======>......................] - ETA: 4:29 - loss: 0.6787 - acc: 0.5796
1408/4849 [=======>......................] - ETA: 4:24 - loss: 0.6776 - acc: 0.5795
1472/4849 [========>.....................] - ETA: 4:19 - loss: 0.6774 - acc: 0.5815
1536/4849 [========>.....................] - ETA: 4:15 - loss: 0.6786 - acc: 0.5794
1600/4849 [========>.....................] - ETA: 4:09 - loss: 0.6785 - acc: 0.5781
1664/4849 [=========>....................] - ETA: 4:06 - loss: 0.6786 - acc: 0.5787
1728/4849 [=========>....................] - ETA: 4:01 - loss: 0.6794 - acc: 0.5723
1792/4849 [==========>...................] - ETA: 3:56 - loss: 0.6794 - acc: 0.5720
1856/4849 [==========>...................] - ETA: 3:51 - loss: 0.6799 - acc: 0.5706
1920/4849 [==========>...................] - ETA: 3:46 - loss: 0.6807 - acc: 0.5687
1984/4849 [===========>..................] - ETA: 3:41 - loss: 0.6816 - acc: 0.5680
2048/4849 [===========>..................] - ETA: 3:37 - loss: 0.6829 - acc: 0.5674
2112/4849 [============>.................] - ETA: 3:32 - loss: 0.6822 - acc: 0.5672
2176/4849 [============>.................] - ETA: 3:27 - loss: 0.6825 - acc: 0.5671
2240/4849 [============>.................] - ETA: 3:23 - loss: 0.6819 - acc: 0.5696
2304/4849 [=============>................] - ETA: 3:18 - loss: 0.6802 - acc: 0.5729
2368/4849 [=============>................] - ETA: 3:13 - loss: 0.6803 - acc: 0.5722
2432/4849 [==============>...............] - ETA: 3:08 - loss: 0.6789 - acc: 0.5757
2496/4849 [==============>...............] - ETA: 3:03 - loss: 0.6795 - acc: 0.5749
2560/4849 [==============>...............] - ETA: 2:58 - loss: 0.6781 - acc: 0.5762
2624/4849 [===============>..............] - ETA: 2:54 - loss: 0.6766 - acc: 0.5770
2688/4849 [===============>..............] - ETA: 2:49 - loss: 0.6772 - acc: 0.5755
2752/4849 [================>.............] - ETA: 2:44 - loss: 0.6774 - acc: 0.5741
2816/4849 [================>.............] - ETA: 2:39 - loss: 0.6773 - acc: 0.5749
2880/4849 [================>.............] - ETA: 2:34 - loss: 0.6769 - acc: 0.5760
2944/4849 [=================>............] - ETA: 2:29 - loss: 0.6772 - acc: 0.5747
3008/4849 [=================>............] - ETA: 2:24 - loss: 0.6765 - acc: 0.5758
3072/4849 [==================>...........] - ETA: 2:19 - loss: 0.6763 - acc: 0.5762
3136/4849 [==================>...........] - ETA: 2:14 - loss: 0.6761 - acc: 0.5778
3200/4849 [==================>...........] - ETA: 2:09 - loss: 0.6762 - acc: 0.5784
3264/4849 [===================>..........] - ETA: 2:04 - loss: 0.6765 - acc: 0.5778
3328/4849 [===================>..........] - ETA: 1:59 - loss: 0.6764 - acc: 0.5790
3392/4849 [===================>..........] - ETA: 1:54 - loss: 0.6762 - acc: 0.5808
3456/4849 [====================>.........] - ETA: 1:49 - loss: 0.6760 - acc: 0.5813
3520/4849 [====================>.........] - ETA: 1:44 - loss: 0.6758 - acc: 0.5815
3584/4849 [=====================>........] - ETA: 1:39 - loss: 0.6751 - acc: 0.5823
3648/4849 [=====================>........] - ETA: 1:34 - loss: 0.6757 - acc: 0.5806
3712/4849 [=====================>........] - ETA: 1:30 - loss: 0.6758 - acc: 0.5792
3776/4849 [======================>.......] - ETA: 1:24 - loss: 0.6757 - acc: 0.5794
3840/4849 [======================>.......] - ETA: 1:19 - loss: 0.6757 - acc: 0.5792
3904/4849 [=======================>......] - ETA: 1:14 - loss: 0.6762 - acc: 0.5786
3968/4849 [=======================>......] - ETA: 1:09 - loss: 0.6769 - acc: 0.5774
4032/4849 [=======================>......] - ETA: 1:04 - loss: 0.6775 - acc: 0.5764
4096/4849 [========================>.....] - ETA: 59s - loss: 0.6770 - acc: 0.5771 
4160/4849 [========================>.....] - ETA: 54s - loss: 0.6766 - acc: 0.5784
4224/4849 [=========================>....] - ETA: 49s - loss: 0.6766 - acc: 0.5788
4288/4849 [=========================>....] - ETA: 44s - loss: 0.6768 - acc: 0.5779
4352/4849 [=========================>....] - ETA: 39s - loss: 0.6772 - acc: 0.5767
4416/4849 [==========================>...] - ETA: 34s - loss: 0.6769 - acc: 0.5772
4480/4849 [==========================>...] - ETA: 29s - loss: 0.6767 - acc: 0.5775
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6774 - acc: 0.5764
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6779 - acc: 0.5762
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6783 - acc: 0.5760
4736/4849 [============================>.] - ETA: 8s - loss: 0.6786 - acc: 0.5741 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6785 - acc: 0.5731
4849/4849 [==============================] - 401s 83ms/step - loss: 0.6792 - acc: 0.5725 - val_loss: 0.6960 - val_acc: 0.5269

Epoch 00009: val_acc did not improve from 0.58256
Epoch 10/10

  64/4849 [..............................] - ETA: 6:28 - loss: 0.7275 - acc: 0.4688
 128/4849 [..............................] - ETA: 6:18 - loss: 0.7055 - acc: 0.5391
 192/4849 [>.............................] - ETA: 6:22 - loss: 0.6870 - acc: 0.5677
 256/4849 [>.............................] - ETA: 6:15 - loss: 0.6864 - acc: 0.5742
 320/4849 [>.............................] - ETA: 6:15 - loss: 0.6829 - acc: 0.5813
 384/4849 [=>............................] - ETA: 6:09 - loss: 0.6833 - acc: 0.5755
 448/4849 [=>............................] - ETA: 6:00 - loss: 0.6873 - acc: 0.5692
 512/4849 [==>...........................] - ETA: 5:54 - loss: 0.6877 - acc: 0.5625
 576/4849 [==>...........................] - ETA: 5:47 - loss: 0.6867 - acc: 0.5660
 640/4849 [==>...........................] - ETA: 5:42 - loss: 0.6845 - acc: 0.5672
 704/4849 [===>..........................] - ETA: 5:37 - loss: 0.6827 - acc: 0.5682
 768/4849 [===>..........................] - ETA: 5:33 - loss: 0.6824 - acc: 0.5664
 832/4849 [====>.........................] - ETA: 5:29 - loss: 0.6841 - acc: 0.5601
 896/4849 [====>.........................] - ETA: 5:26 - loss: 0.6834 - acc: 0.5614
 960/4849 [====>.........................] - ETA: 5:21 - loss: 0.6822 - acc: 0.5646
1024/4849 [=====>........................] - ETA: 5:14 - loss: 0.6800 - acc: 0.5684
1088/4849 [=====>........................] - ETA: 5:10 - loss: 0.6775 - acc: 0.5735
1152/4849 [======>.......................] - ETA: 5:04 - loss: 0.6777 - acc: 0.5755
1216/4849 [======>.......................] - ETA: 4:56 - loss: 0.6783 - acc: 0.5748
1280/4849 [======>.......................] - ETA: 4:52 - loss: 0.6777 - acc: 0.5789
1344/4849 [=======>......................] - ETA: 4:47 - loss: 0.6775 - acc: 0.5811
1408/4849 [=======>......................] - ETA: 4:41 - loss: 0.6780 - acc: 0.5810
1472/4849 [========>.....................] - ETA: 4:35 - loss: 0.6772 - acc: 0.5842
1536/4849 [========>.....................] - ETA: 4:31 - loss: 0.6775 - acc: 0.5840
1600/4849 [========>.....................] - ETA: 4:25 - loss: 0.6779 - acc: 0.5837
1664/4849 [=========>....................] - ETA: 4:20 - loss: 0.6780 - acc: 0.5817
1728/4849 [=========>....................] - ETA: 4:15 - loss: 0.6757 - acc: 0.5833
1792/4849 [==========>...................] - ETA: 4:09 - loss: 0.6767 - acc: 0.5809
1856/4849 [==========>...................] - ETA: 4:03 - loss: 0.6791 - acc: 0.5770
1920/4849 [==========>...................] - ETA: 3:58 - loss: 0.6791 - acc: 0.5750
1984/4849 [===========>..................] - ETA: 3:52 - loss: 0.6793 - acc: 0.5751
2048/4849 [===========>..................] - ETA: 3:48 - loss: 0.6806 - acc: 0.5718
2112/4849 [============>.................] - ETA: 3:43 - loss: 0.6797 - acc: 0.5743
2176/4849 [============>.................] - ETA: 3:38 - loss: 0.6788 - acc: 0.5758
2240/4849 [============>.................] - ETA: 3:32 - loss: 0.6789 - acc: 0.5759
2304/4849 [=============>................] - ETA: 3:27 - loss: 0.6792 - acc: 0.5742
2368/4849 [=============>................] - ETA: 3:22 - loss: 0.6784 - acc: 0.5760
2432/4849 [==============>...............] - ETA: 3:16 - loss: 0.6785 - acc: 0.5748
2496/4849 [==============>...............] - ETA: 3:11 - loss: 0.6781 - acc: 0.5761
2560/4849 [==============>...............] - ETA: 3:05 - loss: 0.6782 - acc: 0.5762
2624/4849 [===============>..............] - ETA: 3:00 - loss: 0.6785 - acc: 0.5739
2688/4849 [===============>..............] - ETA: 2:55 - loss: 0.6789 - acc: 0.5718
2752/4849 [================>.............] - ETA: 2:50 - loss: 0.6785 - acc: 0.5734
2816/4849 [================>.............] - ETA: 2:44 - loss: 0.6791 - acc: 0.5717
2880/4849 [================>.............] - ETA: 2:40 - loss: 0.6792 - acc: 0.5729
2944/4849 [=================>............] - ETA: 2:35 - loss: 0.6794 - acc: 0.5724
3008/4849 [=================>............] - ETA: 2:29 - loss: 0.6803 - acc: 0.5708
3072/4849 [==================>...........] - ETA: 2:24 - loss: 0.6802 - acc: 0.5690
3136/4849 [==================>...........] - ETA: 2:19 - loss: 0.6801 - acc: 0.5682
3200/4849 [==================>...........] - ETA: 2:14 - loss: 0.6797 - acc: 0.5687
3264/4849 [===================>..........] - ETA: 2:09 - loss: 0.6805 - acc: 0.5665
3328/4849 [===================>..........] - ETA: 2:03 - loss: 0.6805 - acc: 0.5664
3392/4849 [===================>..........] - ETA: 1:58 - loss: 0.6801 - acc: 0.5669
3456/4849 [====================>.........] - ETA: 1:53 - loss: 0.6795 - acc: 0.5683
3520/4849 [====================>.........] - ETA: 1:48 - loss: 0.6795 - acc: 0.5693
3584/4849 [=====================>........] - ETA: 1:43 - loss: 0.6791 - acc: 0.5717
3648/4849 [=====================>........] - ETA: 1:37 - loss: 0.6795 - acc: 0.5704
3712/4849 [=====================>........] - ETA: 1:32 - loss: 0.6797 - acc: 0.5690
3776/4849 [======================>.......] - ETA: 1:27 - loss: 0.6802 - acc: 0.5681
3840/4849 [======================>.......] - ETA: 1:22 - loss: 0.6800 - acc: 0.5677
3904/4849 [=======================>......] - ETA: 1:17 - loss: 0.6801 - acc: 0.5674
3968/4849 [=======================>......] - ETA: 1:12 - loss: 0.6801 - acc: 0.5683
4032/4849 [=======================>......] - ETA: 1:06 - loss: 0.6797 - acc: 0.5697
4096/4849 [========================>.....] - ETA: 1:01 - loss: 0.6794 - acc: 0.5703
4160/4849 [========================>.....] - ETA: 56s - loss: 0.6788 - acc: 0.5719 
4224/4849 [=========================>....] - ETA: 51s - loss: 0.6785 - acc: 0.5724
4288/4849 [=========================>....] - ETA: 45s - loss: 0.6788 - acc: 0.5721
4352/4849 [=========================>....] - ETA: 40s - loss: 0.6787 - acc: 0.5726
4416/4849 [==========================>...] - ETA: 35s - loss: 0.6780 - acc: 0.5740
4480/4849 [==========================>...] - ETA: 30s - loss: 0.6776 - acc: 0.5754
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6774 - acc: 0.5761
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6774 - acc: 0.5762
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6772 - acc: 0.5773
4736/4849 [============================>.] - ETA: 9s - loss: 0.6768 - acc: 0.5785 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6764 - acc: 0.5794
4849/4849 [==============================] - 409s 84ms/step - loss: 0.6755 - acc: 0.5805 - val_loss: 0.6878 - val_acc: 0.5677

Epoch 00010: val_acc did not improve from 0.58256
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f2b286bfed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f2b286bfed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f324223e390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f324223e390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b28569990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b28569990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b282f3ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b282f3ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b2826f210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b2826f210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b280c0550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b280c0550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b282d9790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b282d9790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29d00ebb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29d00ebb10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b282e2f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b282e2f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b0877f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b0877f5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b08644910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b08644910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b2826f750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b2826f750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b0879a5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b0879a5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b085f2410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b085f2410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b084a0310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b084a0310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b084af050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b084af050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b08559750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b08559750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b08468c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b08468c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b084a9350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b084a9350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b082edb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2b082edb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b08159510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b08159510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b084a9ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b084a9ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae8797810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae8797810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae86c5a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae86c5a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2ae857dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2ae857dfd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae84eb3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae84eb3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2ae86c5b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2ae86c5b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae843b890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae843b890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae8419110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae8419110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2ae829bad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2ae829bad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae83867d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae83867d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2ae84dd790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2ae84dd790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b08068950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b08068950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae82e2310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae82e2310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2ae07d7390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2ae07d7390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae07d3810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae07d3810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2ae82e2e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2ae82e2e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae06719d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae06719d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae0596b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae0596b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2ae0416810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2ae0416810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae0312c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae0312c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2ae056e850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2ae056e850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae044f110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae044f110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae021ec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae021ec90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2ae023ca50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2ae023ca50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae0462990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae0462990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2ae0312390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2ae0312390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae0462050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae0462050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae010bf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2ae010bf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2aa86723d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2aa86723d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae00baf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae00baf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2aa8679cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2aa8679cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2aa84eadd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2aa84eadd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2aa86f2cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2aa86f2cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2aa83c90d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2aa83c90d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae023f350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae023f350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2aa85d8b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2aa85d8b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2aa81b5350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2aa81b5350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2aa80949d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2aa80949d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2aa803b350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2aa803b350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae8365dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2ae8365dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2aa8423790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2aa8423790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a887f76d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2a887f76d0>>: AttributeError: module 'gast' has no attribute 'Str'
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 2:16
 128/1348 [=>............................] - ETA: 1:24
 192/1348 [===>..........................] - ETA: 1:03
 256/1348 [====>.........................] - ETA: 53s 
 320/1348 [======>.......................] - ETA: 46s
 384/1348 [=======>......................] - ETA: 41s
 448/1348 [========>.....................] - ETA: 36s
 512/1348 [==========>...................] - ETA: 33s
 576/1348 [===========>..................] - ETA: 30s
 640/1348 [=============>................] - ETA: 27s
 704/1348 [==============>...............] - ETA: 24s
 768/1348 [================>.............] - ETA: 21s
 832/1348 [=================>............] - ETA: 18s
 896/1348 [==================>...........] - ETA: 16s
 960/1348 [====================>.........] - ETA: 13s
1024/1348 [=====================>........] - ETA: 11s
1088/1348 [=======================>......] - ETA: 9s 
1152/1348 [========================>.....] - ETA: 6s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 46s 34ms/step
loss: 0.672279532302381
acc: 0.5949554896142433
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f27cc3ccb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f27cc3ccb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f27cc37c050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f27cc37c050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217f49dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f3217f49dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2960454910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2960454910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2960566e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2960566e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b2836e990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b2836e990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29604545d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29604545d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b28466d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b28466d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b2864b650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2b2864b650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27cc342690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27cc342690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b285ffa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2b285ffa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b28641c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2b28641c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27cc348250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27cc348250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27cc0dde50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27cc0dde50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27cc2f4890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27cc2f4890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27cc0a6750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27cc0a6750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27cc2e2cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27cc2e2cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27cc28d190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27cc28d190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28681be310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28681be310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27b051b5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27b051b5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27b035e850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27b035e850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f284872b690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f284872b690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27b03468d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27b03468d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27b02669d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27b02669d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27b0258710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27b0258710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27b0453150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27b0453150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27b05a6ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27b05a6ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27b014f310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27b014f310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f278870cd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f278870cd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27885e4910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27885e4910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27b01538d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27b01538d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f278870cbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f278870cbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27884dca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27884dca90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f278863bd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f278863bd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2788351990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2788351990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27883d8b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27883d8b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27b01cf0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27b01cf0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f278817dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f278817dc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27882d5790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27882d5790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f278811ead0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f278811ead0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27882812d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27882812d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27882d5a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27882d5a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2788090290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2788090290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f276853e210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f276853e210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27684e2b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27684e2b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2768442510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2768442510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2768586c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2768586c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27683ff0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27683ff0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f26c430e150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f26c430e150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f26c4375bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f26c4375bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26c4262510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26c4262510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f26c4364b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f26c4364b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26c42de450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26c42de450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f26c4108350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f26c4108350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f26c421f150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f26c421f150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26c403be50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26c403be50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f26c41080d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f26c41080d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f266c6f7e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f266c6f7e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f266c4f40d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f266c4f40d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f266c3a5990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f266c3a5990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f266c2953d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f266c2953d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f266c4f4b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f266c4f4b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f266c6cb510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f266c6cb510>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 41:07 - loss: 0.7617 - acc: 0.4375
 128/4849 [..............................] - ETA: 24:29 - loss: 0.7536 - acc: 0.4766
 192/4849 [>.............................] - ETA: 18:48 - loss: 0.7420 - acc: 0.4688
 256/4849 [>.............................] - ETA: 16:01 - loss: 0.7480 - acc: 0.4727
 320/4849 [>.............................] - ETA: 14:10 - loss: 0.7413 - acc: 0.4906
 384/4849 [=>............................] - ETA: 12:58 - loss: 0.7400 - acc: 0.4870
 448/4849 [=>............................] - ETA: 12:07 - loss: 0.7435 - acc: 0.4688
 512/4849 [==>...........................] - ETA: 11:22 - loss: 0.7465 - acc: 0.4648
 576/4849 [==>...........................] - ETA: 10:45 - loss: 0.7474 - acc: 0.4722
 640/4849 [==>...........................] - ETA: 10:14 - loss: 0.7459 - acc: 0.4781
 704/4849 [===>..........................] - ETA: 9:49 - loss: 0.7460 - acc: 0.4801 
 768/4849 [===>..........................] - ETA: 9:26 - loss: 0.7443 - acc: 0.4818
 832/4849 [====>.........................] - ETA: 9:07 - loss: 0.7410 - acc: 0.4808
 896/4849 [====>.........................] - ETA: 8:51 - loss: 0.7392 - acc: 0.4866
 960/4849 [====>.........................] - ETA: 8:35 - loss: 0.7377 - acc: 0.4906
1024/4849 [=====>........................] - ETA: 8:20 - loss: 0.7384 - acc: 0.4883
1088/4849 [=====>........................] - ETA: 8:05 - loss: 0.7348 - acc: 0.4954
1152/4849 [======>.......................] - ETA: 7:52 - loss: 0.7324 - acc: 0.4965
1216/4849 [======>.......................] - ETA: 7:40 - loss: 0.7318 - acc: 0.5008
1280/4849 [======>.......................] - ETA: 7:28 - loss: 0.7326 - acc: 0.4992
1344/4849 [=======>......................] - ETA: 7:17 - loss: 0.7333 - acc: 0.4993
1408/4849 [=======>......................] - ETA: 7:07 - loss: 0.7350 - acc: 0.4979
1472/4849 [========>.....................] - ETA: 6:56 - loss: 0.7326 - acc: 0.5020
1536/4849 [========>.....................] - ETA: 6:44 - loss: 0.7320 - acc: 0.5020
1600/4849 [========>.....................] - ETA: 6:34 - loss: 0.7321 - acc: 0.5019
1664/4849 [=========>....................] - ETA: 6:25 - loss: 0.7324 - acc: 0.5018
1728/4849 [=========>....................] - ETA: 6:15 - loss: 0.7305 - acc: 0.5029
1792/4849 [==========>...................] - ETA: 6:07 - loss: 0.7297 - acc: 0.5045
1856/4849 [==========>...................] - ETA: 5:57 - loss: 0.7284 - acc: 0.5048
1920/4849 [==========>...................] - ETA: 5:48 - loss: 0.7271 - acc: 0.5052
1984/4849 [===========>..................] - ETA: 5:39 - loss: 0.7256 - acc: 0.5091
2048/4849 [===========>..................] - ETA: 5:30 - loss: 0.7254 - acc: 0.5098
2112/4849 [============>.................] - ETA: 5:22 - loss: 0.7252 - acc: 0.5099
2176/4849 [============>.................] - ETA: 5:14 - loss: 0.7243 - acc: 0.5101
2240/4849 [============>.................] - ETA: 5:06 - loss: 0.7254 - acc: 0.5089
2304/4849 [=============>................] - ETA: 4:57 - loss: 0.7239 - acc: 0.5122
2368/4849 [=============>................] - ETA: 4:48 - loss: 0.7241 - acc: 0.5127
2432/4849 [==============>...............] - ETA: 4:40 - loss: 0.7251 - acc: 0.5103
2496/4849 [==============>...............] - ETA: 4:32 - loss: 0.7238 - acc: 0.5120
2560/4849 [==============>...............] - ETA: 4:24 - loss: 0.7244 - acc: 0.5121
2624/4849 [===============>..............] - ETA: 4:16 - loss: 0.7239 - acc: 0.5126
2688/4849 [===============>..............] - ETA: 4:08 - loss: 0.7225 - acc: 0.5138
2752/4849 [================>.............] - ETA: 4:00 - loss: 0.7229 - acc: 0.5131
2816/4849 [================>.............] - ETA: 3:52 - loss: 0.7225 - acc: 0.5138
2880/4849 [================>.............] - ETA: 3:44 - loss: 0.7215 - acc: 0.5149
2944/4849 [=================>............] - ETA: 3:37 - loss: 0.7211 - acc: 0.5160
3008/4849 [=================>............] - ETA: 3:29 - loss: 0.7210 - acc: 0.5176
3072/4849 [==================>...........] - ETA: 3:21 - loss: 0.7185 - acc: 0.5218
3136/4849 [==================>...........] - ETA: 3:14 - loss: 0.7189 - acc: 0.5201
3200/4849 [==================>...........] - ETA: 3:06 - loss: 0.7188 - acc: 0.5212
3264/4849 [===================>..........] - ETA: 2:59 - loss: 0.7195 - acc: 0.5211
3328/4849 [===================>..........] - ETA: 2:52 - loss: 0.7203 - acc: 0.5213
3392/4849 [===================>..........] - ETA: 2:44 - loss: 0.7212 - acc: 0.5209
3456/4849 [====================>.........] - ETA: 2:37 - loss: 0.7203 - acc: 0.5229
3520/4849 [====================>.........] - ETA: 2:29 - loss: 0.7199 - acc: 0.5224
3584/4849 [=====================>........] - ETA: 2:22 - loss: 0.7209 - acc: 0.5218
3648/4849 [=====================>........] - ETA: 2:15 - loss: 0.7209 - acc: 0.5206
3712/4849 [=====================>........] - ETA: 2:07 - loss: 0.7206 - acc: 0.5216
3776/4849 [======================>.......] - ETA: 2:00 - loss: 0.7206 - acc: 0.5217
3840/4849 [======================>.......] - ETA: 1:52 - loss: 0.7205 - acc: 0.5211
3904/4849 [=======================>......] - ETA: 1:45 - loss: 0.7216 - acc: 0.5195
3968/4849 [=======================>......] - ETA: 1:38 - loss: 0.7216 - acc: 0.5189
4032/4849 [=======================>......] - ETA: 1:31 - loss: 0.7212 - acc: 0.5188
4096/4849 [========================>.....] - ETA: 1:23 - loss: 0.7196 - acc: 0.5210
4160/4849 [========================>.....] - ETA: 1:16 - loss: 0.7194 - acc: 0.5204
4224/4849 [=========================>....] - ETA: 1:09 - loss: 0.7191 - acc: 0.5208
4288/4849 [=========================>....] - ETA: 1:02 - loss: 0.7188 - acc: 0.5205
4352/4849 [=========================>....] - ETA: 55s - loss: 0.7196 - acc: 0.5188 
4416/4849 [==========================>...] - ETA: 47s - loss: 0.7193 - acc: 0.5186
4480/4849 [==========================>...] - ETA: 40s - loss: 0.7194 - acc: 0.5185
4544/4849 [===========================>..] - ETA: 33s - loss: 0.7190 - acc: 0.5178
4608/4849 [===========================>..] - ETA: 26s - loss: 0.7196 - acc: 0.5176
4672/4849 [===========================>..] - ETA: 19s - loss: 0.7192 - acc: 0.5176
4736/4849 [============================>.] - ETA: 12s - loss: 0.7188 - acc: 0.5179
4800/4849 [============================>.] - ETA: 5s - loss: 0.7187 - acc: 0.5181 
4849/4849 [==============================] - 552s 114ms/step - loss: 0.7182 - acc: 0.5187 - val_loss: 0.6807 - val_acc: 0.5529

Epoch 00001: val_acc improved from -inf to 0.55288, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window04/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 8:01 - loss: 0.7037 - acc: 0.5781
 128/4849 [..............................] - ETA: 7:54 - loss: 0.7040 - acc: 0.5781
 192/4849 [>.............................] - ETA: 7:44 - loss: 0.7126 - acc: 0.5208
 256/4849 [>.............................] - ETA: 7:45 - loss: 0.7110 - acc: 0.5273
 320/4849 [>.............................] - ETA: 7:43 - loss: 0.7095 - acc: 0.5281
 384/4849 [=>............................] - ETA: 7:39 - loss: 0.7033 - acc: 0.5417
 448/4849 [=>............................] - ETA: 7:32 - loss: 0.7091 - acc: 0.5290
 512/4849 [==>...........................] - ETA: 7:27 - loss: 0.7108 - acc: 0.5273
 576/4849 [==>...........................] - ETA: 7:18 - loss: 0.7088 - acc: 0.5295
 640/4849 [==>...........................] - ETA: 7:14 - loss: 0.7053 - acc: 0.5312
 704/4849 [===>..........................] - ETA: 7:06 - loss: 0.7063 - acc: 0.5355
 768/4849 [===>..........................] - ETA: 6:58 - loss: 0.7030 - acc: 0.5417
 832/4849 [====>.........................] - ETA: 6:50 - loss: 0.7029 - acc: 0.5397
 896/4849 [====>.........................] - ETA: 6:43 - loss: 0.6987 - acc: 0.5502
 960/4849 [====>.........................] - ETA: 6:36 - loss: 0.6992 - acc: 0.5479
1024/4849 [=====>........................] - ETA: 6:28 - loss: 0.7008 - acc: 0.5449
1088/4849 [=====>........................] - ETA: 6:21 - loss: 0.7027 - acc: 0.5423
1152/4849 [======>.......................] - ETA: 6:14 - loss: 0.7061 - acc: 0.5399
1216/4849 [======>.......................] - ETA: 6:06 - loss: 0.7039 - acc: 0.5403
1280/4849 [======>.......................] - ETA: 6:00 - loss: 0.7048 - acc: 0.5391
1344/4849 [=======>......................] - ETA: 5:53 - loss: 0.7053 - acc: 0.5350
1408/4849 [=======>......................] - ETA: 5:47 - loss: 0.7037 - acc: 0.5362
1472/4849 [========>.....................] - ETA: 5:40 - loss: 0.7018 - acc: 0.5387
1536/4849 [========>.....................] - ETA: 5:33 - loss: 0.7016 - acc: 0.5404
1600/4849 [========>.....................] - ETA: 5:26 - loss: 0.7021 - acc: 0.5369
1664/4849 [=========>....................] - ETA: 5:20 - loss: 0.7025 - acc: 0.5319
1728/4849 [=========>....................] - ETA: 5:13 - loss: 0.7016 - acc: 0.5312
1792/4849 [==========>...................] - ETA: 5:06 - loss: 0.7016 - acc: 0.5329
1856/4849 [==========>...................] - ETA: 5:00 - loss: 0.6998 - acc: 0.5366
1920/4849 [==========>...................] - ETA: 4:53 - loss: 0.6998 - acc: 0.5375
1984/4849 [===========>..................] - ETA: 4:46 - loss: 0.7005 - acc: 0.5363
2048/4849 [===========>..................] - ETA: 4:39 - loss: 0.7008 - acc: 0.5342
2112/4849 [============>.................] - ETA: 4:33 - loss: 0.6991 - acc: 0.5374
2176/4849 [============>.................] - ETA: 4:26 - loss: 0.6992 - acc: 0.5358
2240/4849 [============>.................] - ETA: 4:19 - loss: 0.6992 - acc: 0.5362
2304/4849 [=============>................] - ETA: 4:12 - loss: 0.6987 - acc: 0.5347
2368/4849 [=============>................] - ETA: 4:06 - loss: 0.6979 - acc: 0.5367
2432/4849 [==============>...............] - ETA: 3:59 - loss: 0.6980 - acc: 0.5370
2496/4849 [==============>...............] - ETA: 3:52 - loss: 0.6981 - acc: 0.5369
2560/4849 [==============>...............] - ETA: 3:46 - loss: 0.6983 - acc: 0.5363
2624/4849 [===============>..............] - ETA: 3:39 - loss: 0.6981 - acc: 0.5358
2688/4849 [===============>..............] - ETA: 3:33 - loss: 0.6980 - acc: 0.5372
2752/4849 [================>.............] - ETA: 3:27 - loss: 0.6983 - acc: 0.5363
2816/4849 [================>.............] - ETA: 3:20 - loss: 0.6986 - acc: 0.5344
2880/4849 [================>.............] - ETA: 3:14 - loss: 0.6998 - acc: 0.5299
2944/4849 [=================>............] - ETA: 3:07 - loss: 0.6996 - acc: 0.5299
3008/4849 [=================>............] - ETA: 3:01 - loss: 0.6993 - acc: 0.5303
3072/4849 [==================>...........] - ETA: 2:55 - loss: 0.6993 - acc: 0.5296
3136/4849 [==================>...........] - ETA: 2:48 - loss: 0.7000 - acc: 0.5274
3200/4849 [==================>...........] - ETA: 2:42 - loss: 0.7002 - acc: 0.5272
3264/4849 [===================>..........] - ETA: 2:36 - loss: 0.7002 - acc: 0.5267
3328/4849 [===================>..........] - ETA: 2:29 - loss: 0.7001 - acc: 0.5258
3392/4849 [===================>..........] - ETA: 2:23 - loss: 0.7005 - acc: 0.5256
3456/4849 [====================>.........] - ETA: 2:16 - loss: 0.7006 - acc: 0.5249
3520/4849 [====================>.........] - ETA: 2:10 - loss: 0.7008 - acc: 0.5256
3584/4849 [=====================>........] - ETA: 2:04 - loss: 0.7005 - acc: 0.5271
3648/4849 [=====================>........] - ETA: 1:57 - loss: 0.7005 - acc: 0.5260
3712/4849 [=====================>........] - ETA: 1:51 - loss: 0.7006 - acc: 0.5267
3776/4849 [======================>.......] - ETA: 1:45 - loss: 0.7004 - acc: 0.5262
3840/4849 [======================>.......] - ETA: 1:38 - loss: 0.7006 - acc: 0.5242
3904/4849 [=======================>......] - ETA: 1:32 - loss: 0.7003 - acc: 0.5251
3968/4849 [=======================>......] - ETA: 1:26 - loss: 0.7003 - acc: 0.5252
4032/4849 [=======================>......] - ETA: 1:19 - loss: 0.7009 - acc: 0.5226
4096/4849 [========================>.....] - ETA: 1:13 - loss: 0.7007 - acc: 0.5225
4160/4849 [========================>.....] - ETA: 1:07 - loss: 0.7005 - acc: 0.5231
4224/4849 [=========================>....] - ETA: 1:01 - loss: 0.7005 - acc: 0.5232
4288/4849 [=========================>....] - ETA: 54s - loss: 0.7003 - acc: 0.5238 
4352/4849 [=========================>....] - ETA: 48s - loss: 0.7004 - acc: 0.5234
4416/4849 [==========================>...] - ETA: 42s - loss: 0.7006 - acc: 0.5222
4480/4849 [==========================>...] - ETA: 35s - loss: 0.7004 - acc: 0.5225
4544/4849 [===========================>..] - ETA: 29s - loss: 0.7005 - acc: 0.5218
4608/4849 [===========================>..] - ETA: 23s - loss: 0.7003 - acc: 0.5221
4672/4849 [===========================>..] - ETA: 17s - loss: 0.7003 - acc: 0.5225
4736/4849 [============================>.] - ETA: 11s - loss: 0.7005 - acc: 0.5222
4800/4849 [============================>.] - ETA: 4s - loss: 0.7004 - acc: 0.5229 
4849/4849 [==============================] - 491s 101ms/step - loss: 0.7000 - acc: 0.5232 - val_loss: 0.6791 - val_acc: 0.5714

Epoch 00002: val_acc improved from 0.55288 to 0.57143, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window04/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 3/10

  64/4849 [..............................] - ETA: 7:40 - loss: 0.7440 - acc: 0.4375
 128/4849 [..............................] - ETA: 7:28 - loss: 0.7090 - acc: 0.4844
 192/4849 [>.............................] - ETA: 7:29 - loss: 0.7019 - acc: 0.5260
 256/4849 [>.............................] - ETA: 7:19 - loss: 0.6954 - acc: 0.5352
 320/4849 [>.............................] - ETA: 7:11 - loss: 0.6974 - acc: 0.5188
 384/4849 [=>............................] - ETA: 7:04 - loss: 0.6938 - acc: 0.5286
 448/4849 [=>............................] - ETA: 6:55 - loss: 0.6947 - acc: 0.5312
 512/4849 [==>...........................] - ETA: 6:50 - loss: 0.6950 - acc: 0.5371
 576/4849 [==>...........................] - ETA: 6:46 - loss: 0.6985 - acc: 0.5312
 640/4849 [==>...........................] - ETA: 6:42 - loss: 0.7027 - acc: 0.5234
 704/4849 [===>..........................] - ETA: 6:36 - loss: 0.7024 - acc: 0.5312
 768/4849 [===>..........................] - ETA: 6:32 - loss: 0.7007 - acc: 0.5339
 832/4849 [====>.........................] - ETA: 6:27 - loss: 0.7005 - acc: 0.5361
 896/4849 [====>.........................] - ETA: 6:22 - loss: 0.6983 - acc: 0.5413
 960/4849 [====>.........................] - ETA: 6:15 - loss: 0.6968 - acc: 0.5427
1024/4849 [=====>........................] - ETA: 6:09 - loss: 0.6970 - acc: 0.5410
1088/4849 [=====>........................] - ETA: 6:03 - loss: 0.6973 - acc: 0.5386
1152/4849 [======>.......................] - ETA: 5:57 - loss: 0.6966 - acc: 0.5399
1216/4849 [======>.......................] - ETA: 5:51 - loss: 0.6965 - acc: 0.5419
1280/4849 [======>.......................] - ETA: 5:44 - loss: 0.6961 - acc: 0.5422
1344/4849 [=======>......................] - ETA: 5:38 - loss: 0.6941 - acc: 0.5439
1408/4849 [=======>......................] - ETA: 5:31 - loss: 0.6948 - acc: 0.5419
1472/4849 [========>.....................] - ETA: 5:25 - loss: 0.6948 - acc: 0.5394
1536/4849 [========>.....................] - ETA: 5:19 - loss: 0.6948 - acc: 0.5417
1600/4849 [========>.....................] - ETA: 5:13 - loss: 0.6947 - acc: 0.5413
1664/4849 [=========>....................] - ETA: 5:07 - loss: 0.6945 - acc: 0.5427
1728/4849 [=========>....................] - ETA: 5:00 - loss: 0.6942 - acc: 0.5440
1792/4849 [==========>...................] - ETA: 4:54 - loss: 0.6940 - acc: 0.5435
1856/4849 [==========>...................] - ETA: 4:48 - loss: 0.6942 - acc: 0.5426
1920/4849 [==========>...................] - ETA: 4:41 - loss: 0.6934 - acc: 0.5432
1984/4849 [===========>..................] - ETA: 4:35 - loss: 0.6930 - acc: 0.5459
2048/4849 [===========>..................] - ETA: 4:28 - loss: 0.6938 - acc: 0.5474
2112/4849 [============>.................] - ETA: 4:22 - loss: 0.6942 - acc: 0.5459
2176/4849 [============>.................] - ETA: 4:16 - loss: 0.6930 - acc: 0.5469
2240/4849 [============>.................] - ETA: 4:10 - loss: 0.6932 - acc: 0.5460
2304/4849 [=============>................] - ETA: 4:04 - loss: 0.6935 - acc: 0.5438
2368/4849 [=============>................] - ETA: 3:58 - loss: 0.6928 - acc: 0.5439
2432/4849 [==============>...............] - ETA: 3:52 - loss: 0.6927 - acc: 0.5444
2496/4849 [==============>...............] - ETA: 3:46 - loss: 0.6925 - acc: 0.5445
2560/4849 [==============>...............] - ETA: 3:40 - loss: 0.6926 - acc: 0.5441
2624/4849 [===============>..............] - ETA: 3:34 - loss: 0.6922 - acc: 0.5431
2688/4849 [===============>..............] - ETA: 3:28 - loss: 0.6925 - acc: 0.5424
2752/4849 [================>.............] - ETA: 3:22 - loss: 0.6912 - acc: 0.5465
2816/4849 [================>.............] - ETA: 3:16 - loss: 0.6906 - acc: 0.5476
2880/4849 [================>.............] - ETA: 3:09 - loss: 0.6918 - acc: 0.5469
2944/4849 [=================>............] - ETA: 3:03 - loss: 0.6918 - acc: 0.5465
3008/4849 [=================>............] - ETA: 2:57 - loss: 0.6916 - acc: 0.5455
3072/4849 [==================>...........] - ETA: 2:51 - loss: 0.6912 - acc: 0.5462
3136/4849 [==================>...........] - ETA: 2:44 - loss: 0.6920 - acc: 0.5443
3200/4849 [==================>...........] - ETA: 2:38 - loss: 0.6919 - acc: 0.5437
3264/4849 [===================>..........] - ETA: 2:32 - loss: 0.6911 - acc: 0.5456
3328/4849 [===================>..........] - ETA: 2:26 - loss: 0.6906 - acc: 0.5466
3392/4849 [===================>..........] - ETA: 2:20 - loss: 0.6909 - acc: 0.5457
3456/4849 [====================>.........] - ETA: 2:14 - loss: 0.6910 - acc: 0.5454
3520/4849 [====================>.........] - ETA: 2:07 - loss: 0.6904 - acc: 0.5480
3584/4849 [=====================>........] - ETA: 2:01 - loss: 0.6902 - acc: 0.5480
3648/4849 [=====================>........] - ETA: 1:55 - loss: 0.6907 - acc: 0.5471
3712/4849 [=====================>........] - ETA: 1:49 - loss: 0.6902 - acc: 0.5488
3776/4849 [======================>.......] - ETA: 1:43 - loss: 0.6910 - acc: 0.5477
3840/4849 [======================>.......] - ETA: 1:36 - loss: 0.6906 - acc: 0.5487
3904/4849 [=======================>......] - ETA: 1:30 - loss: 0.6907 - acc: 0.5479
3968/4849 [=======================>......] - ETA: 1:24 - loss: 0.6910 - acc: 0.5471
4032/4849 [=======================>......] - ETA: 1:18 - loss: 0.6909 - acc: 0.5456
4096/4849 [========================>.....] - ETA: 1:12 - loss: 0.6918 - acc: 0.5442
4160/4849 [========================>.....] - ETA: 1:06 - loss: 0.6927 - acc: 0.5425
4224/4849 [=========================>....] - ETA: 1:00 - loss: 0.6927 - acc: 0.5419
4288/4849 [=========================>....] - ETA: 54s - loss: 0.6923 - acc: 0.5417 
4352/4849 [=========================>....] - ETA: 47s - loss: 0.6922 - acc: 0.5427
4416/4849 [==========================>...] - ETA: 41s - loss: 0.6922 - acc: 0.5421
4480/4849 [==========================>...] - ETA: 35s - loss: 0.6919 - acc: 0.5429
4544/4849 [===========================>..] - ETA: 29s - loss: 0.6915 - acc: 0.5434
4608/4849 [===========================>..] - ETA: 23s - loss: 0.6918 - acc: 0.5421
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6918 - acc: 0.5420
4736/4849 [============================>.] - ETA: 10s - loss: 0.6913 - acc: 0.5433
4800/4849 [============================>.] - ETA: 4s - loss: 0.6913 - acc: 0.5429 
4849/4849 [==============================] - 486s 100ms/step - loss: 0.6913 - acc: 0.5430 - val_loss: 0.6863 - val_acc: 0.5399

Epoch 00003: val_acc did not improve from 0.57143
Epoch 4/10

  64/4849 [..............................] - ETA: 8:29 - loss: 0.7201 - acc: 0.5000
 128/4849 [..............................] - ETA: 8:05 - loss: 0.7198 - acc: 0.4766
 192/4849 [>.............................] - ETA: 7:58 - loss: 0.6982 - acc: 0.5260
 256/4849 [>.............................] - ETA: 7:47 - loss: 0.6977 - acc: 0.5195
 320/4849 [>.............................] - ETA: 7:40 - loss: 0.7001 - acc: 0.5031
 384/4849 [=>............................] - ETA: 7:35 - loss: 0.6975 - acc: 0.5130
 448/4849 [=>............................] - ETA: 7:23 - loss: 0.6935 - acc: 0.5246
 512/4849 [==>...........................] - ETA: 7:14 - loss: 0.6904 - acc: 0.5430
 576/4849 [==>...........................] - ETA: 7:08 - loss: 0.6906 - acc: 0.5382
 640/4849 [==>...........................] - ETA: 6:59 - loss: 0.6889 - acc: 0.5422
 704/4849 [===>..........................] - ETA: 6:51 - loss: 0.6894 - acc: 0.5384
 768/4849 [===>..........................] - ETA: 6:42 - loss: 0.6903 - acc: 0.5326
 832/4849 [====>.........................] - ETA: 6:35 - loss: 0.6867 - acc: 0.5469
 896/4849 [====>.........................] - ETA: 6:28 - loss: 0.6863 - acc: 0.5480
 960/4849 [====>.........................] - ETA: 6:22 - loss: 0.6865 - acc: 0.5500
1024/4849 [=====>........................] - ETA: 6:16 - loss: 0.6879 - acc: 0.5469
1088/4849 [=====>........................] - ETA: 6:09 - loss: 0.6853 - acc: 0.5524
1152/4849 [======>.......................] - ETA: 6:03 - loss: 0.6851 - acc: 0.5547
1216/4849 [======>.......................] - ETA: 5:55 - loss: 0.6841 - acc: 0.5592
1280/4849 [======>.......................] - ETA: 5:49 - loss: 0.6854 - acc: 0.5539
1344/4849 [=======>......................] - ETA: 5:44 - loss: 0.6857 - acc: 0.5551
1408/4849 [=======>......................] - ETA: 5:37 - loss: 0.6869 - acc: 0.5568
1472/4849 [========>.....................] - ETA: 5:32 - loss: 0.6877 - acc: 0.5557
1536/4849 [========>.....................] - ETA: 5:25 - loss: 0.6877 - acc: 0.5534
1600/4849 [========>.....................] - ETA: 5:18 - loss: 0.6871 - acc: 0.5544
1664/4849 [=========>....................] - ETA: 5:12 - loss: 0.6862 - acc: 0.5571
1728/4849 [=========>....................] - ETA: 5:06 - loss: 0.6859 - acc: 0.5573
1792/4849 [==========>...................] - ETA: 5:01 - loss: 0.6860 - acc: 0.5569
1856/4849 [==========>...................] - ETA: 4:54 - loss: 0.6859 - acc: 0.5571
1920/4849 [==========>...................] - ETA: 4:48 - loss: 0.6855 - acc: 0.5563
1984/4849 [===========>..................] - ETA: 4:41 - loss: 0.6869 - acc: 0.5534
2048/4849 [===========>..................] - ETA: 4:34 - loss: 0.6875 - acc: 0.5542
2112/4849 [============>.................] - ETA: 4:28 - loss: 0.6863 - acc: 0.5554
2176/4849 [============>.................] - ETA: 4:22 - loss: 0.6866 - acc: 0.5542
2240/4849 [============>.................] - ETA: 4:16 - loss: 0.6859 - acc: 0.5531
2304/4849 [=============>................] - ETA: 4:10 - loss: 0.6858 - acc: 0.5534
2368/4849 [=============>................] - ETA: 4:03 - loss: 0.6863 - acc: 0.5515
2432/4849 [==============>...............] - ETA: 3:57 - loss: 0.6854 - acc: 0.5539
2496/4849 [==============>...............] - ETA: 3:50 - loss: 0.6857 - acc: 0.5525
2560/4849 [==============>...............] - ETA: 3:43 - loss: 0.6853 - acc: 0.5531
2624/4849 [===============>..............] - ETA: 3:37 - loss: 0.6867 - acc: 0.5503
2688/4849 [===============>..............] - ETA: 3:31 - loss: 0.6866 - acc: 0.5506
2752/4849 [================>.............] - ETA: 3:25 - loss: 0.6869 - acc: 0.5487
2816/4849 [================>.............] - ETA: 3:18 - loss: 0.6864 - acc: 0.5515
2880/4849 [================>.............] - ETA: 3:12 - loss: 0.6865 - acc: 0.5503
2944/4849 [=================>............] - ETA: 3:06 - loss: 0.6857 - acc: 0.5520
3008/4849 [=================>............] - ETA: 2:59 - loss: 0.6858 - acc: 0.5512
3072/4849 [==================>...........] - ETA: 2:53 - loss: 0.6864 - acc: 0.5492
3136/4849 [==================>...........] - ETA: 2:47 - loss: 0.6872 - acc: 0.5482
3200/4849 [==================>...........] - ETA: 2:41 - loss: 0.6875 - acc: 0.5484
3264/4849 [===================>..........] - ETA: 2:35 - loss: 0.6874 - acc: 0.5484
3328/4849 [===================>..........] - ETA: 2:28 - loss: 0.6876 - acc: 0.5490
3392/4849 [===================>..........] - ETA: 2:22 - loss: 0.6878 - acc: 0.5469
3456/4849 [====================>.........] - ETA: 2:15 - loss: 0.6877 - acc: 0.5463
3520/4849 [====================>.........] - ETA: 2:09 - loss: 0.6872 - acc: 0.5463
3584/4849 [=====================>........] - ETA: 2:03 - loss: 0.6867 - acc: 0.5472
3648/4849 [=====================>........] - ETA: 1:57 - loss: 0.6862 - acc: 0.5485
3712/4849 [=====================>........] - ETA: 1:50 - loss: 0.6862 - acc: 0.5498
3776/4849 [======================>.......] - ETA: 1:44 - loss: 0.6857 - acc: 0.5516
3840/4849 [======================>.......] - ETA: 1:38 - loss: 0.6861 - acc: 0.5521
3904/4849 [=======================>......] - ETA: 1:32 - loss: 0.6857 - acc: 0.5533
3968/4849 [=======================>......] - ETA: 1:25 - loss: 0.6862 - acc: 0.5517
4032/4849 [=======================>......] - ETA: 1:19 - loss: 0.6863 - acc: 0.5516
4096/4849 [========================>.....] - ETA: 1:13 - loss: 0.6868 - acc: 0.5496
4160/4849 [========================>.....] - ETA: 1:07 - loss: 0.6867 - acc: 0.5502
4224/4849 [=========================>....] - ETA: 1:00 - loss: 0.6857 - acc: 0.5523
4288/4849 [=========================>....] - ETA: 54s - loss: 0.6856 - acc: 0.5536 
4352/4849 [=========================>....] - ETA: 48s - loss: 0.6854 - acc: 0.5540
4416/4849 [==========================>...] - ETA: 42s - loss: 0.6855 - acc: 0.5546
4480/4849 [==========================>...] - ETA: 35s - loss: 0.6857 - acc: 0.5558
4544/4849 [===========================>..] - ETA: 29s - loss: 0.6858 - acc: 0.5559
4608/4849 [===========================>..] - ETA: 23s - loss: 0.6857 - acc: 0.5566
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6856 - acc: 0.5571
4736/4849 [============================>.] - ETA: 10s - loss: 0.6848 - acc: 0.5598
4800/4849 [============================>.] - ETA: 4s - loss: 0.6845 - acc: 0.5602 
4849/4849 [==============================] - 488s 101ms/step - loss: 0.6841 - acc: 0.5611 - val_loss: 0.6744 - val_acc: 0.5974

Epoch 00004: val_acc improved from 0.57143 to 0.59740, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window04/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 5/10

  64/4849 [..............................] - ETA: 6:55 - loss: 0.7188 - acc: 0.4531
 128/4849 [..............................] - ETA: 6:59 - loss: 0.6794 - acc: 0.5938
 192/4849 [>.............................] - ETA: 6:52 - loss: 0.6677 - acc: 0.6198
 256/4849 [>.............................] - ETA: 6:41 - loss: 0.6700 - acc: 0.6016
 320/4849 [>.............................] - ETA: 6:38 - loss: 0.6688 - acc: 0.6000
 384/4849 [=>............................] - ETA: 6:32 - loss: 0.6707 - acc: 0.6068
 448/4849 [=>............................] - ETA: 6:29 - loss: 0.6692 - acc: 0.6071
 512/4849 [==>...........................] - ETA: 6:24 - loss: 0.6762 - acc: 0.5938
 576/4849 [==>...........................] - ETA: 6:19 - loss: 0.6832 - acc: 0.5833
 640/4849 [==>...........................] - ETA: 6:13 - loss: 0.6819 - acc: 0.5859
 704/4849 [===>..........................] - ETA: 6:09 - loss: 0.6829 - acc: 0.5852
 768/4849 [===>..........................] - ETA: 6:05 - loss: 0.6852 - acc: 0.5768
 832/4849 [====>.........................] - ETA: 5:59 - loss: 0.6888 - acc: 0.5745
 896/4849 [====>.........................] - ETA: 5:55 - loss: 0.6906 - acc: 0.5714
 960/4849 [====>.........................] - ETA: 5:50 - loss: 0.6899 - acc: 0.5729
1024/4849 [=====>........................] - ETA: 5:44 - loss: 0.6865 - acc: 0.5801
1088/4849 [=====>........................] - ETA: 5:39 - loss: 0.6858 - acc: 0.5800
1152/4849 [======>.......................] - ETA: 5:33 - loss: 0.6878 - acc: 0.5764
1216/4849 [======>.......................] - ETA: 5:27 - loss: 0.6874 - acc: 0.5765
1280/4849 [======>.......................] - ETA: 5:22 - loss: 0.6895 - acc: 0.5719
1344/4849 [=======>......................] - ETA: 5:16 - loss: 0.6886 - acc: 0.5744
1408/4849 [=======>......................] - ETA: 5:10 - loss: 0.6873 - acc: 0.5774
1472/4849 [========>.....................] - ETA: 5:04 - loss: 0.6866 - acc: 0.5761
1536/4849 [========>.....................] - ETA: 4:57 - loss: 0.6873 - acc: 0.5749
1600/4849 [========>.....................] - ETA: 4:51 - loss: 0.6882 - acc: 0.5756
1664/4849 [=========>....................] - ETA: 4:45 - loss: 0.6870 - acc: 0.5787
1728/4849 [=========>....................] - ETA: 4:39 - loss: 0.6864 - acc: 0.5816
1792/4849 [==========>...................] - ETA: 4:34 - loss: 0.6861 - acc: 0.5820
1856/4849 [==========>...................] - ETA: 4:28 - loss: 0.6876 - acc: 0.5797
1920/4849 [==========>...................] - ETA: 4:22 - loss: 0.6875 - acc: 0.5786
1984/4849 [===========>..................] - ETA: 4:17 - loss: 0.6882 - acc: 0.5771
2048/4849 [===========>..................] - ETA: 4:11 - loss: 0.6885 - acc: 0.5757
2112/4849 [============>.................] - ETA: 4:05 - loss: 0.6886 - acc: 0.5734
2176/4849 [============>.................] - ETA: 3:59 - loss: 0.6888 - acc: 0.5731
2240/4849 [============>.................] - ETA: 3:53 - loss: 0.6879 - acc: 0.5728
2304/4849 [=============>................] - ETA: 3:48 - loss: 0.6876 - acc: 0.5734
2368/4849 [=============>................] - ETA: 3:42 - loss: 0.6868 - acc: 0.5752
2432/4849 [==============>...............] - ETA: 3:36 - loss: 0.6854 - acc: 0.5785
2496/4849 [==============>...............] - ETA: 3:30 - loss: 0.6840 - acc: 0.5801
2560/4849 [==============>...............] - ETA: 3:24 - loss: 0.6851 - acc: 0.5773
2624/4849 [===============>..............] - ETA: 3:18 - loss: 0.6842 - acc: 0.5793
2688/4849 [===============>..............] - ETA: 3:13 - loss: 0.6849 - acc: 0.5785
2752/4849 [================>.............] - ETA: 3:06 - loss: 0.6848 - acc: 0.5781
2816/4849 [================>.............] - ETA: 3:01 - loss: 0.6843 - acc: 0.5795
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6852 - acc: 0.5778
2944/4849 [=================>............] - ETA: 2:49 - loss: 0.6849 - acc: 0.5791
3008/4849 [=================>............] - ETA: 2:44 - loss: 0.6850 - acc: 0.5775
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6855 - acc: 0.5791
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6863 - acc: 0.5772
3200/4849 [==================>...........] - ETA: 2:27 - loss: 0.6863 - acc: 0.5769
3264/4849 [===================>..........] - ETA: 2:21 - loss: 0.6861 - acc: 0.5769
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6862 - acc: 0.5775
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6866 - acc: 0.5752
3456/4849 [====================>.........] - ETA: 2:04 - loss: 0.6864 - acc: 0.5758
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6868 - acc: 0.5744
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6867 - acc: 0.5753
3648/4849 [=====================>........] - ETA: 1:47 - loss: 0.6862 - acc: 0.5757
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6855 - acc: 0.5762
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6855 - acc: 0.5760
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6858 - acc: 0.5763
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6858 - acc: 0.5763
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6851 - acc: 0.5786
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6854 - acc: 0.5779
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6852 - acc: 0.5776
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6853 - acc: 0.5776
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6855 - acc: 0.5769 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6853 - acc: 0.5777
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6858 - acc: 0.5756
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6854 - acc: 0.5768
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6850 - acc: 0.5775
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6849 - acc: 0.5768
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6848 - acc: 0.5775
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6850 - acc: 0.5764
4736/4849 [============================>.] - ETA: 9s - loss: 0.6851 - acc: 0.5764 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6851 - acc: 0.5765
4849/4849 [==============================] - 442s 91ms/step - loss: 0.6848 - acc: 0.5776 - val_loss: 0.6734 - val_acc: 0.5993

Epoch 00005: val_acc improved from 0.59740 to 0.59926, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window04/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 6/10

  64/4849 [..............................] - ETA: 6:45 - loss: 0.6423 - acc: 0.6250
 128/4849 [..............................] - ETA: 6:36 - loss: 0.6409 - acc: 0.6328
 192/4849 [>.............................] - ETA: 6:26 - loss: 0.6587 - acc: 0.5938
 256/4849 [>.............................] - ETA: 6:31 - loss: 0.6506 - acc: 0.6250
 320/4849 [>.............................] - ETA: 6:18 - loss: 0.6504 - acc: 0.6250
 384/4849 [=>............................] - ETA: 6:15 - loss: 0.6505 - acc: 0.6224
 448/4849 [=>............................] - ETA: 6:07 - loss: 0.6537 - acc: 0.6183
 512/4849 [==>...........................] - ETA: 5:59 - loss: 0.6590 - acc: 0.5996
 576/4849 [==>...........................] - ETA: 5:53 - loss: 0.6632 - acc: 0.5938
 640/4849 [==>...........................] - ETA: 5:48 - loss: 0.6697 - acc: 0.5859
 704/4849 [===>..........................] - ETA: 5:44 - loss: 0.6729 - acc: 0.5810
 768/4849 [===>..........................] - ETA: 5:39 - loss: 0.6735 - acc: 0.5781
 832/4849 [====>.........................] - ETA: 5:33 - loss: 0.6748 - acc: 0.5793
 896/4849 [====>.........................] - ETA: 5:31 - loss: 0.6805 - acc: 0.5692
 960/4849 [====>.........................] - ETA: 5:25 - loss: 0.6791 - acc: 0.5750
1024/4849 [=====>........................] - ETA: 5:19 - loss: 0.6783 - acc: 0.5781
1088/4849 [=====>........................] - ETA: 5:14 - loss: 0.6754 - acc: 0.5818
1152/4849 [======>.......................] - ETA: 5:09 - loss: 0.6758 - acc: 0.5773
1216/4849 [======>.......................] - ETA: 5:05 - loss: 0.6760 - acc: 0.5732
1280/4849 [======>.......................] - ETA: 5:00 - loss: 0.6765 - acc: 0.5727
1344/4849 [=======>......................] - ETA: 4:55 - loss: 0.6785 - acc: 0.5670
1408/4849 [=======>......................] - ETA: 4:49 - loss: 0.6784 - acc: 0.5724
1472/4849 [========>.....................] - ETA: 4:44 - loss: 0.6764 - acc: 0.5768
1536/4849 [========>.....................] - ETA: 4:39 - loss: 0.6759 - acc: 0.5807
1600/4849 [========>.....................] - ETA: 4:33 - loss: 0.6747 - acc: 0.5825
1664/4849 [=========>....................] - ETA: 4:28 - loss: 0.6746 - acc: 0.5817
1728/4849 [=========>....................] - ETA: 4:22 - loss: 0.6765 - acc: 0.5775
1792/4849 [==========>...................] - ETA: 4:15 - loss: 0.6763 - acc: 0.5787
1856/4849 [==========>...................] - ETA: 4:08 - loss: 0.6745 - acc: 0.5841
1920/4849 [==========>...................] - ETA: 4:01 - loss: 0.6756 - acc: 0.5813
1984/4849 [===========>..................] - ETA: 3:56 - loss: 0.6748 - acc: 0.5827
2048/4849 [===========>..................] - ETA: 3:49 - loss: 0.6752 - acc: 0.5825
2112/4849 [============>.................] - ETA: 3:43 - loss: 0.6754 - acc: 0.5810
2176/4849 [============>.................] - ETA: 3:37 - loss: 0.6742 - acc: 0.5818
2240/4849 [============>.................] - ETA: 3:31 - loss: 0.6730 - acc: 0.5830
2304/4849 [=============>................] - ETA: 3:25 - loss: 0.6736 - acc: 0.5842
2368/4849 [=============>................] - ETA: 3:19 - loss: 0.6724 - acc: 0.5870
2432/4849 [==============>...............] - ETA: 3:14 - loss: 0.6737 - acc: 0.5855
2496/4849 [==============>...............] - ETA: 3:08 - loss: 0.6722 - acc: 0.5881
2560/4849 [==============>...............] - ETA: 3:03 - loss: 0.6729 - acc: 0.5863
2624/4849 [===============>..............] - ETA: 2:58 - loss: 0.6741 - acc: 0.5831
2688/4849 [===============>..............] - ETA: 2:53 - loss: 0.6736 - acc: 0.5826
2752/4849 [================>.............] - ETA: 2:48 - loss: 0.6736 - acc: 0.5839
2816/4849 [================>.............] - ETA: 2:43 - loss: 0.6732 - acc: 0.5856
2880/4849 [================>.............] - ETA: 2:38 - loss: 0.6739 - acc: 0.5837
2944/4849 [=================>............] - ETA: 2:33 - loss: 0.6757 - acc: 0.5808
3008/4849 [=================>............] - ETA: 2:28 - loss: 0.6749 - acc: 0.5814
3072/4849 [==================>...........] - ETA: 2:23 - loss: 0.6753 - acc: 0.5811
3136/4849 [==================>...........] - ETA: 2:18 - loss: 0.6754 - acc: 0.5807
3200/4849 [==================>...........] - ETA: 2:13 - loss: 0.6757 - acc: 0.5803
3264/4849 [===================>..........] - ETA: 2:08 - loss: 0.6759 - acc: 0.5800
3328/4849 [===================>..........] - ETA: 2:03 - loss: 0.6758 - acc: 0.5793
3392/4849 [===================>..........] - ETA: 1:58 - loss: 0.6760 - acc: 0.5781
3456/4849 [====================>.........] - ETA: 1:53 - loss: 0.6760 - acc: 0.5790
3520/4849 [====================>.........] - ETA: 1:48 - loss: 0.6761 - acc: 0.5795
3584/4849 [=====================>........] - ETA: 1:43 - loss: 0.6765 - acc: 0.5784
3648/4849 [=====================>........] - ETA: 1:37 - loss: 0.6763 - acc: 0.5792
3712/4849 [=====================>........] - ETA: 1:32 - loss: 0.6766 - acc: 0.5797
3776/4849 [======================>.......] - ETA: 1:27 - loss: 0.6773 - acc: 0.5792
3840/4849 [======================>.......] - ETA: 1:22 - loss: 0.6776 - acc: 0.5766
3904/4849 [=======================>......] - ETA: 1:17 - loss: 0.6780 - acc: 0.5751
3968/4849 [=======================>......] - ETA: 1:12 - loss: 0.6774 - acc: 0.5761
4032/4849 [=======================>......] - ETA: 1:07 - loss: 0.6770 - acc: 0.5779
4096/4849 [========================>.....] - ETA: 1:01 - loss: 0.6766 - acc: 0.5784
4160/4849 [========================>.....] - ETA: 56s - loss: 0.6773 - acc: 0.5774 
4224/4849 [=========================>....] - ETA: 51s - loss: 0.6772 - acc: 0.5760
4288/4849 [=========================>....] - ETA: 46s - loss: 0.6770 - acc: 0.5760
4352/4849 [=========================>....] - ETA: 40s - loss: 0.6775 - acc: 0.5749
4416/4849 [==========================>...] - ETA: 35s - loss: 0.6770 - acc: 0.5765
4480/4849 [==========================>...] - ETA: 30s - loss: 0.6775 - acc: 0.5759
4544/4849 [===========================>..] - ETA: 25s - loss: 0.6771 - acc: 0.5757
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6763 - acc: 0.5770
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6764 - acc: 0.5771
4736/4849 [============================>.] - ETA: 9s - loss: 0.6762 - acc: 0.5771 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6769 - acc: 0.5754
4849/4849 [==============================] - 418s 86ms/step - loss: 0.6768 - acc: 0.5760 - val_loss: 0.6872 - val_acc: 0.5529

Epoch 00006: val_acc did not improve from 0.59926
Epoch 7/10

  64/4849 [..............................] - ETA: 7:10 - loss: 0.7053 - acc: 0.5000
 128/4849 [..............................] - ETA: 7:03 - loss: 0.7090 - acc: 0.5078
 192/4849 [>.............................] - ETA: 7:07 - loss: 0.7101 - acc: 0.5156
 256/4849 [>.............................] - ETA: 6:58 - loss: 0.6998 - acc: 0.5195
 320/4849 [>.............................] - ETA: 6:45 - loss: 0.6951 - acc: 0.5437
 384/4849 [=>............................] - ETA: 6:38 - loss: 0.6881 - acc: 0.5573
 448/4849 [=>............................] - ETA: 6:30 - loss: 0.6834 - acc: 0.5603
 512/4849 [==>...........................] - ETA: 6:21 - loss: 0.6789 - acc: 0.5664
 576/4849 [==>...........................] - ETA: 6:14 - loss: 0.6791 - acc: 0.5660
 640/4849 [==>...........................] - ETA: 6:08 - loss: 0.6790 - acc: 0.5656
 704/4849 [===>..........................] - ETA: 6:04 - loss: 0.6840 - acc: 0.5554
 768/4849 [===>..........................] - ETA: 5:59 - loss: 0.6820 - acc: 0.5677
 832/4849 [====>.........................] - ETA: 5:51 - loss: 0.6803 - acc: 0.5661
 896/4849 [====>.........................] - ETA: 5:45 - loss: 0.6768 - acc: 0.5748
 960/4849 [====>.........................] - ETA: 5:40 - loss: 0.6780 - acc: 0.5729
1024/4849 [=====>........................] - ETA: 5:34 - loss: 0.6808 - acc: 0.5664
1088/4849 [=====>........................] - ETA: 5:29 - loss: 0.6837 - acc: 0.5607
1152/4849 [======>.......................] - ETA: 5:24 - loss: 0.6809 - acc: 0.5660
1216/4849 [======>.......................] - ETA: 5:20 - loss: 0.6822 - acc: 0.5641
1280/4849 [======>.......................] - ETA: 5:14 - loss: 0.6854 - acc: 0.5617
1344/4849 [=======>......................] - ETA: 5:09 - loss: 0.6854 - acc: 0.5603
1408/4849 [=======>......................] - ETA: 5:04 - loss: 0.6865 - acc: 0.5568
1472/4849 [========>.....................] - ETA: 4:58 - loss: 0.6851 - acc: 0.5598
1536/4849 [========>.....................] - ETA: 4:53 - loss: 0.6862 - acc: 0.5566
1600/4849 [========>.....................] - ETA: 4:47 - loss: 0.6854 - acc: 0.5587
1664/4849 [=========>....................] - ETA: 4:42 - loss: 0.6831 - acc: 0.5601
1728/4849 [=========>....................] - ETA: 4:36 - loss: 0.6827 - acc: 0.5602
1792/4849 [==========>...................] - ETA: 4:31 - loss: 0.6832 - acc: 0.5580
1856/4849 [==========>...................] - ETA: 4:26 - loss: 0.6822 - acc: 0.5571
1920/4849 [==========>...................] - ETA: 4:20 - loss: 0.6809 - acc: 0.5609
1984/4849 [===========>..................] - ETA: 4:14 - loss: 0.6804 - acc: 0.5635
2048/4849 [===========>..................] - ETA: 4:08 - loss: 0.6815 - acc: 0.5620
2112/4849 [============>.................] - ETA: 4:03 - loss: 0.6812 - acc: 0.5620
2176/4849 [============>.................] - ETA: 3:56 - loss: 0.6819 - acc: 0.5607
2240/4849 [============>.................] - ETA: 3:51 - loss: 0.6815 - acc: 0.5616
2304/4849 [=============>................] - ETA: 3:46 - loss: 0.6820 - acc: 0.5612
2368/4849 [=============>................] - ETA: 3:40 - loss: 0.6804 - acc: 0.5621
2432/4849 [==============>...............] - ETA: 3:35 - loss: 0.6806 - acc: 0.5609
2496/4849 [==============>...............] - ETA: 3:29 - loss: 0.6810 - acc: 0.5589
2560/4849 [==============>...............] - ETA: 3:24 - loss: 0.6809 - acc: 0.5586
2624/4849 [===============>..............] - ETA: 3:18 - loss: 0.6813 - acc: 0.5587
2688/4849 [===============>..............] - ETA: 3:12 - loss: 0.6802 - acc: 0.5599
2752/4849 [================>.............] - ETA: 3:06 - loss: 0.6795 - acc: 0.5614
2816/4849 [================>.............] - ETA: 3:01 - loss: 0.6794 - acc: 0.5629
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6793 - acc: 0.5622
2944/4849 [=================>............] - ETA: 2:49 - loss: 0.6794 - acc: 0.5635
3008/4849 [=================>............] - ETA: 2:44 - loss: 0.6786 - acc: 0.5652
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6777 - acc: 0.5664
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6788 - acc: 0.5635
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6782 - acc: 0.5637
3264/4849 [===================>..........] - ETA: 2:21 - loss: 0.6785 - acc: 0.5628
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6783 - acc: 0.5637
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6783 - acc: 0.5649
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6794 - acc: 0.5622
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6797 - acc: 0.5628
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6789 - acc: 0.5645
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6788 - acc: 0.5652
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6787 - acc: 0.5657
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6787 - acc: 0.5651
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6789 - acc: 0.5651
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6790 - acc: 0.5645
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6785 - acc: 0.5650
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6789 - acc: 0.5650
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6786 - acc: 0.5654
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6783 - acc: 0.5663
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6782 - acc: 0.5661 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6785 - acc: 0.5651
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6789 - acc: 0.5641
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6797 - acc: 0.5627
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6801 - acc: 0.5632
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6799 - acc: 0.5634
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6803 - acc: 0.5627
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6808 - acc: 0.5621
4736/4849 [============================>.] - ETA: 10s - loss: 0.6806 - acc: 0.5625
4800/4849 [============================>.] - ETA: 4s - loss: 0.6809 - acc: 0.5623 
4849/4849 [==============================] - 448s 92ms/step - loss: 0.6810 - acc: 0.5614 - val_loss: 0.6885 - val_acc: 0.5510

Epoch 00007: val_acc did not improve from 0.59926
Epoch 8/10

  64/4849 [..............................] - ETA: 7:08 - loss: 0.6635 - acc: 0.6250
 128/4849 [..............................] - ETA: 7:11 - loss: 0.6887 - acc: 0.5625
 192/4849 [>.............................] - ETA: 6:52 - loss: 0.6889 - acc: 0.5469
 256/4849 [>.............................] - ETA: 6:49 - loss: 0.6875 - acc: 0.5352
 320/4849 [>.............................] - ETA: 6:41 - loss: 0.6852 - acc: 0.5437
 384/4849 [=>............................] - ETA: 6:37 - loss: 0.6822 - acc: 0.5547
 448/4849 [=>............................] - ETA: 6:33 - loss: 0.6780 - acc: 0.5670
 512/4849 [==>...........................] - ETA: 6:27 - loss: 0.6790 - acc: 0.5625
 576/4849 [==>...........................] - ETA: 6:20 - loss: 0.6745 - acc: 0.5712
 640/4849 [==>...........................] - ETA: 6:16 - loss: 0.6757 - acc: 0.5750
 704/4849 [===>..........................] - ETA: 6:11 - loss: 0.6744 - acc: 0.5810
 768/4849 [===>..........................] - ETA: 6:05 - loss: 0.6707 - acc: 0.5911
 832/4849 [====>.........................] - ETA: 6:01 - loss: 0.6685 - acc: 0.5962
 896/4849 [====>.........................] - ETA: 5:54 - loss: 0.6683 - acc: 0.5960
 960/4849 [====>.........................] - ETA: 5:48 - loss: 0.6682 - acc: 0.5969
1024/4849 [=====>........................] - ETA: 5:44 - loss: 0.6675 - acc: 0.5996
1088/4849 [=====>........................] - ETA: 5:38 - loss: 0.6672 - acc: 0.5993
1152/4849 [======>.......................] - ETA: 5:33 - loss: 0.6681 - acc: 0.5955
1216/4849 [======>.......................] - ETA: 5:28 - loss: 0.6689 - acc: 0.5954
1280/4849 [======>.......................] - ETA: 5:22 - loss: 0.6686 - acc: 0.5953
1344/4849 [=======>......................] - ETA: 5:16 - loss: 0.6694 - acc: 0.5967
1408/4849 [=======>......................] - ETA: 5:11 - loss: 0.6686 - acc: 0.5973
1472/4849 [========>.....................] - ETA: 5:06 - loss: 0.6717 - acc: 0.5883
1536/4849 [========>.....................] - ETA: 5:01 - loss: 0.6733 - acc: 0.5853
1600/4849 [========>.....................] - ETA: 4:55 - loss: 0.6731 - acc: 0.5850
1664/4849 [=========>....................] - ETA: 4:50 - loss: 0.6733 - acc: 0.5841
1728/4849 [=========>....................] - ETA: 4:45 - loss: 0.6735 - acc: 0.5833
1792/4849 [==========>...................] - ETA: 4:39 - loss: 0.6723 - acc: 0.5865
1856/4849 [==========>...................] - ETA: 4:33 - loss: 0.6723 - acc: 0.5884
1920/4849 [==========>...................] - ETA: 4:27 - loss: 0.6736 - acc: 0.5849
1984/4849 [===========>..................] - ETA: 4:21 - loss: 0.6738 - acc: 0.5842
2048/4849 [===========>..................] - ETA: 4:16 - loss: 0.6744 - acc: 0.5830
2112/4849 [============>.................] - ETA: 4:10 - loss: 0.6744 - acc: 0.5838
2176/4849 [============>.................] - ETA: 4:04 - loss: 0.6745 - acc: 0.5827
2240/4849 [============>.................] - ETA: 3:58 - loss: 0.6736 - acc: 0.5844
2304/4849 [=============>................] - ETA: 3:51 - loss: 0.6750 - acc: 0.5820
2368/4849 [=============>................] - ETA: 3:46 - loss: 0.6753 - acc: 0.5798
2432/4849 [==============>...............] - ETA: 3:40 - loss: 0.6771 - acc: 0.5781
2496/4849 [==============>...............] - ETA: 3:34 - loss: 0.6782 - acc: 0.5757
2560/4849 [==============>...............] - ETA: 3:28 - loss: 0.6780 - acc: 0.5758
2624/4849 [===============>..............] - ETA: 3:22 - loss: 0.6775 - acc: 0.5762
2688/4849 [===============>..............] - ETA: 3:17 - loss: 0.6792 - acc: 0.5740
2752/4849 [================>.............] - ETA: 3:11 - loss: 0.6793 - acc: 0.5745
2816/4849 [================>.............] - ETA: 3:05 - loss: 0.6783 - acc: 0.5771
2880/4849 [================>.............] - ETA: 2:59 - loss: 0.6777 - acc: 0.5774
2944/4849 [=================>............] - ETA: 2:53 - loss: 0.6776 - acc: 0.5785
3008/4849 [=================>............] - ETA: 2:48 - loss: 0.6777 - acc: 0.5788
3072/4849 [==================>...........] - ETA: 2:42 - loss: 0.6778 - acc: 0.5785
3136/4849 [==================>...........] - ETA: 2:36 - loss: 0.6778 - acc: 0.5778
3200/4849 [==================>...........] - ETA: 2:30 - loss: 0.6778 - acc: 0.5769
3264/4849 [===================>..........] - ETA: 2:24 - loss: 0.6785 - acc: 0.5748
3328/4849 [===================>..........] - ETA: 2:18 - loss: 0.6777 - acc: 0.5766
3392/4849 [===================>..........] - ETA: 2:12 - loss: 0.6783 - acc: 0.5758
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.6781 - acc: 0.5770
3520/4849 [====================>.........] - ETA: 2:01 - loss: 0.6786 - acc: 0.5773
3584/4849 [=====================>........] - ETA: 1:55 - loss: 0.6780 - acc: 0.5784
3648/4849 [=====================>........] - ETA: 1:49 - loss: 0.6787 - acc: 0.5768
3712/4849 [=====================>........] - ETA: 1:43 - loss: 0.6783 - acc: 0.5768
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6785 - acc: 0.5765
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6790 - acc: 0.5766
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6796 - acc: 0.5748
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6794 - acc: 0.5756
4032/4849 [=======================>......] - ETA: 1:14 - loss: 0.6788 - acc: 0.5776
4096/4849 [========================>.....] - ETA: 1:08 - loss: 0.6784 - acc: 0.5793
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6788 - acc: 0.5791
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6797 - acc: 0.5758 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6795 - acc: 0.5765
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6795 - acc: 0.5770
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6794 - acc: 0.5770
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6787 - acc: 0.5790
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6788 - acc: 0.5790
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6789 - acc: 0.5783
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6790 - acc: 0.5783
4736/4849 [============================>.] - ETA: 10s - loss: 0.6795 - acc: 0.5775
4800/4849 [============================>.] - ETA: 4s - loss: 0.6794 - acc: 0.5775 
4849/4849 [==============================] - 450s 93ms/step - loss: 0.6789 - acc: 0.5783 - val_loss: 0.6732 - val_acc: 0.5955

Epoch 00008: val_acc did not improve from 0.59926
Epoch 9/10

  64/4849 [..............................] - ETA: 6:34 - loss: 0.6263 - acc: 0.6719
 128/4849 [..............................] - ETA: 6:45 - loss: 0.6478 - acc: 0.6172
 192/4849 [>.............................] - ETA: 6:49 - loss: 0.6580 - acc: 0.6094
 256/4849 [>.............................] - ETA: 6:34 - loss: 0.6548 - acc: 0.6055
 320/4849 [>.............................] - ETA: 6:29 - loss: 0.6676 - acc: 0.5813
 384/4849 [=>............................] - ETA: 6:22 - loss: 0.6585 - acc: 0.5990
 448/4849 [=>............................] - ETA: 6:18 - loss: 0.6585 - acc: 0.5982
 512/4849 [==>...........................] - ETA: 6:10 - loss: 0.6597 - acc: 0.6035
 576/4849 [==>...........................] - ETA: 6:04 - loss: 0.6588 - acc: 0.6076
 640/4849 [==>...........................] - ETA: 5:57 - loss: 0.6587 - acc: 0.6094
 704/4849 [===>..........................] - ETA: 5:51 - loss: 0.6604 - acc: 0.6065
 768/4849 [===>..........................] - ETA: 5:47 - loss: 0.6601 - acc: 0.6094
 832/4849 [====>.........................] - ETA: 5:44 - loss: 0.6632 - acc: 0.6010
 896/4849 [====>.........................] - ETA: 5:36 - loss: 0.6674 - acc: 0.5960
 960/4849 [====>.........................] - ETA: 5:31 - loss: 0.6665 - acc: 0.5948
1024/4849 [=====>........................] - ETA: 5:27 - loss: 0.6637 - acc: 0.5986
1088/4849 [=====>........................] - ETA: 5:21 - loss: 0.6650 - acc: 0.5983
1152/4849 [======>.......................] - ETA: 5:15 - loss: 0.6661 - acc: 0.5990
1216/4849 [======>.......................] - ETA: 5:08 - loss: 0.6695 - acc: 0.5946
1280/4849 [======>.......................] - ETA: 5:01 - loss: 0.6720 - acc: 0.5953
1344/4849 [=======>......................] - ETA: 4:54 - loss: 0.6709 - acc: 0.5960
1408/4849 [=======>......................] - ETA: 4:48 - loss: 0.6708 - acc: 0.5966
1472/4849 [========>.....................] - ETA: 4:42 - loss: 0.6704 - acc: 0.5938
1536/4849 [========>.....................] - ETA: 4:36 - loss: 0.6718 - acc: 0.5898
1600/4849 [========>.....................] - ETA: 4:30 - loss: 0.6739 - acc: 0.5856
1664/4849 [=========>....................] - ETA: 4:24 - loss: 0.6747 - acc: 0.5835
1728/4849 [=========>....................] - ETA: 4:19 - loss: 0.6741 - acc: 0.5856
1792/4849 [==========>...................] - ETA: 4:13 - loss: 0.6741 - acc: 0.5871
1856/4849 [==========>...................] - ETA: 4:07 - loss: 0.6746 - acc: 0.5900
1920/4849 [==========>...................] - ETA: 4:03 - loss: 0.6755 - acc: 0.5880
1984/4849 [===========>..................] - ETA: 3:56 - loss: 0.6750 - acc: 0.5887
2048/4849 [===========>..................] - ETA: 3:51 - loss: 0.6758 - acc: 0.5884
2112/4849 [============>.................] - ETA: 3:45 - loss: 0.6755 - acc: 0.5885
2176/4849 [============>.................] - ETA: 3:40 - loss: 0.6768 - acc: 0.5859
2240/4849 [============>.................] - ETA: 3:34 - loss: 0.6787 - acc: 0.5821
2304/4849 [=============>................] - ETA: 3:29 - loss: 0.6783 - acc: 0.5833
2368/4849 [=============>................] - ETA: 3:24 - loss: 0.6794 - acc: 0.5798
2432/4849 [==============>...............] - ETA: 3:19 - loss: 0.6791 - acc: 0.5802
2496/4849 [==============>...............] - ETA: 3:14 - loss: 0.6793 - acc: 0.5781
2560/4849 [==============>...............] - ETA: 3:08 - loss: 0.6799 - acc: 0.5770
2624/4849 [===============>..............] - ETA: 3:03 - loss: 0.6795 - acc: 0.5781
2688/4849 [===============>..............] - ETA: 2:57 - loss: 0.6798 - acc: 0.5770
2752/4849 [================>.............] - ETA: 2:52 - loss: 0.6798 - acc: 0.5759
2816/4849 [================>.............] - ETA: 2:47 - loss: 0.6806 - acc: 0.5742
2880/4849 [================>.............] - ETA: 2:41 - loss: 0.6801 - acc: 0.5764
2944/4849 [=================>............] - ETA: 2:36 - loss: 0.6799 - acc: 0.5774
3008/4849 [=================>............] - ETA: 2:31 - loss: 0.6801 - acc: 0.5771
3072/4849 [==================>...........] - ETA: 2:25 - loss: 0.6799 - acc: 0.5771
3136/4849 [==================>...........] - ETA: 2:20 - loss: 0.6795 - acc: 0.5778
3200/4849 [==================>...........] - ETA: 2:14 - loss: 0.6799 - acc: 0.5763
3264/4849 [===================>..........] - ETA: 2:09 - loss: 0.6803 - acc: 0.5751
3328/4849 [===================>..........] - ETA: 2:04 - loss: 0.6797 - acc: 0.5754
3392/4849 [===================>..........] - ETA: 1:58 - loss: 0.6804 - acc: 0.5734
3456/4849 [====================>.........] - ETA: 1:53 - loss: 0.6798 - acc: 0.5758
3520/4849 [====================>.........] - ETA: 1:48 - loss: 0.6797 - acc: 0.5759
3584/4849 [=====================>........] - ETA: 1:43 - loss: 0.6801 - acc: 0.5751
3648/4849 [=====================>........] - ETA: 1:37 - loss: 0.6795 - acc: 0.5757
3712/4849 [=====================>........] - ETA: 1:32 - loss: 0.6794 - acc: 0.5762
3776/4849 [======================>.......] - ETA: 1:27 - loss: 0.6788 - acc: 0.5771
3840/4849 [======================>.......] - ETA: 1:22 - loss: 0.6787 - acc: 0.5776
3904/4849 [=======================>......] - ETA: 1:16 - loss: 0.6782 - acc: 0.5791
3968/4849 [=======================>......] - ETA: 1:11 - loss: 0.6776 - acc: 0.5799
4032/4849 [=======================>......] - ETA: 1:06 - loss: 0.6782 - acc: 0.5786
4096/4849 [========================>.....] - ETA: 1:01 - loss: 0.6779 - acc: 0.5789
4160/4849 [========================>.....] - ETA: 55s - loss: 0.6774 - acc: 0.5788 
4224/4849 [=========================>....] - ETA: 50s - loss: 0.6773 - acc: 0.5786
4288/4849 [=========================>....] - ETA: 45s - loss: 0.6770 - acc: 0.5798
4352/4849 [=========================>....] - ETA: 40s - loss: 0.6769 - acc: 0.5797
4416/4849 [==========================>...] - ETA: 35s - loss: 0.6769 - acc: 0.5799
4480/4849 [==========================>...] - ETA: 29s - loss: 0.6769 - acc: 0.5804
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6763 - acc: 0.5810
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6765 - acc: 0.5809
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6760 - acc: 0.5815
4736/4849 [============================>.] - ETA: 9s - loss: 0.6760 - acc: 0.5821 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6759 - acc: 0.5827
4849/4849 [==============================] - 410s 85ms/step - loss: 0.6757 - acc: 0.5836 - val_loss: 0.6770 - val_acc: 0.5844

Epoch 00009: val_acc did not improve from 0.59926
Epoch 10/10

  64/4849 [..............................] - ETA: 6:49 - loss: 0.7343 - acc: 0.4531
 128/4849 [..............................] - ETA: 6:46 - loss: 0.6868 - acc: 0.5469
 192/4849 [>.............................] - ETA: 6:31 - loss: 0.6881 - acc: 0.5417
 256/4849 [>.............................] - ETA: 6:29 - loss: 0.6677 - acc: 0.5664
 320/4849 [>.............................] - ETA: 6:24 - loss: 0.6633 - acc: 0.5750
 384/4849 [=>............................] - ETA: 6:15 - loss: 0.6607 - acc: 0.5938
 448/4849 [=>............................] - ETA: 6:11 - loss: 0.6665 - acc: 0.5848
 512/4849 [==>...........................] - ETA: 6:03 - loss: 0.6616 - acc: 0.5957
 576/4849 [==>...........................] - ETA: 5:56 - loss: 0.6585 - acc: 0.5920
 640/4849 [==>...........................] - ETA: 5:52 - loss: 0.6637 - acc: 0.5828
 704/4849 [===>..........................] - ETA: 5:47 - loss: 0.6693 - acc: 0.5753
 768/4849 [===>..........................] - ETA: 5:42 - loss: 0.6721 - acc: 0.5755
 832/4849 [====>.........................] - ETA: 5:36 - loss: 0.6748 - acc: 0.5721
 896/4849 [====>.........................] - ETA: 5:30 - loss: 0.6728 - acc: 0.5759
 960/4849 [====>.........................] - ETA: 5:25 - loss: 0.6697 - acc: 0.5792
1024/4849 [=====>........................] - ETA: 5:21 - loss: 0.6710 - acc: 0.5781
1088/4849 [=====>........................] - ETA: 5:15 - loss: 0.6736 - acc: 0.5781
1152/4849 [======>.......................] - ETA: 5:09 - loss: 0.6742 - acc: 0.5764
1216/4849 [======>.......................] - ETA: 5:03 - loss: 0.6774 - acc: 0.5715
1280/4849 [======>.......................] - ETA: 4:58 - loss: 0.6770 - acc: 0.5742
1344/4849 [=======>......................] - ETA: 4:52 - loss: 0.6776 - acc: 0.5722
1408/4849 [=======>......................] - ETA: 4:47 - loss: 0.6784 - acc: 0.5717
1472/4849 [========>.....................] - ETA: 4:42 - loss: 0.6770 - acc: 0.5713
1536/4849 [========>.....................] - ETA: 4:37 - loss: 0.6763 - acc: 0.5736
1600/4849 [========>.....................] - ETA: 4:32 - loss: 0.6748 - acc: 0.5781
1664/4849 [=========>....................] - ETA: 4:26 - loss: 0.6753 - acc: 0.5757
1728/4849 [=========>....................] - ETA: 4:20 - loss: 0.6759 - acc: 0.5747
1792/4849 [==========>...................] - ETA: 4:15 - loss: 0.6746 - acc: 0.5770
1856/4849 [==========>...................] - ETA: 4:10 - loss: 0.6733 - acc: 0.5776
1920/4849 [==========>...................] - ETA: 4:04 - loss: 0.6722 - acc: 0.5776
1984/4849 [===========>..................] - ETA: 3:58 - loss: 0.6707 - acc: 0.5791
2048/4849 [===========>..................] - ETA: 3:53 - loss: 0.6712 - acc: 0.5781
2112/4849 [============>.................] - ETA: 3:48 - loss: 0.6714 - acc: 0.5781
2176/4849 [============>.................] - ETA: 3:42 - loss: 0.6728 - acc: 0.5740
2240/4849 [============>.................] - ETA: 3:37 - loss: 0.6720 - acc: 0.5759
2304/4849 [=============>................] - ETA: 3:32 - loss: 0.6723 - acc: 0.5768
2368/4849 [=============>................] - ETA: 3:26 - loss: 0.6713 - acc: 0.5781
2432/4849 [==============>...............] - ETA: 3:21 - loss: 0.6720 - acc: 0.5773
2496/4849 [==============>...............] - ETA: 3:16 - loss: 0.6723 - acc: 0.5777
2560/4849 [==============>...............] - ETA: 3:11 - loss: 0.6734 - acc: 0.5758
2624/4849 [===============>..............] - ETA: 3:05 - loss: 0.6735 - acc: 0.5755
2688/4849 [===============>..............] - ETA: 3:00 - loss: 0.6725 - acc: 0.5774
2752/4849 [================>.............] - ETA: 2:55 - loss: 0.6724 - acc: 0.5770
2816/4849 [================>.............] - ETA: 2:50 - loss: 0.6717 - acc: 0.5795
2880/4849 [================>.............] - ETA: 2:44 - loss: 0.6719 - acc: 0.5806
2944/4849 [=================>............] - ETA: 2:39 - loss: 0.6721 - acc: 0.5812
3008/4849 [=================>............] - ETA: 2:34 - loss: 0.6714 - acc: 0.5828
3072/4849 [==================>...........] - ETA: 2:28 - loss: 0.6718 - acc: 0.5820
3136/4849 [==================>...........] - ETA: 2:23 - loss: 0.6726 - acc: 0.5810
3200/4849 [==================>...........] - ETA: 2:17 - loss: 0.6728 - acc: 0.5797
3264/4849 [===================>..........] - ETA: 2:12 - loss: 0.6729 - acc: 0.5800
3328/4849 [===================>..........] - ETA: 2:06 - loss: 0.6724 - acc: 0.5811
3392/4849 [===================>..........] - ETA: 2:01 - loss: 0.6725 - acc: 0.5793
3456/4849 [====================>.........] - ETA: 1:56 - loss: 0.6725 - acc: 0.5796
3520/4849 [====================>.........] - ETA: 1:51 - loss: 0.6724 - acc: 0.5801
3584/4849 [=====================>........] - ETA: 1:45 - loss: 0.6725 - acc: 0.5801
3648/4849 [=====================>........] - ETA: 1:40 - loss: 0.6726 - acc: 0.5800
3712/4849 [=====================>........] - ETA: 1:35 - loss: 0.6724 - acc: 0.5811
3776/4849 [======================>.......] - ETA: 1:29 - loss: 0.6717 - acc: 0.5818
3840/4849 [======================>.......] - ETA: 1:24 - loss: 0.6716 - acc: 0.5820
3904/4849 [=======================>......] - ETA: 1:18 - loss: 0.6717 - acc: 0.5820
3968/4849 [=======================>......] - ETA: 1:13 - loss: 0.6715 - acc: 0.5829
4032/4849 [=======================>......] - ETA: 1:08 - loss: 0.6717 - acc: 0.5821
4096/4849 [========================>.....] - ETA: 1:02 - loss: 0.6716 - acc: 0.5823
4160/4849 [========================>.....] - ETA: 57s - loss: 0.6715 - acc: 0.5827 
4224/4849 [=========================>....] - ETA: 51s - loss: 0.6709 - acc: 0.5836
4288/4849 [=========================>....] - ETA: 46s - loss: 0.6717 - acc: 0.5816
4352/4849 [=========================>....] - ETA: 41s - loss: 0.6716 - acc: 0.5818
4416/4849 [==========================>...] - ETA: 35s - loss: 0.6717 - acc: 0.5824
4480/4849 [==========================>...] - ETA: 30s - loss: 0.6715 - acc: 0.5837
4544/4849 [===========================>..] - ETA: 25s - loss: 0.6717 - acc: 0.5838
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6716 - acc: 0.5836
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6718 - acc: 0.5833
4736/4849 [============================>.] - ETA: 9s - loss: 0.6721 - acc: 0.5828 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6717 - acc: 0.5831
4849/4849 [==============================] - 413s 85ms/step - loss: 0.6720 - acc: 0.5822 - val_loss: 0.6807 - val_acc: 0.5807

Epoch 00010: val_acc did not improve from 0.59926
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f2960662c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f2960662c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f2b28692590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f2b28692590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29601fc150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29601fc150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29601e5f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29601e5f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f29601a7410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f29601a7410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2960068590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2960068590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29600685d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29600685d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29601ada90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29601ada90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f26c4777190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f26c4777190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f292c5ea650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f292c5ea650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26c47703d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26c47703d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29601a7ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29601a7ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26c4761ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26c4761ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f292c4c6790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f292c4c6790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f292c4c0d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f292c4c0d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f292c4e0410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f292c4e0410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f292c795810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f292c795810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f292c2a7bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f292c2a7bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f292c1c4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f292c1c4fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f292c14d990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f292c14d990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f292c211310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f292c211310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f292c1c4b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f292c1c4b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f292c0e5490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f292c0e5490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f292c2162d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f292c2162d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f292079fd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f292079fd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29206aac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29206aac50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f292c0cd910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f292c0cd910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2920577c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2920577c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29203a4310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29203a4310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f292026c750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f292026c750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2920364810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2920364810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29206aaa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29206aaa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f292010f690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f292010f690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f292003c790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f292003c790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28e8688d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28e8688d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e874d990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e874d990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2920281a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2920281a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e86d4b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e86d4b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28e86c6e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28e86c6e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28e8421d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28e8421d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e86cda50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e86cda50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28e86c6d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28e86c6d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e82ea1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e82ea1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28e81bc250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28e81bc250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28e816aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28e816aa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e074d350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e074d350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28e8215e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28e8215e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e816a450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e816a450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28e8158950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28e8158950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28e06f1f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28e06f1f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e056f190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e056f190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28e8158850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28e8158850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e0470250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e0470250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28e036f6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28e036f6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28e02ea750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28e02ea750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e0641cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e0641cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28e036f990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28e036f990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e02bd750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e02bd750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28ac7e2c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f28ac7e2c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28ac73a8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f28ac73a8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28ac7b9150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28ac7b9150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28ac7e2550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f28ac7e2550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29202f5ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29202f5ed0>>: AttributeError: module 'gast' has no attribute 'Str'
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 3:02
 128/1348 [=>............................] - ETA: 1:44
 192/1348 [===>..........................] - ETA: 1:17
 256/1348 [====>.........................] - ETA: 1:02
 320/1348 [======>.......................] - ETA: 53s 
 384/1348 [=======>......................] - ETA: 46s
 448/1348 [========>.....................] - ETA: 40s
 512/1348 [==========>...................] - ETA: 36s
 576/1348 [===========>..................] - ETA: 32s
 640/1348 [=============>................] - ETA: 28s
 704/1348 [==============>...............] - ETA: 24s
 768/1348 [================>.............] - ETA: 21s
 832/1348 [=================>............] - ETA: 19s
 896/1348 [==================>...........] - ETA: 16s
 960/1348 [====================>.........] - ETA: 14s
1024/1348 [=====================>........] - ETA: 11s
1088/1348 [=======================>......] - ETA: 9s 
1152/1348 [========================>.....] - ETA: 6s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 47s 35ms/step
loss: 0.6773036279734943
acc: 0.5830860534124629
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f25c8546610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f25c8546610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f25c8548ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f25c8548ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25c854ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25c854ec50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2490732c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2490732c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f29602e24d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f29602e24d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29604c82d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f29604c82d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2768054e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2768054e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e0263750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28e0263750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29604a0ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f29604a0ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25c8468d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25c8468d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2960441c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2960441c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29602d4ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f29602d4ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28ac5d9450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f28ac5d9450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f25c8248350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f25c8248350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25c8159490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25c8159490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25ac7c0a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25ac7c0a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f25c8248d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f25c8248d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25c8072210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25c8072210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f25ac6d4e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f25ac6d4e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25ac5e73d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25ac5e73d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25ac72b310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25ac72b310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f25c8250710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f25c8250710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25ac4d5a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25ac4d5a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f25ac3ef750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f25ac3ef750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25ac286b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25ac286b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25ac185ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25ac185ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f25ac3ef650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f25ac3ef650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25ac3d8610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25ac3d8610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f25ac120210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f25ac120210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25ac095090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25ac095090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25887b3ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25887b3ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f25ac120410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f25ac120410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25885fc0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25885fc0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f258858a090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f258858a090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f258875ae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f258875ae50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25885414d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25885414d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f258858a590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f258858a590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2588445050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2588445050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f25882a7cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f25882a7cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25881fe790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f25881fe790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f258824b7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f258824b7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2588574650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2588574650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25687be950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25687be950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f258817e750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f258817e750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24906c1c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24906c1c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25c81edd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25c81edd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f258817e050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f258817e050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2588158e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2588158e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24905357d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24905357d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f249052b910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f249052b910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2490444950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2490444950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2490620ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2490620ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f249042c9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f249042c9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24903e1590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24903e1590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24904290d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24904290d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24901f9150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24901f9150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24903e1410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24903e1410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24900ce6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24900ce6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2490418450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2490418450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f247063f450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f247063f450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f247062a490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f247062a490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24900ce2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24900ce2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f247055ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f247055ac50>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 31:07 - loss: 0.7317 - acc: 0.4375
 128/4849 [..............................] - ETA: 18:37 - loss: 0.7197 - acc: 0.5156
 192/4849 [>.............................] - ETA: 14:02 - loss: 0.7392 - acc: 0.5052
 256/4849 [>.............................] - ETA: 12:07 - loss: 0.7352 - acc: 0.5039
 320/4849 [>.............................] - ETA: 10:46 - loss: 0.7305 - acc: 0.5094
 384/4849 [=>............................] - ETA: 9:47 - loss: 0.7298 - acc: 0.5156 
 448/4849 [=>............................] - ETA: 9:06 - loss: 0.7376 - acc: 0.5067
 512/4849 [==>...........................] - ETA: 8:32 - loss: 0.7405 - acc: 0.5078
 576/4849 [==>...........................] - ETA: 8:04 - loss: 0.7417 - acc: 0.5069
 640/4849 [==>...........................] - ETA: 7:45 - loss: 0.7363 - acc: 0.5141
 704/4849 [===>..........................] - ETA: 7:25 - loss: 0.7438 - acc: 0.5043
 768/4849 [===>..........................] - ETA: 7:09 - loss: 0.7431 - acc: 0.5052
 832/4849 [====>.........................] - ETA: 6:55 - loss: 0.7408 - acc: 0.5096
 896/4849 [====>.........................] - ETA: 6:43 - loss: 0.7406 - acc: 0.5123
 960/4849 [====>.........................] - ETA: 6:32 - loss: 0.7396 - acc: 0.5125
1024/4849 [=====>........................] - ETA: 6:21 - loss: 0.7373 - acc: 0.5146
1088/4849 [=====>........................] - ETA: 6:09 - loss: 0.7365 - acc: 0.5129
1152/4849 [======>.......................] - ETA: 6:01 - loss: 0.7346 - acc: 0.5122
1216/4849 [======>.......................] - ETA: 5:52 - loss: 0.7347 - acc: 0.5123
1280/4849 [======>.......................] - ETA: 5:43 - loss: 0.7349 - acc: 0.5102
1344/4849 [=======>......................] - ETA: 5:34 - loss: 0.7332 - acc: 0.5119
1408/4849 [=======>......................] - ETA: 5:26 - loss: 0.7332 - acc: 0.5135
1472/4849 [========>.....................] - ETA: 5:17 - loss: 0.7317 - acc: 0.5149
1536/4849 [========>.....................] - ETA: 5:10 - loss: 0.7280 - acc: 0.5215
1600/4849 [========>.....................] - ETA: 5:02 - loss: 0.7264 - acc: 0.5219
1664/4849 [=========>....................] - ETA: 4:55 - loss: 0.7246 - acc: 0.5222
1728/4849 [=========>....................] - ETA: 4:48 - loss: 0.7240 - acc: 0.5231
1792/4849 [==========>...................] - ETA: 4:41 - loss: 0.7209 - acc: 0.5285
1856/4849 [==========>...................] - ETA: 4:34 - loss: 0.7206 - acc: 0.5307
1920/4849 [==========>...................] - ETA: 4:27 - loss: 0.7220 - acc: 0.5276
1984/4849 [===========>..................] - ETA: 4:20 - loss: 0.7203 - acc: 0.5287
2048/4849 [===========>..................] - ETA: 4:14 - loss: 0.7218 - acc: 0.5259
2112/4849 [============>.................] - ETA: 4:07 - loss: 0.7224 - acc: 0.5251
2176/4849 [============>.................] - ETA: 4:00 - loss: 0.7223 - acc: 0.5257
2240/4849 [============>.................] - ETA: 3:53 - loss: 0.7219 - acc: 0.5250
2304/4849 [=============>................] - ETA: 3:48 - loss: 0.7219 - acc: 0.5256
2368/4849 [=============>................] - ETA: 3:41 - loss: 0.7230 - acc: 0.5220
2432/4849 [==============>...............] - ETA: 3:35 - loss: 0.7215 - acc: 0.5243
2496/4849 [==============>...............] - ETA: 3:28 - loss: 0.7197 - acc: 0.5264
2560/4849 [==============>...............] - ETA: 3:23 - loss: 0.7214 - acc: 0.5230
2624/4849 [===============>..............] - ETA: 3:16 - loss: 0.7205 - acc: 0.5244
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.7213 - acc: 0.5231
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.7218 - acc: 0.5222
2816/4849 [================>.............] - ETA: 2:58 - loss: 0.7222 - acc: 0.5206
2880/4849 [================>.............] - ETA: 2:52 - loss: 0.7217 - acc: 0.5208
2944/4849 [=================>............] - ETA: 2:46 - loss: 0.7214 - acc: 0.5200
3008/4849 [=================>............] - ETA: 2:40 - loss: 0.7204 - acc: 0.5213
3072/4849 [==================>...........] - ETA: 2:35 - loss: 0.7194 - acc: 0.5221
3136/4849 [==================>...........] - ETA: 2:28 - loss: 0.7200 - acc: 0.5217
3200/4849 [==================>...........] - ETA: 2:23 - loss: 0.7190 - acc: 0.5241
3264/4849 [===================>..........] - ETA: 2:17 - loss: 0.7195 - acc: 0.5230
3328/4849 [===================>..........] - ETA: 2:11 - loss: 0.7185 - acc: 0.5249
3392/4849 [===================>..........] - ETA: 2:05 - loss: 0.7184 - acc: 0.5242
3456/4849 [====================>.........] - ETA: 2:00 - loss: 0.7178 - acc: 0.5255
3520/4849 [====================>.........] - ETA: 1:54 - loss: 0.7176 - acc: 0.5241
3584/4849 [=====================>........] - ETA: 1:48 - loss: 0.7170 - acc: 0.5248
3648/4849 [=====================>........] - ETA: 1:42 - loss: 0.7169 - acc: 0.5249
3712/4849 [=====================>........] - ETA: 1:37 - loss: 0.7163 - acc: 0.5253
3776/4849 [======================>.......] - ETA: 1:31 - loss: 0.7154 - acc: 0.5267
3840/4849 [======================>.......] - ETA: 1:26 - loss: 0.7152 - acc: 0.5276
3904/4849 [=======================>......] - ETA: 1:20 - loss: 0.7149 - acc: 0.5269
3968/4849 [=======================>......] - ETA: 1:15 - loss: 0.7164 - acc: 0.5252
4032/4849 [=======================>......] - ETA: 1:09 - loss: 0.7162 - acc: 0.5241
4096/4849 [========================>.....] - ETA: 1:03 - loss: 0.7166 - acc: 0.5222
4160/4849 [========================>.....] - ETA: 58s - loss: 0.7159 - acc: 0.5238 
4224/4849 [=========================>....] - ETA: 52s - loss: 0.7159 - acc: 0.5234
4288/4849 [=========================>....] - ETA: 47s - loss: 0.7157 - acc: 0.5238
4352/4849 [=========================>....] - ETA: 41s - loss: 0.7148 - acc: 0.5260
4416/4849 [==========================>...] - ETA: 36s - loss: 0.7147 - acc: 0.5263
4480/4849 [==========================>...] - ETA: 31s - loss: 0.7145 - acc: 0.5270
4544/4849 [===========================>..] - ETA: 25s - loss: 0.7141 - acc: 0.5275
4608/4849 [===========================>..] - ETA: 20s - loss: 0.7139 - acc: 0.5267
4672/4849 [===========================>..] - ETA: 14s - loss: 0.7138 - acc: 0.5265
4736/4849 [============================>.] - ETA: 9s - loss: 0.7135 - acc: 0.5268 
4800/4849 [============================>.] - ETA: 4s - loss: 0.7132 - acc: 0.5277
4849/4849 [==============================] - 426s 88ms/step - loss: 0.7135 - acc: 0.5273 - val_loss: 0.6857 - val_acc: 0.5640

Epoch 00001: val_acc improved from -inf to 0.56401, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window05/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 6:11 - loss: 0.6955 - acc: 0.5156
 128/4849 [..............................] - ETA: 6:22 - loss: 0.7031 - acc: 0.5234
 192/4849 [>.............................] - ETA: 5:50 - loss: 0.7026 - acc: 0.5312
 256/4849 [>.............................] - ETA: 5:55 - loss: 0.6948 - acc: 0.5430
 320/4849 [>.............................] - ETA: 5:49 - loss: 0.6977 - acc: 0.5250
 384/4849 [=>............................] - ETA: 5:44 - loss: 0.6956 - acc: 0.5286
 448/4849 [=>............................] - ETA: 5:39 - loss: 0.6933 - acc: 0.5379
 512/4849 [==>...........................] - ETA: 5:36 - loss: 0.6940 - acc: 0.5449
 576/4849 [==>...........................] - ETA: 5:28 - loss: 0.6982 - acc: 0.5347
 640/4849 [==>...........................] - ETA: 5:25 - loss: 0.7010 - acc: 0.5234
 704/4849 [===>..........................] - ETA: 5:20 - loss: 0.7005 - acc: 0.5213
 768/4849 [===>..........................] - ETA: 5:14 - loss: 0.7020 - acc: 0.5156
 832/4849 [====>.........................] - ETA: 5:09 - loss: 0.7013 - acc: 0.5180
 896/4849 [====>.........................] - ETA: 5:04 - loss: 0.6998 - acc: 0.5223
 960/4849 [====>.........................] - ETA: 4:59 - loss: 0.6989 - acc: 0.5250
1024/4849 [=====>........................] - ETA: 4:54 - loss: 0.6997 - acc: 0.5225
1088/4849 [=====>........................] - ETA: 4:49 - loss: 0.7009 - acc: 0.5156
1152/4849 [======>.......................] - ETA: 4:44 - loss: 0.7027 - acc: 0.5122
1216/4849 [======>.......................] - ETA: 4:39 - loss: 0.6999 - acc: 0.5189
1280/4849 [======>.......................] - ETA: 4:33 - loss: 0.7004 - acc: 0.5203
1344/4849 [=======>......................] - ETA: 4:30 - loss: 0.6982 - acc: 0.5268
1408/4849 [=======>......................] - ETA: 4:24 - loss: 0.6975 - acc: 0.5298
1472/4849 [========>.....................] - ETA: 4:19 - loss: 0.6984 - acc: 0.5292
1536/4849 [========>.....................] - ETA: 4:15 - loss: 0.7004 - acc: 0.5234
1600/4849 [========>.....................] - ETA: 4:09 - loss: 0.7007 - acc: 0.5219
1664/4849 [=========>....................] - ETA: 4:04 - loss: 0.6991 - acc: 0.5246
1728/4849 [=========>....................] - ETA: 4:01 - loss: 0.7011 - acc: 0.5249
1792/4849 [==========>...................] - ETA: 3:55 - loss: 0.7031 - acc: 0.5218
1856/4849 [==========>...................] - ETA: 3:51 - loss: 0.7016 - acc: 0.5248
1920/4849 [==========>...................] - ETA: 3:46 - loss: 0.7023 - acc: 0.5234
1984/4849 [===========>..................] - ETA: 3:41 - loss: 0.7020 - acc: 0.5232
2048/4849 [===========>..................] - ETA: 3:36 - loss: 0.7015 - acc: 0.5244
2112/4849 [============>.................] - ETA: 3:31 - loss: 0.7022 - acc: 0.5256
2176/4849 [============>.................] - ETA: 3:26 - loss: 0.7016 - acc: 0.5257
2240/4849 [============>.................] - ETA: 3:21 - loss: 0.7014 - acc: 0.5246
2304/4849 [=============>................] - ETA: 3:16 - loss: 0.7017 - acc: 0.5234
2368/4849 [=============>................] - ETA: 3:11 - loss: 0.7010 - acc: 0.5253
2432/4849 [==============>...............] - ETA: 3:06 - loss: 0.7002 - acc: 0.5259
2496/4849 [==============>...............] - ETA: 3:01 - loss: 0.7004 - acc: 0.5240
2560/4849 [==============>...............] - ETA: 2:56 - loss: 0.7002 - acc: 0.5258
2624/4849 [===============>..............] - ETA: 2:51 - loss: 0.7009 - acc: 0.5255
2688/4849 [===============>..............] - ETA: 2:46 - loss: 0.7014 - acc: 0.5238
2752/4849 [================>.............] - ETA: 2:42 - loss: 0.7020 - acc: 0.5222
2816/4849 [================>.............] - ETA: 2:37 - loss: 0.7023 - acc: 0.5213
2880/4849 [================>.............] - ETA: 2:32 - loss: 0.7018 - acc: 0.5222
2944/4849 [=================>............] - ETA: 2:28 - loss: 0.7014 - acc: 0.5221
3008/4849 [=================>............] - ETA: 2:22 - loss: 0.7009 - acc: 0.5243
3072/4849 [==================>...........] - ETA: 2:17 - loss: 0.7005 - acc: 0.5254
3136/4849 [==================>...........] - ETA: 2:13 - loss: 0.7003 - acc: 0.5261
3200/4849 [==================>...........] - ETA: 2:08 - loss: 0.7000 - acc: 0.5247
3264/4849 [===================>..........] - ETA: 2:03 - loss: 0.7005 - acc: 0.5221
3328/4849 [===================>..........] - ETA: 1:57 - loss: 0.7006 - acc: 0.5228
3392/4849 [===================>..........] - ETA: 1:52 - loss: 0.7004 - acc: 0.5239
3456/4849 [====================>.........] - ETA: 1:48 - loss: 0.7002 - acc: 0.5255
3520/4849 [====================>.........] - ETA: 1:43 - loss: 0.6999 - acc: 0.5256
3584/4849 [=====================>........] - ETA: 1:38 - loss: 0.6997 - acc: 0.5262
3648/4849 [=====================>........] - ETA: 1:33 - loss: 0.6999 - acc: 0.5255
3712/4849 [=====================>........] - ETA: 1:28 - loss: 0.7004 - acc: 0.5240
3776/4849 [======================>.......] - ETA: 1:23 - loss: 0.7004 - acc: 0.5246
3840/4849 [======================>.......] - ETA: 1:18 - loss: 0.6998 - acc: 0.5260
3904/4849 [=======================>......] - ETA: 1:13 - loss: 0.7002 - acc: 0.5251
3968/4849 [=======================>......] - ETA: 1:08 - loss: 0.7000 - acc: 0.5257
4032/4849 [=======================>......] - ETA: 1:03 - loss: 0.7001 - acc: 0.5260
4096/4849 [========================>.....] - ETA: 58s - loss: 0.6998 - acc: 0.5259 
4160/4849 [========================>.....] - ETA: 53s - loss: 0.7000 - acc: 0.5252
4224/4849 [=========================>....] - ETA: 48s - loss: 0.7002 - acc: 0.5251
4288/4849 [=========================>....] - ETA: 43s - loss: 0.6999 - acc: 0.5240
4352/4849 [=========================>....] - ETA: 38s - loss: 0.6992 - acc: 0.5257
4416/4849 [==========================>...] - ETA: 33s - loss: 0.6989 - acc: 0.5256
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6988 - acc: 0.5261
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6989 - acc: 0.5260
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6993 - acc: 0.5247
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6989 - acc: 0.5257
4736/4849 [============================>.] - ETA: 8s - loss: 0.6988 - acc: 0.5262 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6988 - acc: 0.5267
4849/4849 [==============================] - 389s 80ms/step - loss: 0.6992 - acc: 0.5257 - val_loss: 0.6910 - val_acc: 0.5399

Epoch 00002: val_acc did not improve from 0.56401
Epoch 3/10

  64/4849 [..............................] - ETA: 6:38 - loss: 0.6976 - acc: 0.4844
 128/4849 [..............................] - ETA: 5:54 - loss: 0.6833 - acc: 0.5234
 192/4849 [>.............................] - ETA: 6:00 - loss: 0.6989 - acc: 0.5104
 256/4849 [>.............................] - ETA: 5:59 - loss: 0.6984 - acc: 0.5234
 320/4849 [>.............................] - ETA: 5:49 - loss: 0.6983 - acc: 0.5312
 384/4849 [=>............................] - ETA: 5:45 - loss: 0.6976 - acc: 0.5234
 448/4849 [=>............................] - ETA: 5:43 - loss: 0.6955 - acc: 0.5201
 512/4849 [==>...........................] - ETA: 5:37 - loss: 0.6975 - acc: 0.5273
 576/4849 [==>...........................] - ETA: 5:32 - loss: 0.6955 - acc: 0.5312
 640/4849 [==>...........................] - ETA: 5:25 - loss: 0.6917 - acc: 0.5406
 704/4849 [===>..........................] - ETA: 5:19 - loss: 0.6905 - acc: 0.5440
 768/4849 [===>..........................] - ETA: 5:15 - loss: 0.6913 - acc: 0.5443
 832/4849 [====>.........................] - ETA: 5:08 - loss: 0.6918 - acc: 0.5421
 896/4849 [====>.........................] - ETA: 5:03 - loss: 0.6924 - acc: 0.5357
 960/4849 [====>.........................] - ETA: 4:56 - loss: 0.6914 - acc: 0.5385
1024/4849 [=====>........................] - ETA: 4:53 - loss: 0.6915 - acc: 0.5371
1088/4849 [=====>........................] - ETA: 4:48 - loss: 0.6918 - acc: 0.5331
1152/4849 [======>.......................] - ETA: 4:42 - loss: 0.6932 - acc: 0.5365
1216/4849 [======>.......................] - ETA: 4:38 - loss: 0.6931 - acc: 0.5370
1280/4849 [======>.......................] - ETA: 4:33 - loss: 0.6930 - acc: 0.5375
1344/4849 [=======>......................] - ETA: 4:28 - loss: 0.6914 - acc: 0.5402
1408/4849 [=======>......................] - ETA: 4:23 - loss: 0.6918 - acc: 0.5355
1472/4849 [========>.....................] - ETA: 4:18 - loss: 0.6935 - acc: 0.5299
1536/4849 [========>.....................] - ETA: 4:13 - loss: 0.6932 - acc: 0.5293
1600/4849 [========>.....................] - ETA: 4:09 - loss: 0.6935 - acc: 0.5281
1664/4849 [=========>....................] - ETA: 4:03 - loss: 0.6941 - acc: 0.5264
1728/4849 [=========>....................] - ETA: 3:59 - loss: 0.6943 - acc: 0.5243
1792/4849 [==========>...................] - ETA: 3:55 - loss: 0.6942 - acc: 0.5262
1856/4849 [==========>...................] - ETA: 3:49 - loss: 0.6929 - acc: 0.5302
1920/4849 [==========>...................] - ETA: 3:45 - loss: 0.6933 - acc: 0.5297
1984/4849 [===========>..................] - ETA: 3:40 - loss: 0.6931 - acc: 0.5307
2048/4849 [===========>..................] - ETA: 3:34 - loss: 0.6926 - acc: 0.5308
2112/4849 [============>.................] - ETA: 3:30 - loss: 0.6927 - acc: 0.5322
2176/4849 [============>.................] - ETA: 3:25 - loss: 0.6927 - acc: 0.5312
2240/4849 [============>.................] - ETA: 3:20 - loss: 0.6921 - acc: 0.5321
2304/4849 [=============>................] - ETA: 3:15 - loss: 0.6916 - acc: 0.5321
2368/4849 [=============>................] - ETA: 3:11 - loss: 0.6916 - acc: 0.5321
2432/4849 [==============>...............] - ETA: 3:05 - loss: 0.6910 - acc: 0.5321
2496/4849 [==============>...............] - ETA: 3:01 - loss: 0.6918 - acc: 0.5300
2560/4849 [==============>...............] - ETA: 2:56 - loss: 0.6918 - acc: 0.5305
2624/4849 [===============>..............] - ETA: 2:51 - loss: 0.6925 - acc: 0.5293
2688/4849 [===============>..............] - ETA: 2:46 - loss: 0.6926 - acc: 0.5290
2752/4849 [================>.............] - ETA: 2:41 - loss: 0.6923 - acc: 0.5294
2816/4849 [================>.............] - ETA: 2:36 - loss: 0.6924 - acc: 0.5295
2880/4849 [================>.............] - ETA: 2:31 - loss: 0.6928 - acc: 0.5295
2944/4849 [=================>............] - ETA: 2:26 - loss: 0.6925 - acc: 0.5292
3008/4849 [=================>............] - ETA: 2:21 - loss: 0.6920 - acc: 0.5306
3072/4849 [==================>...........] - ETA: 2:16 - loss: 0.6917 - acc: 0.5306
3136/4849 [==================>...........] - ETA: 2:11 - loss: 0.6919 - acc: 0.5297
3200/4849 [==================>...........] - ETA: 2:07 - loss: 0.6913 - acc: 0.5303
3264/4849 [===================>..........] - ETA: 2:02 - loss: 0.6913 - acc: 0.5306
3328/4849 [===================>..........] - ETA: 1:57 - loss: 0.6908 - acc: 0.5325
3392/4849 [===================>..........] - ETA: 1:52 - loss: 0.6900 - acc: 0.5336
3456/4849 [====================>.........] - ETA: 1:47 - loss: 0.6901 - acc: 0.5339
3520/4849 [====================>.........] - ETA: 1:42 - loss: 0.6906 - acc: 0.5321
3584/4849 [=====================>........] - ETA: 1:37 - loss: 0.6909 - acc: 0.5329
3648/4849 [=====================>........] - ETA: 1:32 - loss: 0.6907 - acc: 0.5323
3712/4849 [=====================>........] - ETA: 1:27 - loss: 0.6906 - acc: 0.5312
3776/4849 [======================>.......] - ETA: 1:22 - loss: 0.6904 - acc: 0.5318
3840/4849 [======================>.......] - ETA: 1:17 - loss: 0.6912 - acc: 0.5312
3904/4849 [=======================>......] - ETA: 1:12 - loss: 0.6909 - acc: 0.5318
3968/4849 [=======================>......] - ETA: 1:07 - loss: 0.6911 - acc: 0.5307
4032/4849 [=======================>......] - ETA: 1:02 - loss: 0.6912 - acc: 0.5303
4096/4849 [========================>.....] - ETA: 57s - loss: 0.6908 - acc: 0.5310 
4160/4849 [========================>.....] - ETA: 52s - loss: 0.6911 - acc: 0.5305
4224/4849 [=========================>....] - ETA: 47s - loss: 0.6908 - acc: 0.5310
4288/4849 [=========================>....] - ETA: 43s - loss: 0.6908 - acc: 0.5322
4352/4849 [=========================>....] - ETA: 38s - loss: 0.6905 - acc: 0.5324
4416/4849 [==========================>...] - ETA: 33s - loss: 0.6899 - acc: 0.5344
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6897 - acc: 0.5348
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6901 - acc: 0.5343
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6900 - acc: 0.5349
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6903 - acc: 0.5345
4736/4849 [============================>.] - ETA: 8s - loss: 0.6902 - acc: 0.5348 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6902 - acc: 0.5348
4849/4849 [==============================] - 387s 80ms/step - loss: 0.6899 - acc: 0.5360 - val_loss: 0.6916 - val_acc: 0.5158

Epoch 00003: val_acc did not improve from 0.56401
Epoch 4/10

  64/4849 [..............................] - ETA: 6:12 - loss: 0.7156 - acc: 0.5000
 128/4849 [..............................] - ETA: 6:24 - loss: 0.6858 - acc: 0.5156
 192/4849 [>.............................] - ETA: 6:21 - loss: 0.6844 - acc: 0.5417
 256/4849 [>.............................] - ETA: 6:06 - loss: 0.6905 - acc: 0.5195
 320/4849 [>.............................] - ETA: 6:07 - loss: 0.6937 - acc: 0.5250
 384/4849 [=>............................] - ETA: 5:59 - loss: 0.6940 - acc: 0.5260
 448/4849 [=>............................] - ETA: 5:51 - loss: 0.6951 - acc: 0.5268
 512/4849 [==>...........................] - ETA: 5:47 - loss: 0.6913 - acc: 0.5293
 576/4849 [==>...........................] - ETA: 5:43 - loss: 0.6910 - acc: 0.5295
 640/4849 [==>...........................] - ETA: 5:36 - loss: 0.6905 - acc: 0.5281
 704/4849 [===>..........................] - ETA: 5:34 - loss: 0.6879 - acc: 0.5284
 768/4849 [===>..........................] - ETA: 5:27 - loss: 0.6868 - acc: 0.5326
 832/4849 [====>.........................] - ETA: 5:20 - loss: 0.6853 - acc: 0.5409
 896/4849 [====>.........................] - ETA: 5:15 - loss: 0.6830 - acc: 0.5469
 960/4849 [====>.........................] - ETA: 5:09 - loss: 0.6859 - acc: 0.5437
1024/4849 [=====>........................] - ETA: 5:03 - loss: 0.6855 - acc: 0.5430
1088/4849 [=====>........................] - ETA: 4:58 - loss: 0.6826 - acc: 0.5478
1152/4849 [======>.......................] - ETA: 4:51 - loss: 0.6833 - acc: 0.5443
1216/4849 [======>.......................] - ETA: 4:45 - loss: 0.6835 - acc: 0.5469
1280/4849 [======>.......................] - ETA: 4:39 - loss: 0.6861 - acc: 0.5461
1344/4849 [=======>......................] - ETA: 4:32 - loss: 0.6871 - acc: 0.5446
1408/4849 [=======>......................] - ETA: 4:27 - loss: 0.6863 - acc: 0.5469
1472/4849 [========>.....................] - ETA: 4:21 - loss: 0.6866 - acc: 0.5469
1536/4849 [========>.....................] - ETA: 4:16 - loss: 0.6889 - acc: 0.5423
1600/4849 [========>.....................] - ETA: 4:12 - loss: 0.6887 - acc: 0.5437
1664/4849 [=========>....................] - ETA: 4:07 - loss: 0.6886 - acc: 0.5451
1728/4849 [=========>....................] - ETA: 4:00 - loss: 0.6892 - acc: 0.5446
1792/4849 [==========>...................] - ETA: 3:55 - loss: 0.6875 - acc: 0.5463
1856/4849 [==========>...................] - ETA: 3:50 - loss: 0.6890 - acc: 0.5436
1920/4849 [==========>...................] - ETA: 3:45 - loss: 0.6893 - acc: 0.5422
1984/4849 [===========>..................] - ETA: 3:39 - loss: 0.6900 - acc: 0.5449
2048/4849 [===========>..................] - ETA: 3:34 - loss: 0.6897 - acc: 0.5459
2112/4849 [============>.................] - ETA: 3:30 - loss: 0.6890 - acc: 0.5488
2176/4849 [============>.................] - ETA: 3:24 - loss: 0.6884 - acc: 0.5487
2240/4849 [============>.................] - ETA: 3:19 - loss: 0.6882 - acc: 0.5496
2304/4849 [=============>................] - ETA: 3:14 - loss: 0.6886 - acc: 0.5477
2368/4849 [=============>................] - ETA: 3:09 - loss: 0.6892 - acc: 0.5473
2432/4849 [==============>...............] - ETA: 3:04 - loss: 0.6878 - acc: 0.5489
2496/4849 [==============>...............] - ETA: 2:58 - loss: 0.6877 - acc: 0.5489
2560/4849 [==============>...............] - ETA: 2:53 - loss: 0.6874 - acc: 0.5492
2624/4849 [===============>..............] - ETA: 2:48 - loss: 0.6869 - acc: 0.5514
2688/4849 [===============>..............] - ETA: 2:43 - loss: 0.6870 - acc: 0.5506
2752/4849 [================>.............] - ETA: 2:38 - loss: 0.6874 - acc: 0.5498
2816/4849 [================>.............] - ETA: 2:33 - loss: 0.6878 - acc: 0.5490
2880/4849 [================>.............] - ETA: 2:28 - loss: 0.6878 - acc: 0.5486
2944/4849 [=================>............] - ETA: 2:23 - loss: 0.6873 - acc: 0.5493
3008/4849 [=================>............] - ETA: 2:18 - loss: 0.6868 - acc: 0.5519
3072/4849 [==================>...........] - ETA: 2:13 - loss: 0.6874 - acc: 0.5511
3136/4849 [==================>...........] - ETA: 2:08 - loss: 0.6876 - acc: 0.5517
3200/4849 [==================>...........] - ETA: 2:03 - loss: 0.6875 - acc: 0.5519
3264/4849 [===================>..........] - ETA: 1:58 - loss: 0.6882 - acc: 0.5502
3328/4849 [===================>..........] - ETA: 1:53 - loss: 0.6887 - acc: 0.5493
3392/4849 [===================>..........] - ETA: 1:48 - loss: 0.6888 - acc: 0.5501
3456/4849 [====================>.........] - ETA: 1:44 - loss: 0.6893 - acc: 0.5503
3520/4849 [====================>.........] - ETA: 1:39 - loss: 0.6892 - acc: 0.5491
3584/4849 [=====================>........] - ETA: 1:34 - loss: 0.6896 - acc: 0.5483
3648/4849 [=====================>........] - ETA: 1:29 - loss: 0.6898 - acc: 0.5477
3712/4849 [=====================>........] - ETA: 1:25 - loss: 0.6904 - acc: 0.5455
3776/4849 [======================>.......] - ETA: 1:20 - loss: 0.6902 - acc: 0.5461
3840/4849 [======================>.......] - ETA: 1:15 - loss: 0.6895 - acc: 0.5479
3904/4849 [=======================>......] - ETA: 1:10 - loss: 0.6893 - acc: 0.5471
3968/4849 [=======================>......] - ETA: 1:05 - loss: 0.6896 - acc: 0.5449
4032/4849 [=======================>......] - ETA: 1:00 - loss: 0.6897 - acc: 0.5444
4096/4849 [========================>.....] - ETA: 56s - loss: 0.6904 - acc: 0.5430 
4160/4849 [========================>.....] - ETA: 51s - loss: 0.6906 - acc: 0.5421
4224/4849 [=========================>....] - ETA: 46s - loss: 0.6911 - acc: 0.5414
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6912 - acc: 0.5410
4352/4849 [=========================>....] - ETA: 37s - loss: 0.6911 - acc: 0.5407
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6913 - acc: 0.5403
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6913 - acc: 0.5395
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6908 - acc: 0.5401
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6906 - acc: 0.5408
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6902 - acc: 0.5426
4736/4849 [============================>.] - ETA: 8s - loss: 0.6902 - acc: 0.5424 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6899 - acc: 0.5433
4849/4849 [==============================] - 375s 77ms/step - loss: 0.6902 - acc: 0.5426 - val_loss: 0.6908 - val_acc: 0.5473

Epoch 00004: val_acc did not improve from 0.56401
Epoch 5/10

  64/4849 [..............................] - ETA: 5:08 - loss: 0.6853 - acc: 0.5312
 128/4849 [..............................] - ETA: 5:29 - loss: 0.6605 - acc: 0.6172
 192/4849 [>.............................] - ETA: 5:23 - loss: 0.6671 - acc: 0.5885
 256/4849 [>.............................] - ETA: 5:22 - loss: 0.6758 - acc: 0.5742
 320/4849 [>.............................] - ETA: 5:18 - loss: 0.6780 - acc: 0.5625
 384/4849 [=>............................] - ETA: 5:18 - loss: 0.6816 - acc: 0.5521
 448/4849 [=>............................] - ETA: 5:17 - loss: 0.6827 - acc: 0.5558
 512/4849 [==>...........................] - ETA: 5:10 - loss: 0.6788 - acc: 0.5723
 576/4849 [==>...........................] - ETA: 5:08 - loss: 0.6795 - acc: 0.5694
 640/4849 [==>...........................] - ETA: 5:03 - loss: 0.6811 - acc: 0.5547
 704/4849 [===>..........................] - ETA: 4:59 - loss: 0.6805 - acc: 0.5497
 768/4849 [===>..........................] - ETA: 4:52 - loss: 0.6814 - acc: 0.5430
 832/4849 [====>.........................] - ETA: 4:48 - loss: 0.6807 - acc: 0.5469
 896/4849 [====>.........................] - ETA: 4:43 - loss: 0.6822 - acc: 0.5491
 960/4849 [====>.........................] - ETA: 4:38 - loss: 0.6828 - acc: 0.5479
1024/4849 [=====>........................] - ETA: 4:34 - loss: 0.6834 - acc: 0.5439
1088/4849 [=====>........................] - ETA: 4:31 - loss: 0.6853 - acc: 0.5414
1152/4849 [======>.......................] - ETA: 4:27 - loss: 0.6863 - acc: 0.5408
1216/4849 [======>.......................] - ETA: 4:22 - loss: 0.6863 - acc: 0.5403
1280/4849 [======>.......................] - ETA: 4:18 - loss: 0.6857 - acc: 0.5406
1344/4849 [=======>......................] - ETA: 4:15 - loss: 0.6863 - acc: 0.5357
1408/4849 [=======>......................] - ETA: 4:10 - loss: 0.6883 - acc: 0.5291
1472/4849 [========>.....................] - ETA: 4:05 - loss: 0.6873 - acc: 0.5312
1536/4849 [========>.....................] - ETA: 4:02 - loss: 0.6869 - acc: 0.5339
1600/4849 [========>.....................] - ETA: 3:56 - loss: 0.6870 - acc: 0.5331
1664/4849 [=========>....................] - ETA: 3:53 - loss: 0.6872 - acc: 0.5325
1728/4849 [=========>....................] - ETA: 3:48 - loss: 0.6864 - acc: 0.5382
1792/4849 [==========>...................] - ETA: 3:43 - loss: 0.6837 - acc: 0.5441
1856/4849 [==========>...................] - ETA: 3:39 - loss: 0.6832 - acc: 0.5458
1920/4849 [==========>...................] - ETA: 3:34 - loss: 0.6824 - acc: 0.5484
1984/4849 [===========>..................] - ETA: 3:29 - loss: 0.6829 - acc: 0.5479
2048/4849 [===========>..................] - ETA: 3:24 - loss: 0.6825 - acc: 0.5483
2112/4849 [============>.................] - ETA: 3:20 - loss: 0.6843 - acc: 0.5478
2176/4849 [============>.................] - ETA: 3:16 - loss: 0.6847 - acc: 0.5450
2240/4849 [============>.................] - ETA: 3:11 - loss: 0.6862 - acc: 0.5442
2304/4849 [=============>................] - ETA: 3:07 - loss: 0.6872 - acc: 0.5430
2368/4849 [=============>................] - ETA: 3:03 - loss: 0.6849 - acc: 0.5473
2432/4849 [==============>...............] - ETA: 2:58 - loss: 0.6851 - acc: 0.5461
2496/4849 [==============>...............] - ETA: 2:53 - loss: 0.6851 - acc: 0.5453
2560/4849 [==============>...............] - ETA: 2:49 - loss: 0.6854 - acc: 0.5453
2624/4849 [===============>..............] - ETA: 2:44 - loss: 0.6857 - acc: 0.5442
2688/4849 [===============>..............] - ETA: 2:39 - loss: 0.6862 - acc: 0.5435
2752/4849 [================>.............] - ETA: 2:35 - loss: 0.6864 - acc: 0.5451
2816/4849 [================>.............] - ETA: 2:30 - loss: 0.6866 - acc: 0.5458
2880/4849 [================>.............] - ETA: 2:25 - loss: 0.6870 - acc: 0.5444
2944/4849 [=================>............] - ETA: 2:20 - loss: 0.6871 - acc: 0.5445
3008/4849 [=================>............] - ETA: 2:16 - loss: 0.6870 - acc: 0.5449
3072/4849 [==================>...........] - ETA: 2:11 - loss: 0.6870 - acc: 0.5446
3136/4849 [==================>...........] - ETA: 2:06 - loss: 0.6874 - acc: 0.5437
3200/4849 [==================>...........] - ETA: 2:02 - loss: 0.6872 - acc: 0.5450
3264/4849 [===================>..........] - ETA: 1:57 - loss: 0.6870 - acc: 0.5447
3328/4849 [===================>..........] - ETA: 1:52 - loss: 0.6872 - acc: 0.5457
3392/4849 [===================>..........] - ETA: 1:48 - loss: 0.6870 - acc: 0.5457
3456/4849 [====================>.........] - ETA: 1:43 - loss: 0.6875 - acc: 0.5440
3520/4849 [====================>.........] - ETA: 1:38 - loss: 0.6872 - acc: 0.5449
3584/4849 [=====================>........] - ETA: 1:33 - loss: 0.6874 - acc: 0.5449
3648/4849 [=====================>........] - ETA: 1:29 - loss: 0.6874 - acc: 0.5458
3712/4849 [=====================>........] - ETA: 1:24 - loss: 0.6874 - acc: 0.5445
3776/4849 [======================>.......] - ETA: 1:19 - loss: 0.6877 - acc: 0.5440
3840/4849 [======================>.......] - ETA: 1:14 - loss: 0.6877 - acc: 0.5435
3904/4849 [=======================>......] - ETA: 1:10 - loss: 0.6873 - acc: 0.5446
3968/4849 [=======================>......] - ETA: 1:05 - loss: 0.6868 - acc: 0.5464
4032/4849 [=======================>......] - ETA: 1:00 - loss: 0.6871 - acc: 0.5451
4096/4849 [========================>.....] - ETA: 55s - loss: 0.6868 - acc: 0.5459 
4160/4849 [========================>.....] - ETA: 51s - loss: 0.6866 - acc: 0.5474
4224/4849 [=========================>....] - ETA: 46s - loss: 0.6862 - acc: 0.5485
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6861 - acc: 0.5497
4352/4849 [=========================>....] - ETA: 36s - loss: 0.6862 - acc: 0.5489
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6862 - acc: 0.5487
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6865 - acc: 0.5496
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6861 - acc: 0.5500
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6860 - acc: 0.5506
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6853 - acc: 0.5522
4736/4849 [============================>.] - ETA: 8s - loss: 0.6852 - acc: 0.5519 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6851 - acc: 0.5527
4849/4849 [==============================] - 379s 78ms/step - loss: 0.6851 - acc: 0.5525 - val_loss: 0.6953 - val_acc: 0.5455

Epoch 00005: val_acc did not improve from 0.56401
Epoch 6/10

  64/4849 [..............................] - ETA: 6:35 - loss: 0.6468 - acc: 0.6719
 128/4849 [..............................] - ETA: 6:27 - loss: 0.6722 - acc: 0.6094
 192/4849 [>.............................] - ETA: 6:23 - loss: 0.6727 - acc: 0.6094
 256/4849 [>.............................] - ETA: 6:18 - loss: 0.6627 - acc: 0.6250
 320/4849 [>.............................] - ETA: 6:17 - loss: 0.6630 - acc: 0.6219
 384/4849 [=>............................] - ETA: 6:11 - loss: 0.6713 - acc: 0.6016
 448/4849 [=>............................] - ETA: 6:08 - loss: 0.6798 - acc: 0.5826
 512/4849 [==>...........................] - ETA: 6:04 - loss: 0.6764 - acc: 0.5840
 576/4849 [==>...........................] - ETA: 5:58 - loss: 0.6842 - acc: 0.5712
 640/4849 [==>...........................] - ETA: 5:52 - loss: 0.6804 - acc: 0.5766
 704/4849 [===>..........................] - ETA: 5:46 - loss: 0.6821 - acc: 0.5753
 768/4849 [===>..........................] - ETA: 5:41 - loss: 0.6835 - acc: 0.5742
 832/4849 [====>.........................] - ETA: 5:35 - loss: 0.6839 - acc: 0.5793
 896/4849 [====>.........................] - ETA: 5:30 - loss: 0.6824 - acc: 0.5781
 960/4849 [====>.........................] - ETA: 5:24 - loss: 0.6798 - acc: 0.5802
1024/4849 [=====>........................] - ETA: 5:19 - loss: 0.6804 - acc: 0.5791
1088/4849 [=====>........................] - ETA: 5:12 - loss: 0.6814 - acc: 0.5800
1152/4849 [======>.......................] - ETA: 5:07 - loss: 0.6803 - acc: 0.5825
1216/4849 [======>.......................] - ETA: 5:03 - loss: 0.6805 - acc: 0.5789
1280/4849 [======>.......................] - ETA: 4:57 - loss: 0.6810 - acc: 0.5766
1344/4849 [=======>......................] - ETA: 4:50 - loss: 0.6805 - acc: 0.5759
1408/4849 [=======>......................] - ETA: 4:45 - loss: 0.6841 - acc: 0.5682
1472/4849 [========>.....................] - ETA: 4:39 - loss: 0.6857 - acc: 0.5659
1536/4849 [========>.....................] - ETA: 4:34 - loss: 0.6858 - acc: 0.5638
1600/4849 [========>.....................] - ETA: 4:29 - loss: 0.6852 - acc: 0.5625
1664/4849 [=========>....................] - ETA: 4:23 - loss: 0.6850 - acc: 0.5625
1728/4849 [=========>....................] - ETA: 4:19 - loss: 0.6849 - acc: 0.5642
1792/4849 [==========>...................] - ETA: 4:14 - loss: 0.6840 - acc: 0.5658
1856/4849 [==========>...................] - ETA: 4:08 - loss: 0.6835 - acc: 0.5679
1920/4849 [==========>...................] - ETA: 4:02 - loss: 0.6841 - acc: 0.5672
1984/4849 [===========>..................] - ETA: 3:58 - loss: 0.6855 - acc: 0.5665
2048/4849 [===========>..................] - ETA: 3:53 - loss: 0.6858 - acc: 0.5654
2112/4849 [============>.................] - ETA: 3:47 - loss: 0.6849 - acc: 0.5677
2176/4849 [============>.................] - ETA: 3:42 - loss: 0.6835 - acc: 0.5708
2240/4849 [============>.................] - ETA: 3:37 - loss: 0.6835 - acc: 0.5701
2304/4849 [=============>................] - ETA: 3:31 - loss: 0.6831 - acc: 0.5703
2368/4849 [=============>................] - ETA: 3:26 - loss: 0.6826 - acc: 0.5714
2432/4849 [==============>...............] - ETA: 3:20 - loss: 0.6824 - acc: 0.5724
2496/4849 [==============>...............] - ETA: 3:15 - loss: 0.6819 - acc: 0.5729
2560/4849 [==============>...............] - ETA: 3:10 - loss: 0.6812 - acc: 0.5754
2624/4849 [===============>..............] - ETA: 3:04 - loss: 0.6813 - acc: 0.5747
2688/4849 [===============>..............] - ETA: 2:59 - loss: 0.6807 - acc: 0.5766
2752/4849 [================>.............] - ETA: 2:54 - loss: 0.6799 - acc: 0.5789
2816/4849 [================>.............] - ETA: 2:49 - loss: 0.6796 - acc: 0.5785
2880/4849 [================>.............] - ETA: 2:43 - loss: 0.6797 - acc: 0.5771
2944/4849 [=================>............] - ETA: 2:38 - loss: 0.6797 - acc: 0.5764
3008/4849 [=================>............] - ETA: 2:32 - loss: 0.6799 - acc: 0.5768
3072/4849 [==================>...........] - ETA: 2:27 - loss: 0.6801 - acc: 0.5768
3136/4849 [==================>...........] - ETA: 2:22 - loss: 0.6811 - acc: 0.5753
3200/4849 [==================>...........] - ETA: 2:16 - loss: 0.6815 - acc: 0.5747
3264/4849 [===================>..........] - ETA: 2:11 - loss: 0.6817 - acc: 0.5744
3328/4849 [===================>..........] - ETA: 2:06 - loss: 0.6813 - acc: 0.5748
3392/4849 [===================>..........] - ETA: 2:00 - loss: 0.6814 - acc: 0.5749
3456/4849 [====================>.........] - ETA: 1:55 - loss: 0.6811 - acc: 0.5735
3520/4849 [====================>.........] - ETA: 1:49 - loss: 0.6822 - acc: 0.5713
3584/4849 [=====================>........] - ETA: 1:44 - loss: 0.6821 - acc: 0.5709
3648/4849 [=====================>........] - ETA: 1:39 - loss: 0.6824 - acc: 0.5707
3712/4849 [=====================>........] - ETA: 1:34 - loss: 0.6819 - acc: 0.5719
3776/4849 [======================>.......] - ETA: 1:28 - loss: 0.6821 - acc: 0.5712
3840/4849 [======================>.......] - ETA: 1:23 - loss: 0.6829 - acc: 0.5701
3904/4849 [=======================>......] - ETA: 1:18 - loss: 0.6828 - acc: 0.5704
3968/4849 [=======================>......] - ETA: 1:12 - loss: 0.6830 - acc: 0.5693
4032/4849 [=======================>......] - ETA: 1:07 - loss: 0.6842 - acc: 0.5670
4096/4849 [========================>.....] - ETA: 1:02 - loss: 0.6845 - acc: 0.5657
4160/4849 [========================>.....] - ETA: 56s - loss: 0.6847 - acc: 0.5647 
4224/4849 [=========================>....] - ETA: 51s - loss: 0.6848 - acc: 0.5649
4288/4849 [=========================>....] - ETA: 46s - loss: 0.6854 - acc: 0.5625
4352/4849 [=========================>....] - ETA: 40s - loss: 0.6857 - acc: 0.5611
4416/4849 [==========================>...] - ETA: 35s - loss: 0.6853 - acc: 0.5614
4480/4849 [==========================>...] - ETA: 30s - loss: 0.6852 - acc: 0.5607
4544/4849 [===========================>..] - ETA: 25s - loss: 0.6855 - acc: 0.5601
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6857 - acc: 0.5595
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6855 - acc: 0.5604
4736/4849 [============================>.] - ETA: 9s - loss: 0.6855 - acc: 0.5610 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6860 - acc: 0.5600
4849/4849 [==============================] - 414s 85ms/step - loss: 0.6863 - acc: 0.5599 - val_loss: 0.6925 - val_acc: 0.5455

Epoch 00006: val_acc did not improve from 0.56401
Epoch 7/10

  64/4849 [..............................] - ETA: 6:45 - loss: 0.7076 - acc: 0.5938
 128/4849 [..............................] - ETA: 6:29 - loss: 0.7046 - acc: 0.5469
 192/4849 [>.............................] - ETA: 6:21 - loss: 0.7085 - acc: 0.5260
 256/4849 [>.............................] - ETA: 6:23 - loss: 0.6940 - acc: 0.5586
 320/4849 [>.............................] - ETA: 6:11 - loss: 0.6933 - acc: 0.5531
 384/4849 [=>............................] - ETA: 6:06 - loss: 0.6936 - acc: 0.5521
 448/4849 [=>............................] - ETA: 5:58 - loss: 0.6913 - acc: 0.5580
 512/4849 [==>...........................] - ETA: 5:55 - loss: 0.6942 - acc: 0.5527
 576/4849 [==>...........................] - ETA: 5:47 - loss: 0.6910 - acc: 0.5590
 640/4849 [==>...........................] - ETA: 5:45 - loss: 0.6879 - acc: 0.5625
 704/4849 [===>..........................] - ETA: 5:40 - loss: 0.6849 - acc: 0.5639
 768/4849 [===>..........................] - ETA: 5:35 - loss: 0.6864 - acc: 0.5612
 832/4849 [====>.........................] - ETA: 5:29 - loss: 0.6851 - acc: 0.5637
 896/4849 [====>.........................] - ETA: 5:24 - loss: 0.6816 - acc: 0.5725
 960/4849 [====>.........................] - ETA: 5:18 - loss: 0.6806 - acc: 0.5719
1024/4849 [=====>........................] - ETA: 5:13 - loss: 0.6816 - acc: 0.5654
1088/4849 [=====>........................] - ETA: 5:08 - loss: 0.6812 - acc: 0.5671
1152/4849 [======>.......................] - ETA: 5:04 - loss: 0.6811 - acc: 0.5660
1216/4849 [======>.......................] - ETA: 4:59 - loss: 0.6813 - acc: 0.5625
1280/4849 [======>.......................] - ETA: 4:54 - loss: 0.6822 - acc: 0.5602
1344/4849 [=======>......................] - ETA: 4:47 - loss: 0.6826 - acc: 0.5595
1408/4849 [=======>......................] - ETA: 4:42 - loss: 0.6826 - acc: 0.5582
1472/4849 [========>.....................] - ETA: 4:36 - loss: 0.6826 - acc: 0.5598
1536/4849 [========>.....................] - ETA: 4:31 - loss: 0.6817 - acc: 0.5612
1600/4849 [========>.....................] - ETA: 4:26 - loss: 0.6824 - acc: 0.5619
1664/4849 [=========>....................] - ETA: 4:20 - loss: 0.6827 - acc: 0.5619
1728/4849 [=========>....................] - ETA: 4:15 - loss: 0.6817 - acc: 0.5642
1792/4849 [==========>...................] - ETA: 4:10 - loss: 0.6819 - acc: 0.5642
1856/4849 [==========>...................] - ETA: 4:04 - loss: 0.6808 - acc: 0.5663
1920/4849 [==========>...................] - ETA: 4:00 - loss: 0.6805 - acc: 0.5682
1984/4849 [===========>..................] - ETA: 3:54 - loss: 0.6795 - acc: 0.5701
2048/4849 [===========>..................] - ETA: 3:49 - loss: 0.6790 - acc: 0.5728
2112/4849 [============>.................] - ETA: 3:44 - loss: 0.6799 - acc: 0.5720
2176/4849 [============>.................] - ETA: 3:38 - loss: 0.6814 - acc: 0.5685
2240/4849 [============>.................] - ETA: 3:33 - loss: 0.6814 - acc: 0.5710
2304/4849 [=============>................] - ETA: 3:28 - loss: 0.6815 - acc: 0.5703
2368/4849 [=============>................] - ETA: 3:22 - loss: 0.6824 - acc: 0.5697
2432/4849 [==============>...............] - ETA: 3:18 - loss: 0.6815 - acc: 0.5687
2496/4849 [==============>...............] - ETA: 3:12 - loss: 0.6818 - acc: 0.5685
2560/4849 [==============>...............] - ETA: 3:07 - loss: 0.6809 - acc: 0.5703
2624/4849 [===============>..............] - ETA: 3:01 - loss: 0.6813 - acc: 0.5690
2688/4849 [===============>..............] - ETA: 2:56 - loss: 0.6819 - acc: 0.5673
2752/4849 [================>.............] - ETA: 2:51 - loss: 0.6817 - acc: 0.5680
2816/4849 [================>.............] - ETA: 2:45 - loss: 0.6819 - acc: 0.5671
2880/4849 [================>.............] - ETA: 2:40 - loss: 0.6814 - acc: 0.5687
2944/4849 [=================>............] - ETA: 2:35 - loss: 0.6809 - acc: 0.5693
3008/4849 [=================>............] - ETA: 2:30 - loss: 0.6805 - acc: 0.5705
3072/4849 [==================>...........] - ETA: 2:25 - loss: 0.6800 - acc: 0.5729
3136/4849 [==================>...........] - ETA: 2:19 - loss: 0.6798 - acc: 0.5737
3200/4849 [==================>...........] - ETA: 2:14 - loss: 0.6803 - acc: 0.5713
3264/4849 [===================>..........] - ETA: 2:09 - loss: 0.6804 - acc: 0.5702
3328/4849 [===================>..........] - ETA: 2:04 - loss: 0.6801 - acc: 0.5700
3392/4849 [===================>..........] - ETA: 1:59 - loss: 0.6797 - acc: 0.5719
3456/4849 [====================>.........] - ETA: 1:53 - loss: 0.6801 - acc: 0.5726
3520/4849 [====================>.........] - ETA: 1:48 - loss: 0.6804 - acc: 0.5716
3584/4849 [=====================>........] - ETA: 1:43 - loss: 0.6804 - acc: 0.5709
3648/4849 [=====================>........] - ETA: 1:38 - loss: 0.6800 - acc: 0.5715
3712/4849 [=====================>........] - ETA: 1:33 - loss: 0.6798 - acc: 0.5717
3776/4849 [======================>.......] - ETA: 1:27 - loss: 0.6796 - acc: 0.5720
3840/4849 [======================>.......] - ETA: 1:22 - loss: 0.6796 - acc: 0.5714
3904/4849 [=======================>......] - ETA: 1:17 - loss: 0.6791 - acc: 0.5730
3968/4849 [=======================>......] - ETA: 1:12 - loss: 0.6787 - acc: 0.5731
4032/4849 [=======================>......] - ETA: 1:06 - loss: 0.6788 - acc: 0.5742
4096/4849 [========================>.....] - ETA: 1:01 - loss: 0.6796 - acc: 0.5732
4160/4849 [========================>.....] - ETA: 56s - loss: 0.6796 - acc: 0.5738 
4224/4849 [=========================>....] - ETA: 51s - loss: 0.6789 - acc: 0.5739
4288/4849 [=========================>....] - ETA: 45s - loss: 0.6788 - acc: 0.5744
4352/4849 [=========================>....] - ETA: 40s - loss: 0.6782 - acc: 0.5758
4416/4849 [==========================>...] - ETA: 35s - loss: 0.6790 - acc: 0.5745
4480/4849 [==========================>...] - ETA: 30s - loss: 0.6783 - acc: 0.5757
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6784 - acc: 0.5766
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6784 - acc: 0.5764
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6786 - acc: 0.5768
4736/4849 [============================>.] - ETA: 9s - loss: 0.6785 - acc: 0.5769 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6779 - acc: 0.5781
4849/4849 [==============================] - 412s 85ms/step - loss: 0.6779 - acc: 0.5785 - val_loss: 0.6992 - val_acc: 0.5325

Epoch 00007: val_acc did not improve from 0.56401
Epoch 8/10

  64/4849 [..............................] - ETA: 6:35 - loss: 0.6209 - acc: 0.6719
 128/4849 [..............................] - ETA: 6:23 - loss: 0.6535 - acc: 0.6641
 192/4849 [>.............................] - ETA: 6:27 - loss: 0.6508 - acc: 0.6667
 256/4849 [>.............................] - ETA: 6:25 - loss: 0.6554 - acc: 0.6562
 320/4849 [>.............................] - ETA: 6:15 - loss: 0.6555 - acc: 0.6438
 384/4849 [=>............................] - ETA: 6:15 - loss: 0.6593 - acc: 0.6406
 448/4849 [=>............................] - ETA: 6:10 - loss: 0.6588 - acc: 0.6429
 512/4849 [==>...........................] - ETA: 6:02 - loss: 0.6635 - acc: 0.6270
 576/4849 [==>...........................] - ETA: 5:55 - loss: 0.6611 - acc: 0.6267
 640/4849 [==>...........................] - ETA: 5:50 - loss: 0.6637 - acc: 0.6188
 704/4849 [===>..........................] - ETA: 5:44 - loss: 0.6642 - acc: 0.6151
 768/4849 [===>..........................] - ETA: 5:39 - loss: 0.6684 - acc: 0.6055
 832/4849 [====>.........................] - ETA: 5:34 - loss: 0.6686 - acc: 0.6046
 896/4849 [====>.........................] - ETA: 5:28 - loss: 0.6701 - acc: 0.6071
 960/4849 [====>.........................] - ETA: 5:22 - loss: 0.6735 - acc: 0.5979
1024/4849 [=====>........................] - ETA: 5:17 - loss: 0.6764 - acc: 0.5898
1088/4849 [=====>........................] - ETA: 5:12 - loss: 0.6750 - acc: 0.5901
1152/4849 [======>.......................] - ETA: 5:07 - loss: 0.6757 - acc: 0.5868
1216/4849 [======>.......................] - ETA: 5:02 - loss: 0.6760 - acc: 0.5839
1280/4849 [======>.......................] - ETA: 4:56 - loss: 0.6755 - acc: 0.5867
1344/4849 [=======>......................] - ETA: 4:51 - loss: 0.6751 - acc: 0.5856
1408/4849 [=======>......................] - ETA: 4:46 - loss: 0.6761 - acc: 0.5838
1472/4849 [========>.....................] - ETA: 4:41 - loss: 0.6765 - acc: 0.5836
1536/4849 [========>.....................] - ETA: 4:36 - loss: 0.6763 - acc: 0.5846
1600/4849 [========>.....................] - ETA: 4:31 - loss: 0.6772 - acc: 0.5837
1664/4849 [=========>....................] - ETA: 4:25 - loss: 0.6772 - acc: 0.5847
1728/4849 [=========>....................] - ETA: 4:20 - loss: 0.6779 - acc: 0.5839
1792/4849 [==========>...................] - ETA: 4:14 - loss: 0.6753 - acc: 0.5876
1856/4849 [==========>...................] - ETA: 4:09 - loss: 0.6747 - acc: 0.5873
1920/4849 [==========>...................] - ETA: 4:04 - loss: 0.6743 - acc: 0.5896
1984/4849 [===========>..................] - ETA: 3:59 - loss: 0.6734 - acc: 0.5917
2048/4849 [===========>..................] - ETA: 3:53 - loss: 0.6750 - acc: 0.5903
2112/4849 [============>.................] - ETA: 3:48 - loss: 0.6751 - acc: 0.5904
2176/4849 [============>.................] - ETA: 3:43 - loss: 0.6737 - acc: 0.5942
2240/4849 [============>.................] - ETA: 3:37 - loss: 0.6728 - acc: 0.5960
2304/4849 [=============>................] - ETA: 3:32 - loss: 0.6736 - acc: 0.5938
2368/4849 [=============>................] - ETA: 3:26 - loss: 0.6745 - acc: 0.5912
2432/4849 [==============>...............] - ETA: 3:20 - loss: 0.6738 - acc: 0.5929
2496/4849 [==============>...............] - ETA: 3:15 - loss: 0.6744 - acc: 0.5933
2560/4849 [==============>...............] - ETA: 3:10 - loss: 0.6757 - acc: 0.5898
2624/4849 [===============>..............] - ETA: 3:04 - loss: 0.6756 - acc: 0.5884
2688/4849 [===============>..............] - ETA: 2:59 - loss: 0.6764 - acc: 0.5863
2752/4849 [================>.............] - ETA: 2:54 - loss: 0.6754 - acc: 0.5883
2816/4849 [================>.............] - ETA: 2:48 - loss: 0.6766 - acc: 0.5866
2880/4849 [================>.............] - ETA: 2:43 - loss: 0.6755 - acc: 0.5878
2944/4849 [=================>............] - ETA: 2:38 - loss: 0.6758 - acc: 0.5873
3008/4849 [=================>............] - ETA: 2:32 - loss: 0.6757 - acc: 0.5878
3072/4849 [==================>...........] - ETA: 2:27 - loss: 0.6751 - acc: 0.5866
3136/4849 [==================>...........] - ETA: 2:22 - loss: 0.6750 - acc: 0.5861
3200/4849 [==================>...........] - ETA: 2:16 - loss: 0.6751 - acc: 0.5863
3264/4849 [===================>..........] - ETA: 2:11 - loss: 0.6757 - acc: 0.5849
3328/4849 [===================>..........] - ETA: 2:06 - loss: 0.6752 - acc: 0.5865
3392/4849 [===================>..........] - ETA: 2:00 - loss: 0.6747 - acc: 0.5876
3456/4849 [====================>.........] - ETA: 1:55 - loss: 0.6756 - acc: 0.5848
3520/4849 [====================>.........] - ETA: 1:50 - loss: 0.6754 - acc: 0.5849
3584/4849 [=====================>........] - ETA: 1:44 - loss: 0.6752 - acc: 0.5851
3648/4849 [=====================>........] - ETA: 1:39 - loss: 0.6751 - acc: 0.5842
3712/4849 [=====================>........] - ETA: 1:34 - loss: 0.6753 - acc: 0.5835
3776/4849 [======================>.......] - ETA: 1:28 - loss: 0.6759 - acc: 0.5832
3840/4849 [======================>.......] - ETA: 1:23 - loss: 0.6757 - acc: 0.5844
3904/4849 [=======================>......] - ETA: 1:18 - loss: 0.6760 - acc: 0.5830
3968/4849 [=======================>......] - ETA: 1:12 - loss: 0.6765 - acc: 0.5811
4032/4849 [=======================>......] - ETA: 1:07 - loss: 0.6759 - acc: 0.5833
4096/4849 [========================>.....] - ETA: 1:02 - loss: 0.6767 - acc: 0.5830
4160/4849 [========================>.....] - ETA: 56s - loss: 0.6758 - acc: 0.5856 
4224/4849 [=========================>....] - ETA: 51s - loss: 0.6764 - acc: 0.5848
4288/4849 [=========================>....] - ETA: 46s - loss: 0.6761 - acc: 0.5849
4352/4849 [=========================>....] - ETA: 41s - loss: 0.6766 - acc: 0.5839
4416/4849 [==========================>...] - ETA: 35s - loss: 0.6767 - acc: 0.5840
4480/4849 [==========================>...] - ETA: 30s - loss: 0.6764 - acc: 0.5848
4544/4849 [===========================>..] - ETA: 25s - loss: 0.6763 - acc: 0.5854
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6758 - acc: 0.5864
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6755 - acc: 0.5863
4736/4849 [============================>.] - ETA: 9s - loss: 0.6755 - acc: 0.5868 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6756 - acc: 0.5865
4849/4849 [==============================] - 416s 86ms/step - loss: 0.6759 - acc: 0.5861 - val_loss: 0.6866 - val_acc: 0.5733

Epoch 00008: val_acc improved from 0.56401 to 0.57328, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window05/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 9/10

  64/4849 [..............................] - ETA: 7:09 - loss: 0.6887 - acc: 0.5625
 128/4849 [..............................] - ETA: 6:44 - loss: 0.6752 - acc: 0.6094
 192/4849 [>.............................] - ETA: 6:32 - loss: 0.6651 - acc: 0.6146
 256/4849 [>.............................] - ETA: 6:21 - loss: 0.6616 - acc: 0.6367
 320/4849 [>.............................] - ETA: 6:16 - loss: 0.6628 - acc: 0.6250
 384/4849 [=>............................] - ETA: 6:08 - loss: 0.6611 - acc: 0.6250
 448/4849 [=>............................] - ETA: 6:12 - loss: 0.6594 - acc: 0.6183
 512/4849 [==>...........................] - ETA: 6:05 - loss: 0.6629 - acc: 0.6055
 576/4849 [==>...........................] - ETA: 5:55 - loss: 0.6592 - acc: 0.6059
 640/4849 [==>...........................] - ETA: 5:44 - loss: 0.6603 - acc: 0.6047
 704/4849 [===>..........................] - ETA: 5:34 - loss: 0.6634 - acc: 0.6009
 768/4849 [===>..........................] - ETA: 5:25 - loss: 0.6623 - acc: 0.6029
 832/4849 [====>.........................] - ETA: 5:19 - loss: 0.6636 - acc: 0.5998
 896/4849 [====>.........................] - ETA: 5:13 - loss: 0.6660 - acc: 0.5949
 960/4849 [====>.........................] - ETA: 5:05 - loss: 0.6676 - acc: 0.5969
1024/4849 [=====>........................] - ETA: 5:00 - loss: 0.6705 - acc: 0.5908
1088/4849 [=====>........................] - ETA: 4:52 - loss: 0.6724 - acc: 0.5892
1152/4849 [======>.......................] - ETA: 4:45 - loss: 0.6727 - acc: 0.5877
1216/4849 [======>.......................] - ETA: 4:38 - loss: 0.6730 - acc: 0.5863
1280/4849 [======>.......................] - ETA: 4:33 - loss: 0.6726 - acc: 0.5828
1344/4849 [=======>......................] - ETA: 4:26 - loss: 0.6736 - acc: 0.5833
1408/4849 [=======>......................] - ETA: 4:21 - loss: 0.6709 - acc: 0.5881
1472/4849 [========>.....................] - ETA: 4:15 - loss: 0.6713 - acc: 0.5904
1536/4849 [========>.....................] - ETA: 4:09 - loss: 0.6717 - acc: 0.5885
1600/4849 [========>.....................] - ETA: 4:04 - loss: 0.6717 - acc: 0.5906
1664/4849 [=========>....................] - ETA: 3:59 - loss: 0.6699 - acc: 0.5938
1728/4849 [=========>....................] - ETA: 3:53 - loss: 0.6705 - acc: 0.5926
1792/4849 [==========>...................] - ETA: 3:49 - loss: 0.6687 - acc: 0.5954
1856/4849 [==========>...................] - ETA: 3:44 - loss: 0.6687 - acc: 0.5938
1920/4849 [==========>...................] - ETA: 3:39 - loss: 0.6686 - acc: 0.5948
1984/4849 [===========>..................] - ETA: 3:34 - loss: 0.6684 - acc: 0.5963
2048/4849 [===========>..................] - ETA: 3:29 - loss: 0.6689 - acc: 0.5952
2112/4849 [============>.................] - ETA: 3:24 - loss: 0.6705 - acc: 0.5909
2176/4849 [============>.................] - ETA: 3:19 - loss: 0.6718 - acc: 0.5896
2240/4849 [============>.................] - ETA: 3:14 - loss: 0.6724 - acc: 0.5893
2304/4849 [=============>................] - ETA: 3:10 - loss: 0.6724 - acc: 0.5898
2368/4849 [=============>................] - ETA: 3:04 - loss: 0.6721 - acc: 0.5904
2432/4849 [==============>...............] - ETA: 2:59 - loss: 0.6729 - acc: 0.5888
2496/4849 [==============>...............] - ETA: 2:55 - loss: 0.6748 - acc: 0.5845
2560/4849 [==============>...............] - ETA: 2:50 - loss: 0.6748 - acc: 0.5844
2624/4849 [===============>..............] - ETA: 2:45 - loss: 0.6760 - acc: 0.5819
2688/4849 [===============>..............] - ETA: 2:40 - loss: 0.6757 - acc: 0.5815
2752/4849 [================>.............] - ETA: 2:35 - loss: 0.6760 - acc: 0.5799
2816/4849 [================>.............] - ETA: 2:30 - loss: 0.6764 - acc: 0.5778
2880/4849 [================>.............] - ETA: 2:26 - loss: 0.6767 - acc: 0.5757
2944/4849 [=================>............] - ETA: 2:21 - loss: 0.6758 - acc: 0.5791
3008/4849 [=================>............] - ETA: 2:16 - loss: 0.6759 - acc: 0.5791
3072/4849 [==================>...........] - ETA: 2:12 - loss: 0.6760 - acc: 0.5785
3136/4849 [==================>...........] - ETA: 2:07 - loss: 0.6761 - acc: 0.5772
3200/4849 [==================>...........] - ETA: 2:02 - loss: 0.6768 - acc: 0.5744
3264/4849 [===================>..........] - ETA: 1:57 - loss: 0.6768 - acc: 0.5744
3328/4849 [===================>..........] - ETA: 1:52 - loss: 0.6764 - acc: 0.5766
3392/4849 [===================>..........] - ETA: 1:48 - loss: 0.6762 - acc: 0.5781
3456/4849 [====================>.........] - ETA: 1:43 - loss: 0.6761 - acc: 0.5778
3520/4849 [====================>.........] - ETA: 1:38 - loss: 0.6766 - acc: 0.5770
3584/4849 [=====================>........] - ETA: 1:33 - loss: 0.6764 - acc: 0.5776
3648/4849 [=====================>........] - ETA: 1:29 - loss: 0.6762 - acc: 0.5784
3712/4849 [=====================>........] - ETA: 1:24 - loss: 0.6767 - acc: 0.5776
3776/4849 [======================>.......] - ETA: 1:19 - loss: 0.6768 - acc: 0.5779
3840/4849 [======================>.......] - ETA: 1:14 - loss: 0.6770 - acc: 0.5766
3904/4849 [=======================>......] - ETA: 1:10 - loss: 0.6771 - acc: 0.5753
3968/4849 [=======================>......] - ETA: 1:05 - loss: 0.6775 - acc: 0.5733
4032/4849 [=======================>......] - ETA: 1:00 - loss: 0.6769 - acc: 0.5747
4096/4849 [========================>.....] - ETA: 55s - loss: 0.6768 - acc: 0.5742 
4160/4849 [========================>.....] - ETA: 51s - loss: 0.6771 - acc: 0.5740
4224/4849 [=========================>....] - ETA: 46s - loss: 0.6773 - acc: 0.5739
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6776 - acc: 0.5737
4352/4849 [=========================>....] - ETA: 36s - loss: 0.6775 - acc: 0.5738
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6778 - acc: 0.5743
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6775 - acc: 0.5748
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6772 - acc: 0.5759
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6764 - acc: 0.5773
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6770 - acc: 0.5762
4736/4849 [============================>.] - ETA: 8s - loss: 0.6770 - acc: 0.5760 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6770 - acc: 0.5767
4849/4849 [==============================] - 376s 78ms/step - loss: 0.6766 - acc: 0.5772 - val_loss: 0.6955 - val_acc: 0.5417

Epoch 00009: val_acc did not improve from 0.57328
Epoch 10/10

  64/4849 [..............................] - ETA: 6:07 - loss: 0.6447 - acc: 0.6094
 128/4849 [..............................] - ETA: 5:54 - loss: 0.6713 - acc: 0.6094
 192/4849 [>.............................] - ETA: 5:40 - loss: 0.6808 - acc: 0.5677
 256/4849 [>.............................] - ETA: 5:42 - loss: 0.6851 - acc: 0.5703
 320/4849 [>.............................] - ETA: 5:32 - loss: 0.6834 - acc: 0.5750
 384/4849 [=>............................] - ETA: 5:43 - loss: 0.6793 - acc: 0.5859
 448/4849 [=>............................] - ETA: 5:36 - loss: 0.6767 - acc: 0.5960
 512/4849 [==>...........................] - ETA: 5:32 - loss: 0.6814 - acc: 0.5820
 576/4849 [==>...........................] - ETA: 5:27 - loss: 0.6856 - acc: 0.5747
 640/4849 [==>...........................] - ETA: 5:22 - loss: 0.6883 - acc: 0.5656
 704/4849 [===>..........................] - ETA: 5:17 - loss: 0.6870 - acc: 0.5696
 768/4849 [===>..........................] - ETA: 5:10 - loss: 0.6832 - acc: 0.5768
 832/4849 [====>.........................] - ETA: 5:07 - loss: 0.6841 - acc: 0.5745
 896/4849 [====>.........................] - ETA: 5:01 - loss: 0.6839 - acc: 0.5759
 960/4849 [====>.........................] - ETA: 4:57 - loss: 0.6836 - acc: 0.5771
1024/4849 [=====>........................] - ETA: 4:51 - loss: 0.6846 - acc: 0.5723
1088/4849 [=====>........................] - ETA: 4:47 - loss: 0.6824 - acc: 0.5763
1152/4849 [======>.......................] - ETA: 4:41 - loss: 0.6850 - acc: 0.5694
1216/4849 [======>.......................] - ETA: 4:37 - loss: 0.6830 - acc: 0.5715
1280/4849 [======>.......................] - ETA: 4:30 - loss: 0.6851 - acc: 0.5664
1344/4849 [=======>......................] - ETA: 4:26 - loss: 0.6844 - acc: 0.5677
1408/4849 [=======>......................] - ETA: 4:22 - loss: 0.6845 - acc: 0.5653
1472/4849 [========>.....................] - ETA: 4:16 - loss: 0.6839 - acc: 0.5666
1536/4849 [========>.....................] - ETA: 4:12 - loss: 0.6829 - acc: 0.5671
1600/4849 [========>.....................] - ETA: 4:07 - loss: 0.6816 - acc: 0.5687
1664/4849 [=========>....................] - ETA: 4:03 - loss: 0.6825 - acc: 0.5667
1728/4849 [=========>....................] - ETA: 3:58 - loss: 0.6829 - acc: 0.5671
1792/4849 [==========>...................] - ETA: 3:53 - loss: 0.6823 - acc: 0.5664
1856/4849 [==========>...................] - ETA: 3:48 - loss: 0.6812 - acc: 0.5695
1920/4849 [==========>...................] - ETA: 3:43 - loss: 0.6820 - acc: 0.5661
1984/4849 [===========>..................] - ETA: 3:39 - loss: 0.6815 - acc: 0.5680
2048/4849 [===========>..................] - ETA: 3:35 - loss: 0.6814 - acc: 0.5698
2112/4849 [============>.................] - ETA: 3:29 - loss: 0.6823 - acc: 0.5687
2176/4849 [============>.................] - ETA: 3:24 - loss: 0.6833 - acc: 0.5676
2240/4849 [============>.................] - ETA: 3:19 - loss: 0.6829 - acc: 0.5679
2304/4849 [=============>................] - ETA: 3:14 - loss: 0.6842 - acc: 0.5681
2368/4849 [=============>................] - ETA: 3:09 - loss: 0.6856 - acc: 0.5663
2432/4849 [==============>...............] - ETA: 3:04 - loss: 0.6846 - acc: 0.5670
2496/4849 [==============>...............] - ETA: 2:59 - loss: 0.6838 - acc: 0.5677
2560/4849 [==============>...............] - ETA: 2:54 - loss: 0.6845 - acc: 0.5676
2624/4849 [===============>..............] - ETA: 2:49 - loss: 0.6842 - acc: 0.5682
2688/4849 [===============>..............] - ETA: 2:44 - loss: 0.6846 - acc: 0.5677
2752/4849 [================>.............] - ETA: 2:39 - loss: 0.6841 - acc: 0.5676
2816/4849 [================>.............] - ETA: 2:34 - loss: 0.6831 - acc: 0.5700
2880/4849 [================>.............] - ETA: 2:29 - loss: 0.6832 - acc: 0.5687
2944/4849 [=================>............] - ETA: 2:24 - loss: 0.6827 - acc: 0.5683
3008/4849 [=================>............] - ETA: 2:20 - loss: 0.6827 - acc: 0.5695
3072/4849 [==================>...........] - ETA: 2:14 - loss: 0.6832 - acc: 0.5687
3136/4849 [==================>...........] - ETA: 2:10 - loss: 0.6827 - acc: 0.5698
3200/4849 [==================>...........] - ETA: 2:05 - loss: 0.6832 - acc: 0.5687
3264/4849 [===================>..........] - ETA: 2:00 - loss: 0.6827 - acc: 0.5692
3328/4849 [===================>..........] - ETA: 1:55 - loss: 0.6825 - acc: 0.5703
3392/4849 [===================>..........] - ETA: 1:50 - loss: 0.6822 - acc: 0.5699
3456/4849 [====================>.........] - ETA: 1:45 - loss: 0.6818 - acc: 0.5715
3520/4849 [====================>.........] - ETA: 1:40 - loss: 0.6822 - acc: 0.5702
3584/4849 [=====================>........] - ETA: 1:36 - loss: 0.6828 - acc: 0.5678
3648/4849 [=====================>........] - ETA: 1:31 - loss: 0.6820 - acc: 0.5699
3712/4849 [=====================>........] - ETA: 1:26 - loss: 0.6822 - acc: 0.5695
3776/4849 [======================>.......] - ETA: 1:21 - loss: 0.6827 - acc: 0.5691
3840/4849 [======================>.......] - ETA: 1:16 - loss: 0.6831 - acc: 0.5687
3904/4849 [=======================>......] - ETA: 1:11 - loss: 0.6826 - acc: 0.5697
3968/4849 [=======================>......] - ETA: 1:06 - loss: 0.6833 - acc: 0.5691
4032/4849 [=======================>......] - ETA: 1:01 - loss: 0.6835 - acc: 0.5685
4096/4849 [========================>.....] - ETA: 57s - loss: 0.6835 - acc: 0.5686 
4160/4849 [========================>.....] - ETA: 52s - loss: 0.6830 - acc: 0.5700
4224/4849 [=========================>....] - ETA: 47s - loss: 0.6823 - acc: 0.5715
4288/4849 [=========================>....] - ETA: 42s - loss: 0.6825 - acc: 0.5711
4352/4849 [=========================>....] - ETA: 37s - loss: 0.6819 - acc: 0.5722
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6817 - acc: 0.5722
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6812 - acc: 0.5730
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6818 - acc: 0.5720
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6809 - acc: 0.5731
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6809 - acc: 0.5730
4736/4849 [============================>.] - ETA: 8s - loss: 0.6803 - acc: 0.5745 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6798 - acc: 0.5756
4849/4849 [==============================] - 381s 79ms/step - loss: 0.6796 - acc: 0.5764 - val_loss: 0.6928 - val_acc: 0.5584

Epoch 00010: val_acc did not improve from 0.57328
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f27680f9a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f27680f9a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f27680d4950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f27680d4950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25c84a6a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f25c84a6a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27680d4e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27680d4e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2760463d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2760463d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27603444d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27603444d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27680d4b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27680d4b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f276035f510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f276035f510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c8049550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c8049550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f23cc7a8090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f23cc7a8090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27604f0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27604f0fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2760369b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2760369b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27602a8bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27602a8bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f276005f210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f276005f210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27506a1a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27506a1a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27507f0710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27507f0710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27604174d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27604174d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2750566a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2750566a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f275047b350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f275047b350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f275038ed10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f275038ed10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27504bc750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27504bc750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2750769510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2750769510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f276028db50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f276028db50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27503b71d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27503b71d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27500b0b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f27500b0b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27506d3550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27506d3550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2750424d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2750424d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f275009bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f275009bfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f271c6060d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f271c6060d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f271c589a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f271c589a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f271c3c2950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f271c3c2950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27504c00d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27504c00d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f271c518590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f271c518590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f271c3c2710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f271c3c2710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f271c23f950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f271c23f950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f271c5b95d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f271c5b95d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f271c5b90d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f271c5b90d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f271c0ef4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f271c0ef4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27147b7e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27147b7e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f271472a8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f271472a8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2714648750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2714648750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27147b7f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f27147b7f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2714699150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2714699150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27144ede50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f27144ede50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f271468da10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f271468da10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27143bf4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27143bf4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2714693090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2714693090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27143a6d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f27143a6d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f271449acd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f271449acd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f271418b990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f271418b990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f271436f790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f271436f790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2714225b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f2714225b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2714102590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2714102590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f26dc5e7690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f26dc5e7690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2714078690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2714078690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26dc4bb410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26dc4bb410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f26dc5e7c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f26dc5e7c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26dc4c3cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26dc4c3cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f26dc313590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f26dc313590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f26dc266250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f26dc266250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26dc1d78d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26dc1d78d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f26dc5e0150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f26dc5e0150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26dc303e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f26dc303e10>>: AttributeError: module 'gast' has no attribute 'Str'
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 3:08
 128/1348 [=>............................] - ETA: 1:45
 192/1348 [===>..........................] - ETA: 1:15
 256/1348 [====>.........................] - ETA: 1:00
 320/1348 [======>.......................] - ETA: 50s 
 384/1348 [=======>......................] - ETA: 42s
 448/1348 [========>.....................] - ETA: 37s
 512/1348 [==========>...................] - ETA: 33s
 576/1348 [===========>..................] - ETA: 29s
 640/1348 [=============>................] - ETA: 25s
 704/1348 [==============>...............] - ETA: 22s
 768/1348 [================>.............] - ETA: 19s
 832/1348 [=================>............] - ETA: 17s
 896/1348 [==================>...........] - ETA: 15s
 960/1348 [====================>.........] - ETA: 12s
1024/1348 [=====================>........] - ETA: 10s
1088/1348 [=======================>......] - ETA: 8s 
1152/1348 [========================>.....] - ETA: 6s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 42s 31ms/step
loss: 0.6723522504996121
acc: 0.6023738872403561
