nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2694
样本个数 5388
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7febbbd77e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7febbbd77e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fec2227f250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fec2227f250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec22254710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec22254710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febbbd16090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febbbd16090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec221c3690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec221c3690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb3c42610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb3c42610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec22282610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec22282610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb3c42210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb3c42210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febb3b9eed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febb3b9eed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febb3963850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febb3963850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb3bb4550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb3bb4550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febb3b9e890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febb3b9e890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb395a650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb395a650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febb376ce10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febb376ce10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febb3602e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febb3602e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb364c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb364c1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec2228a490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec2228a490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb367abd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febb367abd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febab476290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febab476290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febab4782d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febab4782d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febab40c610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febab40c610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febab4762d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febab4762d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febab273810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febab273810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec22279c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec22279c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febab19d690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febab19d690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febab3521d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febab3521d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febb376b750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febb376b750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febab19d690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febab19d690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febab109a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febab109a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febaae4e2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febaae4e2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febaae28690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febaae28690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febb3558950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febb3558950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febaad29710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febaad29710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7feba2acfc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7feba2acfc90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba2ba2a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba2ba2a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba2c2fed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba2c2fed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feba2acffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feba2acffd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba29c6d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba29c6d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febaaf3bd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febaaf3bd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba268eb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba268eb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba2ae0190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba2ae0190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febaad25c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febaad25c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba26d2490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba26d2490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7feba2a39210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7feba2a39210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba2520310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba2520310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba27e9990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba27e9990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feba25e7d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feba25e7d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba2381650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba2381650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7feba217cbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7feba217cbd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba21c2710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba21c2710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba2520e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba2520e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feba217ced0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feba217ced0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febab31aa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febab31aa10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7feba1f46110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7feba1f46110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba1dbddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba1dbddd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba1e53dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba1e53dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feba2048e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feba2048e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba1db7210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feba1db7210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7feba208edd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7feba208edd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba1da2c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feba1da2c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feb99b27cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feb99b27cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feba1e92550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feba1e92550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feb99ae5c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7feb99ae5c10>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-26 19:02:40.230544: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-26 19:02:40.421154: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-26 19:02:40.541769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562ebf673200 executing computations on platform Host. Devices:
2022-11-26 19:02:40.541895: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-26 19:02:42.021543: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 17:24 - loss: 0.7069 - acc: 0.5156
 128/4849 [..............................] - ETA: 12:40 - loss: 0.7289 - acc: 0.5312
 192/4849 [>.............................] - ETA: 10:57 - loss: 0.7747 - acc: 0.4740
 256/4849 [>.............................] - ETA: 9:57 - loss: 0.7627 - acc: 0.4883 
 320/4849 [>.............................] - ETA: 9:18 - loss: 0.7641 - acc: 0.4906
 384/4849 [=>............................] - ETA: 8:55 - loss: 0.7512 - acc: 0.5078
 448/4849 [=>............................] - ETA: 8:32 - loss: 0.7439 - acc: 0.5201
 512/4849 [==>...........................] - ETA: 8:13 - loss: 0.7315 - acc: 0.5332
 576/4849 [==>...........................] - ETA: 8:05 - loss: 0.7277 - acc: 0.5399
 640/4849 [==>...........................] - ETA: 8:15 - loss: 0.7303 - acc: 0.5266
 704/4849 [===>..........................] - ETA: 8:04 - loss: 0.7285 - acc: 0.5312
 768/4849 [===>..........................] - ETA: 7:50 - loss: 0.7289 - acc: 0.5326
 832/4849 [====>.........................] - ETA: 7:40 - loss: 0.7272 - acc: 0.5373
 896/4849 [====>.........................] - ETA: 7:30 - loss: 0.7259 - acc: 0.5346
 960/4849 [====>.........................] - ETA: 7:22 - loss: 0.7224 - acc: 0.5385
1024/4849 [=====>........................] - ETA: 7:11 - loss: 0.7238 - acc: 0.5342
1088/4849 [=====>........................] - ETA: 7:02 - loss: 0.7261 - acc: 0.5285
1152/4849 [======>.......................] - ETA: 6:53 - loss: 0.7240 - acc: 0.5339
1216/4849 [======>.......................] - ETA: 6:44 - loss: 0.7260 - acc: 0.5312
1280/4849 [======>.......................] - ETA: 6:35 - loss: 0.7283 - acc: 0.5266
1344/4849 [=======>......................] - ETA: 6:26 - loss: 0.7259 - acc: 0.5275
1408/4849 [=======>......................] - ETA: 6:16 - loss: 0.7232 - acc: 0.5305
1472/4849 [========>.....................] - ETA: 6:08 - loss: 0.7203 - acc: 0.5333
1536/4849 [========>.....................] - ETA: 6:01 - loss: 0.7209 - acc: 0.5306
1600/4849 [========>.....................] - ETA: 5:53 - loss: 0.7225 - acc: 0.5306
1664/4849 [=========>....................] - ETA: 5:46 - loss: 0.7238 - acc: 0.5276
1728/4849 [=========>....................] - ETA: 5:38 - loss: 0.7224 - acc: 0.5295
1792/4849 [==========>...................] - ETA: 5:30 - loss: 0.7223 - acc: 0.5285
1856/4849 [==========>...................] - ETA: 5:22 - loss: 0.7239 - acc: 0.5248
1920/4849 [==========>...................] - ETA: 5:16 - loss: 0.7230 - acc: 0.5250
1984/4849 [===========>..................] - ETA: 5:08 - loss: 0.7227 - acc: 0.5257
2048/4849 [===========>..................] - ETA: 5:00 - loss: 0.7220 - acc: 0.5254
2112/4849 [============>.................] - ETA: 4:53 - loss: 0.7247 - acc: 0.5232
2176/4849 [============>.................] - ETA: 4:46 - loss: 0.7255 - acc: 0.5207
2240/4849 [============>.................] - ETA: 4:38 - loss: 0.7258 - acc: 0.5196
2304/4849 [=============>................] - ETA: 4:32 - loss: 0.7262 - acc: 0.5178
2368/4849 [=============>................] - ETA: 4:24 - loss: 0.7256 - acc: 0.5173
2432/4849 [==============>...............] - ETA: 4:17 - loss: 0.7259 - acc: 0.5152
2496/4849 [==============>...............] - ETA: 4:10 - loss: 0.7259 - acc: 0.5156
2560/4849 [==============>...............] - ETA: 4:03 - loss: 0.7248 - acc: 0.5168
2624/4849 [===============>..............] - ETA: 3:56 - loss: 0.7239 - acc: 0.5179
2688/4849 [===============>..............] - ETA: 3:48 - loss: 0.7225 - acc: 0.5197
2752/4849 [================>.............] - ETA: 3:41 - loss: 0.7221 - acc: 0.5218
2816/4849 [================>.............] - ETA: 3:35 - loss: 0.7221 - acc: 0.5227
2880/4849 [================>.............] - ETA: 3:28 - loss: 0.7218 - acc: 0.5233
2944/4849 [=================>............] - ETA: 3:21 - loss: 0.7217 - acc: 0.5224
3008/4849 [=================>............] - ETA: 3:14 - loss: 0.7210 - acc: 0.5236
3072/4849 [==================>...........] - ETA: 3:07 - loss: 0.7201 - acc: 0.5247
3136/4849 [==================>...........] - ETA: 3:00 - loss: 0.7203 - acc: 0.5233
3200/4849 [==================>...........] - ETA: 2:53 - loss: 0.7199 - acc: 0.5250
3264/4849 [===================>..........] - ETA: 2:46 - loss: 0.7192 - acc: 0.5257
3328/4849 [===================>..........] - ETA: 2:39 - loss: 0.7203 - acc: 0.5237
3392/4849 [===================>..........] - ETA: 2:33 - loss: 0.7219 - acc: 0.5224
3456/4849 [====================>.........] - ETA: 2:26 - loss: 0.7208 - acc: 0.5237
3520/4849 [====================>.........] - ETA: 2:19 - loss: 0.7222 - acc: 0.5213
3584/4849 [=====================>........] - ETA: 2:12 - loss: 0.7222 - acc: 0.5215
3648/4849 [=====================>........] - ETA: 2:06 - loss: 0.7220 - acc: 0.5214
3712/4849 [=====================>........] - ETA: 1:59 - loss: 0.7214 - acc: 0.5218
3776/4849 [======================>.......] - ETA: 1:52 - loss: 0.7209 - acc: 0.5220
3840/4849 [======================>.......] - ETA: 1:45 - loss: 0.7201 - acc: 0.5219
3904/4849 [=======================>......] - ETA: 1:39 - loss: 0.7203 - acc: 0.5207
3968/4849 [=======================>......] - ETA: 1:32 - loss: 0.7196 - acc: 0.5209
4032/4849 [=======================>......] - ETA: 1:25 - loss: 0.7193 - acc: 0.5216
4096/4849 [========================>.....] - ETA: 1:18 - loss: 0.7188 - acc: 0.5222
4160/4849 [========================>.....] - ETA: 1:11 - loss: 0.7185 - acc: 0.5221
4224/4849 [=========================>....] - ETA: 1:05 - loss: 0.7182 - acc: 0.5215
4288/4849 [=========================>....] - ETA: 58s - loss: 0.7180 - acc: 0.5210 
4352/4849 [=========================>....] - ETA: 51s - loss: 0.7181 - acc: 0.5207
4416/4849 [==========================>...] - ETA: 45s - loss: 0.7180 - acc: 0.5195
4480/4849 [==========================>...] - ETA: 38s - loss: 0.7178 - acc: 0.5199
4544/4849 [===========================>..] - ETA: 31s - loss: 0.7173 - acc: 0.5211
4608/4849 [===========================>..] - ETA: 25s - loss: 0.7165 - acc: 0.5224
4672/4849 [===========================>..] - ETA: 18s - loss: 0.7166 - acc: 0.5223
4736/4849 [============================>.] - ETA: 11s - loss: 0.7163 - acc: 0.5215
4800/4849 [============================>.] - ETA: 5s - loss: 0.7165 - acc: 0.5208 
4849/4849 [==============================] - 524s 108ms/step - loss: 0.7165 - acc: 0.5209 - val_loss: 0.6888 - val_acc: 0.5288

Epoch 00001: val_acc improved from -inf to 0.52876, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window06/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 7:47 - loss: 0.7139 - acc: 0.4531
 128/4849 [..............................] - ETA: 7:55 - loss: 0.7105 - acc: 0.4922
 192/4849 [>.............................] - ETA: 8:03 - loss: 0.7062 - acc: 0.5052
 256/4849 [>.............................] - ETA: 7:40 - loss: 0.6942 - acc: 0.5391
 320/4849 [>.............................] - ETA: 7:32 - loss: 0.6938 - acc: 0.5375
 384/4849 [=>............................] - ETA: 7:26 - loss: 0.6963 - acc: 0.5208
 448/4849 [=>............................] - ETA: 7:18 - loss: 0.6982 - acc: 0.5290
 512/4849 [==>...........................] - ETA: 7:06 - loss: 0.6970 - acc: 0.5273
 576/4849 [==>...........................] - ETA: 7:01 - loss: 0.6912 - acc: 0.5451
 640/4849 [==>...........................] - ETA: 6:56 - loss: 0.6922 - acc: 0.5391
 704/4849 [===>..........................] - ETA: 6:53 - loss: 0.6899 - acc: 0.5455
 768/4849 [===>..........................] - ETA: 6:46 - loss: 0.6948 - acc: 0.5326
 832/4849 [====>.........................] - ETA: 6:37 - loss: 0.6949 - acc: 0.5312
 896/4849 [====>.........................] - ETA: 6:31 - loss: 0.6942 - acc: 0.5290
 960/4849 [====>.........................] - ETA: 6:26 - loss: 0.6968 - acc: 0.5208
1024/4849 [=====>........................] - ETA: 6:21 - loss: 0.6954 - acc: 0.5273
1088/4849 [=====>........................] - ETA: 6:10 - loss: 0.6962 - acc: 0.5248
1152/4849 [======>.......................] - ETA: 6:00 - loss: 0.6947 - acc: 0.5304
1216/4849 [======>.......................] - ETA: 5:50 - loss: 0.6950 - acc: 0.5280
1280/4849 [======>.......................] - ETA: 5:41 - loss: 0.6951 - acc: 0.5266
1344/4849 [=======>......................] - ETA: 5:36 - loss: 0.6953 - acc: 0.5283
1408/4849 [=======>......................] - ETA: 5:27 - loss: 0.6953 - acc: 0.5298
1472/4849 [========>.....................] - ETA: 5:22 - loss: 0.6944 - acc: 0.5353
1536/4849 [========>.....................] - ETA: 5:16 - loss: 0.6953 - acc: 0.5326
1600/4849 [========>.....................] - ETA: 5:08 - loss: 0.6970 - acc: 0.5275
1664/4849 [=========>....................] - ETA: 5:02 - loss: 0.6971 - acc: 0.5240
1728/4849 [=========>....................] - ETA: 4:56 - loss: 0.6976 - acc: 0.5208
1792/4849 [==========>...................] - ETA: 4:50 - loss: 0.6976 - acc: 0.5195
1856/4849 [==========>...................] - ETA: 4:44 - loss: 0.6976 - acc: 0.5172
1920/4849 [==========>...................] - ETA: 4:37 - loss: 0.6965 - acc: 0.5198
1984/4849 [===========>..................] - ETA: 4:31 - loss: 0.6963 - acc: 0.5202
2048/4849 [===========>..................] - ETA: 4:26 - loss: 0.6965 - acc: 0.5225
2112/4849 [============>.................] - ETA: 4:20 - loss: 0.6968 - acc: 0.5237
2176/4849 [============>.................] - ETA: 4:13 - loss: 0.6974 - acc: 0.5225
2240/4849 [============>.................] - ETA: 4:07 - loss: 0.6956 - acc: 0.5272
2304/4849 [=============>................] - ETA: 4:01 - loss: 0.6946 - acc: 0.5308
2368/4849 [=============>................] - ETA: 3:54 - loss: 0.6939 - acc: 0.5321
2432/4849 [==============>...............] - ETA: 3:48 - loss: 0.6930 - acc: 0.5341
2496/4849 [==============>...............] - ETA: 3:41 - loss: 0.6927 - acc: 0.5353
2560/4849 [==============>...............] - ETA: 3:35 - loss: 0.6918 - acc: 0.5363
2624/4849 [===============>..............] - ETA: 3:28 - loss: 0.6924 - acc: 0.5362
2688/4849 [===============>..............] - ETA: 3:22 - loss: 0.6924 - acc: 0.5368
2752/4849 [================>.............] - ETA: 3:15 - loss: 0.6931 - acc: 0.5356
2816/4849 [================>.............] - ETA: 3:09 - loss: 0.6921 - acc: 0.5373
2880/4849 [================>.............] - ETA: 3:03 - loss: 0.6927 - acc: 0.5368
2944/4849 [=================>............] - ETA: 2:56 - loss: 0.6918 - acc: 0.5394
3008/4849 [=================>............] - ETA: 2:50 - loss: 0.6918 - acc: 0.5399
3072/4849 [==================>...........] - ETA: 2:44 - loss: 0.6920 - acc: 0.5400
3136/4849 [==================>...........] - ETA: 2:38 - loss: 0.6917 - acc: 0.5418
3200/4849 [==================>...........] - ETA: 2:32 - loss: 0.6926 - acc: 0.5419
3264/4849 [===================>..........] - ETA: 2:26 - loss: 0.6933 - acc: 0.5417
3328/4849 [===================>..........] - ETA: 2:20 - loss: 0.6924 - acc: 0.5433
3392/4849 [===================>..........] - ETA: 2:14 - loss: 0.6937 - acc: 0.5419
3456/4849 [====================>.........] - ETA: 2:08 - loss: 0.6936 - acc: 0.5431
3520/4849 [====================>.........] - ETA: 2:02 - loss: 0.6932 - acc: 0.5443
3584/4849 [=====================>........] - ETA: 1:56 - loss: 0.6932 - acc: 0.5444
3648/4849 [=====================>........] - ETA: 1:50 - loss: 0.6939 - acc: 0.5425
3712/4849 [=====================>........] - ETA: 1:44 - loss: 0.6937 - acc: 0.5428
3776/4849 [======================>.......] - ETA: 1:38 - loss: 0.6936 - acc: 0.5426
3840/4849 [======================>.......] - ETA: 1:32 - loss: 0.6931 - acc: 0.5440
3904/4849 [=======================>......] - ETA: 1:27 - loss: 0.6932 - acc: 0.5435
3968/4849 [=======================>......] - ETA: 1:21 - loss: 0.6938 - acc: 0.5416
4032/4849 [=======================>......] - ETA: 1:15 - loss: 0.6937 - acc: 0.5412
4096/4849 [========================>.....] - ETA: 1:09 - loss: 0.6937 - acc: 0.5410
4160/4849 [========================>.....] - ETA: 1:03 - loss: 0.6943 - acc: 0.5394
4224/4849 [=========================>....] - ETA: 57s - loss: 0.6941 - acc: 0.5395 
4288/4849 [=========================>....] - ETA: 51s - loss: 0.6937 - acc: 0.5408
4352/4849 [=========================>....] - ETA: 45s - loss: 0.6937 - acc: 0.5409
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6944 - acc: 0.5403
4480/4849 [==========================>...] - ETA: 34s - loss: 0.6944 - acc: 0.5400
4544/4849 [===========================>..] - ETA: 28s - loss: 0.6940 - acc: 0.5418
4608/4849 [===========================>..] - ETA: 22s - loss: 0.6938 - acc: 0.5417
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6940 - acc: 0.5402
4736/4849 [============================>.] - ETA: 10s - loss: 0.6940 - acc: 0.5399
4800/4849 [============================>.] - ETA: 4s - loss: 0.6940 - acc: 0.5402 
4849/4849 [==============================] - 463s 96ms/step - loss: 0.6945 - acc: 0.5395 - val_loss: 0.6814 - val_acc: 0.5529

Epoch 00002: val_acc improved from 0.52876 to 0.55288, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window06/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 3/10

  64/4849 [..............................] - ETA: 7:25 - loss: 0.6954 - acc: 0.5938
 128/4849 [..............................] - ETA: 6:54 - loss: 0.7037 - acc: 0.5547
 192/4849 [>.............................] - ETA: 6:54 - loss: 0.6929 - acc: 0.5365
 256/4849 [>.............................] - ETA: 6:49 - loss: 0.6917 - acc: 0.5430
 320/4849 [>.............................] - ETA: 6:40 - loss: 0.6920 - acc: 0.5375
 384/4849 [=>............................] - ETA: 6:38 - loss: 0.6953 - acc: 0.5339
 448/4849 [=>............................] - ETA: 6:31 - loss: 0.6895 - acc: 0.5446
 512/4849 [==>...........................] - ETA: 6:21 - loss: 0.6931 - acc: 0.5352
 576/4849 [==>...........................] - ETA: 6:14 - loss: 0.6925 - acc: 0.5365
 640/4849 [==>...........................] - ETA: 6:13 - loss: 0.6949 - acc: 0.5297
 704/4849 [===>..........................] - ETA: 6:05 - loss: 0.6928 - acc: 0.5341
 768/4849 [===>..........................] - ETA: 6:00 - loss: 0.6926 - acc: 0.5339
 832/4849 [====>.........................] - ETA: 5:56 - loss: 0.6924 - acc: 0.5349
 896/4849 [====>.........................] - ETA: 5:50 - loss: 0.6903 - acc: 0.5413
 960/4849 [====>.........................] - ETA: 5:44 - loss: 0.6922 - acc: 0.5365
1024/4849 [=====>........................] - ETA: 5:41 - loss: 0.6910 - acc: 0.5430
1088/4849 [=====>........................] - ETA: 5:36 - loss: 0.6911 - acc: 0.5460
1152/4849 [======>.......................] - ETA: 5:29 - loss: 0.6928 - acc: 0.5443
1216/4849 [======>.......................] - ETA: 5:22 - loss: 0.6923 - acc: 0.5444
1280/4849 [======>.......................] - ETA: 5:17 - loss: 0.6925 - acc: 0.5445
1344/4849 [=======>......................] - ETA: 5:13 - loss: 0.6943 - acc: 0.5409
1408/4849 [=======>......................] - ETA: 5:05 - loss: 0.6940 - acc: 0.5412
1472/4849 [========>.....................] - ETA: 5:00 - loss: 0.6931 - acc: 0.5401
1536/4849 [========>.....................] - ETA: 4:55 - loss: 0.6933 - acc: 0.5404
1600/4849 [========>.....................] - ETA: 4:49 - loss: 0.6928 - acc: 0.5425
1664/4849 [=========>....................] - ETA: 4:43 - loss: 0.6932 - acc: 0.5409
1728/4849 [=========>....................] - ETA: 4:37 - loss: 0.6947 - acc: 0.5411
1792/4849 [==========>...................] - ETA: 4:30 - loss: 0.6974 - acc: 0.5368
1856/4849 [==========>...................] - ETA: 4:25 - loss: 0.6975 - acc: 0.5361
1920/4849 [==========>...................] - ETA: 4:19 - loss: 0.6972 - acc: 0.5375
1984/4849 [===========>..................] - ETA: 4:14 - loss: 0.6975 - acc: 0.5378
2048/4849 [===========>..................] - ETA: 4:09 - loss: 0.6973 - acc: 0.5371
2112/4849 [============>.................] - ETA: 4:03 - loss: 0.6981 - acc: 0.5365
2176/4849 [============>.................] - ETA: 3:57 - loss: 0.6972 - acc: 0.5368
2240/4849 [============>.................] - ETA: 3:52 - loss: 0.6958 - acc: 0.5397
2304/4849 [=============>................] - ETA: 3:46 - loss: 0.6965 - acc: 0.5391
2368/4849 [=============>................] - ETA: 3:41 - loss: 0.6967 - acc: 0.5393
2432/4849 [==============>...............] - ETA: 3:35 - loss: 0.6960 - acc: 0.5407
2496/4849 [==============>...............] - ETA: 3:30 - loss: 0.6967 - acc: 0.5385
2560/4849 [==============>...............] - ETA: 3:24 - loss: 0.6959 - acc: 0.5398
2624/4849 [===============>..............] - ETA: 3:18 - loss: 0.6960 - acc: 0.5408
2688/4849 [===============>..............] - ETA: 3:12 - loss: 0.6958 - acc: 0.5406
2752/4849 [================>.............] - ETA: 3:07 - loss: 0.6968 - acc: 0.5392
2816/4849 [================>.............] - ETA: 3:01 - loss: 0.6969 - acc: 0.5391
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6964 - acc: 0.5396
2944/4849 [=================>............] - ETA: 2:49 - loss: 0.6952 - acc: 0.5428
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6948 - acc: 0.5442
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6940 - acc: 0.5472
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6943 - acc: 0.5466
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6942 - acc: 0.5469
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6945 - acc: 0.5478
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6941 - acc: 0.5481
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6950 - acc: 0.5466
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6955 - acc: 0.5460
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6966 - acc: 0.5435
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6969 - acc: 0.5441
3648/4849 [=====================>........] - ETA: 1:47 - loss: 0.6959 - acc: 0.5458
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6960 - acc: 0.5445
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6951 - acc: 0.5463
3840/4849 [======================>.......] - ETA: 1:30 - loss: 0.6946 - acc: 0.5469
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6937 - acc: 0.5489
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6931 - acc: 0.5496
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6938 - acc: 0.5489
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6943 - acc: 0.5476
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6940 - acc: 0.5478
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6937 - acc: 0.5490 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6938 - acc: 0.5490
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6942 - acc: 0.5487
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6940 - acc: 0.5496
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6933 - acc: 0.5511
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6934 - acc: 0.5513
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6930 - acc: 0.5516
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6933 - acc: 0.5503
4736/4849 [============================>.] - ETA: 10s - loss: 0.6926 - acc: 0.5522
4800/4849 [============================>.] - ETA: 4s - loss: 0.6928 - acc: 0.5517 
4849/4849 [==============================] - 451s 93ms/step - loss: 0.6925 - acc: 0.5519 - val_loss: 0.6837 - val_acc: 0.5696

Epoch 00003: val_acc improved from 0.55288 to 0.56957, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window06/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 4/10

  64/4849 [..............................] - ETA: 7:55 - loss: 0.7331 - acc: 0.4531
 128/4849 [..............................] - ETA: 7:09 - loss: 0.7122 - acc: 0.4844
 192/4849 [>.............................] - ETA: 6:53 - loss: 0.7082 - acc: 0.5052
 256/4849 [>.............................] - ETA: 6:57 - loss: 0.6946 - acc: 0.5273
 320/4849 [>.............................] - ETA: 6:50 - loss: 0.6826 - acc: 0.5531
 384/4849 [=>............................] - ETA: 6:46 - loss: 0.6795 - acc: 0.5625
 448/4849 [=>............................] - ETA: 6:39 - loss: 0.6777 - acc: 0.5692
 512/4849 [==>...........................] - ETA: 6:33 - loss: 0.6753 - acc: 0.5762
 576/4849 [==>...........................] - ETA: 6:24 - loss: 0.6788 - acc: 0.5677
 640/4849 [==>...........................] - ETA: 6:20 - loss: 0.6815 - acc: 0.5641
 704/4849 [===>..........................] - ETA: 6:13 - loss: 0.6823 - acc: 0.5597
 768/4849 [===>..........................] - ETA: 6:06 - loss: 0.6783 - acc: 0.5664
 832/4849 [====>.........................] - ETA: 6:04 - loss: 0.6794 - acc: 0.5649
 896/4849 [====>.........................] - ETA: 5:58 - loss: 0.6814 - acc: 0.5592
 960/4849 [====>.........................] - ETA: 5:49 - loss: 0.6816 - acc: 0.5604
1024/4849 [=====>........................] - ETA: 5:44 - loss: 0.6805 - acc: 0.5635
1088/4849 [=====>........................] - ETA: 5:39 - loss: 0.6797 - acc: 0.5653
1152/4849 [======>.......................] - ETA: 5:31 - loss: 0.6801 - acc: 0.5634
1216/4849 [======>.......................] - ETA: 5:24 - loss: 0.6797 - acc: 0.5641
1280/4849 [======>.......................] - ETA: 5:18 - loss: 0.6787 - acc: 0.5680
1344/4849 [=======>......................] - ETA: 5:10 - loss: 0.6797 - acc: 0.5662
1408/4849 [=======>......................] - ETA: 5:06 - loss: 0.6777 - acc: 0.5668
1472/4849 [========>.....................] - ETA: 5:01 - loss: 0.6772 - acc: 0.5700
1536/4849 [========>.....................] - ETA: 4:54 - loss: 0.6784 - acc: 0.5716
1600/4849 [========>.....................] - ETA: 4:48 - loss: 0.6770 - acc: 0.5731
1664/4849 [=========>....................] - ETA: 4:44 - loss: 0.6773 - acc: 0.5727
1728/4849 [=========>....................] - ETA: 4:39 - loss: 0.6757 - acc: 0.5764
1792/4849 [==========>...................] - ETA: 4:34 - loss: 0.6754 - acc: 0.5765
1856/4849 [==========>...................] - ETA: 4:28 - loss: 0.6778 - acc: 0.5733
1920/4849 [==========>...................] - ETA: 4:22 - loss: 0.6782 - acc: 0.5729
1984/4849 [===========>..................] - ETA: 4:16 - loss: 0.6783 - acc: 0.5721
2048/4849 [===========>..................] - ETA: 4:10 - loss: 0.6798 - acc: 0.5703
2112/4849 [============>.................] - ETA: 4:04 - loss: 0.6811 - acc: 0.5682
2176/4849 [============>.................] - ETA: 3:59 - loss: 0.6824 - acc: 0.5648
2240/4849 [============>.................] - ETA: 3:53 - loss: 0.6825 - acc: 0.5647
2304/4849 [=============>................] - ETA: 3:48 - loss: 0.6825 - acc: 0.5647
2368/4849 [=============>................] - ETA: 3:42 - loss: 0.6826 - acc: 0.5642
2432/4849 [==============>...............] - ETA: 3:36 - loss: 0.6824 - acc: 0.5650
2496/4849 [==============>...............] - ETA: 3:30 - loss: 0.6831 - acc: 0.5621
2560/4849 [==============>...............] - ETA: 3:25 - loss: 0.6836 - acc: 0.5605
2624/4849 [===============>..............] - ETA: 3:19 - loss: 0.6829 - acc: 0.5617
2688/4849 [===============>..............] - ETA: 3:14 - loss: 0.6823 - acc: 0.5629
2752/4849 [================>.............] - ETA: 3:08 - loss: 0.6826 - acc: 0.5625
2816/4849 [================>.............] - ETA: 3:02 - loss: 0.6832 - acc: 0.5604
2880/4849 [================>.............] - ETA: 2:56 - loss: 0.6833 - acc: 0.5590
2944/4849 [=================>............] - ETA: 2:51 - loss: 0.6832 - acc: 0.5591
3008/4849 [=================>............] - ETA: 2:45 - loss: 0.6833 - acc: 0.5585
3072/4849 [==================>...........] - ETA: 2:40 - loss: 0.6831 - acc: 0.5596
3136/4849 [==================>...........] - ETA: 2:34 - loss: 0.6830 - acc: 0.5603
3200/4849 [==================>...........] - ETA: 2:28 - loss: 0.6840 - acc: 0.5581
3264/4849 [===================>..........] - ETA: 2:22 - loss: 0.6842 - acc: 0.5582
3328/4849 [===================>..........] - ETA: 2:17 - loss: 0.6849 - acc: 0.5565
3392/4849 [===================>..........] - ETA: 2:11 - loss: 0.6856 - acc: 0.5545
3456/4849 [====================>.........] - ETA: 2:05 - loss: 0.6852 - acc: 0.5556
3520/4849 [====================>.........] - ETA: 1:59 - loss: 0.6851 - acc: 0.5554
3584/4849 [=====================>........] - ETA: 1:53 - loss: 0.6851 - acc: 0.5550
3648/4849 [=====================>........] - ETA: 1:47 - loss: 0.6847 - acc: 0.5551
3712/4849 [=====================>........] - ETA: 1:42 - loss: 0.6852 - acc: 0.5544
3776/4849 [======================>.......] - ETA: 1:36 - loss: 0.6848 - acc: 0.5553
3840/4849 [======================>.......] - ETA: 1:30 - loss: 0.6850 - acc: 0.5555
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6847 - acc: 0.5548
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6852 - acc: 0.5547
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6853 - acc: 0.5543
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6854 - acc: 0.5542
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6858 - acc: 0.5531
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6857 - acc: 0.5540 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6858 - acc: 0.5536
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6862 - acc: 0.5522
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6860 - acc: 0.5532
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6853 - acc: 0.5547
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6860 - acc: 0.5524
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6856 - acc: 0.5545
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6857 - acc: 0.5535
4736/4849 [============================>.] - ETA: 10s - loss: 0.6856 - acc: 0.5532
4800/4849 [============================>.] - ETA: 4s - loss: 0.6854 - acc: 0.5542 
4849/4849 [==============================] - 455s 94ms/step - loss: 0.6851 - acc: 0.5548 - val_loss: 0.7194 - val_acc: 0.4861

Epoch 00004: val_acc did not improve from 0.56957
Epoch 5/10

  64/4849 [..............................] - ETA: 7:13 - loss: 0.6835 - acc: 0.5156
 128/4849 [..............................] - ETA: 6:59 - loss: 0.6885 - acc: 0.5234
 192/4849 [>.............................] - ETA: 6:59 - loss: 0.6787 - acc: 0.5677
 256/4849 [>.............................] - ETA: 6:52 - loss: 0.6791 - acc: 0.5703
 320/4849 [>.............................] - ETA: 6:47 - loss: 0.6833 - acc: 0.5594
 384/4849 [=>............................] - ETA: 6:45 - loss: 0.6795 - acc: 0.5625
 448/4849 [=>............................] - ETA: 6:42 - loss: 0.6754 - acc: 0.5781
 512/4849 [==>...........................] - ETA: 6:35 - loss: 0.6802 - acc: 0.5762
 576/4849 [==>...........................] - ETA: 6:32 - loss: 0.6824 - acc: 0.5729
 640/4849 [==>...........................] - ETA: 6:32 - loss: 0.6831 - acc: 0.5687
 704/4849 [===>..........................] - ETA: 6:24 - loss: 0.6800 - acc: 0.5724
 768/4849 [===>..........................] - ETA: 6:18 - loss: 0.6793 - acc: 0.5703
 832/4849 [====>.........................] - ETA: 6:13 - loss: 0.6784 - acc: 0.5721
 896/4849 [====>.........................] - ETA: 6:07 - loss: 0.6752 - acc: 0.5815
 960/4849 [====>.........................] - ETA: 6:02 - loss: 0.6765 - acc: 0.5813
1024/4849 [=====>........................] - ETA: 5:56 - loss: 0.6795 - acc: 0.5723
1088/4849 [=====>........................] - ETA: 5:48 - loss: 0.6785 - acc: 0.5763
1152/4849 [======>.......................] - ETA: 5:42 - loss: 0.6777 - acc: 0.5755
1216/4849 [======>.......................] - ETA: 5:36 - loss: 0.6785 - acc: 0.5715
1280/4849 [======>.......................] - ETA: 5:30 - loss: 0.6776 - acc: 0.5711
1344/4849 [=======>......................] - ETA: 5:23 - loss: 0.6784 - acc: 0.5699
1408/4849 [=======>......................] - ETA: 5:16 - loss: 0.6790 - acc: 0.5682
1472/4849 [========>.....................] - ETA: 5:10 - loss: 0.6786 - acc: 0.5666
1536/4849 [========>.....................] - ETA: 5:05 - loss: 0.6786 - acc: 0.5671
1600/4849 [========>.....................] - ETA: 4:59 - loss: 0.6794 - acc: 0.5663
1664/4849 [=========>....................] - ETA: 4:53 - loss: 0.6806 - acc: 0.5625
1728/4849 [=========>....................] - ETA: 4:46 - loss: 0.6803 - acc: 0.5648
1792/4849 [==========>...................] - ETA: 4:40 - loss: 0.6791 - acc: 0.5686
1856/4849 [==========>...................] - ETA: 4:33 - loss: 0.6786 - acc: 0.5695
1920/4849 [==========>...................] - ETA: 4:28 - loss: 0.6805 - acc: 0.5656
1984/4849 [===========>..................] - ETA: 4:21 - loss: 0.6800 - acc: 0.5635
2048/4849 [===========>..................] - ETA: 4:15 - loss: 0.6802 - acc: 0.5649
2112/4849 [============>.................] - ETA: 4:10 - loss: 0.6816 - acc: 0.5630
2176/4849 [============>.................] - ETA: 4:04 - loss: 0.6817 - acc: 0.5648
2240/4849 [============>.................] - ETA: 3:58 - loss: 0.6821 - acc: 0.5656
2304/4849 [=============>................] - ETA: 3:53 - loss: 0.6825 - acc: 0.5634
2368/4849 [=============>................] - ETA: 3:47 - loss: 0.6823 - acc: 0.5642
2432/4849 [==============>...............] - ETA: 3:41 - loss: 0.6834 - acc: 0.5613
2496/4849 [==============>...............] - ETA: 3:35 - loss: 0.6834 - acc: 0.5621
2560/4849 [==============>...............] - ETA: 3:29 - loss: 0.6826 - acc: 0.5625
2624/4849 [===============>..............] - ETA: 3:23 - loss: 0.6833 - acc: 0.5610
2688/4849 [===============>..............] - ETA: 3:17 - loss: 0.6840 - acc: 0.5592
2752/4849 [================>.............] - ETA: 3:11 - loss: 0.6846 - acc: 0.5592
2816/4849 [================>.............] - ETA: 3:05 - loss: 0.6840 - acc: 0.5597
2880/4849 [================>.............] - ETA: 2:59 - loss: 0.6861 - acc: 0.5573
2944/4849 [=================>............] - ETA: 2:53 - loss: 0.6857 - acc: 0.5594
3008/4849 [=================>............] - ETA: 2:47 - loss: 0.6858 - acc: 0.5605
3072/4849 [==================>...........] - ETA: 2:41 - loss: 0.6863 - acc: 0.5583
3136/4849 [==================>...........] - ETA: 2:35 - loss: 0.6864 - acc: 0.5587
3200/4849 [==================>...........] - ETA: 2:30 - loss: 0.6852 - acc: 0.5609
3264/4849 [===================>..........] - ETA: 2:24 - loss: 0.6859 - acc: 0.5594
3328/4849 [===================>..........] - ETA: 2:18 - loss: 0.6859 - acc: 0.5598
3392/4849 [===================>..........] - ETA: 2:12 - loss: 0.6856 - acc: 0.5601
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.6849 - acc: 0.5622
3520/4849 [====================>.........] - ETA: 2:00 - loss: 0.6846 - acc: 0.5631
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6844 - acc: 0.5631
3648/4849 [=====================>........] - ETA: 1:49 - loss: 0.6845 - acc: 0.5625
3712/4849 [=====================>........] - ETA: 1:43 - loss: 0.6844 - acc: 0.5620
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6848 - acc: 0.5606
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6852 - acc: 0.5583
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6855 - acc: 0.5569
3968/4849 [=======================>......] - ETA: 1:20 - loss: 0.6853 - acc: 0.5567
4032/4849 [=======================>......] - ETA: 1:14 - loss: 0.6854 - acc: 0.5570
4096/4849 [========================>.....] - ETA: 1:08 - loss: 0.6855 - acc: 0.5571
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6857 - acc: 0.5572
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6859 - acc: 0.5552 
4288/4849 [=========================>....] - ETA: 51s - loss: 0.6856 - acc: 0.5569
4352/4849 [=========================>....] - ETA: 45s - loss: 0.6859 - acc: 0.5556
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6859 - acc: 0.5555
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6860 - acc: 0.5556
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6863 - acc: 0.5552
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6861 - acc: 0.5560
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6865 - acc: 0.5539
4736/4849 [============================>.] - ETA: 10s - loss: 0.6866 - acc: 0.5538
4800/4849 [============================>.] - ETA: 4s - loss: 0.6867 - acc: 0.5537 
4849/4849 [==============================] - 458s 94ms/step - loss: 0.6869 - acc: 0.5535 - val_loss: 0.6881 - val_acc: 0.5584

Epoch 00005: val_acc did not improve from 0.56957
Epoch 6/10

  64/4849 [..............................] - ETA: 7:44 - loss: 0.6661 - acc: 0.6094
 128/4849 [..............................] - ETA: 7:07 - loss: 0.6675 - acc: 0.5938
 192/4849 [>.............................] - ETA: 6:56 - loss: 0.6875 - acc: 0.5781
 256/4849 [>.............................] - ETA: 6:53 - loss: 0.6862 - acc: 0.5703
 320/4849 [>.............................] - ETA: 6:46 - loss: 0.6895 - acc: 0.5531
 384/4849 [=>............................] - ETA: 6:37 - loss: 0.6863 - acc: 0.5625
 448/4849 [=>............................] - ETA: 6:34 - loss: 0.6828 - acc: 0.5670
 512/4849 [==>...........................] - ETA: 6:35 - loss: 0.6812 - acc: 0.5684
 576/4849 [==>...........................] - ETA: 6:24 - loss: 0.6802 - acc: 0.5729
 640/4849 [==>...........................] - ETA: 6:20 - loss: 0.6783 - acc: 0.5781
 704/4849 [===>..........................] - ETA: 6:15 - loss: 0.6776 - acc: 0.5852
 768/4849 [===>..........................] - ETA: 6:08 - loss: 0.6764 - acc: 0.5859
 832/4849 [====>.........................] - ETA: 6:02 - loss: 0.6739 - acc: 0.5913
 896/4849 [====>.........................] - ETA: 5:56 - loss: 0.6749 - acc: 0.5871
 960/4849 [====>.........................] - ETA: 5:49 - loss: 0.6763 - acc: 0.5802
1024/4849 [=====>........................] - ETA: 5:42 - loss: 0.6799 - acc: 0.5752
1088/4849 [=====>........................] - ETA: 5:40 - loss: 0.6795 - acc: 0.5754
1152/4849 [======>.......................] - ETA: 5:34 - loss: 0.6779 - acc: 0.5807
1216/4849 [======>.......................] - ETA: 5:26 - loss: 0.6775 - acc: 0.5822
1280/4849 [======>.......................] - ETA: 5:20 - loss: 0.6771 - acc: 0.5836
1344/4849 [=======>......................] - ETA: 5:15 - loss: 0.6770 - acc: 0.5841
1408/4849 [=======>......................] - ETA: 5:12 - loss: 0.6775 - acc: 0.5845
1472/4849 [========>.....................] - ETA: 5:05 - loss: 0.6765 - acc: 0.5842
1536/4849 [========>.....................] - ETA: 5:00 - loss: 0.6785 - acc: 0.5781
1600/4849 [========>.....................] - ETA: 4:55 - loss: 0.6789 - acc: 0.5763
1664/4849 [=========>....................] - ETA: 4:48 - loss: 0.6785 - acc: 0.5781
1728/4849 [=========>....................] - ETA: 4:43 - loss: 0.6814 - acc: 0.5718
1792/4849 [==========>...................] - ETA: 4:38 - loss: 0.6804 - acc: 0.5731
1856/4849 [==========>...................] - ETA: 4:32 - loss: 0.6809 - acc: 0.5738
1920/4849 [==========>...................] - ETA: 4:26 - loss: 0.6801 - acc: 0.5719
1984/4849 [===========>..................] - ETA: 4:20 - loss: 0.6812 - acc: 0.5696
2048/4849 [===========>..................] - ETA: 4:14 - loss: 0.6800 - acc: 0.5708
2112/4849 [============>.................] - ETA: 4:09 - loss: 0.6800 - acc: 0.5705
2176/4849 [============>.................] - ETA: 4:04 - loss: 0.6796 - acc: 0.5708
2240/4849 [============>.................] - ETA: 3:58 - loss: 0.6795 - acc: 0.5692
2304/4849 [=============>................] - ETA: 3:51 - loss: 0.6792 - acc: 0.5690
2368/4849 [=============>................] - ETA: 3:45 - loss: 0.6794 - acc: 0.5693
2432/4849 [==============>...............] - ETA: 3:39 - loss: 0.6801 - acc: 0.5650
2496/4849 [==============>...............] - ETA: 3:34 - loss: 0.6801 - acc: 0.5649
2560/4849 [==============>...............] - ETA: 3:28 - loss: 0.6798 - acc: 0.5645
2624/4849 [===============>..............] - ETA: 3:22 - loss: 0.6803 - acc: 0.5640
2688/4849 [===============>..............] - ETA: 3:17 - loss: 0.6795 - acc: 0.5655
2752/4849 [================>.............] - ETA: 3:11 - loss: 0.6804 - acc: 0.5647
2816/4849 [================>.............] - ETA: 3:04 - loss: 0.6800 - acc: 0.5675
2880/4849 [================>.............] - ETA: 2:59 - loss: 0.6798 - acc: 0.5684
2944/4849 [=================>............] - ETA: 2:53 - loss: 0.6803 - acc: 0.5679
3008/4849 [=================>............] - ETA: 2:46 - loss: 0.6805 - acc: 0.5685
3072/4849 [==================>...........] - ETA: 2:41 - loss: 0.6815 - acc: 0.5680
3136/4849 [==================>...........] - ETA: 2:35 - loss: 0.6818 - acc: 0.5673
3200/4849 [==================>...........] - ETA: 2:29 - loss: 0.6818 - acc: 0.5675
3264/4849 [===================>..........] - ETA: 2:23 - loss: 0.6819 - acc: 0.5674
3328/4849 [===================>..........] - ETA: 2:17 - loss: 0.6812 - acc: 0.5691
3392/4849 [===================>..........] - ETA: 2:11 - loss: 0.6812 - acc: 0.5690
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.6808 - acc: 0.5700
3520/4849 [====================>.........] - ETA: 2:00 - loss: 0.6809 - acc: 0.5702
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6804 - acc: 0.5706
3648/4849 [=====================>........] - ETA: 1:49 - loss: 0.6804 - acc: 0.5704
3712/4849 [=====================>........] - ETA: 1:43 - loss: 0.6802 - acc: 0.5709
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6804 - acc: 0.5699
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6808 - acc: 0.5695
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6807 - acc: 0.5686
3968/4849 [=======================>......] - ETA: 1:20 - loss: 0.6811 - acc: 0.5678
4032/4849 [=======================>......] - ETA: 1:14 - loss: 0.6813 - acc: 0.5672
4096/4849 [========================>.....] - ETA: 1:08 - loss: 0.6807 - acc: 0.5679
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6817 - acc: 0.5656
4224/4849 [=========================>....] - ETA: 57s - loss: 0.6813 - acc: 0.5670 
4288/4849 [=========================>....] - ETA: 51s - loss: 0.6809 - acc: 0.5679
4352/4849 [=========================>....] - ETA: 45s - loss: 0.6812 - acc: 0.5673
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6815 - acc: 0.5673
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6813 - acc: 0.5681
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6815 - acc: 0.5682
4608/4849 [===========================>..] - ETA: 22s - loss: 0.6808 - acc: 0.5692
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6809 - acc: 0.5689
4736/4849 [============================>.] - ETA: 10s - loss: 0.6810 - acc: 0.5688
4800/4849 [============================>.] - ETA: 4s - loss: 0.6805 - acc: 0.5700 
4849/4849 [==============================] - 460s 95ms/step - loss: 0.6803 - acc: 0.5704 - val_loss: 0.6862 - val_acc: 0.5584

Epoch 00006: val_acc did not improve from 0.56957
Epoch 7/10

  64/4849 [..............................] - ETA: 6:57 - loss: 0.7067 - acc: 0.4844
 128/4849 [..............................] - ETA: 6:59 - loss: 0.7015 - acc: 0.5156
 192/4849 [>.............................] - ETA: 7:00 - loss: 0.6816 - acc: 0.5365
 256/4849 [>.............................] - ETA: 7:09 - loss: 0.6872 - acc: 0.5195
 320/4849 [>.............................] - ETA: 6:59 - loss: 0.6934 - acc: 0.5125
 384/4849 [=>............................] - ETA: 6:44 - loss: 0.6889 - acc: 0.5156
 448/4849 [=>............................] - ETA: 6:40 - loss: 0.6924 - acc: 0.5156
 512/4849 [==>...........................] - ETA: 6:38 - loss: 0.6880 - acc: 0.5254
 576/4849 [==>...........................] - ETA: 6:28 - loss: 0.6904 - acc: 0.5295
 640/4849 [==>...........................] - ETA: 6:22 - loss: 0.6893 - acc: 0.5281
 704/4849 [===>..........................] - ETA: 6:16 - loss: 0.6886 - acc: 0.5341
 768/4849 [===>..........................] - ETA: 6:10 - loss: 0.6891 - acc: 0.5352
 832/4849 [====>.........................] - ETA: 6:04 - loss: 0.6883 - acc: 0.5433
 896/4849 [====>.........................] - ETA: 5:58 - loss: 0.6869 - acc: 0.5446
 960/4849 [====>.........................] - ETA: 5:54 - loss: 0.6872 - acc: 0.5458
1024/4849 [=====>........................] - ETA: 5:47 - loss: 0.6865 - acc: 0.5459
1088/4849 [=====>........................] - ETA: 5:39 - loss: 0.6859 - acc: 0.5469
1152/4849 [======>.......................] - ETA: 5:33 - loss: 0.6850 - acc: 0.5512
1216/4849 [======>.......................] - ETA: 5:28 - loss: 0.6868 - acc: 0.5428
1280/4849 [======>.......................] - ETA: 5:23 - loss: 0.6867 - acc: 0.5437
1344/4849 [=======>......................] - ETA: 5:17 - loss: 0.6864 - acc: 0.5439
1408/4849 [=======>......................] - ETA: 5:10 - loss: 0.6857 - acc: 0.5455
1472/4849 [========>.....................] - ETA: 5:06 - loss: 0.6853 - acc: 0.5462
1536/4849 [========>.....................] - ETA: 5:00 - loss: 0.6850 - acc: 0.5443
1600/4849 [========>.....................] - ETA: 4:54 - loss: 0.6846 - acc: 0.5456
1664/4849 [=========>....................] - ETA: 4:47 - loss: 0.6847 - acc: 0.5487
1728/4849 [=========>....................] - ETA: 4:41 - loss: 0.6834 - acc: 0.5538
1792/4849 [==========>...................] - ETA: 4:35 - loss: 0.6827 - acc: 0.5564
1856/4849 [==========>...................] - ETA: 4:29 - loss: 0.6827 - acc: 0.5566
1920/4849 [==========>...................] - ETA: 4:23 - loss: 0.6826 - acc: 0.5573
1984/4849 [===========>..................] - ETA: 4:17 - loss: 0.6823 - acc: 0.5575
2048/4849 [===========>..................] - ETA: 4:12 - loss: 0.6832 - acc: 0.5566
2112/4849 [============>.................] - ETA: 4:06 - loss: 0.6825 - acc: 0.5578
2176/4849 [============>.................] - ETA: 4:02 - loss: 0.6814 - acc: 0.5602
2240/4849 [============>.................] - ETA: 3:55 - loss: 0.6809 - acc: 0.5612
2304/4849 [=============>................] - ETA: 3:50 - loss: 0.6806 - acc: 0.5629
2368/4849 [=============>................] - ETA: 3:44 - loss: 0.6810 - acc: 0.5625
2432/4849 [==============>...............] - ETA: 3:38 - loss: 0.6802 - acc: 0.5629
2496/4849 [==============>...............] - ETA: 3:33 - loss: 0.6798 - acc: 0.5649
2560/4849 [==============>...............] - ETA: 3:27 - loss: 0.6797 - acc: 0.5652
2624/4849 [===============>..............] - ETA: 3:21 - loss: 0.6797 - acc: 0.5652
2688/4849 [===============>..............] - ETA: 3:15 - loss: 0.6791 - acc: 0.5673
2752/4849 [================>.............] - ETA: 3:09 - loss: 0.6792 - acc: 0.5676
2816/4849 [================>.............] - ETA: 3:04 - loss: 0.6781 - acc: 0.5700
2880/4849 [================>.............] - ETA: 2:58 - loss: 0.6778 - acc: 0.5698
2944/4849 [=================>............] - ETA: 2:52 - loss: 0.6786 - acc: 0.5686
3008/4849 [=================>............] - ETA: 2:46 - loss: 0.6791 - acc: 0.5685
3072/4849 [==================>...........] - ETA: 2:40 - loss: 0.6804 - acc: 0.5661
3136/4849 [==================>...........] - ETA: 2:34 - loss: 0.6789 - acc: 0.5695
3200/4849 [==================>...........] - ETA: 2:28 - loss: 0.6794 - acc: 0.5666
3264/4849 [===================>..........] - ETA: 2:22 - loss: 0.6795 - acc: 0.5668
3328/4849 [===================>..........] - ETA: 2:16 - loss: 0.6787 - acc: 0.5682
3392/4849 [===================>..........] - ETA: 2:11 - loss: 0.6795 - acc: 0.5669
3456/4849 [====================>.........] - ETA: 2:05 - loss: 0.6787 - acc: 0.5686
3520/4849 [====================>.........] - ETA: 1:59 - loss: 0.6787 - acc: 0.5693
3584/4849 [=====================>........] - ETA: 1:53 - loss: 0.6780 - acc: 0.5706
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.6773 - acc: 0.5721
3712/4849 [=====================>........] - ETA: 1:42 - loss: 0.6774 - acc: 0.5719
3776/4849 [======================>.......] - ETA: 1:36 - loss: 0.6770 - acc: 0.5726
3840/4849 [======================>.......] - ETA: 1:30 - loss: 0.6763 - acc: 0.5734
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6760 - acc: 0.5735
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6765 - acc: 0.5738
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6767 - acc: 0.5732
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6758 - acc: 0.5747
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6757 - acc: 0.5755
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6755 - acc: 0.5765 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6761 - acc: 0.5753
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6761 - acc: 0.5754
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6765 - acc: 0.5745
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6764 - acc: 0.5748
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6763 - acc: 0.5750
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6765 - acc: 0.5751
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6762 - acc: 0.5751
4736/4849 [============================>.] - ETA: 10s - loss: 0.6763 - acc: 0.5747
4800/4849 [============================>.] - ETA: 4s - loss: 0.6765 - acc: 0.5735 
4849/4849 [==============================] - 454s 94ms/step - loss: 0.6760 - acc: 0.5748 - val_loss: 0.6850 - val_acc: 0.5640

Epoch 00007: val_acc did not improve from 0.56957
Epoch 8/10

  64/4849 [..............................] - ETA: 6:55 - loss: 0.6501 - acc: 0.6250
 128/4849 [..............................] - ETA: 6:43 - loss: 0.6617 - acc: 0.6094
 192/4849 [>.............................] - ETA: 6:35 - loss: 0.6486 - acc: 0.6458
 256/4849 [>.............................] - ETA: 6:33 - loss: 0.6523 - acc: 0.6406
 320/4849 [>.............................] - ETA: 6:33 - loss: 0.6519 - acc: 0.6375
 384/4849 [=>............................] - ETA: 6:18 - loss: 0.6546 - acc: 0.6276
 448/4849 [=>............................] - ETA: 6:18 - loss: 0.6552 - acc: 0.6183
 512/4849 [==>...........................] - ETA: 6:16 - loss: 0.6620 - acc: 0.6094
 576/4849 [==>...........................] - ETA: 6:11 - loss: 0.6595 - acc: 0.6181
 640/4849 [==>...........................] - ETA: 6:03 - loss: 0.6608 - acc: 0.6172
 704/4849 [===>..........................] - ETA: 5:56 - loss: 0.6612 - acc: 0.6122
 768/4849 [===>..........................] - ETA: 5:51 - loss: 0.6643 - acc: 0.6107
 832/4849 [====>.........................] - ETA: 5:45 - loss: 0.6652 - acc: 0.6094
 896/4849 [====>.........................] - ETA: 5:40 - loss: 0.6662 - acc: 0.6116
 960/4849 [====>.........................] - ETA: 5:34 - loss: 0.6669 - acc: 0.6052
1024/4849 [=====>........................] - ETA: 5:31 - loss: 0.6674 - acc: 0.6006
1088/4849 [=====>........................] - ETA: 5:28 - loss: 0.6673 - acc: 0.6002
1152/4849 [======>.......................] - ETA: 5:21 - loss: 0.6679 - acc: 0.5964
1216/4849 [======>.......................] - ETA: 5:15 - loss: 0.6697 - acc: 0.5913
1280/4849 [======>.......................] - ETA: 5:10 - loss: 0.6720 - acc: 0.5867
1344/4849 [=======>......................] - ETA: 5:06 - loss: 0.6713 - acc: 0.5871
1408/4849 [=======>......................] - ETA: 5:00 - loss: 0.6734 - acc: 0.5831
1472/4849 [========>.....................] - ETA: 4:54 - loss: 0.6746 - acc: 0.5815
1536/4849 [========>.....................] - ETA: 4:49 - loss: 0.6746 - acc: 0.5820
1600/4849 [========>.....................] - ETA: 4:42 - loss: 0.6754 - acc: 0.5781
1664/4849 [=========>....................] - ETA: 4:37 - loss: 0.6747 - acc: 0.5805
1728/4849 [=========>....................] - ETA: 4:32 - loss: 0.6752 - acc: 0.5804
1792/4849 [==========>...................] - ETA: 4:27 - loss: 0.6741 - acc: 0.5837
1856/4849 [==========>...................] - ETA: 4:22 - loss: 0.6736 - acc: 0.5873
1920/4849 [==========>...................] - ETA: 4:17 - loss: 0.6734 - acc: 0.5859
1984/4849 [===========>..................] - ETA: 4:10 - loss: 0.6731 - acc: 0.5882
2048/4849 [===========>..................] - ETA: 4:05 - loss: 0.6749 - acc: 0.5830
2112/4849 [============>.................] - ETA: 3:59 - loss: 0.6760 - acc: 0.5805
2176/4849 [============>.................] - ETA: 3:53 - loss: 0.6761 - acc: 0.5790
2240/4849 [============>.................] - ETA: 3:48 - loss: 0.6762 - acc: 0.5795
2304/4849 [=============>................] - ETA: 3:43 - loss: 0.6758 - acc: 0.5803
2368/4849 [=============>................] - ETA: 3:38 - loss: 0.6756 - acc: 0.5798
2432/4849 [==============>...............] - ETA: 3:33 - loss: 0.6754 - acc: 0.5814
2496/4849 [==============>...............] - ETA: 3:27 - loss: 0.6747 - acc: 0.5845
2560/4849 [==============>...............] - ETA: 3:21 - loss: 0.6739 - acc: 0.5855
2624/4849 [===============>..............] - ETA: 3:16 - loss: 0.6741 - acc: 0.5835
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.6739 - acc: 0.5837
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.6755 - acc: 0.5803
2816/4849 [================>.............] - ETA: 2:59 - loss: 0.6751 - acc: 0.5813
2880/4849 [================>.............] - ETA: 2:54 - loss: 0.6753 - acc: 0.5806
2944/4849 [=================>............] - ETA: 2:48 - loss: 0.6751 - acc: 0.5829
3008/4849 [=================>............] - ETA: 2:42 - loss: 0.6753 - acc: 0.5828
3072/4849 [==================>...........] - ETA: 2:37 - loss: 0.6764 - acc: 0.5811
3136/4849 [==================>...........] - ETA: 2:31 - loss: 0.6765 - acc: 0.5813
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6761 - acc: 0.5822
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6779 - acc: 0.5800
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6776 - acc: 0.5802
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6769 - acc: 0.5805
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6774 - acc: 0.5790
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6779 - acc: 0.5773
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6769 - acc: 0.5792
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6765 - acc: 0.5792
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6768 - acc: 0.5779
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6761 - acc: 0.5794
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6767 - acc: 0.5792
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6766 - acc: 0.5794
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6772 - acc: 0.5784
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6774 - acc: 0.5776
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6779 - acc: 0.5759
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6777 - acc: 0.5772
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6774 - acc: 0.5788 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6772 - acc: 0.5798
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6777 - acc: 0.5779
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6771 - acc: 0.5804
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6766 - acc: 0.5821
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6770 - acc: 0.5814
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6767 - acc: 0.5816
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6769 - acc: 0.5813
4736/4849 [============================>.] - ETA: 10s - loss: 0.6771 - acc: 0.5798
4800/4849 [============================>.] - ETA: 4s - loss: 0.6767 - acc: 0.5804 
4849/4849 [==============================] - 449s 93ms/step - loss: 0.6771 - acc: 0.5797 - val_loss: 0.6933 - val_acc: 0.5603

Epoch 00008: val_acc did not improve from 0.56957
Epoch 9/10

  64/4849 [..............................] - ETA: 7:20 - loss: 0.6754 - acc: 0.5938
 128/4849 [..............................] - ETA: 7:18 - loss: 0.6869 - acc: 0.5391
 192/4849 [>.............................] - ETA: 7:00 - loss: 0.6889 - acc: 0.5469
 256/4849 [>.............................] - ETA: 6:55 - loss: 0.6944 - acc: 0.5273
 320/4849 [>.............................] - ETA: 6:43 - loss: 0.6866 - acc: 0.5531
 384/4849 [=>............................] - ETA: 6:35 - loss: 0.6911 - acc: 0.5495
 448/4849 [=>............................] - ETA: 6:27 - loss: 0.6890 - acc: 0.5580
 512/4849 [==>...........................] - ETA: 6:21 - loss: 0.6877 - acc: 0.5625
 576/4849 [==>...........................] - ETA: 6:16 - loss: 0.6893 - acc: 0.5573
 640/4849 [==>...........................] - ETA: 6:12 - loss: 0.6884 - acc: 0.5547
 704/4849 [===>..........................] - ETA: 6:05 - loss: 0.6878 - acc: 0.5582
 768/4849 [===>..........................] - ETA: 5:58 - loss: 0.6840 - acc: 0.5677
 832/4849 [====>.........................] - ETA: 5:52 - loss: 0.6825 - acc: 0.5661
 896/4849 [====>.........................] - ETA: 5:46 - loss: 0.6827 - acc: 0.5625
 960/4849 [====>.........................] - ETA: 5:39 - loss: 0.6803 - acc: 0.5646
1024/4849 [=====>........................] - ETA: 5:35 - loss: 0.6765 - acc: 0.5674
1088/4849 [=====>........................] - ETA: 5:30 - loss: 0.6782 - acc: 0.5643
1152/4849 [======>.......................] - ETA: 5:25 - loss: 0.6791 - acc: 0.5590
1216/4849 [======>.......................] - ETA: 5:19 - loss: 0.6781 - acc: 0.5625
1280/4849 [======>.......................] - ETA: 5:13 - loss: 0.6773 - acc: 0.5617
1344/4849 [=======>......................] - ETA: 5:07 - loss: 0.6787 - acc: 0.5595
1408/4849 [=======>......................] - ETA: 5:02 - loss: 0.6783 - acc: 0.5611
1472/4849 [========>.....................] - ETA: 4:56 - loss: 0.6785 - acc: 0.5639
1536/4849 [========>.....................] - ETA: 4:51 - loss: 0.6775 - acc: 0.5658
1600/4849 [========>.....................] - ETA: 4:43 - loss: 0.6762 - acc: 0.5681
1664/4849 [=========>....................] - ETA: 4:38 - loss: 0.6764 - acc: 0.5697
1728/4849 [=========>....................] - ETA: 4:33 - loss: 0.6761 - acc: 0.5718
1792/4849 [==========>...................] - ETA: 4:27 - loss: 0.6761 - acc: 0.5731
1856/4849 [==========>...................] - ETA: 4:22 - loss: 0.6765 - acc: 0.5727
1920/4849 [==========>...................] - ETA: 4:17 - loss: 0.6769 - acc: 0.5703
1984/4849 [===========>..................] - ETA: 4:11 - loss: 0.6763 - acc: 0.5711
2048/4849 [===========>..................] - ETA: 4:06 - loss: 0.6761 - acc: 0.5718
2112/4849 [============>.................] - ETA: 4:00 - loss: 0.6767 - acc: 0.5720
2176/4849 [============>.................] - ETA: 3:55 - loss: 0.6761 - acc: 0.5712
2240/4849 [============>.................] - ETA: 3:49 - loss: 0.6761 - acc: 0.5732
2304/4849 [=============>................] - ETA: 3:44 - loss: 0.6756 - acc: 0.5755
2368/4849 [=============>................] - ETA: 3:38 - loss: 0.6746 - acc: 0.5794
2432/4849 [==============>...............] - ETA: 3:33 - loss: 0.6740 - acc: 0.5802
2496/4849 [==============>...............] - ETA: 3:28 - loss: 0.6736 - acc: 0.5817
2560/4849 [==============>...............] - ETA: 3:22 - loss: 0.6749 - acc: 0.5789
2624/4849 [===============>..............] - ETA: 3:16 - loss: 0.6746 - acc: 0.5796
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.6722 - acc: 0.5841
2752/4849 [================>.............] - ETA: 3:05 - loss: 0.6727 - acc: 0.5836
2816/4849 [================>.............] - ETA: 2:59 - loss: 0.6714 - acc: 0.5856
2880/4849 [================>.............] - ETA: 2:54 - loss: 0.6708 - acc: 0.5875
2944/4849 [=================>............] - ETA: 2:48 - loss: 0.6718 - acc: 0.5880
3008/4849 [=================>............] - ETA: 2:42 - loss: 0.6720 - acc: 0.5874
3072/4849 [==================>...........] - ETA: 2:37 - loss: 0.6717 - acc: 0.5885
3136/4849 [==================>...........] - ETA: 2:31 - loss: 0.6731 - acc: 0.5877
3200/4849 [==================>...........] - ETA: 2:25 - loss: 0.6741 - acc: 0.5856
3264/4849 [===================>..........] - ETA: 2:19 - loss: 0.6754 - acc: 0.5836
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6757 - acc: 0.5850
3392/4849 [===================>..........] - ETA: 2:08 - loss: 0.6750 - acc: 0.5861
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6761 - acc: 0.5842
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6761 - acc: 0.5835
3584/4849 [=====================>........] - ETA: 1:51 - loss: 0.6758 - acc: 0.5829
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6755 - acc: 0.5839
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6754 - acc: 0.5832
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6750 - acc: 0.5832
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6751 - acc: 0.5833
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6751 - acc: 0.5832
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6751 - acc: 0.5819
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6755 - acc: 0.5811
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6760 - acc: 0.5784
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6762 - acc: 0.5779
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6764 - acc: 0.5772 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6767 - acc: 0.5760
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6769 - acc: 0.5751
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6767 - acc: 0.5756
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6768 - acc: 0.5763
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6767 - acc: 0.5764
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6770 - acc: 0.5753
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6770 - acc: 0.5762
4736/4849 [============================>.] - ETA: 9s - loss: 0.6767 - acc: 0.5771 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6772 - acc: 0.5763
4849/4849 [==============================] - 443s 91ms/step - loss: 0.6768 - acc: 0.5766 - val_loss: 0.6816 - val_acc: 0.5733

Epoch 00009: val_acc improved from 0.56957 to 0.57328, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window06/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 10/10

  64/4849 [..............................] - ETA: 7:03 - loss: 0.6831 - acc: 0.5938
 128/4849 [..............................] - ETA: 7:12 - loss: 0.6807 - acc: 0.5547
 192/4849 [>.............................] - ETA: 6:49 - loss: 0.6763 - acc: 0.5573
 256/4849 [>.............................] - ETA: 6:43 - loss: 0.6634 - acc: 0.5781
 320/4849 [>.............................] - ETA: 6:42 - loss: 0.6709 - acc: 0.5563
 384/4849 [=>............................] - ETA: 6:35 - loss: 0.6736 - acc: 0.5599
 448/4849 [=>............................] - ETA: 6:27 - loss: 0.6752 - acc: 0.5580
 512/4849 [==>...........................] - ETA: 6:20 - loss: 0.6756 - acc: 0.5664
 576/4849 [==>...........................] - ETA: 6:10 - loss: 0.6718 - acc: 0.5764
 640/4849 [==>...........................] - ETA: 6:04 - loss: 0.6726 - acc: 0.5766
 704/4849 [===>..........................] - ETA: 6:03 - loss: 0.6766 - acc: 0.5724
 768/4849 [===>..........................] - ETA: 5:56 - loss: 0.6772 - acc: 0.5690
 832/4849 [====>.........................] - ETA: 5:51 - loss: 0.6790 - acc: 0.5673
 896/4849 [====>.........................] - ETA: 5:45 - loss: 0.6771 - acc: 0.5737
 960/4849 [====>.........................] - ETA: 5:41 - loss: 0.6799 - acc: 0.5687
1024/4849 [=====>........................] - ETA: 5:34 - loss: 0.6800 - acc: 0.5703
1088/4849 [=====>........................] - ETA: 5:30 - loss: 0.6764 - acc: 0.5744
1152/4849 [======>.......................] - ETA: 5:24 - loss: 0.6786 - acc: 0.5703
1216/4849 [======>.......................] - ETA: 5:18 - loss: 0.6763 - acc: 0.5724
1280/4849 [======>.......................] - ETA: 5:12 - loss: 0.6753 - acc: 0.5750
1344/4849 [=======>......................] - ETA: 5:08 - loss: 0.6774 - acc: 0.5685
1408/4849 [=======>......................] - ETA: 5:03 - loss: 0.6753 - acc: 0.5739
1472/4849 [========>.....................] - ETA: 4:57 - loss: 0.6734 - acc: 0.5795
1536/4849 [========>.....................] - ETA: 4:51 - loss: 0.6722 - acc: 0.5807
1600/4849 [========>.....................] - ETA: 4:44 - loss: 0.6725 - acc: 0.5787
1664/4849 [=========>....................] - ETA: 4:39 - loss: 0.6728 - acc: 0.5811
1728/4849 [=========>....................] - ETA: 4:34 - loss: 0.6735 - acc: 0.5793
1792/4849 [==========>...................] - ETA: 4:28 - loss: 0.6733 - acc: 0.5826
1856/4849 [==========>...................] - ETA: 4:22 - loss: 0.6736 - acc: 0.5797
1920/4849 [==========>...................] - ETA: 4:17 - loss: 0.6738 - acc: 0.5802
1984/4849 [===========>..................] - ETA: 4:11 - loss: 0.6745 - acc: 0.5806
2048/4849 [===========>..................] - ETA: 4:05 - loss: 0.6755 - acc: 0.5796
2112/4849 [============>.................] - ETA: 4:00 - loss: 0.6755 - acc: 0.5791
2176/4849 [============>.................] - ETA: 3:54 - loss: 0.6760 - acc: 0.5758
2240/4849 [============>.................] - ETA: 3:48 - loss: 0.6765 - acc: 0.5728
2304/4849 [=============>................] - ETA: 3:43 - loss: 0.6761 - acc: 0.5734
2368/4849 [=============>................] - ETA: 3:37 - loss: 0.6759 - acc: 0.5752
2432/4849 [==============>...............] - ETA: 3:31 - loss: 0.6763 - acc: 0.5736
2496/4849 [==============>...............] - ETA: 3:26 - loss: 0.6761 - acc: 0.5749
2560/4849 [==============>...............] - ETA: 3:21 - loss: 0.6760 - acc: 0.5750
2624/4849 [===============>..............] - ETA: 3:15 - loss: 0.6751 - acc: 0.5777
2688/4849 [===============>..............] - ETA: 3:09 - loss: 0.6751 - acc: 0.5785
2752/4849 [================>.............] - ETA: 3:03 - loss: 0.6757 - acc: 0.5774
2816/4849 [================>.............] - ETA: 2:58 - loss: 0.6762 - acc: 0.5763
2880/4849 [================>.............] - ETA: 2:52 - loss: 0.6769 - acc: 0.5760
2944/4849 [=================>............] - ETA: 2:47 - loss: 0.6771 - acc: 0.5751
3008/4849 [=================>............] - ETA: 2:41 - loss: 0.6764 - acc: 0.5765
3072/4849 [==================>...........] - ETA: 2:35 - loss: 0.6756 - acc: 0.5778
3136/4849 [==================>...........] - ETA: 2:30 - loss: 0.6756 - acc: 0.5784
3200/4849 [==================>...........] - ETA: 2:24 - loss: 0.6755 - acc: 0.5791
3264/4849 [===================>..........] - ETA: 2:18 - loss: 0.6754 - acc: 0.5800
3328/4849 [===================>..........] - ETA: 2:13 - loss: 0.6761 - acc: 0.5775
3392/4849 [===================>..........] - ETA: 2:07 - loss: 0.6763 - acc: 0.5769
3456/4849 [====================>.........] - ETA: 2:01 - loss: 0.6760 - acc: 0.5770
3520/4849 [====================>.........] - ETA: 1:55 - loss: 0.6760 - acc: 0.5776
3584/4849 [=====================>........] - ETA: 1:50 - loss: 0.6758 - acc: 0.5790
3648/4849 [=====================>........] - ETA: 1:44 - loss: 0.6756 - acc: 0.5795
3712/4849 [=====================>........] - ETA: 1:38 - loss: 0.6760 - acc: 0.5787
3776/4849 [======================>.......] - ETA: 1:33 - loss: 0.6752 - acc: 0.5800
3840/4849 [======================>.......] - ETA: 1:27 - loss: 0.6751 - acc: 0.5794
3904/4849 [=======================>......] - ETA: 1:21 - loss: 0.6744 - acc: 0.5804
3968/4849 [=======================>......] - ETA: 1:16 - loss: 0.6745 - acc: 0.5811
4032/4849 [=======================>......] - ETA: 1:10 - loss: 0.6750 - acc: 0.5796
4096/4849 [========================>.....] - ETA: 1:05 - loss: 0.6741 - acc: 0.5801
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6748 - acc: 0.5774 
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6746 - acc: 0.5777
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6742 - acc: 0.5793
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6744 - acc: 0.5800
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6741 - acc: 0.5808
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6741 - acc: 0.5810
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6737 - acc: 0.5819
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6726 - acc: 0.5838
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6728 - acc: 0.5835
4736/4849 [============================>.] - ETA: 9s - loss: 0.6731 - acc: 0.5826 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6727 - acc: 0.5831
4849/4849 [==============================] - 434s 90ms/step - loss: 0.6725 - acc: 0.5832 - val_loss: 0.6883 - val_acc: 0.5547

Epoch 00010: val_acc did not improve from 0.57328
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fec2261d650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fec2261d650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fec221a2a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fec221a2a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec2228a610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec2228a610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec2217fdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec2217fdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec22106cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec22106cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec21e60450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec21e60450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec2217fd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec2217fd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec21e70210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec21e70210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec21ff3890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec21ff3890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec21eb5f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec21eb5f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec21d65250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec21d65250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febbbefe590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febbbefe590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec21dc2e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec21dc2e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec19c0c750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec19c0c750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec21ee0710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec21ee0710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a84e3b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a84e3b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec21d2ea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec21d2ea10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec19a32dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec19a32dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec19b35090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec19b35090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec197e2110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec197e2110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec19966650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec19966650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec19c3a850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec19c3a850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec19832310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec19832310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec198c4d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec198c4d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec19508490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec19508490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec195e2990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec195e2990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec198c4a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec198c4a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec197dd190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec197dd190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec19408590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec19408590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec192fef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec192fef50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec192f3f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec192f3f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec194080d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec194080d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec191f49d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec191f49d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec18fa1b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec18fa1b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec18e9bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec18e9bc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec1927a5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec1927a5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec18fb37d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec18fb37d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec18e9ecd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec18e9ecd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec18fd0250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec18fd0250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec00b6bed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec00b6bed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec18ce55d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec18ce55d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec18f01450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec18f01450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec00bb9c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec00bb9c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec00c12850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec00c12850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec00baf250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec00baf250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec00c264d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec00c264d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec18dc5250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec18dc5250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec00baf210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec00baf210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec00684a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec00684a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec0065a7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec0065a7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec006aff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec006aff10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec00684510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec00684510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec00590e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec00590e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febf837e910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febf837e910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febf837b450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febf837b450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec00576690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec00576690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febf82a35d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febf82a35d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febf82a1990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febf82a1990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febf8038a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febf8038a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febf806ca10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febf806ca10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec0057bb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec0057bb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febf80388d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febf80388d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febf7f46fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febf7f46fd0>>: AttributeError: module 'gast' has no attribute 'Str'
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 1:44
 128/1348 [=>............................] - ETA: 1:18
 192/1348 [===>..........................] - ETA: 1:03
 256/1348 [====>.........................] - ETA: 56s 
 320/1348 [======>.......................] - ETA: 50s
 384/1348 [=======>......................] - ETA: 46s
 448/1348 [========>.....................] - ETA: 42s
 512/1348 [==========>...................] - ETA: 37s
 576/1348 [===========>..................] - ETA: 34s
 640/1348 [=============>................] - ETA: 30s
 704/1348 [==============>...............] - ETA: 27s
 768/1348 [================>.............] - ETA: 24s
 832/1348 [=================>............] - ETA: 21s
 896/1348 [==================>...........] - ETA: 18s
 960/1348 [====================>.........] - ETA: 15s
1024/1348 [=====================>........] - ETA: 13s
1088/1348 [=======================>......] - ETA: 10s
1152/1348 [========================>.....] - ETA: 7s 
1216/1348 [==========================>...] - ETA: 5s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 53s 40ms/step
loss: 0.6772802480957982
acc: 0.5845697329376854
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe7a8506c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe7a8506c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe7a848c5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe7a848c5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec2228a190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec2228a190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febf6d30fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febf6d30fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec22337090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec22337090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe690537990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe690537990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febf6d30590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febf6d30590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec22595a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec22595a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec226265d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fec226265d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec2231a9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fec2231a9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec22600e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec22600e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec2247f9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fec2247f9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a81fe750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a81fe750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7a81d7a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7a81d7a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7a84330d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7a84330d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a81f0a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a81f0a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7a81e6110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7a81e6110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a80e7950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a80e7950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7a06b6c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7a06b6c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7a06df390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7a06df390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a0540b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a0540b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7a06ea990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7a06ea990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a069fe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a069fe50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7a0387190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7a0387190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7a06f4490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7a06f4490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a0350750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a0350750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7a0387810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7a0387810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a020ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a020ac50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7a020d450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7a020d450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7987a3510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7987a3510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a020d650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a020d650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7a020d750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7a020d750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a00a26d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a00a26d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7984d1ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7984d1ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe79838e790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe79838e790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7986c0f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7986c0f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7987cb190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7987cb190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe79843df50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe79843df50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7984eb590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7984eb590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7980a9fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7980a9fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7980f0750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7980f0750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7984f2d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7984f2d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe79807dbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe79807dbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe79817b990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe79817b990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7905db7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7905db7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe790652f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe790652f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7983ce5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7983ce5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7905ee250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7905ee250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe690304e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe690304e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6903ad890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6903ad890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe690355910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe690355910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe690304d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe690304d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe690272d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe690272d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe690049050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe690049050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe69026b590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe69026b590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe69003f890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe69003f890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe690049bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe690049bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6886d4890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6886d4890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6901115d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6901115d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6883ac950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6883ac950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe688525510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe688525510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe688752150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe688752150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe68839de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe68839de90>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 26:31 - loss: 0.7719 - acc: 0.4219
 128/4849 [..............................] - ETA: 15:56 - loss: 0.7114 - acc: 0.4922
 192/4849 [>.............................] - ETA: 12:18 - loss: 0.6968 - acc: 0.5365
 256/4849 [>.............................] - ETA: 10:17 - loss: 0.7320 - acc: 0.5312
 320/4849 [>.............................] - ETA: 9:13 - loss: 0.7369 - acc: 0.5219 
 384/4849 [=>............................] - ETA: 8:34 - loss: 0.7379 - acc: 0.5208
 448/4849 [=>............................] - ETA: 8:02 - loss: 0.7377 - acc: 0.5067
 512/4849 [==>...........................] - ETA: 7:33 - loss: 0.7342 - acc: 0.5156
 576/4849 [==>...........................] - ETA: 7:09 - loss: 0.7413 - acc: 0.5087
 640/4849 [==>...........................] - ETA: 6:47 - loss: 0.7418 - acc: 0.5047
 704/4849 [===>..........................] - ETA: 6:29 - loss: 0.7358 - acc: 0.5085
 768/4849 [===>..........................] - ETA: 6:12 - loss: 0.7355 - acc: 0.4961
 832/4849 [====>.........................] - ETA: 5:59 - loss: 0.7294 - acc: 0.5060
 896/4849 [====>.........................] - ETA: 5:47 - loss: 0.7285 - acc: 0.5078
 960/4849 [====>.........................] - ETA: 5:36 - loss: 0.7290 - acc: 0.5052
1024/4849 [=====>........................] - ETA: 5:27 - loss: 0.7282 - acc: 0.5117
1088/4849 [=====>........................] - ETA: 5:16 - loss: 0.7287 - acc: 0.5129
1152/4849 [======>.......................] - ETA: 5:07 - loss: 0.7284 - acc: 0.5148
1216/4849 [======>.......................] - ETA: 4:58 - loss: 0.7303 - acc: 0.5123
1280/4849 [======>.......................] - ETA: 4:50 - loss: 0.7333 - acc: 0.5102
1344/4849 [=======>......................] - ETA: 4:43 - loss: 0.7311 - acc: 0.5156
1408/4849 [=======>......................] - ETA: 4:38 - loss: 0.7332 - acc: 0.5156
1472/4849 [========>.....................] - ETA: 4:31 - loss: 0.7330 - acc: 0.5129
1536/4849 [========>.....................] - ETA: 4:24 - loss: 0.7340 - acc: 0.5111
1600/4849 [========>.....................] - ETA: 4:16 - loss: 0.7342 - acc: 0.5100
1664/4849 [=========>....................] - ETA: 4:10 - loss: 0.7353 - acc: 0.5078
1728/4849 [=========>....................] - ETA: 4:02 - loss: 0.7348 - acc: 0.5069
1792/4849 [==========>...................] - ETA: 3:57 - loss: 0.7325 - acc: 0.5095
1856/4849 [==========>...................] - ETA: 3:51 - loss: 0.7321 - acc: 0.5086
1920/4849 [==========>...................] - ETA: 3:44 - loss: 0.7311 - acc: 0.5094
1984/4849 [===========>..................] - ETA: 3:38 - loss: 0.7304 - acc: 0.5111
2048/4849 [===========>..................] - ETA: 3:33 - loss: 0.7282 - acc: 0.5127
2112/4849 [============>.................] - ETA: 3:27 - loss: 0.7276 - acc: 0.5133
2176/4849 [============>.................] - ETA: 3:21 - loss: 0.7258 - acc: 0.5156
2240/4849 [============>.................] - ETA: 3:15 - loss: 0.7252 - acc: 0.5165
2304/4849 [=============>................] - ETA: 3:10 - loss: 0.7250 - acc: 0.5165
2368/4849 [=============>................] - ETA: 3:04 - loss: 0.7251 - acc: 0.5144
2432/4849 [==============>...............] - ETA: 2:59 - loss: 0.7241 - acc: 0.5164
2496/4849 [==============>...............] - ETA: 2:53 - loss: 0.7247 - acc: 0.5148
2560/4849 [==============>...............] - ETA: 2:48 - loss: 0.7244 - acc: 0.5168
2624/4849 [===============>..............] - ETA: 2:43 - loss: 0.7249 - acc: 0.5145
2688/4849 [===============>..............] - ETA: 2:38 - loss: 0.7240 - acc: 0.5160
2752/4849 [================>.............] - ETA: 2:33 - loss: 0.7234 - acc: 0.5174
2816/4849 [================>.............] - ETA: 2:28 - loss: 0.7229 - acc: 0.5178
2880/4849 [================>.............] - ETA: 2:23 - loss: 0.7220 - acc: 0.5198
2944/4849 [=================>............] - ETA: 2:18 - loss: 0.7211 - acc: 0.5224
3008/4849 [=================>............] - ETA: 2:13 - loss: 0.7201 - acc: 0.5239
3072/4849 [==================>...........] - ETA: 2:09 - loss: 0.7191 - acc: 0.5244
3136/4849 [==================>...........] - ETA: 2:04 - loss: 0.7190 - acc: 0.5239
3200/4849 [==================>...........] - ETA: 1:59 - loss: 0.7189 - acc: 0.5234
3264/4849 [===================>..........] - ETA: 1:54 - loss: 0.7177 - acc: 0.5245
3328/4849 [===================>..........] - ETA: 1:49 - loss: 0.7176 - acc: 0.5252
3392/4849 [===================>..........] - ETA: 1:44 - loss: 0.7164 - acc: 0.5262
3456/4849 [====================>.........] - ETA: 1:39 - loss: 0.7170 - acc: 0.5258
3520/4849 [====================>.........] - ETA: 1:35 - loss: 0.7166 - acc: 0.5259
3584/4849 [=====================>........] - ETA: 1:30 - loss: 0.7161 - acc: 0.5257
3648/4849 [=====================>........] - ETA: 1:25 - loss: 0.7161 - acc: 0.5266
3712/4849 [=====================>........] - ETA: 1:21 - loss: 0.7158 - acc: 0.5267
3776/4849 [======================>.......] - ETA: 1:16 - loss: 0.7168 - acc: 0.5262
3840/4849 [======================>.......] - ETA: 1:11 - loss: 0.7164 - acc: 0.5258
3904/4849 [=======================>......] - ETA: 1:07 - loss: 0.7167 - acc: 0.5241
3968/4849 [=======================>......] - ETA: 1:02 - loss: 0.7161 - acc: 0.5249
4032/4849 [=======================>......] - ETA: 58s - loss: 0.7162 - acc: 0.5233 
4096/4849 [========================>.....] - ETA: 53s - loss: 0.7159 - acc: 0.5227
4160/4849 [========================>.....] - ETA: 48s - loss: 0.7159 - acc: 0.5219
4224/4849 [=========================>....] - ETA: 44s - loss: 0.7158 - acc: 0.5218
4288/4849 [=========================>....] - ETA: 39s - loss: 0.7152 - acc: 0.5226
4352/4849 [=========================>....] - ETA: 35s - loss: 0.7147 - acc: 0.5227
4416/4849 [==========================>...] - ETA: 30s - loss: 0.7144 - acc: 0.5240
4480/4849 [==========================>...] - ETA: 26s - loss: 0.7138 - acc: 0.5252
4544/4849 [===========================>..] - ETA: 21s - loss: 0.7136 - acc: 0.5260
4608/4849 [===========================>..] - ETA: 16s - loss: 0.7132 - acc: 0.5273
4672/4849 [===========================>..] - ETA: 12s - loss: 0.7136 - acc: 0.5263
4736/4849 [============================>.] - ETA: 7s - loss: 0.7137 - acc: 0.5264 
4800/4849 [============================>.] - ETA: 3s - loss: 0.7141 - acc: 0.5258
4849/4849 [==============================] - 356s 73ms/step - loss: 0.7140 - acc: 0.5257 - val_loss: 0.7041 - val_acc: 0.5362

Epoch 00001: val_acc improved from -inf to 0.53618, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window07/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 5:38 - loss: 0.7096 - acc: 0.5156
 128/4849 [..............................] - ETA: 5:29 - loss: 0.7166 - acc: 0.5391
 192/4849 [>.............................] - ETA: 5:33 - loss: 0.7124 - acc: 0.5365
 256/4849 [>.............................] - ETA: 5:20 - loss: 0.7106 - acc: 0.5430
 320/4849 [>.............................] - ETA: 5:10 - loss: 0.7125 - acc: 0.5188
 384/4849 [=>............................] - ETA: 5:05 - loss: 0.7082 - acc: 0.5260
 448/4849 [=>............................] - ETA: 4:59 - loss: 0.7065 - acc: 0.5246
 512/4849 [==>...........................] - ETA: 4:59 - loss: 0.7081 - acc: 0.5254
 576/4849 [==>...........................] - ETA: 4:56 - loss: 0.7046 - acc: 0.5365
 640/4849 [==>...........................] - ETA: 4:53 - loss: 0.6978 - acc: 0.5484
 704/4849 [===>..........................] - ETA: 4:47 - loss: 0.6994 - acc: 0.5455
 768/4849 [===>..........................] - ETA: 4:43 - loss: 0.6976 - acc: 0.5456
 832/4849 [====>.........................] - ETA: 4:39 - loss: 0.6994 - acc: 0.5445
 896/4849 [====>.........................] - ETA: 4:31 - loss: 0.7002 - acc: 0.5391
 960/4849 [====>.........................] - ETA: 4:28 - loss: 0.6998 - acc: 0.5365
1024/4849 [=====>........................] - ETA: 4:25 - loss: 0.6980 - acc: 0.5410
1088/4849 [=====>........................] - ETA: 4:19 - loss: 0.6949 - acc: 0.5432
1152/4849 [======>.......................] - ETA: 4:13 - loss: 0.6975 - acc: 0.5382
1216/4849 [======>.......................] - ETA: 4:09 - loss: 0.6978 - acc: 0.5378
1280/4849 [======>.......................] - ETA: 4:04 - loss: 0.6956 - acc: 0.5422
1344/4849 [=======>......................] - ETA: 3:59 - loss: 0.6931 - acc: 0.5446
1408/4849 [=======>......................] - ETA: 3:54 - loss: 0.6926 - acc: 0.5433
1472/4849 [========>.....................] - ETA: 3:51 - loss: 0.6943 - acc: 0.5394
1536/4849 [========>.....................] - ETA: 3:47 - loss: 0.6940 - acc: 0.5397
1600/4849 [========>.....................] - ETA: 3:42 - loss: 0.6937 - acc: 0.5381
1664/4849 [=========>....................] - ETA: 3:37 - loss: 0.6948 - acc: 0.5367
1728/4849 [=========>....................] - ETA: 3:32 - loss: 0.6968 - acc: 0.5318
1792/4849 [==========>...................] - ETA: 3:27 - loss: 0.6962 - acc: 0.5329
1856/4849 [==========>...................] - ETA: 3:23 - loss: 0.6964 - acc: 0.5312
1920/4849 [==========>...................] - ETA: 3:20 - loss: 0.6970 - acc: 0.5312
1984/4849 [===========>..................] - ETA: 3:16 - loss: 0.6989 - acc: 0.5267
2048/4849 [===========>..................] - ETA: 3:11 - loss: 0.6990 - acc: 0.5264
2112/4849 [============>.................] - ETA: 3:07 - loss: 0.6994 - acc: 0.5260
2176/4849 [============>.................] - ETA: 3:02 - loss: 0.6983 - acc: 0.5290
2240/4849 [============>.................] - ETA: 2:58 - loss: 0.6985 - acc: 0.5259
2304/4849 [=============>................] - ETA: 2:54 - loss: 0.6995 - acc: 0.5243
2368/4849 [=============>................] - ETA: 2:49 - loss: 0.6992 - acc: 0.5262
2432/4849 [==============>...............] - ETA: 2:45 - loss: 0.7001 - acc: 0.5243
2496/4849 [==============>...............] - ETA: 2:41 - loss: 0.7005 - acc: 0.5236
2560/4849 [==============>...............] - ETA: 2:36 - loss: 0.7011 - acc: 0.5234
2624/4849 [===============>..............] - ETA: 2:32 - loss: 0.7007 - acc: 0.5236
2688/4849 [===============>..............] - ETA: 2:27 - loss: 0.7014 - acc: 0.5216
2752/4849 [================>.............] - ETA: 2:23 - loss: 0.7010 - acc: 0.5225
2816/4849 [================>.............] - ETA: 2:19 - loss: 0.7012 - acc: 0.5217
2880/4849 [================>.............] - ETA: 2:14 - loss: 0.7012 - acc: 0.5201
2944/4849 [=================>............] - ETA: 2:10 - loss: 0.7012 - acc: 0.5217
3008/4849 [=================>............] - ETA: 2:05 - loss: 0.7006 - acc: 0.5229
3072/4849 [==================>...........] - ETA: 2:01 - loss: 0.6997 - acc: 0.5254
3136/4849 [==================>...........] - ETA: 1:57 - loss: 0.6999 - acc: 0.5249
3200/4849 [==================>...........] - ETA: 1:52 - loss: 0.7000 - acc: 0.5231
3264/4849 [===================>..........] - ETA: 1:48 - loss: 0.6997 - acc: 0.5242
3328/4849 [===================>..........] - ETA: 1:43 - loss: 0.6999 - acc: 0.5249
3392/4849 [===================>..........] - ETA: 1:38 - loss: 0.6996 - acc: 0.5251
3456/4849 [====================>.........] - ETA: 1:34 - loss: 0.6994 - acc: 0.5255
3520/4849 [====================>.........] - ETA: 1:30 - loss: 0.6999 - acc: 0.5250
3584/4849 [=====================>........] - ETA: 1:25 - loss: 0.6995 - acc: 0.5257
3648/4849 [=====================>........] - ETA: 1:21 - loss: 0.6994 - acc: 0.5255
3712/4849 [=====================>........] - ETA: 1:16 - loss: 0.6996 - acc: 0.5242
3776/4849 [======================>.......] - ETA: 1:12 - loss: 0.6998 - acc: 0.5254
3840/4849 [======================>.......] - ETA: 1:08 - loss: 0.6994 - acc: 0.5273
3904/4849 [=======================>......] - ETA: 1:03 - loss: 0.6991 - acc: 0.5277
3968/4849 [=======================>......] - ETA: 59s - loss: 0.6992 - acc: 0.5275 
4032/4849 [=======================>......] - ETA: 55s - loss: 0.6991 - acc: 0.5288
4096/4849 [========================>.....] - ETA: 51s - loss: 0.6986 - acc: 0.5303
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6984 - acc: 0.5312
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6979 - acc: 0.5327
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6975 - acc: 0.5326
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6975 - acc: 0.5326
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6981 - acc: 0.5317
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6981 - acc: 0.5315
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6980 - acc: 0.5315
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6978 - acc: 0.5328
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6969 - acc: 0.5353
4736/4849 [============================>.] - ETA: 7s - loss: 0.6967 - acc: 0.5353 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6964 - acc: 0.5354
4849/4849 [==============================] - 347s 72ms/step - loss: 0.6968 - acc: 0.5347 - val_loss: 0.6874 - val_acc: 0.5306

Epoch 00002: val_acc did not improve from 0.53618
Epoch 3/10

  64/4849 [..............................] - ETA: 5:14 - loss: 0.7324 - acc: 0.3750
 128/4849 [..............................] - ETA: 5:06 - loss: 0.6935 - acc: 0.4766
 192/4849 [>.............................] - ETA: 5:09 - loss: 0.6872 - acc: 0.4896
 256/4849 [>.............................] - ETA: 5:19 - loss: 0.6768 - acc: 0.5117
 320/4849 [>.............................] - ETA: 5:23 - loss: 0.6785 - acc: 0.5281
 384/4849 [=>............................] - ETA: 5:15 - loss: 0.6815 - acc: 0.5339
 448/4849 [=>............................] - ETA: 5:08 - loss: 0.6799 - acc: 0.5379
 512/4849 [==>...........................] - ETA: 5:04 - loss: 0.6868 - acc: 0.5312
 576/4849 [==>...........................] - ETA: 4:57 - loss: 0.6878 - acc: 0.5312
 640/4849 [==>...........................] - ETA: 4:50 - loss: 0.6871 - acc: 0.5406
 704/4849 [===>..........................] - ETA: 4:49 - loss: 0.6864 - acc: 0.5426
 768/4849 [===>..........................] - ETA: 4:47 - loss: 0.6903 - acc: 0.5326
 832/4849 [====>.........................] - ETA: 4:43 - loss: 0.6944 - acc: 0.5228
 896/4849 [====>.........................] - ETA: 4:36 - loss: 0.6935 - acc: 0.5279
 960/4849 [====>.........................] - ETA: 4:32 - loss: 0.6953 - acc: 0.5271
1024/4849 [=====>........................] - ETA: 4:25 - loss: 0.6962 - acc: 0.5205
1088/4849 [=====>........................] - ETA: 4:20 - loss: 0.6948 - acc: 0.5230
1152/4849 [======>.......................] - ETA: 4:17 - loss: 0.6944 - acc: 0.5208
1216/4849 [======>.......................] - ETA: 4:13 - loss: 0.6935 - acc: 0.5238
1280/4849 [======>.......................] - ETA: 4:07 - loss: 0.6930 - acc: 0.5227
1344/4849 [=======>......................] - ETA: 4:02 - loss: 0.6924 - acc: 0.5268
1408/4849 [=======>......................] - ETA: 3:57 - loss: 0.6925 - acc: 0.5277
1472/4849 [========>.....................] - ETA: 3:51 - loss: 0.6937 - acc: 0.5245
1536/4849 [========>.....................] - ETA: 3:46 - loss: 0.6932 - acc: 0.5280
1600/4849 [========>.....................] - ETA: 3:41 - loss: 0.6914 - acc: 0.5319
1664/4849 [=========>....................] - ETA: 3:37 - loss: 0.6908 - acc: 0.5331
1728/4849 [=========>....................] - ETA: 3:33 - loss: 0.6924 - acc: 0.5301
1792/4849 [==========>...................] - ETA: 3:29 - loss: 0.6922 - acc: 0.5301
1856/4849 [==========>...................] - ETA: 3:25 - loss: 0.6932 - acc: 0.5280
1920/4849 [==========>...................] - ETA: 3:20 - loss: 0.6927 - acc: 0.5302
1984/4849 [===========>..................] - ETA: 3:16 - loss: 0.6931 - acc: 0.5287
2048/4849 [===========>..................] - ETA: 3:11 - loss: 0.6927 - acc: 0.5298
2112/4849 [============>.................] - ETA: 3:08 - loss: 0.6926 - acc: 0.5279
2176/4849 [============>.................] - ETA: 3:03 - loss: 0.6928 - acc: 0.5280
2240/4849 [============>.................] - ETA: 2:58 - loss: 0.6936 - acc: 0.5254
2304/4849 [=============>................] - ETA: 2:54 - loss: 0.6945 - acc: 0.5234
2368/4849 [=============>................] - ETA: 2:49 - loss: 0.6941 - acc: 0.5245
2432/4849 [==============>...............] - ETA: 2:44 - loss: 0.6933 - acc: 0.5263
2496/4849 [==============>...............] - ETA: 2:41 - loss: 0.6926 - acc: 0.5264
2560/4849 [==============>...............] - ETA: 2:37 - loss: 0.6918 - acc: 0.5289
2624/4849 [===============>..............] - ETA: 2:32 - loss: 0.6915 - acc: 0.5309
2688/4849 [===============>..............] - ETA: 2:28 - loss: 0.6913 - acc: 0.5320
2752/4849 [================>.............] - ETA: 2:23 - loss: 0.6916 - acc: 0.5327
2816/4849 [================>.............] - ETA: 2:19 - loss: 0.6913 - acc: 0.5327
2880/4849 [================>.............] - ETA: 2:14 - loss: 0.6911 - acc: 0.5326
2944/4849 [=================>............] - ETA: 2:10 - loss: 0.6910 - acc: 0.5326
3008/4849 [=================>............] - ETA: 2:06 - loss: 0.6914 - acc: 0.5322
3072/4849 [==================>...........] - ETA: 2:01 - loss: 0.6915 - acc: 0.5316
3136/4849 [==================>...........] - ETA: 1:57 - loss: 0.6919 - acc: 0.5316
3200/4849 [==================>...........] - ETA: 1:52 - loss: 0.6916 - acc: 0.5319
3264/4849 [===================>..........] - ETA: 1:48 - loss: 0.6910 - acc: 0.5328
3328/4849 [===================>..........] - ETA: 1:43 - loss: 0.6908 - acc: 0.5328
3392/4849 [===================>..........] - ETA: 1:39 - loss: 0.6903 - acc: 0.5339
3456/4849 [====================>.........] - ETA: 1:35 - loss: 0.6901 - acc: 0.5336
3520/4849 [====================>.........] - ETA: 1:30 - loss: 0.6896 - acc: 0.5338
3584/4849 [=====================>........] - ETA: 1:26 - loss: 0.6895 - acc: 0.5343
3648/4849 [=====================>........] - ETA: 1:22 - loss: 0.6897 - acc: 0.5340
3712/4849 [=====================>........] - ETA: 1:17 - loss: 0.6889 - acc: 0.5348
3776/4849 [======================>.......] - ETA: 1:13 - loss: 0.6895 - acc: 0.5352
3840/4849 [======================>.......] - ETA: 1:08 - loss: 0.6893 - acc: 0.5362
3904/4849 [=======================>......] - ETA: 1:04 - loss: 0.6891 - acc: 0.5371
3968/4849 [=======================>......] - ETA: 1:00 - loss: 0.6888 - acc: 0.5378
4032/4849 [=======================>......] - ETA: 55s - loss: 0.6890 - acc: 0.5372 
4096/4849 [========================>.....] - ETA: 51s - loss: 0.6893 - acc: 0.5371
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6895 - acc: 0.5368
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6896 - acc: 0.5369
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6888 - acc: 0.5394
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6889 - acc: 0.5400
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6887 - acc: 0.5410
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6886 - acc: 0.5413
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6882 - acc: 0.5423
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6882 - acc: 0.5430
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6885 - acc: 0.5432
4736/4849 [============================>.] - ETA: 7s - loss: 0.6884 - acc: 0.5433 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6884 - acc: 0.5437
4849/4849 [==============================] - 343s 71ms/step - loss: 0.6881 - acc: 0.5449 - val_loss: 0.7046 - val_acc: 0.4898

Epoch 00003: val_acc did not improve from 0.53618
Epoch 4/10

  64/4849 [..............................] - ETA: 4:54 - loss: 0.7480 - acc: 0.4688
 128/4849 [..............................] - ETA: 4:52 - loss: 0.7142 - acc: 0.5469
 192/4849 [>.............................] - ETA: 4:56 - loss: 0.7111 - acc: 0.5260
 256/4849 [>.............................] - ETA: 5:05 - loss: 0.7016 - acc: 0.5273
 320/4849 [>.............................] - ETA: 5:04 - loss: 0.6975 - acc: 0.5312
 384/4849 [=>............................] - ETA: 4:59 - loss: 0.6967 - acc: 0.5312
 448/4849 [=>............................] - ETA: 4:52 - loss: 0.6915 - acc: 0.5446
 512/4849 [==>...........................] - ETA: 4:49 - loss: 0.6943 - acc: 0.5371
 576/4849 [==>...........................] - ETA: 4:42 - loss: 0.6916 - acc: 0.5399
 640/4849 [==>...........................] - ETA: 4:36 - loss: 0.6936 - acc: 0.5375
 704/4849 [===>..........................] - ETA: 4:30 - loss: 0.6904 - acc: 0.5469
 768/4849 [===>..........................] - ETA: 4:29 - loss: 0.6874 - acc: 0.5508
 832/4849 [====>.........................] - ETA: 4:26 - loss: 0.6879 - acc: 0.5469
 896/4849 [====>.........................] - ETA: 4:20 - loss: 0.6888 - acc: 0.5480
 960/4849 [====>.........................] - ETA: 4:16 - loss: 0.6909 - acc: 0.5427
1024/4849 [=====>........................] - ETA: 4:12 - loss: 0.6890 - acc: 0.5459
1088/4849 [=====>........................] - ETA: 4:07 - loss: 0.6907 - acc: 0.5377
1152/4849 [======>.......................] - ETA: 4:04 - loss: 0.6916 - acc: 0.5339
1216/4849 [======>.......................] - ETA: 3:59 - loss: 0.6903 - acc: 0.5378
1280/4849 [======>.......................] - ETA: 3:54 - loss: 0.6895 - acc: 0.5398
1344/4849 [=======>......................] - ETA: 3:49 - loss: 0.6914 - acc: 0.5350
1408/4849 [=======>......................] - ETA: 3:44 - loss: 0.6906 - acc: 0.5369
1472/4849 [========>.....................] - ETA: 3:40 - loss: 0.6906 - acc: 0.5360
1536/4849 [========>.....................] - ETA: 3:37 - loss: 0.6896 - acc: 0.5404
1600/4849 [========>.....................] - ETA: 3:34 - loss: 0.6915 - acc: 0.5356
1664/4849 [=========>....................] - ETA: 3:29 - loss: 0.6907 - acc: 0.5349
1728/4849 [=========>....................] - ETA: 3:24 - loss: 0.6900 - acc: 0.5365
1792/4849 [==========>...................] - ETA: 3:20 - loss: 0.6894 - acc: 0.5368
1856/4849 [==========>...................] - ETA: 3:16 - loss: 0.6894 - acc: 0.5377
1920/4849 [==========>...................] - ETA: 3:11 - loss: 0.6888 - acc: 0.5396
1984/4849 [===========>..................] - ETA: 3:08 - loss: 0.6895 - acc: 0.5398
2048/4849 [===========>..................] - ETA: 3:04 - loss: 0.6903 - acc: 0.5376
2112/4849 [============>.................] - ETA: 3:00 - loss: 0.6895 - acc: 0.5402
2176/4849 [============>.................] - ETA: 2:56 - loss: 0.6876 - acc: 0.5460
2240/4849 [============>.................] - ETA: 2:51 - loss: 0.6874 - acc: 0.5460
2304/4849 [=============>................] - ETA: 2:47 - loss: 0.6877 - acc: 0.5447
2368/4849 [=============>................] - ETA: 2:43 - loss: 0.6884 - acc: 0.5439
2432/4849 [==============>...............] - ETA: 2:39 - loss: 0.6875 - acc: 0.5456
2496/4849 [==============>...............] - ETA: 2:34 - loss: 0.6873 - acc: 0.5461
2560/4849 [==============>...............] - ETA: 2:30 - loss: 0.6880 - acc: 0.5437
2624/4849 [===============>..............] - ETA: 2:26 - loss: 0.6882 - acc: 0.5431
2688/4849 [===============>..............] - ETA: 2:22 - loss: 0.6886 - acc: 0.5428
2752/4849 [================>.............] - ETA: 2:17 - loss: 0.6876 - acc: 0.5458
2816/4849 [================>.............] - ETA: 2:14 - loss: 0.6876 - acc: 0.5458
2880/4849 [================>.............] - ETA: 2:09 - loss: 0.6876 - acc: 0.5476
2944/4849 [=================>............] - ETA: 2:05 - loss: 0.6869 - acc: 0.5493
3008/4849 [=================>............] - ETA: 2:00 - loss: 0.6868 - acc: 0.5499
3072/4849 [==================>...........] - ETA: 1:56 - loss: 0.6872 - acc: 0.5498
3136/4849 [==================>...........] - ETA: 1:52 - loss: 0.6866 - acc: 0.5513
3200/4849 [==================>...........] - ETA: 1:47 - loss: 0.6879 - acc: 0.5475
3264/4849 [===================>..........] - ETA: 1:43 - loss: 0.6876 - acc: 0.5493
3328/4849 [===================>..........] - ETA: 1:39 - loss: 0.6866 - acc: 0.5523
3392/4849 [===================>..........] - ETA: 1:35 - loss: 0.6864 - acc: 0.5525
3456/4849 [====================>.........] - ETA: 1:30 - loss: 0.6859 - acc: 0.5538
3520/4849 [====================>.........] - ETA: 1:26 - loss: 0.6851 - acc: 0.5557
3584/4849 [=====================>........] - ETA: 1:22 - loss: 0.6850 - acc: 0.5566
3648/4849 [=====================>........] - ETA: 1:18 - loss: 0.6850 - acc: 0.5570
3712/4849 [=====================>........] - ETA: 1:14 - loss: 0.6847 - acc: 0.5579
3776/4849 [======================>.......] - ETA: 1:10 - loss: 0.6848 - acc: 0.5583
3840/4849 [======================>.......] - ETA: 1:05 - loss: 0.6844 - acc: 0.5581
3904/4849 [=======================>......] - ETA: 1:01 - loss: 0.6847 - acc: 0.5587
3968/4849 [=======================>......] - ETA: 57s - loss: 0.6852 - acc: 0.5582 
4032/4849 [=======================>......] - ETA: 53s - loss: 0.6850 - acc: 0.5585
4096/4849 [========================>.....] - ETA: 48s - loss: 0.6843 - acc: 0.5593
4160/4849 [========================>.....] - ETA: 44s - loss: 0.6843 - acc: 0.5594
4224/4849 [=========================>....] - ETA: 40s - loss: 0.6844 - acc: 0.5587
4288/4849 [=========================>....] - ETA: 36s - loss: 0.6842 - acc: 0.5597
4352/4849 [=========================>....] - ETA: 32s - loss: 0.6836 - acc: 0.5611
4416/4849 [==========================>...] - ETA: 28s - loss: 0.6831 - acc: 0.5620
4480/4849 [==========================>...] - ETA: 23s - loss: 0.6833 - acc: 0.5616
4544/4849 [===========================>..] - ETA: 19s - loss: 0.6826 - acc: 0.5632
4608/4849 [===========================>..] - ETA: 15s - loss: 0.6827 - acc: 0.5632
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6834 - acc: 0.5627
4736/4849 [============================>.] - ETA: 7s - loss: 0.6842 - acc: 0.5617 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6847 - acc: 0.5610
4849/4849 [==============================] - 324s 67ms/step - loss: 0.6849 - acc: 0.5597 - val_loss: 0.6852 - val_acc: 0.5510

Epoch 00004: val_acc improved from 0.53618 to 0.55102, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window07/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 5/10

  64/4849 [..............................] - ETA: 5:33 - loss: 0.6644 - acc: 0.5781
 128/4849 [..............................] - ETA: 5:32 - loss: 0.6682 - acc: 0.5859
 192/4849 [>.............................] - ETA: 5:07 - loss: 0.6775 - acc: 0.5625
 256/4849 [>.............................] - ETA: 4:59 - loss: 0.6789 - acc: 0.5664
 320/4849 [>.............................] - ETA: 4:51 - loss: 0.6758 - acc: 0.5594
 384/4849 [=>............................] - ETA: 4:39 - loss: 0.6768 - acc: 0.5599
 448/4849 [=>............................] - ETA: 4:33 - loss: 0.6795 - acc: 0.5670
 512/4849 [==>...........................] - ETA: 4:30 - loss: 0.6839 - acc: 0.5605
 576/4849 [==>...........................] - ETA: 4:26 - loss: 0.6794 - acc: 0.5729
 640/4849 [==>...........................] - ETA: 4:20 - loss: 0.6812 - acc: 0.5578
 704/4849 [===>..........................] - ETA: 4:15 - loss: 0.6847 - acc: 0.5540
 768/4849 [===>..........................] - ETA: 4:08 - loss: 0.6830 - acc: 0.5573
 832/4849 [====>.........................] - ETA: 4:03 - loss: 0.6841 - acc: 0.5577
 896/4849 [====>.........................] - ETA: 3:59 - loss: 0.6840 - acc: 0.5614
 960/4849 [====>.........................] - ETA: 3:57 - loss: 0.6838 - acc: 0.5635
1024/4849 [=====>........................] - ETA: 3:51 - loss: 0.6861 - acc: 0.5576
1088/4849 [=====>........................] - ETA: 3:45 - loss: 0.6863 - acc: 0.5579
1152/4849 [======>.......................] - ETA: 3:39 - loss: 0.6855 - acc: 0.5616
1216/4849 [======>.......................] - ETA: 3:34 - loss: 0.6852 - acc: 0.5625
1280/4849 [======>.......................] - ETA: 3:31 - loss: 0.6844 - acc: 0.5633
1344/4849 [=======>......................] - ETA: 3:26 - loss: 0.6820 - acc: 0.5662
1408/4849 [=======>......................] - ETA: 3:21 - loss: 0.6828 - acc: 0.5653
1472/4849 [========>.....................] - ETA: 3:17 - loss: 0.6827 - acc: 0.5666
1536/4849 [========>.....................] - ETA: 3:13 - loss: 0.6816 - acc: 0.5677
1600/4849 [========>.....................] - ETA: 3:08 - loss: 0.6820 - acc: 0.5644
1664/4849 [=========>....................] - ETA: 3:06 - loss: 0.6813 - acc: 0.5661
1728/4849 [=========>....................] - ETA: 3:01 - loss: 0.6807 - acc: 0.5648
1792/4849 [==========>...................] - ETA: 2:57 - loss: 0.6800 - acc: 0.5675
1856/4849 [==========>...................] - ETA: 2:53 - loss: 0.6796 - acc: 0.5706
1920/4849 [==========>...................] - ETA: 2:48 - loss: 0.6814 - acc: 0.5677
1984/4849 [===========>..................] - ETA: 2:44 - loss: 0.6833 - acc: 0.5625
2048/4849 [===========>..................] - ETA: 2:41 - loss: 0.6829 - acc: 0.5645
2112/4849 [============>.................] - ETA: 2:37 - loss: 0.6813 - acc: 0.5672
2176/4849 [============>.................] - ETA: 2:33 - loss: 0.6806 - acc: 0.5689
2240/4849 [============>.................] - ETA: 2:29 - loss: 0.6805 - acc: 0.5701
2304/4849 [=============>................] - ETA: 2:25 - loss: 0.6814 - acc: 0.5677
2368/4849 [=============>................] - ETA: 2:21 - loss: 0.6815 - acc: 0.5697
2432/4849 [==============>...............] - ETA: 2:18 - loss: 0.6824 - acc: 0.5666
2496/4849 [==============>...............] - ETA: 2:14 - loss: 0.6816 - acc: 0.5677
2560/4849 [==============>...............] - ETA: 2:10 - loss: 0.6798 - acc: 0.5703
2624/4849 [===============>..............] - ETA: 2:06 - loss: 0.6799 - acc: 0.5694
2688/4849 [===============>..............] - ETA: 2:03 - loss: 0.6805 - acc: 0.5677
2752/4849 [================>.............] - ETA: 2:00 - loss: 0.6796 - acc: 0.5698
2816/4849 [================>.............] - ETA: 1:57 - loss: 0.6798 - acc: 0.5696
2880/4849 [================>.............] - ETA: 1:54 - loss: 0.6802 - acc: 0.5684
2944/4849 [=================>............] - ETA: 1:51 - loss: 0.6810 - acc: 0.5676
3008/4849 [=================>............] - ETA: 1:47 - loss: 0.6802 - acc: 0.5688
3072/4849 [==================>...........] - ETA: 1:44 - loss: 0.6798 - acc: 0.5693
3136/4849 [==================>...........] - ETA: 1:41 - loss: 0.6805 - acc: 0.5679
3200/4849 [==================>...........] - ETA: 1:37 - loss: 0.6795 - acc: 0.5709
3264/4849 [===================>..........] - ETA: 1:34 - loss: 0.6800 - acc: 0.5695
3328/4849 [===================>..........] - ETA: 1:31 - loss: 0.6798 - acc: 0.5700
3392/4849 [===================>..........] - ETA: 1:27 - loss: 0.6797 - acc: 0.5705
3456/4849 [====================>.........] - ETA: 1:23 - loss: 0.6789 - acc: 0.5709
3520/4849 [====================>.........] - ETA: 1:20 - loss: 0.6788 - acc: 0.5719
3584/4849 [=====================>........] - ETA: 1:16 - loss: 0.6785 - acc: 0.5725
3648/4849 [=====================>........] - ETA: 1:13 - loss: 0.6795 - acc: 0.5704
3712/4849 [=====================>........] - ETA: 1:09 - loss: 0.6793 - acc: 0.5714
3776/4849 [======================>.......] - ETA: 1:05 - loss: 0.6795 - acc: 0.5702
3840/4849 [======================>.......] - ETA: 1:01 - loss: 0.6791 - acc: 0.5701
3904/4849 [=======================>......] - ETA: 57s - loss: 0.6791 - acc: 0.5697 
3968/4849 [=======================>......] - ETA: 54s - loss: 0.6791 - acc: 0.5698
4032/4849 [=======================>......] - ETA: 50s - loss: 0.6798 - acc: 0.5689
4096/4849 [========================>.....] - ETA: 46s - loss: 0.6797 - acc: 0.5686
4160/4849 [========================>.....] - ETA: 42s - loss: 0.6797 - acc: 0.5680
4224/4849 [=========================>....] - ETA: 38s - loss: 0.6799 - acc: 0.5675
4288/4849 [=========================>....] - ETA: 34s - loss: 0.6802 - acc: 0.5674
4352/4849 [=========================>....] - ETA: 31s - loss: 0.6806 - acc: 0.5659
4416/4849 [==========================>...] - ETA: 27s - loss: 0.6805 - acc: 0.5659
4480/4849 [==========================>...] - ETA: 23s - loss: 0.6797 - acc: 0.5679
4544/4849 [===========================>..] - ETA: 19s - loss: 0.6793 - acc: 0.5682
4608/4849 [===========================>..] - ETA: 15s - loss: 0.6796 - acc: 0.5684
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6800 - acc: 0.5661
4736/4849 [============================>.] - ETA: 7s - loss: 0.6796 - acc: 0.5676 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6793 - acc: 0.5681
4849/4849 [==============================] - 333s 69ms/step - loss: 0.6798 - acc: 0.5677 - val_loss: 0.6892 - val_acc: 0.5473

Epoch 00005: val_acc did not improve from 0.55102
Epoch 6/10

  64/4849 [..............................] - ETA: 7:09 - loss: 0.6584 - acc: 0.6250
 128/4849 [..............................] - ETA: 6:59 - loss: 0.6543 - acc: 0.6406
 192/4849 [>.............................] - ETA: 6:59 - loss: 0.6617 - acc: 0.6042
 256/4849 [>.............................] - ETA: 6:59 - loss: 0.6696 - acc: 0.5742
 320/4849 [>.............................] - ETA: 6:52 - loss: 0.6732 - acc: 0.5687
 384/4849 [=>............................] - ETA: 6:34 - loss: 0.6702 - acc: 0.5755
 448/4849 [=>............................] - ETA: 6:25 - loss: 0.6681 - acc: 0.5804
 512/4849 [==>...........................] - ETA: 6:18 - loss: 0.6673 - acc: 0.5898
 576/4849 [==>...........................] - ETA: 6:14 - loss: 0.6665 - acc: 0.5885
 640/4849 [==>...........................] - ETA: 6:07 - loss: 0.6706 - acc: 0.5797
 704/4849 [===>..........................] - ETA: 6:02 - loss: 0.6749 - acc: 0.5767
 768/4849 [===>..........................] - ETA: 5:57 - loss: 0.6711 - acc: 0.5872
 832/4849 [====>.........................] - ETA: 5:51 - loss: 0.6718 - acc: 0.5853
 896/4849 [====>.........................] - ETA: 5:45 - loss: 0.6745 - acc: 0.5848
 960/4849 [====>.........................] - ETA: 5:41 - loss: 0.6744 - acc: 0.5823
1024/4849 [=====>........................] - ETA: 5:33 - loss: 0.6747 - acc: 0.5811
1088/4849 [=====>........................] - ETA: 5:26 - loss: 0.6758 - acc: 0.5790
1152/4849 [======>.......................] - ETA: 5:21 - loss: 0.6753 - acc: 0.5799
1216/4849 [======>.......................] - ETA: 5:14 - loss: 0.6792 - acc: 0.5773
1280/4849 [======>.......................] - ETA: 5:09 - loss: 0.6773 - acc: 0.5797
1344/4849 [=======>......................] - ETA: 5:03 - loss: 0.6757 - acc: 0.5818
1408/4849 [=======>......................] - ETA: 4:57 - loss: 0.6761 - acc: 0.5817
1472/4849 [========>.....................] - ETA: 4:51 - loss: 0.6755 - acc: 0.5836
1536/4849 [========>.....................] - ETA: 4:45 - loss: 0.6755 - acc: 0.5840
1600/4849 [========>.....................] - ETA: 4:40 - loss: 0.6768 - acc: 0.5825
1664/4849 [=========>....................] - ETA: 4:34 - loss: 0.6775 - acc: 0.5805
1728/4849 [=========>....................] - ETA: 4:28 - loss: 0.6760 - acc: 0.5845
1792/4849 [==========>...................] - ETA: 4:23 - loss: 0.6759 - acc: 0.5854
1856/4849 [==========>...................] - ETA: 4:17 - loss: 0.6776 - acc: 0.5841
1920/4849 [==========>...................] - ETA: 4:12 - loss: 0.6792 - acc: 0.5807
1984/4849 [===========>..................] - ETA: 4:06 - loss: 0.6797 - acc: 0.5806
2048/4849 [===========>..................] - ETA: 4:01 - loss: 0.6795 - acc: 0.5806
2112/4849 [============>.................] - ETA: 3:55 - loss: 0.6795 - acc: 0.5805
2176/4849 [============>.................] - ETA: 3:49 - loss: 0.6796 - acc: 0.5795
2240/4849 [============>.................] - ETA: 3:43 - loss: 0.6810 - acc: 0.5754
2304/4849 [=============>................] - ETA: 3:38 - loss: 0.6809 - acc: 0.5747
2368/4849 [=============>................] - ETA: 3:32 - loss: 0.6804 - acc: 0.5764
2432/4849 [==============>...............] - ETA: 3:26 - loss: 0.6795 - acc: 0.5773
2496/4849 [==============>...............] - ETA: 3:21 - loss: 0.6789 - acc: 0.5785
2560/4849 [==============>...............] - ETA: 3:16 - loss: 0.6795 - acc: 0.5770
2624/4849 [===============>..............] - ETA: 3:10 - loss: 0.6796 - acc: 0.5766
2688/4849 [===============>..............] - ETA: 3:05 - loss: 0.6801 - acc: 0.5744
2752/4849 [================>.............] - ETA: 2:59 - loss: 0.6798 - acc: 0.5741
2816/4849 [================>.............] - ETA: 2:54 - loss: 0.6793 - acc: 0.5763
2880/4849 [================>.............] - ETA: 2:48 - loss: 0.6796 - acc: 0.5750
2944/4849 [=================>............] - ETA: 2:43 - loss: 0.6795 - acc: 0.5757
3008/4849 [=================>............] - ETA: 2:38 - loss: 0.6793 - acc: 0.5741
3072/4849 [==================>...........] - ETA: 2:32 - loss: 0.6799 - acc: 0.5723
3136/4849 [==================>...........] - ETA: 2:27 - loss: 0.6797 - acc: 0.5721
3200/4849 [==================>...........] - ETA: 2:21 - loss: 0.6795 - acc: 0.5716
3264/4849 [===================>..........] - ETA: 2:16 - loss: 0.6795 - acc: 0.5726
3328/4849 [===================>..........] - ETA: 2:11 - loss: 0.6795 - acc: 0.5721
3392/4849 [===================>..........] - ETA: 2:05 - loss: 0.6800 - acc: 0.5705
3456/4849 [====================>.........] - ETA: 2:00 - loss: 0.6797 - acc: 0.5703
3520/4849 [====================>.........] - ETA: 1:54 - loss: 0.6803 - acc: 0.5682
3584/4849 [=====================>........] - ETA: 1:49 - loss: 0.6797 - acc: 0.5695
3648/4849 [=====================>........] - ETA: 1:43 - loss: 0.6791 - acc: 0.5710
3712/4849 [=====================>........] - ETA: 1:38 - loss: 0.6789 - acc: 0.5711
3776/4849 [======================>.......] - ETA: 1:32 - loss: 0.6785 - acc: 0.5712
3840/4849 [======================>.......] - ETA: 1:27 - loss: 0.6788 - acc: 0.5708
3904/4849 [=======================>......] - ETA: 1:21 - loss: 0.6789 - acc: 0.5699
3968/4849 [=======================>......] - ETA: 1:16 - loss: 0.6788 - acc: 0.5703
4032/4849 [=======================>......] - ETA: 1:10 - loss: 0.6787 - acc: 0.5709
4096/4849 [========================>.....] - ETA: 1:05 - loss: 0.6779 - acc: 0.5725
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6776 - acc: 0.5736 
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6780 - acc: 0.5724
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6782 - acc: 0.5718
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6786 - acc: 0.5715
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6781 - acc: 0.5709
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6784 - acc: 0.5705
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6790 - acc: 0.5704
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6797 - acc: 0.5692
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6791 - acc: 0.5698
4736/4849 [============================>.] - ETA: 9s - loss: 0.6797 - acc: 0.5682 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6791 - acc: 0.5702
4849/4849 [==============================] - 436s 90ms/step - loss: 0.6792 - acc: 0.5702 - val_loss: 0.6936 - val_acc: 0.5547

Epoch 00006: val_acc improved from 0.55102 to 0.55473, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window07/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 7/10

  64/4849 [..............................] - ETA: 7:07 - loss: 0.7145 - acc: 0.5156
 128/4849 [..............................] - ETA: 7:02 - loss: 0.7042 - acc: 0.5312
 192/4849 [>.............................] - ETA: 6:49 - loss: 0.6935 - acc: 0.5625
 256/4849 [>.............................] - ETA: 6:47 - loss: 0.6842 - acc: 0.5586
 320/4849 [>.............................] - ETA: 6:38 - loss: 0.6815 - acc: 0.5625
 384/4849 [=>............................] - ETA: 6:31 - loss: 0.6811 - acc: 0.5625
 448/4849 [=>............................] - ETA: 6:30 - loss: 0.6815 - acc: 0.5558
 512/4849 [==>...........................] - ETA: 6:23 - loss: 0.6815 - acc: 0.5605
 576/4849 [==>...........................] - ETA: 6:17 - loss: 0.6801 - acc: 0.5625
 640/4849 [==>...........................] - ETA: 6:16 - loss: 0.6801 - acc: 0.5594
 704/4849 [===>..........................] - ETA: 6:09 - loss: 0.6776 - acc: 0.5668
 768/4849 [===>..........................] - ETA: 6:01 - loss: 0.6754 - acc: 0.5716
 832/4849 [====>.........................] - ETA: 5:54 - loss: 0.6740 - acc: 0.5745
 896/4849 [====>.........................] - ETA: 5:48 - loss: 0.6736 - acc: 0.5725
 960/4849 [====>.........................] - ETA: 5:38 - loss: 0.6758 - acc: 0.5698
1024/4849 [=====>........................] - ETA: 5:29 - loss: 0.6768 - acc: 0.5684
1088/4849 [=====>........................] - ETA: 5:20 - loss: 0.6772 - acc: 0.5653
1152/4849 [======>.......................] - ETA: 5:11 - loss: 0.6763 - acc: 0.5703
1216/4849 [======>.......................] - ETA: 5:03 - loss: 0.6781 - acc: 0.5641
1280/4849 [======>.......................] - ETA: 4:56 - loss: 0.6782 - acc: 0.5656
1344/4849 [=======>......................] - ETA: 4:50 - loss: 0.6785 - acc: 0.5670
1408/4849 [=======>......................] - ETA: 4:43 - loss: 0.6775 - acc: 0.5717
1472/4849 [========>.....................] - ETA: 4:38 - loss: 0.6770 - acc: 0.5734
1536/4849 [========>.....................] - ETA: 4:32 - loss: 0.6749 - acc: 0.5775
1600/4849 [========>.....................] - ETA: 4:24 - loss: 0.6740 - acc: 0.5787
1664/4849 [=========>....................] - ETA: 4:17 - loss: 0.6722 - acc: 0.5829
1728/4849 [=========>....................] - ETA: 4:11 - loss: 0.6711 - acc: 0.5851
1792/4849 [==========>...................] - ETA: 4:05 - loss: 0.6706 - acc: 0.5854
1856/4849 [==========>...................] - ETA: 3:59 - loss: 0.6709 - acc: 0.5841
1920/4849 [==========>...................] - ETA: 3:53 - loss: 0.6728 - acc: 0.5823
1984/4849 [===========>..................] - ETA: 3:47 - loss: 0.6732 - acc: 0.5837
2048/4849 [===========>..................] - ETA: 3:40 - loss: 0.6733 - acc: 0.5835
2112/4849 [============>.................] - ETA: 3:35 - loss: 0.6728 - acc: 0.5843
2176/4849 [============>.................] - ETA: 3:30 - loss: 0.6707 - acc: 0.5882
2240/4849 [============>.................] - ETA: 3:24 - loss: 0.6717 - acc: 0.5871
2304/4849 [=============>................] - ETA: 3:18 - loss: 0.6702 - acc: 0.5894
2368/4849 [=============>................] - ETA: 3:13 - loss: 0.6709 - acc: 0.5887
2432/4849 [==============>...............] - ETA: 3:08 - loss: 0.6734 - acc: 0.5855
2496/4849 [==============>...............] - ETA: 3:02 - loss: 0.6719 - acc: 0.5873
2560/4849 [==============>...............] - ETA: 2:57 - loss: 0.6722 - acc: 0.5859
2624/4849 [===============>..............] - ETA: 2:51 - loss: 0.6727 - acc: 0.5842
2688/4849 [===============>..............] - ETA: 2:46 - loss: 0.6717 - acc: 0.5859
2752/4849 [================>.............] - ETA: 2:41 - loss: 0.6720 - acc: 0.5858
2816/4849 [================>.............] - ETA: 2:36 - loss: 0.6732 - acc: 0.5845
2880/4849 [================>.............] - ETA: 2:31 - loss: 0.6731 - acc: 0.5847
2944/4849 [=================>............] - ETA: 2:26 - loss: 0.6729 - acc: 0.5856
3008/4849 [=================>............] - ETA: 2:20 - loss: 0.6734 - acc: 0.5861
3072/4849 [==================>...........] - ETA: 2:15 - loss: 0.6726 - acc: 0.5879
3136/4849 [==================>...........] - ETA: 2:10 - loss: 0.6728 - acc: 0.5877
3200/4849 [==================>...........] - ETA: 2:05 - loss: 0.6743 - acc: 0.5850
3264/4849 [===================>..........] - ETA: 2:00 - loss: 0.6747 - acc: 0.5839
3328/4849 [===================>..........] - ETA: 1:55 - loss: 0.6744 - acc: 0.5841
3392/4849 [===================>..........] - ETA: 1:50 - loss: 0.6754 - acc: 0.5823
3456/4849 [====================>.........] - ETA: 1:45 - loss: 0.6760 - acc: 0.5813
3520/4849 [====================>.........] - ETA: 1:40 - loss: 0.6760 - acc: 0.5815
3584/4849 [=====================>........] - ETA: 1:35 - loss: 0.6760 - acc: 0.5809
3648/4849 [=====================>........] - ETA: 1:30 - loss: 0.6761 - acc: 0.5803
3712/4849 [=====================>........] - ETA: 1:25 - loss: 0.6768 - acc: 0.5792
3776/4849 [======================>.......] - ETA: 1:20 - loss: 0.6776 - acc: 0.5781
3840/4849 [======================>.......] - ETA: 1:15 - loss: 0.6782 - acc: 0.5758
3904/4849 [=======================>......] - ETA: 1:10 - loss: 0.6777 - acc: 0.5774
3968/4849 [=======================>......] - ETA: 1:05 - loss: 0.6779 - acc: 0.5766
4032/4849 [=======================>......] - ETA: 1:01 - loss: 0.6779 - acc: 0.5764
4096/4849 [========================>.....] - ETA: 56s - loss: 0.6773 - acc: 0.5769 
4160/4849 [========================>.....] - ETA: 51s - loss: 0.6770 - acc: 0.5779
4224/4849 [=========================>....] - ETA: 46s - loss: 0.6766 - acc: 0.5784
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6763 - acc: 0.5779
4352/4849 [=========================>....] - ETA: 37s - loss: 0.6763 - acc: 0.5786
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6760 - acc: 0.5786
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6759 - acc: 0.5792
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6763 - acc: 0.5781
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6766 - acc: 0.5777
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6766 - acc: 0.5777
4736/4849 [============================>.] - ETA: 8s - loss: 0.6761 - acc: 0.5779 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6760 - acc: 0.5779
4849/4849 [==============================] - 373s 77ms/step - loss: 0.6757 - acc: 0.5783 - val_loss: 0.6869 - val_acc: 0.5529

Epoch 00007: val_acc did not improve from 0.55473
Epoch 8/10

  64/4849 [..............................] - ETA: 6:29 - loss: 0.6994 - acc: 0.5781
 128/4849 [..............................] - ETA: 5:52 - loss: 0.6926 - acc: 0.5625
 192/4849 [>.............................] - ETA: 5:48 - loss: 0.6930 - acc: 0.5625
 256/4849 [>.............................] - ETA: 5:34 - loss: 0.6947 - acc: 0.5508
 320/4849 [>.............................] - ETA: 5:27 - loss: 0.6901 - acc: 0.5563
 384/4849 [=>............................] - ETA: 5:24 - loss: 0.6836 - acc: 0.5729
 448/4849 [=>............................] - ETA: 5:19 - loss: 0.6789 - acc: 0.5804
 512/4849 [==>...........................] - ETA: 5:13 - loss: 0.6817 - acc: 0.5742
 576/4849 [==>...........................] - ETA: 5:03 - loss: 0.6815 - acc: 0.5747
 640/4849 [==>...........................] - ETA: 4:59 - loss: 0.6806 - acc: 0.5750
 704/4849 [===>..........................] - ETA: 4:53 - loss: 0.6786 - acc: 0.5781
 768/4849 [===>..........................] - ETA: 4:49 - loss: 0.6775 - acc: 0.5820
 832/4849 [====>.........................] - ETA: 4:44 - loss: 0.6770 - acc: 0.5793
 896/4849 [====>.........................] - ETA: 4:41 - loss: 0.6733 - acc: 0.5826
 960/4849 [====>.........................] - ETA: 4:37 - loss: 0.6700 - acc: 0.5958
1024/4849 [=====>........................] - ETA: 4:31 - loss: 0.6697 - acc: 0.5957
1088/4849 [=====>........................] - ETA: 4:27 - loss: 0.6686 - acc: 0.5965
1152/4849 [======>.......................] - ETA: 4:21 - loss: 0.6687 - acc: 0.5955
1216/4849 [======>.......................] - ETA: 4:17 - loss: 0.6694 - acc: 0.5962
1280/4849 [======>.......................] - ETA: 4:14 - loss: 0.6701 - acc: 0.5969
1344/4849 [=======>......................] - ETA: 4:10 - loss: 0.6681 - acc: 0.6004
1408/4849 [=======>......................] - ETA: 4:04 - loss: 0.6667 - acc: 0.6037
1472/4849 [========>.....................] - ETA: 3:59 - loss: 0.6663 - acc: 0.6012
1536/4849 [========>.....................] - ETA: 3:55 - loss: 0.6680 - acc: 0.6003
1600/4849 [========>.....................] - ETA: 3:50 - loss: 0.6708 - acc: 0.5981
1664/4849 [=========>....................] - ETA: 3:46 - loss: 0.6705 - acc: 0.5974
1728/4849 [=========>....................] - ETA: 3:43 - loss: 0.6698 - acc: 0.5961
1792/4849 [==========>...................] - ETA: 3:37 - loss: 0.6700 - acc: 0.5960
1856/4849 [==========>...................] - ETA: 3:33 - loss: 0.6723 - acc: 0.5927
1920/4849 [==========>...................] - ETA: 3:28 - loss: 0.6711 - acc: 0.5964
1984/4849 [===========>..................] - ETA: 3:24 - loss: 0.6695 - acc: 0.5968
2048/4849 [===========>..................] - ETA: 3:20 - loss: 0.6699 - acc: 0.5962
2112/4849 [============>.................] - ETA: 3:15 - loss: 0.6689 - acc: 0.5985
2176/4849 [============>.................] - ETA: 3:11 - loss: 0.6688 - acc: 0.5979
2240/4849 [============>.................] - ETA: 3:06 - loss: 0.6698 - acc: 0.5960
2304/4849 [=============>................] - ETA: 3:01 - loss: 0.6698 - acc: 0.5964
2368/4849 [=============>................] - ETA: 2:56 - loss: 0.6702 - acc: 0.5963
2432/4849 [==============>...............] - ETA: 2:52 - loss: 0.6694 - acc: 0.5966
2496/4849 [==============>...............] - ETA: 2:47 - loss: 0.6692 - acc: 0.5966
2560/4849 [==============>...............] - ETA: 2:43 - loss: 0.6697 - acc: 0.5941
2624/4849 [===============>..............] - ETA: 2:38 - loss: 0.6698 - acc: 0.5930
2688/4849 [===============>..............] - ETA: 2:34 - loss: 0.6712 - acc: 0.5897
2752/4849 [================>.............] - ETA: 2:29 - loss: 0.6717 - acc: 0.5883
2816/4849 [================>.............] - ETA: 2:25 - loss: 0.6717 - acc: 0.5866
2880/4849 [================>.............] - ETA: 2:20 - loss: 0.6732 - acc: 0.5840
2944/4849 [=================>............] - ETA: 2:16 - loss: 0.6744 - acc: 0.5812
3008/4849 [=================>............] - ETA: 2:11 - loss: 0.6741 - acc: 0.5824
3072/4849 [==================>...........] - ETA: 2:06 - loss: 0.6749 - acc: 0.5804
3136/4849 [==================>...........] - ETA: 2:02 - loss: 0.6745 - acc: 0.5813
3200/4849 [==================>...........] - ETA: 1:57 - loss: 0.6746 - acc: 0.5816
3264/4849 [===================>..........] - ETA: 1:53 - loss: 0.6751 - acc: 0.5809
3328/4849 [===================>..........] - ETA: 1:48 - loss: 0.6743 - acc: 0.5829
3392/4849 [===================>..........] - ETA: 1:43 - loss: 0.6741 - acc: 0.5831
3456/4849 [====================>.........] - ETA: 1:39 - loss: 0.6751 - acc: 0.5816
3520/4849 [====================>.........] - ETA: 1:34 - loss: 0.6755 - acc: 0.5798
3584/4849 [=====================>........] - ETA: 1:29 - loss: 0.6751 - acc: 0.5809
3648/4849 [=====================>........] - ETA: 1:25 - loss: 0.6749 - acc: 0.5817
3712/4849 [=====================>........] - ETA: 1:20 - loss: 0.6746 - acc: 0.5816
3776/4849 [======================>.......] - ETA: 1:16 - loss: 0.6748 - acc: 0.5818
3840/4849 [======================>.......] - ETA: 1:11 - loss: 0.6740 - acc: 0.5818
3904/4849 [=======================>......] - ETA: 1:06 - loss: 0.6743 - acc: 0.5812
3968/4849 [=======================>......] - ETA: 1:02 - loss: 0.6746 - acc: 0.5814
4032/4849 [=======================>......] - ETA: 57s - loss: 0.6747 - acc: 0.5809 
4096/4849 [========================>.....] - ETA: 53s - loss: 0.6746 - acc: 0.5811
4160/4849 [========================>.....] - ETA: 48s - loss: 0.6745 - acc: 0.5815
4224/4849 [=========================>....] - ETA: 44s - loss: 0.6745 - acc: 0.5814
4288/4849 [=========================>....] - ETA: 39s - loss: 0.6747 - acc: 0.5816
4352/4849 [=========================>....] - ETA: 35s - loss: 0.6751 - acc: 0.5804
4416/4849 [==========================>...] - ETA: 30s - loss: 0.6747 - acc: 0.5808
4480/4849 [==========================>...] - ETA: 26s - loss: 0.6747 - acc: 0.5815
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6753 - acc: 0.5805
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6759 - acc: 0.5786
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6763 - acc: 0.5773
4736/4849 [============================>.] - ETA: 8s - loss: 0.6764 - acc: 0.5764 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6765 - acc: 0.5769
4849/4849 [==============================] - 357s 74ms/step - loss: 0.6766 - acc: 0.5766 - val_loss: 0.7111 - val_acc: 0.5250

Epoch 00008: val_acc did not improve from 0.55473
Epoch 9/10

  64/4849 [..............................] - ETA: 5:24 - loss: 0.6829 - acc: 0.5625
 128/4849 [..............................] - ETA: 5:47 - loss: 0.6888 - acc: 0.5391
 192/4849 [>.............................] - ETA: 5:37 - loss: 0.7024 - acc: 0.5156
 256/4849 [>.............................] - ETA: 5:31 - loss: 0.6995 - acc: 0.5273
 320/4849 [>.............................] - ETA: 5:27 - loss: 0.6935 - acc: 0.5500
 384/4849 [=>............................] - ETA: 5:18 - loss: 0.6911 - acc: 0.5469
 448/4849 [=>............................] - ETA: 5:14 - loss: 0.6878 - acc: 0.5536
 512/4849 [==>...........................] - ETA: 5:15 - loss: 0.6873 - acc: 0.5547
 576/4849 [==>...........................] - ETA: 5:08 - loss: 0.6841 - acc: 0.5642
 640/4849 [==>...........................] - ETA: 5:03 - loss: 0.6837 - acc: 0.5609
 704/4849 [===>..........................] - ETA: 4:57 - loss: 0.6813 - acc: 0.5653
 768/4849 [===>..........................] - ETA: 4:52 - loss: 0.6788 - acc: 0.5716
 832/4849 [====>.........................] - ETA: 4:47 - loss: 0.6807 - acc: 0.5673
 896/4849 [====>.........................] - ETA: 4:43 - loss: 0.6815 - acc: 0.5614
 960/4849 [====>.........................] - ETA: 4:39 - loss: 0.6819 - acc: 0.5583
1024/4849 [=====>........................] - ETA: 4:33 - loss: 0.6801 - acc: 0.5625
1088/4849 [=====>........................] - ETA: 4:30 - loss: 0.6809 - acc: 0.5634
1152/4849 [======>.......................] - ETA: 4:24 - loss: 0.6809 - acc: 0.5599
1216/4849 [======>.......................] - ETA: 4:20 - loss: 0.6803 - acc: 0.5650
1280/4849 [======>.......................] - ETA: 4:17 - loss: 0.6787 - acc: 0.5703
1344/4849 [=======>......................] - ETA: 4:12 - loss: 0.6788 - acc: 0.5647
1408/4849 [=======>......................] - ETA: 4:07 - loss: 0.6777 - acc: 0.5682
1472/4849 [========>.....................] - ETA: 4:01 - loss: 0.6800 - acc: 0.5666
1536/4849 [========>.....................] - ETA: 3:56 - loss: 0.6804 - acc: 0.5664
1600/4849 [========>.....................] - ETA: 3:50 - loss: 0.6812 - acc: 0.5650
1664/4849 [=========>....................] - ETA: 3:45 - loss: 0.6814 - acc: 0.5655
1728/4849 [=========>....................] - ETA: 3:40 - loss: 0.6818 - acc: 0.5642
1792/4849 [==========>...................] - ETA: 3:36 - loss: 0.6806 - acc: 0.5658
1856/4849 [==========>...................] - ETA: 3:31 - loss: 0.6800 - acc: 0.5690
1920/4849 [==========>...................] - ETA: 3:26 - loss: 0.6799 - acc: 0.5677
1984/4849 [===========>..................] - ETA: 3:21 - loss: 0.6793 - acc: 0.5696
2048/4849 [===========>..................] - ETA: 3:16 - loss: 0.6790 - acc: 0.5679
2112/4849 [============>.................] - ETA: 3:12 - loss: 0.6787 - acc: 0.5687
2176/4849 [============>.................] - ETA: 3:07 - loss: 0.6787 - acc: 0.5694
2240/4849 [============>.................] - ETA: 3:03 - loss: 0.6785 - acc: 0.5692
2304/4849 [=============>................] - ETA: 2:58 - loss: 0.6781 - acc: 0.5694
2368/4849 [=============>................] - ETA: 2:53 - loss: 0.6790 - acc: 0.5655
2432/4849 [==============>...............] - ETA: 2:48 - loss: 0.6801 - acc: 0.5637
2496/4849 [==============>...............] - ETA: 2:43 - loss: 0.6804 - acc: 0.5625
2560/4849 [==============>...............] - ETA: 2:38 - loss: 0.6806 - acc: 0.5617
2624/4849 [===============>..............] - ETA: 2:34 - loss: 0.6808 - acc: 0.5617
2688/4849 [===============>..............] - ETA: 2:30 - loss: 0.6799 - acc: 0.5632
2752/4849 [================>.............] - ETA: 2:25 - loss: 0.6806 - acc: 0.5621
2816/4849 [================>.............] - ETA: 2:21 - loss: 0.6801 - acc: 0.5629
2880/4849 [================>.............] - ETA: 2:16 - loss: 0.6804 - acc: 0.5635
2944/4849 [=================>............] - ETA: 2:11 - loss: 0.6804 - acc: 0.5628
3008/4849 [=================>............] - ETA: 2:07 - loss: 0.6797 - acc: 0.5632
3072/4849 [==================>...........] - ETA: 2:02 - loss: 0.6800 - acc: 0.5612
3136/4849 [==================>...........] - ETA: 1:58 - loss: 0.6803 - acc: 0.5606
3200/4849 [==================>...........] - ETA: 1:54 - loss: 0.6790 - acc: 0.5641
3264/4849 [===================>..........] - ETA: 1:49 - loss: 0.6781 - acc: 0.5650
3328/4849 [===================>..........] - ETA: 1:45 - loss: 0.6781 - acc: 0.5661
3392/4849 [===================>..........] - ETA: 1:40 - loss: 0.6778 - acc: 0.5654
3456/4849 [====================>.........] - ETA: 1:35 - loss: 0.6785 - acc: 0.5634
3520/4849 [====================>.........] - ETA: 1:31 - loss: 0.6782 - acc: 0.5651
3584/4849 [=====================>........] - ETA: 1:26 - loss: 0.6787 - acc: 0.5636
3648/4849 [=====================>........] - ETA: 1:22 - loss: 0.6792 - acc: 0.5633
3712/4849 [=====================>........] - ETA: 1:17 - loss: 0.6795 - acc: 0.5630
3776/4849 [======================>.......] - ETA: 1:13 - loss: 0.6790 - acc: 0.5636
3840/4849 [======================>.......] - ETA: 1:08 - loss: 0.6794 - acc: 0.5625
3904/4849 [=======================>......] - ETA: 1:04 - loss: 0.6792 - acc: 0.5625
3968/4849 [=======================>......] - ETA: 59s - loss: 0.6786 - acc: 0.5640 
4032/4849 [=======================>......] - ETA: 55s - loss: 0.6783 - acc: 0.5652
4096/4849 [========================>.....] - ETA: 51s - loss: 0.6778 - acc: 0.5659
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6777 - acc: 0.5661
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6775 - acc: 0.5663
4288/4849 [=========================>....] - ETA: 37s - loss: 0.6773 - acc: 0.5667
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6776 - acc: 0.5659
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6780 - acc: 0.5654
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6783 - acc: 0.5645
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6785 - acc: 0.5640
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6785 - acc: 0.5642
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6782 - acc: 0.5651
4736/4849 [============================>.] - ETA: 7s - loss: 0.6786 - acc: 0.5646 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6785 - acc: 0.5648
4849/4849 [==============================] - 351s 72ms/step - loss: 0.6789 - acc: 0.5651 - val_loss: 0.6905 - val_acc: 0.5529

Epoch 00009: val_acc did not improve from 0.55473
Epoch 10/10

  64/4849 [..............................] - ETA: 6:43 - loss: 0.6291 - acc: 0.7656
 128/4849 [..............................] - ETA: 6:39 - loss: 0.6382 - acc: 0.7188
 192/4849 [>.............................] - ETA: 6:32 - loss: 0.6444 - acc: 0.6823
 256/4849 [>.............................] - ETA: 6:31 - loss: 0.6512 - acc: 0.6562
 320/4849 [>.............................] - ETA: 6:19 - loss: 0.6561 - acc: 0.6406
 384/4849 [=>............................] - ETA: 6:11 - loss: 0.6633 - acc: 0.6042
 448/4849 [=>............................] - ETA: 6:03 - loss: 0.6647 - acc: 0.5960
 512/4849 [==>...........................] - ETA: 5:57 - loss: 0.6646 - acc: 0.5938
 576/4849 [==>...........................] - ETA: 5:54 - loss: 0.6690 - acc: 0.5903
 640/4849 [==>...........................] - ETA: 5:49 - loss: 0.6710 - acc: 0.5844
 704/4849 [===>..........................] - ETA: 5:45 - loss: 0.6662 - acc: 0.5966
 768/4849 [===>..........................] - ETA: 5:40 - loss: 0.6740 - acc: 0.5859
 832/4849 [====>.........................] - ETA: 5:36 - loss: 0.6747 - acc: 0.5817
 896/4849 [====>.........................] - ETA: 5:29 - loss: 0.6731 - acc: 0.5859
 960/4849 [====>.........................] - ETA: 5:24 - loss: 0.6782 - acc: 0.5740
1024/4849 [=====>........................] - ETA: 5:20 - loss: 0.6770 - acc: 0.5771
1088/4849 [=====>........................] - ETA: 5:13 - loss: 0.6769 - acc: 0.5781
1152/4849 [======>.......................] - ETA: 5:07 - loss: 0.6779 - acc: 0.5781
1216/4849 [======>.......................] - ETA: 5:00 - loss: 0.6767 - acc: 0.5781
1280/4849 [======>.......................] - ETA: 4:55 - loss: 0.6778 - acc: 0.5742
1344/4849 [=======>......................] - ETA: 4:50 - loss: 0.6786 - acc: 0.5707
1408/4849 [=======>......................] - ETA: 4:44 - loss: 0.6772 - acc: 0.5724
1472/4849 [========>.....................] - ETA: 4:40 - loss: 0.6768 - acc: 0.5747
1536/4849 [========>.....................] - ETA: 4:34 - loss: 0.6760 - acc: 0.5762
1600/4849 [========>.....................] - ETA: 4:29 - loss: 0.6773 - acc: 0.5763
1664/4849 [=========>....................] - ETA: 4:25 - loss: 0.6776 - acc: 0.5781
1728/4849 [=========>....................] - ETA: 4:20 - loss: 0.6775 - acc: 0.5787
1792/4849 [==========>...................] - ETA: 4:14 - loss: 0.6763 - acc: 0.5809
1856/4849 [==========>...................] - ETA: 4:09 - loss: 0.6776 - acc: 0.5781
1920/4849 [==========>...................] - ETA: 4:04 - loss: 0.6779 - acc: 0.5792
1984/4849 [===========>..................] - ETA: 3:59 - loss: 0.6763 - acc: 0.5806
2048/4849 [===========>..................] - ETA: 3:53 - loss: 0.6758 - acc: 0.5820
2112/4849 [============>.................] - ETA: 3:48 - loss: 0.6766 - acc: 0.5791
2176/4849 [============>.................] - ETA: 3:42 - loss: 0.6755 - acc: 0.5813
2240/4849 [============>.................] - ETA: 3:38 - loss: 0.6754 - acc: 0.5813
2304/4849 [=============>................] - ETA: 3:33 - loss: 0.6762 - acc: 0.5786
2368/4849 [=============>................] - ETA: 3:27 - loss: 0.6754 - acc: 0.5815
2432/4849 [==============>...............] - ETA: 3:21 - loss: 0.6753 - acc: 0.5814
2496/4849 [==============>...............] - ETA: 3:16 - loss: 0.6762 - acc: 0.5793
2560/4849 [==============>...............] - ETA: 3:11 - loss: 0.6760 - acc: 0.5789
2624/4849 [===============>..............] - ETA: 3:05 - loss: 0.6775 - acc: 0.5770
2688/4849 [===============>..............] - ETA: 2:59 - loss: 0.6768 - acc: 0.5781
2752/4849 [================>.............] - ETA: 2:53 - loss: 0.6761 - acc: 0.5792
2816/4849 [================>.............] - ETA: 2:47 - loss: 0.6766 - acc: 0.5781
2880/4849 [================>.............] - ETA: 2:41 - loss: 0.6773 - acc: 0.5771
2944/4849 [=================>............] - ETA: 2:36 - loss: 0.6780 - acc: 0.5764
3008/4849 [=================>............] - ETA: 2:30 - loss: 0.6782 - acc: 0.5758
3072/4849 [==================>...........] - ETA: 2:25 - loss: 0.6780 - acc: 0.5752
3136/4849 [==================>...........] - ETA: 2:19 - loss: 0.6781 - acc: 0.5743
3200/4849 [==================>...........] - ETA: 2:14 - loss: 0.6785 - acc: 0.5734
3264/4849 [===================>..........] - ETA: 2:09 - loss: 0.6794 - acc: 0.5711
3328/4849 [===================>..........] - ETA: 2:03 - loss: 0.6793 - acc: 0.5712
3392/4849 [===================>..........] - ETA: 1:57 - loss: 0.6789 - acc: 0.5728
3456/4849 [====================>.........] - ETA: 1:52 - loss: 0.6796 - acc: 0.5715
3520/4849 [====================>.........] - ETA: 1:47 - loss: 0.6790 - acc: 0.5730
3584/4849 [=====================>........] - ETA: 1:41 - loss: 0.6787 - acc: 0.5742
3648/4849 [=====================>........] - ETA: 1:36 - loss: 0.6780 - acc: 0.5768
3712/4849 [=====================>........] - ETA: 1:31 - loss: 0.6778 - acc: 0.5770
3776/4849 [======================>.......] - ETA: 1:25 - loss: 0.6779 - acc: 0.5760
3840/4849 [======================>.......] - ETA: 1:20 - loss: 0.6779 - acc: 0.5760
3904/4849 [=======================>......] - ETA: 1:15 - loss: 0.6779 - acc: 0.5766
3968/4849 [=======================>......] - ETA: 1:10 - loss: 0.6777 - acc: 0.5779
4032/4849 [=======================>......] - ETA: 1:04 - loss: 0.6777 - acc: 0.5789
4096/4849 [========================>.....] - ETA: 59s - loss: 0.6777 - acc: 0.5784 
4160/4849 [========================>.....] - ETA: 54s - loss: 0.6779 - acc: 0.5779
4224/4849 [=========================>....] - ETA: 49s - loss: 0.6786 - acc: 0.5762
4288/4849 [=========================>....] - ETA: 44s - loss: 0.6785 - acc: 0.5760
4352/4849 [=========================>....] - ETA: 39s - loss: 0.6788 - acc: 0.5770
4416/4849 [==========================>...] - ETA: 34s - loss: 0.6781 - acc: 0.5788
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6780 - acc: 0.5790
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6777 - acc: 0.5797
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6770 - acc: 0.5809
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6775 - acc: 0.5794
4736/4849 [============================>.] - ETA: 8s - loss: 0.6773 - acc: 0.5790 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6776 - acc: 0.5787
4849/4849 [==============================] - 391s 81ms/step - loss: 0.6771 - acc: 0.5795 - val_loss: 0.7170 - val_acc: 0.4787

Epoch 00010: val_acc did not improve from 0.55473
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7febf6f36ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7febf6f36ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7febf6ed2c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7febf6ed2c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a8446b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7a8446b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febc4ad4850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febc4ad4850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3a45ddf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3a45ddf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febc4ad3990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febc4ad3990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febc4ad4950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febc4ad4950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febc480de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febc480de90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febc4a7f690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febc4a7f690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febc4a6c490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febc4a6c490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febc48e60d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febc48e60d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febc4a7f210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febc4a7f210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febf7617910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febf7617910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febc4951f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febc4951f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febc4ab5850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febc4ab5850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febc48d4650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febc48d4650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febf7b7f090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febf7b7f090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbc434b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbc434b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febbc30ec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febbc30ec90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febbc2e3fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febbc2e3fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbc38ce50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbc38ce50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febc45e2590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febc45e2590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbc26c750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbc26c750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febbc010650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febbc010650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febbbfa4090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febbbfa4090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe79065cb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe79065cb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febbc010910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febbc010910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbbfbf410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbbfbf410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febbbf196d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febbbf196d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feb7fcd2310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7feb7fcd2310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbc2262d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbc2262d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feb7fd70450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7feb7fd70450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe79031ca50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe79031ca50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febbbf167d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febbbf167d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe79027e0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe79027e0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe790271e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe790271e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe790366b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe790366b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe790221410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe790221410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe780706e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe780706e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe790201ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe790201ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe79014f310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe79014f310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7900a0e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7900a0e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7903b14d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7903b14d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe78041eb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe78041eb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe78030b110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe78030b110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7804ce510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7804ce510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7804d6550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7804d6550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7801d4d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7801d4d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7802d4810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7802d4810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7800dfc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7800dfc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7800e7210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7800e7210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7802d4990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7802d4990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe780602e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe780602e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7607b23d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7607b23d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe76045ae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe76045ae50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbc38c910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febbc38c910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe76059e2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe76059e2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe76045a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe76045a710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7605bfa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7605bfa10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe76023fa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe76023fa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7604b7c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7604b7c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7803276d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7803276d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe760141990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe760141990>>: AttributeError: module 'gast' has no attribute 'Str'
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 2:19
 128/1348 [=>............................] - ETA: 1:22
 192/1348 [===>..........................] - ETA: 1:03
 256/1348 [====>.........................] - ETA: 51s 
 320/1348 [======>.......................] - ETA: 44s
 384/1348 [=======>......................] - ETA: 39s
 448/1348 [========>.....................] - ETA: 35s
 512/1348 [==========>...................] - ETA: 32s
 576/1348 [===========>..................] - ETA: 29s
 640/1348 [=============>................] - ETA: 26s
 704/1348 [==============>...............] - ETA: 23s
 768/1348 [================>.............] - ETA: 20s
 832/1348 [=================>............] - ETA: 18s
 896/1348 [==================>...........] - ETA: 15s
 960/1348 [====================>.........] - ETA: 13s
1024/1348 [=====================>........] - ETA: 11s
1088/1348 [=======================>......] - ETA: 8s 
1152/1348 [========================>.....] - ETA: 6s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 45s 33ms/step
loss: 0.6877704986125143
acc: 0.5408011869436202
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe3f8614e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe3f8614e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe7a8492710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe7a8492710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec2228a0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fec2228a0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6c825c9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6c825c9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febf6c9b610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febf6c9b610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febf6cbf1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febf6cbf1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6c825cdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6c825cdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6881c3390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6881c3390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febf6dc3450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7febf6dc3450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febf6da7e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7febf6da7e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febc4b481d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7febc4b481d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febf6dc36d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7febf6dc36d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f84f1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f84f1c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3f8292210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3f8292210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3f8289390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3f8289390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f84ba0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f84ba0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3f832a650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3f832a650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f81a4290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f81a4290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3f0770390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3f0770390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3f0768890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3f0768890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f0604150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f0604150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3f0770290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3f0770290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f060ddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f060ddd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3f0468690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3f0468690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3f0658710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3f0658710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f0571390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f0571390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3f04ace90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3f04ace90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f0376450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f0376450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3f0105110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3f0105110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3dc7c9650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3dc7c9650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f036bbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f036bbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3f04a7410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3f04a7410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f0210290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f0210290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3dc6f2750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3dc6f2750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3dc4ee450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3dc4ee450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3dc597210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3dc597210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3dc785450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3dc785450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3dc574990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3dc574990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3dc290b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3dc290b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3dc283fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3dc283fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3dc2c8810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3dc2c8810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3dc586750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3dc586750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3dc18fdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3dc18fdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3a46fa590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3a46fa590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3a474d190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3a474d190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3a479fd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3a479fd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3a467add0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3a467add0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3dc4a1990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3dc4a1990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3a461d490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3a461d490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2c45b0ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2c45b0ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2c464db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2c464db90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3a4660550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3a4660550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f8299410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3f8299410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe2c4378790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe2c4378790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2c4123d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2c4123d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2c411a2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2c411a2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe2c4443a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe2c4443a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2c416d890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2c416d890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe2c44cfb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe2c44cfb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2a46b14d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2a46b14d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2c413add0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2c413add0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe2c4439890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe2c4439890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2a449b110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2a449b110>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 24:52 - loss: 0.6836 - acc: 0.5938
 128/4849 [..............................] - ETA: 15:04 - loss: 0.7517 - acc: 0.5234
 192/4849 [>.............................] - ETA: 11:46 - loss: 0.7496 - acc: 0.5104
 256/4849 [>.............................] - ETA: 10:08 - loss: 0.7474 - acc: 0.5234
 320/4849 [>.............................] - ETA: 9:06 - loss: 0.7367 - acc: 0.5344 
 384/4849 [=>............................] - ETA: 8:32 - loss: 0.7470 - acc: 0.5182
 448/4849 [=>............................] - ETA: 8:00 - loss: 0.7444 - acc: 0.5112
 512/4849 [==>...........................] - ETA: 7:30 - loss: 0.7368 - acc: 0.5234
 576/4849 [==>...........................] - ETA: 7:11 - loss: 0.7370 - acc: 0.5208
 640/4849 [==>...........................] - ETA: 6:50 - loss: 0.7394 - acc: 0.5078
 704/4849 [===>..........................] - ETA: 6:40 - loss: 0.7387 - acc: 0.5043
 768/4849 [===>..........................] - ETA: 6:26 - loss: 0.7349 - acc: 0.5104
 832/4849 [====>.........................] - ETA: 6:14 - loss: 0.7342 - acc: 0.5120
 896/4849 [====>.........................] - ETA: 5:59 - loss: 0.7338 - acc: 0.5078
 960/4849 [====>.........................] - ETA: 5:50 - loss: 0.7310 - acc: 0.5125
1024/4849 [=====>........................] - ETA: 5:38 - loss: 0.7314 - acc: 0.5098
1088/4849 [=====>........................] - ETA: 5:28 - loss: 0.7303 - acc: 0.5101
1152/4849 [======>.......................] - ETA: 5:19 - loss: 0.7307 - acc: 0.5095
1216/4849 [======>.......................] - ETA: 5:13 - loss: 0.7294 - acc: 0.5082
1280/4849 [======>.......................] - ETA: 5:04 - loss: 0.7279 - acc: 0.5070
1344/4849 [=======>......................] - ETA: 4:57 - loss: 0.7286 - acc: 0.5060
1408/4849 [=======>......................] - ETA: 4:49 - loss: 0.7263 - acc: 0.5071
1472/4849 [========>.....................] - ETA: 4:43 - loss: 0.7267 - acc: 0.5075
1536/4849 [========>.....................] - ETA: 4:37 - loss: 0.7260 - acc: 0.5072
1600/4849 [========>.....................] - ETA: 4:31 - loss: 0.7277 - acc: 0.5075
1664/4849 [=========>....................] - ETA: 4:24 - loss: 0.7265 - acc: 0.5096
1728/4849 [=========>....................] - ETA: 4:18 - loss: 0.7260 - acc: 0.5098
1792/4849 [==========>...................] - ETA: 4:12 - loss: 0.7243 - acc: 0.5128
1856/4849 [==========>...................] - ETA: 4:06 - loss: 0.7240 - acc: 0.5135
1920/4849 [==========>...................] - ETA: 4:00 - loss: 0.7211 - acc: 0.5161
1984/4849 [===========>..................] - ETA: 3:54 - loss: 0.7209 - acc: 0.5151
2048/4849 [===========>..................] - ETA: 3:48 - loss: 0.7207 - acc: 0.5132
2112/4849 [============>.................] - ETA: 3:43 - loss: 0.7187 - acc: 0.5161
2176/4849 [============>.................] - ETA: 3:38 - loss: 0.7199 - acc: 0.5152
2240/4849 [============>.................] - ETA: 3:33 - loss: 0.7186 - acc: 0.5174
2304/4849 [=============>................] - ETA: 3:26 - loss: 0.7178 - acc: 0.5200
2368/4849 [=============>................] - ETA: 3:21 - loss: 0.7184 - acc: 0.5177
2432/4849 [==============>...............] - ETA: 3:16 - loss: 0.7176 - acc: 0.5189
2496/4849 [==============>...............] - ETA: 3:10 - loss: 0.7172 - acc: 0.5192
2560/4849 [==============>...............] - ETA: 3:05 - loss: 0.7181 - acc: 0.5180
2624/4849 [===============>..............] - ETA: 2:59 - loss: 0.7170 - acc: 0.5210
2688/4849 [===============>..............] - ETA: 2:54 - loss: 0.7167 - acc: 0.5197
2752/4849 [================>.............] - ETA: 2:48 - loss: 0.7160 - acc: 0.5189
2816/4849 [================>.............] - ETA: 2:43 - loss: 0.7158 - acc: 0.5188
2880/4849 [================>.............] - ETA: 2:37 - loss: 0.7152 - acc: 0.5205
2944/4849 [=================>............] - ETA: 2:32 - loss: 0.7148 - acc: 0.5207
3008/4849 [=================>............] - ETA: 2:27 - loss: 0.7147 - acc: 0.5213
3072/4849 [==================>...........] - ETA: 2:22 - loss: 0.7146 - acc: 0.5212
3136/4849 [==================>...........] - ETA: 2:16 - loss: 0.7147 - acc: 0.5226
3200/4849 [==================>...........] - ETA: 2:11 - loss: 0.7150 - acc: 0.5219
3264/4849 [===================>..........] - ETA: 2:06 - loss: 0.7145 - acc: 0.5221
3328/4849 [===================>..........] - ETA: 2:00 - loss: 0.7135 - acc: 0.5228
3392/4849 [===================>..........] - ETA: 1:55 - loss: 0.7133 - acc: 0.5230
3456/4849 [====================>.........] - ETA: 1:50 - loss: 0.7132 - acc: 0.5237
3520/4849 [====================>.........] - ETA: 1:45 - loss: 0.7134 - acc: 0.5236
3584/4849 [=====================>........] - ETA: 1:40 - loss: 0.7138 - acc: 0.5220
3648/4849 [=====================>........] - ETA: 1:35 - loss: 0.7129 - acc: 0.5233
3712/4849 [=====================>........] - ETA: 1:29 - loss: 0.7135 - acc: 0.5218
3776/4849 [======================>.......] - ETA: 1:24 - loss: 0.7135 - acc: 0.5212
3840/4849 [======================>.......] - ETA: 1:19 - loss: 0.7129 - acc: 0.5221
3904/4849 [=======================>......] - ETA: 1:14 - loss: 0.7132 - acc: 0.5210
3968/4849 [=======================>......] - ETA: 1:09 - loss: 0.7120 - acc: 0.5242
4032/4849 [=======================>......] - ETA: 1:04 - loss: 0.7121 - acc: 0.5233
4096/4849 [========================>.....] - ETA: 59s - loss: 0.7118 - acc: 0.5237 
4160/4849 [========================>.....] - ETA: 54s - loss: 0.7117 - acc: 0.5228
4224/4849 [=========================>....] - ETA: 49s - loss: 0.7112 - acc: 0.5232
4288/4849 [=========================>....] - ETA: 44s - loss: 0.7103 - acc: 0.5245
4352/4849 [=========================>....] - ETA: 39s - loss: 0.7099 - acc: 0.5257
4416/4849 [==========================>...] - ETA: 34s - loss: 0.7100 - acc: 0.5256
4480/4849 [==========================>...] - ETA: 28s - loss: 0.7093 - acc: 0.5261
4544/4849 [===========================>..] - ETA: 23s - loss: 0.7094 - acc: 0.5249
4608/4849 [===========================>..] - ETA: 18s - loss: 0.7097 - acc: 0.5239
4672/4849 [===========================>..] - ETA: 13s - loss: 0.7095 - acc: 0.5240
4736/4849 [============================>.] - ETA: 8s - loss: 0.7091 - acc: 0.5249 
4800/4849 [============================>.] - ETA: 3s - loss: 0.7092 - acc: 0.5244
4849/4849 [==============================] - 399s 82ms/step - loss: 0.7095 - acc: 0.5236 - val_loss: 0.6845 - val_acc: 0.5677

Epoch 00001: val_acc improved from -inf to 0.56772, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window08/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 5:41 - loss: 0.6867 - acc: 0.5312
 128/4849 [..............................] - ETA: 5:52 - loss: 0.6845 - acc: 0.5547
 192/4849 [>.............................] - ETA: 5:51 - loss: 0.6930 - acc: 0.5365
 256/4849 [>.............................] - ETA: 5:37 - loss: 0.6866 - acc: 0.5508
 320/4849 [>.............................] - ETA: 5:46 - loss: 0.6896 - acc: 0.5563
 384/4849 [=>............................] - ETA: 5:38 - loss: 0.6970 - acc: 0.5365
 448/4849 [=>............................] - ETA: 5:31 - loss: 0.6934 - acc: 0.5491
 512/4849 [==>...........................] - ETA: 5:31 - loss: 0.6945 - acc: 0.5410
 576/4849 [==>...........................] - ETA: 5:30 - loss: 0.6913 - acc: 0.5486
 640/4849 [==>...........................] - ETA: 5:23 - loss: 0.6939 - acc: 0.5391
 704/4849 [===>..........................] - ETA: 5:21 - loss: 0.6920 - acc: 0.5426
 768/4849 [===>..........................] - ETA: 5:16 - loss: 0.6926 - acc: 0.5404
 832/4849 [====>.........................] - ETA: 5:09 - loss: 0.6932 - acc: 0.5385
 896/4849 [====>.........................] - ETA: 5:06 - loss: 0.6975 - acc: 0.5324
 960/4849 [====>.........................] - ETA: 4:59 - loss: 0.6970 - acc: 0.5333
1024/4849 [=====>........................] - ETA: 4:50 - loss: 0.6980 - acc: 0.5312
1088/4849 [=====>........................] - ETA: 4:45 - loss: 0.6981 - acc: 0.5322
1152/4849 [======>.......................] - ETA: 4:40 - loss: 0.6973 - acc: 0.5373
1216/4849 [======>.......................] - ETA: 4:33 - loss: 0.6981 - acc: 0.5378
1280/4849 [======>.......................] - ETA: 4:26 - loss: 0.6992 - acc: 0.5336
1344/4849 [=======>......................] - ETA: 4:20 - loss: 0.6995 - acc: 0.5312
1408/4849 [=======>......................] - ETA: 4:14 - loss: 0.6997 - acc: 0.5305
1472/4849 [========>.....................] - ETA: 4:08 - loss: 0.6990 - acc: 0.5306
1536/4849 [========>.....................] - ETA: 4:03 - loss: 0.6983 - acc: 0.5332
1600/4849 [========>.....................] - ETA: 3:59 - loss: 0.6961 - acc: 0.5363
1664/4849 [=========>....................] - ETA: 3:54 - loss: 0.6961 - acc: 0.5355
1728/4849 [=========>....................] - ETA: 3:48 - loss: 0.6967 - acc: 0.5336
1792/4849 [==========>...................] - ETA: 3:43 - loss: 0.6963 - acc: 0.5346
1856/4849 [==========>...................] - ETA: 3:38 - loss: 0.6958 - acc: 0.5361
1920/4849 [==========>...................] - ETA: 3:33 - loss: 0.6945 - acc: 0.5385
1984/4849 [===========>..................] - ETA: 3:29 - loss: 0.6949 - acc: 0.5353
2048/4849 [===========>..................] - ETA: 3:24 - loss: 0.6959 - acc: 0.5347
2112/4849 [============>.................] - ETA: 3:19 - loss: 0.6945 - acc: 0.5365
2176/4849 [============>.................] - ETA: 3:14 - loss: 0.6941 - acc: 0.5372
2240/4849 [============>.................] - ETA: 3:09 - loss: 0.6966 - acc: 0.5344
2304/4849 [=============>................] - ETA: 3:04 - loss: 0.6947 - acc: 0.5386
2368/4849 [=============>................] - ETA: 2:59 - loss: 0.6960 - acc: 0.5367
2432/4849 [==============>...............] - ETA: 2:55 - loss: 0.6973 - acc: 0.5341
2496/4849 [==============>...............] - ETA: 2:50 - loss: 0.6975 - acc: 0.5333
2560/4849 [==============>...............] - ETA: 2:45 - loss: 0.6967 - acc: 0.5352
2624/4849 [===============>..............] - ETA: 2:40 - loss: 0.6974 - acc: 0.5332
2688/4849 [===============>..............] - ETA: 2:36 - loss: 0.6968 - acc: 0.5346
2752/4849 [================>.............] - ETA: 2:31 - loss: 0.6957 - acc: 0.5360
2816/4849 [================>.............] - ETA: 2:27 - loss: 0.6952 - acc: 0.5369
2880/4849 [================>.............] - ETA: 2:22 - loss: 0.6946 - acc: 0.5392
2944/4849 [=================>............] - ETA: 2:18 - loss: 0.6947 - acc: 0.5401
3008/4849 [=================>............] - ETA: 2:13 - loss: 0.6948 - acc: 0.5406
3072/4849 [==================>...........] - ETA: 2:08 - loss: 0.6935 - acc: 0.5443
3136/4849 [==================>...........] - ETA: 2:04 - loss: 0.6936 - acc: 0.5440
3200/4849 [==================>...........] - ETA: 1:59 - loss: 0.6940 - acc: 0.5428
3264/4849 [===================>..........] - ETA: 1:54 - loss: 0.6937 - acc: 0.5447
3328/4849 [===================>..........] - ETA: 1:50 - loss: 0.6945 - acc: 0.5433
3392/4849 [===================>..........] - ETA: 1:45 - loss: 0.6947 - acc: 0.5430
3456/4849 [====================>.........] - ETA: 1:40 - loss: 0.6945 - acc: 0.5446
3520/4849 [====================>.........] - ETA: 1:36 - loss: 0.6941 - acc: 0.5455
3584/4849 [=====================>........] - ETA: 1:31 - loss: 0.6935 - acc: 0.5463
3648/4849 [=====================>........] - ETA: 1:27 - loss: 0.6939 - acc: 0.5450
3712/4849 [=====================>........] - ETA: 1:22 - loss: 0.6939 - acc: 0.5439
3776/4849 [======================>.......] - ETA: 1:17 - loss: 0.6936 - acc: 0.5448
3840/4849 [======================>.......] - ETA: 1:13 - loss: 0.6938 - acc: 0.5451
3904/4849 [=======================>......] - ETA: 1:08 - loss: 0.6947 - acc: 0.5420
3968/4849 [=======================>......] - ETA: 1:03 - loss: 0.6942 - acc: 0.5431
4032/4849 [=======================>......] - ETA: 59s - loss: 0.6944 - acc: 0.5422 
4096/4849 [========================>.....] - ETA: 54s - loss: 0.6941 - acc: 0.5422
4160/4849 [========================>.....] - ETA: 50s - loss: 0.6943 - acc: 0.5411
4224/4849 [=========================>....] - ETA: 45s - loss: 0.6941 - acc: 0.5417
4288/4849 [=========================>....] - ETA: 40s - loss: 0.6944 - acc: 0.5406
4352/4849 [=========================>....] - ETA: 36s - loss: 0.6943 - acc: 0.5404
4416/4849 [==========================>...] - ETA: 31s - loss: 0.6946 - acc: 0.5394
4480/4849 [==========================>...] - ETA: 26s - loss: 0.6947 - acc: 0.5393
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6948 - acc: 0.5385
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6954 - acc: 0.5362
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6953 - acc: 0.5372
4736/4849 [============================>.] - ETA: 8s - loss: 0.6950 - acc: 0.5376 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6941 - acc: 0.5398
4849/4849 [==============================] - 372s 77ms/step - loss: 0.6946 - acc: 0.5385 - val_loss: 0.6912 - val_acc: 0.5325

Epoch 00002: val_acc did not improve from 0.56772
Epoch 3/10

  64/4849 [..............................] - ETA: 6:26 - loss: 0.6668 - acc: 0.6250
 128/4849 [..............................] - ETA: 6:39 - loss: 0.6712 - acc: 0.6484
 192/4849 [>.............................] - ETA: 6:42 - loss: 0.6917 - acc: 0.5781
 256/4849 [>.............................] - ETA: 6:32 - loss: 0.6980 - acc: 0.5625
 320/4849 [>.............................] - ETA: 6:24 - loss: 0.6990 - acc: 0.5594
 384/4849 [=>............................] - ETA: 6:19 - loss: 0.6958 - acc: 0.5651
 448/4849 [=>............................] - ETA: 6:11 - loss: 0.6953 - acc: 0.5580
 512/4849 [==>...........................] - ETA: 6:09 - loss: 0.6973 - acc: 0.5527
 576/4849 [==>...........................] - ETA: 6:04 - loss: 0.6941 - acc: 0.5556
 640/4849 [==>...........................] - ETA: 5:59 - loss: 0.6897 - acc: 0.5609
 704/4849 [===>..........................] - ETA: 5:52 - loss: 0.6869 - acc: 0.5582
 768/4849 [===>..........................] - ETA: 5:47 - loss: 0.6864 - acc: 0.5560
 832/4849 [====>.........................] - ETA: 5:42 - loss: 0.6865 - acc: 0.5589
 896/4849 [====>.........................] - ETA: 5:38 - loss: 0.6883 - acc: 0.5580
 960/4849 [====>.........................] - ETA: 5:32 - loss: 0.6897 - acc: 0.5521
1024/4849 [=====>........................] - ETA: 5:25 - loss: 0.6874 - acc: 0.5537
1088/4849 [=====>........................] - ETA: 5:21 - loss: 0.6861 - acc: 0.5570
1152/4849 [======>.......................] - ETA: 5:15 - loss: 0.6858 - acc: 0.5573
1216/4849 [======>.......................] - ETA: 5:10 - loss: 0.6896 - acc: 0.5485
1280/4849 [======>.......................] - ETA: 5:05 - loss: 0.6869 - acc: 0.5516
1344/4849 [=======>......................] - ETA: 5:00 - loss: 0.6892 - acc: 0.5461
1408/4849 [=======>......................] - ETA: 4:54 - loss: 0.6895 - acc: 0.5462
1472/4849 [========>.....................] - ETA: 4:49 - loss: 0.6886 - acc: 0.5482
1536/4849 [========>.....................] - ETA: 4:43 - loss: 0.6900 - acc: 0.5469
1600/4849 [========>.....................] - ETA: 4:37 - loss: 0.6918 - acc: 0.5444
1664/4849 [=========>....................] - ETA: 4:32 - loss: 0.6906 - acc: 0.5499
1728/4849 [=========>....................] - ETA: 4:27 - loss: 0.6896 - acc: 0.5515
1792/4849 [==========>...................] - ETA: 4:21 - loss: 0.6899 - acc: 0.5519
1856/4849 [==========>...................] - ETA: 4:16 - loss: 0.6896 - acc: 0.5517
1920/4849 [==========>...................] - ETA: 4:11 - loss: 0.6912 - acc: 0.5484
1984/4849 [===========>..................] - ETA: 4:06 - loss: 0.6907 - acc: 0.5494
2048/4849 [===========>..................] - ETA: 4:00 - loss: 0.6908 - acc: 0.5479
2112/4849 [============>.................] - ETA: 3:54 - loss: 0.6916 - acc: 0.5455
2176/4849 [============>.................] - ETA: 3:49 - loss: 0.6901 - acc: 0.5487
2240/4849 [============>.................] - ETA: 3:44 - loss: 0.6902 - acc: 0.5504
2304/4849 [=============>................] - ETA: 3:38 - loss: 0.6907 - acc: 0.5490
2368/4849 [=============>................] - ETA: 3:33 - loss: 0.6908 - acc: 0.5486
2432/4849 [==============>...............] - ETA: 3:27 - loss: 0.6913 - acc: 0.5485
2496/4849 [==============>...............] - ETA: 3:21 - loss: 0.6912 - acc: 0.5497
2560/4849 [==============>...............] - ETA: 3:16 - loss: 0.6906 - acc: 0.5523
2624/4849 [===============>..............] - ETA: 3:10 - loss: 0.6906 - acc: 0.5530
2688/4849 [===============>..............] - ETA: 3:05 - loss: 0.6912 - acc: 0.5510
2752/4849 [================>.............] - ETA: 3:00 - loss: 0.6913 - acc: 0.5505
2816/4849 [================>.............] - ETA: 2:54 - loss: 0.6904 - acc: 0.5522
2880/4849 [================>.............] - ETA: 2:49 - loss: 0.6897 - acc: 0.5531
2944/4849 [=================>............] - ETA: 2:43 - loss: 0.6889 - acc: 0.5547
3008/4849 [=================>............] - ETA: 2:38 - loss: 0.6885 - acc: 0.5562
3072/4849 [==================>...........] - ETA: 2:32 - loss: 0.6890 - acc: 0.5563
3136/4849 [==================>...........] - ETA: 2:27 - loss: 0.6890 - acc: 0.5571
3200/4849 [==================>...........] - ETA: 2:22 - loss: 0.6892 - acc: 0.5575
3264/4849 [===================>..........] - ETA: 2:16 - loss: 0.6894 - acc: 0.5573
3328/4849 [===================>..........] - ETA: 2:10 - loss: 0.6893 - acc: 0.5580
3392/4849 [===================>..........] - ETA: 2:05 - loss: 0.6893 - acc: 0.5575
3456/4849 [====================>.........] - ETA: 1:59 - loss: 0.6900 - acc: 0.5553
3520/4849 [====================>.........] - ETA: 1:54 - loss: 0.6900 - acc: 0.5557
3584/4849 [=====================>........] - ETA: 1:48 - loss: 0.6897 - acc: 0.5566
3648/4849 [=====================>........] - ETA: 1:43 - loss: 0.6891 - acc: 0.5581
3712/4849 [=====================>........] - ETA: 1:37 - loss: 0.6886 - acc: 0.5590
3776/4849 [======================>.......] - ETA: 1:32 - loss: 0.6884 - acc: 0.5585
3840/4849 [======================>.......] - ETA: 1:26 - loss: 0.6886 - acc: 0.5583
3904/4849 [=======================>......] - ETA: 1:21 - loss: 0.6890 - acc: 0.5569
3968/4849 [=======================>......] - ETA: 1:15 - loss: 0.6890 - acc: 0.5565
4032/4849 [=======================>......] - ETA: 1:10 - loss: 0.6890 - acc: 0.5558
4096/4849 [========================>.....] - ETA: 1:04 - loss: 0.6894 - acc: 0.5552
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6890 - acc: 0.5560 
4224/4849 [=========================>....] - ETA: 53s - loss: 0.6895 - acc: 0.5563
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6890 - acc: 0.5571
4352/4849 [=========================>....] - ETA: 42s - loss: 0.6889 - acc: 0.5574
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6889 - acc: 0.5573
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6889 - acc: 0.5578
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6896 - acc: 0.5570
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6902 - acc: 0.5562
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6900 - acc: 0.5563
4736/4849 [============================>.] - ETA: 9s - loss: 0.6904 - acc: 0.5560 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6903 - acc: 0.5567
4849/4849 [==============================] - 431s 89ms/step - loss: 0.6902 - acc: 0.5576 - val_loss: 0.6898 - val_acc: 0.5250

Epoch 00003: val_acc did not improve from 0.56772
Epoch 4/10

  64/4849 [..............................] - ETA: 7:37 - loss: 0.6566 - acc: 0.5625
 128/4849 [..............................] - ETA: 7:07 - loss: 0.6753 - acc: 0.5781
 192/4849 [>.............................] - ETA: 6:47 - loss: 0.6765 - acc: 0.5885
 256/4849 [>.............................] - ETA: 6:37 - loss: 0.6663 - acc: 0.5938
 320/4849 [>.............................] - ETA: 6:24 - loss: 0.6717 - acc: 0.5813
 384/4849 [=>............................] - ETA: 6:12 - loss: 0.6692 - acc: 0.5885
 448/4849 [=>............................] - ETA: 6:10 - loss: 0.6836 - acc: 0.5714
 512/4849 [==>...........................] - ETA: 6:15 - loss: 0.6860 - acc: 0.5625
 576/4849 [==>...........................] - ETA: 6:05 - loss: 0.6870 - acc: 0.5556
 640/4849 [==>...........................] - ETA: 6:00 - loss: 0.6862 - acc: 0.5578
 704/4849 [===>..........................] - ETA: 5:50 - loss: 0.6805 - acc: 0.5682
 768/4849 [===>..........................] - ETA: 5:45 - loss: 0.6830 - acc: 0.5664
 832/4849 [====>.........................] - ETA: 5:39 - loss: 0.6855 - acc: 0.5613
 896/4849 [====>.........................] - ETA: 5:35 - loss: 0.6838 - acc: 0.5636
 960/4849 [====>.........................] - ETA: 5:30 - loss: 0.6857 - acc: 0.5573
1024/4849 [=====>........................] - ETA: 5:24 - loss: 0.6891 - acc: 0.5508
1088/4849 [=====>........................] - ETA: 5:17 - loss: 0.6864 - acc: 0.5597
1152/4849 [======>.......................] - ETA: 5:13 - loss: 0.6844 - acc: 0.5642
1216/4849 [======>.......................] - ETA: 5:06 - loss: 0.6837 - acc: 0.5641
1280/4849 [======>.......................] - ETA: 5:01 - loss: 0.6845 - acc: 0.5641
1344/4849 [=======>......................] - ETA: 4:56 - loss: 0.6839 - acc: 0.5655
1408/4849 [=======>......................] - ETA: 4:50 - loss: 0.6857 - acc: 0.5632
1472/4849 [========>.....................] - ETA: 4:45 - loss: 0.6846 - acc: 0.5659
1536/4849 [========>.....................] - ETA: 4:39 - loss: 0.6841 - acc: 0.5658
1600/4849 [========>.....................] - ETA: 4:33 - loss: 0.6839 - acc: 0.5650
1664/4849 [=========>....................] - ETA: 4:28 - loss: 0.6844 - acc: 0.5625
1728/4849 [=========>....................] - ETA: 4:22 - loss: 0.6853 - acc: 0.5631
1792/4849 [==========>...................] - ETA: 4:17 - loss: 0.6850 - acc: 0.5631
1856/4849 [==========>...................] - ETA: 4:11 - loss: 0.6849 - acc: 0.5657
1920/4849 [==========>...................] - ETA: 4:06 - loss: 0.6846 - acc: 0.5672
1984/4849 [===========>..................] - ETA: 4:01 - loss: 0.6850 - acc: 0.5680
2048/4849 [===========>..................] - ETA: 3:54 - loss: 0.6847 - acc: 0.5708
2112/4849 [============>.................] - ETA: 3:49 - loss: 0.6842 - acc: 0.5715
2176/4849 [============>.................] - ETA: 3:44 - loss: 0.6841 - acc: 0.5722
2240/4849 [============>.................] - ETA: 3:39 - loss: 0.6830 - acc: 0.5737
2304/4849 [=============>................] - ETA: 3:33 - loss: 0.6819 - acc: 0.5738
2368/4849 [=============>................] - ETA: 3:28 - loss: 0.6817 - acc: 0.5735
2432/4849 [==============>...............] - ETA: 3:23 - loss: 0.6821 - acc: 0.5724
2496/4849 [==============>...............] - ETA: 3:17 - loss: 0.6824 - acc: 0.5725
2560/4849 [==============>...............] - ETA: 3:11 - loss: 0.6829 - acc: 0.5727
2624/4849 [===============>..............] - ETA: 3:06 - loss: 0.6835 - acc: 0.5736
2688/4849 [===============>..............] - ETA: 3:00 - loss: 0.6823 - acc: 0.5751
2752/4849 [================>.............] - ETA: 2:55 - loss: 0.6815 - acc: 0.5759
2816/4849 [================>.............] - ETA: 2:50 - loss: 0.6821 - acc: 0.5753
2880/4849 [================>.............] - ETA: 2:44 - loss: 0.6822 - acc: 0.5757
2944/4849 [=================>............] - ETA: 2:39 - loss: 0.6816 - acc: 0.5764
3008/4849 [=================>............] - ETA: 2:34 - loss: 0.6819 - acc: 0.5761
3072/4849 [==================>...........] - ETA: 2:28 - loss: 0.6817 - acc: 0.5755
3136/4849 [==================>...........] - ETA: 2:23 - loss: 0.6831 - acc: 0.5724
3200/4849 [==================>...........] - ETA: 2:17 - loss: 0.6826 - acc: 0.5728
3264/4849 [===================>..........] - ETA: 2:12 - loss: 0.6830 - acc: 0.5720
3328/4849 [===================>..........] - ETA: 2:07 - loss: 0.6834 - acc: 0.5709
3392/4849 [===================>..........] - ETA: 2:01 - loss: 0.6837 - acc: 0.5710
3456/4849 [====================>.........] - ETA: 1:56 - loss: 0.6846 - acc: 0.5692
3520/4849 [====================>.........] - ETA: 1:51 - loss: 0.6847 - acc: 0.5690
3584/4849 [=====================>........] - ETA: 1:45 - loss: 0.6854 - acc: 0.5678
3648/4849 [=====================>........] - ETA: 1:40 - loss: 0.6857 - acc: 0.5672
3712/4849 [=====================>........] - ETA: 1:34 - loss: 0.6849 - acc: 0.5684
3776/4849 [======================>.......] - ETA: 1:29 - loss: 0.6846 - acc: 0.5686
3840/4849 [======================>.......] - ETA: 1:24 - loss: 0.6850 - acc: 0.5680
3904/4849 [=======================>......] - ETA: 1:18 - loss: 0.6847 - acc: 0.5684
3968/4849 [=======================>......] - ETA: 1:13 - loss: 0.6844 - acc: 0.5680
4032/4849 [=======================>......] - ETA: 1:08 - loss: 0.6848 - acc: 0.5667
4096/4849 [========================>.....] - ETA: 1:02 - loss: 0.6849 - acc: 0.5654
4160/4849 [========================>.....] - ETA: 57s - loss: 0.6847 - acc: 0.5647 
4224/4849 [=========================>....] - ETA: 51s - loss: 0.6843 - acc: 0.5651
4288/4849 [=========================>....] - ETA: 46s - loss: 0.6839 - acc: 0.5669
4352/4849 [=========================>....] - ETA: 41s - loss: 0.6841 - acc: 0.5669
4416/4849 [==========================>...] - ETA: 36s - loss: 0.6838 - acc: 0.5670
4480/4849 [==========================>...] - ETA: 30s - loss: 0.6835 - acc: 0.5672
4544/4849 [===========================>..] - ETA: 25s - loss: 0.6834 - acc: 0.5667
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6838 - acc: 0.5662
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6840 - acc: 0.5659
4736/4849 [============================>.] - ETA: 9s - loss: 0.6835 - acc: 0.5671 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6841 - acc: 0.5665
4849/4849 [==============================] - 423s 87ms/step - loss: 0.6840 - acc: 0.5669 - val_loss: 0.7106 - val_acc: 0.5195

Epoch 00004: val_acc did not improve from 0.56772
Epoch 5/10

  64/4849 [..............................] - ETA: 6:56 - loss: 0.6690 - acc: 0.5781
 128/4849 [..............................] - ETA: 6:38 - loss: 0.6643 - acc: 0.5938
 192/4849 [>.............................] - ETA: 6:30 - loss: 0.6719 - acc: 0.5833
 256/4849 [>.............................] - ETA: 6:33 - loss: 0.6724 - acc: 0.5859
 320/4849 [>.............................] - ETA: 6:31 - loss: 0.6715 - acc: 0.5906
 384/4849 [=>............................] - ETA: 6:24 - loss: 0.6740 - acc: 0.5781
 448/4849 [=>............................] - ETA: 6:24 - loss: 0.6742 - acc: 0.5781
 512/4849 [==>...........................] - ETA: 6:15 - loss: 0.6777 - acc: 0.5684
 576/4849 [==>...........................] - ETA: 6:06 - loss: 0.6767 - acc: 0.5625
 640/4849 [==>...........................] - ETA: 6:04 - loss: 0.6742 - acc: 0.5719
 704/4849 [===>..........................] - ETA: 5:54 - loss: 0.6734 - acc: 0.5696
 768/4849 [===>..........................] - ETA: 5:49 - loss: 0.6698 - acc: 0.5794
 832/4849 [====>.........................] - ETA: 5:44 - loss: 0.6653 - acc: 0.5950
 896/4849 [====>.........................] - ETA: 5:40 - loss: 0.6630 - acc: 0.6016
 960/4849 [====>.........................] - ETA: 5:32 - loss: 0.6639 - acc: 0.6000
1024/4849 [=====>........................] - ETA: 5:28 - loss: 0.6652 - acc: 0.5986
1088/4849 [=====>........................] - ETA: 5:20 - loss: 0.6652 - acc: 0.5983
1152/4849 [======>.......................] - ETA: 5:16 - loss: 0.6652 - acc: 0.5981
1216/4849 [======>.......................] - ETA: 5:10 - loss: 0.6666 - acc: 0.5946
1280/4849 [======>.......................] - ETA: 5:04 - loss: 0.6668 - acc: 0.5945
1344/4849 [=======>......................] - ETA: 5:00 - loss: 0.6671 - acc: 0.5982
1408/4849 [=======>......................] - ETA: 4:54 - loss: 0.6667 - acc: 0.6037
1472/4849 [========>.....................] - ETA: 4:49 - loss: 0.6666 - acc: 0.6046
1536/4849 [========>.....................] - ETA: 4:44 - loss: 0.6681 - acc: 0.6061
1600/4849 [========>.....................] - ETA: 4:38 - loss: 0.6674 - acc: 0.6088
1664/4849 [=========>....................] - ETA: 4:32 - loss: 0.6666 - acc: 0.6124
1728/4849 [=========>....................] - ETA: 4:27 - loss: 0.6649 - acc: 0.6134
1792/4849 [==========>...................] - ETA: 4:20 - loss: 0.6658 - acc: 0.6122
1856/4849 [==========>...................] - ETA: 4:15 - loss: 0.6649 - acc: 0.6148
1920/4849 [==========>...................] - ETA: 4:11 - loss: 0.6674 - acc: 0.6115
1984/4849 [===========>..................] - ETA: 4:05 - loss: 0.6680 - acc: 0.6094
2048/4849 [===========>..................] - ETA: 3:59 - loss: 0.6689 - acc: 0.6099
2112/4849 [============>.................] - ETA: 3:54 - loss: 0.6702 - acc: 0.6084
2176/4849 [============>.................] - ETA: 3:49 - loss: 0.6705 - acc: 0.6071
2240/4849 [============>.................] - ETA: 3:43 - loss: 0.6711 - acc: 0.6058
2304/4849 [=============>................] - ETA: 3:38 - loss: 0.6711 - acc: 0.6050
2368/4849 [=============>................] - ETA: 3:32 - loss: 0.6696 - acc: 0.6077
2432/4849 [==============>...............] - ETA: 3:27 - loss: 0.6683 - acc: 0.6094
2496/4849 [==============>...............] - ETA: 3:22 - loss: 0.6682 - acc: 0.6094
2560/4849 [==============>...............] - ETA: 3:16 - loss: 0.6695 - acc: 0.6078
2624/4849 [===============>..............] - ETA: 3:11 - loss: 0.6712 - acc: 0.6059
2688/4849 [===============>..............] - ETA: 3:06 - loss: 0.6720 - acc: 0.6060
2752/4849 [================>.............] - ETA: 3:01 - loss: 0.6726 - acc: 0.6043
2816/4849 [================>.............] - ETA: 2:55 - loss: 0.6743 - acc: 0.6019
2880/4849 [================>.............] - ETA: 2:49 - loss: 0.6749 - acc: 0.6014
2944/4849 [=================>............] - ETA: 2:44 - loss: 0.6763 - acc: 0.5985
3008/4849 [=================>............] - ETA: 2:38 - loss: 0.6755 - acc: 0.5991
3072/4849 [==================>...........] - ETA: 2:33 - loss: 0.6755 - acc: 0.5996
3136/4849 [==================>...........] - ETA: 2:27 - loss: 0.6754 - acc: 0.6004
3200/4849 [==================>...........] - ETA: 2:21 - loss: 0.6755 - acc: 0.5991
3264/4849 [===================>..........] - ETA: 2:16 - loss: 0.6758 - acc: 0.5977
3328/4849 [===================>..........] - ETA: 2:10 - loss: 0.6767 - acc: 0.5962
3392/4849 [===================>..........] - ETA: 2:05 - loss: 0.6770 - acc: 0.5949
3456/4849 [====================>.........] - ETA: 1:59 - loss: 0.6770 - acc: 0.5943
3520/4849 [====================>.........] - ETA: 1:54 - loss: 0.6773 - acc: 0.5923
3584/4849 [=====================>........] - ETA: 1:48 - loss: 0.6776 - acc: 0.5904
3648/4849 [=====================>........] - ETA: 1:43 - loss: 0.6779 - acc: 0.5899
3712/4849 [=====================>........] - ETA: 1:37 - loss: 0.6779 - acc: 0.5889
3776/4849 [======================>.......] - ETA: 1:32 - loss: 0.6775 - acc: 0.5887
3840/4849 [======================>.......] - ETA: 1:26 - loss: 0.6774 - acc: 0.5891
3904/4849 [=======================>......] - ETA: 1:21 - loss: 0.6776 - acc: 0.5884
3968/4849 [=======================>......] - ETA: 1:15 - loss: 0.6780 - acc: 0.5872
4032/4849 [=======================>......] - ETA: 1:10 - loss: 0.6776 - acc: 0.5885
4096/4849 [========================>.....] - ETA: 1:04 - loss: 0.6786 - acc: 0.5862
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6788 - acc: 0.5853 
4224/4849 [=========================>....] - ETA: 53s - loss: 0.6785 - acc: 0.5862
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6799 - acc: 0.5833
4352/4849 [=========================>....] - ETA: 42s - loss: 0.6802 - acc: 0.5823
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6803 - acc: 0.5817
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6805 - acc: 0.5815
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6806 - acc: 0.5816
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6806 - acc: 0.5816
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6806 - acc: 0.5815
4736/4849 [============================>.] - ETA: 9s - loss: 0.6810 - acc: 0.5809 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6813 - acc: 0.5796
4849/4849 [==============================] - 435s 90ms/step - loss: 0.6816 - acc: 0.5789 - val_loss: 0.6965 - val_acc: 0.5158

Epoch 00005: val_acc did not improve from 0.56772
Epoch 6/10

  64/4849 [..............................] - ETA: 6:50 - loss: 0.6794 - acc: 0.5938
 128/4849 [..............................] - ETA: 6:24 - loss: 0.6748 - acc: 0.5859
 192/4849 [>.............................] - ETA: 6:33 - loss: 0.6715 - acc: 0.5938
 256/4849 [>.............................] - ETA: 6:35 - loss: 0.6744 - acc: 0.5898
 320/4849 [>.............................] - ETA: 6:34 - loss: 0.6690 - acc: 0.5938
 384/4849 [=>............................] - ETA: 6:30 - loss: 0.6694 - acc: 0.5833
 448/4849 [=>............................] - ETA: 6:29 - loss: 0.6710 - acc: 0.5848
 512/4849 [==>...........................] - ETA: 6:17 - loss: 0.6727 - acc: 0.5820
 576/4849 [==>...........................] - ETA: 6:12 - loss: 0.6739 - acc: 0.5747
 640/4849 [==>...........................] - ETA: 6:08 - loss: 0.6760 - acc: 0.5734
 704/4849 [===>..........................] - ETA: 6:03 - loss: 0.6782 - acc: 0.5710
 768/4849 [===>..........................] - ETA: 5:58 - loss: 0.6783 - acc: 0.5768
 832/4849 [====>.........................] - ETA: 5:50 - loss: 0.6750 - acc: 0.5829
 896/4849 [====>.........................] - ETA: 5:47 - loss: 0.6785 - acc: 0.5770
 960/4849 [====>.........................] - ETA: 5:39 - loss: 0.6766 - acc: 0.5781
1024/4849 [=====>........................] - ETA: 5:35 - loss: 0.6761 - acc: 0.5762
1088/4849 [=====>........................] - ETA: 5:28 - loss: 0.6736 - acc: 0.5781
1152/4849 [======>.......................] - ETA: 5:21 - loss: 0.6744 - acc: 0.5799
1216/4849 [======>.......................] - ETA: 5:18 - loss: 0.6748 - acc: 0.5806
1280/4849 [======>.......................] - ETA: 5:11 - loss: 0.6747 - acc: 0.5797
1344/4849 [=======>......................] - ETA: 5:05 - loss: 0.6759 - acc: 0.5774
1408/4849 [=======>......................] - ETA: 5:01 - loss: 0.6737 - acc: 0.5803
1472/4849 [========>.....................] - ETA: 4:56 - loss: 0.6763 - acc: 0.5795
1536/4849 [========>.....................] - ETA: 4:50 - loss: 0.6749 - acc: 0.5827
1600/4849 [========>.....................] - ETA: 4:44 - loss: 0.6755 - acc: 0.5813
1664/4849 [=========>....................] - ETA: 4:38 - loss: 0.6752 - acc: 0.5835
1728/4849 [=========>....................] - ETA: 4:32 - loss: 0.6743 - acc: 0.5880
1792/4849 [==========>...................] - ETA: 4:25 - loss: 0.6762 - acc: 0.5854
1856/4849 [==========>...................] - ETA: 4:20 - loss: 0.6757 - acc: 0.5862
1920/4849 [==========>...................] - ETA: 4:14 - loss: 0.6753 - acc: 0.5854
1984/4849 [===========>..................] - ETA: 4:08 - loss: 0.6767 - acc: 0.5822
2048/4849 [===========>..................] - ETA: 4:03 - loss: 0.6759 - acc: 0.5845
2112/4849 [============>.................] - ETA: 3:57 - loss: 0.6757 - acc: 0.5852
2176/4849 [============>.................] - ETA: 3:52 - loss: 0.6769 - acc: 0.5813
2240/4849 [============>.................] - ETA: 3:46 - loss: 0.6769 - acc: 0.5830
2304/4849 [=============>................] - ETA: 3:40 - loss: 0.6778 - acc: 0.5825
2368/4849 [=============>................] - ETA: 3:35 - loss: 0.6774 - acc: 0.5849
2432/4849 [==============>...............] - ETA: 3:29 - loss: 0.6765 - acc: 0.5863
2496/4849 [==============>...............] - ETA: 3:23 - loss: 0.6768 - acc: 0.5869
2560/4849 [==============>...............] - ETA: 3:17 - loss: 0.6781 - acc: 0.5852
2624/4849 [===============>..............] - ETA: 3:11 - loss: 0.6776 - acc: 0.5869
2688/4849 [===============>..............] - ETA: 3:04 - loss: 0.6785 - acc: 0.5856
2752/4849 [================>.............] - ETA: 2:59 - loss: 0.6786 - acc: 0.5861
2816/4849 [================>.............] - ETA: 2:53 - loss: 0.6783 - acc: 0.5870
2880/4849 [================>.............] - ETA: 2:47 - loss: 0.6775 - acc: 0.5878
2944/4849 [=================>............] - ETA: 2:42 - loss: 0.6779 - acc: 0.5880
3008/4849 [=================>............] - ETA: 2:36 - loss: 0.6769 - acc: 0.5901
3072/4849 [==================>...........] - ETA: 2:30 - loss: 0.6758 - acc: 0.5924
3136/4849 [==================>...........] - ETA: 2:25 - loss: 0.6765 - acc: 0.5902
3200/4849 [==================>...........] - ETA: 2:20 - loss: 0.6761 - acc: 0.5903
3264/4849 [===================>..........] - ETA: 2:14 - loss: 0.6760 - acc: 0.5922
3328/4849 [===================>..........] - ETA: 2:08 - loss: 0.6751 - acc: 0.5947
3392/4849 [===================>..........] - ETA: 2:03 - loss: 0.6743 - acc: 0.5961
3456/4849 [====================>.........] - ETA: 1:57 - loss: 0.6742 - acc: 0.5955
3520/4849 [====================>.........] - ETA: 1:52 - loss: 0.6735 - acc: 0.5955
3584/4849 [=====================>........] - ETA: 1:46 - loss: 0.6739 - acc: 0.5940
3648/4849 [=====================>........] - ETA: 1:41 - loss: 0.6743 - acc: 0.5932
3712/4849 [=====================>........] - ETA: 1:35 - loss: 0.6751 - acc: 0.5911
3776/4849 [======================>.......] - ETA: 1:30 - loss: 0.6753 - acc: 0.5911
3840/4849 [======================>.......] - ETA: 1:24 - loss: 0.6764 - acc: 0.5893
3904/4849 [=======================>......] - ETA: 1:19 - loss: 0.6761 - acc: 0.5886
3968/4849 [=======================>......] - ETA: 1:13 - loss: 0.6755 - acc: 0.5897
4032/4849 [=======================>......] - ETA: 1:08 - loss: 0.6751 - acc: 0.5900
4096/4849 [========================>.....] - ETA: 1:03 - loss: 0.6749 - acc: 0.5903
4160/4849 [========================>.....] - ETA: 57s - loss: 0.6741 - acc: 0.5921 
4224/4849 [=========================>....] - ETA: 52s - loss: 0.6739 - acc: 0.5926
4288/4849 [=========================>....] - ETA: 46s - loss: 0.6744 - acc: 0.5917
4352/4849 [=========================>....] - ETA: 41s - loss: 0.6745 - acc: 0.5915
4416/4849 [==========================>...] - ETA: 36s - loss: 0.6739 - acc: 0.5931
4480/4849 [==========================>...] - ETA: 30s - loss: 0.6739 - acc: 0.5926
4544/4849 [===========================>..] - ETA: 25s - loss: 0.6738 - acc: 0.5929
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6735 - acc: 0.5940
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6738 - acc: 0.5935
4736/4849 [============================>.] - ETA: 9s - loss: 0.6743 - acc: 0.5931 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6744 - acc: 0.5931
4849/4849 [==============================] - 421s 87ms/step - loss: 0.6744 - acc: 0.5931 - val_loss: 0.6923 - val_acc: 0.5603

Epoch 00006: val_acc did not improve from 0.56772
Epoch 7/10

  64/4849 [..............................] - ETA: 5:47 - loss: 0.5678 - acc: 0.7812
 128/4849 [..............................] - ETA: 6:04 - loss: 0.6198 - acc: 0.6641
 192/4849 [>.............................] - ETA: 6:05 - loss: 0.6352 - acc: 0.6354
 256/4849 [>.............................] - ETA: 6:08 - loss: 0.6554 - acc: 0.6016
 320/4849 [>.............................] - ETA: 5:54 - loss: 0.6681 - acc: 0.5844
 384/4849 [=>............................] - ETA: 5:54 - loss: 0.6731 - acc: 0.5807
 448/4849 [=>............................] - ETA: 5:51 - loss: 0.6694 - acc: 0.5893
 512/4849 [==>...........................] - ETA: 5:47 - loss: 0.6701 - acc: 0.5879
 576/4849 [==>...........................] - ETA: 5:42 - loss: 0.6650 - acc: 0.5955
 640/4849 [==>...........................] - ETA: 5:37 - loss: 0.6628 - acc: 0.5938
 704/4849 [===>..........................] - ETA: 5:30 - loss: 0.6623 - acc: 0.5923
 768/4849 [===>..........................] - ETA: 5:27 - loss: 0.6636 - acc: 0.5924
 832/4849 [====>.........................] - ETA: 5:20 - loss: 0.6630 - acc: 0.5950
 896/4849 [====>.........................] - ETA: 5:16 - loss: 0.6620 - acc: 0.5960
 960/4849 [====>.........................] - ETA: 5:11 - loss: 0.6660 - acc: 0.5885
1024/4849 [=====>........................] - ETA: 5:07 - loss: 0.6652 - acc: 0.5889
1088/4849 [=====>........................] - ETA: 5:05 - loss: 0.6636 - acc: 0.5956
1152/4849 [======>.......................] - ETA: 4:58 - loss: 0.6625 - acc: 0.5972
1216/4849 [======>.......................] - ETA: 4:54 - loss: 0.6608 - acc: 0.6020
1280/4849 [======>.......................] - ETA: 4:47 - loss: 0.6610 - acc: 0.6023
1344/4849 [=======>......................] - ETA: 4:42 - loss: 0.6610 - acc: 0.6019
1408/4849 [=======>......................] - ETA: 4:37 - loss: 0.6615 - acc: 0.6030
1472/4849 [========>.....................] - ETA: 4:35 - loss: 0.6653 - acc: 0.5999
1536/4849 [========>.....................] - ETA: 4:32 - loss: 0.6682 - acc: 0.5970
1600/4849 [========>.....................] - ETA: 4:29 - loss: 0.6684 - acc: 0.5969
1664/4849 [=========>....................] - ETA: 4:25 - loss: 0.6687 - acc: 0.5956
1728/4849 [=========>....................] - ETA: 4:21 - loss: 0.6700 - acc: 0.5949
1792/4849 [==========>...................] - ETA: 4:16 - loss: 0.6711 - acc: 0.5943
1856/4849 [==========>...................] - ETA: 4:11 - loss: 0.6734 - acc: 0.5921
1920/4849 [==========>...................] - ETA: 4:07 - loss: 0.6739 - acc: 0.5917
1984/4849 [===========>..................] - ETA: 4:02 - loss: 0.6741 - acc: 0.5922
2048/4849 [===========>..................] - ETA: 3:58 - loss: 0.6734 - acc: 0.5918
2112/4849 [============>.................] - ETA: 3:53 - loss: 0.6740 - acc: 0.5919
2176/4849 [============>.................] - ETA: 3:48 - loss: 0.6743 - acc: 0.5915
2240/4849 [============>.................] - ETA: 3:43 - loss: 0.6749 - acc: 0.5915
2304/4849 [=============>................] - ETA: 3:39 - loss: 0.6735 - acc: 0.5946
2368/4849 [=============>................] - ETA: 3:34 - loss: 0.6741 - acc: 0.5921
2432/4849 [==============>...............] - ETA: 3:29 - loss: 0.6746 - acc: 0.5913
2496/4849 [==============>...............] - ETA: 3:24 - loss: 0.6745 - acc: 0.5913
2560/4849 [==============>...............] - ETA: 3:19 - loss: 0.6746 - acc: 0.5918
2624/4849 [===============>..............] - ETA: 3:14 - loss: 0.6743 - acc: 0.5926
2688/4849 [===============>..............] - ETA: 3:09 - loss: 0.6743 - acc: 0.5911
2752/4849 [================>.............] - ETA: 3:03 - loss: 0.6739 - acc: 0.5919
2816/4849 [================>.............] - ETA: 2:58 - loss: 0.6732 - acc: 0.5930
2880/4849 [================>.............] - ETA: 2:52 - loss: 0.6727 - acc: 0.5931
2944/4849 [=================>............] - ETA: 2:47 - loss: 0.6730 - acc: 0.5924
3008/4849 [=================>............] - ETA: 2:41 - loss: 0.6731 - acc: 0.5908
3072/4849 [==================>...........] - ETA: 2:36 - loss: 0.6733 - acc: 0.5915
3136/4849 [==================>...........] - ETA: 2:31 - loss: 0.6732 - acc: 0.5922
3200/4849 [==================>...........] - ETA: 2:25 - loss: 0.6734 - acc: 0.5919
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6726 - acc: 0.5934
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6729 - acc: 0.5931
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6724 - acc: 0.5943
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6721 - acc: 0.5958
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6728 - acc: 0.5935
3584/4849 [=====================>........] - ETA: 1:53 - loss: 0.6725 - acc: 0.5960
3648/4849 [=====================>........] - ETA: 1:47 - loss: 0.6728 - acc: 0.5959
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6726 - acc: 0.5970
3776/4849 [======================>.......] - ETA: 1:36 - loss: 0.6729 - acc: 0.5956
3840/4849 [======================>.......] - ETA: 1:30 - loss: 0.6732 - acc: 0.5948
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6737 - acc: 0.5927
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6732 - acc: 0.5930
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6730 - acc: 0.5940
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6726 - acc: 0.5942
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6728 - acc: 0.5935
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6734 - acc: 0.5923 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6740 - acc: 0.5907
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6737 - acc: 0.5908
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6735 - acc: 0.5913
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6740 - acc: 0.5906
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6741 - acc: 0.5902
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6736 - acc: 0.5903
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6741 - acc: 0.5893
4736/4849 [============================>.] - ETA: 10s - loss: 0.6742 - acc: 0.5895
4800/4849 [============================>.] - ETA: 4s - loss: 0.6739 - acc: 0.5906 
4849/4849 [==============================] - 455s 94ms/step - loss: 0.6737 - acc: 0.5915 - val_loss: 0.6875 - val_acc: 0.5417

Epoch 00007: val_acc did not improve from 0.56772
Epoch 8/10

  64/4849 [..............................] - ETA: 7:23 - loss: 0.7051 - acc: 0.5000
 128/4849 [..............................] - ETA: 6:59 - loss: 0.6825 - acc: 0.5547
 192/4849 [>.............................] - ETA: 7:08 - loss: 0.6711 - acc: 0.5781
 256/4849 [>.............................] - ETA: 6:54 - loss: 0.6767 - acc: 0.5664
 320/4849 [>.............................] - ETA: 6:33 - loss: 0.6744 - acc: 0.5781
 384/4849 [=>............................] - ETA: 6:30 - loss: 0.6721 - acc: 0.5859
 448/4849 [=>............................] - ETA: 6:19 - loss: 0.6754 - acc: 0.5781
 512/4849 [==>...........................] - ETA: 6:14 - loss: 0.6728 - acc: 0.5762
 576/4849 [==>...........................] - ETA: 6:08 - loss: 0.6740 - acc: 0.5799
 640/4849 [==>...........................] - ETA: 6:05 - loss: 0.6743 - acc: 0.5859
 704/4849 [===>..........................] - ETA: 5:59 - loss: 0.6746 - acc: 0.5824
 768/4849 [===>..........................] - ETA: 5:51 - loss: 0.6738 - acc: 0.5833
 832/4849 [====>.........................] - ETA: 5:43 - loss: 0.6732 - acc: 0.5877
 896/4849 [====>.........................] - ETA: 5:38 - loss: 0.6733 - acc: 0.5882
 960/4849 [====>.........................] - ETA: 5:31 - loss: 0.6739 - acc: 0.5885
1024/4849 [=====>........................] - ETA: 5:28 - loss: 0.6743 - acc: 0.5869
1088/4849 [=====>........................] - ETA: 5:22 - loss: 0.6750 - acc: 0.5836
1152/4849 [======>.......................] - ETA: 5:15 - loss: 0.6728 - acc: 0.5885
1216/4849 [======>.......................] - ETA: 5:11 - loss: 0.6726 - acc: 0.5905
1280/4849 [======>.......................] - ETA: 5:03 - loss: 0.6721 - acc: 0.5914
1344/4849 [=======>......................] - ETA: 5:01 - loss: 0.6722 - acc: 0.5923
1408/4849 [=======>......................] - ETA: 4:55 - loss: 0.6719 - acc: 0.5909
1472/4849 [========>.....................] - ETA: 4:49 - loss: 0.6720 - acc: 0.5904
1536/4849 [========>.....................] - ETA: 4:44 - loss: 0.6713 - acc: 0.5911
1600/4849 [========>.....................] - ETA: 4:37 - loss: 0.6719 - acc: 0.5894
1664/4849 [=========>....................] - ETA: 4:31 - loss: 0.6717 - acc: 0.5901
1728/4849 [=========>....................] - ETA: 4:26 - loss: 0.6726 - acc: 0.5885
1792/4849 [==========>...................] - ETA: 4:21 - loss: 0.6733 - acc: 0.5871
1856/4849 [==========>...................] - ETA: 4:15 - loss: 0.6739 - acc: 0.5851
1920/4849 [==========>...................] - ETA: 4:10 - loss: 0.6732 - acc: 0.5870
1984/4849 [===========>..................] - ETA: 4:04 - loss: 0.6743 - acc: 0.5847
2048/4849 [===========>..................] - ETA: 3:58 - loss: 0.6752 - acc: 0.5835
2112/4849 [============>.................] - ETA: 3:53 - loss: 0.6749 - acc: 0.5848
2176/4849 [============>.................] - ETA: 3:48 - loss: 0.6750 - acc: 0.5836
2240/4849 [============>.................] - ETA: 3:43 - loss: 0.6757 - acc: 0.5826
2304/4849 [=============>................] - ETA: 3:38 - loss: 0.6762 - acc: 0.5794
2368/4849 [=============>................] - ETA: 3:32 - loss: 0.6754 - acc: 0.5815
2432/4849 [==============>...............] - ETA: 3:26 - loss: 0.6760 - acc: 0.5814
2496/4849 [==============>...............] - ETA: 3:21 - loss: 0.6755 - acc: 0.5805
2560/4849 [==============>...............] - ETA: 3:15 - loss: 0.6764 - acc: 0.5773
2624/4849 [===============>..............] - ETA: 3:09 - loss: 0.6757 - acc: 0.5793
2688/4849 [===============>..............] - ETA: 3:04 - loss: 0.6768 - acc: 0.5770
2752/4849 [================>.............] - ETA: 2:58 - loss: 0.6763 - acc: 0.5781
2816/4849 [================>.............] - ETA: 2:53 - loss: 0.6759 - acc: 0.5788
2880/4849 [================>.............] - ETA: 2:47 - loss: 0.6754 - acc: 0.5809
2944/4849 [=================>............] - ETA: 2:42 - loss: 0.6764 - acc: 0.5795
3008/4849 [=================>............] - ETA: 2:37 - loss: 0.6763 - acc: 0.5808
3072/4849 [==================>...........] - ETA: 2:31 - loss: 0.6771 - acc: 0.5801
3136/4849 [==================>...........] - ETA: 2:26 - loss: 0.6763 - acc: 0.5813
3200/4849 [==================>...........] - ETA: 2:20 - loss: 0.6759 - acc: 0.5825
3264/4849 [===================>..........] - ETA: 2:14 - loss: 0.6766 - acc: 0.5818
3328/4849 [===================>..........] - ETA: 2:09 - loss: 0.6765 - acc: 0.5835
3392/4849 [===================>..........] - ETA: 2:03 - loss: 0.6759 - acc: 0.5849
3456/4849 [====================>.........] - ETA: 1:58 - loss: 0.6755 - acc: 0.5859
3520/4849 [====================>.........] - ETA: 1:52 - loss: 0.6758 - acc: 0.5852
3584/4849 [=====================>........] - ETA: 1:47 - loss: 0.6765 - acc: 0.5834
3648/4849 [=====================>........] - ETA: 1:41 - loss: 0.6762 - acc: 0.5847
3712/4849 [=====================>........] - ETA: 1:36 - loss: 0.6758 - acc: 0.5851
3776/4849 [======================>.......] - ETA: 1:31 - loss: 0.6757 - acc: 0.5858
3840/4849 [======================>.......] - ETA: 1:25 - loss: 0.6754 - acc: 0.5854
3904/4849 [=======================>......] - ETA: 1:20 - loss: 0.6757 - acc: 0.5845
3968/4849 [=======================>......] - ETA: 1:14 - loss: 0.6755 - acc: 0.5839
4032/4849 [=======================>......] - ETA: 1:09 - loss: 0.6756 - acc: 0.5836
4096/4849 [========================>.....] - ETA: 1:03 - loss: 0.6759 - acc: 0.5837
4160/4849 [========================>.....] - ETA: 58s - loss: 0.6752 - acc: 0.5849 
4224/4849 [=========================>....] - ETA: 53s - loss: 0.6746 - acc: 0.5859
4288/4849 [=========================>....] - ETA: 47s - loss: 0.6750 - acc: 0.5854
4352/4849 [=========================>....] - ETA: 42s - loss: 0.6754 - acc: 0.5843
4416/4849 [==========================>...] - ETA: 36s - loss: 0.6747 - acc: 0.5863
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6750 - acc: 0.5846
4544/4849 [===========================>..] - ETA: 25s - loss: 0.6748 - acc: 0.5847
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6749 - acc: 0.5849
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6745 - acc: 0.5860
4736/4849 [============================>.] - ETA: 9s - loss: 0.6740 - acc: 0.5870 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6739 - acc: 0.5877
4849/4849 [==============================] - 428s 88ms/step - loss: 0.6741 - acc: 0.5867 - val_loss: 0.6831 - val_acc: 0.5807

Epoch 00008: val_acc improved from 0.56772 to 0.58071, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window08/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 9/10

  64/4849 [..............................] - ETA: 6:56 - loss: 0.6203 - acc: 0.6562
 128/4849 [..............................] - ETA: 6:42 - loss: 0.6551 - acc: 0.6172
 192/4849 [>.............................] - ETA: 6:43 - loss: 0.6522 - acc: 0.6198
 256/4849 [>.............................] - ETA: 6:25 - loss: 0.6567 - acc: 0.6172
 320/4849 [>.............................] - ETA: 6:19 - loss: 0.6621 - acc: 0.6094
 384/4849 [=>............................] - ETA: 6:07 - loss: 0.6590 - acc: 0.6120
 448/4849 [=>............................] - ETA: 6:08 - loss: 0.6665 - acc: 0.6004
 512/4849 [==>...........................] - ETA: 6:10 - loss: 0.6618 - acc: 0.6113
 576/4849 [==>...........................] - ETA: 6:09 - loss: 0.6623 - acc: 0.6111
 640/4849 [==>...........................] - ETA: 6:10 - loss: 0.6609 - acc: 0.6125
 704/4849 [===>..........................] - ETA: 6:07 - loss: 0.6619 - acc: 0.6065
 768/4849 [===>..........................] - ETA: 6:02 - loss: 0.6638 - acc: 0.6068
 832/4849 [====>.........................] - ETA: 6:00 - loss: 0.6751 - acc: 0.5901
 896/4849 [====>.........................] - ETA: 5:57 - loss: 0.6739 - acc: 0.5915
 960/4849 [====>.........................] - ETA: 5:53 - loss: 0.6707 - acc: 0.5969
1024/4849 [=====>........................] - ETA: 5:50 - loss: 0.6721 - acc: 0.6006
1088/4849 [=====>........................] - ETA: 5:44 - loss: 0.6734 - acc: 0.5983
1152/4849 [======>.......................] - ETA: 5:38 - loss: 0.6741 - acc: 0.5981
1216/4849 [======>.......................] - ETA: 5:34 - loss: 0.6736 - acc: 0.5987
1280/4849 [======>.......................] - ETA: 5:29 - loss: 0.6716 - acc: 0.5992
1344/4849 [=======>......................] - ETA: 5:22 - loss: 0.6695 - acc: 0.6019
1408/4849 [=======>......................] - ETA: 5:17 - loss: 0.6700 - acc: 0.5987
1472/4849 [========>.....................] - ETA: 5:11 - loss: 0.6716 - acc: 0.5951
1536/4849 [========>.....................] - ETA: 5:06 - loss: 0.6728 - acc: 0.5924
1600/4849 [========>.....................] - ETA: 5:00 - loss: 0.6709 - acc: 0.5962
1664/4849 [=========>....................] - ETA: 4:54 - loss: 0.6698 - acc: 0.5986
1728/4849 [=========>....................] - ETA: 4:49 - loss: 0.6703 - acc: 0.5972
1792/4849 [==========>...................] - ETA: 4:42 - loss: 0.6687 - acc: 0.6016
1856/4849 [==========>...................] - ETA: 4:37 - loss: 0.6685 - acc: 0.6002
1920/4849 [==========>...................] - ETA: 4:31 - loss: 0.6682 - acc: 0.5990
1984/4849 [===========>..................] - ETA: 4:26 - loss: 0.6687 - acc: 0.5978
2048/4849 [===========>..................] - ETA: 4:19 - loss: 0.6687 - acc: 0.5981
2112/4849 [============>.................] - ETA: 4:13 - loss: 0.6686 - acc: 0.5980
2176/4849 [============>.................] - ETA: 4:07 - loss: 0.6684 - acc: 0.5983
2240/4849 [============>.................] - ETA: 4:02 - loss: 0.6685 - acc: 0.5969
2304/4849 [=============>................] - ETA: 3:56 - loss: 0.6673 - acc: 0.5985
2368/4849 [=============>................] - ETA: 3:51 - loss: 0.6670 - acc: 0.6009
2432/4849 [==============>...............] - ETA: 3:45 - loss: 0.6689 - acc: 0.5975
2496/4849 [==============>...............] - ETA: 3:40 - loss: 0.6690 - acc: 0.5978
2560/4849 [==============>...............] - ETA: 3:34 - loss: 0.6692 - acc: 0.5969
2624/4849 [===============>..............] - ETA: 3:28 - loss: 0.6688 - acc: 0.5964
2688/4849 [===============>..............] - ETA: 3:22 - loss: 0.6674 - acc: 0.5982
2752/4849 [================>.............] - ETA: 3:16 - loss: 0.6672 - acc: 0.5985
2816/4849 [================>.............] - ETA: 3:10 - loss: 0.6671 - acc: 0.5994
2880/4849 [================>.............] - ETA: 3:04 - loss: 0.6675 - acc: 0.5986
2944/4849 [=================>............] - ETA: 2:58 - loss: 0.6676 - acc: 0.5985
3008/4849 [=================>............] - ETA: 2:52 - loss: 0.6691 - acc: 0.5954
3072/4849 [==================>...........] - ETA: 2:46 - loss: 0.6692 - acc: 0.5954
3136/4849 [==================>...........] - ETA: 2:40 - loss: 0.6688 - acc: 0.5957
3200/4849 [==================>...........] - ETA: 2:34 - loss: 0.6691 - acc: 0.5947
3264/4849 [===================>..........] - ETA: 2:28 - loss: 0.6683 - acc: 0.5965
3328/4849 [===================>..........] - ETA: 2:22 - loss: 0.6692 - acc: 0.5944
3392/4849 [===================>..........] - ETA: 2:16 - loss: 0.6700 - acc: 0.5935
3456/4849 [====================>.........] - ETA: 2:10 - loss: 0.6695 - acc: 0.5943
3520/4849 [====================>.........] - ETA: 2:04 - loss: 0.6699 - acc: 0.5938
3584/4849 [=====================>........] - ETA: 1:58 - loss: 0.6708 - acc: 0.5918
3648/4849 [=====================>........] - ETA: 1:52 - loss: 0.6710 - acc: 0.5913
3712/4849 [=====================>........] - ETA: 1:46 - loss: 0.6705 - acc: 0.5924
3776/4849 [======================>.......] - ETA: 1:40 - loss: 0.6706 - acc: 0.5924
3840/4849 [======================>.......] - ETA: 1:34 - loss: 0.6704 - acc: 0.5932
3904/4849 [=======================>......] - ETA: 1:28 - loss: 0.6705 - acc: 0.5932
3968/4849 [=======================>......] - ETA: 1:22 - loss: 0.6714 - acc: 0.5915
4032/4849 [=======================>......] - ETA: 1:16 - loss: 0.6714 - acc: 0.5910
4096/4849 [========================>.....] - ETA: 1:10 - loss: 0.6716 - acc: 0.5911
4160/4849 [========================>.....] - ETA: 1:04 - loss: 0.6716 - acc: 0.5906
4224/4849 [=========================>....] - ETA: 58s - loss: 0.6727 - acc: 0.5876 
4288/4849 [=========================>....] - ETA: 52s - loss: 0.6725 - acc: 0.5872
4352/4849 [=========================>....] - ETA: 46s - loss: 0.6725 - acc: 0.5869
4416/4849 [==========================>...] - ETA: 40s - loss: 0.6728 - acc: 0.5856
4480/4849 [==========================>...] - ETA: 34s - loss: 0.6728 - acc: 0.5850
4544/4849 [===========================>..] - ETA: 28s - loss: 0.6732 - acc: 0.5845
4608/4849 [===========================>..] - ETA: 22s - loss: 0.6733 - acc: 0.5849
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6734 - acc: 0.5850
4736/4849 [============================>.] - ETA: 10s - loss: 0.6732 - acc: 0.5859
4800/4849 [============================>.] - ETA: 4s - loss: 0.6734 - acc: 0.5854 
4849/4849 [==============================] - 474s 98ms/step - loss: 0.6731 - acc: 0.5859 - val_loss: 0.6986 - val_acc: 0.5325

Epoch 00009: val_acc did not improve from 0.58071
Epoch 10/10

  64/4849 [..............................] - ETA: 7:21 - loss: 0.6432 - acc: 0.6406
 128/4849 [..............................] - ETA: 7:16 - loss: 0.6280 - acc: 0.6719
 192/4849 [>.............................] - ETA: 7:08 - loss: 0.6276 - acc: 0.6875
 256/4849 [>.............................] - ETA: 7:02 - loss: 0.6454 - acc: 0.6641
 320/4849 [>.............................] - ETA: 6:57 - loss: 0.6621 - acc: 0.6281
 384/4849 [=>............................] - ETA: 6:57 - loss: 0.6695 - acc: 0.6016
 448/4849 [=>............................] - ETA: 6:53 - loss: 0.6651 - acc: 0.6071
 512/4849 [==>...........................] - ETA: 6:44 - loss: 0.6645 - acc: 0.6074
 576/4849 [==>...........................] - ETA: 6:37 - loss: 0.6631 - acc: 0.6042
 640/4849 [==>...........................] - ETA: 6:31 - loss: 0.6694 - acc: 0.5891
 704/4849 [===>..........................] - ETA: 6:26 - loss: 0.6704 - acc: 0.5895
 768/4849 [===>..........................] - ETA: 6:22 - loss: 0.6662 - acc: 0.6003
 832/4849 [====>.........................] - ETA: 6:17 - loss: 0.6678 - acc: 0.5986
 896/4849 [====>.........................] - ETA: 6:09 - loss: 0.6685 - acc: 0.5982
 960/4849 [====>.........................] - ETA: 6:05 - loss: 0.6696 - acc: 0.5969
1024/4849 [=====>........................] - ETA: 5:59 - loss: 0.6690 - acc: 0.5967
1088/4849 [=====>........................] - ETA: 5:51 - loss: 0.6650 - acc: 0.6085
1152/4849 [======>.......................] - ETA: 5:47 - loss: 0.6641 - acc: 0.6102
1216/4849 [======>.......................] - ETA: 5:41 - loss: 0.6653 - acc: 0.6061
1280/4849 [======>.......................] - ETA: 5:35 - loss: 0.6674 - acc: 0.6016
1344/4849 [=======>......................] - ETA: 5:30 - loss: 0.6667 - acc: 0.6019
1408/4849 [=======>......................] - ETA: 5:24 - loss: 0.6665 - acc: 0.6044
1472/4849 [========>.....................] - ETA: 5:18 - loss: 0.6675 - acc: 0.6060
1536/4849 [========>.....................] - ETA: 5:12 - loss: 0.6659 - acc: 0.6074
1600/4849 [========>.....................] - ETA: 5:06 - loss: 0.6668 - acc: 0.6050
1664/4849 [=========>....................] - ETA: 5:00 - loss: 0.6668 - acc: 0.6034
1728/4849 [=========>....................] - ETA: 4:54 - loss: 0.6649 - acc: 0.6065
1792/4849 [==========>...................] - ETA: 4:48 - loss: 0.6635 - acc: 0.6066
1856/4849 [==========>...................] - ETA: 4:42 - loss: 0.6625 - acc: 0.6078
1920/4849 [==========>...................] - ETA: 4:36 - loss: 0.6617 - acc: 0.6083
1984/4849 [===========>..................] - ETA: 4:30 - loss: 0.6628 - acc: 0.6064
2048/4849 [===========>..................] - ETA: 4:23 - loss: 0.6649 - acc: 0.6030
2112/4849 [============>.................] - ETA: 4:18 - loss: 0.6637 - acc: 0.6056
2176/4849 [============>.................] - ETA: 4:12 - loss: 0.6642 - acc: 0.6057
2240/4849 [============>.................] - ETA: 4:06 - loss: 0.6648 - acc: 0.6049
2304/4849 [=============>................] - ETA: 4:01 - loss: 0.6648 - acc: 0.6059
2368/4849 [=============>................] - ETA: 3:55 - loss: 0.6650 - acc: 0.6056
2432/4849 [==============>...............] - ETA: 3:48 - loss: 0.6640 - acc: 0.6086
2496/4849 [==============>...............] - ETA: 3:43 - loss: 0.6656 - acc: 0.6074
2560/4849 [==============>...............] - ETA: 3:36 - loss: 0.6663 - acc: 0.6066
2624/4849 [===============>..............] - ETA: 3:30 - loss: 0.6657 - acc: 0.6071
2688/4849 [===============>..............] - ETA: 3:25 - loss: 0.6650 - acc: 0.6094
2752/4849 [================>.............] - ETA: 3:18 - loss: 0.6659 - acc: 0.6072
2816/4849 [================>.............] - ETA: 3:12 - loss: 0.6653 - acc: 0.6083
2880/4849 [================>.............] - ETA: 3:07 - loss: 0.6656 - acc: 0.6066
2944/4849 [=================>............] - ETA: 3:01 - loss: 0.6669 - acc: 0.6050
3008/4849 [=================>............] - ETA: 2:55 - loss: 0.6670 - acc: 0.6047
3072/4849 [==================>...........] - ETA: 2:48 - loss: 0.6665 - acc: 0.6064
3136/4849 [==================>...........] - ETA: 2:42 - loss: 0.6664 - acc: 0.6052
3200/4849 [==================>...........] - ETA: 2:37 - loss: 0.6656 - acc: 0.6062
3264/4849 [===================>..........] - ETA: 2:31 - loss: 0.6654 - acc: 0.6051
3328/4849 [===================>..........] - ETA: 2:25 - loss: 0.6648 - acc: 0.6064
3392/4849 [===================>..........] - ETA: 2:18 - loss: 0.6650 - acc: 0.6055
3456/4849 [====================>.........] - ETA: 2:12 - loss: 0.6649 - acc: 0.6056
3520/4849 [====================>.........] - ETA: 2:06 - loss: 0.6650 - acc: 0.6051
3584/4849 [=====================>........] - ETA: 2:00 - loss: 0.6649 - acc: 0.6057
3648/4849 [=====================>........] - ETA: 1:54 - loss: 0.6648 - acc: 0.6055
3712/4849 [=====================>........] - ETA: 1:48 - loss: 0.6648 - acc: 0.6051
3776/4849 [======================>.......] - ETA: 1:42 - loss: 0.6648 - acc: 0.6049
3840/4849 [======================>.......] - ETA: 1:36 - loss: 0.6650 - acc: 0.6052
3904/4849 [=======================>......] - ETA: 1:30 - loss: 0.6656 - acc: 0.6048
3968/4849 [=======================>......] - ETA: 1:23 - loss: 0.6662 - acc: 0.6041
4032/4849 [=======================>......] - ETA: 1:17 - loss: 0.6668 - acc: 0.6024
4096/4849 [========================>.....] - ETA: 1:11 - loss: 0.6669 - acc: 0.6016
4160/4849 [========================>.....] - ETA: 1:05 - loss: 0.6671 - acc: 0.6012
4224/4849 [=========================>....] - ETA: 59s - loss: 0.6664 - acc: 0.6025 
4288/4849 [=========================>....] - ETA: 53s - loss: 0.6667 - acc: 0.6024
4352/4849 [=========================>....] - ETA: 47s - loss: 0.6663 - acc: 0.6025
4416/4849 [==========================>...] - ETA: 41s - loss: 0.6663 - acc: 0.6024
4480/4849 [==========================>...] - ETA: 35s - loss: 0.6668 - acc: 0.6009
4544/4849 [===========================>..] - ETA: 28s - loss: 0.6669 - acc: 0.6004
4608/4849 [===========================>..] - ETA: 22s - loss: 0.6668 - acc: 0.6009
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6672 - acc: 0.6000
4736/4849 [============================>.] - ETA: 10s - loss: 0.6667 - acc: 0.6003
4800/4849 [============================>.] - ETA: 4s - loss: 0.6667 - acc: 0.5996 
4849/4849 [==============================] - 478s 99ms/step - loss: 0.6670 - acc: 0.5983 - val_loss: 0.6857 - val_acc: 0.5529

Epoch 00010: val_acc did not improve from 0.58071
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe6c8344e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe6c8344e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe6c831ffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe6c831ffd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c8321550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c8321550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6c0695bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6c0695bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2c476a290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2c476a290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c0695fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c0695fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6c0695d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6c0695d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2c427fad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2c427fad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6c0203050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6c0203050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6c0412990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6c0412990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c02147d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c02147d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6c0203250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6c0203250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2e4044c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2e4044c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe2a4636e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe2a4636e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6c00a2750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6c00a2750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c0200b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c0200b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6c02c5810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6c02c5810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c016b110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c016b110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6c0156f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6c0156f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6c0177690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6c0177690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b85e6b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b85e6b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b8689ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b8689ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b8409910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b8409910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6b8474c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6b8474c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6b8202ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6b8202ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b8562390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b8562390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b84743d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b84743d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b8275410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b8275410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6b80434d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6b80434d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6b076e650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6b076e650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b81ffa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b81ffa10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b80f8ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b80f8ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b076e4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b076e4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6b0491a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6b0491a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6b048fa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6b048fa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b0748e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b0748e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b0491e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b0491e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b0384fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b0384fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6b0182290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6b0182290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6b012c4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6b012c4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b0782110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b0782110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b0182750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b0182750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b06d6990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b06d6990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6b0057410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6b0057410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6b017d650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6b017d650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b0106e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b0106e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b0057350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6b0057350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b009ecd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b009ecd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6a83ba390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6a83ba390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6a81dee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6a81dee10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a83baa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a83baa10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6a82bd8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6a82bd8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a8225e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a8225e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6a0794110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6a0794110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6a81d8650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6a81d8650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a0768950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a0768950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6a8572cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6a8572cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a0717f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a0717f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6a068aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6a068aa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6a042b410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6a042b410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a0443890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a0443890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6a822f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe6a822f5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a035b550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6a035b550>>: AttributeError: module 'gast' has no attribute 'Str'
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 4:17
 128/1348 [=>............................] - ETA: 2:21
 192/1348 [===>..........................] - ETA: 1:41
 256/1348 [====>.........................] - ETA: 1:21
 320/1348 [======>.......................] - ETA: 1:08
 384/1348 [=======>......................] - ETA: 58s 
 448/1348 [========>.....................] - ETA: 51s
 512/1348 [==========>...................] - ETA: 45s
 576/1348 [===========>..................] - ETA: 39s
 640/1348 [=============>................] - ETA: 34s
 704/1348 [==============>...............] - ETA: 30s
 768/1348 [================>.............] - ETA: 27s
 832/1348 [=================>............] - ETA: 23s
 896/1348 [==================>...........] - ETA: 20s
 960/1348 [====================>.........] - ETA: 17s
1024/1348 [=====================>........] - ETA: 13s
1088/1348 [=======================>......] - ETA: 10s
1152/1348 [========================>.....] - ETA: 8s 
1216/1348 [==========================>...] - ETA: 5s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 55s 41ms/step
loss: 0.6693030861081992
acc: 0.6068249258160238
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe22c05d6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe22c05d6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe2207ccdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe2207ccdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c8321f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c8321f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3a4155390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3a4155390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0f0178210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0f0178210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f016cc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f016cc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3a41558d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3a41558d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f016aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f016aed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6c82aff90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe6c82aff90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6c0787210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe6c0787210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f0178f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f0178f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe22068d110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe22068d110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2206d64d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2206d64d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe22048f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe22048f590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2204dae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2204dae50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b07d1250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b07d1250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe220557350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe220557350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe22047a110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe22047a110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3a42df7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3a42df7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2201864d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe2201864d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2201832d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2201832d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3f864b3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3f864b3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2200a2c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2200a2c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe220230b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe220230b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1e464c050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1e464c050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b0137150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6b0137150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe2201587d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe2201587d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1e446b190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1e446b190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1e4382e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1e4382e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1e4229ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1e4229ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1e4478910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1e4478910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1e4553c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1e4553c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1e4501a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1e4501a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1e425c990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1e425c990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1d468cb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1d468cb50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d467e510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d467e510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1e425c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1e425c390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d47aaa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d47aaa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1d46c75d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1d46c75d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1d4577610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1d4577610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d457a4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d457a4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1d4583b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1d4583b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d43e3ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d43e3ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1d4288590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1d4288590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1d411aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1d411aa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d41e9390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d41e9390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1d4681250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1d4681250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d4081790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1d4081790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0e87030d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0e87030d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1d40b8890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1d40b8890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0e86c7c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0e86c7c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0e8703bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0e8703bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0e8669950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0e8669950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0e86e9190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0e86e9190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0e82fccd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0e82fccd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0e8468050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0e8468050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0e8306190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0e8306190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0e830b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0e830b8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0e8223c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0e8223c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0e80b2310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0e80b2310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0e81bc850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0e81bc850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0e838d550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0e838d550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2201d3790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe2201d3790>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 31:21 - loss: 0.7637 - acc: 0.4375
 128/4849 [..............................] - ETA: 18:47 - loss: 0.7614 - acc: 0.4453
 192/4849 [>.............................] - ETA: 14:44 - loss: 0.7695 - acc: 0.4375
 256/4849 [>.............................] - ETA: 12:46 - loss: 0.7613 - acc: 0.4336
 320/4849 [>.............................] - ETA: 11:32 - loss: 0.7572 - acc: 0.4406
 384/4849 [=>............................] - ETA: 10:36 - loss: 0.7539 - acc: 0.4635
 448/4849 [=>............................] - ETA: 9:58 - loss: 0.7476 - acc: 0.4777 
 512/4849 [==>...........................] - ETA: 9:28 - loss: 0.7513 - acc: 0.4805
 576/4849 [==>...........................] - ETA: 9:02 - loss: 0.7526 - acc: 0.4757
 640/4849 [==>...........................] - ETA: 8:41 - loss: 0.7502 - acc: 0.4797
 704/4849 [===>..........................] - ETA: 8:22 - loss: 0.7470 - acc: 0.4801
 768/4849 [===>..........................] - ETA: 8:07 - loss: 0.7430 - acc: 0.4870
 832/4849 [====>.........................] - ETA: 7:54 - loss: 0.7477 - acc: 0.4856
 896/4849 [====>.........................] - ETA: 7:41 - loss: 0.7469 - acc: 0.4877
 960/4849 [====>.........................] - ETA: 7:28 - loss: 0.7455 - acc: 0.4844
1024/4849 [=====>........................] - ETA: 7:15 - loss: 0.7421 - acc: 0.4922
1088/4849 [=====>........................] - ETA: 7:04 - loss: 0.7396 - acc: 0.4936
1152/4849 [======>.......................] - ETA: 6:56 - loss: 0.7347 - acc: 0.4991
1216/4849 [======>.......................] - ETA: 6:45 - loss: 0.7362 - acc: 0.4959
1280/4849 [======>.......................] - ETA: 6:35 - loss: 0.7329 - acc: 0.5031
1344/4849 [=======>......................] - ETA: 6:25 - loss: 0.7290 - acc: 0.5052
1408/4849 [=======>......................] - ETA: 6:17 - loss: 0.7285 - acc: 0.5036
1472/4849 [========>.....................] - ETA: 6:08 - loss: 0.7250 - acc: 0.5061
1536/4849 [========>.....................] - ETA: 6:00 - loss: 0.7227 - acc: 0.5117
1600/4849 [========>.....................] - ETA: 5:51 - loss: 0.7252 - acc: 0.5069
1664/4849 [=========>....................] - ETA: 5:44 - loss: 0.7251 - acc: 0.5096
1728/4849 [=========>....................] - ETA: 5:36 - loss: 0.7259 - acc: 0.5087
1792/4849 [==========>...................] - ETA: 5:28 - loss: 0.7242 - acc: 0.5123
1856/4849 [==========>...................] - ETA: 5:20 - loss: 0.7231 - acc: 0.5119
1920/4849 [==========>...................] - ETA: 5:13 - loss: 0.7237 - acc: 0.5120
1984/4849 [===========>..................] - ETA: 5:05 - loss: 0.7226 - acc: 0.5116
2048/4849 [===========>..................] - ETA: 4:57 - loss: 0.7242 - acc: 0.5093
2112/4849 [============>.................] - ETA: 4:50 - loss: 0.7233 - acc: 0.5109
2176/4849 [============>.................] - ETA: 4:42 - loss: 0.7233 - acc: 0.5110
2240/4849 [============>.................] - ETA: 4:35 - loss: 0.7228 - acc: 0.5107
2304/4849 [=============>................] - ETA: 4:28 - loss: 0.7223 - acc: 0.5095
2368/4849 [=============>................] - ETA: 4:21 - loss: 0.7231 - acc: 0.5080
2432/4849 [==============>...............] - ETA: 4:14 - loss: 0.7224 - acc: 0.5082
2496/4849 [==============>...............] - ETA: 4:07 - loss: 0.7216 - acc: 0.5084
2560/4849 [==============>...............] - ETA: 4:00 - loss: 0.7211 - acc: 0.5094
2624/4849 [===============>..............] - ETA: 3:53 - loss: 0.7194 - acc: 0.5103
2688/4849 [===============>..............] - ETA: 3:46 - loss: 0.7196 - acc: 0.5100
2752/4849 [================>.............] - ETA: 3:38 - loss: 0.7189 - acc: 0.5109
2816/4849 [================>.............] - ETA: 3:31 - loss: 0.7186 - acc: 0.5107
2880/4849 [================>.............] - ETA: 3:24 - loss: 0.7185 - acc: 0.5115
2944/4849 [=================>............] - ETA: 3:18 - loss: 0.7209 - acc: 0.5088
3008/4849 [=================>............] - ETA: 3:11 - loss: 0.7207 - acc: 0.5103
3072/4849 [==================>...........] - ETA: 3:04 - loss: 0.7201 - acc: 0.5120
3136/4849 [==================>...........] - ETA: 2:57 - loss: 0.7194 - acc: 0.5131
3200/4849 [==================>...........] - ETA: 2:50 - loss: 0.7190 - acc: 0.5131
3264/4849 [===================>..........] - ETA: 2:44 - loss: 0.7186 - acc: 0.5132
3328/4849 [===================>..........] - ETA: 2:37 - loss: 0.7185 - acc: 0.5138
3392/4849 [===================>..........] - ETA: 2:30 - loss: 0.7197 - acc: 0.5100
3456/4849 [====================>.........] - ETA: 2:24 - loss: 0.7188 - acc: 0.5107
3520/4849 [====================>.........] - ETA: 2:17 - loss: 0.7185 - acc: 0.5108
3584/4849 [=====================>........] - ETA: 2:10 - loss: 0.7178 - acc: 0.5126
3648/4849 [=====================>........] - ETA: 2:03 - loss: 0.7175 - acc: 0.5112
3712/4849 [=====================>........] - ETA: 1:57 - loss: 0.7171 - acc: 0.5110
3776/4849 [======================>.......] - ETA: 1:50 - loss: 0.7165 - acc: 0.5130
3840/4849 [======================>.......] - ETA: 1:44 - loss: 0.7162 - acc: 0.5128
3904/4849 [=======================>......] - ETA: 1:37 - loss: 0.7162 - acc: 0.5123
3968/4849 [=======================>......] - ETA: 1:30 - loss: 0.7156 - acc: 0.5129
4032/4849 [=======================>......] - ETA: 1:24 - loss: 0.7154 - acc: 0.5131
4096/4849 [========================>.....] - ETA: 1:17 - loss: 0.7155 - acc: 0.5127
4160/4849 [========================>.....] - ETA: 1:10 - loss: 0.7151 - acc: 0.5137
4224/4849 [=========================>....] - ETA: 1:04 - loss: 0.7152 - acc: 0.5130
4288/4849 [=========================>....] - ETA: 57s - loss: 0.7148 - acc: 0.5126 
4352/4849 [=========================>....] - ETA: 51s - loss: 0.7151 - acc: 0.5110
4416/4849 [==========================>...] - ETA: 44s - loss: 0.7149 - acc: 0.5118
4480/4849 [==========================>...] - ETA: 37s - loss: 0.7147 - acc: 0.5123
4544/4849 [===========================>..] - ETA: 31s - loss: 0.7138 - acc: 0.5143
4608/4849 [===========================>..] - ETA: 24s - loss: 0.7134 - acc: 0.5150
4672/4849 [===========================>..] - ETA: 18s - loss: 0.7133 - acc: 0.5148
4736/4849 [============================>.] - ETA: 11s - loss: 0.7128 - acc: 0.5152
4800/4849 [============================>.] - ETA: 5s - loss: 0.7123 - acc: 0.5156 
4849/4849 [==============================] - 523s 108ms/step - loss: 0.7124 - acc: 0.5158 - val_loss: 0.6814 - val_acc: 0.5547

Epoch 00001: val_acc improved from -inf to 0.55473, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window09/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 8:01 - loss: 0.7007 - acc: 0.5156
 128/4849 [..............................] - ETA: 8:07 - loss: 0.6976 - acc: 0.5312
 192/4849 [>.............................] - ETA: 8:00 - loss: 0.6924 - acc: 0.5469
 256/4849 [>.............................] - ETA: 7:55 - loss: 0.6968 - acc: 0.5469
 320/4849 [>.............................] - ETA: 7:48 - loss: 0.6972 - acc: 0.5437
 384/4849 [=>............................] - ETA: 7:43 - loss: 0.6956 - acc: 0.5495
 448/4849 [=>............................] - ETA: 7:32 - loss: 0.6990 - acc: 0.5379
 512/4849 [==>...........................] - ETA: 7:27 - loss: 0.6966 - acc: 0.5391
 576/4849 [==>...........................] - ETA: 7:22 - loss: 0.6967 - acc: 0.5451
 640/4849 [==>...........................] - ETA: 7:14 - loss: 0.6971 - acc: 0.5531
 704/4849 [===>..........................] - ETA: 7:08 - loss: 0.6943 - acc: 0.5540
 768/4849 [===>..........................] - ETA: 7:00 - loss: 0.6954 - acc: 0.5469
 832/4849 [====>.........................] - ETA: 6:52 - loss: 0.6942 - acc: 0.5445
 896/4849 [====>.........................] - ETA: 6:45 - loss: 0.6968 - acc: 0.5435
 960/4849 [====>.........................] - ETA: 6:39 - loss: 0.6966 - acc: 0.5417
1024/4849 [=====>........................] - ETA: 6:32 - loss: 0.6978 - acc: 0.5420
1088/4849 [=====>........................] - ETA: 6:25 - loss: 0.6980 - acc: 0.5368
1152/4849 [======>.......................] - ETA: 6:19 - loss: 0.6984 - acc: 0.5356
1216/4849 [======>.......................] - ETA: 6:12 - loss: 0.6998 - acc: 0.5337
1280/4849 [======>.......................] - ETA: 6:05 - loss: 0.7013 - acc: 0.5289
1344/4849 [=======>......................] - ETA: 5:59 - loss: 0.7008 - acc: 0.5283
1408/4849 [=======>......................] - ETA: 5:52 - loss: 0.7009 - acc: 0.5249
1472/4849 [========>.....................] - ETA: 5:46 - loss: 0.7023 - acc: 0.5217
1536/4849 [========>.....................] - ETA: 5:39 - loss: 0.7034 - acc: 0.5182
1600/4849 [========>.....................] - ETA: 5:33 - loss: 0.7030 - acc: 0.5212
1664/4849 [=========>....................] - ETA: 5:27 - loss: 0.7026 - acc: 0.5228
1728/4849 [=========>....................] - ETA: 5:20 - loss: 0.7037 - acc: 0.5208
1792/4849 [==========>...................] - ETA: 5:14 - loss: 0.7021 - acc: 0.5229
1856/4849 [==========>...................] - ETA: 5:08 - loss: 0.7022 - acc: 0.5216
1920/4849 [==========>...................] - ETA: 5:01 - loss: 0.7024 - acc: 0.5214
1984/4849 [===========>..................] - ETA: 4:54 - loss: 0.7031 - acc: 0.5202
2048/4849 [===========>..................] - ETA: 4:48 - loss: 0.7019 - acc: 0.5205
2112/4849 [============>.................] - ETA: 4:41 - loss: 0.7027 - acc: 0.5175
2176/4849 [============>.................] - ETA: 4:34 - loss: 0.7035 - acc: 0.5138
2240/4849 [============>.................] - ETA: 4:28 - loss: 0.7031 - acc: 0.5134
2304/4849 [=============>................] - ETA: 4:21 - loss: 0.7020 - acc: 0.5161
2368/4849 [=============>................] - ETA: 4:14 - loss: 0.7014 - acc: 0.5169
2432/4849 [==============>...............] - ETA: 4:07 - loss: 0.7006 - acc: 0.5173
2496/4849 [==============>...............] - ETA: 4:01 - loss: 0.6999 - acc: 0.5172
2560/4849 [==============>...............] - ETA: 3:54 - loss: 0.6998 - acc: 0.5180
2624/4849 [===============>..............] - ETA: 3:48 - loss: 0.7009 - acc: 0.5171
2688/4849 [===============>..............] - ETA: 3:41 - loss: 0.6999 - acc: 0.5208
2752/4849 [================>.............] - ETA: 3:35 - loss: 0.7004 - acc: 0.5189
2816/4849 [================>.............] - ETA: 3:28 - loss: 0.7007 - acc: 0.5188
2880/4849 [================>.............] - ETA: 3:21 - loss: 0.7001 - acc: 0.5194
2944/4849 [=================>............] - ETA: 3:15 - loss: 0.6991 - acc: 0.5211
3008/4849 [=================>............] - ETA: 3:09 - loss: 0.6980 - acc: 0.5246
3072/4849 [==================>...........] - ETA: 3:02 - loss: 0.6982 - acc: 0.5228
3136/4849 [==================>...........] - ETA: 2:56 - loss: 0.6977 - acc: 0.5252
3200/4849 [==================>...........] - ETA: 2:49 - loss: 0.6975 - acc: 0.5253
3264/4849 [===================>..........] - ETA: 2:43 - loss: 0.6967 - acc: 0.5282
3328/4849 [===================>..........] - ETA: 2:36 - loss: 0.6966 - acc: 0.5294
3392/4849 [===================>..........] - ETA: 2:29 - loss: 0.6961 - acc: 0.5318
3456/4849 [====================>.........] - ETA: 2:23 - loss: 0.6959 - acc: 0.5318
3520/4849 [====================>.........] - ETA: 2:16 - loss: 0.6957 - acc: 0.5318
3584/4849 [=====================>........] - ETA: 2:10 - loss: 0.6955 - acc: 0.5318
3648/4849 [=====================>........] - ETA: 2:03 - loss: 0.6958 - acc: 0.5312
3712/4849 [=====================>........] - ETA: 1:56 - loss: 0.6962 - acc: 0.5299
3776/4849 [======================>.......] - ETA: 1:50 - loss: 0.6954 - acc: 0.5323
3840/4849 [======================>.......] - ETA: 1:43 - loss: 0.6947 - acc: 0.5341
3904/4849 [=======================>......] - ETA: 1:37 - loss: 0.6942 - acc: 0.5353
3968/4849 [=======================>......] - ETA: 1:30 - loss: 0.6943 - acc: 0.5355
4032/4849 [=======================>......] - ETA: 1:24 - loss: 0.6944 - acc: 0.5365
4096/4849 [========================>.....] - ETA: 1:17 - loss: 0.6941 - acc: 0.5371
4160/4849 [========================>.....] - ETA: 1:10 - loss: 0.6937 - acc: 0.5382
4224/4849 [=========================>....] - ETA: 1:04 - loss: 0.6932 - acc: 0.5386
4288/4849 [=========================>....] - ETA: 57s - loss: 0.6930 - acc: 0.5394 
4352/4849 [=========================>....] - ETA: 51s - loss: 0.6924 - acc: 0.5407
4416/4849 [==========================>...] - ETA: 44s - loss: 0.6919 - acc: 0.5414
4480/4849 [==========================>...] - ETA: 37s - loss: 0.6924 - acc: 0.5411
4544/4849 [===========================>..] - ETA: 31s - loss: 0.6923 - acc: 0.5414
4608/4849 [===========================>..] - ETA: 24s - loss: 0.6920 - acc: 0.5414
4672/4849 [===========================>..] - ETA: 18s - loss: 0.6918 - acc: 0.5411
4736/4849 [============================>.] - ETA: 11s - loss: 0.6912 - acc: 0.5427
4800/4849 [============================>.] - ETA: 5s - loss: 0.6911 - acc: 0.5427 
4849/4849 [==============================] - 514s 106ms/step - loss: 0.6913 - acc: 0.5424 - val_loss: 0.6782 - val_acc: 0.5696

Epoch 00002: val_acc improved from 0.55473 to 0.56957, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window09/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 3/10

  64/4849 [..............................] - ETA: 7:38 - loss: 0.7157 - acc: 0.5312
 128/4849 [..............................] - ETA: 7:26 - loss: 0.7136 - acc: 0.5312
 192/4849 [>.............................] - ETA: 7:20 - loss: 0.7069 - acc: 0.5260
 256/4849 [>.............................] - ETA: 7:14 - loss: 0.7016 - acc: 0.5352
 320/4849 [>.............................] - ETA: 7:11 - loss: 0.6976 - acc: 0.5437
 384/4849 [=>............................] - ETA: 7:00 - loss: 0.6962 - acc: 0.5391
 448/4849 [=>............................] - ETA: 6:53 - loss: 0.6937 - acc: 0.5446
 512/4849 [==>...........................] - ETA: 6:47 - loss: 0.6946 - acc: 0.5449
 576/4849 [==>...........................] - ETA: 6:40 - loss: 0.6901 - acc: 0.5486
 640/4849 [==>...........................] - ETA: 6:35 - loss: 0.6911 - acc: 0.5500
 704/4849 [===>..........................] - ETA: 6:28 - loss: 0.6887 - acc: 0.5469
 768/4849 [===>..........................] - ETA: 6:21 - loss: 0.6880 - acc: 0.5469
 832/4849 [====>.........................] - ETA: 6:15 - loss: 0.6862 - acc: 0.5565
 896/4849 [====>.........................] - ETA: 6:08 - loss: 0.6877 - acc: 0.5502
 960/4849 [====>.........................] - ETA: 6:03 - loss: 0.6872 - acc: 0.5479
1024/4849 [=====>........................] - ETA: 5:58 - loss: 0.6869 - acc: 0.5479
1088/4849 [=====>........................] - ETA: 5:51 - loss: 0.6868 - acc: 0.5515
1152/4849 [======>.......................] - ETA: 5:45 - loss: 0.6873 - acc: 0.5512
1216/4849 [======>.......................] - ETA: 5:38 - loss: 0.6893 - acc: 0.5477
1280/4849 [======>.......................] - ETA: 5:33 - loss: 0.6889 - acc: 0.5484
1344/4849 [=======>......................] - ETA: 5:28 - loss: 0.6911 - acc: 0.5432
1408/4849 [=======>......................] - ETA: 5:22 - loss: 0.6910 - acc: 0.5455
1472/4849 [========>.....................] - ETA: 5:16 - loss: 0.6916 - acc: 0.5469
1536/4849 [========>.....................] - ETA: 5:10 - loss: 0.6927 - acc: 0.5443
1600/4849 [========>.....................] - ETA: 5:04 - loss: 0.6920 - acc: 0.5481
1664/4849 [=========>....................] - ETA: 4:58 - loss: 0.6930 - acc: 0.5469
1728/4849 [=========>....................] - ETA: 4:52 - loss: 0.6944 - acc: 0.5457
1792/4849 [==========>...................] - ETA: 4:47 - loss: 0.6942 - acc: 0.5435
1856/4849 [==========>...................] - ETA: 4:41 - loss: 0.6945 - acc: 0.5431
1920/4849 [==========>...................] - ETA: 4:35 - loss: 0.6955 - acc: 0.5427
1984/4849 [===========>..................] - ETA: 4:28 - loss: 0.6952 - acc: 0.5439
2048/4849 [===========>..................] - ETA: 4:21 - loss: 0.6944 - acc: 0.5449
2112/4849 [============>.................] - ETA: 4:14 - loss: 0.6936 - acc: 0.5469
2176/4849 [============>.................] - ETA: 4:08 - loss: 0.6943 - acc: 0.5441
2240/4849 [============>.................] - ETA: 4:02 - loss: 0.6943 - acc: 0.5446
2304/4849 [=============>................] - ETA: 3:55 - loss: 0.6951 - acc: 0.5425
2368/4849 [=============>................] - ETA: 3:49 - loss: 0.6950 - acc: 0.5422
2432/4849 [==============>...............] - ETA: 3:43 - loss: 0.6948 - acc: 0.5424
2496/4849 [==============>...............] - ETA: 3:36 - loss: 0.6953 - acc: 0.5405
2560/4849 [==============>...............] - ETA: 3:30 - loss: 0.6956 - acc: 0.5395
2624/4849 [===============>..............] - ETA: 3:24 - loss: 0.6953 - acc: 0.5393
2688/4849 [===============>..............] - ETA: 3:18 - loss: 0.6950 - acc: 0.5406
2752/4849 [================>.............] - ETA: 3:11 - loss: 0.6954 - acc: 0.5403
2816/4849 [================>.............] - ETA: 3:05 - loss: 0.6953 - acc: 0.5412
2880/4849 [================>.............] - ETA: 2:59 - loss: 0.6958 - acc: 0.5385
2944/4849 [=================>............] - ETA: 2:53 - loss: 0.6953 - acc: 0.5397
3008/4849 [=================>............] - ETA: 2:47 - loss: 0.6950 - acc: 0.5406
3072/4849 [==================>...........] - ETA: 2:41 - loss: 0.6949 - acc: 0.5404
3136/4849 [==================>...........] - ETA: 2:36 - loss: 0.6950 - acc: 0.5395
3200/4849 [==================>...........] - ETA: 2:30 - loss: 0.6951 - acc: 0.5394
3264/4849 [===================>..........] - ETA: 2:24 - loss: 0.6948 - acc: 0.5386
3328/4849 [===================>..........] - ETA: 2:18 - loss: 0.6954 - acc: 0.5379
3392/4849 [===================>..........] - ETA: 2:12 - loss: 0.6964 - acc: 0.5357
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.6951 - acc: 0.5376
3520/4849 [====================>.........] - ETA: 2:00 - loss: 0.6950 - acc: 0.5378
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6947 - acc: 0.5391
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.6945 - acc: 0.5397
3712/4849 [=====================>........] - ETA: 1:43 - loss: 0.6945 - acc: 0.5396
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6949 - acc: 0.5387
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6944 - acc: 0.5396
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6944 - acc: 0.5392
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6941 - acc: 0.5401
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6939 - acc: 0.5404
4096/4849 [========================>.....] - ETA: 1:08 - loss: 0.6933 - acc: 0.5425
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6926 - acc: 0.5437
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6921 - acc: 0.5445 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6919 - acc: 0.5445
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6915 - acc: 0.5450
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6914 - acc: 0.5457
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6916 - acc: 0.5446
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6911 - acc: 0.5460
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6902 - acc: 0.5480
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6899 - acc: 0.5484
4736/4849 [============================>.] - ETA: 10s - loss: 0.6899 - acc: 0.5479
4800/4849 [============================>.] - ETA: 4s - loss: 0.6901 - acc: 0.5477 
4849/4849 [==============================] - 452s 93ms/step - loss: 0.6898 - acc: 0.5484 - val_loss: 0.6821 - val_acc: 0.5584

Epoch 00003: val_acc did not improve from 0.56957
Epoch 4/10

  64/4849 [..............................] - ETA: 6:55 - loss: 0.7163 - acc: 0.5000
 128/4849 [..............................] - ETA: 6:59 - loss: 0.7187 - acc: 0.5078
 192/4849 [>.............................] - ETA: 6:58 - loss: 0.7046 - acc: 0.5573
 256/4849 [>.............................] - ETA: 6:55 - loss: 0.6998 - acc: 0.5664
 320/4849 [>.............................] - ETA: 6:49 - loss: 0.6890 - acc: 0.5844
 384/4849 [=>............................] - ETA: 6:39 - loss: 0.6858 - acc: 0.5938
 448/4849 [=>............................] - ETA: 6:32 - loss: 0.6900 - acc: 0.5714
 512/4849 [==>...........................] - ETA: 6:26 - loss: 0.6865 - acc: 0.5664
 576/4849 [==>...........................] - ETA: 6:22 - loss: 0.6922 - acc: 0.5625
 640/4849 [==>...........................] - ETA: 6:17 - loss: 0.6855 - acc: 0.5734
 704/4849 [===>..........................] - ETA: 6:11 - loss: 0.6897 - acc: 0.5625
 768/4849 [===>..........................] - ETA: 6:06 - loss: 0.6941 - acc: 0.5534
 832/4849 [====>.........................] - ETA: 6:00 - loss: 0.6903 - acc: 0.5661
 896/4849 [====>.........................] - ETA: 5:55 - loss: 0.6863 - acc: 0.5714
 960/4849 [====>.........................] - ETA: 5:50 - loss: 0.6862 - acc: 0.5729
1024/4849 [=====>........................] - ETA: 5:46 - loss: 0.6835 - acc: 0.5801
1088/4849 [=====>........................] - ETA: 5:40 - loss: 0.6803 - acc: 0.5882
1152/4849 [======>.......................] - ETA: 5:35 - loss: 0.6793 - acc: 0.5894
1216/4849 [======>.......................] - ETA: 5:29 - loss: 0.6807 - acc: 0.5896
1280/4849 [======>.......................] - ETA: 5:23 - loss: 0.6802 - acc: 0.5898
1344/4849 [=======>......................] - ETA: 5:18 - loss: 0.6799 - acc: 0.5871
1408/4849 [=======>......................] - ETA: 5:12 - loss: 0.6784 - acc: 0.5895
1472/4849 [========>.....................] - ETA: 5:06 - loss: 0.6784 - acc: 0.5890
1536/4849 [========>.....................] - ETA: 5:00 - loss: 0.6776 - acc: 0.5892
1600/4849 [========>.....................] - ETA: 4:54 - loss: 0.6794 - acc: 0.5881
1664/4849 [=========>....................] - ETA: 4:48 - loss: 0.6790 - acc: 0.5889
1728/4849 [=========>....................] - ETA: 4:43 - loss: 0.6777 - acc: 0.5932
1792/4849 [==========>...................] - ETA: 4:38 - loss: 0.6791 - acc: 0.5898
1856/4849 [==========>...................] - ETA: 4:31 - loss: 0.6782 - acc: 0.5900
1920/4849 [==========>...................] - ETA: 4:26 - loss: 0.6781 - acc: 0.5885
1984/4849 [===========>..................] - ETA: 4:20 - loss: 0.6780 - acc: 0.5882
2048/4849 [===========>..................] - ETA: 4:14 - loss: 0.6788 - acc: 0.5850
2112/4849 [============>.................] - ETA: 4:08 - loss: 0.6793 - acc: 0.5838
2176/4849 [============>.................] - ETA: 4:03 - loss: 0.6789 - acc: 0.5855
2240/4849 [============>.................] - ETA: 3:57 - loss: 0.6795 - acc: 0.5857
2304/4849 [=============>................] - ETA: 3:52 - loss: 0.6803 - acc: 0.5838
2368/4849 [=============>................] - ETA: 3:46 - loss: 0.6805 - acc: 0.5836
2432/4849 [==============>...............] - ETA: 3:40 - loss: 0.6796 - acc: 0.5843
2496/4849 [==============>...............] - ETA: 3:34 - loss: 0.6803 - acc: 0.5825
2560/4849 [==============>...............] - ETA: 3:28 - loss: 0.6799 - acc: 0.5836
2624/4849 [===============>..............] - ETA: 3:22 - loss: 0.6801 - acc: 0.5831
2688/4849 [===============>..............] - ETA: 3:17 - loss: 0.6806 - acc: 0.5841
2752/4849 [================>.............] - ETA: 3:11 - loss: 0.6811 - acc: 0.5828
2816/4849 [================>.............] - ETA: 3:05 - loss: 0.6818 - acc: 0.5824
2880/4849 [================>.............] - ETA: 2:59 - loss: 0.6819 - acc: 0.5819
2944/4849 [=================>............] - ETA: 2:53 - loss: 0.6815 - acc: 0.5822
3008/4849 [=================>............] - ETA: 2:48 - loss: 0.6812 - acc: 0.5821
3072/4849 [==================>...........] - ETA: 2:42 - loss: 0.6810 - acc: 0.5827
3136/4849 [==================>...........] - ETA: 2:36 - loss: 0.6806 - acc: 0.5832
3200/4849 [==================>...........] - ETA: 2:31 - loss: 0.6811 - acc: 0.5825
3264/4849 [===================>..........] - ETA: 2:25 - loss: 0.6805 - acc: 0.5830
3328/4849 [===================>..........] - ETA: 2:19 - loss: 0.6795 - acc: 0.5847
3392/4849 [===================>..........] - ETA: 2:13 - loss: 0.6799 - acc: 0.5843
3456/4849 [====================>.........] - ETA: 2:07 - loss: 0.6810 - acc: 0.5825
3520/4849 [====================>.........] - ETA: 2:01 - loss: 0.6810 - acc: 0.5815
3584/4849 [=====================>........] - ETA: 1:56 - loss: 0.6817 - acc: 0.5790
3648/4849 [=====================>........] - ETA: 1:50 - loss: 0.6816 - acc: 0.5789
3712/4849 [=====================>........] - ETA: 1:44 - loss: 0.6823 - acc: 0.5773
3776/4849 [======================>.......] - ETA: 1:38 - loss: 0.6825 - acc: 0.5776
3840/4849 [======================>.......] - ETA: 1:32 - loss: 0.6824 - acc: 0.5776
3904/4849 [=======================>......] - ETA: 1:26 - loss: 0.6824 - acc: 0.5774
3968/4849 [=======================>......] - ETA: 1:20 - loss: 0.6823 - acc: 0.5776
4032/4849 [=======================>......] - ETA: 1:15 - loss: 0.6821 - acc: 0.5776
4096/4849 [========================>.....] - ETA: 1:09 - loss: 0.6824 - acc: 0.5767
4160/4849 [========================>.....] - ETA: 1:03 - loss: 0.6822 - acc: 0.5774
4224/4849 [=========================>....] - ETA: 57s - loss: 0.6820 - acc: 0.5784 
4288/4849 [=========================>....] - ETA: 51s - loss: 0.6818 - acc: 0.5781
4352/4849 [=========================>....] - ETA: 45s - loss: 0.6821 - acc: 0.5774
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6817 - acc: 0.5777
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6815 - acc: 0.5777
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6815 - acc: 0.5775
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6810 - acc: 0.5779
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6811 - acc: 0.5775
4736/4849 [============================>.] - ETA: 10s - loss: 0.6810 - acc: 0.5777
4800/4849 [============================>.] - ETA: 4s - loss: 0.6809 - acc: 0.5779 
4849/4849 [==============================] - 455s 94ms/step - loss: 0.6810 - acc: 0.5779 - val_loss: 0.6931 - val_acc: 0.5659

Epoch 00004: val_acc did not improve from 0.56957
Epoch 5/10

  64/4849 [..............................] - ETA: 6:03 - loss: 0.6942 - acc: 0.4844
 128/4849 [..............................] - ETA: 5:58 - loss: 0.6945 - acc: 0.5078
 192/4849 [>.............................] - ETA: 6:05 - loss: 0.6852 - acc: 0.5052
 256/4849 [>.............................] - ETA: 5:58 - loss: 0.7005 - acc: 0.4844
 320/4849 [>.............................] - ETA: 5:50 - loss: 0.6965 - acc: 0.5094
 384/4849 [=>............................] - ETA: 5:43 - loss: 0.6992 - acc: 0.5078
 448/4849 [=>............................] - ETA: 5:39 - loss: 0.7056 - acc: 0.5000
 512/4849 [==>...........................] - ETA: 5:35 - loss: 0.7029 - acc: 0.4980
 576/4849 [==>...........................] - ETA: 5:30 - loss: 0.7057 - acc: 0.4948
 640/4849 [==>...........................] - ETA: 5:28 - loss: 0.7020 - acc: 0.5109
 704/4849 [===>..........................] - ETA: 5:22 - loss: 0.7015 - acc: 0.5099
 768/4849 [===>..........................] - ETA: 5:16 - loss: 0.7015 - acc: 0.5078
 832/4849 [====>.........................] - ETA: 5:11 - loss: 0.7003 - acc: 0.5084
 896/4849 [====>.........................] - ETA: 5:05 - loss: 0.7001 - acc: 0.5134
 960/4849 [====>.........................] - ETA: 5:02 - loss: 0.6995 - acc: 0.5177
1024/4849 [=====>........................] - ETA: 4:56 - loss: 0.6987 - acc: 0.5195
1088/4849 [=====>........................] - ETA: 4:52 - loss: 0.6988 - acc: 0.5202
1152/4849 [======>.......................] - ETA: 4:45 - loss: 0.6975 - acc: 0.5217
1216/4849 [======>.......................] - ETA: 4:40 - loss: 0.6963 - acc: 0.5263
1280/4849 [======>.......................] - ETA: 4:34 - loss: 0.6950 - acc: 0.5273
1344/4849 [=======>......................] - ETA: 4:30 - loss: 0.6941 - acc: 0.5275
1408/4849 [=======>......................] - ETA: 4:25 - loss: 0.6944 - acc: 0.5277
1472/4849 [========>.....................] - ETA: 4:19 - loss: 0.6953 - acc: 0.5245
1536/4849 [========>.....................] - ETA: 4:14 - loss: 0.6948 - acc: 0.5299
1600/4849 [========>.....................] - ETA: 4:10 - loss: 0.6951 - acc: 0.5288
1664/4849 [=========>....................] - ETA: 4:04 - loss: 0.6953 - acc: 0.5276
1728/4849 [=========>....................] - ETA: 4:00 - loss: 0.6956 - acc: 0.5278
1792/4849 [==========>...................] - ETA: 3:54 - loss: 0.6959 - acc: 0.5273
1856/4849 [==========>...................] - ETA: 3:48 - loss: 0.6952 - acc: 0.5296
1920/4849 [==========>...................] - ETA: 3:43 - loss: 0.6947 - acc: 0.5312
1984/4849 [===========>..................] - ETA: 3:38 - loss: 0.6944 - acc: 0.5328
2048/4849 [===========>..................] - ETA: 3:32 - loss: 0.6930 - acc: 0.5391
2112/4849 [============>.................] - ETA: 3:27 - loss: 0.6928 - acc: 0.5407
2176/4849 [============>.................] - ETA: 3:22 - loss: 0.6928 - acc: 0.5391
2240/4849 [============>.................] - ETA: 3:16 - loss: 0.6923 - acc: 0.5402
2304/4849 [=============>................] - ETA: 3:11 - loss: 0.6918 - acc: 0.5399
2368/4849 [=============>................] - ETA: 3:06 - loss: 0.6908 - acc: 0.5443
2432/4849 [==============>...............] - ETA: 3:01 - loss: 0.6904 - acc: 0.5440
2496/4849 [==============>...............] - ETA: 2:56 - loss: 0.6903 - acc: 0.5429
2560/4849 [==============>...............] - ETA: 2:50 - loss: 0.6906 - acc: 0.5426
2624/4849 [===============>..............] - ETA: 2:45 - loss: 0.6911 - acc: 0.5434
2688/4849 [===============>..............] - ETA: 2:40 - loss: 0.6904 - acc: 0.5450
2752/4849 [================>.............] - ETA: 2:35 - loss: 0.6908 - acc: 0.5454
2816/4849 [================>.............] - ETA: 2:30 - loss: 0.6911 - acc: 0.5462
2880/4849 [================>.............] - ETA: 2:26 - loss: 0.6909 - acc: 0.5472
2944/4849 [=================>............] - ETA: 2:20 - loss: 0.6913 - acc: 0.5476
3008/4849 [=================>............] - ETA: 2:16 - loss: 0.6907 - acc: 0.5482
3072/4849 [==================>...........] - ETA: 2:11 - loss: 0.6905 - acc: 0.5492
3136/4849 [==================>...........] - ETA: 2:06 - loss: 0.6899 - acc: 0.5507
3200/4849 [==================>...........] - ETA: 2:01 - loss: 0.6889 - acc: 0.5519
3264/4849 [===================>..........] - ETA: 1:56 - loss: 0.6890 - acc: 0.5530
3328/4849 [===================>..........] - ETA: 1:51 - loss: 0.6892 - acc: 0.5520
3392/4849 [===================>..........] - ETA: 1:46 - loss: 0.6892 - acc: 0.5525
3456/4849 [====================>.........] - ETA: 1:41 - loss: 0.6893 - acc: 0.5518
3520/4849 [====================>.........] - ETA: 1:37 - loss: 0.6888 - acc: 0.5528
3584/4849 [=====================>........] - ETA: 1:32 - loss: 0.6883 - acc: 0.5530
3648/4849 [=====================>........] - ETA: 1:27 - loss: 0.6883 - acc: 0.5537
3712/4849 [=====================>........] - ETA: 1:22 - loss: 0.6882 - acc: 0.5544
3776/4849 [======================>.......] - ETA: 1:18 - loss: 0.6879 - acc: 0.5556
3840/4849 [======================>.......] - ETA: 1:13 - loss: 0.6876 - acc: 0.5555
3904/4849 [=======================>......] - ETA: 1:08 - loss: 0.6870 - acc: 0.5566
3968/4849 [=======================>......] - ETA: 1:04 - loss: 0.6869 - acc: 0.5567
4032/4849 [=======================>......] - ETA: 59s - loss: 0.6861 - acc: 0.5585 
4096/4849 [========================>.....] - ETA: 54s - loss: 0.6858 - acc: 0.5576
4160/4849 [========================>.....] - ETA: 49s - loss: 0.6858 - acc: 0.5582
4224/4849 [=========================>....] - ETA: 44s - loss: 0.6854 - acc: 0.5587
4288/4849 [=========================>....] - ETA: 40s - loss: 0.6846 - acc: 0.5599
4352/4849 [=========================>....] - ETA: 35s - loss: 0.6850 - acc: 0.5597
4416/4849 [==========================>...] - ETA: 30s - loss: 0.6843 - acc: 0.5616
4480/4849 [==========================>...] - ETA: 26s - loss: 0.6838 - acc: 0.5629
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6833 - acc: 0.5636
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6829 - acc: 0.5638
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6820 - acc: 0.5659
4736/4849 [============================>.] - ETA: 7s - loss: 0.6827 - acc: 0.5642 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6823 - acc: 0.5644
4849/4849 [==============================] - 352s 73ms/step - loss: 0.6822 - acc: 0.5644 - val_loss: 0.6921 - val_acc: 0.5714

Epoch 00005: val_acc improved from 0.56957 to 0.57143, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window09/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 6/10

  64/4849 [..............................] - ETA: 4:04 - loss: 0.6858 - acc: 0.5781
 128/4849 [..............................] - ETA: 4:28 - loss: 0.6965 - acc: 0.5469
 192/4849 [>.............................] - ETA: 4:17 - loss: 0.6978 - acc: 0.5417
 256/4849 [>.............................] - ETA: 4:09 - loss: 0.6986 - acc: 0.5273
 320/4849 [>.............................] - ETA: 4:02 - loss: 0.6911 - acc: 0.5375
 384/4849 [=>............................] - ETA: 3:58 - loss: 0.6823 - acc: 0.5625
 448/4849 [=>............................] - ETA: 3:53 - loss: 0.6827 - acc: 0.5603
 512/4849 [==>...........................] - ETA: 3:50 - loss: 0.6889 - acc: 0.5586
 576/4849 [==>...........................] - ETA: 3:47 - loss: 0.6804 - acc: 0.5799
 640/4849 [==>...........................] - ETA: 3:44 - loss: 0.6760 - acc: 0.5797
 704/4849 [===>..........................] - ETA: 3:42 - loss: 0.6751 - acc: 0.5852
 768/4849 [===>..........................] - ETA: 3:40 - loss: 0.6720 - acc: 0.5924
 832/4849 [====>.........................] - ETA: 3:38 - loss: 0.6719 - acc: 0.5925
 896/4849 [====>.........................] - ETA: 3:35 - loss: 0.6710 - acc: 0.5960
 960/4849 [====>.........................] - ETA: 3:33 - loss: 0.6711 - acc: 0.5958
1024/4849 [=====>........................] - ETA: 3:30 - loss: 0.6693 - acc: 0.6025
1088/4849 [=====>........................] - ETA: 3:26 - loss: 0.6698 - acc: 0.6048
1152/4849 [======>.......................] - ETA: 3:22 - loss: 0.6702 - acc: 0.6033
1216/4849 [======>.......................] - ETA: 3:20 - loss: 0.6733 - acc: 0.5970
1280/4849 [======>.......................] - ETA: 3:16 - loss: 0.6746 - acc: 0.5938
1344/4849 [=======>......................] - ETA: 3:12 - loss: 0.6740 - acc: 0.5930
1408/4849 [=======>......................] - ETA: 3:08 - loss: 0.6738 - acc: 0.5888
1472/4849 [========>.....................] - ETA: 3:06 - loss: 0.6738 - acc: 0.5876
1536/4849 [========>.....................] - ETA: 3:04 - loss: 0.6727 - acc: 0.5866
1600/4849 [========>.....................] - ETA: 3:02 - loss: 0.6722 - acc: 0.5894
1664/4849 [=========>....................] - ETA: 2:59 - loss: 0.6712 - acc: 0.5901
1728/4849 [=========>....................] - ETA: 2:57 - loss: 0.6706 - acc: 0.5909
1792/4849 [==========>...................] - ETA: 2:55 - loss: 0.6706 - acc: 0.5943
1856/4849 [==========>...................] - ETA: 2:51 - loss: 0.6715 - acc: 0.5927
1920/4849 [==========>...................] - ETA: 2:49 - loss: 0.6722 - acc: 0.5906
1984/4849 [===========>..................] - ETA: 2:46 - loss: 0.6739 - acc: 0.5877
2048/4849 [===========>..................] - ETA: 2:43 - loss: 0.6733 - acc: 0.5903
2112/4849 [============>.................] - ETA: 2:40 - loss: 0.6749 - acc: 0.5857
2176/4849 [============>.................] - ETA: 2:37 - loss: 0.6753 - acc: 0.5864
2240/4849 [============>.................] - ETA: 2:34 - loss: 0.6747 - acc: 0.5879
2304/4849 [=============>................] - ETA: 2:31 - loss: 0.6745 - acc: 0.5881
2368/4849 [=============>................] - ETA: 2:27 - loss: 0.6739 - acc: 0.5916
2432/4849 [==============>...............] - ETA: 2:24 - loss: 0.6734 - acc: 0.5938
2496/4849 [==============>...............] - ETA: 2:21 - loss: 0.6732 - acc: 0.5946
2560/4849 [==============>...............] - ETA: 2:17 - loss: 0.6743 - acc: 0.5922
2624/4849 [===============>..............] - ETA: 2:14 - loss: 0.6748 - acc: 0.5911
2688/4849 [===============>..............] - ETA: 2:10 - loss: 0.6745 - acc: 0.5923
2752/4849 [================>.............] - ETA: 2:07 - loss: 0.6745 - acc: 0.5927
2816/4849 [================>.............] - ETA: 2:03 - loss: 0.6740 - acc: 0.5945
2880/4849 [================>.............] - ETA: 2:00 - loss: 0.6733 - acc: 0.5951
2944/4849 [=================>............] - ETA: 1:56 - loss: 0.6737 - acc: 0.5951
3008/4849 [=================>............] - ETA: 1:53 - loss: 0.6742 - acc: 0.5941
3072/4849 [==================>...........] - ETA: 1:49 - loss: 0.6744 - acc: 0.5944
3136/4849 [==================>...........] - ETA: 1:45 - loss: 0.6752 - acc: 0.5934
3200/4849 [==================>...........] - ETA: 1:42 - loss: 0.6757 - acc: 0.5919
3264/4849 [===================>..........] - ETA: 1:38 - loss: 0.6761 - acc: 0.5901
3328/4849 [===================>..........] - ETA: 1:34 - loss: 0.6756 - acc: 0.5901
3392/4849 [===================>..........] - ETA: 1:31 - loss: 0.6762 - acc: 0.5890
3456/4849 [====================>.........] - ETA: 1:27 - loss: 0.6757 - acc: 0.5897
3520/4849 [====================>.........] - ETA: 1:23 - loss: 0.6749 - acc: 0.5912
3584/4849 [=====================>........] - ETA: 1:19 - loss: 0.6750 - acc: 0.5918
3648/4849 [=====================>........] - ETA: 1:15 - loss: 0.6756 - acc: 0.5896
3712/4849 [=====================>........] - ETA: 1:11 - loss: 0.6758 - acc: 0.5897
3776/4849 [======================>.......] - ETA: 1:08 - loss: 0.6758 - acc: 0.5890
3840/4849 [======================>.......] - ETA: 1:04 - loss: 0.6764 - acc: 0.5875
3904/4849 [=======================>......] - ETA: 1:00 - loss: 0.6761 - acc: 0.5884
3968/4849 [=======================>......] - ETA: 57s - loss: 0.6762 - acc: 0.5887 
4032/4849 [=======================>......] - ETA: 53s - loss: 0.6759 - acc: 0.5883
4096/4849 [========================>.....] - ETA: 49s - loss: 0.6753 - acc: 0.5891
4160/4849 [========================>.....] - ETA: 45s - loss: 0.6754 - acc: 0.5882
4224/4849 [=========================>....] - ETA: 41s - loss: 0.6756 - acc: 0.5883
4288/4849 [=========================>....] - ETA: 37s - loss: 0.6756 - acc: 0.5875
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6764 - acc: 0.5864
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6764 - acc: 0.5865
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6763 - acc: 0.5866
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6754 - acc: 0.5876
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6757 - acc: 0.5872
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6754 - acc: 0.5867
4736/4849 [============================>.] - ETA: 7s - loss: 0.6755 - acc: 0.5872 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6763 - acc: 0.5850
4849/4849 [==============================] - 352s 73ms/step - loss: 0.6759 - acc: 0.5861 - val_loss: 0.6859 - val_acc: 0.5826

Epoch 00006: val_acc improved from 0.57143 to 0.58256, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window09/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 7/10

  64/4849 [..............................] - ETA: 8:03 - loss: 0.6427 - acc: 0.6250
 128/4849 [..............................] - ETA: 8:07 - loss: 0.6850 - acc: 0.5703
 192/4849 [>.............................] - ETA: 7:57 - loss: 0.6839 - acc: 0.5521
 256/4849 [>.............................] - ETA: 7:48 - loss: 0.6712 - acc: 0.5742
 320/4849 [>.............................] - ETA: 7:45 - loss: 0.6725 - acc: 0.5813
 384/4849 [=>............................] - ETA: 7:37 - loss: 0.6756 - acc: 0.5833
 448/4849 [=>............................] - ETA: 7:30 - loss: 0.6767 - acc: 0.5781
 512/4849 [==>...........................] - ETA: 7:24 - loss: 0.6754 - acc: 0.5898
 576/4849 [==>...........................] - ETA: 7:17 - loss: 0.6751 - acc: 0.5868
 640/4849 [==>...........................] - ETA: 7:11 - loss: 0.6764 - acc: 0.5844
 704/4849 [===>..........................] - ETA: 7:06 - loss: 0.6778 - acc: 0.5852
 768/4849 [===>..........................] - ETA: 6:58 - loss: 0.6763 - acc: 0.5898
 832/4849 [====>.........................] - ETA: 6:52 - loss: 0.6782 - acc: 0.5829
 896/4849 [====>.........................] - ETA: 6:46 - loss: 0.6793 - acc: 0.5781
 960/4849 [====>.........................] - ETA: 6:39 - loss: 0.6783 - acc: 0.5781
1024/4849 [=====>........................] - ETA: 6:34 - loss: 0.6793 - acc: 0.5781
1088/4849 [=====>........................] - ETA: 6:27 - loss: 0.6765 - acc: 0.5855
1152/4849 [======>.......................] - ETA: 6:21 - loss: 0.6781 - acc: 0.5781
1216/4849 [======>.......................] - ETA: 6:14 - loss: 0.6790 - acc: 0.5740
1280/4849 [======>.......................] - ETA: 6:07 - loss: 0.6795 - acc: 0.5719
1344/4849 [=======>......................] - ETA: 6:01 - loss: 0.6768 - acc: 0.5766
1408/4849 [=======>......................] - ETA: 5:53 - loss: 0.6776 - acc: 0.5767
1472/4849 [========>.....................] - ETA: 5:48 - loss: 0.6782 - acc: 0.5740
1536/4849 [========>.....................] - ETA: 5:43 - loss: 0.6777 - acc: 0.5775
1600/4849 [========>.....................] - ETA: 5:37 - loss: 0.6778 - acc: 0.5756
1664/4849 [=========>....................] - ETA: 5:31 - loss: 0.6771 - acc: 0.5775
1728/4849 [=========>....................] - ETA: 5:26 - loss: 0.6762 - acc: 0.5799
1792/4849 [==========>...................] - ETA: 5:20 - loss: 0.6770 - acc: 0.5781
1856/4849 [==========>...................] - ETA: 5:14 - loss: 0.6764 - acc: 0.5797
1920/4849 [==========>...................] - ETA: 5:08 - loss: 0.6769 - acc: 0.5781
1984/4849 [===========>..................] - ETA: 5:02 - loss: 0.6768 - acc: 0.5776
2048/4849 [===========>..................] - ETA: 4:56 - loss: 0.6760 - acc: 0.5781
2112/4849 [============>.................] - ETA: 4:49 - loss: 0.6752 - acc: 0.5795
2176/4849 [============>.................] - ETA: 4:42 - loss: 0.6758 - acc: 0.5777
2240/4849 [============>.................] - ETA: 4:36 - loss: 0.6741 - acc: 0.5808
2304/4849 [=============>................] - ETA: 4:29 - loss: 0.6748 - acc: 0.5790
2368/4849 [=============>................] - ETA: 4:23 - loss: 0.6740 - acc: 0.5815
2432/4849 [==============>...............] - ETA: 4:16 - loss: 0.6731 - acc: 0.5822
2496/4849 [==============>...............] - ETA: 4:09 - loss: 0.6733 - acc: 0.5813
2560/4849 [==============>...............] - ETA: 4:03 - loss: 0.6739 - acc: 0.5801
2624/4849 [===============>..............] - ETA: 3:56 - loss: 0.6733 - acc: 0.5835
2688/4849 [===============>..............] - ETA: 3:50 - loss: 0.6734 - acc: 0.5830
2752/4849 [================>.............] - ETA: 3:43 - loss: 0.6739 - acc: 0.5821
2816/4849 [================>.............] - ETA: 3:37 - loss: 0.6734 - acc: 0.5842
2880/4849 [================>.............] - ETA: 3:30 - loss: 0.6745 - acc: 0.5816
2944/4849 [=================>............] - ETA: 3:23 - loss: 0.6747 - acc: 0.5815
3008/4849 [=================>............] - ETA: 3:16 - loss: 0.6740 - acc: 0.5834
3072/4849 [==================>...........] - ETA: 3:10 - loss: 0.6741 - acc: 0.5827
3136/4849 [==================>...........] - ETA: 3:03 - loss: 0.6739 - acc: 0.5826
3200/4849 [==================>...........] - ETA: 2:56 - loss: 0.6729 - acc: 0.5841
3264/4849 [===================>..........] - ETA: 2:49 - loss: 0.6729 - acc: 0.5843
3328/4849 [===================>..........] - ETA: 2:42 - loss: 0.6716 - acc: 0.5871
3392/4849 [===================>..........] - ETA: 2:35 - loss: 0.6732 - acc: 0.5861
3456/4849 [====================>.........] - ETA: 2:29 - loss: 0.6728 - acc: 0.5871
3520/4849 [====================>.........] - ETA: 2:22 - loss: 0.6739 - acc: 0.5866
3584/4849 [=====================>........] - ETA: 2:15 - loss: 0.6730 - acc: 0.5876
3648/4849 [=====================>........] - ETA: 2:08 - loss: 0.6730 - acc: 0.5872
3712/4849 [=====================>........] - ETA: 2:01 - loss: 0.6729 - acc: 0.5873
3776/4849 [======================>.......] - ETA: 1:54 - loss: 0.6737 - acc: 0.5874
3840/4849 [======================>.......] - ETA: 1:48 - loss: 0.6738 - acc: 0.5870
3904/4849 [=======================>......] - ETA: 1:41 - loss: 0.6738 - acc: 0.5871
3968/4849 [=======================>......] - ETA: 1:34 - loss: 0.6741 - acc: 0.5869
4032/4849 [=======================>......] - ETA: 1:28 - loss: 0.6739 - acc: 0.5878
4096/4849 [========================>.....] - ETA: 1:21 - loss: 0.6741 - acc: 0.5864
4160/4849 [========================>.....] - ETA: 1:14 - loss: 0.6736 - acc: 0.5868
4224/4849 [=========================>....] - ETA: 1:07 - loss: 0.6733 - acc: 0.5874
4288/4849 [=========================>....] - ETA: 1:00 - loss: 0.6733 - acc: 0.5872
4352/4849 [=========================>....] - ETA: 53s - loss: 0.6733 - acc: 0.5869 
4416/4849 [==========================>...] - ETA: 46s - loss: 0.6728 - acc: 0.5881
4480/4849 [==========================>...] - ETA: 40s - loss: 0.6725 - acc: 0.5886
4544/4849 [===========================>..] - ETA: 33s - loss: 0.6722 - acc: 0.5900
4608/4849 [===========================>..] - ETA: 26s - loss: 0.6721 - acc: 0.5898
4672/4849 [===========================>..] - ETA: 19s - loss: 0.6719 - acc: 0.5899
4736/4849 [============================>.] - ETA: 12s - loss: 0.6720 - acc: 0.5893
4800/4849 [============================>.] - ETA: 5s - loss: 0.6720 - acc: 0.5890 
4849/4849 [==============================] - 551s 114ms/step - loss: 0.6717 - acc: 0.5896 - val_loss: 0.6803 - val_acc: 0.5603

Epoch 00007: val_acc did not improve from 0.58256
Epoch 8/10

  64/4849 [..............................] - ETA: 9:16 - loss: 0.6494 - acc: 0.6250
 128/4849 [..............................] - ETA: 9:09 - loss: 0.6639 - acc: 0.5859
 192/4849 [>.............................] - ETA: 9:05 - loss: 0.6635 - acc: 0.5885
 256/4849 [>.............................] - ETA: 9:01 - loss: 0.6658 - acc: 0.5742
 320/4849 [>.............................] - ETA: 8:58 - loss: 0.6748 - acc: 0.5531
 384/4849 [=>............................] - ETA: 8:51 - loss: 0.6692 - acc: 0.5755
 448/4849 [=>............................] - ETA: 8:44 - loss: 0.6675 - acc: 0.5759
 512/4849 [==>...........................] - ETA: 8:39 - loss: 0.6614 - acc: 0.5879
 576/4849 [==>...........................] - ETA: 8:32 - loss: 0.6658 - acc: 0.5764
 640/4849 [==>...........................] - ETA: 8:27 - loss: 0.6650 - acc: 0.5813
 704/4849 [===>..........................] - ETA: 8:17 - loss: 0.6653 - acc: 0.5795
 768/4849 [===>..........................] - ETA: 8:09 - loss: 0.6654 - acc: 0.5781
 832/4849 [====>.........................] - ETA: 8:00 - loss: 0.6659 - acc: 0.5781
 896/4849 [====>.........................] - ETA: 7:52 - loss: 0.6657 - acc: 0.5848
 960/4849 [====>.........................] - ETA: 7:43 - loss: 0.6635 - acc: 0.5927
1024/4849 [=====>........................] - ETA: 7:36 - loss: 0.6637 - acc: 0.5947
1088/4849 [=====>........................] - ETA: 7:28 - loss: 0.6652 - acc: 0.5928
1152/4849 [======>.......................] - ETA: 7:21 - loss: 0.6636 - acc: 0.5946
1216/4849 [======>.......................] - ETA: 7:13 - loss: 0.6642 - acc: 0.5954
1280/4849 [======>.......................] - ETA: 7:05 - loss: 0.6640 - acc: 0.5961
1344/4849 [=======>......................] - ETA: 6:57 - loss: 0.6669 - acc: 0.5960
1408/4849 [=======>......................] - ETA: 6:49 - loss: 0.6672 - acc: 0.5987
1472/4849 [========>.....................] - ETA: 6:42 - loss: 0.6678 - acc: 0.5951
1536/4849 [========>.....................] - ETA: 6:35 - loss: 0.6682 - acc: 0.5938
1600/4849 [========>.....................] - ETA: 6:27 - loss: 0.6672 - acc: 0.5962
1664/4849 [=========>....................] - ETA: 6:20 - loss: 0.6678 - acc: 0.5962
1728/4849 [=========>....................] - ETA: 6:13 - loss: 0.6667 - acc: 0.5978
1792/4849 [==========>...................] - ETA: 6:05 - loss: 0.6674 - acc: 0.5960
1856/4849 [==========>...................] - ETA: 5:57 - loss: 0.6676 - acc: 0.5975
1920/4849 [==========>...................] - ETA: 5:50 - loss: 0.6661 - acc: 0.6000
1984/4849 [===========>..................] - ETA: 5:42 - loss: 0.6679 - acc: 0.5993
2048/4849 [===========>..................] - ETA: 5:35 - loss: 0.6681 - acc: 0.5991
2112/4849 [============>.................] - ETA: 5:28 - loss: 0.6674 - acc: 0.6009
2176/4849 [============>.................] - ETA: 5:20 - loss: 0.6669 - acc: 0.6020
2240/4849 [============>.................] - ETA: 5:12 - loss: 0.6667 - acc: 0.6036
2304/4849 [=============>................] - ETA: 5:04 - loss: 0.6667 - acc: 0.6037
2368/4849 [=============>................] - ETA: 4:57 - loss: 0.6675 - acc: 0.6026
2432/4849 [==============>...............] - ETA: 4:49 - loss: 0.6675 - acc: 0.6012
2496/4849 [==============>...............] - ETA: 4:42 - loss: 0.6670 - acc: 0.6026
2560/4849 [==============>...............] - ETA: 4:35 - loss: 0.6670 - acc: 0.6020
2624/4849 [===============>..............] - ETA: 4:27 - loss: 0.6685 - acc: 0.5995
2688/4849 [===============>..............] - ETA: 4:20 - loss: 0.6671 - acc: 0.6019
2752/4849 [================>.............] - ETA: 4:12 - loss: 0.6675 - acc: 0.6017
2816/4849 [================>.............] - ETA: 4:04 - loss: 0.6666 - acc: 0.6033
2880/4849 [================>.............] - ETA: 3:56 - loss: 0.6668 - acc: 0.6028
2944/4849 [=================>............] - ETA: 3:48 - loss: 0.6669 - acc: 0.6029
3008/4849 [=================>............] - ETA: 3:40 - loss: 0.6663 - acc: 0.6034
3072/4849 [==================>...........] - ETA: 3:32 - loss: 0.6661 - acc: 0.6045
3136/4849 [==================>...........] - ETA: 3:24 - loss: 0.6665 - acc: 0.6049
3200/4849 [==================>...........] - ETA: 3:16 - loss: 0.6678 - acc: 0.6028
3264/4849 [===================>..........] - ETA: 3:09 - loss: 0.6673 - acc: 0.6039
3328/4849 [===================>..........] - ETA: 3:01 - loss: 0.6669 - acc: 0.6043
3392/4849 [===================>..........] - ETA: 2:53 - loss: 0.6674 - acc: 0.6038
3456/4849 [====================>.........] - ETA: 2:45 - loss: 0.6677 - acc: 0.6019
3520/4849 [====================>.........] - ETA: 2:38 - loss: 0.6684 - acc: 0.6014
3584/4849 [=====================>........] - ETA: 2:30 - loss: 0.6688 - acc: 0.6018
3648/4849 [=====================>........] - ETA: 2:22 - loss: 0.6682 - acc: 0.6033
3712/4849 [=====================>........] - ETA: 2:14 - loss: 0.6685 - acc: 0.6026
3776/4849 [======================>.......] - ETA: 2:07 - loss: 0.6679 - acc: 0.6038
3840/4849 [======================>.......] - ETA: 1:59 - loss: 0.6680 - acc: 0.6031
3904/4849 [=======================>......] - ETA: 1:51 - loss: 0.6688 - acc: 0.6019
3968/4849 [=======================>......] - ETA: 1:44 - loss: 0.6690 - acc: 0.6011
4032/4849 [=======================>......] - ETA: 1:36 - loss: 0.6689 - acc: 0.6014
4096/4849 [========================>.....] - ETA: 1:28 - loss: 0.6691 - acc: 0.6003
4160/4849 [========================>.....] - ETA: 1:21 - loss: 0.6695 - acc: 0.5995
4224/4849 [=========================>....] - ETA: 1:13 - loss: 0.6693 - acc: 0.5994
4288/4849 [=========================>....] - ETA: 1:06 - loss: 0.6690 - acc: 0.5998
4352/4849 [=========================>....] - ETA: 58s - loss: 0.6694 - acc: 0.5983 
4416/4849 [==========================>...] - ETA: 51s - loss: 0.6693 - acc: 0.5981
4480/4849 [==========================>...] - ETA: 43s - loss: 0.6691 - acc: 0.5982
4544/4849 [===========================>..] - ETA: 36s - loss: 0.6692 - acc: 0.5984
4608/4849 [===========================>..] - ETA: 28s - loss: 0.6699 - acc: 0.5977
4672/4849 [===========================>..] - ETA: 20s - loss: 0.6697 - acc: 0.5978
4736/4849 [============================>.] - ETA: 13s - loss: 0.6696 - acc: 0.5976
4800/4849 [============================>.] - ETA: 5s - loss: 0.6692 - acc: 0.5971 
4849/4849 [==============================] - 594s 122ms/step - loss: 0.6696 - acc: 0.5968 - val_loss: 0.6791 - val_acc: 0.5807

Epoch 00008: val_acc did not improve from 0.58256
Epoch 9/10

  64/4849 [..............................] - ETA: 9:23 - loss: 0.6771 - acc: 0.5625
 128/4849 [..............................] - ETA: 9:15 - loss: 0.6725 - acc: 0.6094
 192/4849 [>.............................] - ETA: 8:59 - loss: 0.6636 - acc: 0.6094
 256/4849 [>.............................] - ETA: 8:52 - loss: 0.6600 - acc: 0.6133
 320/4849 [>.............................] - ETA: 8:48 - loss: 0.6691 - acc: 0.6125
 384/4849 [=>............................] - ETA: 8:36 - loss: 0.6673 - acc: 0.6146
 448/4849 [=>............................] - ETA: 8:27 - loss: 0.6729 - acc: 0.5938
 512/4849 [==>...........................] - ETA: 8:21 - loss: 0.6714 - acc: 0.5918
 576/4849 [==>...........................] - ETA: 8:15 - loss: 0.6719 - acc: 0.5885
 640/4849 [==>...........................] - ETA: 8:12 - loss: 0.6749 - acc: 0.5875
 704/4849 [===>..........................] - ETA: 8:06 - loss: 0.6718 - acc: 0.5895
 768/4849 [===>..........................] - ETA: 8:00 - loss: 0.6733 - acc: 0.5885
 832/4849 [====>.........................] - ETA: 7:54 - loss: 0.6720 - acc: 0.5865
 896/4849 [====>.........................] - ETA: 7:48 - loss: 0.6764 - acc: 0.5792
 960/4849 [====>.........................] - ETA: 7:39 - loss: 0.6742 - acc: 0.5833
1024/4849 [=====>........................] - ETA: 7:34 - loss: 0.6742 - acc: 0.5830
1088/4849 [=====>........................] - ETA: 7:28 - loss: 0.6746 - acc: 0.5827
1152/4849 [======>.......................] - ETA: 7:20 - loss: 0.6727 - acc: 0.5868
1216/4849 [======>.......................] - ETA: 7:11 - loss: 0.6711 - acc: 0.5913
1280/4849 [======>.......................] - ETA: 7:04 - loss: 0.6698 - acc: 0.5914
1344/4849 [=======>......................] - ETA: 6:56 - loss: 0.6708 - acc: 0.5893
1408/4849 [=======>......................] - ETA: 6:49 - loss: 0.6705 - acc: 0.5916
1472/4849 [========>.....................] - ETA: 6:42 - loss: 0.6716 - acc: 0.5890
1536/4849 [========>.....................] - ETA: 6:35 - loss: 0.6698 - acc: 0.5938
1600/4849 [========>.....................] - ETA: 6:26 - loss: 0.6685 - acc: 0.5962
1664/4849 [=========>....................] - ETA: 6:19 - loss: 0.6663 - acc: 0.6016
1728/4849 [=========>....................] - ETA: 6:11 - loss: 0.6683 - acc: 0.5984
1792/4849 [==========>...................] - ETA: 6:04 - loss: 0.6691 - acc: 0.5982
1856/4849 [==========>...................] - ETA: 5:56 - loss: 0.6683 - acc: 0.5997
1920/4849 [==========>...................] - ETA: 5:48 - loss: 0.6695 - acc: 0.5979
1984/4849 [===========>..................] - ETA: 5:41 - loss: 0.6703 - acc: 0.5973
2048/4849 [===========>..................] - ETA: 5:33 - loss: 0.6716 - acc: 0.5952
2112/4849 [============>.................] - ETA: 5:26 - loss: 0.6733 - acc: 0.5909
2176/4849 [============>.................] - ETA: 5:18 - loss: 0.6736 - acc: 0.5924
2240/4849 [============>.................] - ETA: 5:10 - loss: 0.6734 - acc: 0.5938
2304/4849 [=============>................] - ETA: 5:02 - loss: 0.6732 - acc: 0.5942
2368/4849 [=============>................] - ETA: 4:55 - loss: 0.6720 - acc: 0.5954
2432/4849 [==============>...............] - ETA: 4:47 - loss: 0.6713 - acc: 0.5958
2496/4849 [==============>...............] - ETA: 4:40 - loss: 0.6723 - acc: 0.5950
2560/4849 [==============>...............] - ETA: 4:32 - loss: 0.6719 - acc: 0.5953
2624/4849 [===============>..............] - ETA: 4:24 - loss: 0.6708 - acc: 0.5979
2688/4849 [===============>..............] - ETA: 4:17 - loss: 0.6700 - acc: 0.6001
2752/4849 [================>.............] - ETA: 4:09 - loss: 0.6702 - acc: 0.5999
2816/4849 [================>.............] - ETA: 4:01 - loss: 0.6700 - acc: 0.5987
2880/4849 [================>.............] - ETA: 3:54 - loss: 0.6700 - acc: 0.5993
2944/4849 [=================>............] - ETA: 3:46 - loss: 0.6705 - acc: 0.5975
3008/4849 [=================>............] - ETA: 3:39 - loss: 0.6709 - acc: 0.5974
3072/4849 [==================>...........] - ETA: 3:31 - loss: 0.6708 - acc: 0.5983
3136/4849 [==================>...........] - ETA: 3:23 - loss: 0.6717 - acc: 0.5976
3200/4849 [==================>...........] - ETA: 3:16 - loss: 0.6711 - acc: 0.5984
3264/4849 [===================>..........] - ETA: 3:08 - loss: 0.6720 - acc: 0.5962
3328/4849 [===================>..........] - ETA: 3:00 - loss: 0.6719 - acc: 0.5959
3392/4849 [===================>..........] - ETA: 2:52 - loss: 0.6718 - acc: 0.5952
3456/4849 [====================>.........] - ETA: 2:44 - loss: 0.6725 - acc: 0.5949
3520/4849 [====================>.........] - ETA: 2:36 - loss: 0.6724 - acc: 0.5955
3584/4849 [=====================>........] - ETA: 2:29 - loss: 0.6724 - acc: 0.5940
3648/4849 [=====================>........] - ETA: 2:21 - loss: 0.6710 - acc: 0.5965
3712/4849 [=====================>........] - ETA: 2:13 - loss: 0.6710 - acc: 0.5956
3776/4849 [======================>.......] - ETA: 2:06 - loss: 0.6711 - acc: 0.5948
3840/4849 [======================>.......] - ETA: 1:58 - loss: 0.6701 - acc: 0.5966
3904/4849 [=======================>......] - ETA: 1:50 - loss: 0.6699 - acc: 0.5978
3968/4849 [=======================>......] - ETA: 1:43 - loss: 0.6692 - acc: 0.5980
4032/4849 [=======================>......] - ETA: 1:35 - loss: 0.6701 - acc: 0.5965
4096/4849 [========================>.....] - ETA: 1:28 - loss: 0.6707 - acc: 0.5950
4160/4849 [========================>.....] - ETA: 1:20 - loss: 0.6712 - acc: 0.5942
4224/4849 [=========================>....] - ETA: 1:12 - loss: 0.6713 - acc: 0.5940
4288/4849 [=========================>....] - ETA: 1:05 - loss: 0.6711 - acc: 0.5951
4352/4849 [=========================>....] - ETA: 57s - loss: 0.6710 - acc: 0.5951 
4416/4849 [==========================>...] - ETA: 50s - loss: 0.6704 - acc: 0.5958
4480/4849 [==========================>...] - ETA: 42s - loss: 0.6707 - acc: 0.5960
4544/4849 [===========================>..] - ETA: 35s - loss: 0.6713 - acc: 0.5955
4608/4849 [===========================>..] - ETA: 28s - loss: 0.6714 - acc: 0.5946
4672/4849 [===========================>..] - ETA: 20s - loss: 0.6709 - acc: 0.5959
4736/4849 [============================>.] - ETA: 13s - loss: 0.6706 - acc: 0.5963
4800/4849 [============================>.] - ETA: 5s - loss: 0.6710 - acc: 0.5956 
4849/4849 [==============================] - 583s 120ms/step - loss: 0.6708 - acc: 0.5962 - val_loss: 0.6846 - val_acc: 0.5751

Epoch 00009: val_acc did not improve from 0.58256
Epoch 10/10

  64/4849 [..............................] - ETA: 8:57 - loss: 0.6704 - acc: 0.5469
 128/4849 [..............................] - ETA: 8:38 - loss: 0.6465 - acc: 0.6172
 192/4849 [>.............................] - ETA: 8:31 - loss: 0.6667 - acc: 0.5990
 256/4849 [>.............................] - ETA: 8:22 - loss: 0.6782 - acc: 0.5938
 320/4849 [>.............................] - ETA: 8:12 - loss: 0.6689 - acc: 0.6000
 384/4849 [=>............................] - ETA: 8:12 - loss: 0.6614 - acc: 0.6120
 448/4849 [=>............................] - ETA: 8:03 - loss: 0.6653 - acc: 0.6071
 512/4849 [==>...........................] - ETA: 7:56 - loss: 0.6644 - acc: 0.6211
 576/4849 [==>...........................] - ETA: 7:50 - loss: 0.6687 - acc: 0.6146
 640/4849 [==>...........................] - ETA: 7:44 - loss: 0.6701 - acc: 0.6109
 704/4849 [===>..........................] - ETA: 7:38 - loss: 0.6673 - acc: 0.6136
 768/4849 [===>..........................] - ETA: 7:32 - loss: 0.6678 - acc: 0.6120
 832/4849 [====>.........................] - ETA: 7:25 - loss: 0.6701 - acc: 0.6034
 896/4849 [====>.........................] - ETA: 7:18 - loss: 0.6699 - acc: 0.6027
 960/4849 [====>.........................] - ETA: 7:12 - loss: 0.6722 - acc: 0.5979
1024/4849 [=====>........................] - ETA: 7:06 - loss: 0.6719 - acc: 0.5928
1088/4849 [=====>........................] - ETA: 6:57 - loss: 0.6696 - acc: 0.5947
1152/4849 [======>.......................] - ETA: 6:50 - loss: 0.6688 - acc: 0.5981
1216/4849 [======>.......................] - ETA: 6:45 - loss: 0.6704 - acc: 0.5970
1280/4849 [======>.......................] - ETA: 6:38 - loss: 0.6726 - acc: 0.5914
1344/4849 [=======>......................] - ETA: 6:31 - loss: 0.6725 - acc: 0.5930
1408/4849 [=======>......................] - ETA: 6:24 - loss: 0.6714 - acc: 0.5959
1472/4849 [========>.....................] - ETA: 6:17 - loss: 0.6735 - acc: 0.5917
1536/4849 [========>.....................] - ETA: 6:10 - loss: 0.6767 - acc: 0.5885
1600/4849 [========>.....................] - ETA: 6:02 - loss: 0.6771 - acc: 0.5869
1664/4849 [=========>....................] - ETA: 5:55 - loss: 0.6790 - acc: 0.5859
1728/4849 [=========>....................] - ETA: 5:48 - loss: 0.6791 - acc: 0.5845
1792/4849 [==========>...................] - ETA: 5:40 - loss: 0.6780 - acc: 0.5859
1856/4849 [==========>...................] - ETA: 5:33 - loss: 0.6784 - acc: 0.5835
1920/4849 [==========>...................] - ETA: 5:26 - loss: 0.6792 - acc: 0.5818
1984/4849 [===========>..................] - ETA: 5:19 - loss: 0.6789 - acc: 0.5817
2048/4849 [===========>..................] - ETA: 5:12 - loss: 0.6788 - acc: 0.5801
2112/4849 [============>.................] - ETA: 5:06 - loss: 0.6800 - acc: 0.5758
2176/4849 [============>.................] - ETA: 4:59 - loss: 0.6801 - acc: 0.5763
2240/4849 [============>.................] - ETA: 4:52 - loss: 0.6790 - acc: 0.5786
2304/4849 [=============>................] - ETA: 4:45 - loss: 0.6796 - acc: 0.5768
2368/4849 [=============>................] - ETA: 4:38 - loss: 0.6793 - acc: 0.5773
2432/4849 [==============>...............] - ETA: 4:31 - loss: 0.6791 - acc: 0.5773
2496/4849 [==============>...............] - ETA: 4:24 - loss: 0.6782 - acc: 0.5793
2560/4849 [==============>...............] - ETA: 4:17 - loss: 0.6789 - acc: 0.5773
2624/4849 [===============>..............] - ETA: 4:10 - loss: 0.6800 - acc: 0.5747
2688/4849 [===============>..............] - ETA: 4:03 - loss: 0.6792 - acc: 0.5759
2752/4849 [================>.............] - ETA: 3:56 - loss: 0.6782 - acc: 0.5778
2816/4849 [================>.............] - ETA: 3:49 - loss: 0.6780 - acc: 0.5774
2880/4849 [================>.............] - ETA: 3:42 - loss: 0.6773 - acc: 0.5799
2944/4849 [=================>............] - ETA: 3:35 - loss: 0.6765 - acc: 0.5805
3008/4849 [=================>............] - ETA: 3:27 - loss: 0.6767 - acc: 0.5791
3072/4849 [==================>...........] - ETA: 3:20 - loss: 0.6759 - acc: 0.5794
3136/4849 [==================>...........] - ETA: 3:13 - loss: 0.6761 - acc: 0.5794
3200/4849 [==================>...........] - ETA: 3:06 - loss: 0.6760 - acc: 0.5806
3264/4849 [===================>..........] - ETA: 2:59 - loss: 0.6756 - acc: 0.5815
3328/4849 [===================>..........] - ETA: 2:52 - loss: 0.6751 - acc: 0.5826
3392/4849 [===================>..........] - ETA: 2:44 - loss: 0.6741 - acc: 0.5846
3456/4849 [====================>.........] - ETA: 2:37 - loss: 0.6740 - acc: 0.5854
3520/4849 [====================>.........] - ETA: 2:30 - loss: 0.6743 - acc: 0.5835
3584/4849 [=====================>........] - ETA: 2:23 - loss: 0.6745 - acc: 0.5826
3648/4849 [=====================>........] - ETA: 2:16 - loss: 0.6750 - acc: 0.5820
3712/4849 [=====================>........] - ETA: 2:08 - loss: 0.6747 - acc: 0.5832
3776/4849 [======================>.......] - ETA: 2:01 - loss: 0.6747 - acc: 0.5834
3840/4849 [======================>.......] - ETA: 1:54 - loss: 0.6741 - acc: 0.5854
3904/4849 [=======================>......] - ETA: 1:47 - loss: 0.6738 - acc: 0.5853
3968/4849 [=======================>......] - ETA: 1:40 - loss: 0.6731 - acc: 0.5867
4032/4849 [=======================>......] - ETA: 1:33 - loss: 0.6726 - acc: 0.5883
4096/4849 [========================>.....] - ETA: 1:25 - loss: 0.6727 - acc: 0.5874
4160/4849 [========================>.....] - ETA: 1:18 - loss: 0.6728 - acc: 0.5875
4224/4849 [=========================>....] - ETA: 1:11 - loss: 0.6732 - acc: 0.5878
4288/4849 [=========================>....] - ETA: 1:03 - loss: 0.6731 - acc: 0.5879
4352/4849 [=========================>....] - ETA: 56s - loss: 0.6737 - acc: 0.5869 
4416/4849 [==========================>...] - ETA: 49s - loss: 0.6732 - acc: 0.5874
4480/4849 [==========================>...] - ETA: 42s - loss: 0.6733 - acc: 0.5877
4544/4849 [===========================>..] - ETA: 34s - loss: 0.6727 - acc: 0.5882
4608/4849 [===========================>..] - ETA: 27s - loss: 0.6732 - acc: 0.5875
4672/4849 [===========================>..] - ETA: 20s - loss: 0.6729 - acc: 0.5873
4736/4849 [============================>.] - ETA: 12s - loss: 0.6728 - acc: 0.5876
4800/4849 [============================>.] - ETA: 5s - loss: 0.6734 - acc: 0.5867 
4849/4849 [==============================] - 574s 118ms/step - loss: 0.6735 - acc: 0.5861 - val_loss: 0.7079 - val_acc: 0.5213

Epoch 00010: val_acc did not improve from 0.58256
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe3a4289e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe3a4289e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe39c7d1ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe39c7d1ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c06ff450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe6c06ff450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe39c5dc710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe39c5dc710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe39c6199d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe39c6199d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe39c608910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe39c608910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe39c5dc990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe39c5dc990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe39c5a4d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe39c5a4d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0f03b1e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0f03b1e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe39c39f690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe39c39f690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe39c463890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe39c463890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0f03b1990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0f03b1990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f0150890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f0150890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe39c1cc290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe39c1cc290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe39c15c690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe39c15c690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe39c1a4f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe39c1a4f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe39c152690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe39c152690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3946afe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3946afe50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe394619050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe394619050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe394576dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe394576dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3944e4950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3944e4950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3944e4510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3944e4510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3944d83d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3944d83d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3944d0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3944d0fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe39413eb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe39413eb50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3942ea210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3942ea210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3944d0b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3944d0b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3942bbf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3942bbf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe340773d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe340773d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3406f5cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3406f5cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe394180b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe394180b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe340773750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe340773750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe34066ff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe34066ff10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe340403c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe340403c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3406d3a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3406d3a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe34041c810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe34041c810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe340314810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe340314810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe340230cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe340230cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe28c099d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe28c099d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3307eb790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3307eb790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe34010be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe34010be10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3402fbb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3402fbb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3307b9810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3307b9810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3305e2890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe3305e2890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe34006d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe34006d710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3305b3d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3305b3d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3404a36d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3404a36d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe330561790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe330561790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe33027e750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe33027e750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3301af0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe3301af0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3301f7510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3301f7510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe33027e0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe33027e0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3302951d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3302951d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe304772590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe304772590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe30474bf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe30474bf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3301af590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3301af590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe340449e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe340449e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3044e10d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3044e10d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe304616350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe304616350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe30442a490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe30442a490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe304512990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe304512990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3044b9550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3044b9550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe30464a0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe30464a0d0>>: AttributeError: module 'gast' has no attribute 'Str'
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 4:00
 128/1348 [=>............................] - ETA: 2:18
 192/1348 [===>..........................] - ETA: 1:38
 256/1348 [====>.........................] - ETA: 1:16
 320/1348 [======>.......................] - ETA: 1:02
 384/1348 [=======>......................] - ETA: 53s 
 448/1348 [========>.....................] - ETA: 46s
 512/1348 [==========>...................] - ETA: 39s
 576/1348 [===========>..................] - ETA: 34s
 640/1348 [=============>................] - ETA: 30s
 704/1348 [==============>...............] - ETA: 27s
 768/1348 [================>.............] - ETA: 23s
 832/1348 [=================>............] - ETA: 20s
 896/1348 [==================>...........] - ETA: 17s
 960/1348 [====================>.........] - ETA: 14s
1024/1348 [=====================>........] - ETA: 12s
1088/1348 [=======================>......] - ETA: 9s 
1152/1348 [========================>.....] - ETA: 7s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 48s 36ms/step
loss: 0.6768808929078303
acc: 0.5801186943620178
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe0b8235e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe0b8235e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe0581cf5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe0581cf5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3a40eb710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3a40eb710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1bc0fe950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1bc0fe950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0581b7cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0581b7cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe39c7c3450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe39c7c3450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe39c7c3610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe39c7c3610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3040b90d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe3040b90d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe39c7be3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe39c7be3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe058079c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe058079c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe050739fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe050739fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3a417bdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe3a417bdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0580d6350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0580d6350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0580bee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0580bee10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe050558f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe050558f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe050638650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe050638650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe050702210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe050702210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0506b45d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0506b45d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe050409290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe050409290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe050280790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe050280790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe050540c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe050540c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe050664f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe050664f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe050287e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe050287e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0500fda50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0500fda50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0486d7c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0486d7c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe05003ead0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe05003ead0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe05006f350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe05006f350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe05003c950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe05003c950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0484f7b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0484f7b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0483e7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0483e7290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0482f2fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0482f2fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0487daa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0487daa10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe04828d0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe04828d0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0483edf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0483edf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe048396890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe048396890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0483d7f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0483d7f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0484d5c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0484d5c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe048088310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe048088310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe038683a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe038683a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe03855a3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe03855a3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe048092f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe048092f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe038683c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe038683c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03851cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03851cf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe03851f6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe03851f6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe038346a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe038346a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03855a8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03855a8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe038302910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe038302910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf301fb250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf301fb250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf302fac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf302fac90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf30162c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf30162c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf301982d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf301982d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdee0736dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdee0736dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf3014a5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf3014a5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdee05dc310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdee05dc310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdee047f8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdee047f8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf3010da90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf3010da90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdee04c3690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdee04c3690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdee047f8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdee047f8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdee02f8590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdee02f8590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdee01a9990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdee01a9990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdee0246e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdee0246e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdee02f8850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdee02f8850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdee01eab50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdee01eab50>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 44:27 - loss: 0.7151 - acc: 0.5469
 128/4849 [..............................] - ETA: 25:37 - loss: 0.8429 - acc: 0.4844
 192/4849 [>.............................] - ETA: 18:48 - loss: 0.7808 - acc: 0.5417
 256/4849 [>.............................] - ETA: 15:43 - loss: 0.7832 - acc: 0.5508
 320/4849 [>.............................] - ETA: 13:33 - loss: 0.8042 - acc: 0.5188
 384/4849 [=>............................] - ETA: 12:09 - loss: 0.8018 - acc: 0.5000
 448/4849 [=>............................] - ETA: 11:08 - loss: 0.7815 - acc: 0.5112
 512/4849 [==>...........................] - ETA: 10:27 - loss: 0.7790 - acc: 0.5078
 576/4849 [==>...........................] - ETA: 9:52 - loss: 0.7706 - acc: 0.5156 
 640/4849 [==>...........................] - ETA: 9:19 - loss: 0.7622 - acc: 0.5188
 704/4849 [===>..........................] - ETA: 8:55 - loss: 0.7549 - acc: 0.5241
 768/4849 [===>..........................] - ETA: 8:31 - loss: 0.7472 - acc: 0.5299
 832/4849 [====>.........................] - ETA: 8:10 - loss: 0.7433 - acc: 0.5349
 896/4849 [====>.........................] - ETA: 7:54 - loss: 0.7388 - acc: 0.5402
 960/4849 [====>.........................] - ETA: 7:39 - loss: 0.7395 - acc: 0.5333
1024/4849 [=====>........................] - ETA: 7:22 - loss: 0.7382 - acc: 0.5312
1088/4849 [=====>........................] - ETA: 7:09 - loss: 0.7368 - acc: 0.5312
1152/4849 [======>.......................] - ETA: 6:55 - loss: 0.7330 - acc: 0.5391
1216/4849 [======>.......................] - ETA: 6:42 - loss: 0.7287 - acc: 0.5428
1280/4849 [======>.......................] - ETA: 6:32 - loss: 0.7322 - acc: 0.5398
1344/4849 [=======>......................] - ETA: 6:22 - loss: 0.7316 - acc: 0.5402
1408/4849 [=======>......................] - ETA: 6:10 - loss: 0.7328 - acc: 0.5334
1472/4849 [========>.....................] - ETA: 6:03 - loss: 0.7338 - acc: 0.5312
1536/4849 [========>.....................] - ETA: 5:53 - loss: 0.7323 - acc: 0.5312
1600/4849 [========>.....................] - ETA: 5:44 - loss: 0.7299 - acc: 0.5325
1664/4849 [=========>....................] - ETA: 5:36 - loss: 0.7288 - acc: 0.5319
1728/4849 [=========>....................] - ETA: 5:26 - loss: 0.7293 - acc: 0.5301
1792/4849 [==========>...................] - ETA: 5:18 - loss: 0.7291 - acc: 0.5279
1856/4849 [==========>...................] - ETA: 5:10 - loss: 0.7274 - acc: 0.5291
1920/4849 [==========>...................] - ETA: 5:01 - loss: 0.7264 - acc: 0.5292
1984/4849 [===========>..................] - ETA: 4:53 - loss: 0.7263 - acc: 0.5282
2048/4849 [===========>..................] - ETA: 4:46 - loss: 0.7262 - acc: 0.5259
2112/4849 [============>.................] - ETA: 4:37 - loss: 0.7250 - acc: 0.5251
2176/4849 [============>.................] - ETA: 4:31 - loss: 0.7246 - acc: 0.5234
2240/4849 [============>.................] - ETA: 4:23 - loss: 0.7247 - acc: 0.5223
2304/4849 [=============>................] - ETA: 4:16 - loss: 0.7229 - acc: 0.5239
2368/4849 [=============>................] - ETA: 4:08 - loss: 0.7232 - acc: 0.5211
2432/4849 [==============>...............] - ETA: 4:01 - loss: 0.7231 - acc: 0.5197
2496/4849 [==============>...............] - ETA: 3:54 - loss: 0.7239 - acc: 0.5180
2560/4849 [==============>...............] - ETA: 3:47 - loss: 0.7225 - acc: 0.5207
2624/4849 [===============>..............] - ETA: 3:40 - loss: 0.7233 - acc: 0.5187
2688/4849 [===============>..............] - ETA: 3:33 - loss: 0.7227 - acc: 0.5182
2752/4849 [================>.............] - ETA: 3:26 - loss: 0.7226 - acc: 0.5185
2816/4849 [================>.............] - ETA: 3:20 - loss: 0.7218 - acc: 0.5188
2880/4849 [================>.............] - ETA: 3:13 - loss: 0.7218 - acc: 0.5181
2944/4849 [=================>............] - ETA: 3:06 - loss: 0.7220 - acc: 0.5166
3008/4849 [=================>............] - ETA: 2:59 - loss: 0.7213 - acc: 0.5170
3072/4849 [==================>...........] - ETA: 2:53 - loss: 0.7207 - acc: 0.5169
3136/4849 [==================>...........] - ETA: 2:46 - loss: 0.7200 - acc: 0.5175
3200/4849 [==================>...........] - ETA: 2:39 - loss: 0.7190 - acc: 0.5184
3264/4849 [===================>..........] - ETA: 2:33 - loss: 0.7192 - acc: 0.5162
3328/4849 [===================>..........] - ETA: 2:26 - loss: 0.7195 - acc: 0.5135
3392/4849 [===================>..........] - ETA: 2:20 - loss: 0.7185 - acc: 0.5139
3456/4849 [====================>.........] - ETA: 2:14 - loss: 0.7187 - acc: 0.5130
3520/4849 [====================>.........] - ETA: 2:07 - loss: 0.7175 - acc: 0.5156
3584/4849 [=====================>........] - ETA: 2:01 - loss: 0.7170 - acc: 0.5165
3648/4849 [=====================>........] - ETA: 1:55 - loss: 0.7170 - acc: 0.5154
3712/4849 [=====================>........] - ETA: 1:49 - loss: 0.7162 - acc: 0.5164
3776/4849 [======================>.......] - ETA: 1:42 - loss: 0.7162 - acc: 0.5180
3840/4849 [======================>.......] - ETA: 1:36 - loss: 0.7160 - acc: 0.5180
3904/4849 [=======================>......] - ETA: 1:30 - loss: 0.7157 - acc: 0.5184
3968/4849 [=======================>......] - ETA: 1:24 - loss: 0.7150 - acc: 0.5199
4032/4849 [=======================>......] - ETA: 1:17 - loss: 0.7146 - acc: 0.5206
4096/4849 [========================>.....] - ETA: 1:11 - loss: 0.7141 - acc: 0.5215
4160/4849 [========================>.....] - ETA: 1:05 - loss: 0.7144 - acc: 0.5216
4224/4849 [=========================>....] - ETA: 59s - loss: 0.7147 - acc: 0.5211 
4288/4849 [=========================>....] - ETA: 53s - loss: 0.7142 - acc: 0.5219
4352/4849 [=========================>....] - ETA: 47s - loss: 0.7137 - acc: 0.5227
4416/4849 [==========================>...] - ETA: 41s - loss: 0.7129 - acc: 0.5222
4480/4849 [==========================>...] - ETA: 34s - loss: 0.7134 - acc: 0.5217
4544/4849 [===========================>..] - ETA: 28s - loss: 0.7131 - acc: 0.5220
4608/4849 [===========================>..] - ETA: 22s - loss: 0.7134 - acc: 0.5215
4672/4849 [===========================>..] - ETA: 16s - loss: 0.7134 - acc: 0.5210
4736/4849 [============================>.] - ETA: 10s - loss: 0.7128 - acc: 0.5222
4800/4849 [============================>.] - ETA: 4s - loss: 0.7127 - acc: 0.5215 
4849/4849 [==============================] - 479s 99ms/step - loss: 0.7125 - acc: 0.5216 - val_loss: 0.6827 - val_acc: 0.5306

Epoch 00001: val_acc improved from -inf to 0.53061, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window10/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 7:05 - loss: 0.6907 - acc: 0.5938
 128/4849 [..............................] - ETA: 6:47 - loss: 0.6859 - acc: 0.5547
 192/4849 [>.............................] - ETA: 6:58 - loss: 0.6864 - acc: 0.5573
 256/4849 [>.............................] - ETA: 6:53 - loss: 0.6816 - acc: 0.5469
 320/4849 [>.............................] - ETA: 6:44 - loss: 0.6836 - acc: 0.5531
 384/4849 [=>............................] - ETA: 6:46 - loss: 0.6837 - acc: 0.5625
 448/4849 [=>............................] - ETA: 6:40 - loss: 0.6835 - acc: 0.5580
 512/4849 [==>...........................] - ETA: 6:32 - loss: 0.6929 - acc: 0.5371
 576/4849 [==>...........................] - ETA: 6:24 - loss: 0.6888 - acc: 0.5434
 640/4849 [==>...........................] - ETA: 6:18 - loss: 0.6923 - acc: 0.5359
 704/4849 [===>..........................] - ETA: 6:13 - loss: 0.6900 - acc: 0.5384
 768/4849 [===>..........................] - ETA: 6:08 - loss: 0.6881 - acc: 0.5443
 832/4849 [====>.........................] - ETA: 6:03 - loss: 0.6880 - acc: 0.5481
 896/4849 [====>.........................] - ETA: 5:56 - loss: 0.6878 - acc: 0.5513
 960/4849 [====>.........................] - ETA: 5:50 - loss: 0.6886 - acc: 0.5542
1024/4849 [=====>........................] - ETA: 5:44 - loss: 0.6907 - acc: 0.5488
1088/4849 [=====>........................] - ETA: 5:38 - loss: 0.6878 - acc: 0.5478
1152/4849 [======>.......................] - ETA: 5:32 - loss: 0.6859 - acc: 0.5503
1216/4849 [======>.......................] - ETA: 5:26 - loss: 0.6885 - acc: 0.5502
1280/4849 [======>.......................] - ETA: 5:20 - loss: 0.6878 - acc: 0.5523
1344/4849 [=======>......................] - ETA: 5:15 - loss: 0.6898 - acc: 0.5476
1408/4849 [=======>......................] - ETA: 5:09 - loss: 0.6916 - acc: 0.5476
1472/4849 [========>.....................] - ETA: 5:03 - loss: 0.6918 - acc: 0.5462
1536/4849 [========>.....................] - ETA: 4:58 - loss: 0.6911 - acc: 0.5456
1600/4849 [========>.....................] - ETA: 4:53 - loss: 0.6916 - acc: 0.5450
1664/4849 [=========>....................] - ETA: 4:46 - loss: 0.6926 - acc: 0.5463
1728/4849 [=========>....................] - ETA: 4:40 - loss: 0.6935 - acc: 0.5446
1792/4849 [==========>...................] - ETA: 4:35 - loss: 0.6942 - acc: 0.5441
1856/4849 [==========>...................] - ETA: 4:29 - loss: 0.6945 - acc: 0.5436
1920/4849 [==========>...................] - ETA: 4:23 - loss: 0.6932 - acc: 0.5474
1984/4849 [===========>..................] - ETA: 4:16 - loss: 0.6933 - acc: 0.5484
2048/4849 [===========>..................] - ETA: 4:10 - loss: 0.6938 - acc: 0.5483
2112/4849 [============>.................] - ETA: 4:04 - loss: 0.6941 - acc: 0.5478
2176/4849 [============>.................] - ETA: 3:58 - loss: 0.6940 - acc: 0.5473
2240/4849 [============>.................] - ETA: 3:51 - loss: 0.6949 - acc: 0.5469
2304/4849 [=============>................] - ETA: 3:45 - loss: 0.6946 - acc: 0.5477
2368/4849 [=============>................] - ETA: 3:39 - loss: 0.6951 - acc: 0.5473
2432/4849 [==============>...............] - ETA: 3:33 - loss: 0.6945 - acc: 0.5469
2496/4849 [==============>...............] - ETA: 3:27 - loss: 0.6950 - acc: 0.5473
2560/4849 [==============>...............] - ETA: 3:22 - loss: 0.6967 - acc: 0.5437
2624/4849 [===============>..............] - ETA: 3:16 - loss: 0.6960 - acc: 0.5438
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.6956 - acc: 0.5454
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.6951 - acc: 0.5454
2816/4849 [================>.............] - ETA: 2:58 - loss: 0.6968 - acc: 0.5405
2880/4849 [================>.............] - ETA: 2:52 - loss: 0.6968 - acc: 0.5403
2944/4849 [=================>............] - ETA: 2:47 - loss: 0.6962 - acc: 0.5421
3008/4849 [=================>............] - ETA: 2:41 - loss: 0.6952 - acc: 0.5436
3072/4849 [==================>...........] - ETA: 2:35 - loss: 0.6947 - acc: 0.5446
3136/4849 [==================>...........] - ETA: 2:29 - loss: 0.6948 - acc: 0.5437
3200/4849 [==================>...........] - ETA: 2:23 - loss: 0.6953 - acc: 0.5428
3264/4849 [===================>..........] - ETA: 2:18 - loss: 0.6945 - acc: 0.5432
3328/4849 [===================>..........] - ETA: 2:12 - loss: 0.6946 - acc: 0.5430
3392/4849 [===================>..........] - ETA: 2:06 - loss: 0.6954 - acc: 0.5404
3456/4849 [====================>.........] - ETA: 2:01 - loss: 0.6956 - acc: 0.5414
3520/4849 [====================>.........] - ETA: 1:55 - loss: 0.6949 - acc: 0.5432
3584/4849 [=====================>........] - ETA: 1:49 - loss: 0.6946 - acc: 0.5432
3648/4849 [=====================>........] - ETA: 1:44 - loss: 0.6939 - acc: 0.5447
3712/4849 [=====================>........] - ETA: 1:38 - loss: 0.6940 - acc: 0.5436
3776/4849 [======================>.......] - ETA: 1:32 - loss: 0.6929 - acc: 0.5458
3840/4849 [======================>.......] - ETA: 1:27 - loss: 0.6926 - acc: 0.5461
3904/4849 [=======================>......] - ETA: 1:21 - loss: 0.6932 - acc: 0.5453
3968/4849 [=======================>......] - ETA: 1:15 - loss: 0.6937 - acc: 0.5441
4032/4849 [=======================>......] - ETA: 1:10 - loss: 0.6943 - acc: 0.5427
4096/4849 [========================>.....] - ETA: 1:04 - loss: 0.6943 - acc: 0.5435
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6939 - acc: 0.5437 
4224/4849 [=========================>....] - ETA: 53s - loss: 0.6944 - acc: 0.5419
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6942 - acc: 0.5415
4352/4849 [=========================>....] - ETA: 42s - loss: 0.6949 - acc: 0.5395
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6947 - acc: 0.5401
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6949 - acc: 0.5388
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6946 - acc: 0.5396
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6948 - acc: 0.5388
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6948 - acc: 0.5390
4736/4849 [============================>.] - ETA: 9s - loss: 0.6952 - acc: 0.5378 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6946 - acc: 0.5385
4849/4849 [==============================] - 430s 89ms/step - loss: 0.6945 - acc: 0.5383 - val_loss: 0.6927 - val_acc: 0.5510

Epoch 00002: val_acc improved from 0.53061 to 0.55102, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window10/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 3/10

  64/4849 [..............................] - ETA: 6:39 - loss: 0.7057 - acc: 0.5156
 128/4849 [..............................] - ETA: 6:42 - loss: 0.7084 - acc: 0.5156
 192/4849 [>.............................] - ETA: 6:32 - loss: 0.7118 - acc: 0.4844
 256/4849 [>.............................] - ETA: 6:24 - loss: 0.7122 - acc: 0.4883
 320/4849 [>.............................] - ETA: 6:25 - loss: 0.7094 - acc: 0.4969
 384/4849 [=>............................] - ETA: 6:08 - loss: 0.7081 - acc: 0.5052
 448/4849 [=>............................] - ETA: 5:56 - loss: 0.7078 - acc: 0.5067
 512/4849 [==>...........................] - ETA: 5:42 - loss: 0.7052 - acc: 0.5117
 576/4849 [==>...........................] - ETA: 5:36 - loss: 0.7056 - acc: 0.5104
 640/4849 [==>...........................] - ETA: 5:25 - loss: 0.7052 - acc: 0.5141
 704/4849 [===>..........................] - ETA: 5:16 - loss: 0.7008 - acc: 0.5199
 768/4849 [===>..........................] - ETA: 5:10 - loss: 0.7002 - acc: 0.5234
 832/4849 [====>.........................] - ETA: 5:03 - loss: 0.6986 - acc: 0.5192
 896/4849 [====>.........................] - ETA: 4:57 - loss: 0.6990 - acc: 0.5212
 960/4849 [====>.........................] - ETA: 4:52 - loss: 0.6963 - acc: 0.5250
1024/4849 [=====>........................] - ETA: 4:48 - loss: 0.6960 - acc: 0.5264
1088/4849 [=====>........................] - ETA: 4:41 - loss: 0.6940 - acc: 0.5312
1152/4849 [======>.......................] - ETA: 4:37 - loss: 0.6934 - acc: 0.5321
1216/4849 [======>.......................] - ETA: 4:33 - loss: 0.6923 - acc: 0.5329
1280/4849 [======>.......................] - ETA: 4:27 - loss: 0.6919 - acc: 0.5375
1344/4849 [=======>......................] - ETA: 4:23 - loss: 0.6902 - acc: 0.5439
1408/4849 [=======>......................] - ETA: 4:18 - loss: 0.6932 - acc: 0.5384
1472/4849 [========>.....................] - ETA: 4:12 - loss: 0.6941 - acc: 0.5387
1536/4849 [========>.....................] - ETA: 4:06 - loss: 0.6949 - acc: 0.5378
1600/4849 [========>.....................] - ETA: 4:00 - loss: 0.6942 - acc: 0.5413
1664/4849 [=========>....................] - ETA: 3:55 - loss: 0.6962 - acc: 0.5403
1728/4849 [=========>....................] - ETA: 3:50 - loss: 0.6956 - acc: 0.5422
1792/4849 [==========>...................] - ETA: 3:45 - loss: 0.6949 - acc: 0.5446
1856/4849 [==========>...................] - ETA: 3:40 - loss: 0.6952 - acc: 0.5458
1920/4849 [==========>...................] - ETA: 3:35 - loss: 0.6956 - acc: 0.5464
1984/4849 [===========>..................] - ETA: 3:29 - loss: 0.6960 - acc: 0.5454
2048/4849 [===========>..................] - ETA: 3:24 - loss: 0.6957 - acc: 0.5459
2112/4849 [============>.................] - ETA: 3:19 - loss: 0.6969 - acc: 0.5445
2176/4849 [============>.................] - ETA: 3:14 - loss: 0.6962 - acc: 0.5450
2240/4849 [============>.................] - ETA: 3:09 - loss: 0.6949 - acc: 0.5469
2304/4849 [=============>................] - ETA: 3:05 - loss: 0.6944 - acc: 0.5464
2368/4849 [=============>................] - ETA: 3:00 - loss: 0.6939 - acc: 0.5456
2432/4849 [==============>...............] - ETA: 2:55 - loss: 0.6931 - acc: 0.5469
2496/4849 [==============>...............] - ETA: 2:50 - loss: 0.6927 - acc: 0.5481
2560/4849 [==============>...............] - ETA: 2:45 - loss: 0.6928 - acc: 0.5477
2624/4849 [===============>..............] - ETA: 2:41 - loss: 0.6930 - acc: 0.5461
2688/4849 [===============>..............] - ETA: 2:36 - loss: 0.6944 - acc: 0.5439
2752/4849 [================>.............] - ETA: 2:32 - loss: 0.6948 - acc: 0.5425
2816/4849 [================>.............] - ETA: 2:27 - loss: 0.6947 - acc: 0.5451
2880/4849 [================>.............] - ETA: 2:22 - loss: 0.6951 - acc: 0.5448
2944/4849 [=================>............] - ETA: 2:17 - loss: 0.6940 - acc: 0.5469
3008/4849 [=================>............] - ETA: 2:13 - loss: 0.6938 - acc: 0.5475
3072/4849 [==================>...........] - ETA: 2:08 - loss: 0.6934 - acc: 0.5482
3136/4849 [==================>...........] - ETA: 2:03 - loss: 0.6930 - acc: 0.5491
3200/4849 [==================>...........] - ETA: 1:59 - loss: 0.6936 - acc: 0.5481
3264/4849 [===================>..........] - ETA: 1:54 - loss: 0.6925 - acc: 0.5502
3328/4849 [===================>..........] - ETA: 1:49 - loss: 0.6920 - acc: 0.5514
3392/4849 [===================>..........] - ETA: 1:45 - loss: 0.6915 - acc: 0.5540
3456/4849 [====================>.........] - ETA: 1:40 - loss: 0.6909 - acc: 0.5556
3520/4849 [====================>.........] - ETA: 1:35 - loss: 0.6912 - acc: 0.5545
3584/4849 [=====================>........] - ETA: 1:30 - loss: 0.6911 - acc: 0.5530
3648/4849 [=====================>........] - ETA: 1:26 - loss: 0.6911 - acc: 0.5543
3712/4849 [=====================>........] - ETA: 1:21 - loss: 0.6913 - acc: 0.5536
3776/4849 [======================>.......] - ETA: 1:17 - loss: 0.6910 - acc: 0.5543
3840/4849 [======================>.......] - ETA: 1:12 - loss: 0.6913 - acc: 0.5539
3904/4849 [=======================>......] - ETA: 1:08 - loss: 0.6908 - acc: 0.5548
3968/4849 [=======================>......] - ETA: 1:04 - loss: 0.6909 - acc: 0.5552
4032/4849 [=======================>......] - ETA: 59s - loss: 0.6905 - acc: 0.5563 
4096/4849 [========================>.....] - ETA: 55s - loss: 0.6905 - acc: 0.5564
4160/4849 [========================>.....] - ETA: 50s - loss: 0.6906 - acc: 0.5563
4224/4849 [=========================>....] - ETA: 46s - loss: 0.6901 - acc: 0.5573
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6900 - acc: 0.5576
4352/4849 [=========================>....] - ETA: 36s - loss: 0.6897 - acc: 0.5574
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6891 - acc: 0.5580
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6890 - acc: 0.5583
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6891 - acc: 0.5572
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6886 - acc: 0.5582
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6889 - acc: 0.5582
4736/4849 [============================>.] - ETA: 8s - loss: 0.6889 - acc: 0.5576 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6895 - acc: 0.5571
4849/4849 [==============================] - 382s 79ms/step - loss: 0.6890 - acc: 0.5581 - val_loss: 0.6776 - val_acc: 0.5622

Epoch 00003: val_acc improved from 0.55102 to 0.56215, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window10/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 4/10

  64/4849 [..............................] - ETA: 6:18 - loss: 0.6909 - acc: 0.5312
 128/4849 [..............................] - ETA: 6:40 - loss: 0.6936 - acc: 0.5312
 192/4849 [>.............................] - ETA: 6:47 - loss: 0.6809 - acc: 0.5573
 256/4849 [>.............................] - ETA: 6:37 - loss: 0.6883 - acc: 0.5508
 320/4849 [>.............................] - ETA: 6:26 - loss: 0.6877 - acc: 0.5500
 384/4849 [=>............................] - ETA: 6:22 - loss: 0.6841 - acc: 0.5677
 448/4849 [=>............................] - ETA: 6:14 - loss: 0.6846 - acc: 0.5737
 512/4849 [==>...........................] - ETA: 6:10 - loss: 0.6887 - acc: 0.5625
 576/4849 [==>...........................] - ETA: 6:06 - loss: 0.6931 - acc: 0.5538
 640/4849 [==>...........................] - ETA: 5:59 - loss: 0.6910 - acc: 0.5516
 704/4849 [===>..........................] - ETA: 5:51 - loss: 0.6914 - acc: 0.5469
 768/4849 [===>..........................] - ETA: 5:43 - loss: 0.6874 - acc: 0.5586
 832/4849 [====>.........................] - ETA: 5:33 - loss: 0.6827 - acc: 0.5685
 896/4849 [====>.........................] - ETA: 5:26 - loss: 0.6832 - acc: 0.5692
 960/4849 [====>.........................] - ETA: 5:19 - loss: 0.6833 - acc: 0.5687
1024/4849 [=====>........................] - ETA: 5:13 - loss: 0.6832 - acc: 0.5703
1088/4849 [=====>........................] - ETA: 5:05 - loss: 0.6833 - acc: 0.5689
1152/4849 [======>.......................] - ETA: 4:58 - loss: 0.6835 - acc: 0.5712
1216/4849 [======>.......................] - ETA: 4:53 - loss: 0.6842 - acc: 0.5707
1280/4849 [======>.......................] - ETA: 4:48 - loss: 0.6834 - acc: 0.5703
1344/4849 [=======>......................] - ETA: 4:42 - loss: 0.6835 - acc: 0.5699
1408/4849 [=======>......................] - ETA: 4:36 - loss: 0.6804 - acc: 0.5739
1472/4849 [========>.....................] - ETA: 4:31 - loss: 0.6805 - acc: 0.5734
1536/4849 [========>.....................] - ETA: 4:26 - loss: 0.6809 - acc: 0.5710
1600/4849 [========>.....................] - ETA: 4:20 - loss: 0.6798 - acc: 0.5725
1664/4849 [=========>....................] - ETA: 4:15 - loss: 0.6795 - acc: 0.5739
1728/4849 [=========>....................] - ETA: 4:10 - loss: 0.6791 - acc: 0.5735
1792/4849 [==========>...................] - ETA: 4:05 - loss: 0.6790 - acc: 0.5731
1856/4849 [==========>...................] - ETA: 4:00 - loss: 0.6794 - acc: 0.5706
1920/4849 [==========>...................] - ETA: 3:54 - loss: 0.6804 - acc: 0.5677
1984/4849 [===========>..................] - ETA: 3:49 - loss: 0.6811 - acc: 0.5670
2048/4849 [===========>..................] - ETA: 3:43 - loss: 0.6791 - acc: 0.5718
2112/4849 [============>.................] - ETA: 3:39 - loss: 0.6795 - acc: 0.5729
2176/4849 [============>.................] - ETA: 3:33 - loss: 0.6790 - acc: 0.5740
2240/4849 [============>.................] - ETA: 3:28 - loss: 0.6783 - acc: 0.5741
2304/4849 [=============>................] - ETA: 3:22 - loss: 0.6776 - acc: 0.5747
2368/4849 [=============>................] - ETA: 3:18 - loss: 0.6770 - acc: 0.5764
2432/4849 [==============>...............] - ETA: 3:12 - loss: 0.6762 - acc: 0.5777
2496/4849 [==============>...............] - ETA: 3:08 - loss: 0.6753 - acc: 0.5793
2560/4849 [==============>...............] - ETA: 3:02 - loss: 0.6753 - acc: 0.5785
2624/4849 [===============>..............] - ETA: 2:57 - loss: 0.6750 - acc: 0.5796
2688/4849 [===============>..............] - ETA: 2:52 - loss: 0.6739 - acc: 0.5830
2752/4849 [================>.............] - ETA: 2:47 - loss: 0.6745 - acc: 0.5810
2816/4849 [================>.............] - ETA: 2:41 - loss: 0.6734 - acc: 0.5831
2880/4849 [================>.............] - ETA: 2:36 - loss: 0.6732 - acc: 0.5826
2944/4849 [=================>............] - ETA: 2:31 - loss: 0.6735 - acc: 0.5829
3008/4849 [=================>............] - ETA: 2:26 - loss: 0.6726 - acc: 0.5848
3072/4849 [==================>...........] - ETA: 2:21 - loss: 0.6727 - acc: 0.5846
3136/4849 [==================>...........] - ETA: 2:16 - loss: 0.6746 - acc: 0.5807
3200/4849 [==================>...........] - ETA: 2:11 - loss: 0.6749 - acc: 0.5794
3264/4849 [===================>..........] - ETA: 2:06 - loss: 0.6750 - acc: 0.5803
3328/4849 [===================>..........] - ETA: 2:01 - loss: 0.6749 - acc: 0.5811
3392/4849 [===================>..........] - ETA: 1:56 - loss: 0.6744 - acc: 0.5817
3456/4849 [====================>.........] - ETA: 1:51 - loss: 0.6737 - acc: 0.5836
3520/4849 [====================>.........] - ETA: 1:45 - loss: 0.6735 - acc: 0.5835
3584/4849 [=====================>........] - ETA: 1:40 - loss: 0.6738 - acc: 0.5831
3648/4849 [=====================>........] - ETA: 1:35 - loss: 0.6736 - acc: 0.5836
3712/4849 [=====================>........] - ETA: 1:30 - loss: 0.6743 - acc: 0.5816
3776/4849 [======================>.......] - ETA: 1:25 - loss: 0.6750 - acc: 0.5810
3840/4849 [======================>.......] - ETA: 1:20 - loss: 0.6739 - acc: 0.5826
3904/4849 [=======================>......] - ETA: 1:15 - loss: 0.6743 - acc: 0.5822
3968/4849 [=======================>......] - ETA: 1:10 - loss: 0.6741 - acc: 0.5822
4032/4849 [=======================>......] - ETA: 1:04 - loss: 0.6731 - acc: 0.5836
4096/4849 [========================>.....] - ETA: 59s - loss: 0.6730 - acc: 0.5850 
4160/4849 [========================>.....] - ETA: 54s - loss: 0.6732 - acc: 0.5853
4224/4849 [=========================>....] - ETA: 49s - loss: 0.6737 - acc: 0.5843
4288/4849 [=========================>....] - ETA: 44s - loss: 0.6735 - acc: 0.5844
4352/4849 [=========================>....] - ETA: 39s - loss: 0.6731 - acc: 0.5846
4416/4849 [==========================>...] - ETA: 34s - loss: 0.6728 - acc: 0.5845
4480/4849 [==========================>...] - ETA: 29s - loss: 0.6732 - acc: 0.5835
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6732 - acc: 0.5847
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6728 - acc: 0.5862
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6724 - acc: 0.5871
4736/4849 [============================>.] - ETA: 8s - loss: 0.6724 - acc: 0.5861 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6718 - acc: 0.5875
4849/4849 [==============================] - 402s 83ms/step - loss: 0.6716 - acc: 0.5882 - val_loss: 0.6873 - val_acc: 0.5659

Epoch 00004: val_acc improved from 0.56215 to 0.56586, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window10/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 5/10

  64/4849 [..............................] - ETA: 7:17 - loss: 0.6897 - acc: 0.5469
 128/4849 [..............................] - ETA: 7:02 - loss: 0.6537 - acc: 0.6094
 192/4849 [>.............................] - ETA: 6:55 - loss: 0.6637 - acc: 0.5938
 256/4849 [>.............................] - ETA: 6:46 - loss: 0.6735 - acc: 0.5977
 320/4849 [>.............................] - ETA: 6:52 - loss: 0.6867 - acc: 0.5656
 384/4849 [=>............................] - ETA: 6:45 - loss: 0.6838 - acc: 0.5677
 448/4849 [=>............................] - ETA: 6:40 - loss: 0.6793 - acc: 0.5759
 512/4849 [==>...........................] - ETA: 6:31 - loss: 0.6841 - acc: 0.5664
 576/4849 [==>...........................] - ETA: 6:29 - loss: 0.6787 - acc: 0.5799
 640/4849 [==>...........................] - ETA: 6:21 - loss: 0.6801 - acc: 0.5750
 704/4849 [===>..........................] - ETA: 6:17 - loss: 0.6807 - acc: 0.5710
 768/4849 [===>..........................] - ETA: 6:14 - loss: 0.6742 - acc: 0.5820
 832/4849 [====>.........................] - ETA: 6:06 - loss: 0.6708 - acc: 0.5853
 896/4849 [====>.........................] - ETA: 6:01 - loss: 0.6695 - acc: 0.5915
 960/4849 [====>.........................] - ETA: 5:56 - loss: 0.6730 - acc: 0.5865
1024/4849 [=====>........................] - ETA: 5:50 - loss: 0.6711 - acc: 0.5908
1088/4849 [=====>........................] - ETA: 5:43 - loss: 0.6715 - acc: 0.5901
1152/4849 [======>.......................] - ETA: 5:38 - loss: 0.6708 - acc: 0.5894
1216/4849 [======>.......................] - ETA: 5:32 - loss: 0.6738 - acc: 0.5839
1280/4849 [======>.......................] - ETA: 5:26 - loss: 0.6730 - acc: 0.5844
1344/4849 [=======>......................] - ETA: 5:20 - loss: 0.6736 - acc: 0.5818
1408/4849 [=======>......................] - ETA: 5:13 - loss: 0.6739 - acc: 0.5803
1472/4849 [========>.....................] - ETA: 5:07 - loss: 0.6770 - acc: 0.5774
1536/4849 [========>.....................] - ETA: 5:01 - loss: 0.6766 - acc: 0.5814
1600/4849 [========>.....................] - ETA: 4:55 - loss: 0.6766 - acc: 0.5806
1664/4849 [=========>....................] - ETA: 4:50 - loss: 0.6748 - acc: 0.5829
1728/4849 [=========>....................] - ETA: 4:44 - loss: 0.6768 - acc: 0.5775
1792/4849 [==========>...................] - ETA: 4:37 - loss: 0.6761 - acc: 0.5798
1856/4849 [==========>...................] - ETA: 4:32 - loss: 0.6761 - acc: 0.5792
1920/4849 [==========>...................] - ETA: 4:26 - loss: 0.6764 - acc: 0.5776
1984/4849 [===========>..................] - ETA: 4:20 - loss: 0.6750 - acc: 0.5817
2048/4849 [===========>..................] - ETA: 4:14 - loss: 0.6745 - acc: 0.5835
2112/4849 [============>.................] - ETA: 4:08 - loss: 0.6730 - acc: 0.5866
2176/4849 [============>.................] - ETA: 4:02 - loss: 0.6723 - acc: 0.5878
2240/4849 [============>.................] - ETA: 3:56 - loss: 0.6732 - acc: 0.5866
2304/4849 [=============>................] - ETA: 3:51 - loss: 0.6741 - acc: 0.5846
2368/4849 [=============>................] - ETA: 3:44 - loss: 0.6750 - acc: 0.5849
2432/4849 [==============>...............] - ETA: 3:39 - loss: 0.6738 - acc: 0.5863
2496/4849 [==============>...............] - ETA: 3:33 - loss: 0.6730 - acc: 0.5873
2560/4849 [==============>...............] - ETA: 3:27 - loss: 0.6734 - acc: 0.5863
2624/4849 [===============>..............] - ETA: 3:22 - loss: 0.6733 - acc: 0.5861
2688/4849 [===============>..............] - ETA: 3:16 - loss: 0.6721 - acc: 0.5874
2752/4849 [================>.............] - ETA: 3:10 - loss: 0.6718 - acc: 0.5894
2816/4849 [================>.............] - ETA: 3:04 - loss: 0.6721 - acc: 0.5888
2880/4849 [================>.............] - ETA: 2:59 - loss: 0.6730 - acc: 0.5872
2944/4849 [=================>............] - ETA: 2:53 - loss: 0.6730 - acc: 0.5870
3008/4849 [=================>............] - ETA: 2:47 - loss: 0.6745 - acc: 0.5858
3072/4849 [==================>...........] - ETA: 2:41 - loss: 0.6744 - acc: 0.5850
3136/4849 [==================>...........] - ETA: 2:35 - loss: 0.6744 - acc: 0.5842
3200/4849 [==================>...........] - ETA: 2:29 - loss: 0.6733 - acc: 0.5856
3264/4849 [===================>..........] - ETA: 2:23 - loss: 0.6737 - acc: 0.5849
3328/4849 [===================>..........] - ETA: 2:18 - loss: 0.6746 - acc: 0.5823
3392/4849 [===================>..........] - ETA: 2:12 - loss: 0.6746 - acc: 0.5825
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.6746 - acc: 0.5825
3520/4849 [====================>.........] - ETA: 2:00 - loss: 0.6749 - acc: 0.5818
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6751 - acc: 0.5815
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.6748 - acc: 0.5817
3712/4849 [=====================>........] - ETA: 1:43 - loss: 0.6745 - acc: 0.5827
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6742 - acc: 0.5829
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6745 - acc: 0.5828
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6740 - acc: 0.5835
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6737 - acc: 0.5839
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6737 - acc: 0.5833
4096/4849 [========================>.....] - ETA: 1:08 - loss: 0.6740 - acc: 0.5837
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6731 - acc: 0.5851
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6726 - acc: 0.5859 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6728 - acc: 0.5856
4352/4849 [=========================>....] - ETA: 45s - loss: 0.6729 - acc: 0.5862
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6729 - acc: 0.5865
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6730 - acc: 0.5866
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6727 - acc: 0.5874
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6726 - acc: 0.5872
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6723 - acc: 0.5886
4736/4849 [============================>.] - ETA: 10s - loss: 0.6721 - acc: 0.5891
4800/4849 [============================>.] - ETA: 4s - loss: 0.6726 - acc: 0.5877 
4849/4849 [==============================] - 457s 94ms/step - loss: 0.6730 - acc: 0.5867 - val_loss: 0.6798 - val_acc: 0.5751

Epoch 00005: val_acc improved from 0.56586 to 0.57514, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window10/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 6/10

  64/4849 [..............................] - ETA: 7:01 - loss: 0.6693 - acc: 0.5938
 128/4849 [..............................] - ETA: 6:57 - loss: 0.6515 - acc: 0.6328
 192/4849 [>.............................] - ETA: 6:55 - loss: 0.6569 - acc: 0.6094
 256/4849 [>.............................] - ETA: 6:54 - loss: 0.6564 - acc: 0.6094
 320/4849 [>.............................] - ETA: 6:53 - loss: 0.6575 - acc: 0.6094
 384/4849 [=>............................] - ETA: 6:50 - loss: 0.6559 - acc: 0.6068
 448/4849 [=>............................] - ETA: 6:46 - loss: 0.6597 - acc: 0.6027
 512/4849 [==>...........................] - ETA: 6:35 - loss: 0.6618 - acc: 0.5996
 576/4849 [==>...........................] - ETA: 6:30 - loss: 0.6617 - acc: 0.6007
 640/4849 [==>...........................] - ETA: 6:21 - loss: 0.6626 - acc: 0.6000
 704/4849 [===>..........................] - ETA: 6:14 - loss: 0.6592 - acc: 0.6037
 768/4849 [===>..........................] - ETA: 6:11 - loss: 0.6610 - acc: 0.5964
 832/4849 [====>.........................] - ETA: 6:06 - loss: 0.6593 - acc: 0.6010
 896/4849 [====>.........................] - ETA: 6:00 - loss: 0.6608 - acc: 0.6004
 960/4849 [====>.........................] - ETA: 5:53 - loss: 0.6577 - acc: 0.6052
1024/4849 [=====>........................] - ETA: 5:47 - loss: 0.6600 - acc: 0.6016
1088/4849 [=====>........................] - ETA: 5:41 - loss: 0.6600 - acc: 0.6020
1152/4849 [======>.......................] - ETA: 5:36 - loss: 0.6607 - acc: 0.6016
1216/4849 [======>.......................] - ETA: 5:30 - loss: 0.6588 - acc: 0.6061
1280/4849 [======>.......................] - ETA: 5:24 - loss: 0.6606 - acc: 0.6039
1344/4849 [=======>......................] - ETA: 5:18 - loss: 0.6603 - acc: 0.6049
1408/4849 [=======>......................] - ETA: 5:12 - loss: 0.6628 - acc: 0.6037
1472/4849 [========>.....................] - ETA: 5:07 - loss: 0.6642 - acc: 0.5992
1536/4849 [========>.....................] - ETA: 5:00 - loss: 0.6652 - acc: 0.5964
1600/4849 [========>.....................] - ETA: 4:55 - loss: 0.6635 - acc: 0.5975
1664/4849 [=========>....................] - ETA: 4:49 - loss: 0.6655 - acc: 0.5907
1728/4849 [=========>....................] - ETA: 4:43 - loss: 0.6641 - acc: 0.5949
1792/4849 [==========>...................] - ETA: 4:36 - loss: 0.6646 - acc: 0.5965
1856/4849 [==========>...................] - ETA: 4:30 - loss: 0.6649 - acc: 0.5970
1920/4849 [==========>...................] - ETA: 4:24 - loss: 0.6654 - acc: 0.5958
1984/4849 [===========>..................] - ETA: 4:18 - loss: 0.6645 - acc: 0.5958
2048/4849 [===========>..................] - ETA: 4:12 - loss: 0.6638 - acc: 0.5986
2112/4849 [============>.................] - ETA: 4:06 - loss: 0.6652 - acc: 0.5961
2176/4849 [============>.................] - ETA: 3:59 - loss: 0.6648 - acc: 0.5956
2240/4849 [============>.................] - ETA: 3:54 - loss: 0.6644 - acc: 0.5973
2304/4849 [=============>................] - ETA: 3:48 - loss: 0.6644 - acc: 0.5968
2368/4849 [=============>................] - ETA: 3:42 - loss: 0.6654 - acc: 0.5954
2432/4849 [==============>...............] - ETA: 3:36 - loss: 0.6652 - acc: 0.5946
2496/4849 [==============>...............] - ETA: 3:31 - loss: 0.6667 - acc: 0.5921
2560/4849 [==============>...............] - ETA: 3:25 - loss: 0.6679 - acc: 0.5910
2624/4849 [===============>..............] - ETA: 3:19 - loss: 0.6678 - acc: 0.5915
2688/4849 [===============>..............] - ETA: 3:13 - loss: 0.6687 - acc: 0.5897
2752/4849 [================>.............] - ETA: 3:07 - loss: 0.6685 - acc: 0.5879
2816/4849 [================>.............] - ETA: 3:01 - loss: 0.6693 - acc: 0.5870
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6685 - acc: 0.5896
2944/4849 [=================>............] - ETA: 2:49 - loss: 0.6686 - acc: 0.5887
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6688 - acc: 0.5878
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6682 - acc: 0.5889
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6687 - acc: 0.5890
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6694 - acc: 0.5872
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6704 - acc: 0.5858
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6705 - acc: 0.5856
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6705 - acc: 0.5852
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6713 - acc: 0.5833
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6712 - acc: 0.5835
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6708 - acc: 0.5843
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6717 - acc: 0.5831
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6727 - acc: 0.5827
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6728 - acc: 0.5821
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6726 - acc: 0.5828
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6730 - acc: 0.5815
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6729 - acc: 0.5824
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6721 - acc: 0.5848
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6716 - acc: 0.5852
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6717 - acc: 0.5844
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6722 - acc: 0.5836 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6721 - acc: 0.5837
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6728 - acc: 0.5820
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6722 - acc: 0.5838
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6722 - acc: 0.5830
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6732 - acc: 0.5816
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6728 - acc: 0.5818
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6723 - acc: 0.5818
4736/4849 [============================>.] - ETA: 9s - loss: 0.6721 - acc: 0.5830 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6716 - acc: 0.5840
4849/4849 [==============================] - 443s 91ms/step - loss: 0.6711 - acc: 0.5851 - val_loss: 0.6877 - val_acc: 0.5659

Epoch 00006: val_acc did not improve from 0.57514
Epoch 7/10

  64/4849 [..............................] - ETA: 6:36 - loss: 0.6153 - acc: 0.6562
 128/4849 [..............................] - ETA: 6:50 - loss: 0.6378 - acc: 0.6484
 192/4849 [>.............................] - ETA: 6:25 - loss: 0.6582 - acc: 0.6198
 256/4849 [>.............................] - ETA: 6:26 - loss: 0.6622 - acc: 0.6094
 320/4849 [>.............................] - ETA: 6:23 - loss: 0.6595 - acc: 0.6250
 384/4849 [=>............................] - ETA: 6:15 - loss: 0.6665 - acc: 0.6198
 448/4849 [=>............................] - ETA: 6:11 - loss: 0.6762 - acc: 0.6071
 512/4849 [==>...........................] - ETA: 6:13 - loss: 0.6761 - acc: 0.5996
 576/4849 [==>...........................] - ETA: 6:08 - loss: 0.6736 - acc: 0.6076
 640/4849 [==>...........................] - ETA: 6:04 - loss: 0.6740 - acc: 0.6062
 704/4849 [===>..........................] - ETA: 6:00 - loss: 0.6726 - acc: 0.6108
 768/4849 [===>..........................] - ETA: 5:54 - loss: 0.6737 - acc: 0.6055
 832/4849 [====>.........................] - ETA: 5:50 - loss: 0.6716 - acc: 0.6094
 896/4849 [====>.........................] - ETA: 5:45 - loss: 0.6750 - acc: 0.6016
 960/4849 [====>.........................] - ETA: 5:38 - loss: 0.6743 - acc: 0.6000
1024/4849 [=====>........................] - ETA: 5:33 - loss: 0.6750 - acc: 0.5967
1088/4849 [=====>........................] - ETA: 5:29 - loss: 0.6750 - acc: 0.5965
1152/4849 [======>.......................] - ETA: 5:22 - loss: 0.6708 - acc: 0.6007
1216/4849 [======>.......................] - ETA: 5:18 - loss: 0.6711 - acc: 0.5995
1280/4849 [======>.......................] - ETA: 5:12 - loss: 0.6681 - acc: 0.6008
1344/4849 [=======>......................] - ETA: 5:07 - loss: 0.6681 - acc: 0.6004
1408/4849 [=======>......................] - ETA: 5:01 - loss: 0.6671 - acc: 0.6009
1472/4849 [========>.....................] - ETA: 4:56 - loss: 0.6663 - acc: 0.6026
1536/4849 [========>.....................] - ETA: 4:51 - loss: 0.6658 - acc: 0.6035
1600/4849 [========>.....................] - ETA: 4:45 - loss: 0.6672 - acc: 0.6006
1664/4849 [=========>....................] - ETA: 4:39 - loss: 0.6656 - acc: 0.6022
1728/4849 [=========>....................] - ETA: 4:34 - loss: 0.6664 - acc: 0.6007
1792/4849 [==========>...................] - ETA: 4:28 - loss: 0.6691 - acc: 0.5949
1856/4849 [==========>...................] - ETA: 4:22 - loss: 0.6681 - acc: 0.5981
1920/4849 [==========>...................] - ETA: 4:17 - loss: 0.6670 - acc: 0.6005
1984/4849 [===========>..................] - ETA: 4:12 - loss: 0.6637 - acc: 0.6043
2048/4849 [===========>..................] - ETA: 4:06 - loss: 0.6636 - acc: 0.6035
2112/4849 [============>.................] - ETA: 4:01 - loss: 0.6629 - acc: 0.6046
2176/4849 [============>.................] - ETA: 3:56 - loss: 0.6652 - acc: 0.6006
2240/4849 [============>.................] - ETA: 3:49 - loss: 0.6659 - acc: 0.5991
2304/4849 [=============>................] - ETA: 3:45 - loss: 0.6651 - acc: 0.6003
2368/4849 [=============>................] - ETA: 3:39 - loss: 0.6648 - acc: 0.6005
2432/4849 [==============>...............] - ETA: 3:33 - loss: 0.6647 - acc: 0.6007
2496/4849 [==============>...............] - ETA: 3:28 - loss: 0.6639 - acc: 0.6022
2560/4849 [==============>...............] - ETA: 3:23 - loss: 0.6653 - acc: 0.5996
2624/4849 [===============>..............] - ETA: 3:17 - loss: 0.6664 - acc: 0.5964
2688/4849 [===============>..............] - ETA: 3:11 - loss: 0.6657 - acc: 0.5986
2752/4849 [================>.............] - ETA: 3:05 - loss: 0.6652 - acc: 0.5996
2816/4849 [================>.............] - ETA: 3:00 - loss: 0.6651 - acc: 0.5994
2880/4849 [================>.............] - ETA: 2:54 - loss: 0.6649 - acc: 0.6000
2944/4849 [=================>............] - ETA: 2:48 - loss: 0.6644 - acc: 0.6002
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6648 - acc: 0.5994
3072/4849 [==================>...........] - ETA: 2:37 - loss: 0.6653 - acc: 0.5993
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6649 - acc: 0.5985
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6650 - acc: 0.5981
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6662 - acc: 0.5965
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6666 - acc: 0.5962
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6666 - acc: 0.5970
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6661 - acc: 0.5975
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6676 - acc: 0.5955
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6679 - acc: 0.5946
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6683 - acc: 0.5935
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6685 - acc: 0.5929
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6673 - acc: 0.5948
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6673 - acc: 0.5953
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6668 - acc: 0.5966
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6667 - acc: 0.5955
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6671 - acc: 0.5950
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6666 - acc: 0.5955
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6676 - acc: 0.5945
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6683 - acc: 0.5930 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6687 - acc: 0.5933
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6687 - acc: 0.5942
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6694 - acc: 0.5940
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6694 - acc: 0.5940
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6698 - acc: 0.5926
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6699 - acc: 0.5924
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6700 - acc: 0.5916
4736/4849 [============================>.] - ETA: 10s - loss: 0.6694 - acc: 0.5925
4800/4849 [============================>.] - ETA: 4s - loss: 0.6696 - acc: 0.5925 
4849/4849 [==============================] - 447s 92ms/step - loss: 0.6697 - acc: 0.5921 - val_loss: 0.6915 - val_acc: 0.5659

Epoch 00007: val_acc did not improve from 0.57514
Epoch 8/10

  64/4849 [..............................] - ETA: 6:46 - loss: 0.6552 - acc: 0.5938
 128/4849 [..............................] - ETA: 7:07 - loss: 0.6589 - acc: 0.6016
 192/4849 [>.............................] - ETA: 6:51 - loss: 0.6576 - acc: 0.5938
 256/4849 [>.............................] - ETA: 6:56 - loss: 0.6712 - acc: 0.5703
 320/4849 [>.............................] - ETA: 6:50 - loss: 0.6721 - acc: 0.5656
 384/4849 [=>............................] - ETA: 6:40 - loss: 0.6681 - acc: 0.5781
 448/4849 [=>............................] - ETA: 6:36 - loss: 0.6695 - acc: 0.5848
 512/4849 [==>...........................] - ETA: 6:26 - loss: 0.6696 - acc: 0.5879
 576/4849 [==>...........................] - ETA: 6:19 - loss: 0.6728 - acc: 0.5764
 640/4849 [==>...........................] - ETA: 6:16 - loss: 0.6725 - acc: 0.5813
 704/4849 [===>..........................] - ETA: 6:10 - loss: 0.6735 - acc: 0.5881
 768/4849 [===>..........................] - ETA: 6:01 - loss: 0.6733 - acc: 0.5898
 832/4849 [====>.........................] - ETA: 5:56 - loss: 0.6730 - acc: 0.5938
 896/4849 [====>.........................] - ETA: 5:49 - loss: 0.6747 - acc: 0.5893
 960/4849 [====>.........................] - ETA: 5:43 - loss: 0.6712 - acc: 0.5948
1024/4849 [=====>........................] - ETA: 5:38 - loss: 0.6726 - acc: 0.5928
1088/4849 [=====>........................] - ETA: 5:33 - loss: 0.6714 - acc: 0.5956
1152/4849 [======>.......................] - ETA: 5:25 - loss: 0.6713 - acc: 0.5946
1216/4849 [======>.......................] - ETA: 5:22 - loss: 0.6715 - acc: 0.5938
1280/4849 [======>.......................] - ETA: 5:16 - loss: 0.6720 - acc: 0.5883
1344/4849 [=======>......................] - ETA: 5:10 - loss: 0.6707 - acc: 0.5893
1408/4849 [=======>......................] - ETA: 5:05 - loss: 0.6708 - acc: 0.5902
1472/4849 [========>.....................] - ETA: 4:59 - loss: 0.6690 - acc: 0.5931
1536/4849 [========>.....................] - ETA: 4:53 - loss: 0.6680 - acc: 0.5951
1600/4849 [========>.....................] - ETA: 4:47 - loss: 0.6684 - acc: 0.5962
1664/4849 [=========>....................] - ETA: 4:42 - loss: 0.6694 - acc: 0.5950
1728/4849 [=========>....................] - ETA: 4:36 - loss: 0.6691 - acc: 0.5972
1792/4849 [==========>...................] - ETA: 4:30 - loss: 0.6685 - acc: 0.5999
1856/4849 [==========>...................] - ETA: 4:25 - loss: 0.6660 - acc: 0.6034
1920/4849 [==========>...................] - ETA: 4:19 - loss: 0.6669 - acc: 0.6036
1984/4849 [===========>..................] - ETA: 4:14 - loss: 0.6660 - acc: 0.6064
2048/4849 [===========>..................] - ETA: 4:08 - loss: 0.6645 - acc: 0.6084
2112/4849 [============>.................] - ETA: 4:03 - loss: 0.6663 - acc: 0.6061
2176/4849 [============>.................] - ETA: 3:57 - loss: 0.6666 - acc: 0.6057
2240/4849 [============>.................] - ETA: 3:52 - loss: 0.6671 - acc: 0.6071
2304/4849 [=============>................] - ETA: 3:46 - loss: 0.6678 - acc: 0.6072
2368/4849 [=============>................] - ETA: 3:40 - loss: 0.6664 - acc: 0.6073
2432/4849 [==============>...............] - ETA: 3:34 - loss: 0.6667 - acc: 0.6065
2496/4849 [==============>...............] - ETA: 3:29 - loss: 0.6660 - acc: 0.6078
2560/4849 [==============>...............] - ETA: 3:22 - loss: 0.6669 - acc: 0.6059
2624/4849 [===============>..............] - ETA: 3:17 - loss: 0.6677 - acc: 0.6048
2688/4849 [===============>..............] - ETA: 3:11 - loss: 0.6679 - acc: 0.6038
2752/4849 [================>.............] - ETA: 3:05 - loss: 0.6678 - acc: 0.6039
2816/4849 [================>.............] - ETA: 3:00 - loss: 0.6682 - acc: 0.6030
2880/4849 [================>.............] - ETA: 2:54 - loss: 0.6678 - acc: 0.6024
2944/4849 [=================>............] - ETA: 2:48 - loss: 0.6687 - acc: 0.6022
3008/4849 [=================>............] - ETA: 2:42 - loss: 0.6664 - acc: 0.6054
3072/4849 [==================>...........] - ETA: 2:36 - loss: 0.6664 - acc: 0.6045
3136/4849 [==================>...........] - ETA: 2:30 - loss: 0.6665 - acc: 0.6046
3200/4849 [==================>...........] - ETA: 2:25 - loss: 0.6664 - acc: 0.6056
3264/4849 [===================>..........] - ETA: 2:19 - loss: 0.6666 - acc: 0.6057
3328/4849 [===================>..........] - ETA: 2:13 - loss: 0.6672 - acc: 0.6046
3392/4849 [===================>..........] - ETA: 2:08 - loss: 0.6674 - acc: 0.6038
3456/4849 [====================>.........] - ETA: 2:02 - loss: 0.6671 - acc: 0.6045
3520/4849 [====================>.........] - ETA: 1:56 - loss: 0.6677 - acc: 0.6040
3584/4849 [=====================>........] - ETA: 1:51 - loss: 0.6681 - acc: 0.6038
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6673 - acc: 0.6061
3712/4849 [=====================>........] - ETA: 1:39 - loss: 0.6661 - acc: 0.6075
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6665 - acc: 0.6065
3840/4849 [======================>.......] - ETA: 1:28 - loss: 0.6668 - acc: 0.6057
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6666 - acc: 0.6058
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6675 - acc: 0.6033
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6666 - acc: 0.6044
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6666 - acc: 0.6042
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6674 - acc: 0.6036
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6675 - acc: 0.6037 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6679 - acc: 0.6026
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6678 - acc: 0.6036
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6677 - acc: 0.6039
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6671 - acc: 0.6051
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6671 - acc: 0.6048
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6667 - acc: 0.6050
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6670 - acc: 0.6049
4736/4849 [============================>.] - ETA: 9s - loss: 0.6664 - acc: 0.6056 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6661 - acc: 0.6067
4849/4849 [==============================] - 442s 91ms/step - loss: 0.6661 - acc: 0.6055 - val_loss: 0.6923 - val_acc: 0.5584

Epoch 00008: val_acc did not improve from 0.57514
Epoch 9/10

  64/4849 [..............................] - ETA: 6:58 - loss: 0.6400 - acc: 0.6406
 128/4849 [..............................] - ETA: 6:39 - loss: 0.6276 - acc: 0.6406
 192/4849 [>.............................] - ETA: 6:37 - loss: 0.6463 - acc: 0.6406
 256/4849 [>.............................] - ETA: 6:29 - loss: 0.6554 - acc: 0.6172
 320/4849 [>.............................] - ETA: 6:25 - loss: 0.6561 - acc: 0.6156
 384/4849 [=>............................] - ETA: 6:21 - loss: 0.6611 - acc: 0.6120
 448/4849 [=>............................] - ETA: 6:21 - loss: 0.6585 - acc: 0.6205
 512/4849 [==>...........................] - ETA: 6:13 - loss: 0.6618 - acc: 0.6113
 576/4849 [==>...........................] - ETA: 6:09 - loss: 0.6609 - acc: 0.6094
 640/4849 [==>...........................] - ETA: 6:03 - loss: 0.6638 - acc: 0.6016
 704/4849 [===>..........................] - ETA: 5:58 - loss: 0.6660 - acc: 0.5980
 768/4849 [===>..........................] - ETA: 5:52 - loss: 0.6662 - acc: 0.6003
 832/4849 [====>.........................] - ETA: 5:47 - loss: 0.6635 - acc: 0.6046
 896/4849 [====>.........................] - ETA: 5:42 - loss: 0.6656 - acc: 0.6027
 960/4849 [====>.........................] - ETA: 5:35 - loss: 0.6625 - acc: 0.6073
1024/4849 [=====>........................] - ETA: 5:31 - loss: 0.6615 - acc: 0.6084
1088/4849 [=====>........................] - ETA: 5:25 - loss: 0.6583 - acc: 0.6149
1152/4849 [======>.......................] - ETA: 5:20 - loss: 0.6569 - acc: 0.6155
1216/4849 [======>.......................] - ETA: 5:14 - loss: 0.6560 - acc: 0.6168
1280/4849 [======>.......................] - ETA: 5:08 - loss: 0.6585 - acc: 0.6117
1344/4849 [=======>......................] - ETA: 5:03 - loss: 0.6597 - acc: 0.6079
1408/4849 [=======>......................] - ETA: 4:58 - loss: 0.6615 - acc: 0.6037
1472/4849 [========>.....................] - ETA: 4:51 - loss: 0.6640 - acc: 0.5985
1536/4849 [========>.....................] - ETA: 4:46 - loss: 0.6655 - acc: 0.5983
1600/4849 [========>.....................] - ETA: 4:41 - loss: 0.6654 - acc: 0.5981
1664/4849 [=========>....................] - ETA: 4:35 - loss: 0.6663 - acc: 0.5998
1728/4849 [=========>....................] - ETA: 4:30 - loss: 0.6650 - acc: 0.6007
1792/4849 [==========>...................] - ETA: 4:25 - loss: 0.6636 - acc: 0.6027
1856/4849 [==========>...................] - ETA: 4:17 - loss: 0.6653 - acc: 0.6018
1920/4849 [==========>...................] - ETA: 4:10 - loss: 0.6643 - acc: 0.6036
1984/4849 [===========>..................] - ETA: 4:03 - loss: 0.6638 - acc: 0.6048
2048/4849 [===========>..................] - ETA: 3:57 - loss: 0.6650 - acc: 0.6025
2112/4849 [============>.................] - ETA: 3:50 - loss: 0.6646 - acc: 0.6037
2176/4849 [============>.................] - ETA: 3:44 - loss: 0.6664 - acc: 0.6002
2240/4849 [============>.................] - ETA: 3:38 - loss: 0.6655 - acc: 0.6031
2304/4849 [=============>................] - ETA: 3:32 - loss: 0.6658 - acc: 0.6042
2368/4849 [=============>................] - ETA: 3:27 - loss: 0.6668 - acc: 0.6022
2432/4849 [==============>...............] - ETA: 3:21 - loss: 0.6677 - acc: 0.5999
2496/4849 [==============>...............] - ETA: 3:15 - loss: 0.6685 - acc: 0.5978
2560/4849 [==============>...............] - ETA: 3:09 - loss: 0.6693 - acc: 0.5949
2624/4849 [===============>..............] - ETA: 3:03 - loss: 0.6697 - acc: 0.5945
2688/4849 [===============>..............] - ETA: 2:58 - loss: 0.6708 - acc: 0.5926
2752/4849 [================>.............] - ETA: 2:52 - loss: 0.6705 - acc: 0.5919
2816/4849 [================>.............] - ETA: 2:46 - loss: 0.6699 - acc: 0.5941
2880/4849 [================>.............] - ETA: 2:40 - loss: 0.6704 - acc: 0.5941
2944/4849 [=================>............] - ETA: 2:35 - loss: 0.6696 - acc: 0.5961
3008/4849 [=================>............] - ETA: 2:30 - loss: 0.6692 - acc: 0.5964
3072/4849 [==================>...........] - ETA: 2:25 - loss: 0.6693 - acc: 0.5967
3136/4849 [==================>...........] - ETA: 2:19 - loss: 0.6701 - acc: 0.5947
3200/4849 [==================>...........] - ETA: 2:14 - loss: 0.6693 - acc: 0.5956
3264/4849 [===================>..........] - ETA: 2:08 - loss: 0.6689 - acc: 0.5962
3328/4849 [===================>..........] - ETA: 2:03 - loss: 0.6694 - acc: 0.5950
3392/4849 [===================>..........] - ETA: 1:57 - loss: 0.6693 - acc: 0.5938
3456/4849 [====================>.........] - ETA: 1:52 - loss: 0.6686 - acc: 0.5964
3520/4849 [====================>.........] - ETA: 1:46 - loss: 0.6687 - acc: 0.5963
3584/4849 [=====================>........] - ETA: 1:41 - loss: 0.6694 - acc: 0.5963
3648/4849 [=====================>........] - ETA: 1:36 - loss: 0.6689 - acc: 0.5968
3712/4849 [=====================>........] - ETA: 1:31 - loss: 0.6686 - acc: 0.5970
3776/4849 [======================>.......] - ETA: 1:25 - loss: 0.6693 - acc: 0.5956
3840/4849 [======================>.......] - ETA: 1:20 - loss: 0.6695 - acc: 0.5951
3904/4849 [=======================>......] - ETA: 1:15 - loss: 0.6702 - acc: 0.5943
3968/4849 [=======================>......] - ETA: 1:10 - loss: 0.6699 - acc: 0.5940
4032/4849 [=======================>......] - ETA: 1:05 - loss: 0.6696 - acc: 0.5942
4096/4849 [========================>.....] - ETA: 59s - loss: 0.6694 - acc: 0.5947 
4160/4849 [========================>.....] - ETA: 54s - loss: 0.6688 - acc: 0.5947
4224/4849 [=========================>....] - ETA: 49s - loss: 0.6677 - acc: 0.5971
4288/4849 [=========================>....] - ETA: 44s - loss: 0.6669 - acc: 0.5986
4352/4849 [=========================>....] - ETA: 39s - loss: 0.6664 - acc: 0.5986
4416/4849 [==========================>...] - ETA: 34s - loss: 0.6663 - acc: 0.5985
4480/4849 [==========================>...] - ETA: 29s - loss: 0.6663 - acc: 0.5984
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6663 - acc: 0.5988
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6658 - acc: 0.5996
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6663 - acc: 0.5993
4736/4849 [============================>.] - ETA: 8s - loss: 0.6661 - acc: 0.5995 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6654 - acc: 0.6008
4849/4849 [==============================] - 393s 81ms/step - loss: 0.6658 - acc: 0.6005 - val_loss: 0.6895 - val_acc: 0.5659

Epoch 00009: val_acc did not improve from 0.57514
Epoch 10/10

  64/4849 [..............................] - ETA: 4:43 - loss: 0.6848 - acc: 0.5469
 128/4849 [..............................] - ETA: 5:16 - loss: 0.6530 - acc: 0.6328
 192/4849 [>.............................] - ETA: 5:11 - loss: 0.6460 - acc: 0.6250
 256/4849 [>.............................] - ETA: 5:12 - loss: 0.6491 - acc: 0.6211
 320/4849 [>.............................] - ETA: 5:09 - loss: 0.6517 - acc: 0.6125
 384/4849 [=>............................] - ETA: 5:12 - loss: 0.6582 - acc: 0.6094
 448/4849 [=>............................] - ETA: 5:08 - loss: 0.6688 - acc: 0.5982
 512/4849 [==>...........................] - ETA: 5:00 - loss: 0.6649 - acc: 0.5996
 576/4849 [==>...........................] - ETA: 4:56 - loss: 0.6584 - acc: 0.6059
 640/4849 [==>...........................] - ETA: 4:54 - loss: 0.6629 - acc: 0.6016
 704/4849 [===>..........................] - ETA: 4:49 - loss: 0.6706 - acc: 0.5938
 768/4849 [===>..........................] - ETA: 4:46 - loss: 0.6750 - acc: 0.5820
 832/4849 [====>.........................] - ETA: 4:43 - loss: 0.6760 - acc: 0.5829
 896/4849 [====>.........................] - ETA: 4:39 - loss: 0.6729 - acc: 0.5893
 960/4849 [====>.........................] - ETA: 4:34 - loss: 0.6711 - acc: 0.5938
1024/4849 [=====>........................] - ETA: 4:29 - loss: 0.6730 - acc: 0.5898
1088/4849 [=====>........................] - ETA: 4:26 - loss: 0.6723 - acc: 0.5928
1152/4849 [======>.......................] - ETA: 4:20 - loss: 0.6733 - acc: 0.5911
1216/4849 [======>.......................] - ETA: 4:17 - loss: 0.6742 - acc: 0.5880
1280/4849 [======>.......................] - ETA: 4:11 - loss: 0.6744 - acc: 0.5859
1344/4849 [=======>......................] - ETA: 4:07 - loss: 0.6740 - acc: 0.5900
1408/4849 [=======>......................] - ETA: 4:03 - loss: 0.6741 - acc: 0.5923
1472/4849 [========>.....................] - ETA: 3:58 - loss: 0.6730 - acc: 0.5924
1536/4849 [========>.....................] - ETA: 3:53 - loss: 0.6721 - acc: 0.5944
1600/4849 [========>.....................] - ETA: 3:48 - loss: 0.6718 - acc: 0.5950
1664/4849 [=========>....................] - ETA: 3:45 - loss: 0.6726 - acc: 0.5925
1728/4849 [=========>....................] - ETA: 3:40 - loss: 0.6720 - acc: 0.5943
1792/4849 [==========>...................] - ETA: 3:35 - loss: 0.6702 - acc: 0.5949
1856/4849 [==========>...................] - ETA: 3:31 - loss: 0.6703 - acc: 0.5948
1920/4849 [==========>...................] - ETA: 3:27 - loss: 0.6706 - acc: 0.5964
1984/4849 [===========>..................] - ETA: 3:23 - loss: 0.6719 - acc: 0.5932
2048/4849 [===========>..................] - ETA: 3:18 - loss: 0.6705 - acc: 0.5957
2112/4849 [============>.................] - ETA: 3:14 - loss: 0.6703 - acc: 0.5961
2176/4849 [============>.................] - ETA: 3:09 - loss: 0.6701 - acc: 0.5974
2240/4849 [============>.................] - ETA: 3:05 - loss: 0.6706 - acc: 0.5987
2304/4849 [=============>................] - ETA: 3:01 - loss: 0.6682 - acc: 0.6029
2368/4849 [=============>................] - ETA: 2:56 - loss: 0.6679 - acc: 0.6018
2432/4849 [==============>...............] - ETA: 2:52 - loss: 0.6681 - acc: 0.6024
2496/4849 [==============>...............] - ETA: 2:47 - loss: 0.6694 - acc: 0.5986
2560/4849 [==============>...............] - ETA: 2:43 - loss: 0.6693 - acc: 0.5988
2624/4849 [===============>..............] - ETA: 2:38 - loss: 0.6694 - acc: 0.5983
2688/4849 [===============>..............] - ETA: 2:34 - loss: 0.6696 - acc: 0.5978
2752/4849 [================>.............] - ETA: 2:29 - loss: 0.6685 - acc: 0.5988
2816/4849 [================>.............] - ETA: 2:25 - loss: 0.6679 - acc: 0.5987
2880/4849 [================>.............] - ETA: 2:20 - loss: 0.6667 - acc: 0.6003
2944/4849 [=================>............] - ETA: 2:15 - loss: 0.6671 - acc: 0.5999
3008/4849 [=================>............] - ETA: 2:11 - loss: 0.6658 - acc: 0.6017
3072/4849 [==================>...........] - ETA: 2:07 - loss: 0.6648 - acc: 0.6029
3136/4849 [==================>...........] - ETA: 2:02 - loss: 0.6650 - acc: 0.6027
3200/4849 [==================>...........] - ETA: 1:57 - loss: 0.6648 - acc: 0.6044
3264/4849 [===================>..........] - ETA: 1:53 - loss: 0.6647 - acc: 0.6051
3328/4849 [===================>..........] - ETA: 1:49 - loss: 0.6657 - acc: 0.6040
3392/4849 [===================>..........] - ETA: 1:44 - loss: 0.6646 - acc: 0.6050
3456/4849 [====================>.........] - ETA: 1:40 - loss: 0.6648 - acc: 0.6050
3520/4849 [====================>.........] - ETA: 1:35 - loss: 0.6631 - acc: 0.6077
3584/4849 [=====================>........] - ETA: 1:31 - loss: 0.6627 - acc: 0.6094
3648/4849 [=====================>........] - ETA: 1:26 - loss: 0.6628 - acc: 0.6091
3712/4849 [=====================>........] - ETA: 1:21 - loss: 0.6633 - acc: 0.6086
3776/4849 [======================>.......] - ETA: 1:17 - loss: 0.6623 - acc: 0.6091
3840/4849 [======================>.......] - ETA: 1:12 - loss: 0.6610 - acc: 0.6104
3904/4849 [=======================>......] - ETA: 1:08 - loss: 0.6629 - acc: 0.6084
3968/4849 [=======================>......] - ETA: 1:03 - loss: 0.6617 - acc: 0.6091
4032/4849 [=======================>......] - ETA: 58s - loss: 0.6619 - acc: 0.6089 
4096/4849 [========================>.....] - ETA: 54s - loss: 0.6621 - acc: 0.6084
4160/4849 [========================>.....] - ETA: 49s - loss: 0.6621 - acc: 0.6079
4224/4849 [=========================>....] - ETA: 45s - loss: 0.6618 - acc: 0.6077
4288/4849 [=========================>....] - ETA: 40s - loss: 0.6620 - acc: 0.6080
4352/4849 [=========================>....] - ETA: 35s - loss: 0.6627 - acc: 0.6062
4416/4849 [==========================>...] - ETA: 31s - loss: 0.6632 - acc: 0.6046
4480/4849 [==========================>...] - ETA: 26s - loss: 0.6640 - acc: 0.6040
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6648 - acc: 0.6034
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6644 - acc: 0.6039
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6633 - acc: 0.6057
4736/4849 [============================>.] - ETA: 8s - loss: 0.6641 - acc: 0.6041 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6641 - acc: 0.6033
4849/4849 [==============================] - 364s 75ms/step - loss: 0.6642 - acc: 0.6032 - val_loss: 0.6841 - val_acc: 0.5770

Epoch 00010: val_acc improved from 0.57514 to 0.57699, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window10/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe1bc4f9410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe1bc4f9410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe1bc358a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe1bc358a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe22077ee90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe22077ee90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1bc0ea450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1bc0ea450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1bc0b8410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1bc0b8410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1bc3588d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1bc3588d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1bc0eaa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1bc0eaa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf30504d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf30504d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1987114d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1987114d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe05818e490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe05818e490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddd837fb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddd837fb10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdee03de850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdee03de850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0b8235f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0b8235f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1987f0410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1987f0410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1982daf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1982daf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe198779390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe198779390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe19840df10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe19840df10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1982d72d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1982d72d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1bc0ecf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1bc0ecf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1980bf310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1980bf310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe198312a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe198312a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe19831b5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe19831b5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1983c7890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1983c7890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1585a2c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1585a2c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1585e5a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1585e5a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1586a0c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1586a0c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1585a25d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1585a25d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1584b5e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1584b5e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe158497610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe158497610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe158101850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe158101850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1582e4c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1582e4c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1582568d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1582568d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe158365a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe158365a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe158072710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe158072710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1406b1e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1406b1e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1405de5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1405de5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe14070d510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe14070d510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe140622e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe140622e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe140429290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe140429290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1403d5510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1403d5510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1402c8750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1402c8750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1406b1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1406b1c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1403ea9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1403ea9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1400d1550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1400d1550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1400d4890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1400d4890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1406ad350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1406ad350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1400d1750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1400d1750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1387bbe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1387bbe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe138550690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe138550690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe138655250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe138655250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe138407610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe138407610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe138537a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe138537a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe138582b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe138582b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1384048d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1384048d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe138100190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe138100190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1384d64d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1384d64d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe138602910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe138602910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe13845e290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe13845e290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1086c1410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe1086c1410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1085d8790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe1085d8790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe108779050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe108779050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1086f4590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe1086f4590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1085e02d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe1085e02d0>>: AttributeError: module 'gast' has no attribute 'Str'
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 3:59
 128/1348 [=>............................] - ETA: 2:11
 192/1348 [===>..........................] - ETA: 1:33
 256/1348 [====>.........................] - ETA: 1:13
 320/1348 [======>.......................] - ETA: 1:01
 384/1348 [=======>......................] - ETA: 52s 
 448/1348 [========>.....................] - ETA: 46s
 512/1348 [==========>...................] - ETA: 40s
 576/1348 [===========>..................] - ETA: 35s
 640/1348 [=============>................] - ETA: 31s
 704/1348 [==============>...............] - ETA: 27s
 768/1348 [================>.............] - ETA: 24s
 832/1348 [=================>............] - ETA: 20s
 896/1348 [==================>...........] - ETA: 17s
 960/1348 [====================>.........] - ETA: 15s
1024/1348 [=====================>........] - ETA: 12s
1088/1348 [=======================>......] - ETA: 9s 
1152/1348 [========================>.....] - ETA: 7s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 49s 36ms/step
loss: 0.6619501651571483
acc: 0.6120178041543026
