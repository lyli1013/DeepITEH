nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2694
样本个数 5388
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9bea0dbf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9bea0dbf50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9bf85dc650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9bf85dc650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf85dc6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf85dc6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf85abed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf85abed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bea06ce50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bea06ce50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be9ff6f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be9ff6f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf85abd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf85abd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf85d6750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf85d6750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be9ff6dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be9ff6dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9be9e48810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9be9e48810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf84f4e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf84f4e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bea079b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bea079b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be9b8df50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be9b8df50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be9abadd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be9abadd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9be9a342d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9be9a342d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be9abeb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be9abeb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9be9d0e450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9be9d0e450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be99b1d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be99b1d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be9802090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be9802090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd967d650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd967d650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be9808690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be9808690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd9769ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd9769ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd957a5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd957a5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd96b3f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd96b3f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd96cc350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd96cc350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf8aa8dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf8aa8dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd96b3b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd96b3b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd945ab50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd945ab50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd91a2f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd91a2f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd9126c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd9126c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0f8bb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0f8bb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd935f890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd935f890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd9084c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd9084c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd9072f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd9072f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd9069e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd9069e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0d96810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0d96810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd0ec4810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd0ec4810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0d23550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0d23550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be98c22d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be98c22d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd09ecc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd09ecc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0b67090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0b67090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd94fa750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd94fa750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0908110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0908110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd0a2aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd0a2aa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd070b910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd070b910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0894490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0894490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd088e450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd088e450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd09e70d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd09e70d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd060c750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd060c750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd04637d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd04637d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0852710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0852710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd09ea350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd09ea350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0558790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd0558790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd01b26d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bd01b26d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd022d9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bd022d9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd01935d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd01935d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd0908e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bd0908e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd01068d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd01068d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bcff3fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bcff3fc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bcfed9d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bcfed9d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd022d0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bd022d0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bcfede8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bcfede8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bcfd69390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bcfd69390>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-26 19:02:56.763050: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-26 19:02:56.845663: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-26 19:02:56.942375: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564dd775f600 executing computations on platform Host. Devices:
2022-11-26 19:02:56.942463: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-26 19:02:58.014828: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 27:52 - loss: 0.7516 - acc: 0.5000
 128/4849 [..............................] - ETA: 19:18 - loss: 0.7208 - acc: 0.5312
 192/4849 [>.............................] - ETA: 15:42 - loss: 0.7301 - acc: 0.5260
 256/4849 [>.............................] - ETA: 14:10 - loss: 0.7484 - acc: 0.5273
 320/4849 [>.............................] - ETA: 13:02 - loss: 0.7458 - acc: 0.5250
 384/4849 [=>............................] - ETA: 12:14 - loss: 0.7426 - acc: 0.5234
 448/4849 [=>............................] - ETA: 11:32 - loss: 0.7494 - acc: 0.5112
 512/4849 [==>...........................] - ETA: 11:04 - loss: 0.7481 - acc: 0.5215
 576/4849 [==>...........................] - ETA: 10:38 - loss: 0.7524 - acc: 0.5191
 640/4849 [==>...........................] - ETA: 10:48 - loss: 0.7511 - acc: 0.5219
 704/4849 [===>..........................] - ETA: 10:23 - loss: 0.7445 - acc: 0.5284
 768/4849 [===>..........................] - ETA: 10:05 - loss: 0.7456 - acc: 0.5234
 832/4849 [====>.........................] - ETA: 9:42 - loss: 0.7540 - acc: 0.5156 
 896/4849 [====>.........................] - ETA: 9:18 - loss: 0.7504 - acc: 0.5190
 960/4849 [====>.........................] - ETA: 8:59 - loss: 0.7487 - acc: 0.5167
1024/4849 [=====>........................] - ETA: 8:40 - loss: 0.7472 - acc: 0.5166
1088/4849 [=====>........................] - ETA: 8:19 - loss: 0.7477 - acc: 0.5147
1152/4849 [======>.......................] - ETA: 8:02 - loss: 0.7457 - acc: 0.5191
1216/4849 [======>.......................] - ETA: 7:48 - loss: 0.7468 - acc: 0.5173
1280/4849 [======>.......................] - ETA: 7:33 - loss: 0.7444 - acc: 0.5180
1344/4849 [=======>......................] - ETA: 7:19 - loss: 0.7430 - acc: 0.5208
1408/4849 [=======>......................] - ETA: 7:14 - loss: 0.7428 - acc: 0.5199
1472/4849 [========>.....................] - ETA: 7:03 - loss: 0.7452 - acc: 0.5170
1536/4849 [========>.....................] - ETA: 6:50 - loss: 0.7445 - acc: 0.5150
1600/4849 [========>.....................] - ETA: 6:39 - loss: 0.7432 - acc: 0.5150
1664/4849 [=========>....................] - ETA: 6:28 - loss: 0.7429 - acc: 0.5138
1728/4849 [=========>....................] - ETA: 6:17 - loss: 0.7433 - acc: 0.5098
1792/4849 [==========>...................] - ETA: 6:07 - loss: 0.7418 - acc: 0.5106
1856/4849 [==========>...................] - ETA: 5:57 - loss: 0.7423 - acc: 0.5102
1920/4849 [==========>...................] - ETA: 5:48 - loss: 0.7433 - acc: 0.5078
1984/4849 [===========>..................] - ETA: 5:38 - loss: 0.7426 - acc: 0.5066
2048/4849 [===========>..................] - ETA: 5:28 - loss: 0.7396 - acc: 0.5107
2112/4849 [============>.................] - ETA: 5:18 - loss: 0.7401 - acc: 0.5123
2176/4849 [============>.................] - ETA: 5:10 - loss: 0.7382 - acc: 0.5152
2240/4849 [============>.................] - ETA: 5:00 - loss: 0.7373 - acc: 0.5183
2304/4849 [=============>................] - ETA: 4:52 - loss: 0.7357 - acc: 0.5191
2368/4849 [=============>................] - ETA: 4:44 - loss: 0.7363 - acc: 0.5198
2432/4849 [==============>...............] - ETA: 4:37 - loss: 0.7356 - acc: 0.5214
2496/4849 [==============>...............] - ETA: 4:29 - loss: 0.7344 - acc: 0.5228
2560/4849 [==============>...............] - ETA: 4:20 - loss: 0.7323 - acc: 0.5246
2624/4849 [===============>..............] - ETA: 4:12 - loss: 0.7324 - acc: 0.5236
2688/4849 [===============>..............] - ETA: 4:04 - loss: 0.7308 - acc: 0.5253
2752/4849 [================>.............] - ETA: 3:56 - loss: 0.7316 - acc: 0.5243
2816/4849 [================>.............] - ETA: 3:48 - loss: 0.7305 - acc: 0.5252
2880/4849 [================>.............] - ETA: 3:41 - loss: 0.7301 - acc: 0.5250
2944/4849 [=================>............] - ETA: 3:33 - loss: 0.7290 - acc: 0.5255
3008/4849 [=================>............] - ETA: 3:25 - loss: 0.7288 - acc: 0.5246
3072/4849 [==================>...........] - ETA: 3:18 - loss: 0.7267 - acc: 0.5270
3136/4849 [==================>...........] - ETA: 3:10 - loss: 0.7255 - acc: 0.5274
3200/4849 [==================>...........] - ETA: 3:02 - loss: 0.7244 - acc: 0.5297
3264/4849 [===================>..........] - ETA: 2:55 - loss: 0.7234 - acc: 0.5319
3328/4849 [===================>..........] - ETA: 2:48 - loss: 0.7232 - acc: 0.5312
3392/4849 [===================>..........] - ETA: 2:40 - loss: 0.7239 - acc: 0.5301
3456/4849 [====================>.........] - ETA: 2:33 - loss: 0.7248 - acc: 0.5284
3520/4849 [====================>.........] - ETA: 2:26 - loss: 0.7250 - acc: 0.5278
3584/4849 [=====================>........] - ETA: 2:18 - loss: 0.7252 - acc: 0.5268
3648/4849 [=====================>........] - ETA: 2:11 - loss: 0.7247 - acc: 0.5274
3712/4849 [=====================>........] - ETA: 2:04 - loss: 0.7238 - acc: 0.5277
3776/4849 [======================>.......] - ETA: 1:56 - loss: 0.7236 - acc: 0.5273
3840/4849 [======================>.......] - ETA: 1:49 - loss: 0.7220 - acc: 0.5286
3904/4849 [=======================>......] - ETA: 1:42 - loss: 0.7216 - acc: 0.5287
3968/4849 [=======================>......] - ETA: 1:35 - loss: 0.7209 - acc: 0.5297
4032/4849 [=======================>......] - ETA: 1:28 - loss: 0.7208 - acc: 0.5295
4096/4849 [========================>.....] - ETA: 1:21 - loss: 0.7202 - acc: 0.5291
4160/4849 [========================>.....] - ETA: 1:14 - loss: 0.7195 - acc: 0.5298
4224/4849 [=========================>....] - ETA: 1:07 - loss: 0.7183 - acc: 0.5315
4288/4849 [=========================>....] - ETA: 1:00 - loss: 0.7183 - acc: 0.5308
4352/4849 [=========================>....] - ETA: 53s - loss: 0.7174 - acc: 0.5317 
4416/4849 [==========================>...] - ETA: 46s - loss: 0.7162 - acc: 0.5328
4480/4849 [==========================>...] - ETA: 39s - loss: 0.7157 - acc: 0.5335
4544/4849 [===========================>..] - ETA: 32s - loss: 0.7155 - acc: 0.5341
4608/4849 [===========================>..] - ETA: 25s - loss: 0.7157 - acc: 0.5339
4672/4849 [===========================>..] - ETA: 18s - loss: 0.7155 - acc: 0.5338
4736/4849 [============================>.] - ETA: 12s - loss: 0.7156 - acc: 0.5334
4800/4849 [============================>.] - ETA: 5s - loss: 0.7150 - acc: 0.5342 
4849/4849 [==============================] - 534s 110ms/step - loss: 0.7146 - acc: 0.5343 - val_loss: 0.7084 - val_acc: 0.5417

Epoch 00001: val_acc improved from -inf to 0.54174, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window16/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 8:16 - loss: 0.7465 - acc: 0.4531
 128/4849 [..............................] - ETA: 8:12 - loss: 0.7378 - acc: 0.5000
 192/4849 [>.............................] - ETA: 7:56 - loss: 0.7280 - acc: 0.5000
 256/4849 [>.............................] - ETA: 7:43 - loss: 0.7266 - acc: 0.5039
 320/4849 [>.............................] - ETA: 7:28 - loss: 0.7287 - acc: 0.5000
 384/4849 [=>............................] - ETA: 7:20 - loss: 0.7208 - acc: 0.5104
 448/4849 [=>............................] - ETA: 7:16 - loss: 0.7127 - acc: 0.5179
 512/4849 [==>...........................] - ETA: 7:09 - loss: 0.7114 - acc: 0.5195
 576/4849 [==>...........................] - ETA: 7:01 - loss: 0.7072 - acc: 0.5226
 640/4849 [==>...........................] - ETA: 6:55 - loss: 0.7038 - acc: 0.5266
 704/4849 [===>..........................] - ETA: 6:47 - loss: 0.7077 - acc: 0.5128
 768/4849 [===>..........................] - ETA: 6:40 - loss: 0.7055 - acc: 0.5195
 832/4849 [====>.........................] - ETA: 6:35 - loss: 0.7077 - acc: 0.5192
 896/4849 [====>.........................] - ETA: 6:30 - loss: 0.7047 - acc: 0.5234
 960/4849 [====>.........................] - ETA: 6:21 - loss: 0.7071 - acc: 0.5156
1024/4849 [=====>........................] - ETA: 6:16 - loss: 0.7047 - acc: 0.5205
1088/4849 [=====>........................] - ETA: 6:11 - loss: 0.7027 - acc: 0.5267
1152/4849 [======>.......................] - ETA: 6:04 - loss: 0.7015 - acc: 0.5278
1216/4849 [======>.......................] - ETA: 5:58 - loss: 0.7004 - acc: 0.5288
1280/4849 [======>.......................] - ETA: 5:51 - loss: 0.6991 - acc: 0.5320
1344/4849 [=======>......................] - ETA: 5:45 - loss: 0.6971 - acc: 0.5350
1408/4849 [=======>......................] - ETA: 5:40 - loss: 0.6970 - acc: 0.5369
1472/4849 [========>.....................] - ETA: 5:32 - loss: 0.6948 - acc: 0.5414
1536/4849 [========>.....................] - ETA: 5:26 - loss: 0.6962 - acc: 0.5417
1600/4849 [========>.....................] - ETA: 5:20 - loss: 0.6972 - acc: 0.5406
1664/4849 [=========>....................] - ETA: 5:16 - loss: 0.6990 - acc: 0.5409
1728/4849 [=========>....................] - ETA: 5:09 - loss: 0.6974 - acc: 0.5446
1792/4849 [==========>...................] - ETA: 5:02 - loss: 0.6963 - acc: 0.5480
1856/4849 [==========>...................] - ETA: 4:55 - loss: 0.6964 - acc: 0.5490
1920/4849 [==========>...................] - ETA: 4:49 - loss: 0.6969 - acc: 0.5474
1984/4849 [===========>..................] - ETA: 4:43 - loss: 0.6986 - acc: 0.5423
2048/4849 [===========>..................] - ETA: 4:36 - loss: 0.6984 - acc: 0.5400
2112/4849 [============>.................] - ETA: 4:29 - loss: 0.6959 - acc: 0.5440
2176/4849 [============>.................] - ETA: 4:24 - loss: 0.6963 - acc: 0.5446
2240/4849 [============>.................] - ETA: 4:18 - loss: 0.6976 - acc: 0.5420
2304/4849 [=============>................] - ETA: 4:11 - loss: 0.6964 - acc: 0.5438
2368/4849 [=============>................] - ETA: 4:05 - loss: 0.6957 - acc: 0.5439
2432/4849 [==============>...............] - ETA: 3:59 - loss: 0.6958 - acc: 0.5432
2496/4849 [==============>...............] - ETA: 3:53 - loss: 0.6963 - acc: 0.5413
2560/4849 [==============>...............] - ETA: 3:45 - loss: 0.6974 - acc: 0.5418
2624/4849 [===============>..............] - ETA: 3:38 - loss: 0.6980 - acc: 0.5408
2688/4849 [===============>..............] - ETA: 3:32 - loss: 0.6969 - acc: 0.5435
2752/4849 [================>.............] - ETA: 3:25 - loss: 0.6974 - acc: 0.5418
2816/4849 [================>.............] - ETA: 3:18 - loss: 0.6982 - acc: 0.5387
2880/4849 [================>.............] - ETA: 3:12 - loss: 0.6985 - acc: 0.5378
2944/4849 [=================>............] - ETA: 3:06 - loss: 0.6980 - acc: 0.5387
3008/4849 [=================>............] - ETA: 2:59 - loss: 0.6977 - acc: 0.5389
3072/4849 [==================>...........] - ETA: 2:53 - loss: 0.6975 - acc: 0.5397
3136/4849 [==================>...........] - ETA: 2:47 - loss: 0.6969 - acc: 0.5405
3200/4849 [==================>...........] - ETA: 2:41 - loss: 0.6970 - acc: 0.5400
3264/4849 [===================>..........] - ETA: 2:34 - loss: 0.6971 - acc: 0.5380
3328/4849 [===================>..........] - ETA: 2:28 - loss: 0.6972 - acc: 0.5370
3392/4849 [===================>..........] - ETA: 2:21 - loss: 0.6973 - acc: 0.5354
3456/4849 [====================>.........] - ETA: 2:15 - loss: 0.6972 - acc: 0.5353
3520/4849 [====================>.........] - ETA: 2:09 - loss: 0.6972 - acc: 0.5355
3584/4849 [=====================>........] - ETA: 2:02 - loss: 0.6971 - acc: 0.5357
3648/4849 [=====================>........] - ETA: 1:56 - loss: 0.6981 - acc: 0.5340
3712/4849 [=====================>........] - ETA: 1:49 - loss: 0.6985 - acc: 0.5329
3776/4849 [======================>.......] - ETA: 1:43 - loss: 0.6989 - acc: 0.5323
3840/4849 [======================>.......] - ETA: 1:37 - loss: 0.6989 - acc: 0.5318
3904/4849 [=======================>......] - ETA: 1:30 - loss: 0.6989 - acc: 0.5320
3968/4849 [=======================>......] - ETA: 1:24 - loss: 0.6989 - acc: 0.5323
4032/4849 [=======================>......] - ETA: 1:18 - loss: 0.6983 - acc: 0.5332
4096/4849 [========================>.....] - ETA: 1:11 - loss: 0.6984 - acc: 0.5330
4160/4849 [========================>.....] - ETA: 1:05 - loss: 0.6988 - acc: 0.5317
4224/4849 [=========================>....] - ETA: 59s - loss: 0.6984 - acc: 0.5329 
4288/4849 [=========================>....] - ETA: 53s - loss: 0.6991 - acc: 0.5310
4352/4849 [=========================>....] - ETA: 47s - loss: 0.6995 - acc: 0.5306
4416/4849 [==========================>...] - ETA: 41s - loss: 0.6992 - acc: 0.5299
4480/4849 [==========================>...] - ETA: 35s - loss: 0.6990 - acc: 0.5306
4544/4849 [===========================>..] - ETA: 28s - loss: 0.6986 - acc: 0.5312
4608/4849 [===========================>..] - ETA: 22s - loss: 0.6986 - acc: 0.5306
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6990 - acc: 0.5304
4736/4849 [============================>.] - ETA: 10s - loss: 0.6986 - acc: 0.5308
4800/4849 [============================>.] - ETA: 4s - loss: 0.6984 - acc: 0.5310 
4849/4849 [==============================] - 477s 98ms/step - loss: 0.6983 - acc: 0.5310 - val_loss: 0.6877 - val_acc: 0.5343

Epoch 00002: val_acc did not improve from 0.54174
Epoch 3/10

  64/4849 [..............................] - ETA: 7:41 - loss: 0.6224 - acc: 0.6719
 128/4849 [..............................] - ETA: 7:07 - loss: 0.6334 - acc: 0.6406
 192/4849 [>.............................] - ETA: 7:06 - loss: 0.6461 - acc: 0.6250
 256/4849 [>.............................] - ETA: 6:58 - loss: 0.6556 - acc: 0.6250
 320/4849 [>.............................] - ETA: 6:47 - loss: 0.6503 - acc: 0.6312
 384/4849 [=>............................] - ETA: 6:47 - loss: 0.6611 - acc: 0.6198
 448/4849 [=>............................] - ETA: 6:38 - loss: 0.6697 - acc: 0.6049
 512/4849 [==>...........................] - ETA: 6:35 - loss: 0.6746 - acc: 0.5938
 576/4849 [==>...........................] - ETA: 6:29 - loss: 0.6758 - acc: 0.5920
 640/4849 [==>...........................] - ETA: 6:22 - loss: 0.6762 - acc: 0.5922
 704/4849 [===>..........................] - ETA: 6:18 - loss: 0.6753 - acc: 0.5952
 768/4849 [===>..........................] - ETA: 6:11 - loss: 0.6747 - acc: 0.5990
 832/4849 [====>.........................] - ETA: 6:04 - loss: 0.6753 - acc: 0.5950
 896/4849 [====>.........................] - ETA: 5:58 - loss: 0.6802 - acc: 0.5882
 960/4849 [====>.........................] - ETA: 5:51 - loss: 0.6788 - acc: 0.5875
1024/4849 [=====>........................] - ETA: 5:46 - loss: 0.6811 - acc: 0.5781
1088/4849 [=====>........................] - ETA: 5:41 - loss: 0.6812 - acc: 0.5754
1152/4849 [======>.......................] - ETA: 5:34 - loss: 0.6820 - acc: 0.5720
1216/4849 [======>.......................] - ETA: 5:28 - loss: 0.6819 - acc: 0.5707
1280/4849 [======>.......................] - ETA: 5:22 - loss: 0.6819 - acc: 0.5695
1344/4849 [=======>......................] - ETA: 5:16 - loss: 0.6815 - acc: 0.5737
1408/4849 [=======>......................] - ETA: 5:09 - loss: 0.6828 - acc: 0.5689
1472/4849 [========>.....................] - ETA: 5:04 - loss: 0.6829 - acc: 0.5700
1536/4849 [========>.....................] - ETA: 4:58 - loss: 0.6833 - acc: 0.5710
1600/4849 [========>.....................] - ETA: 4:52 - loss: 0.6836 - acc: 0.5700
1664/4849 [=========>....................] - ETA: 4:46 - loss: 0.6838 - acc: 0.5691
1728/4849 [=========>....................] - ETA: 4:40 - loss: 0.6852 - acc: 0.5660
1792/4849 [==========>...................] - ETA: 4:35 - loss: 0.6864 - acc: 0.5625
1856/4849 [==========>...................] - ETA: 4:28 - loss: 0.6856 - acc: 0.5636
1920/4849 [==========>...................] - ETA: 4:22 - loss: 0.6868 - acc: 0.5599
1984/4849 [===========>..................] - ETA: 4:17 - loss: 0.6872 - acc: 0.5595
2048/4849 [===========>..................] - ETA: 4:11 - loss: 0.6872 - acc: 0.5596
2112/4849 [============>.................] - ETA: 4:05 - loss: 0.6873 - acc: 0.5592
2176/4849 [============>.................] - ETA: 4:00 - loss: 0.6880 - acc: 0.5570
2240/4849 [============>.................] - ETA: 3:54 - loss: 0.6883 - acc: 0.5540
2304/4849 [=============>................] - ETA: 3:48 - loss: 0.6887 - acc: 0.5521
2368/4849 [=============>................] - ETA: 3:42 - loss: 0.6886 - acc: 0.5511
2432/4849 [==============>...............] - ETA: 3:36 - loss: 0.6893 - acc: 0.5498
2496/4849 [==============>...............] - ETA: 3:30 - loss: 0.6890 - acc: 0.5497
2560/4849 [==============>...............] - ETA: 3:25 - loss: 0.6887 - acc: 0.5508
2624/4849 [===============>..............] - ETA: 3:19 - loss: 0.6880 - acc: 0.5518
2688/4849 [===============>..............] - ETA: 3:13 - loss: 0.6879 - acc: 0.5521
2752/4849 [================>.............] - ETA: 3:07 - loss: 0.6880 - acc: 0.5512
2816/4849 [================>.............] - ETA: 3:02 - loss: 0.6882 - acc: 0.5508
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6871 - acc: 0.5528
2944/4849 [=================>............] - ETA: 2:50 - loss: 0.6867 - acc: 0.5537
3008/4849 [=================>............] - ETA: 2:44 - loss: 0.6873 - acc: 0.5522
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6873 - acc: 0.5534
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6891 - acc: 0.5513
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6884 - acc: 0.5528
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6882 - acc: 0.5539
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6886 - acc: 0.5544
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6891 - acc: 0.5537
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6891 - acc: 0.5544
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6895 - acc: 0.5531
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6901 - acc: 0.5525
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6903 - acc: 0.5521
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6896 - acc: 0.5536
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6895 - acc: 0.5532
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6901 - acc: 0.5523
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6904 - acc: 0.5497
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6901 - acc: 0.5496
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6904 - acc: 0.5489
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6904 - acc: 0.5483
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6906 - acc: 0.5466
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6908 - acc: 0.5462 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6908 - acc: 0.5466
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6913 - acc: 0.5448
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6910 - acc: 0.5451
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6912 - acc: 0.5451
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6913 - acc: 0.5453
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6913 - acc: 0.5449
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6906 - acc: 0.5469
4736/4849 [============================>.] - ETA: 9s - loss: 0.6901 - acc: 0.5473 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6898 - acc: 0.5475
4849/4849 [==============================] - 448s 92ms/step - loss: 0.6899 - acc: 0.5471 - val_loss: 0.6962 - val_acc: 0.5325

Epoch 00003: val_acc did not improve from 0.54174
Epoch 4/10

  64/4849 [..............................] - ETA: 6:11 - loss: 0.6771 - acc: 0.6250
 128/4849 [..............................] - ETA: 6:31 - loss: 0.6781 - acc: 0.6328
 192/4849 [>.............................] - ETA: 6:29 - loss: 0.6831 - acc: 0.6042
 256/4849 [>.............................] - ETA: 6:27 - loss: 0.6816 - acc: 0.5898
 320/4849 [>.............................] - ETA: 6:25 - loss: 0.6790 - acc: 0.5813
 384/4849 [=>............................] - ETA: 6:28 - loss: 0.6770 - acc: 0.5781
 448/4849 [=>............................] - ETA: 6:18 - loss: 0.6825 - acc: 0.5737
 512/4849 [==>...........................] - ETA: 6:12 - loss: 0.6844 - acc: 0.5723
 576/4849 [==>...........................] - ETA: 6:08 - loss: 0.6892 - acc: 0.5625
 640/4849 [==>...........................] - ETA: 6:03 - loss: 0.6927 - acc: 0.5516
 704/4849 [===>..........................] - ETA: 5:57 - loss: 0.6894 - acc: 0.5611
 768/4849 [===>..........................] - ETA: 5:53 - loss: 0.6910 - acc: 0.5547
 832/4849 [====>.........................] - ETA: 5:49 - loss: 0.6901 - acc: 0.5505
 896/4849 [====>.........................] - ETA: 5:44 - loss: 0.6932 - acc: 0.5458
 960/4849 [====>.........................] - ETA: 5:39 - loss: 0.6915 - acc: 0.5490
1024/4849 [=====>........................] - ETA: 5:31 - loss: 0.6905 - acc: 0.5527
1088/4849 [=====>........................] - ETA: 5:26 - loss: 0.6903 - acc: 0.5515
1152/4849 [======>.......................] - ETA: 5:20 - loss: 0.6913 - acc: 0.5486
1216/4849 [======>.......................] - ETA: 5:14 - loss: 0.6924 - acc: 0.5436
1280/4849 [======>.......................] - ETA: 5:10 - loss: 0.6932 - acc: 0.5414
1344/4849 [=======>......................] - ETA: 5:05 - loss: 0.6924 - acc: 0.5409
1408/4849 [=======>......................] - ETA: 4:59 - loss: 0.6921 - acc: 0.5426
1472/4849 [========>.....................] - ETA: 4:54 - loss: 0.6910 - acc: 0.5435
1536/4849 [========>.....................] - ETA: 4:49 - loss: 0.6922 - acc: 0.5397
1600/4849 [========>.....................] - ETA: 4:45 - loss: 0.6910 - acc: 0.5387
1664/4849 [=========>....................] - ETA: 4:40 - loss: 0.6898 - acc: 0.5403
1728/4849 [=========>....................] - ETA: 4:35 - loss: 0.6892 - acc: 0.5417
1792/4849 [==========>...................] - ETA: 4:29 - loss: 0.6896 - acc: 0.5385
1856/4849 [==========>...................] - ETA: 4:23 - loss: 0.6898 - acc: 0.5372
1920/4849 [==========>...................] - ETA: 4:18 - loss: 0.6887 - acc: 0.5385
1984/4849 [===========>..................] - ETA: 4:13 - loss: 0.6888 - acc: 0.5383
2048/4849 [===========>..................] - ETA: 4:07 - loss: 0.6892 - acc: 0.5381
2112/4849 [============>.................] - ETA: 4:01 - loss: 0.6886 - acc: 0.5388
2176/4849 [============>.................] - ETA: 3:56 - loss: 0.6881 - acc: 0.5404
2240/4849 [============>.................] - ETA: 3:50 - loss: 0.6893 - acc: 0.5393
2304/4849 [=============>................] - ETA: 3:45 - loss: 0.6888 - acc: 0.5425
2368/4849 [=============>................] - ETA: 3:39 - loss: 0.6885 - acc: 0.5439
2432/4849 [==============>...............] - ETA: 3:34 - loss: 0.6898 - acc: 0.5395
2496/4849 [==============>...............] - ETA: 3:28 - loss: 0.6912 - acc: 0.5357
2560/4849 [==============>...............] - ETA: 3:22 - loss: 0.6910 - acc: 0.5367
2624/4849 [===============>..............] - ETA: 3:17 - loss: 0.6912 - acc: 0.5377
2688/4849 [===============>..............] - ETA: 3:11 - loss: 0.6909 - acc: 0.5383
2752/4849 [================>.............] - ETA: 3:05 - loss: 0.6906 - acc: 0.5382
2816/4849 [================>.............] - ETA: 2:59 - loss: 0.6897 - acc: 0.5394
2880/4849 [================>.............] - ETA: 2:53 - loss: 0.6890 - acc: 0.5417
2944/4849 [=================>............] - ETA: 2:48 - loss: 0.6881 - acc: 0.5435
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6882 - acc: 0.5436
3072/4849 [==================>...........] - ETA: 2:37 - loss: 0.6872 - acc: 0.5449
3136/4849 [==================>...........] - ETA: 2:31 - loss: 0.6872 - acc: 0.5450
3200/4849 [==================>...........] - ETA: 2:25 - loss: 0.6866 - acc: 0.5441
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6869 - acc: 0.5438
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6870 - acc: 0.5433
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6870 - acc: 0.5442
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6871 - acc: 0.5443
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6876 - acc: 0.5440
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6879 - acc: 0.5444
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6873 - acc: 0.5474
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6867 - acc: 0.5477
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6876 - acc: 0.5469
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6879 - acc: 0.5456
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6878 - acc: 0.5456
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6867 - acc: 0.5489
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6865 - acc: 0.5489
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6872 - acc: 0.5466
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6867 - acc: 0.5476
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6859 - acc: 0.5497 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6855 - acc: 0.5504
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6849 - acc: 0.5519
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6845 - acc: 0.5525
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6845 - acc: 0.5529
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6838 - acc: 0.5535
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6838 - acc: 0.5534
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6841 - acc: 0.5527
4736/4849 [============================>.] - ETA: 10s - loss: 0.6840 - acc: 0.5522
4800/4849 [============================>.] - ETA: 4s - loss: 0.6840 - acc: 0.5525 
4849/4849 [==============================] - 449s 93ms/step - loss: 0.6841 - acc: 0.5519 - val_loss: 0.7015 - val_acc: 0.5473

Epoch 00004: val_acc improved from 0.54174 to 0.54731, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window16/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 5/10

  64/4849 [..............................] - ETA: 6:58 - loss: 0.6787 - acc: 0.5781
 128/4849 [..............................] - ETA: 7:09 - loss: 0.6938 - acc: 0.5469
 192/4849 [>.............................] - ETA: 6:50 - loss: 0.6867 - acc: 0.5521
 256/4849 [>.............................] - ETA: 6:41 - loss: 0.6764 - acc: 0.5664
 320/4849 [>.............................] - ETA: 6:36 - loss: 0.6658 - acc: 0.5875
 384/4849 [=>............................] - ETA: 6:36 - loss: 0.6640 - acc: 0.5885
 448/4849 [=>............................] - ETA: 6:28 - loss: 0.6638 - acc: 0.5826
 512/4849 [==>...........................] - ETA: 6:27 - loss: 0.6664 - acc: 0.5762
 576/4849 [==>...........................] - ETA: 6:19 - loss: 0.6708 - acc: 0.5712
 640/4849 [==>...........................] - ETA: 6:09 - loss: 0.6716 - acc: 0.5734
 704/4849 [===>..........................] - ETA: 6:03 - loss: 0.6723 - acc: 0.5724
 768/4849 [===>..........................] - ETA: 5:58 - loss: 0.6755 - acc: 0.5638
 832/4849 [====>.........................] - ETA: 5:54 - loss: 0.6773 - acc: 0.5613
 896/4849 [====>.........................] - ETA: 5:48 - loss: 0.6813 - acc: 0.5502
 960/4849 [====>.........................] - ETA: 5:43 - loss: 0.6804 - acc: 0.5521
1024/4849 [=====>........................] - ETA: 5:36 - loss: 0.6783 - acc: 0.5547
1088/4849 [=====>........................] - ETA: 5:30 - loss: 0.6778 - acc: 0.5570
1152/4849 [======>.......................] - ETA: 5:26 - loss: 0.6814 - acc: 0.5564
1216/4849 [======>.......................] - ETA: 5:20 - loss: 0.6837 - acc: 0.5518
1280/4849 [======>.......................] - ETA: 5:13 - loss: 0.6850 - acc: 0.5523
1344/4849 [=======>......................] - ETA: 5:08 - loss: 0.6863 - acc: 0.5499
1408/4849 [=======>......................] - ETA: 5:01 - loss: 0.6879 - acc: 0.5462
1472/4849 [========>.....................] - ETA: 4:55 - loss: 0.6862 - acc: 0.5510
1536/4849 [========>.....................] - ETA: 4:50 - loss: 0.6877 - acc: 0.5462
1600/4849 [========>.....................] - ETA: 4:46 - loss: 0.6869 - acc: 0.5469
1664/4849 [=========>....................] - ETA: 4:41 - loss: 0.6861 - acc: 0.5487
1728/4849 [=========>....................] - ETA: 4:36 - loss: 0.6857 - acc: 0.5521
1792/4849 [==========>...................] - ETA: 4:31 - loss: 0.6864 - acc: 0.5485
1856/4849 [==========>...................] - ETA: 4:25 - loss: 0.6867 - acc: 0.5496
1920/4849 [==========>...................] - ETA: 4:21 - loss: 0.6877 - acc: 0.5469
1984/4849 [===========>..................] - ETA: 4:15 - loss: 0.6881 - acc: 0.5459
2048/4849 [===========>..................] - ETA: 4:09 - loss: 0.6895 - acc: 0.5430
2112/4849 [============>.................] - ETA: 4:03 - loss: 0.6886 - acc: 0.5455
2176/4849 [============>.................] - ETA: 3:58 - loss: 0.6900 - acc: 0.5423
2240/4849 [============>.................] - ETA: 3:53 - loss: 0.6890 - acc: 0.5424
2304/4849 [=============>................] - ETA: 3:46 - loss: 0.6885 - acc: 0.5430
2368/4849 [=============>................] - ETA: 3:40 - loss: 0.6875 - acc: 0.5452
2432/4849 [==============>...............] - ETA: 3:35 - loss: 0.6877 - acc: 0.5452
2496/4849 [==============>...............] - ETA: 3:29 - loss: 0.6871 - acc: 0.5461
2560/4849 [==============>...............] - ETA: 3:23 - loss: 0.6869 - acc: 0.5461
2624/4849 [===============>..............] - ETA: 3:17 - loss: 0.6867 - acc: 0.5473
2688/4849 [===============>..............] - ETA: 3:11 - loss: 0.6869 - acc: 0.5469
2752/4849 [================>.............] - ETA: 3:05 - loss: 0.6863 - acc: 0.5480
2816/4849 [================>.............] - ETA: 3:00 - loss: 0.6856 - acc: 0.5472
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6855 - acc: 0.5476
2944/4849 [=================>............] - ETA: 2:49 - loss: 0.6860 - acc: 0.5476
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6849 - acc: 0.5499
3072/4849 [==================>...........] - ETA: 2:37 - loss: 0.6845 - acc: 0.5505
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6834 - acc: 0.5526
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6824 - acc: 0.5553
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6826 - acc: 0.5555
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6818 - acc: 0.5559
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6820 - acc: 0.5560
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6819 - acc: 0.5558
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6813 - acc: 0.5582
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6810 - acc: 0.5589
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6808 - acc: 0.5595
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6815 - acc: 0.5582
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6818 - acc: 0.5585
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6814 - acc: 0.5589
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6815 - acc: 0.5581
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6816 - acc: 0.5582
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6814 - acc: 0.5585
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6817 - acc: 0.5579
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6814 - acc: 0.5582
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6805 - acc: 0.5597 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6803 - acc: 0.5595
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6802 - acc: 0.5593
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6802 - acc: 0.5596
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6798 - acc: 0.5614
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6802 - acc: 0.5599
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6806 - acc: 0.5597
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6809 - acc: 0.5589
4736/4849 [============================>.] - ETA: 10s - loss: 0.6809 - acc: 0.5591
4800/4849 [============================>.] - ETA: 4s - loss: 0.6810 - acc: 0.5587 
4849/4849 [==============================] - 450s 93ms/step - loss: 0.6811 - acc: 0.5583 - val_loss: 0.7149 - val_acc: 0.4972

Epoch 00005: val_acc did not improve from 0.54731
Epoch 6/10

  64/4849 [..............................] - ETA: 7:06 - loss: 0.6682 - acc: 0.6719
 128/4849 [..............................] - ETA: 7:06 - loss: 0.6847 - acc: 0.5547
 192/4849 [>.............................] - ETA: 6:58 - loss: 0.7019 - acc: 0.5156
 256/4849 [>.............................] - ETA: 6:59 - loss: 0.6933 - acc: 0.5391
 320/4849 [>.............................] - ETA: 7:01 - loss: 0.6804 - acc: 0.5625
 384/4849 [=>............................] - ETA: 6:57 - loss: 0.6807 - acc: 0.5729
 448/4849 [=>............................] - ETA: 6:42 - loss: 0.6785 - acc: 0.5759
 512/4849 [==>...........................] - ETA: 6:36 - loss: 0.6779 - acc: 0.5742
 576/4849 [==>...........................] - ETA: 6:28 - loss: 0.6806 - acc: 0.5729
 640/4849 [==>...........................] - ETA: 6:24 - loss: 0.6766 - acc: 0.5828
 704/4849 [===>..........................] - ETA: 6:20 - loss: 0.6782 - acc: 0.5838
 768/4849 [===>..........................] - ETA: 6:13 - loss: 0.6789 - acc: 0.5768
 832/4849 [====>.........................] - ETA: 6:06 - loss: 0.6777 - acc: 0.5757
 896/4849 [====>.........................] - ETA: 6:00 - loss: 0.6776 - acc: 0.5737
 960/4849 [====>.........................] - ETA: 5:57 - loss: 0.6768 - acc: 0.5792
1024/4849 [=====>........................] - ETA: 5:52 - loss: 0.6752 - acc: 0.5830
1088/4849 [=====>........................] - ETA: 5:44 - loss: 0.6751 - acc: 0.5827
1152/4849 [======>.......................] - ETA: 5:38 - loss: 0.6726 - acc: 0.5894
1216/4849 [======>.......................] - ETA: 5:31 - loss: 0.6718 - acc: 0.5946
1280/4849 [======>.......................] - ETA: 5:23 - loss: 0.6716 - acc: 0.5953
1344/4849 [=======>......................] - ETA: 5:18 - loss: 0.6724 - acc: 0.5945
1408/4849 [=======>......................] - ETA: 5:14 - loss: 0.6737 - acc: 0.5895
1472/4849 [========>.....................] - ETA: 5:08 - loss: 0.6754 - acc: 0.5836
1536/4849 [========>.....................] - ETA: 5:01 - loss: 0.6766 - acc: 0.5840
1600/4849 [========>.....................] - ETA: 4:55 - loss: 0.6766 - acc: 0.5850
1664/4849 [=========>....................] - ETA: 4:50 - loss: 0.6750 - acc: 0.5871
1728/4849 [=========>....................] - ETA: 4:44 - loss: 0.6748 - acc: 0.5839
1792/4849 [==========>...................] - ETA: 4:38 - loss: 0.6767 - acc: 0.5815
1856/4849 [==========>...................] - ETA: 4:32 - loss: 0.6760 - acc: 0.5835
1920/4849 [==========>...................] - ETA: 4:26 - loss: 0.6756 - acc: 0.5839
1984/4849 [===========>..................] - ETA: 4:19 - loss: 0.6766 - acc: 0.5822
2048/4849 [===========>..................] - ETA: 4:13 - loss: 0.6769 - acc: 0.5815
2112/4849 [============>.................] - ETA: 4:07 - loss: 0.6767 - acc: 0.5805
2176/4849 [============>.................] - ETA: 4:01 - loss: 0.6765 - acc: 0.5823
2240/4849 [============>.................] - ETA: 3:55 - loss: 0.6776 - acc: 0.5790
2304/4849 [=============>................] - ETA: 3:49 - loss: 0.6782 - acc: 0.5773
2368/4849 [=============>................] - ETA: 3:43 - loss: 0.6785 - acc: 0.5769
2432/4849 [==============>...............] - ETA: 3:37 - loss: 0.6784 - acc: 0.5769
2496/4849 [==============>...............] - ETA: 3:31 - loss: 0.6780 - acc: 0.5769
2560/4849 [==============>...............] - ETA: 3:25 - loss: 0.6791 - acc: 0.5750
2624/4849 [===============>..............] - ETA: 3:19 - loss: 0.6780 - acc: 0.5770
2688/4849 [===============>..............] - ETA: 3:13 - loss: 0.6774 - acc: 0.5770
2752/4849 [================>.............] - ETA: 3:08 - loss: 0.6774 - acc: 0.5749
2816/4849 [================>.............] - ETA: 3:02 - loss: 0.6774 - acc: 0.5742
2880/4849 [================>.............] - ETA: 2:56 - loss: 0.6771 - acc: 0.5747
2944/4849 [=================>............] - ETA: 2:50 - loss: 0.6774 - acc: 0.5751
3008/4849 [=================>............] - ETA: 2:44 - loss: 0.6771 - acc: 0.5758
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6767 - acc: 0.5775
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6774 - acc: 0.5768
3200/4849 [==================>...........] - ETA: 2:27 - loss: 0.6778 - acc: 0.5763
3264/4849 [===================>..........] - ETA: 2:21 - loss: 0.6776 - acc: 0.5757
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6778 - acc: 0.5751
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6793 - acc: 0.5725
3456/4849 [====================>.........] - ETA: 2:04 - loss: 0.6802 - acc: 0.5712
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6807 - acc: 0.5687
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6810 - acc: 0.5672
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6809 - acc: 0.5680
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6818 - acc: 0.5652
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6813 - acc: 0.5654
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6817 - acc: 0.5641
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6819 - acc: 0.5633
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6823 - acc: 0.5620
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6829 - acc: 0.5608
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6832 - acc: 0.5601
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6828 - acc: 0.5618
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6832 - acc: 0.5613 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6830 - acc: 0.5613
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6824 - acc: 0.5636
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6820 - acc: 0.5650
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6819 - acc: 0.5647
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6820 - acc: 0.5647
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6825 - acc: 0.5647
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6824 - acc: 0.5655
4736/4849 [============================>.] - ETA: 10s - loss: 0.6819 - acc: 0.5665
4800/4849 [============================>.] - ETA: 4s - loss: 0.6819 - acc: 0.5656 
4849/4849 [==============================] - 449s 93ms/step - loss: 0.6819 - acc: 0.5655 - val_loss: 0.7040 - val_acc: 0.5343

Epoch 00006: val_acc did not improve from 0.54731
Epoch 7/10

  64/4849 [..............................] - ETA: 6:43 - loss: 0.6746 - acc: 0.5156
 128/4849 [..............................] - ETA: 6:51 - loss: 0.6769 - acc: 0.5312
 192/4849 [>.............................] - ETA: 6:52 - loss: 0.6721 - acc: 0.5417
 256/4849 [>.............................] - ETA: 6:51 - loss: 0.6640 - acc: 0.5703
 320/4849 [>.............................] - ETA: 6:50 - loss: 0.6737 - acc: 0.5500
 384/4849 [=>............................] - ETA: 6:42 - loss: 0.6715 - acc: 0.5651
 448/4849 [=>............................] - ETA: 6:35 - loss: 0.6708 - acc: 0.5670
 512/4849 [==>...........................] - ETA: 6:32 - loss: 0.6711 - acc: 0.5742
 576/4849 [==>...........................] - ETA: 6:29 - loss: 0.6752 - acc: 0.5677
 640/4849 [==>...........................] - ETA: 6:25 - loss: 0.6777 - acc: 0.5641
 704/4849 [===>..........................] - ETA: 6:17 - loss: 0.6768 - acc: 0.5710
 768/4849 [===>..........................] - ETA: 6:14 - loss: 0.6765 - acc: 0.5690
 832/4849 [====>.........................] - ETA: 6:09 - loss: 0.6755 - acc: 0.5733
 896/4849 [====>.........................] - ETA: 6:04 - loss: 0.6735 - acc: 0.5770
 960/4849 [====>.........................] - ETA: 5:57 - loss: 0.6723 - acc: 0.5792
1024/4849 [=====>........................] - ETA: 5:51 - loss: 0.6709 - acc: 0.5820
1088/4849 [=====>........................] - ETA: 5:44 - loss: 0.6732 - acc: 0.5763
1152/4849 [======>.......................] - ETA: 5:39 - loss: 0.6741 - acc: 0.5764
1216/4849 [======>.......................] - ETA: 5:35 - loss: 0.6729 - acc: 0.5798
1280/4849 [======>.......................] - ETA: 5:28 - loss: 0.6742 - acc: 0.5758
1344/4849 [=======>......................] - ETA: 5:22 - loss: 0.6730 - acc: 0.5781
1408/4849 [=======>......................] - ETA: 5:15 - loss: 0.6745 - acc: 0.5717
1472/4849 [========>.....................] - ETA: 5:09 - loss: 0.6754 - acc: 0.5700
1536/4849 [========>.....................] - ETA: 5:03 - loss: 0.6734 - acc: 0.5755
1600/4849 [========>.....................] - ETA: 4:59 - loss: 0.6727 - acc: 0.5769
1664/4849 [=========>....................] - ETA: 4:52 - loss: 0.6733 - acc: 0.5757
1728/4849 [=========>....................] - ETA: 4:45 - loss: 0.6736 - acc: 0.5758
1792/4849 [==========>...................] - ETA: 4:38 - loss: 0.6728 - acc: 0.5776
1856/4849 [==========>...................] - ETA: 4:32 - loss: 0.6730 - acc: 0.5803
1920/4849 [==========>...................] - ETA: 4:27 - loss: 0.6733 - acc: 0.5781
1984/4849 [===========>..................] - ETA: 4:21 - loss: 0.6733 - acc: 0.5781
2048/4849 [===========>..................] - ETA: 4:16 - loss: 0.6735 - acc: 0.5791
2112/4849 [============>.................] - ETA: 4:09 - loss: 0.6745 - acc: 0.5777
2176/4849 [============>.................] - ETA: 4:03 - loss: 0.6754 - acc: 0.5763
2240/4849 [============>.................] - ETA: 3:58 - loss: 0.6757 - acc: 0.5763
2304/4849 [=============>................] - ETA: 3:51 - loss: 0.6761 - acc: 0.5760
2368/4849 [=============>................] - ETA: 3:46 - loss: 0.6759 - acc: 0.5764
2432/4849 [==============>...............] - ETA: 3:40 - loss: 0.6756 - acc: 0.5785
2496/4849 [==============>...............] - ETA: 3:34 - loss: 0.6757 - acc: 0.5785
2560/4849 [==============>...............] - ETA: 3:28 - loss: 0.6762 - acc: 0.5770
2624/4849 [===============>..............] - ETA: 3:22 - loss: 0.6760 - acc: 0.5770
2688/4849 [===============>..............] - ETA: 3:16 - loss: 0.6759 - acc: 0.5770
2752/4849 [================>.............] - ETA: 3:10 - loss: 0.6753 - acc: 0.5774
2816/4849 [================>.............] - ETA: 3:04 - loss: 0.6751 - acc: 0.5774
2880/4849 [================>.............] - ETA: 2:58 - loss: 0.6752 - acc: 0.5778
2944/4849 [=================>............] - ETA: 2:52 - loss: 0.6748 - acc: 0.5795
3008/4849 [=================>............] - ETA: 2:46 - loss: 0.6746 - acc: 0.5811
3072/4849 [==================>...........] - ETA: 2:40 - loss: 0.6740 - acc: 0.5820
3136/4849 [==================>...........] - ETA: 2:34 - loss: 0.6745 - acc: 0.5816
3200/4849 [==================>...........] - ETA: 2:29 - loss: 0.6740 - acc: 0.5834
3264/4849 [===================>..........] - ETA: 2:23 - loss: 0.6729 - acc: 0.5855
3328/4849 [===================>..........] - ETA: 2:17 - loss: 0.6723 - acc: 0.5871
3392/4849 [===================>..........] - ETA: 2:12 - loss: 0.6731 - acc: 0.5855
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.6723 - acc: 0.5865
3520/4849 [====================>.........] - ETA: 2:00 - loss: 0.6718 - acc: 0.5861
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6721 - acc: 0.5859
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.6729 - acc: 0.5844
3712/4849 [=====================>........] - ETA: 1:42 - loss: 0.6726 - acc: 0.5835
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6728 - acc: 0.5837
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6719 - acc: 0.5839
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6724 - acc: 0.5832
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6724 - acc: 0.5844
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6723 - acc: 0.5853
4096/4849 [========================>.....] - ETA: 1:08 - loss: 0.6727 - acc: 0.5840
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6728 - acc: 0.5832
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6730 - acc: 0.5819 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6724 - acc: 0.5830
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6725 - acc: 0.5825
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6730 - acc: 0.5815
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6733 - acc: 0.5808
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6729 - acc: 0.5814
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6736 - acc: 0.5803
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6746 - acc: 0.5796
4736/4849 [============================>.] - ETA: 10s - loss: 0.6746 - acc: 0.5796
4800/4849 [============================>.] - ETA: 4s - loss: 0.6753 - acc: 0.5781 
4849/4849 [==============================] - 454s 94ms/step - loss: 0.6755 - acc: 0.5776 - val_loss: 0.7049 - val_acc: 0.5213

Epoch 00007: val_acc did not improve from 0.54731
Epoch 8/10

  64/4849 [..............................] - ETA: 6:37 - loss: 0.6590 - acc: 0.6406
 128/4849 [..............................] - ETA: 6:46 - loss: 0.6779 - acc: 0.5859
 192/4849 [>.............................] - ETA: 6:52 - loss: 0.6801 - acc: 0.5625
 256/4849 [>.............................] - ETA: 6:44 - loss: 0.6735 - acc: 0.5859
 320/4849 [>.............................] - ETA: 6:41 - loss: 0.6655 - acc: 0.6125
 384/4849 [=>............................] - ETA: 6:44 - loss: 0.6719 - acc: 0.5990
 448/4849 [=>............................] - ETA: 6:40 - loss: 0.6736 - acc: 0.6049
 512/4849 [==>...........................] - ETA: 6:34 - loss: 0.6757 - acc: 0.6035
 576/4849 [==>...........................] - ETA: 6:29 - loss: 0.6737 - acc: 0.5990
 640/4849 [==>...........................] - ETA: 6:23 - loss: 0.6752 - acc: 0.5922
 704/4849 [===>..........................] - ETA: 6:16 - loss: 0.6692 - acc: 0.6023
 768/4849 [===>..........................] - ETA: 6:11 - loss: 0.6671 - acc: 0.6042
 832/4849 [====>.........................] - ETA: 6:05 - loss: 0.6618 - acc: 0.6166
 896/4849 [====>.........................] - ETA: 6:00 - loss: 0.6652 - acc: 0.6138
 960/4849 [====>.........................] - ETA: 5:54 - loss: 0.6670 - acc: 0.6104
1024/4849 [=====>........................] - ETA: 5:49 - loss: 0.6676 - acc: 0.6045
1088/4849 [=====>........................] - ETA: 5:42 - loss: 0.6701 - acc: 0.6029
1152/4849 [======>.......................] - ETA: 5:37 - loss: 0.6714 - acc: 0.5990
1216/4849 [======>.......................] - ETA: 5:32 - loss: 0.6750 - acc: 0.5946
1280/4849 [======>.......................] - ETA: 5:26 - loss: 0.6775 - acc: 0.5906
1344/4849 [=======>......................] - ETA: 5:19 - loss: 0.6802 - acc: 0.5863
1408/4849 [=======>......................] - ETA: 5:13 - loss: 0.6811 - acc: 0.5859
1472/4849 [========>.....................] - ETA: 5:06 - loss: 0.6809 - acc: 0.5897
1536/4849 [========>.....................] - ETA: 5:00 - loss: 0.6791 - acc: 0.5924
1600/4849 [========>.....................] - ETA: 4:54 - loss: 0.6775 - acc: 0.5938
1664/4849 [=========>....................] - ETA: 4:48 - loss: 0.6779 - acc: 0.5931
1728/4849 [=========>....................] - ETA: 4:42 - loss: 0.6774 - acc: 0.5920
1792/4849 [==========>...................] - ETA: 4:36 - loss: 0.6768 - acc: 0.5926
1856/4849 [==========>...................] - ETA: 4:30 - loss: 0.6763 - acc: 0.5916
1920/4849 [==========>...................] - ETA: 4:24 - loss: 0.6755 - acc: 0.5948
1984/4849 [===========>..................] - ETA: 4:18 - loss: 0.6749 - acc: 0.5948
2048/4849 [===========>..................] - ETA: 4:12 - loss: 0.6762 - acc: 0.5918
2112/4849 [============>.................] - ETA: 4:05 - loss: 0.6758 - acc: 0.5919
2176/4849 [============>.................] - ETA: 3:59 - loss: 0.6757 - acc: 0.5915
2240/4849 [============>.................] - ETA: 3:53 - loss: 0.6762 - acc: 0.5906
2304/4849 [=============>................] - ETA: 3:47 - loss: 0.6768 - acc: 0.5881
2368/4849 [=============>................] - ETA: 3:42 - loss: 0.6779 - acc: 0.5861
2432/4849 [==============>...............] - ETA: 3:36 - loss: 0.6785 - acc: 0.5851
2496/4849 [==============>...............] - ETA: 3:30 - loss: 0.6781 - acc: 0.5857
2560/4849 [==============>...............] - ETA: 3:24 - loss: 0.6784 - acc: 0.5844
2624/4849 [===============>..............] - ETA: 3:18 - loss: 0.6788 - acc: 0.5823
2688/4849 [===============>..............] - ETA: 3:12 - loss: 0.6769 - acc: 0.5848
2752/4849 [================>.............] - ETA: 3:06 - loss: 0.6771 - acc: 0.5854
2816/4849 [================>.............] - ETA: 3:00 - loss: 0.6777 - acc: 0.5838
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6791 - acc: 0.5809
2944/4849 [=================>............] - ETA: 2:49 - loss: 0.6787 - acc: 0.5808
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6776 - acc: 0.5814
3072/4849 [==================>...........] - ETA: 2:37 - loss: 0.6770 - acc: 0.5820
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6772 - acc: 0.5816
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6768 - acc: 0.5813
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6766 - acc: 0.5815
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6769 - acc: 0.5808
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6772 - acc: 0.5802
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6767 - acc: 0.5816
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6768 - acc: 0.5815
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6773 - acc: 0.5815
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6770 - acc: 0.5809
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6770 - acc: 0.5808
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6776 - acc: 0.5808
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6770 - acc: 0.5810
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6762 - acc: 0.5812
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6759 - acc: 0.5817
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6750 - acc: 0.5833
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6751 - acc: 0.5835
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6755 - acc: 0.5837
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6743 - acc: 0.5855 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6739 - acc: 0.5865
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6749 - acc: 0.5855
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6752 - acc: 0.5863
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6762 - acc: 0.5857
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6765 - acc: 0.5852
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6769 - acc: 0.5844
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6765 - acc: 0.5856
4736/4849 [============================>.] - ETA: 10s - loss: 0.6766 - acc: 0.5861
4800/4849 [============================>.] - ETA: 4s - loss: 0.6764 - acc: 0.5869 
4849/4849 [==============================] - 448s 92ms/step - loss: 0.6765 - acc: 0.5865 - val_loss: 0.6940 - val_acc: 0.5584

Epoch 00008: val_acc improved from 0.54731 to 0.55844, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window16/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 9/10

  64/4849 [..............................] - ETA: 6:55 - loss: 0.7024 - acc: 0.5156
 128/4849 [..............................] - ETA: 6:47 - loss: 0.6707 - acc: 0.5625
 192/4849 [>.............................] - ETA: 6:39 - loss: 0.6546 - acc: 0.6198
 256/4849 [>.............................] - ETA: 6:51 - loss: 0.6526 - acc: 0.6211
 320/4849 [>.............................] - ETA: 6:45 - loss: 0.6527 - acc: 0.6281
 384/4849 [=>............................] - ETA: 6:37 - loss: 0.6589 - acc: 0.6146
 448/4849 [=>............................] - ETA: 6:33 - loss: 0.6615 - acc: 0.6071
 512/4849 [==>...........................] - ETA: 6:25 - loss: 0.6656 - acc: 0.5977
 576/4849 [==>...........................] - ETA: 6:18 - loss: 0.6646 - acc: 0.6059
 640/4849 [==>...........................] - ETA: 6:16 - loss: 0.6628 - acc: 0.6078
 704/4849 [===>..........................] - ETA: 6:10 - loss: 0.6608 - acc: 0.6122
 768/4849 [===>..........................] - ETA: 6:02 - loss: 0.6632 - acc: 0.6042
 832/4849 [====>.........................] - ETA: 5:54 - loss: 0.6644 - acc: 0.5986
 896/4849 [====>.........................] - ETA: 5:50 - loss: 0.6624 - acc: 0.6049
 960/4849 [====>.........................] - ETA: 5:44 - loss: 0.6632 - acc: 0.6031
1024/4849 [=====>........................] - ETA: 5:39 - loss: 0.6639 - acc: 0.6016
1088/4849 [=====>........................] - ETA: 5:35 - loss: 0.6671 - acc: 0.5983
1152/4849 [======>.......................] - ETA: 5:29 - loss: 0.6655 - acc: 0.6007
1216/4849 [======>.......................] - ETA: 5:22 - loss: 0.6651 - acc: 0.6028
1280/4849 [======>.......................] - ETA: 5:17 - loss: 0.6667 - acc: 0.6000
1344/4849 [=======>......................] - ETA: 5:09 - loss: 0.6662 - acc: 0.6004
1408/4849 [=======>......................] - ETA: 5:02 - loss: 0.6669 - acc: 0.5987
1472/4849 [========>.....................] - ETA: 4:58 - loss: 0.6659 - acc: 0.5999
1536/4849 [========>.....................] - ETA: 4:51 - loss: 0.6633 - acc: 0.6048
1600/4849 [========>.....................] - ETA: 4:44 - loss: 0.6662 - acc: 0.5994
1664/4849 [=========>....................] - ETA: 4:38 - loss: 0.6669 - acc: 0.5992
1728/4849 [=========>....................] - ETA: 4:32 - loss: 0.6672 - acc: 0.5978
1792/4849 [==========>...................] - ETA: 4:27 - loss: 0.6680 - acc: 0.5965
1856/4849 [==========>...................] - ETA: 4:21 - loss: 0.6685 - acc: 0.5938
1920/4849 [==========>...................] - ETA: 4:16 - loss: 0.6695 - acc: 0.5917
1984/4849 [===========>..................] - ETA: 4:11 - loss: 0.6700 - acc: 0.5907
2048/4849 [===========>..................] - ETA: 4:06 - loss: 0.6717 - acc: 0.5884
2112/4849 [============>.................] - ETA: 4:00 - loss: 0.6721 - acc: 0.5881
2176/4849 [============>.................] - ETA: 3:55 - loss: 0.6711 - acc: 0.5905
2240/4849 [============>.................] - ETA: 3:50 - loss: 0.6731 - acc: 0.5871
2304/4849 [=============>................] - ETA: 3:44 - loss: 0.6738 - acc: 0.5842
2368/4849 [=============>................] - ETA: 3:39 - loss: 0.6745 - acc: 0.5836
2432/4849 [==============>...............] - ETA: 3:33 - loss: 0.6744 - acc: 0.5839
2496/4849 [==============>...............] - ETA: 3:27 - loss: 0.6761 - acc: 0.5805
2560/4849 [==============>...............] - ETA: 3:21 - loss: 0.6763 - acc: 0.5797
2624/4849 [===============>..............] - ETA: 3:15 - loss: 0.6764 - acc: 0.5789
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.6757 - acc: 0.5789
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.6763 - acc: 0.5759
2816/4849 [================>.............] - ETA: 2:58 - loss: 0.6762 - acc: 0.5767
2880/4849 [================>.............] - ETA: 2:53 - loss: 0.6762 - acc: 0.5764
2944/4849 [=================>............] - ETA: 2:47 - loss: 0.6756 - acc: 0.5778
3008/4849 [=================>............] - ETA: 2:41 - loss: 0.6754 - acc: 0.5781
3072/4849 [==================>...........] - ETA: 2:35 - loss: 0.6752 - acc: 0.5794
3136/4849 [==================>...........] - ETA: 2:29 - loss: 0.6750 - acc: 0.5791
3200/4849 [==================>...........] - ETA: 2:24 - loss: 0.6753 - acc: 0.5778
3264/4849 [===================>..........] - ETA: 2:18 - loss: 0.6756 - acc: 0.5766
3328/4849 [===================>..........] - ETA: 2:12 - loss: 0.6748 - acc: 0.5781
3392/4849 [===================>..........] - ETA: 2:06 - loss: 0.6748 - acc: 0.5772
3456/4849 [====================>.........] - ETA: 2:00 - loss: 0.6749 - acc: 0.5761
3520/4849 [====================>.........] - ETA: 1:55 - loss: 0.6744 - acc: 0.5781
3584/4849 [=====================>........] - ETA: 1:49 - loss: 0.6745 - acc: 0.5776
3648/4849 [=====================>........] - ETA: 1:44 - loss: 0.6738 - acc: 0.5787
3712/4849 [=====================>........] - ETA: 1:38 - loss: 0.6735 - acc: 0.5795
3776/4849 [======================>.......] - ETA: 1:33 - loss: 0.6731 - acc: 0.5805
3840/4849 [======================>.......] - ETA: 1:27 - loss: 0.6728 - acc: 0.5818
3904/4849 [=======================>......] - ETA: 1:22 - loss: 0.6732 - acc: 0.5812
3968/4849 [=======================>......] - ETA: 1:16 - loss: 0.6724 - acc: 0.5832
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6736 - acc: 0.5813
4096/4849 [========================>.....] - ETA: 1:05 - loss: 0.6737 - acc: 0.5818
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6743 - acc: 0.5805
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6737 - acc: 0.5821 
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6742 - acc: 0.5814
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6737 - acc: 0.5823
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6735 - acc: 0.5824
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6738 - acc: 0.5824
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6737 - acc: 0.5830
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6731 - acc: 0.5840
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6728 - acc: 0.5848
4736/4849 [============================>.] - ETA: 9s - loss: 0.6727 - acc: 0.5847 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6725 - acc: 0.5846
4849/4849 [==============================] - 439s 91ms/step - loss: 0.6723 - acc: 0.5859 - val_loss: 0.7036 - val_acc: 0.5083

Epoch 00009: val_acc did not improve from 0.55844
Epoch 10/10

  64/4849 [..............................] - ETA: 6:29 - loss: 0.6463 - acc: 0.6250
 128/4849 [..............................] - ETA: 6:26 - loss: 0.6369 - acc: 0.6406
 192/4849 [>.............................] - ETA: 6:34 - loss: 0.6664 - acc: 0.5885
 256/4849 [>.............................] - ETA: 6:32 - loss: 0.6626 - acc: 0.5977
 320/4849 [>.............................] - ETA: 6:24 - loss: 0.6582 - acc: 0.6094
 384/4849 [=>............................] - ETA: 6:27 - loss: 0.6614 - acc: 0.6042
 448/4849 [=>............................] - ETA: 6:19 - loss: 0.6630 - acc: 0.6004
 512/4849 [==>...........................] - ETA: 6:10 - loss: 0.6592 - acc: 0.6016
 576/4849 [==>...........................] - ETA: 6:06 - loss: 0.6601 - acc: 0.5990
 640/4849 [==>...........................] - ETA: 6:00 - loss: 0.6583 - acc: 0.6062
 704/4849 [===>..........................] - ETA: 5:57 - loss: 0.6560 - acc: 0.6108
 768/4849 [===>..........................] - ETA: 5:50 - loss: 0.6554 - acc: 0.6133
 832/4849 [====>.........................] - ETA: 5:45 - loss: 0.6605 - acc: 0.6046
 896/4849 [====>.........................] - ETA: 5:38 - loss: 0.6600 - acc: 0.6071
 960/4849 [====>.........................] - ETA: 5:34 - loss: 0.6636 - acc: 0.5979
1024/4849 [=====>........................] - ETA: 5:29 - loss: 0.6638 - acc: 0.6016
1088/4849 [=====>........................] - ETA: 5:22 - loss: 0.6654 - acc: 0.5993
1152/4849 [======>.......................] - ETA: 5:18 - loss: 0.6705 - acc: 0.5929
1216/4849 [======>.......................] - ETA: 5:12 - loss: 0.6708 - acc: 0.5929
1280/4849 [======>.......................] - ETA: 5:06 - loss: 0.6686 - acc: 0.5984
1344/4849 [=======>......................] - ETA: 5:02 - loss: 0.6670 - acc: 0.6012
1408/4849 [=======>......................] - ETA: 4:56 - loss: 0.6683 - acc: 0.6009
1472/4849 [========>.....................] - ETA: 4:50 - loss: 0.6672 - acc: 0.6026
1536/4849 [========>.....................] - ETA: 4:45 - loss: 0.6669 - acc: 0.6029
1600/4849 [========>.....................] - ETA: 4:38 - loss: 0.6672 - acc: 0.6056
1664/4849 [=========>....................] - ETA: 4:32 - loss: 0.6696 - acc: 0.6004
1728/4849 [=========>....................] - ETA: 4:27 - loss: 0.6693 - acc: 0.6013
1792/4849 [==========>...................] - ETA: 4:21 - loss: 0.6697 - acc: 0.6010
1856/4849 [==========>...................] - ETA: 4:16 - loss: 0.6694 - acc: 0.6008
1920/4849 [==========>...................] - ETA: 4:12 - loss: 0.6703 - acc: 0.6005
1984/4849 [===========>..................] - ETA: 4:06 - loss: 0.6726 - acc: 0.5963
2048/4849 [===========>..................] - ETA: 4:01 - loss: 0.6722 - acc: 0.5967
2112/4849 [============>.................] - ETA: 3:55 - loss: 0.6727 - acc: 0.5966
2176/4849 [============>.................] - ETA: 3:50 - loss: 0.6736 - acc: 0.5956
2240/4849 [============>.................] - ETA: 3:45 - loss: 0.6726 - acc: 0.5982
2304/4849 [=============>................] - ETA: 3:39 - loss: 0.6735 - acc: 0.5968
2368/4849 [=============>................] - ETA: 3:33 - loss: 0.6724 - acc: 0.5980
2432/4849 [==============>...............] - ETA: 3:28 - loss: 0.6724 - acc: 0.5979
2496/4849 [==============>...............] - ETA: 3:22 - loss: 0.6726 - acc: 0.5974
2560/4849 [==============>...............] - ETA: 3:17 - loss: 0.6718 - acc: 0.5992
2624/4849 [===============>..............] - ETA: 3:12 - loss: 0.6723 - acc: 0.5976
2688/4849 [===============>..............] - ETA: 3:06 - loss: 0.6730 - acc: 0.5960
2752/4849 [================>.............] - ETA: 3:01 - loss: 0.6740 - acc: 0.5952
2816/4849 [================>.............] - ETA: 2:55 - loss: 0.6738 - acc: 0.5966
2880/4849 [================>.............] - ETA: 2:50 - loss: 0.6739 - acc: 0.5972
2944/4849 [=================>............] - ETA: 2:45 - loss: 0.6735 - acc: 0.5978
3008/4849 [=================>............] - ETA: 2:39 - loss: 0.6734 - acc: 0.5977
3072/4849 [==================>...........] - ETA: 2:33 - loss: 0.6735 - acc: 0.5967
3136/4849 [==================>...........] - ETA: 2:28 - loss: 0.6740 - acc: 0.5947
3200/4849 [==================>...........] - ETA: 2:22 - loss: 0.6737 - acc: 0.5953
3264/4849 [===================>..........] - ETA: 2:17 - loss: 0.6744 - acc: 0.5934
3328/4849 [===================>..........] - ETA: 2:12 - loss: 0.6743 - acc: 0.5919
3392/4849 [===================>..........] - ETA: 2:06 - loss: 0.6749 - acc: 0.5908
3456/4849 [====================>.........] - ETA: 2:00 - loss: 0.6747 - acc: 0.5909
3520/4849 [====================>.........] - ETA: 1:55 - loss: 0.6743 - acc: 0.5912
3584/4849 [=====================>........] - ETA: 1:49 - loss: 0.6747 - acc: 0.5898
3648/4849 [=====================>........] - ETA: 1:44 - loss: 0.6747 - acc: 0.5896
3712/4849 [=====================>........] - ETA: 1:38 - loss: 0.6749 - acc: 0.5886
3776/4849 [======================>.......] - ETA: 1:33 - loss: 0.6750 - acc: 0.5887
3840/4849 [======================>.......] - ETA: 1:27 - loss: 0.6748 - acc: 0.5878
3904/4849 [=======================>......] - ETA: 1:22 - loss: 0.6743 - acc: 0.5897
3968/4849 [=======================>......] - ETA: 1:16 - loss: 0.6743 - acc: 0.5892
4032/4849 [=======================>......] - ETA: 1:10 - loss: 0.6744 - acc: 0.5895
4096/4849 [========================>.....] - ETA: 1:05 - loss: 0.6735 - acc: 0.5903
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6734 - acc: 0.5904 
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6735 - acc: 0.5902
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6736 - acc: 0.5900
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6730 - acc: 0.5908
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6736 - acc: 0.5892
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6733 - acc: 0.5893
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6730 - acc: 0.5900
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6729 - acc: 0.5894
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6734 - acc: 0.5882
4736/4849 [============================>.] - ETA: 9s - loss: 0.6737 - acc: 0.5872 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6742 - acc: 0.5867
4849/4849 [==============================] - 437s 90ms/step - loss: 0.6747 - acc: 0.5855 - val_loss: 0.6928 - val_acc: 0.5566

Epoch 00010: val_acc did not improve from 0.55844
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9bf897a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9bf897a710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9bf8791a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9bf8791a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf85d6e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf85d6e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf84cb790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf84cb790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf83e2890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf83e2890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf83f2910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf83f2910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf82be050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf82be050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf83ae910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf83ae910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf8388a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf8388a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf8279cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf8279cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf81d4dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf81d4dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf83fc610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf83fc610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf819a510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf819a510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf823ba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf823ba90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf81b9d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf81b9d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7d95450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7d95450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7f800d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7f800d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96e004e310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96e004e310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf7e6d850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf7e6d850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf7b7d410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf7b7d410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7ee0cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7ee0cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7c34410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7c34410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7aa6310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7aa6310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf794c310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf794c310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf7994e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf7994e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7b96710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7b96710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7a65050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7a65050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7836bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7836bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf75fec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf75fec90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf78c9790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf78c9790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf775c190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf775c190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf81b9390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf81b9390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7559510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7559510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf7382950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf7382950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf75601d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf75601d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7358950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7358950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7b44650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7b44650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf754f790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf754f790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf7315790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf7315790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf7015990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf7015990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf82ef310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf82ef310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7107b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7107b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf6f9b610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf6f9b610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf6d42950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf6d42950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf6ccf190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf6ccf190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf6cdb450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf6cdb450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf6db1e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf6db1e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf6bc4690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf6bc4690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96e00e8990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96e00e8990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf6b70e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf6b70e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf6ca3210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf6ca3210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7045dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf7045dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf68dc250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf68dc250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf66aa850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf66aa850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf68c8150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf68c8150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf69e8ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf69e8ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf66aa250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf66aa250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf6699550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf6699550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf6a1c0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf6a1c0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf76b1610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf76b1610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf61507d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf61507d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf634fe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf634fe50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf632e110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf632e110>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 1:24
 128/1348 [=>............................] - ETA: 1:03
 192/1348 [===>..........................] - ETA: 52s 
 256/1348 [====>.........................] - ETA: 48s
 320/1348 [======>.......................] - ETA: 44s
 384/1348 [=======>......................] - ETA: 41s
 448/1348 [========>.....................] - ETA: 38s
 512/1348 [==========>...................] - ETA: 37s
 576/1348 [===========>..................] - ETA: 33s
 640/1348 [=============>................] - ETA: 30s
 704/1348 [==============>...............] - ETA: 26s
 768/1348 [================>.............] - ETA: 23s
 832/1348 [=================>............] - ETA: 21s
 896/1348 [==================>...........] - ETA: 18s
 960/1348 [====================>.........] - ETA: 15s
1024/1348 [=====================>........] - ETA: 12s
1088/1348 [=======================>......] - ETA: 10s
1152/1348 [========================>.....] - ETA: 7s 
1216/1348 [==========================>...] - ETA: 5s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 53s 39ms/step
loss: 0.6726121578796681
acc: 0.5845697329376854
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f96e00c68d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f96e00c68d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f96e00493d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f96e00493d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beb108150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beb108150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9beb118b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9beb118b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf870fc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bf870fc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96e0065810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96e0065810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96e0052890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96e0052890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf8836590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf8836590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf8819c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bf8819c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96c07126d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96c07126d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96e0059f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96e0059f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf8819850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bf8819850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c0760710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c0760710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96c04fc590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96c04fc590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96c0589590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96c0589590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c063ef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c063ef90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96c04fc910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96c04fc910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c0410d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c0410d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96c0217c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96c0217c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96c053f9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96c053f9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a07ee510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a07ee510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96c0367d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96c0367d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c02b2d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c02b2d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96a0709cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96a0709cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96a0666990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96a0666990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a06ff450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a06ff450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96a05d4990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96a05d4990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a06e14d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a06e14d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96a040f350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96a040f350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96a0239c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96a0239c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a0433890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a0433890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96a042d050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96a042d050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a027dc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a027dc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96a03b2750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96a03b2750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f969870fd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f969870fd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a00d4150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a00d4150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96a00c1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96a00c1c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a027b6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96a027b6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969859c950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969859c950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f969842c510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f969842c510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9698478ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9698478ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96985af950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96985af950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9698557050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9698557050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969824dd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969824dd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f969821d550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f969821d550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969830c790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969830c790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f969824d690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f969824d690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9698119350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9698119350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95047b7390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95047b7390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95047b3810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95047b3810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95046e9250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95046e9250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95046f27d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95046f27d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9504466ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9504466ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95043746d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95043746d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9504348d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9504348d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95042f9cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95042f9cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9504374850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9504374850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f950412c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f950412c1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f950408c150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f950408c150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94f0679590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94f0679590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94f05d0690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94f05d0690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95040c5410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95040c5410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95046aeb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95046aeb50>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 21:13 - loss: 0.7448 - acc: 0.4844
 128/4849 [..............................] - ETA: 14:02 - loss: 0.7468 - acc: 0.5000
 192/4849 [>.............................] - ETA: 11:46 - loss: 0.7306 - acc: 0.5365
 256/4849 [>.............................] - ETA: 10:40 - loss: 0.7348 - acc: 0.5312
 320/4849 [>.............................] - ETA: 10:03 - loss: 0.7237 - acc: 0.5469
 384/4849 [=>............................] - ETA: 9:29 - loss: 0.7264 - acc: 0.5521 
 448/4849 [=>............................] - ETA: 9:05 - loss: 0.7225 - acc: 0.5580
 512/4849 [==>...........................] - ETA: 8:48 - loss: 0.7302 - acc: 0.5547
 576/4849 [==>...........................] - ETA: 8:38 - loss: 0.7218 - acc: 0.5573
 640/4849 [==>...........................] - ETA: 8:20 - loss: 0.7190 - acc: 0.5516
 704/4849 [===>..........................] - ETA: 8:06 - loss: 0.7163 - acc: 0.5568
 768/4849 [===>..........................] - ETA: 7:52 - loss: 0.7296 - acc: 0.5443
 832/4849 [====>.........................] - ETA: 7:41 - loss: 0.7324 - acc: 0.5397
 896/4849 [====>.........................] - ETA: 7:31 - loss: 0.7319 - acc: 0.5424
 960/4849 [====>.........................] - ETA: 7:19 - loss: 0.7282 - acc: 0.5479
1024/4849 [=====>........................] - ETA: 7:10 - loss: 0.7299 - acc: 0.5430
1088/4849 [=====>........................] - ETA: 7:01 - loss: 0.7284 - acc: 0.5414
1152/4849 [======>.......................] - ETA: 6:52 - loss: 0.7279 - acc: 0.5391
1216/4849 [======>.......................] - ETA: 6:42 - loss: 0.7287 - acc: 0.5321
1280/4849 [======>.......................] - ETA: 6:33 - loss: 0.7319 - acc: 0.5297
1344/4849 [=======>......................] - ETA: 6:27 - loss: 0.7338 - acc: 0.5246
1408/4849 [=======>......................] - ETA: 6:19 - loss: 0.7309 - acc: 0.5277
1472/4849 [========>.....................] - ETA: 6:10 - loss: 0.7293 - acc: 0.5285
1536/4849 [========>.....................] - ETA: 6:03 - loss: 0.7315 - acc: 0.5254
1600/4849 [========>.....................] - ETA: 5:55 - loss: 0.7321 - acc: 0.5256
1664/4849 [=========>....................] - ETA: 5:47 - loss: 0.7317 - acc: 0.5264
1728/4849 [=========>....................] - ETA: 5:40 - loss: 0.7332 - acc: 0.5243
1792/4849 [==========>...................] - ETA: 5:33 - loss: 0.7333 - acc: 0.5212
1856/4849 [==========>...................] - ETA: 5:25 - loss: 0.7324 - acc: 0.5232
1920/4849 [==========>...................] - ETA: 5:17 - loss: 0.7308 - acc: 0.5255
1984/4849 [===========>..................] - ETA: 5:10 - loss: 0.7298 - acc: 0.5242
2048/4849 [===========>..................] - ETA: 5:02 - loss: 0.7307 - acc: 0.5186
2112/4849 [============>.................] - ETA: 4:56 - loss: 0.7310 - acc: 0.5175
2176/4849 [============>.................] - ETA: 4:48 - loss: 0.7297 - acc: 0.5184
2240/4849 [============>.................] - ETA: 4:41 - loss: 0.7287 - acc: 0.5170
2304/4849 [=============>................] - ETA: 4:35 - loss: 0.7285 - acc: 0.5174
2368/4849 [=============>................] - ETA: 4:27 - loss: 0.7279 - acc: 0.5156
2432/4849 [==============>...............] - ETA: 4:20 - loss: 0.7274 - acc: 0.5160
2496/4849 [==============>...............] - ETA: 4:14 - loss: 0.7288 - acc: 0.5116
2560/4849 [==============>...............] - ETA: 4:07 - loss: 0.7281 - acc: 0.5129
2624/4849 [===============>..............] - ETA: 3:59 - loss: 0.7271 - acc: 0.5145
2688/4849 [===============>..............] - ETA: 3:52 - loss: 0.7265 - acc: 0.5156
2752/4849 [================>.............] - ETA: 3:45 - loss: 0.7253 - acc: 0.5160
2816/4849 [================>.............] - ETA: 3:38 - loss: 0.7251 - acc: 0.5156
2880/4849 [================>.............] - ETA: 3:30 - loss: 0.7250 - acc: 0.5156
2944/4849 [=================>............] - ETA: 3:24 - loss: 0.7247 - acc: 0.5146
3008/4849 [=================>............] - ETA: 3:17 - loss: 0.7241 - acc: 0.5153
3072/4849 [==================>...........] - ETA: 3:10 - loss: 0.7238 - acc: 0.5153
3136/4849 [==================>...........] - ETA: 3:03 - loss: 0.7245 - acc: 0.5137
3200/4849 [==================>...........] - ETA: 2:56 - loss: 0.7244 - acc: 0.5122
3264/4849 [===================>..........] - ETA: 2:49 - loss: 0.7241 - acc: 0.5116
3328/4849 [===================>..........] - ETA: 2:42 - loss: 0.7234 - acc: 0.5135
3392/4849 [===================>..........] - ETA: 2:35 - loss: 0.7228 - acc: 0.5144
3456/4849 [====================>.........] - ETA: 2:28 - loss: 0.7227 - acc: 0.5139
3520/4849 [====================>.........] - ETA: 2:21 - loss: 0.7225 - acc: 0.5151
3584/4849 [=====================>........] - ETA: 2:14 - loss: 0.7223 - acc: 0.5156
3648/4849 [=====================>........] - ETA: 2:07 - loss: 0.7228 - acc: 0.5137
3712/4849 [=====================>........] - ETA: 2:00 - loss: 0.7221 - acc: 0.5148
3776/4849 [======================>.......] - ETA: 1:53 - loss: 0.7217 - acc: 0.5148
3840/4849 [======================>.......] - ETA: 1:46 - loss: 0.7213 - acc: 0.5154
3904/4849 [=======================>......] - ETA: 1:40 - loss: 0.7210 - acc: 0.5161
3968/4849 [=======================>......] - ETA: 1:33 - loss: 0.7209 - acc: 0.5171
4032/4849 [=======================>......] - ETA: 1:26 - loss: 0.7207 - acc: 0.5164
4096/4849 [========================>.....] - ETA: 1:19 - loss: 0.7205 - acc: 0.5173
4160/4849 [========================>.....] - ETA: 1:12 - loss: 0.7193 - acc: 0.5190
4224/4849 [=========================>....] - ETA: 1:05 - loss: 0.7192 - acc: 0.5180
4288/4849 [=========================>....] - ETA: 58s - loss: 0.7192 - acc: 0.5189 
4352/4849 [=========================>....] - ETA: 51s - loss: 0.7186 - acc: 0.5188
4416/4849 [==========================>...] - ETA: 45s - loss: 0.7182 - acc: 0.5197
4480/4849 [==========================>...] - ETA: 38s - loss: 0.7174 - acc: 0.5212
4544/4849 [===========================>..] - ETA: 31s - loss: 0.7169 - acc: 0.5222
4608/4849 [===========================>..] - ETA: 24s - loss: 0.7176 - acc: 0.5208
4672/4849 [===========================>..] - ETA: 18s - loss: 0.7172 - acc: 0.5214
4736/4849 [============================>.] - ETA: 11s - loss: 0.7172 - acc: 0.5215
4800/4849 [============================>.] - ETA: 5s - loss: 0.7169 - acc: 0.5221 
4849/4849 [==============================] - 517s 107ms/step - loss: 0.7165 - acc: 0.5220 - val_loss: 0.6938 - val_acc: 0.5046

Epoch 00001: val_acc improved from -inf to 0.50464, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window17/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 7:01 - loss: 0.6830 - acc: 0.5469
 128/4849 [..............................] - ETA: 6:50 - loss: 0.6867 - acc: 0.5391
 192/4849 [>.............................] - ETA: 6:52 - loss: 0.7020 - acc: 0.5000
 256/4849 [>.............................] - ETA: 6:41 - loss: 0.7006 - acc: 0.5039
 320/4849 [>.............................] - ETA: 6:39 - loss: 0.7046 - acc: 0.5188
 384/4849 [=>............................] - ETA: 6:40 - loss: 0.6952 - acc: 0.5417
 448/4849 [=>............................] - ETA: 6:35 - loss: 0.6944 - acc: 0.5312
 512/4849 [==>...........................] - ETA: 6:30 - loss: 0.6894 - acc: 0.5371
 576/4849 [==>...........................] - ETA: 6:25 - loss: 0.6892 - acc: 0.5330
 640/4849 [==>...........................] - ETA: 6:19 - loss: 0.6905 - acc: 0.5312
 704/4849 [===>..........................] - ETA: 6:17 - loss: 0.6904 - acc: 0.5412
 768/4849 [===>..........................] - ETA: 6:09 - loss: 0.6873 - acc: 0.5404
 832/4849 [====>.........................] - ETA: 6:03 - loss: 0.6858 - acc: 0.5457
 896/4849 [====>.........................] - ETA: 5:58 - loss: 0.6869 - acc: 0.5458
 960/4849 [====>.........................] - ETA: 5:53 - loss: 0.6900 - acc: 0.5396
1024/4849 [=====>........................] - ETA: 5:47 - loss: 0.6898 - acc: 0.5371
1088/4849 [=====>........................] - ETA: 5:43 - loss: 0.6923 - acc: 0.5377
1152/4849 [======>.......................] - ETA: 5:36 - loss: 0.6947 - acc: 0.5321
1216/4849 [======>.......................] - ETA: 5:30 - loss: 0.6963 - acc: 0.5312
1280/4849 [======>.......................] - ETA: 5:24 - loss: 0.6961 - acc: 0.5344
1344/4849 [=======>......................] - ETA: 5:18 - loss: 0.6958 - acc: 0.5335
1408/4849 [=======>......................] - ETA: 5:14 - loss: 0.6985 - acc: 0.5291
1472/4849 [========>.....................] - ETA: 5:07 - loss: 0.6989 - acc: 0.5265
1536/4849 [========>.....................] - ETA: 5:01 - loss: 0.6987 - acc: 0.5247
1600/4849 [========>.....................] - ETA: 4:54 - loss: 0.6974 - acc: 0.5281
1664/4849 [=========>....................] - ETA: 4:48 - loss: 0.6986 - acc: 0.5276
1728/4849 [=========>....................] - ETA: 4:42 - loss: 0.6993 - acc: 0.5255
1792/4849 [==========>...................] - ETA: 4:36 - loss: 0.6991 - acc: 0.5268
1856/4849 [==========>...................] - ETA: 4:31 - loss: 0.6987 - acc: 0.5259
1920/4849 [==========>...................] - ETA: 4:24 - loss: 0.7002 - acc: 0.5240
1984/4849 [===========>..................] - ETA: 4:19 - loss: 0.6995 - acc: 0.5257
2048/4849 [===========>..................] - ETA: 4:13 - loss: 0.7000 - acc: 0.5239
2112/4849 [============>.................] - ETA: 4:07 - loss: 0.7007 - acc: 0.5223
2176/4849 [============>.................] - ETA: 4:01 - loss: 0.7003 - acc: 0.5244
2240/4849 [============>.................] - ETA: 3:55 - loss: 0.6992 - acc: 0.5259
2304/4849 [=============>................] - ETA: 3:49 - loss: 0.6986 - acc: 0.5282
2368/4849 [=============>................] - ETA: 3:43 - loss: 0.6993 - acc: 0.5274
2432/4849 [==============>...............] - ETA: 3:38 - loss: 0.6986 - acc: 0.5300
2496/4849 [==============>...............] - ETA: 3:32 - loss: 0.6991 - acc: 0.5292
2560/4849 [==============>...............] - ETA: 3:26 - loss: 0.6996 - acc: 0.5281
2624/4849 [===============>..............] - ETA: 3:21 - loss: 0.7006 - acc: 0.5274
2688/4849 [===============>..............] - ETA: 3:15 - loss: 0.7008 - acc: 0.5272
2752/4849 [================>.............] - ETA: 3:09 - loss: 0.7006 - acc: 0.5280
2816/4849 [================>.............] - ETA: 3:04 - loss: 0.7005 - acc: 0.5291
2880/4849 [================>.............] - ETA: 2:58 - loss: 0.7002 - acc: 0.5295
2944/4849 [=================>............] - ETA: 2:52 - loss: 0.7004 - acc: 0.5292
3008/4849 [=================>............] - ETA: 2:46 - loss: 0.7007 - acc: 0.5283
3072/4849 [==================>...........] - ETA: 2:40 - loss: 0.7003 - acc: 0.5283
3136/4849 [==================>...........] - ETA: 2:34 - loss: 0.6995 - acc: 0.5300
3200/4849 [==================>...........] - ETA: 2:29 - loss: 0.6996 - acc: 0.5294
3264/4849 [===================>..........] - ETA: 2:23 - loss: 0.7002 - acc: 0.5270
3328/4849 [===================>..........] - ETA: 2:17 - loss: 0.7000 - acc: 0.5270
3392/4849 [===================>..........] - ETA: 2:11 - loss: 0.6990 - acc: 0.5295
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.6984 - acc: 0.5307
3520/4849 [====================>.........] - ETA: 2:00 - loss: 0.6984 - acc: 0.5298
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6988 - acc: 0.5290
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.6982 - acc: 0.5307
3712/4849 [=====================>........] - ETA: 1:43 - loss: 0.6985 - acc: 0.5318
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6987 - acc: 0.5318
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6983 - acc: 0.5331
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6981 - acc: 0.5343
3968/4849 [=======================>......] - ETA: 1:20 - loss: 0.6983 - acc: 0.5330
4032/4849 [=======================>......] - ETA: 1:14 - loss: 0.6977 - acc: 0.5342
4096/4849 [========================>.....] - ETA: 1:08 - loss: 0.6971 - acc: 0.5352
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6967 - acc: 0.5351
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6968 - acc: 0.5336 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6964 - acc: 0.5336
4352/4849 [=========================>....] - ETA: 45s - loss: 0.6963 - acc: 0.5340
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6967 - acc: 0.5331
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6959 - acc: 0.5348
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6960 - acc: 0.5359
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6957 - acc: 0.5369
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6955 - acc: 0.5370
4736/4849 [============================>.] - ETA: 10s - loss: 0.6948 - acc: 0.5380
4800/4849 [============================>.] - ETA: 4s - loss: 0.6950 - acc: 0.5379 
4849/4849 [==============================] - 455s 94ms/step - loss: 0.6950 - acc: 0.5380 - val_loss: 0.6965 - val_acc: 0.5213

Epoch 00002: val_acc improved from 0.50464 to 0.52134, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window17/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 3/10

  64/4849 [..............................] - ETA: 6:46 - loss: 0.7431 - acc: 0.3906
 128/4849 [..............................] - ETA: 7:02 - loss: 0.7012 - acc: 0.5078
 192/4849 [>.............................] - ETA: 7:05 - loss: 0.7047 - acc: 0.5156
 256/4849 [>.............................] - ETA: 6:56 - loss: 0.6870 - acc: 0.5508
 320/4849 [>.............................] - ETA: 6:47 - loss: 0.6775 - acc: 0.5719
 384/4849 [=>............................] - ETA: 6:44 - loss: 0.6738 - acc: 0.5859
 448/4849 [=>............................] - ETA: 6:42 - loss: 0.6709 - acc: 0.5938
 512/4849 [==>...........................] - ETA: 6:31 - loss: 0.6785 - acc: 0.5859
 576/4849 [==>...........................] - ETA: 6:24 - loss: 0.6821 - acc: 0.5747
 640/4849 [==>...........................] - ETA: 6:19 - loss: 0.6831 - acc: 0.5719
 704/4849 [===>..........................] - ETA: 6:14 - loss: 0.6810 - acc: 0.5724
 768/4849 [===>..........................] - ETA: 6:10 - loss: 0.6840 - acc: 0.5677
 832/4849 [====>.........................] - ETA: 6:04 - loss: 0.6819 - acc: 0.5709
 896/4849 [====>.........................] - ETA: 5:58 - loss: 0.6818 - acc: 0.5714
 960/4849 [====>.........................] - ETA: 5:50 - loss: 0.6849 - acc: 0.5656
1024/4849 [=====>........................] - ETA: 5:46 - loss: 0.6835 - acc: 0.5645
1088/4849 [=====>........................] - ETA: 5:39 - loss: 0.6846 - acc: 0.5597
1152/4849 [======>.......................] - ETA: 5:33 - loss: 0.6831 - acc: 0.5634
1216/4849 [======>.......................] - ETA: 5:27 - loss: 0.6828 - acc: 0.5625
1280/4849 [======>.......................] - ETA: 5:19 - loss: 0.6854 - acc: 0.5578
1344/4849 [=======>......................] - ETA: 5:12 - loss: 0.6861 - acc: 0.5595
1408/4849 [=======>......................] - ETA: 5:06 - loss: 0.6868 - acc: 0.5568
1472/4849 [========>.....................] - ETA: 5:00 - loss: 0.6878 - acc: 0.5523
1536/4849 [========>.....................] - ETA: 4:54 - loss: 0.6913 - acc: 0.5469
1600/4849 [========>.....................] - ETA: 4:48 - loss: 0.6927 - acc: 0.5437
1664/4849 [=========>....................] - ETA: 4:41 - loss: 0.6919 - acc: 0.5475
1728/4849 [=========>....................] - ETA: 4:35 - loss: 0.6917 - acc: 0.5469
1792/4849 [==========>...................] - ETA: 4:29 - loss: 0.6912 - acc: 0.5497
1856/4849 [==========>...................] - ETA: 4:23 - loss: 0.6907 - acc: 0.5501
1920/4849 [==========>...................] - ETA: 4:18 - loss: 0.6903 - acc: 0.5500
1984/4849 [===========>..................] - ETA: 4:12 - loss: 0.6917 - acc: 0.5464
2048/4849 [===========>..................] - ETA: 4:07 - loss: 0.6908 - acc: 0.5508
2112/4849 [============>.................] - ETA: 4:01 - loss: 0.6909 - acc: 0.5502
2176/4849 [============>.................] - ETA: 3:55 - loss: 0.6902 - acc: 0.5496
2240/4849 [============>.................] - ETA: 3:50 - loss: 0.6894 - acc: 0.5518
2304/4849 [=============>................] - ETA: 3:45 - loss: 0.6891 - acc: 0.5516
2368/4849 [=============>................] - ETA: 3:39 - loss: 0.6894 - acc: 0.5511
2432/4849 [==============>...............] - ETA: 3:33 - loss: 0.6901 - acc: 0.5502
2496/4849 [==============>...............] - ETA: 3:27 - loss: 0.6893 - acc: 0.5509
2560/4849 [==============>...............] - ETA: 3:21 - loss: 0.6887 - acc: 0.5512
2624/4849 [===============>..............] - ETA: 3:16 - loss: 0.6891 - acc: 0.5511
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.6888 - acc: 0.5521
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.6888 - acc: 0.5534
2816/4849 [================>.............] - ETA: 2:59 - loss: 0.6894 - acc: 0.5526
2880/4849 [================>.............] - ETA: 2:53 - loss: 0.6899 - acc: 0.5510
2944/4849 [=================>............] - ETA: 2:47 - loss: 0.6913 - acc: 0.5493
3008/4849 [=================>............] - ETA: 2:41 - loss: 0.6913 - acc: 0.5482
3072/4849 [==================>...........] - ETA: 2:36 - loss: 0.6910 - acc: 0.5492
3136/4849 [==================>...........] - ETA: 2:30 - loss: 0.6905 - acc: 0.5497
3200/4849 [==================>...........] - ETA: 2:24 - loss: 0.6901 - acc: 0.5519
3264/4849 [===================>..........] - ETA: 2:18 - loss: 0.6902 - acc: 0.5518
3328/4849 [===================>..........] - ETA: 2:13 - loss: 0.6905 - acc: 0.5508
3392/4849 [===================>..........] - ETA: 2:07 - loss: 0.6906 - acc: 0.5501
3456/4849 [====================>.........] - ETA: 2:02 - loss: 0.6906 - acc: 0.5498
3520/4849 [====================>.........] - ETA: 1:56 - loss: 0.6907 - acc: 0.5506
3584/4849 [=====================>........] - ETA: 1:50 - loss: 0.6907 - acc: 0.5513
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6908 - acc: 0.5507
3712/4849 [=====================>........] - ETA: 1:39 - loss: 0.6912 - acc: 0.5501
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6912 - acc: 0.5503
3840/4849 [======================>.......] - ETA: 1:28 - loss: 0.6910 - acc: 0.5508
3904/4849 [=======================>......] - ETA: 1:22 - loss: 0.6912 - acc: 0.5512
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6918 - acc: 0.5499
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6914 - acc: 0.5516
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6918 - acc: 0.5510
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6916 - acc: 0.5502
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6914 - acc: 0.5509 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6912 - acc: 0.5515
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6913 - acc: 0.5496
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6912 - acc: 0.5494
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6915 - acc: 0.5498
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6911 - acc: 0.5511
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6906 - acc: 0.5521
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6900 - acc: 0.5537
4736/4849 [============================>.] - ETA: 9s - loss: 0.6895 - acc: 0.5545 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6895 - acc: 0.5546
4849/4849 [==============================] - 443s 91ms/step - loss: 0.6896 - acc: 0.5552 - val_loss: 0.6933 - val_acc: 0.5399

Epoch 00003: val_acc improved from 0.52134 to 0.53989, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window17/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 4/10

  64/4849 [..............................] - ETA: 7:16 - loss: 0.6731 - acc: 0.5312
 128/4849 [..............................] - ETA: 7:08 - loss: 0.6865 - acc: 0.5391
 192/4849 [>.............................] - ETA: 6:59 - loss: 0.7065 - acc: 0.5052
 256/4849 [>.............................] - ETA: 6:43 - loss: 0.7143 - acc: 0.4766
 320/4849 [>.............................] - ETA: 6:37 - loss: 0.7067 - acc: 0.5062
 384/4849 [=>............................] - ETA: 6:31 - loss: 0.6996 - acc: 0.5234
 448/4849 [=>............................] - ETA: 6:26 - loss: 0.7013 - acc: 0.5112
 512/4849 [==>...........................] - ETA: 6:18 - loss: 0.7044 - acc: 0.5117
 576/4849 [==>...........................] - ETA: 6:13 - loss: 0.7032 - acc: 0.5156
 640/4849 [==>...........................] - ETA: 6:07 - loss: 0.7071 - acc: 0.5016
 704/4849 [===>..........................] - ETA: 6:02 - loss: 0.7016 - acc: 0.5170
 768/4849 [===>..........................] - ETA: 5:56 - loss: 0.6971 - acc: 0.5299
 832/4849 [====>.........................] - ETA: 5:53 - loss: 0.6952 - acc: 0.5337
 896/4849 [====>.........................] - ETA: 5:48 - loss: 0.6960 - acc: 0.5324
 960/4849 [====>.........................] - ETA: 5:43 - loss: 0.6967 - acc: 0.5312
1024/4849 [=====>........................] - ETA: 5:36 - loss: 0.6976 - acc: 0.5264
1088/4849 [=====>........................] - ETA: 5:30 - loss: 0.6971 - acc: 0.5294
1152/4849 [======>.......................] - ETA: 5:24 - loss: 0.6975 - acc: 0.5286
1216/4849 [======>.......................] - ETA: 5:17 - loss: 0.6972 - acc: 0.5304
1280/4849 [======>.......................] - ETA: 5:13 - loss: 0.6941 - acc: 0.5391
1344/4849 [=======>......................] - ETA: 5:06 - loss: 0.6948 - acc: 0.5365
1408/4849 [=======>......................] - ETA: 5:00 - loss: 0.6929 - acc: 0.5384
1472/4849 [========>.....................] - ETA: 4:54 - loss: 0.6931 - acc: 0.5374
1536/4849 [========>.....................] - ETA: 4:48 - loss: 0.6928 - acc: 0.5378
1600/4849 [========>.....................] - ETA: 4:42 - loss: 0.6900 - acc: 0.5437
1664/4849 [=========>....................] - ETA: 4:36 - loss: 0.6896 - acc: 0.5457
1728/4849 [=========>....................] - ETA: 4:31 - loss: 0.6900 - acc: 0.5451
1792/4849 [==========>...................] - ETA: 4:26 - loss: 0.6893 - acc: 0.5485
1856/4849 [==========>...................] - ETA: 4:20 - loss: 0.6899 - acc: 0.5496
1920/4849 [==========>...................] - ETA: 4:15 - loss: 0.6898 - acc: 0.5484
1984/4849 [===========>..................] - ETA: 4:08 - loss: 0.6886 - acc: 0.5509
2048/4849 [===========>..................] - ETA: 4:02 - loss: 0.6876 - acc: 0.5513
2112/4849 [============>.................] - ETA: 3:57 - loss: 0.6865 - acc: 0.5521
2176/4849 [============>.................] - ETA: 3:51 - loss: 0.6873 - acc: 0.5501
2240/4849 [============>.................] - ETA: 3:46 - loss: 0.6875 - acc: 0.5482
2304/4849 [=============>................] - ETA: 3:40 - loss: 0.6873 - acc: 0.5486
2368/4849 [=============>................] - ETA: 3:33 - loss: 0.6882 - acc: 0.5465
2432/4849 [==============>...............] - ETA: 3:28 - loss: 0.6880 - acc: 0.5465
2496/4849 [==============>...............] - ETA: 3:23 - loss: 0.6877 - acc: 0.5477
2560/4849 [==============>...............] - ETA: 3:17 - loss: 0.6877 - acc: 0.5465
2624/4849 [===============>..............] - ETA: 3:11 - loss: 0.6881 - acc: 0.5446
2688/4849 [===============>..............] - ETA: 3:06 - loss: 0.6876 - acc: 0.5435
2752/4849 [================>.............] - ETA: 3:00 - loss: 0.6879 - acc: 0.5440
2816/4849 [================>.............] - ETA: 2:54 - loss: 0.6868 - acc: 0.5465
2880/4849 [================>.............] - ETA: 2:48 - loss: 0.6862 - acc: 0.5493
2944/4849 [=================>............] - ETA: 2:43 - loss: 0.6862 - acc: 0.5499
3008/4849 [=================>............] - ETA: 2:37 - loss: 0.6860 - acc: 0.5502
3072/4849 [==================>...........] - ETA: 2:32 - loss: 0.6868 - acc: 0.5482
3136/4849 [==================>...........] - ETA: 2:26 - loss: 0.6867 - acc: 0.5482
3200/4849 [==================>...........] - ETA: 2:21 - loss: 0.6861 - acc: 0.5491
3264/4849 [===================>..........] - ETA: 2:15 - loss: 0.6860 - acc: 0.5499
3328/4849 [===================>..........] - ETA: 2:10 - loss: 0.6863 - acc: 0.5496
3392/4849 [===================>..........] - ETA: 2:05 - loss: 0.6862 - acc: 0.5498
3456/4849 [====================>.........] - ETA: 1:59 - loss: 0.6863 - acc: 0.5498
3520/4849 [====================>.........] - ETA: 1:54 - loss: 0.6863 - acc: 0.5500
3584/4849 [=====================>........] - ETA: 1:48 - loss: 0.6866 - acc: 0.5485
3648/4849 [=====================>........] - ETA: 1:42 - loss: 0.6862 - acc: 0.5496
3712/4849 [=====================>........] - ETA: 1:37 - loss: 0.6854 - acc: 0.5528
3776/4849 [======================>.......] - ETA: 1:32 - loss: 0.6855 - acc: 0.5524
3840/4849 [======================>.......] - ETA: 1:26 - loss: 0.6854 - acc: 0.5523
3904/4849 [=======================>......] - ETA: 1:21 - loss: 0.6849 - acc: 0.5540
3968/4849 [=======================>......] - ETA: 1:15 - loss: 0.6841 - acc: 0.5562
4032/4849 [=======================>......] - ETA: 1:10 - loss: 0.6841 - acc: 0.5553
4096/4849 [========================>.....] - ETA: 1:04 - loss: 0.6839 - acc: 0.5557
4160/4849 [========================>.....] - ETA: 58s - loss: 0.6842 - acc: 0.5541 
4224/4849 [=========================>....] - ETA: 53s - loss: 0.6843 - acc: 0.5549
4288/4849 [=========================>....] - ETA: 47s - loss: 0.6843 - acc: 0.5553
4352/4849 [=========================>....] - ETA: 42s - loss: 0.6841 - acc: 0.5565
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6846 - acc: 0.5559
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6843 - acc: 0.5569
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6844 - acc: 0.5563
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6842 - acc: 0.5573
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6841 - acc: 0.5582
4736/4849 [============================>.] - ETA: 9s - loss: 0.6841 - acc: 0.5574 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6841 - acc: 0.5571
4849/4849 [==============================] - 431s 89ms/step - loss: 0.6844 - acc: 0.5564 - val_loss: 0.6994 - val_acc: 0.5065

Epoch 00004: val_acc did not improve from 0.53989
Epoch 5/10

  64/4849 [..............................] - ETA: 7:19 - loss: 0.6513 - acc: 0.5781
 128/4849 [..............................] - ETA: 6:40 - loss: 0.6709 - acc: 0.5703
 192/4849 [>.............................] - ETA: 6:33 - loss: 0.6618 - acc: 0.6094
 256/4849 [>.............................] - ETA: 6:25 - loss: 0.6622 - acc: 0.6055
 320/4849 [>.............................] - ETA: 6:17 - loss: 0.6669 - acc: 0.5813
 384/4849 [=>............................] - ETA: 6:15 - loss: 0.6688 - acc: 0.5807
 448/4849 [=>............................] - ETA: 6:06 - loss: 0.6720 - acc: 0.5714
 512/4849 [==>...........................] - ETA: 6:02 - loss: 0.6751 - acc: 0.5703
 576/4849 [==>...........................] - ETA: 5:57 - loss: 0.6736 - acc: 0.5712
 640/4849 [==>...........................] - ETA: 5:55 - loss: 0.6717 - acc: 0.5844
 704/4849 [===>..........................] - ETA: 5:47 - loss: 0.6740 - acc: 0.5781
 768/4849 [===>..........................] - ETA: 5:42 - loss: 0.6758 - acc: 0.5690
 832/4849 [====>.........................] - ETA: 5:37 - loss: 0.6755 - acc: 0.5685
 896/4849 [====>.........................] - ETA: 5:34 - loss: 0.6768 - acc: 0.5703
 960/4849 [====>.........................] - ETA: 5:26 - loss: 0.6781 - acc: 0.5708
1024/4849 [=====>........................] - ETA: 5:16 - loss: 0.6779 - acc: 0.5664
1088/4849 [=====>........................] - ETA: 5:09 - loss: 0.6761 - acc: 0.5708
1152/4849 [======>.......................] - ETA: 5:03 - loss: 0.6767 - acc: 0.5677
1216/4849 [======>.......................] - ETA: 4:56 - loss: 0.6790 - acc: 0.5666
1280/4849 [======>.......................] - ETA: 4:51 - loss: 0.6788 - acc: 0.5672
1344/4849 [=======>......................] - ETA: 4:45 - loss: 0.6768 - acc: 0.5714
1408/4849 [=======>......................] - ETA: 4:37 - loss: 0.6749 - acc: 0.5753
1472/4849 [========>.....................] - ETA: 4:31 - loss: 0.6743 - acc: 0.5761
1536/4849 [========>.....................] - ETA: 4:25 - loss: 0.6729 - acc: 0.5801
1600/4849 [========>.....................] - ETA: 4:19 - loss: 0.6739 - acc: 0.5775
1664/4849 [=========>....................] - ETA: 4:15 - loss: 0.6754 - acc: 0.5739
1728/4849 [=========>....................] - ETA: 4:10 - loss: 0.6739 - acc: 0.5758
1792/4849 [==========>...................] - ETA: 4:04 - loss: 0.6753 - acc: 0.5759
1856/4849 [==========>...................] - ETA: 3:59 - loss: 0.6777 - acc: 0.5711
1920/4849 [==========>...................] - ETA: 3:53 - loss: 0.6784 - acc: 0.5719
1984/4849 [===========>..................] - ETA: 3:46 - loss: 0.6783 - acc: 0.5701
2048/4849 [===========>..................] - ETA: 3:41 - loss: 0.6777 - acc: 0.5703
2112/4849 [============>.................] - ETA: 3:36 - loss: 0.6781 - acc: 0.5682
2176/4849 [============>.................] - ETA: 3:31 - loss: 0.6771 - acc: 0.5699
2240/4849 [============>.................] - ETA: 3:25 - loss: 0.6771 - acc: 0.5696
2304/4849 [=============>................] - ETA: 3:21 - loss: 0.6763 - acc: 0.5725
2368/4849 [=============>................] - ETA: 3:15 - loss: 0.6773 - acc: 0.5697
2432/4849 [==============>...............] - ETA: 3:11 - loss: 0.6771 - acc: 0.5699
2496/4849 [==============>...............] - ETA: 3:06 - loss: 0.6777 - acc: 0.5685
2560/4849 [==============>...............] - ETA: 3:00 - loss: 0.6778 - acc: 0.5668
2624/4849 [===============>..............] - ETA: 2:56 - loss: 0.6768 - acc: 0.5690
2688/4849 [===============>..............] - ETA: 2:50 - loss: 0.6777 - acc: 0.5685
2752/4849 [================>.............] - ETA: 2:45 - loss: 0.6771 - acc: 0.5698
2816/4849 [================>.............] - ETA: 2:40 - loss: 0.6773 - acc: 0.5696
2880/4849 [================>.............] - ETA: 2:35 - loss: 0.6784 - acc: 0.5677
2944/4849 [=================>............] - ETA: 2:30 - loss: 0.6786 - acc: 0.5686
3008/4849 [=================>............] - ETA: 2:25 - loss: 0.6795 - acc: 0.5675
3072/4849 [==================>...........] - ETA: 2:20 - loss: 0.6791 - acc: 0.5674
3136/4849 [==================>...........] - ETA: 2:14 - loss: 0.6788 - acc: 0.5682
3200/4849 [==================>...........] - ETA: 2:09 - loss: 0.6794 - acc: 0.5678
3264/4849 [===================>..........] - ETA: 2:04 - loss: 0.6794 - acc: 0.5674
3328/4849 [===================>..........] - ETA: 1:59 - loss: 0.6783 - acc: 0.5694
3392/4849 [===================>..........] - ETA: 1:55 - loss: 0.6783 - acc: 0.5690
3456/4849 [====================>.........] - ETA: 1:49 - loss: 0.6782 - acc: 0.5700
3520/4849 [====================>.........] - ETA: 1:44 - loss: 0.6777 - acc: 0.5716
3584/4849 [=====================>........] - ETA: 1:39 - loss: 0.6773 - acc: 0.5734
3648/4849 [=====================>........] - ETA: 1:34 - loss: 0.6777 - acc: 0.5721
3712/4849 [=====================>........] - ETA: 1:28 - loss: 0.6772 - acc: 0.5733
3776/4849 [======================>.......] - ETA: 1:23 - loss: 0.6777 - acc: 0.5723
3840/4849 [======================>.......] - ETA: 1:18 - loss: 0.6777 - acc: 0.5732
3904/4849 [=======================>......] - ETA: 1:13 - loss: 0.6777 - acc: 0.5751
3968/4849 [=======================>......] - ETA: 1:08 - loss: 0.6777 - acc: 0.5746
4032/4849 [=======================>......] - ETA: 1:03 - loss: 0.6774 - acc: 0.5754
4096/4849 [========================>.....] - ETA: 58s - loss: 0.6782 - acc: 0.5745 
4160/4849 [========================>.....] - ETA: 53s - loss: 0.6789 - acc: 0.5738
4224/4849 [=========================>....] - ETA: 48s - loss: 0.6780 - acc: 0.5755
4288/4849 [=========================>....] - ETA: 43s - loss: 0.6776 - acc: 0.5760
4352/4849 [=========================>....] - ETA: 38s - loss: 0.6774 - acc: 0.5758
4416/4849 [==========================>...] - ETA: 33s - loss: 0.6780 - acc: 0.5754
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6787 - acc: 0.5748
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6787 - acc: 0.5748
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6788 - acc: 0.5740
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6784 - acc: 0.5762
4736/4849 [============================>.] - ETA: 8s - loss: 0.6784 - acc: 0.5762 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6788 - acc: 0.5748
4849/4849 [==============================] - 391s 81ms/step - loss: 0.6788 - acc: 0.5746 - val_loss: 0.7128 - val_acc: 0.5009

Epoch 00005: val_acc did not improve from 0.53989
Epoch 6/10

  64/4849 [..............................] - ETA: 5:32 - loss: 0.6844 - acc: 0.5156
 128/4849 [..............................] - ETA: 5:41 - loss: 0.7015 - acc: 0.5078
 192/4849 [>.............................] - ETA: 5:38 - loss: 0.6906 - acc: 0.5417
 256/4849 [>.............................] - ETA: 5:39 - loss: 0.6864 - acc: 0.5625
 320/4849 [>.............................] - ETA: 5:44 - loss: 0.6823 - acc: 0.5687
 384/4849 [=>............................] - ETA: 5:35 - loss: 0.6819 - acc: 0.5651
 448/4849 [=>............................] - ETA: 5:31 - loss: 0.6813 - acc: 0.5714
 512/4849 [==>...........................] - ETA: 5:24 - loss: 0.6797 - acc: 0.5684
 576/4849 [==>...........................] - ETA: 5:18 - loss: 0.6788 - acc: 0.5729
 640/4849 [==>...........................] - ETA: 5:11 - loss: 0.6813 - acc: 0.5672
 704/4849 [===>..........................] - ETA: 5:07 - loss: 0.6782 - acc: 0.5682
 768/4849 [===>..........................] - ETA: 5:01 - loss: 0.6774 - acc: 0.5677
 832/4849 [====>.........................] - ETA: 4:56 - loss: 0.6757 - acc: 0.5721
 896/4849 [====>.........................] - ETA: 4:51 - loss: 0.6740 - acc: 0.5792
 960/4849 [====>.........................] - ETA: 4:48 - loss: 0.6696 - acc: 0.5854
1024/4849 [=====>........................] - ETA: 4:42 - loss: 0.6688 - acc: 0.5869
1088/4849 [=====>........................] - ETA: 4:39 - loss: 0.6680 - acc: 0.5855
1152/4849 [======>.......................] - ETA: 4:31 - loss: 0.6690 - acc: 0.5868
1216/4849 [======>.......................] - ETA: 4:27 - loss: 0.6701 - acc: 0.5822
1280/4849 [======>.......................] - ETA: 4:23 - loss: 0.6718 - acc: 0.5813
1344/4849 [=======>......................] - ETA: 4:20 - loss: 0.6693 - acc: 0.5848
1408/4849 [=======>......................] - ETA: 4:15 - loss: 0.6692 - acc: 0.5838
1472/4849 [========>.....................] - ETA: 4:10 - loss: 0.6691 - acc: 0.5836
1536/4849 [========>.....................] - ETA: 4:08 - loss: 0.6723 - acc: 0.5781
1600/4849 [========>.....................] - ETA: 4:03 - loss: 0.6719 - acc: 0.5806
1664/4849 [=========>....................] - ETA: 3:57 - loss: 0.6708 - acc: 0.5817
1728/4849 [=========>....................] - ETA: 3:54 - loss: 0.6685 - acc: 0.5839
1792/4849 [==========>...................] - ETA: 3:49 - loss: 0.6697 - acc: 0.5820
1856/4849 [==========>...................] - ETA: 3:44 - loss: 0.6699 - acc: 0.5824
1920/4849 [==========>...................] - ETA: 3:40 - loss: 0.6710 - acc: 0.5802
1984/4849 [===========>..................] - ETA: 3:35 - loss: 0.6727 - acc: 0.5771
2048/4849 [===========>..................] - ETA: 3:28 - loss: 0.6740 - acc: 0.5767
2112/4849 [============>.................] - ETA: 3:21 - loss: 0.6733 - acc: 0.5767
2176/4849 [============>.................] - ETA: 3:16 - loss: 0.6741 - acc: 0.5777
2240/4849 [============>.................] - ETA: 3:11 - loss: 0.6738 - acc: 0.5772
2304/4849 [=============>................] - ETA: 3:05 - loss: 0.6741 - acc: 0.5777
2368/4849 [=============>................] - ETA: 2:59 - loss: 0.6753 - acc: 0.5760
2432/4849 [==============>...............] - ETA: 2:54 - loss: 0.6746 - acc: 0.5761
2496/4849 [==============>...............] - ETA: 2:49 - loss: 0.6744 - acc: 0.5769
2560/4849 [==============>...............] - ETA: 2:43 - loss: 0.6751 - acc: 0.5750
2624/4849 [===============>..............] - ETA: 2:37 - loss: 0.6759 - acc: 0.5747
2688/4849 [===============>..............] - ETA: 2:33 - loss: 0.6767 - acc: 0.5725
2752/4849 [================>.............] - ETA: 2:28 - loss: 0.6771 - acc: 0.5716
2816/4849 [================>.............] - ETA: 2:23 - loss: 0.6777 - acc: 0.5707
2880/4849 [================>.............] - ETA: 2:18 - loss: 0.6777 - acc: 0.5705
2944/4849 [=================>............] - ETA: 2:14 - loss: 0.6785 - acc: 0.5686
3008/4849 [=================>............] - ETA: 2:09 - loss: 0.6787 - acc: 0.5678
3072/4849 [==================>...........] - ETA: 2:04 - loss: 0.6784 - acc: 0.5690
3136/4849 [==================>...........] - ETA: 2:00 - loss: 0.6783 - acc: 0.5708
3200/4849 [==================>...........] - ETA: 1:55 - loss: 0.6777 - acc: 0.5719
3264/4849 [===================>..........] - ETA: 1:51 - loss: 0.6775 - acc: 0.5723
3328/4849 [===================>..........] - ETA: 1:46 - loss: 0.6770 - acc: 0.5742
3392/4849 [===================>..........] - ETA: 1:41 - loss: 0.6770 - acc: 0.5728
3456/4849 [====================>.........] - ETA: 1:37 - loss: 0.6778 - acc: 0.5718
3520/4849 [====================>.........] - ETA: 1:32 - loss: 0.6774 - acc: 0.5727
3584/4849 [=====================>........] - ETA: 1:27 - loss: 0.6779 - acc: 0.5720
3648/4849 [=====================>........] - ETA: 1:23 - loss: 0.6772 - acc: 0.5726
3712/4849 [=====================>........] - ETA: 1:18 - loss: 0.6765 - acc: 0.5735
3776/4849 [======================>.......] - ETA: 1:14 - loss: 0.6766 - acc: 0.5731
3840/4849 [======================>.......] - ETA: 1:09 - loss: 0.6764 - acc: 0.5729
3904/4849 [=======================>......] - ETA: 1:05 - loss: 0.6764 - acc: 0.5725
3968/4849 [=======================>......] - ETA: 1:00 - loss: 0.6772 - acc: 0.5716
4032/4849 [=======================>......] - ETA: 56s - loss: 0.6776 - acc: 0.5707 
4096/4849 [========================>.....] - ETA: 51s - loss: 0.6769 - acc: 0.5713
4160/4849 [========================>.....] - ETA: 47s - loss: 0.6770 - acc: 0.5712
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6779 - acc: 0.5698
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6783 - acc: 0.5700
4352/4849 [=========================>....] - ETA: 34s - loss: 0.6781 - acc: 0.5699
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6786 - acc: 0.5693
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6786 - acc: 0.5696
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6789 - acc: 0.5698
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6793 - acc: 0.5697
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6790 - acc: 0.5711
4736/4849 [============================>.] - ETA: 7s - loss: 0.6787 - acc: 0.5716 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6784 - acc: 0.5717
4849/4849 [==============================] - 341s 70ms/step - loss: 0.6783 - acc: 0.5723 - val_loss: 0.6816 - val_acc: 0.5955

Epoch 00006: val_acc improved from 0.53989 to 0.59555, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window17/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 7/10

  64/4849 [..............................] - ETA: 5:17 - loss: 0.6462 - acc: 0.6250
 128/4849 [..............................] - ETA: 5:05 - loss: 0.6658 - acc: 0.5781
 192/4849 [>.............................] - ETA: 4:51 - loss: 0.6695 - acc: 0.5677
 256/4849 [>.............................] - ETA: 4:43 - loss: 0.6762 - acc: 0.5742
 320/4849 [>.............................] - ETA: 4:35 - loss: 0.6770 - acc: 0.5875
 384/4849 [=>............................] - ETA: 4:26 - loss: 0.6774 - acc: 0.5885
 448/4849 [=>............................] - ETA: 4:25 - loss: 0.6774 - acc: 0.5871
 512/4849 [==>...........................] - ETA: 4:22 - loss: 0.6816 - acc: 0.5703
 576/4849 [==>...........................] - ETA: 4:22 - loss: 0.6803 - acc: 0.5747
 640/4849 [==>...........................] - ETA: 4:19 - loss: 0.6812 - acc: 0.5687
 704/4849 [===>..........................] - ETA: 4:19 - loss: 0.6837 - acc: 0.5668
 768/4849 [===>..........................] - ETA: 4:17 - loss: 0.6801 - acc: 0.5742
 832/4849 [====>.........................] - ETA: 4:11 - loss: 0.6798 - acc: 0.5745
 896/4849 [====>.........................] - ETA: 4:05 - loss: 0.6812 - acc: 0.5670
 960/4849 [====>.........................] - ETA: 4:02 - loss: 0.6799 - acc: 0.5698
1024/4849 [=====>........................] - ETA: 3:58 - loss: 0.6800 - acc: 0.5723
1088/4849 [=====>........................] - ETA: 3:53 - loss: 0.6832 - acc: 0.5643
1152/4849 [======>.......................] - ETA: 3:49 - loss: 0.6830 - acc: 0.5616
1216/4849 [======>.......................] - ETA: 3:44 - loss: 0.6819 - acc: 0.5633
1280/4849 [======>.......................] - ETA: 3:39 - loss: 0.6797 - acc: 0.5687
1344/4849 [=======>......................] - ETA: 3:35 - loss: 0.6798 - acc: 0.5707
1408/4849 [=======>......................] - ETA: 3:30 - loss: 0.6778 - acc: 0.5753
1472/4849 [========>.....................] - ETA: 3:26 - loss: 0.6792 - acc: 0.5713
1536/4849 [========>.....................] - ETA: 3:24 - loss: 0.6787 - acc: 0.5736
1600/4849 [========>.....................] - ETA: 3:21 - loss: 0.6790 - acc: 0.5731
1664/4849 [=========>....................] - ETA: 3:16 - loss: 0.6785 - acc: 0.5733
1728/4849 [=========>....................] - ETA: 3:12 - loss: 0.6791 - acc: 0.5718
1792/4849 [==========>...................] - ETA: 3:08 - loss: 0.6785 - acc: 0.5737
1856/4849 [==========>...................] - ETA: 3:03 - loss: 0.6779 - acc: 0.5760
1920/4849 [==========>...................] - ETA: 2:59 - loss: 0.6783 - acc: 0.5760
1984/4849 [===========>..................] - ETA: 2:56 - loss: 0.6780 - acc: 0.5761
2048/4849 [===========>..................] - ETA: 2:52 - loss: 0.6780 - acc: 0.5762
2112/4849 [============>.................] - ETA: 2:48 - loss: 0.6773 - acc: 0.5772
2176/4849 [============>.................] - ETA: 2:44 - loss: 0.6781 - acc: 0.5758
2240/4849 [============>.................] - ETA: 2:40 - loss: 0.6769 - acc: 0.5772
2304/4849 [=============>................] - ETA: 2:36 - loss: 0.6773 - acc: 0.5773
2368/4849 [=============>................] - ETA: 2:32 - loss: 0.6768 - acc: 0.5760
2432/4849 [==============>...............] - ETA: 2:28 - loss: 0.6758 - acc: 0.5769
2496/4849 [==============>...............] - ETA: 2:24 - loss: 0.6756 - acc: 0.5789
2560/4849 [==============>...............] - ETA: 2:21 - loss: 0.6757 - acc: 0.5777
2624/4849 [===============>..............] - ETA: 2:17 - loss: 0.6752 - acc: 0.5796
2688/4849 [===============>..............] - ETA: 2:13 - loss: 0.6748 - acc: 0.5804
2752/4849 [================>.............] - ETA: 2:09 - loss: 0.6753 - acc: 0.5799
2816/4849 [================>.............] - ETA: 2:05 - loss: 0.6740 - acc: 0.5817
2880/4849 [================>.............] - ETA: 2:01 - loss: 0.6739 - acc: 0.5823
2944/4849 [=================>............] - ETA: 1:57 - loss: 0.6739 - acc: 0.5812
3008/4849 [=================>............] - ETA: 1:53 - loss: 0.6735 - acc: 0.5824
3072/4849 [==================>...........] - ETA: 1:49 - loss: 0.6742 - acc: 0.5811
3136/4849 [==================>...........] - ETA: 1:45 - loss: 0.6745 - acc: 0.5829
3200/4849 [==================>...........] - ETA: 1:41 - loss: 0.6747 - acc: 0.5825
3264/4849 [===================>..........] - ETA: 1:37 - loss: 0.6748 - acc: 0.5824
3328/4849 [===================>..........] - ETA: 1:33 - loss: 0.6750 - acc: 0.5826
3392/4849 [===================>..........] - ETA: 1:29 - loss: 0.6744 - acc: 0.5834
3456/4849 [====================>.........] - ETA: 1:25 - loss: 0.6742 - acc: 0.5839
3520/4849 [====================>.........] - ETA: 1:21 - loss: 0.6736 - acc: 0.5835
3584/4849 [=====================>........] - ETA: 1:17 - loss: 0.6741 - acc: 0.5829
3648/4849 [=====================>........] - ETA: 1:13 - loss: 0.6740 - acc: 0.5820
3712/4849 [=====================>........] - ETA: 1:09 - loss: 0.6743 - acc: 0.5808
3776/4849 [======================>.......] - ETA: 1:05 - loss: 0.6741 - acc: 0.5813
3840/4849 [======================>.......] - ETA: 1:02 - loss: 0.6739 - acc: 0.5815
3904/4849 [=======================>......] - ETA: 58s - loss: 0.6743 - acc: 0.5802 
3968/4849 [=======================>......] - ETA: 54s - loss: 0.6746 - acc: 0.5794
4032/4849 [=======================>......] - ETA: 50s - loss: 0.6737 - acc: 0.5813
4096/4849 [========================>.....] - ETA: 46s - loss: 0.6732 - acc: 0.5828
4160/4849 [========================>.....] - ETA: 42s - loss: 0.6731 - acc: 0.5837
4224/4849 [=========================>....] - ETA: 38s - loss: 0.6730 - acc: 0.5831
4288/4849 [=========================>....] - ETA: 34s - loss: 0.6732 - acc: 0.5833
4352/4849 [=========================>....] - ETA: 30s - loss: 0.6734 - acc: 0.5825
4416/4849 [==========================>...] - ETA: 26s - loss: 0.6736 - acc: 0.5829
4480/4849 [==========================>...] - ETA: 22s - loss: 0.6738 - acc: 0.5826
4544/4849 [===========================>..] - ETA: 18s - loss: 0.6736 - acc: 0.5832
4608/4849 [===========================>..] - ETA: 14s - loss: 0.6738 - acc: 0.5822
4672/4849 [===========================>..] - ETA: 10s - loss: 0.6731 - acc: 0.5828
4736/4849 [============================>.] - ETA: 6s - loss: 0.6734 - acc: 0.5819 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6738 - acc: 0.5813
4849/4849 [==============================] - 313s 64ms/step - loss: 0.6737 - acc: 0.5820 - val_loss: 0.6883 - val_acc: 0.5510

Epoch 00007: val_acc did not improve from 0.59555
Epoch 8/10

  64/4849 [..............................] - ETA: 4:30 - loss: 0.7075 - acc: 0.5938
 128/4849 [..............................] - ETA: 4:47 - loss: 0.6916 - acc: 0.5859
 192/4849 [>.............................] - ETA: 4:35 - loss: 0.6937 - acc: 0.5833
 256/4849 [>.............................] - ETA: 4:32 - loss: 0.6998 - acc: 0.5742
 320/4849 [>.............................] - ETA: 4:31 - loss: 0.6935 - acc: 0.5781
 384/4849 [=>............................] - ETA: 4:44 - loss: 0.6850 - acc: 0.5885
 448/4849 [=>............................] - ETA: 4:41 - loss: 0.6863 - acc: 0.5804
 512/4849 [==>...........................] - ETA: 4:35 - loss: 0.6880 - acc: 0.5820
 576/4849 [==>...........................] - ETA: 4:29 - loss: 0.6860 - acc: 0.5851
 640/4849 [==>...........................] - ETA: 4:22 - loss: 0.6883 - acc: 0.5734
 704/4849 [===>..........................] - ETA: 4:19 - loss: 0.6881 - acc: 0.5724
 768/4849 [===>..........................] - ETA: 4:16 - loss: 0.6864 - acc: 0.5690
 832/4849 [====>.........................] - ETA: 4:12 - loss: 0.6869 - acc: 0.5661
 896/4849 [====>.........................] - ETA: 4:07 - loss: 0.6848 - acc: 0.5725
 960/4849 [====>.........................] - ETA: 4:04 - loss: 0.6844 - acc: 0.5708
1024/4849 [=====>........................] - ETA: 4:00 - loss: 0.6834 - acc: 0.5752
1088/4849 [=====>........................] - ETA: 3:56 - loss: 0.6817 - acc: 0.5781
1152/4849 [======>.......................] - ETA: 3:52 - loss: 0.6809 - acc: 0.5799
1216/4849 [======>.......................] - ETA: 3:46 - loss: 0.6826 - acc: 0.5765
1280/4849 [======>.......................] - ETA: 3:42 - loss: 0.6807 - acc: 0.5789
1344/4849 [=======>......................] - ETA: 3:39 - loss: 0.6785 - acc: 0.5848
1408/4849 [=======>......................] - ETA: 3:37 - loss: 0.6809 - acc: 0.5774
1472/4849 [========>.....................] - ETA: 3:33 - loss: 0.6815 - acc: 0.5754
1536/4849 [========>.....................] - ETA: 3:29 - loss: 0.6813 - acc: 0.5736
1600/4849 [========>.....................] - ETA: 3:25 - loss: 0.6811 - acc: 0.5725
1664/4849 [=========>....................] - ETA: 3:21 - loss: 0.6791 - acc: 0.5745
1728/4849 [=========>....................] - ETA: 3:17 - loss: 0.6796 - acc: 0.5758
1792/4849 [==========>...................] - ETA: 3:13 - loss: 0.6791 - acc: 0.5776
1856/4849 [==========>...................] - ETA: 3:09 - loss: 0.6797 - acc: 0.5754
1920/4849 [==========>...................] - ETA: 3:05 - loss: 0.6785 - acc: 0.5760
1984/4849 [===========>..................] - ETA: 3:01 - loss: 0.6789 - acc: 0.5756
2048/4849 [===========>..................] - ETA: 2:56 - loss: 0.6796 - acc: 0.5742
2112/4849 [============>.................] - ETA: 2:53 - loss: 0.6796 - acc: 0.5720
2176/4849 [============>.................] - ETA: 2:49 - loss: 0.6793 - acc: 0.5712
2240/4849 [============>.................] - ETA: 2:45 - loss: 0.6806 - acc: 0.5692
2304/4849 [=============>................] - ETA: 2:40 - loss: 0.6799 - acc: 0.5694
2368/4849 [=============>................] - ETA: 2:36 - loss: 0.6794 - acc: 0.5726
2432/4849 [==============>...............] - ETA: 2:32 - loss: 0.6786 - acc: 0.5744
2496/4849 [==============>...............] - ETA: 2:27 - loss: 0.6785 - acc: 0.5769
2560/4849 [==============>...............] - ETA: 2:23 - loss: 0.6784 - acc: 0.5770
2624/4849 [===============>..............] - ETA: 2:19 - loss: 0.6780 - acc: 0.5781
2688/4849 [===============>..............] - ETA: 2:15 - loss: 0.6787 - acc: 0.5770
2752/4849 [================>.............] - ETA: 2:12 - loss: 0.6777 - acc: 0.5785
2816/4849 [================>.............] - ETA: 2:07 - loss: 0.6772 - acc: 0.5792
2880/4849 [================>.............] - ETA: 2:04 - loss: 0.6772 - acc: 0.5809
2944/4849 [=================>............] - ETA: 2:00 - loss: 0.6772 - acc: 0.5815
3008/4849 [=================>............] - ETA: 1:56 - loss: 0.6771 - acc: 0.5821
3072/4849 [==================>...........] - ETA: 1:52 - loss: 0.6772 - acc: 0.5827
3136/4849 [==================>...........] - ETA: 1:48 - loss: 0.6766 - acc: 0.5835
3200/4849 [==================>...........] - ETA: 1:44 - loss: 0.6758 - acc: 0.5847
3264/4849 [===================>..........] - ETA: 1:40 - loss: 0.6767 - acc: 0.5830
3328/4849 [===================>..........] - ETA: 1:36 - loss: 0.6776 - acc: 0.5811
3392/4849 [===================>..........] - ETA: 1:32 - loss: 0.6766 - acc: 0.5834
3456/4849 [====================>.........] - ETA: 1:28 - loss: 0.6766 - acc: 0.5845
3520/4849 [====================>.........] - ETA: 1:23 - loss: 0.6780 - acc: 0.5821
3584/4849 [=====================>........] - ETA: 1:19 - loss: 0.6774 - acc: 0.5840
3648/4849 [=====================>........] - ETA: 1:16 - loss: 0.6775 - acc: 0.5853
3712/4849 [=====================>........] - ETA: 1:12 - loss: 0.6774 - acc: 0.5859
3776/4849 [======================>.......] - ETA: 1:08 - loss: 0.6775 - acc: 0.5866
3840/4849 [======================>.......] - ETA: 1:04 - loss: 0.6764 - acc: 0.5878
3904/4849 [=======================>......] - ETA: 59s - loss: 0.6768 - acc: 0.5873 
3968/4849 [=======================>......] - ETA: 55s - loss: 0.6763 - acc: 0.5874
4032/4849 [=======================>......] - ETA: 51s - loss: 0.6764 - acc: 0.5875
4096/4849 [========================>.....] - ETA: 47s - loss: 0.6768 - acc: 0.5869
4160/4849 [========================>.....] - ETA: 43s - loss: 0.6769 - acc: 0.5865
4224/4849 [=========================>....] - ETA: 39s - loss: 0.6772 - acc: 0.5866
4288/4849 [=========================>....] - ETA: 35s - loss: 0.6767 - acc: 0.5870
4352/4849 [=========================>....] - ETA: 31s - loss: 0.6773 - acc: 0.5859
4416/4849 [==========================>...] - ETA: 27s - loss: 0.6772 - acc: 0.5858
4480/4849 [==========================>...] - ETA: 23s - loss: 0.6767 - acc: 0.5871
4544/4849 [===========================>..] - ETA: 19s - loss: 0.6766 - acc: 0.5874
4608/4849 [===========================>..] - ETA: 15s - loss: 0.6769 - acc: 0.5872
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6769 - acc: 0.5865
4736/4849 [============================>.] - ETA: 7s - loss: 0.6767 - acc: 0.5868 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6764 - acc: 0.5875
4849/4849 [==============================] - 323s 67ms/step - loss: 0.6767 - acc: 0.5869 - val_loss: 0.6945 - val_acc: 0.5455

Epoch 00008: val_acc did not improve from 0.59555
Epoch 9/10

  64/4849 [..............................] - ETA: 5:14 - loss: 0.7362 - acc: 0.4531
 128/4849 [..............................] - ETA: 4:56 - loss: 0.6993 - acc: 0.5156
 192/4849 [>.............................] - ETA: 4:49 - loss: 0.6867 - acc: 0.5312
 256/4849 [>.............................] - ETA: 4:43 - loss: 0.6739 - acc: 0.5664
 320/4849 [>.............................] - ETA: 4:43 - loss: 0.6777 - acc: 0.5625
 384/4849 [=>............................] - ETA: 4:36 - loss: 0.6792 - acc: 0.5677
 448/4849 [=>............................] - ETA: 4:35 - loss: 0.6776 - acc: 0.5625
 512/4849 [==>...........................] - ETA: 4:35 - loss: 0.6773 - acc: 0.5645
 576/4849 [==>...........................] - ETA: 4:27 - loss: 0.6744 - acc: 0.5712
 640/4849 [==>...........................] - ETA: 4:22 - loss: 0.6736 - acc: 0.5750
 704/4849 [===>..........................] - ETA: 4:19 - loss: 0.6724 - acc: 0.5824
 768/4849 [===>..........................] - ETA: 4:14 - loss: 0.6710 - acc: 0.5872
 832/4849 [====>.........................] - ETA: 4:11 - loss: 0.6707 - acc: 0.5877
 896/4849 [====>.........................] - ETA: 4:07 - loss: 0.6703 - acc: 0.5871
 960/4849 [====>.........................] - ETA: 4:03 - loss: 0.6740 - acc: 0.5802
1024/4849 [=====>........................] - ETA: 4:02 - loss: 0.6745 - acc: 0.5811
1088/4849 [=====>........................] - ETA: 3:58 - loss: 0.6737 - acc: 0.5800
1152/4849 [======>.......................] - ETA: 3:54 - loss: 0.6730 - acc: 0.5807
1216/4849 [======>.......................] - ETA: 3:51 - loss: 0.6714 - acc: 0.5831
1280/4849 [======>.......................] - ETA: 3:46 - loss: 0.6726 - acc: 0.5781
1344/4849 [=======>......................] - ETA: 3:40 - loss: 0.6706 - acc: 0.5856
1408/4849 [=======>......................] - ETA: 3:38 - loss: 0.6710 - acc: 0.5859
1472/4849 [========>.....................] - ETA: 3:35 - loss: 0.6720 - acc: 0.5836
1536/4849 [========>.....................] - ETA: 3:29 - loss: 0.6725 - acc: 0.5833
1600/4849 [========>.....................] - ETA: 3:26 - loss: 0.6730 - acc: 0.5806
1664/4849 [=========>....................] - ETA: 3:22 - loss: 0.6748 - acc: 0.5769
1728/4849 [=========>....................] - ETA: 3:17 - loss: 0.6761 - acc: 0.5758
1792/4849 [==========>...................] - ETA: 3:13 - loss: 0.6754 - acc: 0.5765
1856/4849 [==========>...................] - ETA: 3:09 - loss: 0.6757 - acc: 0.5765
1920/4849 [==========>...................] - ETA: 3:05 - loss: 0.6766 - acc: 0.5740
1984/4849 [===========>..................] - ETA: 3:01 - loss: 0.6758 - acc: 0.5766
2048/4849 [===========>..................] - ETA: 2:57 - loss: 0.6737 - acc: 0.5796
2112/4849 [============>.................] - ETA: 2:52 - loss: 0.6739 - acc: 0.5805
2176/4849 [============>.................] - ETA: 2:48 - loss: 0.6729 - acc: 0.5827
2240/4849 [============>.................] - ETA: 2:44 - loss: 0.6715 - acc: 0.5853
2304/4849 [=============>................] - ETA: 2:39 - loss: 0.6710 - acc: 0.5864
2368/4849 [=============>................] - ETA: 2:36 - loss: 0.6708 - acc: 0.5857
2432/4849 [==============>...............] - ETA: 2:32 - loss: 0.6694 - acc: 0.5900
2496/4849 [==============>...............] - ETA: 2:28 - loss: 0.6685 - acc: 0.5905
2560/4849 [==============>...............] - ETA: 2:24 - loss: 0.6679 - acc: 0.5891
2624/4849 [===============>..............] - ETA: 2:20 - loss: 0.6688 - acc: 0.5888
2688/4849 [===============>..............] - ETA: 2:15 - loss: 0.6712 - acc: 0.5844
2752/4849 [================>.............] - ETA: 2:11 - loss: 0.6704 - acc: 0.5850
2816/4849 [================>.............] - ETA: 2:07 - loss: 0.6699 - acc: 0.5863
2880/4849 [================>.............] - ETA: 2:03 - loss: 0.6697 - acc: 0.5875
2944/4849 [=================>............] - ETA: 1:59 - loss: 0.6699 - acc: 0.5883
3008/4849 [=================>............] - ETA: 1:55 - loss: 0.6701 - acc: 0.5881
3072/4849 [==================>...........] - ETA: 1:51 - loss: 0.6696 - acc: 0.5895
3136/4849 [==================>...........] - ETA: 1:47 - loss: 0.6691 - acc: 0.5918
3200/4849 [==================>...........] - ETA: 1:42 - loss: 0.6694 - acc: 0.5916
3264/4849 [===================>..........] - ETA: 1:38 - loss: 0.6710 - acc: 0.5898
3328/4849 [===================>..........] - ETA: 1:34 - loss: 0.6697 - acc: 0.5922
3392/4849 [===================>..........] - ETA: 1:31 - loss: 0.6699 - acc: 0.5923
3456/4849 [====================>.........] - ETA: 1:27 - loss: 0.6697 - acc: 0.5920
3520/4849 [====================>.........] - ETA: 1:23 - loss: 0.6701 - acc: 0.5923
3584/4849 [=====================>........] - ETA: 1:19 - loss: 0.6699 - acc: 0.5929
3648/4849 [=====================>........] - ETA: 1:14 - loss: 0.6708 - acc: 0.5913
3712/4849 [=====================>........] - ETA: 1:11 - loss: 0.6708 - acc: 0.5916
3776/4849 [======================>.......] - ETA: 1:06 - loss: 0.6708 - acc: 0.5919
3840/4849 [======================>.......] - ETA: 1:02 - loss: 0.6709 - acc: 0.5922
3904/4849 [=======================>......] - ETA: 58s - loss: 0.6718 - acc: 0.5907 
3968/4849 [=======================>......] - ETA: 54s - loss: 0.6711 - acc: 0.5922
4032/4849 [=======================>......] - ETA: 50s - loss: 0.6708 - acc: 0.5933
4096/4849 [========================>.....] - ETA: 46s - loss: 0.6708 - acc: 0.5930
4160/4849 [========================>.....] - ETA: 42s - loss: 0.6700 - acc: 0.5947
4224/4849 [=========================>....] - ETA: 38s - loss: 0.6697 - acc: 0.5956
4288/4849 [=========================>....] - ETA: 34s - loss: 0.6701 - acc: 0.5940
4352/4849 [=========================>....] - ETA: 30s - loss: 0.6702 - acc: 0.5938
4416/4849 [==========================>...] - ETA: 26s - loss: 0.6697 - acc: 0.5953
4480/4849 [==========================>...] - ETA: 22s - loss: 0.6702 - acc: 0.5949
4544/4849 [===========================>..] - ETA: 18s - loss: 0.6713 - acc: 0.5922
4608/4849 [===========================>..] - ETA: 14s - loss: 0.6722 - acc: 0.5903
4672/4849 [===========================>..] - ETA: 10s - loss: 0.6724 - acc: 0.5893
4736/4849 [============================>.] - ETA: 7s - loss: 0.6725 - acc: 0.5889 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6730 - acc: 0.5887
4849/4849 [==============================] - 314s 65ms/step - loss: 0.6729 - acc: 0.5890 - val_loss: 0.6792 - val_acc: 0.5733

Epoch 00009: val_acc did not improve from 0.59555
Epoch 10/10

  64/4849 [..............................] - ETA: 5:03 - loss: 0.6853 - acc: 0.5781
 128/4849 [..............................] - ETA: 5:17 - loss: 0.6758 - acc: 0.6328
 192/4849 [>.............................] - ETA: 4:57 - loss: 0.6793 - acc: 0.5938
 256/4849 [>.............................] - ETA: 4:46 - loss: 0.6820 - acc: 0.5781
 320/4849 [>.............................] - ETA: 4:40 - loss: 0.6783 - acc: 0.5750
 384/4849 [=>............................] - ETA: 4:37 - loss: 0.6757 - acc: 0.5703
 448/4849 [=>............................] - ETA: 4:32 - loss: 0.6748 - acc: 0.5714
 512/4849 [==>...........................] - ETA: 4:28 - loss: 0.6748 - acc: 0.5762
 576/4849 [==>...........................] - ETA: 4:22 - loss: 0.6787 - acc: 0.5712
 640/4849 [==>...........................] - ETA: 4:16 - loss: 0.6804 - acc: 0.5687
 704/4849 [===>..........................] - ETA: 4:13 - loss: 0.6830 - acc: 0.5653
 768/4849 [===>..........................] - ETA: 4:08 - loss: 0.6811 - acc: 0.5677
 832/4849 [====>.........................] - ETA: 4:06 - loss: 0.6778 - acc: 0.5769
 896/4849 [====>.........................] - ETA: 4:03 - loss: 0.6755 - acc: 0.5781
 960/4849 [====>.........................] - ETA: 3:57 - loss: 0.6752 - acc: 0.5792
1024/4849 [=====>........................] - ETA: 3:53 - loss: 0.6739 - acc: 0.5840
1088/4849 [=====>........................] - ETA: 3:48 - loss: 0.6736 - acc: 0.5846
1152/4849 [======>.......................] - ETA: 3:43 - loss: 0.6727 - acc: 0.5868
1216/4849 [======>.......................] - ETA: 3:39 - loss: 0.6715 - acc: 0.5888
1280/4849 [======>.......................] - ETA: 3:37 - loss: 0.6731 - acc: 0.5859
1344/4849 [=======>......................] - ETA: 3:35 - loss: 0.6736 - acc: 0.5826
1408/4849 [=======>......................] - ETA: 3:31 - loss: 0.6736 - acc: 0.5859
1472/4849 [========>.....................] - ETA: 3:27 - loss: 0.6738 - acc: 0.5849
1536/4849 [========>.....................] - ETA: 3:23 - loss: 0.6733 - acc: 0.5833
1600/4849 [========>.....................] - ETA: 3:20 - loss: 0.6744 - acc: 0.5781
1664/4849 [=========>....................] - ETA: 3:16 - loss: 0.6756 - acc: 0.5757
1728/4849 [=========>....................] - ETA: 3:11 - loss: 0.6750 - acc: 0.5775
1792/4849 [==========>...................] - ETA: 3:07 - loss: 0.6747 - acc: 0.5781
1856/4849 [==========>...................] - ETA: 3:03 - loss: 0.6747 - acc: 0.5787
1920/4849 [==========>...................] - ETA: 3:00 - loss: 0.6740 - acc: 0.5786
1984/4849 [===========>..................] - ETA: 2:56 - loss: 0.6737 - acc: 0.5771
2048/4849 [===========>..................] - ETA: 2:52 - loss: 0.6726 - acc: 0.5801
2112/4849 [============>.................] - ETA: 2:48 - loss: 0.6728 - acc: 0.5819
2176/4849 [============>.................] - ETA: 2:44 - loss: 0.6727 - acc: 0.5827
2240/4849 [============>.................] - ETA: 2:40 - loss: 0.6737 - acc: 0.5804
2304/4849 [=============>................] - ETA: 2:36 - loss: 0.6746 - acc: 0.5794
2368/4849 [=============>................] - ETA: 2:32 - loss: 0.6747 - acc: 0.5777
2432/4849 [==============>...............] - ETA: 2:28 - loss: 0.6734 - acc: 0.5818
2496/4849 [==============>...............] - ETA: 2:24 - loss: 0.6736 - acc: 0.5809
2560/4849 [==============>...............] - ETA: 2:20 - loss: 0.6736 - acc: 0.5797
2624/4849 [===============>..............] - ETA: 2:16 - loss: 0.6730 - acc: 0.5796
2688/4849 [===============>..............] - ETA: 2:12 - loss: 0.6728 - acc: 0.5815
2752/4849 [================>.............] - ETA: 2:08 - loss: 0.6727 - acc: 0.5814
2816/4849 [================>.............] - ETA: 2:03 - loss: 0.6733 - acc: 0.5817
2880/4849 [================>.............] - ETA: 1:59 - loss: 0.6727 - acc: 0.5844
2944/4849 [=================>............] - ETA: 1:56 - loss: 0.6723 - acc: 0.5856
3008/4849 [=================>............] - ETA: 1:52 - loss: 0.6723 - acc: 0.5854
3072/4849 [==================>...........] - ETA: 1:48 - loss: 0.6727 - acc: 0.5843
3136/4849 [==================>...........] - ETA: 1:44 - loss: 0.6724 - acc: 0.5871
3200/4849 [==================>...........] - ETA: 1:40 - loss: 0.6721 - acc: 0.5869
3264/4849 [===================>..........] - ETA: 1:36 - loss: 0.6729 - acc: 0.5858
3328/4849 [===================>..........] - ETA: 1:32 - loss: 0.6725 - acc: 0.5862
3392/4849 [===================>..........] - ETA: 1:28 - loss: 0.6728 - acc: 0.5864
3456/4849 [====================>.........] - ETA: 1:24 - loss: 0.6724 - acc: 0.5856
3520/4849 [====================>.........] - ETA: 1:20 - loss: 0.6724 - acc: 0.5861
3584/4849 [=====================>........] - ETA: 1:16 - loss: 0.6723 - acc: 0.5859
3648/4849 [=====================>........] - ETA: 1:13 - loss: 0.6731 - acc: 0.5850
3712/4849 [=====================>........] - ETA: 1:09 - loss: 0.6735 - acc: 0.5843
3776/4849 [======================>.......] - ETA: 1:05 - loss: 0.6729 - acc: 0.5847
3840/4849 [======================>.......] - ETA: 1:01 - loss: 0.6729 - acc: 0.5854
3904/4849 [=======================>......] - ETA: 57s - loss: 0.6727 - acc: 0.5848 
3968/4849 [=======================>......] - ETA: 53s - loss: 0.6729 - acc: 0.5847
4032/4849 [=======================>......] - ETA: 49s - loss: 0.6725 - acc: 0.5851
4096/4849 [========================>.....] - ETA: 45s - loss: 0.6719 - acc: 0.5867
4160/4849 [========================>.....] - ETA: 41s - loss: 0.6714 - acc: 0.5868
4224/4849 [=========================>....] - ETA: 37s - loss: 0.6711 - acc: 0.5878
4288/4849 [=========================>....] - ETA: 33s - loss: 0.6715 - acc: 0.5868
4352/4849 [=========================>....] - ETA: 30s - loss: 0.6720 - acc: 0.5857
4416/4849 [==========================>...] - ETA: 26s - loss: 0.6717 - acc: 0.5863
4480/4849 [==========================>...] - ETA: 22s - loss: 0.6719 - acc: 0.5862
4544/4849 [===========================>..] - ETA: 18s - loss: 0.6719 - acc: 0.5863
4608/4849 [===========================>..] - ETA: 14s - loss: 0.6723 - acc: 0.5859
4672/4849 [===========================>..] - ETA: 10s - loss: 0.6721 - acc: 0.5856
4736/4849 [============================>.] - ETA: 6s - loss: 0.6723 - acc: 0.5847 
4800/4849 [============================>.] - ETA: 2s - loss: 0.6735 - acc: 0.5823
4849/4849 [==============================] - 303s 62ms/step - loss: 0.6732 - acc: 0.5824 - val_loss: 0.6792 - val_acc: 0.5863

Epoch 00010: val_acc did not improve from 0.59555
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9beb28cf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9beb28cf50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9beb22bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9beb22bc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c07c66d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c07c66d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9beae4dcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9beae4dcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9beadbea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9beadbea90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beae92dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beae92dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9beae4ddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9beae4ddd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c0786190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96c0786190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9520204990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9520204990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9beabccbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9beabccbd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beaa74390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beaa74390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95202db5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95202db5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7d858d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf7d858d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f949c1058d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f949c1058d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bea868ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bea868ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c1a5390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c1a5390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94f01d1410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94f01d1410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beac42dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beac42dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bea6cd750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bea6cd750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bea8a1d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bea8a1d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bea622dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bea622dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bea58c250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bea58c250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bea548050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bea548050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bea340c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bea340c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bea51e610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bea51e610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bc987e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bc987e8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bea340ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bea340ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bea2d1f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bea2d1f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bea1c9090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9bea1c9090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f967c7f19d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f967c7f19d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bc988a8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bc988a8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bea423dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9bea423dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969806f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969806f1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f967c611590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f967c611590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f967c4dff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f967c4dff10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f967c5e3090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f967c5e3090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f967c57e590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f967c57e590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f967c7de5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f967c7de5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f967c4cf450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f967c4cf450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f967c1b4350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f967c1b4350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f967c4e18d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f967c4e18d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f967c4ca490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f967c4ca490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f967c1862d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f967c1862d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f967c0b2f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f967c0b2f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96606c9ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96606c9ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f967c1fba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f967c1fba50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f966073bd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f966073bd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9660702ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9660702ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f966040d950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f966040d950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f966064b190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f966064b190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96606c99d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96606c99d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f966040df50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f966040df50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96603030d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96603030d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9660152410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9660152410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96207be0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96207be0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bea91c990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bea91c990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96606ea710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96606ea710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9660105f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9660105f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96207d7bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96207d7bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9620517250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9620517250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9620483650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9620483650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96207d7890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96207d7890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f962048ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f962048ec50>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 1:53
 128/1348 [=>............................] - ETA: 1:07
 192/1348 [===>..........................] - ETA: 50s 
 256/1348 [====>.........................] - ETA: 42s
 320/1348 [======>.......................] - ETA: 36s
 384/1348 [=======>......................] - ETA: 31s
 448/1348 [========>.....................] - ETA: 29s
 512/1348 [==========>...................] - ETA: 27s
 576/1348 [===========>..................] - ETA: 24s
 640/1348 [=============>................] - ETA: 21s
 704/1348 [==============>...............] - ETA: 19s
 768/1348 [================>.............] - ETA: 17s
 832/1348 [=================>............] - ETA: 15s
 896/1348 [==================>...........] - ETA: 13s
 960/1348 [====================>.........] - ETA: 11s
1024/1348 [=====================>........] - ETA: 9s 
1088/1348 [=======================>......] - ETA: 7s
1152/1348 [========================>.....] - ETA: 5s
1216/1348 [==========================>...] - ETA: 3s
1280/1348 [===========================>..] - ETA: 1s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 38s 28ms/step
loss: 0.6757048393215553
acc: 0.5875370919881305
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f949c16f710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f949c16f710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f96e004f290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f96e004f290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f960046e050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f960046e050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f960023ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f960023ded0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96004a0210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96004a0210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f960046e990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f960046e990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96004b1910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96004b1910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beb08e5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beb08e5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9beb209c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9beb209c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f949c0af7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f949c0af7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c0cd410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c0cd410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9beb209250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9beb209250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beafff810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beafff810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f949c0e6290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f949c0e6290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f949c0be2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f949c0be2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f946054afd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f946054afd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f949c0be8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f949c0be8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f946052cad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f946052cad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94604d0cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94604d0cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94601d67d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94601d67d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94604e37d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94604e37d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94604d08d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94604d08d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94602f4410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94602f4410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94407400d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94407400d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9440743890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9440743890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9460041f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9460041f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9440740950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9440740950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9440665490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9440665490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94407ecdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94407ecdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f944065b090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f944065b090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9440558050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9440558050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94407ec590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94407ec590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94403e7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94403e7290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f944065b550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f944065b550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94400832d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94400832d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9440104790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9440104790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9440151390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9440151390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9440076a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9440076a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94205f0790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94205f0790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f942055eb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f942055eb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9440174e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9440174e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94205f0390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94205f0390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94204c55d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94204c55d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9420291090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9420291090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94202cbfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94202cbfd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f942031f110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f942031f110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9420291110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9420291110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93401a9e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93401a9e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93400ef650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93400ef650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9320743f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9320743f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93400c4a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93400c4a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93401dccd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93401dccd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9340051910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9340051910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f932054fc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f932054fc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9320462650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9320462650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9320525090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9320525090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f932054f110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f932054f110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93205bf7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93205bf7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f932026fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f932026fc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f932027d250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f932027d250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9320327750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9320327750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f932035d290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f932035d290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f932014aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f932014aa90>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 23:39 - loss: 0.7783 - acc: 0.4531
 128/4849 [..............................] - ETA: 15:18 - loss: 0.7566 - acc: 0.5000
 192/4849 [>.............................] - ETA: 12:05 - loss: 0.7648 - acc: 0.4688
 256/4849 [>.............................] - ETA: 10:39 - loss: 0.7623 - acc: 0.4961
 320/4849 [>.............................] - ETA: 9:49 - loss: 0.7480 - acc: 0.5031 
 384/4849 [=>............................] - ETA: 9:13 - loss: 0.7441 - acc: 0.5052
 448/4849 [=>............................] - ETA: 8:41 - loss: 0.7421 - acc: 0.5112
 512/4849 [==>...........................] - ETA: 8:16 - loss: 0.7426 - acc: 0.5117
 576/4849 [==>...........................] - ETA: 8:00 - loss: 0.7361 - acc: 0.5208
 640/4849 [==>...........................] - ETA: 7:44 - loss: 0.7315 - acc: 0.5234
 704/4849 [===>..........................] - ETA: 7:31 - loss: 0.7295 - acc: 0.5199
 768/4849 [===>..........................] - ETA: 7:18 - loss: 0.7258 - acc: 0.5234
 832/4849 [====>.........................] - ETA: 7:04 - loss: 0.7184 - acc: 0.5325
 896/4849 [====>.........................] - ETA: 6:56 - loss: 0.7142 - acc: 0.5335
 960/4849 [====>.........................] - ETA: 6:46 - loss: 0.7110 - acc: 0.5365
1024/4849 [=====>........................] - ETA: 6:36 - loss: 0.7163 - acc: 0.5342
1088/4849 [=====>........................] - ETA: 6:27 - loss: 0.7179 - acc: 0.5349
1152/4849 [======>.......................] - ETA: 6:18 - loss: 0.7167 - acc: 0.5347
1216/4849 [======>.......................] - ETA: 6:10 - loss: 0.7164 - acc: 0.5345
1280/4849 [======>.......................] - ETA: 6:01 - loss: 0.7126 - acc: 0.5367
1344/4849 [=======>......................] - ETA: 5:53 - loss: 0.7117 - acc: 0.5365
1408/4849 [=======>......................] - ETA: 5:44 - loss: 0.7114 - acc: 0.5355
1472/4849 [========>.....................] - ETA: 5:35 - loss: 0.7123 - acc: 0.5346
1536/4849 [========>.....................] - ETA: 5:28 - loss: 0.7129 - acc: 0.5345
1600/4849 [========>.....................] - ETA: 5:20 - loss: 0.7151 - acc: 0.5325
1664/4849 [=========>....................] - ETA: 5:14 - loss: 0.7146 - acc: 0.5331
1728/4849 [=========>....................] - ETA: 5:06 - loss: 0.7176 - acc: 0.5289
1792/4849 [==========>...................] - ETA: 4:58 - loss: 0.7181 - acc: 0.5307
1856/4849 [==========>...................] - ETA: 4:52 - loss: 0.7200 - acc: 0.5259
1920/4849 [==========>...................] - ETA: 4:45 - loss: 0.7205 - acc: 0.5260
1984/4849 [===========>..................] - ETA: 4:38 - loss: 0.7210 - acc: 0.5267
2048/4849 [===========>..................] - ETA: 4:30 - loss: 0.7217 - acc: 0.5269
2112/4849 [============>.................] - ETA: 4:23 - loss: 0.7213 - acc: 0.5279
2176/4849 [============>.................] - ETA: 4:17 - loss: 0.7201 - acc: 0.5285
2240/4849 [============>.................] - ETA: 4:10 - loss: 0.7209 - acc: 0.5286
2304/4849 [=============>................] - ETA: 4:04 - loss: 0.7218 - acc: 0.5260
2368/4849 [=============>................] - ETA: 3:57 - loss: 0.7225 - acc: 0.5241
2432/4849 [==============>...............] - ETA: 3:51 - loss: 0.7222 - acc: 0.5226
2496/4849 [==============>...............] - ETA: 3:44 - loss: 0.7214 - acc: 0.5224
2560/4849 [==============>...............] - ETA: 3:38 - loss: 0.7195 - acc: 0.5246
2624/4849 [===============>..............] - ETA: 3:31 - loss: 0.7195 - acc: 0.5244
2688/4849 [===============>..............] - ETA: 3:25 - loss: 0.7187 - acc: 0.5249
2752/4849 [================>.............] - ETA: 3:19 - loss: 0.7174 - acc: 0.5273
2816/4849 [================>.............] - ETA: 3:13 - loss: 0.7179 - acc: 0.5252
2880/4849 [================>.............] - ETA: 3:06 - loss: 0.7174 - acc: 0.5260
2944/4849 [=================>............] - ETA: 3:00 - loss: 0.7181 - acc: 0.5248
3008/4849 [=================>............] - ETA: 2:54 - loss: 0.7186 - acc: 0.5233
3072/4849 [==================>...........] - ETA: 2:47 - loss: 0.7185 - acc: 0.5228
3136/4849 [==================>...........] - ETA: 2:41 - loss: 0.7184 - acc: 0.5220
3200/4849 [==================>...........] - ETA: 2:35 - loss: 0.7186 - acc: 0.5216
3264/4849 [===================>..........] - ETA: 2:29 - loss: 0.7191 - acc: 0.5208
3328/4849 [===================>..........] - ETA: 2:22 - loss: 0.7179 - acc: 0.5237
3392/4849 [===================>..........] - ETA: 2:16 - loss: 0.7180 - acc: 0.5239
3456/4849 [====================>.........] - ETA: 2:10 - loss: 0.7174 - acc: 0.5258
3520/4849 [====================>.........] - ETA: 2:04 - loss: 0.7178 - acc: 0.5264
3584/4849 [=====================>........] - ETA: 1:58 - loss: 0.7175 - acc: 0.5265
3648/4849 [=====================>........] - ETA: 1:51 - loss: 0.7176 - acc: 0.5260
3712/4849 [=====================>........] - ETA: 1:45 - loss: 0.7174 - acc: 0.5269
3776/4849 [======================>.......] - ETA: 1:39 - loss: 0.7172 - acc: 0.5267
3840/4849 [======================>.......] - ETA: 1:33 - loss: 0.7163 - acc: 0.5273
3904/4849 [=======================>......] - ETA: 1:27 - loss: 0.7163 - acc: 0.5272
3968/4849 [=======================>......] - ETA: 1:21 - loss: 0.7155 - acc: 0.5277
4032/4849 [=======================>......] - ETA: 1:15 - loss: 0.7153 - acc: 0.5280
4096/4849 [========================>.....] - ETA: 1:09 - loss: 0.7156 - acc: 0.5269
4160/4849 [========================>.....] - ETA: 1:03 - loss: 0.7155 - acc: 0.5269
4224/4849 [=========================>....] - ETA: 57s - loss: 0.7154 - acc: 0.5260 
4288/4849 [=========================>....] - ETA: 51s - loss: 0.7153 - acc: 0.5261
4352/4849 [=========================>....] - ETA: 45s - loss: 0.7148 - acc: 0.5273
4416/4849 [==========================>...] - ETA: 39s - loss: 0.7149 - acc: 0.5269
4480/4849 [==========================>...] - ETA: 34s - loss: 0.7146 - acc: 0.5277
4544/4849 [===========================>..] - ETA: 28s - loss: 0.7145 - acc: 0.5279
4608/4849 [===========================>..] - ETA: 22s - loss: 0.7141 - acc: 0.5284
4672/4849 [===========================>..] - ETA: 16s - loss: 0.7134 - acc: 0.5300
4736/4849 [============================>.] - ETA: 10s - loss: 0.7132 - acc: 0.5293
4800/4849 [============================>.] - ETA: 4s - loss: 0.7137 - acc: 0.5285 
4849/4849 [==============================] - 464s 96ms/step - loss: 0.7138 - acc: 0.5275 - val_loss: 0.6921 - val_acc: 0.5436

Epoch 00001: val_acc improved from -inf to 0.54360, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window18/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 7:15 - loss: 0.6806 - acc: 0.5938
 128/4849 [..............................] - ETA: 7:07 - loss: 0.6864 - acc: 0.5781
 192/4849 [>.............................] - ETA: 6:55 - loss: 0.6836 - acc: 0.5729
 256/4849 [>.............................] - ETA: 6:51 - loss: 0.6708 - acc: 0.5859
 320/4849 [>.............................] - ETA: 6:43 - loss: 0.6767 - acc: 0.5906
 384/4849 [=>............................] - ETA: 6:40 - loss: 0.6838 - acc: 0.5729
 448/4849 [=>............................] - ETA: 6:29 - loss: 0.6895 - acc: 0.5692
 512/4849 [==>...........................] - ETA: 6:24 - loss: 0.6868 - acc: 0.5645
 576/4849 [==>...........................] - ETA: 6:20 - loss: 0.6844 - acc: 0.5764
 640/4849 [==>...........................] - ETA: 6:17 - loss: 0.6847 - acc: 0.5750
 704/4849 [===>..........................] - ETA: 6:13 - loss: 0.6893 - acc: 0.5710
 768/4849 [===>..........................] - ETA: 6:06 - loss: 0.6922 - acc: 0.5677
 832/4849 [====>.........................] - ETA: 6:02 - loss: 0.6938 - acc: 0.5649
 896/4849 [====>.........................] - ETA: 5:54 - loss: 0.6941 - acc: 0.5547
 960/4849 [====>.........................] - ETA: 5:50 - loss: 0.6930 - acc: 0.5552
1024/4849 [=====>........................] - ETA: 5:44 - loss: 0.6932 - acc: 0.5557
1088/4849 [=====>........................] - ETA: 5:36 - loss: 0.6940 - acc: 0.5561
1152/4849 [======>.......................] - ETA: 5:32 - loss: 0.6940 - acc: 0.5556
1216/4849 [======>.......................] - ETA: 5:26 - loss: 0.6947 - acc: 0.5518
1280/4849 [======>.......................] - ETA: 5:20 - loss: 0.6948 - acc: 0.5508
1344/4849 [=======>......................] - ETA: 5:14 - loss: 0.6954 - acc: 0.5484
1408/4849 [=======>......................] - ETA: 5:09 - loss: 0.6958 - acc: 0.5490
1472/4849 [========>.....................] - ETA: 5:03 - loss: 0.6971 - acc: 0.5469
1536/4849 [========>.....................] - ETA: 4:57 - loss: 0.6975 - acc: 0.5469
1600/4849 [========>.....................] - ETA: 4:52 - loss: 0.6985 - acc: 0.5450
1664/4849 [=========>....................] - ETA: 4:46 - loss: 0.6970 - acc: 0.5469
1728/4849 [=========>....................] - ETA: 4:40 - loss: 0.6974 - acc: 0.5451
1792/4849 [==========>...................] - ETA: 4:34 - loss: 0.6969 - acc: 0.5469
1856/4849 [==========>...................] - ETA: 4:28 - loss: 0.6969 - acc: 0.5469
1920/4849 [==========>...................] - ETA: 4:23 - loss: 0.6974 - acc: 0.5443
1984/4849 [===========>..................] - ETA: 4:16 - loss: 0.6980 - acc: 0.5413
2048/4849 [===========>..................] - ETA: 4:11 - loss: 0.6964 - acc: 0.5454
2112/4849 [============>.................] - ETA: 4:06 - loss: 0.6968 - acc: 0.5445
2176/4849 [============>.................] - ETA: 3:59 - loss: 0.6967 - acc: 0.5450
2240/4849 [============>.................] - ETA: 3:54 - loss: 0.6967 - acc: 0.5455
2304/4849 [=============>................] - ETA: 3:48 - loss: 0.6979 - acc: 0.5417
2368/4849 [=============>................] - ETA: 3:42 - loss: 0.6980 - acc: 0.5410
2432/4849 [==============>...............] - ETA: 3:37 - loss: 0.6986 - acc: 0.5382
2496/4849 [==============>...............] - ETA: 3:31 - loss: 0.6979 - acc: 0.5393
2560/4849 [==============>...............] - ETA: 3:25 - loss: 0.6973 - acc: 0.5414
2624/4849 [===============>..............] - ETA: 3:19 - loss: 0.6977 - acc: 0.5393
2688/4849 [===============>..............] - ETA: 3:14 - loss: 0.6987 - acc: 0.5368
2752/4849 [================>.............] - ETA: 3:07 - loss: 0.6989 - acc: 0.5367
2816/4849 [================>.............] - ETA: 3:02 - loss: 0.6983 - acc: 0.5394
2880/4849 [================>.............] - ETA: 2:56 - loss: 0.6986 - acc: 0.5392
2944/4849 [=================>............] - ETA: 2:51 - loss: 0.6973 - acc: 0.5408
3008/4849 [=================>............] - ETA: 2:45 - loss: 0.6975 - acc: 0.5399
3072/4849 [==================>...........] - ETA: 2:39 - loss: 0.6986 - acc: 0.5368
3136/4849 [==================>...........] - ETA: 2:34 - loss: 0.6977 - acc: 0.5399
3200/4849 [==================>...........] - ETA: 2:28 - loss: 0.6980 - acc: 0.5413
3264/4849 [===================>..........] - ETA: 2:22 - loss: 0.6973 - acc: 0.5411
3328/4849 [===================>..........] - ETA: 2:17 - loss: 0.6970 - acc: 0.5421
3392/4849 [===================>..........] - ETA: 2:11 - loss: 0.6966 - acc: 0.5427
3456/4849 [====================>.........] - ETA: 2:05 - loss: 0.6963 - acc: 0.5431
3520/4849 [====================>.........] - ETA: 1:59 - loss: 0.6963 - acc: 0.5440
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6956 - acc: 0.5452
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.6949 - acc: 0.5466
3712/4849 [=====================>........] - ETA: 1:42 - loss: 0.6946 - acc: 0.5477
3776/4849 [======================>.......] - ETA: 1:36 - loss: 0.6952 - acc: 0.5458
3840/4849 [======================>.......] - ETA: 1:30 - loss: 0.6959 - acc: 0.5448
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6957 - acc: 0.5451
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6961 - acc: 0.5439
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6957 - acc: 0.5437
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6960 - acc: 0.5430
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6959 - acc: 0.5428
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6963 - acc: 0.5419 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6963 - acc: 0.5427
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6964 - acc: 0.5437
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6963 - acc: 0.5442
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6962 - acc: 0.5440
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6963 - acc: 0.5429
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6963 - acc: 0.5425
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6959 - acc: 0.5443
4736/4849 [============================>.] - ETA: 10s - loss: 0.6962 - acc: 0.5450
4800/4849 [============================>.] - ETA: 4s - loss: 0.6960 - acc: 0.5450 
4849/4849 [==============================] - 451s 93ms/step - loss: 0.6959 - acc: 0.5446 - val_loss: 0.7006 - val_acc: 0.5158

Epoch 00002: val_acc did not improve from 0.54360
Epoch 3/10

  64/4849 [..............................] - ETA: 6:57 - loss: 0.6627 - acc: 0.6094
 128/4849 [..............................] - ETA: 7:00 - loss: 0.6933 - acc: 0.5469
 192/4849 [>.............................] - ETA: 6:42 - loss: 0.6980 - acc: 0.5469
 256/4849 [>.............................] - ETA: 6:49 - loss: 0.7015 - acc: 0.5391
 320/4849 [>.............................] - ETA: 6:54 - loss: 0.7063 - acc: 0.5219
 384/4849 [=>............................] - ETA: 6:45 - loss: 0.7080 - acc: 0.5234
 448/4849 [=>............................] - ETA: 6:38 - loss: 0.7105 - acc: 0.5179
 512/4849 [==>...........................] - ETA: 6:30 - loss: 0.7124 - acc: 0.5117
 576/4849 [==>...........................] - ETA: 6:21 - loss: 0.7111 - acc: 0.5139
 640/4849 [==>...........................] - ETA: 6:19 - loss: 0.7076 - acc: 0.5203
 704/4849 [===>..........................] - ETA: 6:11 - loss: 0.7057 - acc: 0.5213
 768/4849 [===>..........................] - ETA: 6:05 - loss: 0.7090 - acc: 0.5143
 832/4849 [====>.........................] - ETA: 6:01 - loss: 0.7089 - acc: 0.5132
 896/4849 [====>.........................] - ETA: 5:53 - loss: 0.7079 - acc: 0.5190
 960/4849 [====>.........................] - ETA: 5:49 - loss: 0.7075 - acc: 0.5167
1024/4849 [=====>........................] - ETA: 5:43 - loss: 0.7099 - acc: 0.5098
1088/4849 [=====>........................] - ETA: 5:37 - loss: 0.7096 - acc: 0.5092
1152/4849 [======>.......................] - ETA: 5:33 - loss: 0.7077 - acc: 0.5130
1216/4849 [======>.......................] - ETA: 5:27 - loss: 0.7055 - acc: 0.5164
1280/4849 [======>.......................] - ETA: 5:20 - loss: 0.7062 - acc: 0.5164
1344/4849 [=======>......................] - ETA: 5:13 - loss: 0.7068 - acc: 0.5193
1408/4849 [=======>......................] - ETA: 5:09 - loss: 0.7065 - acc: 0.5206
1472/4849 [========>.....................] - ETA: 5:03 - loss: 0.7050 - acc: 0.5224
1536/4849 [========>.....................] - ETA: 4:58 - loss: 0.7056 - acc: 0.5202
1600/4849 [========>.....................] - ETA: 4:52 - loss: 0.7032 - acc: 0.5262
1664/4849 [=========>....................] - ETA: 4:46 - loss: 0.7023 - acc: 0.5294
1728/4849 [=========>....................] - ETA: 4:40 - loss: 0.6994 - acc: 0.5353
1792/4849 [==========>...................] - ETA: 4:35 - loss: 0.6989 - acc: 0.5357
1856/4849 [==========>...................] - ETA: 4:29 - loss: 0.7008 - acc: 0.5350
1920/4849 [==========>...................] - ETA: 4:23 - loss: 0.6989 - acc: 0.5391
1984/4849 [===========>..................] - ETA: 4:17 - loss: 0.6987 - acc: 0.5388
2048/4849 [===========>..................] - ETA: 4:12 - loss: 0.6988 - acc: 0.5376
2112/4849 [============>.................] - ETA: 4:05 - loss: 0.6983 - acc: 0.5369
2176/4849 [============>.................] - ETA: 4:01 - loss: 0.6977 - acc: 0.5372
2240/4849 [============>.................] - ETA: 3:55 - loss: 0.6997 - acc: 0.5339
2304/4849 [=============>................] - ETA: 3:49 - loss: 0.6990 - acc: 0.5365
2368/4849 [=============>................] - ETA: 3:43 - loss: 0.6983 - acc: 0.5384
2432/4849 [==============>...............] - ETA: 3:37 - loss: 0.6966 - acc: 0.5419
2496/4849 [==============>...............] - ETA: 3:32 - loss: 0.6952 - acc: 0.5437
2560/4849 [==============>...............] - ETA: 3:26 - loss: 0.6964 - acc: 0.5414
2624/4849 [===============>..............] - ETA: 3:20 - loss: 0.6970 - acc: 0.5396
2688/4849 [===============>..............] - ETA: 3:14 - loss: 0.6961 - acc: 0.5406
2752/4849 [================>.............] - ETA: 3:08 - loss: 0.6966 - acc: 0.5396
2816/4849 [================>.............] - ETA: 3:02 - loss: 0.6958 - acc: 0.5405
2880/4849 [================>.............] - ETA: 2:56 - loss: 0.6965 - acc: 0.5378
2944/4849 [=================>............] - ETA: 2:51 - loss: 0.6965 - acc: 0.5357
3008/4849 [=================>............] - ETA: 2:45 - loss: 0.6959 - acc: 0.5369
3072/4849 [==================>...........] - ETA: 2:40 - loss: 0.6946 - acc: 0.5394
3136/4849 [==================>...........] - ETA: 2:34 - loss: 0.6947 - acc: 0.5392
3200/4849 [==================>...........] - ETA: 2:28 - loss: 0.6941 - acc: 0.5413
3264/4849 [===================>..........] - ETA: 2:23 - loss: 0.6941 - acc: 0.5411
3328/4849 [===================>..........] - ETA: 2:17 - loss: 0.6943 - acc: 0.5415
3392/4849 [===================>..........] - ETA: 2:11 - loss: 0.6929 - acc: 0.5445
3456/4849 [====================>.........] - ETA: 2:05 - loss: 0.6932 - acc: 0.5434
3520/4849 [====================>.........] - ETA: 1:59 - loss: 0.6925 - acc: 0.5457
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6927 - acc: 0.5452
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.6927 - acc: 0.5452
3712/4849 [=====================>........] - ETA: 1:42 - loss: 0.6933 - acc: 0.5439
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6932 - acc: 0.5445
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6927 - acc: 0.5445
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6929 - acc: 0.5443
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6929 - acc: 0.5436
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6922 - acc: 0.5449
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6921 - acc: 0.5457
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6923 - acc: 0.5464
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6918 - acc: 0.5466 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6917 - acc: 0.5469
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6914 - acc: 0.5483
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6914 - acc: 0.5476
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6906 - acc: 0.5491
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6905 - acc: 0.5489
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6906 - acc: 0.5482
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6906 - acc: 0.5482
4736/4849 [============================>.] - ETA: 10s - loss: 0.6905 - acc: 0.5481
4800/4849 [============================>.] - ETA: 4s - loss: 0.6912 - acc: 0.5477 
4849/4849 [==============================] - 452s 93ms/step - loss: 0.6910 - acc: 0.5471 - val_loss: 0.6793 - val_acc: 0.5770

Epoch 00003: val_acc improved from 0.54360 to 0.57699, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window18/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 4/10

  64/4849 [..............................] - ETA: 6:17 - loss: 0.6533 - acc: 0.6406
 128/4849 [..............................] - ETA: 6:44 - loss: 0.6755 - acc: 0.6094
 192/4849 [>.............................] - ETA: 6:44 - loss: 0.6876 - acc: 0.5833
 256/4849 [>.............................] - ETA: 6:39 - loss: 0.6758 - acc: 0.6094
 320/4849 [>.............................] - ETA: 6:30 - loss: 0.6751 - acc: 0.6062
 384/4849 [=>............................] - ETA: 6:30 - loss: 0.6727 - acc: 0.6094
 448/4849 [=>............................] - ETA: 6:26 - loss: 0.6842 - acc: 0.5938
 512/4849 [==>...........................] - ETA: 6:17 - loss: 0.6843 - acc: 0.5918
 576/4849 [==>...........................] - ETA: 6:15 - loss: 0.6825 - acc: 0.5955
 640/4849 [==>...........................] - ETA: 6:08 - loss: 0.6822 - acc: 0.5922
 704/4849 [===>..........................] - ETA: 6:04 - loss: 0.6805 - acc: 0.5952
 768/4849 [===>..........................] - ETA: 5:57 - loss: 0.6837 - acc: 0.5846
 832/4849 [====>.........................] - ETA: 5:51 - loss: 0.6868 - acc: 0.5781
 896/4849 [====>.........................] - ETA: 5:50 - loss: 0.6875 - acc: 0.5781
 960/4849 [====>.........................] - ETA: 5:43 - loss: 0.6864 - acc: 0.5781
1024/4849 [=====>........................] - ETA: 5:40 - loss: 0.6864 - acc: 0.5791
1088/4849 [=====>........................] - ETA: 5:34 - loss: 0.6885 - acc: 0.5763
1152/4849 [======>.......................] - ETA: 5:28 - loss: 0.6883 - acc: 0.5781
1216/4849 [======>.......................] - ETA: 5:24 - loss: 0.6896 - acc: 0.5757
1280/4849 [======>.......................] - ETA: 5:18 - loss: 0.6888 - acc: 0.5742
1344/4849 [=======>......................] - ETA: 5:11 - loss: 0.6879 - acc: 0.5737
1408/4849 [=======>......................] - ETA: 5:04 - loss: 0.6870 - acc: 0.5732
1472/4849 [========>.....................] - ETA: 4:59 - loss: 0.6861 - acc: 0.5761
1536/4849 [========>.....................] - ETA: 4:52 - loss: 0.6869 - acc: 0.5716
1600/4849 [========>.....................] - ETA: 4:47 - loss: 0.6872 - acc: 0.5681
1664/4849 [=========>....................] - ETA: 4:41 - loss: 0.6869 - acc: 0.5667
1728/4849 [=========>....................] - ETA: 4:35 - loss: 0.6889 - acc: 0.5608
1792/4849 [==========>...................] - ETA: 4:30 - loss: 0.6882 - acc: 0.5625
1856/4849 [==========>...................] - ETA: 4:24 - loss: 0.6885 - acc: 0.5630
1920/4849 [==========>...................] - ETA: 4:18 - loss: 0.6884 - acc: 0.5635
1984/4849 [===========>..................] - ETA: 4:13 - loss: 0.6878 - acc: 0.5655
2048/4849 [===========>..................] - ETA: 4:07 - loss: 0.6865 - acc: 0.5669
2112/4849 [============>.................] - ETA: 4:01 - loss: 0.6865 - acc: 0.5663
2176/4849 [============>.................] - ETA: 3:55 - loss: 0.6856 - acc: 0.5671
2240/4849 [============>.................] - ETA: 3:50 - loss: 0.6858 - acc: 0.5656
2304/4849 [=============>................] - ETA: 3:44 - loss: 0.6849 - acc: 0.5668
2368/4849 [=============>................] - ETA: 3:39 - loss: 0.6851 - acc: 0.5667
2432/4849 [==============>...............] - ETA: 3:33 - loss: 0.6855 - acc: 0.5658
2496/4849 [==============>...............] - ETA: 3:27 - loss: 0.6864 - acc: 0.5649
2560/4849 [==============>...............] - ETA: 3:21 - loss: 0.6866 - acc: 0.5652
2624/4849 [===============>..............] - ETA: 3:16 - loss: 0.6867 - acc: 0.5652
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.6867 - acc: 0.5655
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.6860 - acc: 0.5669
2816/4849 [================>.............] - ETA: 2:58 - loss: 0.6863 - acc: 0.5668
2880/4849 [================>.............] - ETA: 2:52 - loss: 0.6863 - acc: 0.5656
2944/4849 [=================>............] - ETA: 2:47 - loss: 0.6861 - acc: 0.5645
3008/4849 [=================>............] - ETA: 2:41 - loss: 0.6861 - acc: 0.5652
3072/4849 [==================>...........] - ETA: 2:35 - loss: 0.6851 - acc: 0.5674
3136/4849 [==================>...........] - ETA: 2:30 - loss: 0.6859 - acc: 0.5657
3200/4849 [==================>...........] - ETA: 2:24 - loss: 0.6858 - acc: 0.5663
3264/4849 [===================>..........] - ETA: 2:18 - loss: 0.6855 - acc: 0.5680
3328/4849 [===================>..........] - ETA: 2:12 - loss: 0.6854 - acc: 0.5667
3392/4849 [===================>..........] - ETA: 2:07 - loss: 0.6856 - acc: 0.5666
3456/4849 [====================>.........] - ETA: 2:02 - loss: 0.6866 - acc: 0.5648
3520/4849 [====================>.........] - ETA: 1:56 - loss: 0.6869 - acc: 0.5653
3584/4849 [=====================>........] - ETA: 1:50 - loss: 0.6869 - acc: 0.5653
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6868 - acc: 0.5661
3712/4849 [=====================>........] - ETA: 1:39 - loss: 0.6859 - acc: 0.5671
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6869 - acc: 0.5651
3840/4849 [======================>.......] - ETA: 1:28 - loss: 0.6864 - acc: 0.5648
3904/4849 [=======================>......] - ETA: 1:22 - loss: 0.6866 - acc: 0.5643
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6862 - acc: 0.5643
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6857 - acc: 0.5647
4096/4849 [========================>.....] - ETA: 1:05 - loss: 0.6860 - acc: 0.5645
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6859 - acc: 0.5649
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6860 - acc: 0.5644 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6863 - acc: 0.5641
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6859 - acc: 0.5643
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6853 - acc: 0.5648
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6857 - acc: 0.5645
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6859 - acc: 0.5640
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6856 - acc: 0.5647
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6863 - acc: 0.5634
4736/4849 [============================>.] - ETA: 9s - loss: 0.6869 - acc: 0.5621 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6879 - acc: 0.5602
4849/4849 [==============================] - 441s 91ms/step - loss: 0.6883 - acc: 0.5597 - val_loss: 0.6806 - val_acc: 0.5622

Epoch 00004: val_acc did not improve from 0.57699
Epoch 5/10

  64/4849 [..............................] - ETA: 7:09 - loss: 0.7037 - acc: 0.4688
 128/4849 [..............................] - ETA: 6:39 - loss: 0.6969 - acc: 0.5000
 192/4849 [>.............................] - ETA: 6:28 - loss: 0.6958 - acc: 0.5052
 256/4849 [>.............................] - ETA: 6:23 - loss: 0.6949 - acc: 0.5117
 320/4849 [>.............................] - ETA: 6:28 - loss: 0.6931 - acc: 0.5281
 384/4849 [=>............................] - ETA: 6:20 - loss: 0.6955 - acc: 0.5260
 448/4849 [=>............................] - ETA: 6:17 - loss: 0.6924 - acc: 0.5246
 512/4849 [==>...........................] - ETA: 6:15 - loss: 0.6915 - acc: 0.5293
 576/4849 [==>...........................] - ETA: 6:07 - loss: 0.6956 - acc: 0.5243
 640/4849 [==>...........................] - ETA: 6:04 - loss: 0.6945 - acc: 0.5266
 704/4849 [===>..........................] - ETA: 5:57 - loss: 0.6937 - acc: 0.5284
 768/4849 [===>..........................] - ETA: 5:52 - loss: 0.6921 - acc: 0.5339
 832/4849 [====>.........................] - ETA: 5:46 - loss: 0.6904 - acc: 0.5397
 896/4849 [====>.........................] - ETA: 5:39 - loss: 0.6904 - acc: 0.5391
 960/4849 [====>.........................] - ETA: 5:32 - loss: 0.6901 - acc: 0.5427
1024/4849 [=====>........................] - ETA: 5:28 - loss: 0.6896 - acc: 0.5391
1088/4849 [=====>........................] - ETA: 5:21 - loss: 0.6878 - acc: 0.5404
1152/4849 [======>.......................] - ETA: 5:15 - loss: 0.6885 - acc: 0.5373
1216/4849 [======>.......................] - ETA: 5:10 - loss: 0.6873 - acc: 0.5370
1280/4849 [======>.......................] - ETA: 5:06 - loss: 0.6860 - acc: 0.5406
1344/4849 [=======>......................] - ETA: 5:01 - loss: 0.6878 - acc: 0.5357
1408/4849 [=======>......................] - ETA: 4:56 - loss: 0.6882 - acc: 0.5362
1472/4849 [========>.....................] - ETA: 4:51 - loss: 0.6879 - acc: 0.5394
1536/4849 [========>.....................] - ETA: 4:45 - loss: 0.6884 - acc: 0.5358
1600/4849 [========>.....................] - ETA: 4:40 - loss: 0.6896 - acc: 0.5331
1664/4849 [=========>....................] - ETA: 4:34 - loss: 0.6905 - acc: 0.5294
1728/4849 [=========>....................] - ETA: 4:29 - loss: 0.6903 - acc: 0.5301
1792/4849 [==========>...................] - ETA: 4:24 - loss: 0.6902 - acc: 0.5290
1856/4849 [==========>...................] - ETA: 4:18 - loss: 0.6902 - acc: 0.5280
1920/4849 [==========>...................] - ETA: 4:13 - loss: 0.6898 - acc: 0.5307
1984/4849 [===========>..................] - ETA: 4:08 - loss: 0.6904 - acc: 0.5302
2048/4849 [===========>..................] - ETA: 4:03 - loss: 0.6907 - acc: 0.5308
2112/4849 [============>.................] - ETA: 3:57 - loss: 0.6907 - acc: 0.5303
2176/4849 [============>.................] - ETA: 3:51 - loss: 0.6916 - acc: 0.5257
2240/4849 [============>.................] - ETA: 3:46 - loss: 0.6912 - acc: 0.5281
2304/4849 [=============>................] - ETA: 3:40 - loss: 0.6906 - acc: 0.5308
2368/4849 [=============>................] - ETA: 3:35 - loss: 0.6906 - acc: 0.5304
2432/4849 [==============>...............] - ETA: 3:29 - loss: 0.6905 - acc: 0.5317
2496/4849 [==============>...............] - ETA: 3:23 - loss: 0.6899 - acc: 0.5365
2560/4849 [==============>...............] - ETA: 3:18 - loss: 0.6889 - acc: 0.5402
2624/4849 [===============>..............] - ETA: 3:12 - loss: 0.6878 - acc: 0.5434
2688/4849 [===============>..............] - ETA: 3:06 - loss: 0.6867 - acc: 0.5476
2752/4849 [================>.............] - ETA: 3:01 - loss: 0.6855 - acc: 0.5498
2816/4849 [================>.............] - ETA: 2:56 - loss: 0.6847 - acc: 0.5511
2880/4849 [================>.............] - ETA: 2:50 - loss: 0.6832 - acc: 0.5549
2944/4849 [=================>............] - ETA: 2:45 - loss: 0.6842 - acc: 0.5526
3008/4849 [=================>............] - ETA: 2:39 - loss: 0.6838 - acc: 0.5529
3072/4849 [==================>...........] - ETA: 2:34 - loss: 0.6833 - acc: 0.5540
3136/4849 [==================>...........] - ETA: 2:28 - loss: 0.6833 - acc: 0.5536
3200/4849 [==================>...........] - ETA: 2:22 - loss: 0.6848 - acc: 0.5509
3264/4849 [===================>..........] - ETA: 2:17 - loss: 0.6853 - acc: 0.5512
3328/4849 [===================>..........] - ETA: 2:12 - loss: 0.6853 - acc: 0.5508
3392/4849 [===================>..........] - ETA: 2:06 - loss: 0.6855 - acc: 0.5498
3456/4849 [====================>.........] - ETA: 2:01 - loss: 0.6853 - acc: 0.5509
3520/4849 [====================>.........] - ETA: 1:55 - loss: 0.6855 - acc: 0.5509
3584/4849 [=====================>........] - ETA: 1:49 - loss: 0.6848 - acc: 0.5522
3648/4849 [=====================>........] - ETA: 1:44 - loss: 0.6847 - acc: 0.5524
3712/4849 [=====================>........] - ETA: 1:38 - loss: 0.6840 - acc: 0.5533
3776/4849 [======================>.......] - ETA: 1:33 - loss: 0.6842 - acc: 0.5535
3840/4849 [======================>.......] - ETA: 1:27 - loss: 0.6843 - acc: 0.5539
3904/4849 [=======================>......] - ETA: 1:21 - loss: 0.6852 - acc: 0.5517
3968/4849 [=======================>......] - ETA: 1:16 - loss: 0.6855 - acc: 0.5507
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6861 - acc: 0.5503
4096/4849 [========================>.....] - ETA: 1:05 - loss: 0.6853 - acc: 0.5518
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6852 - acc: 0.5529 
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6851 - acc: 0.5533
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6855 - acc: 0.5513
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6852 - acc: 0.5512
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6849 - acc: 0.5514
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6844 - acc: 0.5531
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6839 - acc: 0.5550
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6843 - acc: 0.5532
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6842 - acc: 0.5522
4736/4849 [============================>.] - ETA: 9s - loss: 0.6840 - acc: 0.5528 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6838 - acc: 0.5523
4849/4849 [==============================] - 440s 91ms/step - loss: 0.6837 - acc: 0.5527 - val_loss: 0.6747 - val_acc: 0.5918

Epoch 00005: val_acc improved from 0.57699 to 0.59184, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window18/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 6/10

  64/4849 [..............................] - ETA: 7:19 - loss: 0.7543 - acc: 0.4062
 128/4849 [..............................] - ETA: 6:56 - loss: 0.6995 - acc: 0.5312
 192/4849 [>.............................] - ETA: 6:58 - loss: 0.7031 - acc: 0.5208
 256/4849 [>.............................] - ETA: 6:51 - loss: 0.6982 - acc: 0.5352
 320/4849 [>.............................] - ETA: 6:40 - loss: 0.6991 - acc: 0.5219
 384/4849 [=>............................] - ETA: 6:39 - loss: 0.6928 - acc: 0.5391
 448/4849 [=>............................] - ETA: 6:32 - loss: 0.6878 - acc: 0.5469
 512/4849 [==>...........................] - ETA: 6:32 - loss: 0.6852 - acc: 0.5527
 576/4849 [==>...........................] - ETA: 6:28 - loss: 0.6820 - acc: 0.5573
 640/4849 [==>...........................] - ETA: 6:21 - loss: 0.6828 - acc: 0.5594
 704/4849 [===>..........................] - ETA: 6:16 - loss: 0.6810 - acc: 0.5625
 768/4849 [===>..........................] - ETA: 6:07 - loss: 0.6859 - acc: 0.5547
 832/4849 [====>.........................] - ETA: 6:02 - loss: 0.6848 - acc: 0.5565
 896/4849 [====>.........................] - ETA: 5:53 - loss: 0.6863 - acc: 0.5547
 960/4849 [====>.........................] - ETA: 5:46 - loss: 0.6867 - acc: 0.5531
1024/4849 [=====>........................] - ETA: 5:39 - loss: 0.6856 - acc: 0.5547
1088/4849 [=====>........................] - ETA: 5:30 - loss: 0.6859 - acc: 0.5515
1152/4849 [======>.......................] - ETA: 5:22 - loss: 0.6866 - acc: 0.5495
1216/4849 [======>.......................] - ETA: 5:14 - loss: 0.6834 - acc: 0.5526
1280/4849 [======>.......................] - ETA: 5:05 - loss: 0.6821 - acc: 0.5555
1344/4849 [=======>......................] - ETA: 4:57 - loss: 0.6807 - acc: 0.5603
1408/4849 [=======>......................] - ETA: 4:49 - loss: 0.6814 - acc: 0.5625
1472/4849 [========>.....................] - ETA: 4:42 - loss: 0.6823 - acc: 0.5618
1536/4849 [========>.....................] - ETA: 4:36 - loss: 0.6838 - acc: 0.5592
1600/4849 [========>.....................] - ETA: 4:30 - loss: 0.6837 - acc: 0.5581
1664/4849 [=========>....................] - ETA: 4:23 - loss: 0.6840 - acc: 0.5589
1728/4849 [=========>....................] - ETA: 4:17 - loss: 0.6839 - acc: 0.5584
1792/4849 [==========>...................] - ETA: 4:11 - loss: 0.6839 - acc: 0.5564
1856/4849 [==========>...................] - ETA: 4:06 - loss: 0.6849 - acc: 0.5517
1920/4849 [==========>...................] - ETA: 4:00 - loss: 0.6841 - acc: 0.5552
1984/4849 [===========>..................] - ETA: 3:55 - loss: 0.6833 - acc: 0.5554
2048/4849 [===========>..................] - ETA: 3:49 - loss: 0.6836 - acc: 0.5542
2112/4849 [============>.................] - ETA: 3:43 - loss: 0.6820 - acc: 0.5582
2176/4849 [============>.................] - ETA: 3:38 - loss: 0.6812 - acc: 0.5611
2240/4849 [============>.................] - ETA: 3:32 - loss: 0.6825 - acc: 0.5576
2304/4849 [=============>................] - ETA: 3:26 - loss: 0.6820 - acc: 0.5586
2368/4849 [=============>................] - ETA: 3:21 - loss: 0.6814 - acc: 0.5608
2432/4849 [==============>...............] - ETA: 3:16 - loss: 0.6808 - acc: 0.5629
2496/4849 [==============>...............] - ETA: 3:11 - loss: 0.6813 - acc: 0.5633
2560/4849 [==============>...............] - ETA: 3:05 - loss: 0.6824 - acc: 0.5629
2624/4849 [===============>..............] - ETA: 3:00 - loss: 0.6816 - acc: 0.5633
2688/4849 [===============>..............] - ETA: 2:54 - loss: 0.6815 - acc: 0.5636
2752/4849 [================>.............] - ETA: 2:49 - loss: 0.6809 - acc: 0.5643
2816/4849 [================>.............] - ETA: 2:44 - loss: 0.6819 - acc: 0.5625
2880/4849 [================>.............] - ETA: 2:39 - loss: 0.6813 - acc: 0.5642
2944/4849 [=================>............] - ETA: 2:34 - loss: 0.6813 - acc: 0.5645
3008/4849 [=================>............] - ETA: 2:29 - loss: 0.6805 - acc: 0.5658
3072/4849 [==================>...........] - ETA: 2:23 - loss: 0.6807 - acc: 0.5654
3136/4849 [==================>...........] - ETA: 2:18 - loss: 0.6812 - acc: 0.5641
3200/4849 [==================>...........] - ETA: 2:13 - loss: 0.6818 - acc: 0.5634
3264/4849 [===================>..........] - ETA: 2:08 - loss: 0.6821 - acc: 0.5634
3328/4849 [===================>..........] - ETA: 2:02 - loss: 0.6820 - acc: 0.5634
3392/4849 [===================>..........] - ETA: 1:57 - loss: 0.6820 - acc: 0.5637
3456/4849 [====================>.........] - ETA: 1:52 - loss: 0.6819 - acc: 0.5634
3520/4849 [====================>.........] - ETA: 1:47 - loss: 0.6817 - acc: 0.5636
3584/4849 [=====================>........] - ETA: 1:42 - loss: 0.6813 - acc: 0.5636
3648/4849 [=====================>........] - ETA: 1:36 - loss: 0.6812 - acc: 0.5650
3712/4849 [=====================>........] - ETA: 1:31 - loss: 0.6818 - acc: 0.5633
3776/4849 [======================>.......] - ETA: 1:26 - loss: 0.6812 - acc: 0.5659
3840/4849 [======================>.......] - ETA: 1:21 - loss: 0.6815 - acc: 0.5646
3904/4849 [=======================>......] - ETA: 1:16 - loss: 0.6821 - acc: 0.5625
3968/4849 [=======================>......] - ETA: 1:11 - loss: 0.6820 - acc: 0.5628
4032/4849 [=======================>......] - ETA: 1:06 - loss: 0.6822 - acc: 0.5627
4096/4849 [========================>.....] - ETA: 1:01 - loss: 0.6818 - acc: 0.5635
4160/4849 [========================>.....] - ETA: 55s - loss: 0.6819 - acc: 0.5632 
4224/4849 [=========================>....] - ETA: 50s - loss: 0.6812 - acc: 0.5649
4288/4849 [=========================>....] - ETA: 45s - loss: 0.6807 - acc: 0.5662
4352/4849 [=========================>....] - ETA: 40s - loss: 0.6806 - acc: 0.5664
4416/4849 [==========================>...] - ETA: 35s - loss: 0.6806 - acc: 0.5659
4480/4849 [==========================>...] - ETA: 29s - loss: 0.6807 - acc: 0.5661
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6816 - acc: 0.5645
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6811 - acc: 0.5658
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6810 - acc: 0.5661
4736/4849 [============================>.] - ETA: 9s - loss: 0.6800 - acc: 0.5684 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6800 - acc: 0.5681
4849/4849 [==============================] - 408s 84ms/step - loss: 0.6799 - acc: 0.5682 - val_loss: 0.6805 - val_acc: 0.5770

Epoch 00006: val_acc did not improve from 0.59184
Epoch 7/10

  64/4849 [..............................] - ETA: 6:48 - loss: 0.6740 - acc: 0.5781
 128/4849 [..............................] - ETA: 6:24 - loss: 0.6714 - acc: 0.5703
 192/4849 [>.............................] - ETA: 6:31 - loss: 0.6810 - acc: 0.5781
 256/4849 [>.............................] - ETA: 6:28 - loss: 0.6755 - acc: 0.5859
 320/4849 [>.............................] - ETA: 6:09 - loss: 0.6730 - acc: 0.5781
 384/4849 [=>............................] - ETA: 6:07 - loss: 0.6694 - acc: 0.5807
 448/4849 [=>............................] - ETA: 6:05 - loss: 0.6717 - acc: 0.5714
 512/4849 [==>...........................] - ETA: 6:03 - loss: 0.6657 - acc: 0.5840
 576/4849 [==>...........................] - ETA: 5:51 - loss: 0.6653 - acc: 0.5781
 640/4849 [==>...........................] - ETA: 5:46 - loss: 0.6666 - acc: 0.5813
 704/4849 [===>..........................] - ETA: 5:35 - loss: 0.6704 - acc: 0.5795
 768/4849 [===>..........................] - ETA: 5:31 - loss: 0.6734 - acc: 0.5755
 832/4849 [====>.........................] - ETA: 5:28 - loss: 0.6735 - acc: 0.5781
 896/4849 [====>.........................] - ETA: 5:23 - loss: 0.6735 - acc: 0.5770
 960/4849 [====>.........................] - ETA: 5:16 - loss: 0.6773 - acc: 0.5719
1024/4849 [=====>........................] - ETA: 5:12 - loss: 0.6785 - acc: 0.5713
1088/4849 [=====>........................] - ETA: 5:06 - loss: 0.6780 - acc: 0.5708
1152/4849 [======>.......................] - ETA: 4:59 - loss: 0.6790 - acc: 0.5668
1216/4849 [======>.......................] - ETA: 4:51 - loss: 0.6807 - acc: 0.5650
1280/4849 [======>.......................] - ETA: 4:48 - loss: 0.6812 - acc: 0.5672
1344/4849 [=======>......................] - ETA: 4:43 - loss: 0.6804 - acc: 0.5699
1408/4849 [=======>......................] - ETA: 4:38 - loss: 0.6785 - acc: 0.5746
1472/4849 [========>.....................] - ETA: 4:34 - loss: 0.6774 - acc: 0.5781
1536/4849 [========>.....................] - ETA: 4:28 - loss: 0.6779 - acc: 0.5768
1600/4849 [========>.....................] - ETA: 4:22 - loss: 0.6769 - acc: 0.5787
1664/4849 [=========>....................] - ETA: 4:18 - loss: 0.6775 - acc: 0.5769
1728/4849 [=========>....................] - ETA: 4:12 - loss: 0.6764 - acc: 0.5770
1792/4849 [==========>...................] - ETA: 4:08 - loss: 0.6763 - acc: 0.5742
1856/4849 [==========>...................] - ETA: 4:03 - loss: 0.6774 - acc: 0.5749
1920/4849 [==========>...................] - ETA: 3:57 - loss: 0.6791 - acc: 0.5740
1984/4849 [===========>..................] - ETA: 3:52 - loss: 0.6805 - acc: 0.5731
2048/4849 [===========>..................] - ETA: 3:47 - loss: 0.6819 - acc: 0.5708
2112/4849 [============>.................] - ETA: 3:42 - loss: 0.6821 - acc: 0.5701
2176/4849 [============>.................] - ETA: 3:37 - loss: 0.6820 - acc: 0.5712
2240/4849 [============>.................] - ETA: 3:32 - loss: 0.6811 - acc: 0.5728
2304/4849 [=============>................] - ETA: 3:26 - loss: 0.6809 - acc: 0.5712
2368/4849 [=============>................] - ETA: 3:20 - loss: 0.6809 - acc: 0.5709
2432/4849 [==============>...............] - ETA: 3:15 - loss: 0.6816 - acc: 0.5691
2496/4849 [==============>...............] - ETA: 3:10 - loss: 0.6813 - acc: 0.5701
2560/4849 [==============>...............] - ETA: 3:04 - loss: 0.6813 - acc: 0.5699
2624/4849 [===============>..............] - ETA: 2:59 - loss: 0.6810 - acc: 0.5724
2688/4849 [===============>..............] - ETA: 2:54 - loss: 0.6807 - acc: 0.5733
2752/4849 [================>.............] - ETA: 2:48 - loss: 0.6815 - acc: 0.5701
2816/4849 [================>.............] - ETA: 2:43 - loss: 0.6804 - acc: 0.5714
2880/4849 [================>.............] - ETA: 2:38 - loss: 0.6799 - acc: 0.5708
2944/4849 [=================>............] - ETA: 2:33 - loss: 0.6795 - acc: 0.5724
3008/4849 [=================>............] - ETA: 2:27 - loss: 0.6796 - acc: 0.5721
3072/4849 [==================>...........] - ETA: 2:22 - loss: 0.6790 - acc: 0.5739
3136/4849 [==================>...........] - ETA: 2:17 - loss: 0.6776 - acc: 0.5753
3200/4849 [==================>...........] - ETA: 2:12 - loss: 0.6771 - acc: 0.5756
3264/4849 [===================>..........] - ETA: 2:07 - loss: 0.6766 - acc: 0.5769
3328/4849 [===================>..........] - ETA: 2:02 - loss: 0.6771 - acc: 0.5757
3392/4849 [===================>..........] - ETA: 1:57 - loss: 0.6770 - acc: 0.5764
3456/4849 [====================>.........] - ETA: 1:51 - loss: 0.6773 - acc: 0.5758
3520/4849 [====================>.........] - ETA: 1:46 - loss: 0.6765 - acc: 0.5773
3584/4849 [=====================>........] - ETA: 1:41 - loss: 0.6757 - acc: 0.5784
3648/4849 [=====================>........] - ETA: 1:36 - loss: 0.6762 - acc: 0.5776
3712/4849 [=====================>........] - ETA: 1:31 - loss: 0.6768 - acc: 0.5762
3776/4849 [======================>.......] - ETA: 1:26 - loss: 0.6765 - acc: 0.5773
3840/4849 [======================>.......] - ETA: 1:20 - loss: 0.6759 - acc: 0.5789
3904/4849 [=======================>......] - ETA: 1:15 - loss: 0.6755 - acc: 0.5802
3968/4849 [=======================>......] - ETA: 1:10 - loss: 0.6756 - acc: 0.5804
4032/4849 [=======================>......] - ETA: 1:05 - loss: 0.6755 - acc: 0.5811
4096/4849 [========================>.....] - ETA: 1:00 - loss: 0.6762 - acc: 0.5798
4160/4849 [========================>.....] - ETA: 55s - loss: 0.6760 - acc: 0.5798 
4224/4849 [=========================>....] - ETA: 50s - loss: 0.6758 - acc: 0.5798
4288/4849 [=========================>....] - ETA: 45s - loss: 0.6760 - acc: 0.5793
4352/4849 [=========================>....] - ETA: 39s - loss: 0.6763 - acc: 0.5790
4416/4849 [==========================>...] - ETA: 34s - loss: 0.6759 - acc: 0.5793
4480/4849 [==========================>...] - ETA: 29s - loss: 0.6753 - acc: 0.5799
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6751 - acc: 0.5799
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6754 - acc: 0.5794
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6762 - acc: 0.5781
4736/4849 [============================>.] - ETA: 9s - loss: 0.6762 - acc: 0.5781 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6762 - acc: 0.5771
4849/4849 [==============================] - 404s 83ms/step - loss: 0.6772 - acc: 0.5754 - val_loss: 0.6839 - val_acc: 0.5659

Epoch 00007: val_acc did not improve from 0.59184
Epoch 8/10

  64/4849 [..............................] - ETA: 6:20 - loss: 0.6806 - acc: 0.6250
 128/4849 [..............................] - ETA: 6:18 - loss: 0.6660 - acc: 0.6094
 192/4849 [>.............................] - ETA: 6:08 - loss: 0.6678 - acc: 0.6042
 256/4849 [>.............................] - ETA: 5:59 - loss: 0.6643 - acc: 0.6055
 320/4849 [>.............................] - ETA: 6:03 - loss: 0.6737 - acc: 0.5875
 384/4849 [=>............................] - ETA: 5:55 - loss: 0.6818 - acc: 0.5703
 448/4849 [=>............................] - ETA: 5:54 - loss: 0.6820 - acc: 0.5670
 512/4849 [==>...........................] - ETA: 5:46 - loss: 0.6818 - acc: 0.5723
 576/4849 [==>...........................] - ETA: 5:42 - loss: 0.6806 - acc: 0.5712
 640/4849 [==>...........................] - ETA: 5:35 - loss: 0.6830 - acc: 0.5703
 704/4849 [===>..........................] - ETA: 5:36 - loss: 0.6811 - acc: 0.5739
 768/4849 [===>..........................] - ETA: 5:27 - loss: 0.6814 - acc: 0.5729
 832/4849 [====>.........................] - ETA: 5:26 - loss: 0.6807 - acc: 0.5781
 896/4849 [====>.........................] - ETA: 5:21 - loss: 0.6801 - acc: 0.5792
 960/4849 [====>.........................] - ETA: 5:16 - loss: 0.6774 - acc: 0.5802
1024/4849 [=====>........................] - ETA: 5:12 - loss: 0.6762 - acc: 0.5820
1088/4849 [=====>........................] - ETA: 5:05 - loss: 0.6752 - acc: 0.5827
1152/4849 [======>.......................] - ETA: 5:00 - loss: 0.6756 - acc: 0.5807
1216/4849 [======>.......................] - ETA: 4:53 - loss: 0.6733 - acc: 0.5839
1280/4849 [======>.......................] - ETA: 4:48 - loss: 0.6733 - acc: 0.5844
1344/4849 [=======>......................] - ETA: 4:45 - loss: 0.6731 - acc: 0.5871
1408/4849 [=======>......................] - ETA: 4:40 - loss: 0.6724 - acc: 0.5866
1472/4849 [========>.....................] - ETA: 4:34 - loss: 0.6717 - acc: 0.5883
1536/4849 [========>.....................] - ETA: 4:29 - loss: 0.6700 - acc: 0.5911
1600/4849 [========>.....................] - ETA: 4:23 - loss: 0.6715 - acc: 0.5887
1664/4849 [=========>....................] - ETA: 4:18 - loss: 0.6727 - acc: 0.5871
1728/4849 [=========>....................] - ETA: 4:12 - loss: 0.6727 - acc: 0.5885
1792/4849 [==========>...................] - ETA: 4:07 - loss: 0.6726 - acc: 0.5876
1856/4849 [==========>...................] - ETA: 4:00 - loss: 0.6714 - acc: 0.5889
1920/4849 [==========>...................] - ETA: 3:56 - loss: 0.6720 - acc: 0.5901
1984/4849 [===========>..................] - ETA: 3:49 - loss: 0.6704 - acc: 0.5932
2048/4849 [===========>..................] - ETA: 3:44 - loss: 0.6695 - acc: 0.5957
2112/4849 [============>.................] - ETA: 3:39 - loss: 0.6683 - acc: 0.5975
2176/4849 [============>.................] - ETA: 3:34 - loss: 0.6687 - acc: 0.5983
2240/4849 [============>.................] - ETA: 3:28 - loss: 0.6671 - acc: 0.6018
2304/4849 [=============>................] - ETA: 3:23 - loss: 0.6682 - acc: 0.6003
2368/4849 [=============>................] - ETA: 3:17 - loss: 0.6686 - acc: 0.6001
2432/4849 [==============>...............] - ETA: 3:12 - loss: 0.6700 - acc: 0.5987
2496/4849 [==============>...............] - ETA: 3:08 - loss: 0.6706 - acc: 0.5978
2560/4849 [==============>...............] - ETA: 3:03 - loss: 0.6701 - acc: 0.5992
2624/4849 [===============>..............] - ETA: 2:57 - loss: 0.6707 - acc: 0.5987
2688/4849 [===============>..............] - ETA: 2:53 - loss: 0.6700 - acc: 0.6004
2752/4849 [================>.............] - ETA: 2:47 - loss: 0.6725 - acc: 0.5967
2816/4849 [================>.............] - ETA: 2:42 - loss: 0.6730 - acc: 0.5962
2880/4849 [================>.............] - ETA: 2:37 - loss: 0.6731 - acc: 0.5962
2944/4849 [=================>............] - ETA: 2:32 - loss: 0.6736 - acc: 0.5958
3008/4849 [=================>............] - ETA: 2:27 - loss: 0.6740 - acc: 0.5944
3072/4849 [==================>...........] - ETA: 2:22 - loss: 0.6737 - acc: 0.5928
3136/4849 [==================>...........] - ETA: 2:17 - loss: 0.6741 - acc: 0.5922
3200/4849 [==================>...........] - ETA: 2:11 - loss: 0.6739 - acc: 0.5913
3264/4849 [===================>..........] - ETA: 2:06 - loss: 0.6730 - acc: 0.5925
3328/4849 [===================>..........] - ETA: 2:01 - loss: 0.6727 - acc: 0.5931
3392/4849 [===================>..........] - ETA: 1:56 - loss: 0.6735 - acc: 0.5914
3456/4849 [====================>.........] - ETA: 1:51 - loss: 0.6733 - acc: 0.5917
3520/4849 [====================>.........] - ETA: 1:46 - loss: 0.6732 - acc: 0.5920
3584/4849 [=====================>........] - ETA: 1:41 - loss: 0.6736 - acc: 0.5924
3648/4849 [=====================>........] - ETA: 1:36 - loss: 0.6734 - acc: 0.5929
3712/4849 [=====================>........] - ETA: 1:31 - loss: 0.6734 - acc: 0.5919
3776/4849 [======================>.......] - ETA: 1:25 - loss: 0.6736 - acc: 0.5911
3840/4849 [======================>.......] - ETA: 1:20 - loss: 0.6739 - acc: 0.5901
3904/4849 [=======================>......] - ETA: 1:15 - loss: 0.6737 - acc: 0.5902
3968/4849 [=======================>......] - ETA: 1:10 - loss: 0.6737 - acc: 0.5890
4032/4849 [=======================>......] - ETA: 1:05 - loss: 0.6739 - acc: 0.5885
4096/4849 [========================>.....] - ETA: 1:00 - loss: 0.6744 - acc: 0.5869
4160/4849 [========================>.....] - ETA: 55s - loss: 0.6743 - acc: 0.5865 
4224/4849 [=========================>....] - ETA: 49s - loss: 0.6745 - acc: 0.5857
4288/4849 [=========================>....] - ETA: 44s - loss: 0.6743 - acc: 0.5863
4352/4849 [=========================>....] - ETA: 39s - loss: 0.6746 - acc: 0.5862
4416/4849 [==========================>...] - ETA: 34s - loss: 0.6743 - acc: 0.5867
4480/4849 [==========================>...] - ETA: 29s - loss: 0.6742 - acc: 0.5875
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6747 - acc: 0.5858
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6740 - acc: 0.5872
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6740 - acc: 0.5869
4736/4849 [============================>.] - ETA: 9s - loss: 0.6746 - acc: 0.5859 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6751 - acc: 0.5852
4849/4849 [==============================] - 403s 83ms/step - loss: 0.6747 - acc: 0.5863 - val_loss: 0.6753 - val_acc: 0.5826

Epoch 00008: val_acc did not improve from 0.59184
Epoch 9/10

  64/4849 [..............................] - ETA: 6:16 - loss: 0.6244 - acc: 0.6875
 128/4849 [..............................] - ETA: 6:19 - loss: 0.6483 - acc: 0.6406
 192/4849 [>.............................] - ETA: 6:22 - loss: 0.6409 - acc: 0.6615
 256/4849 [>.............................] - ETA: 6:11 - loss: 0.6432 - acc: 0.6367
 320/4849 [>.............................] - ETA: 6:02 - loss: 0.6504 - acc: 0.6312
 384/4849 [=>............................] - ETA: 5:58 - loss: 0.6515 - acc: 0.6302
 448/4849 [=>............................] - ETA: 5:50 - loss: 0.6574 - acc: 0.6228
 512/4849 [==>...........................] - ETA: 5:47 - loss: 0.6606 - acc: 0.6055
 576/4849 [==>...........................] - ETA: 5:42 - loss: 0.6618 - acc: 0.6042
 640/4849 [==>...........................] - ETA: 5:35 - loss: 0.6620 - acc: 0.6062
 704/4849 [===>..........................] - ETA: 5:28 - loss: 0.6641 - acc: 0.6051
 768/4849 [===>..........................] - ETA: 5:24 - loss: 0.6678 - acc: 0.5964
 832/4849 [====>.........................] - ETA: 5:16 - loss: 0.6701 - acc: 0.5913
 896/4849 [====>.........................] - ETA: 5:13 - loss: 0.6707 - acc: 0.5882
 960/4849 [====>.........................] - ETA: 5:08 - loss: 0.6718 - acc: 0.5854
1024/4849 [=====>........................] - ETA: 5:03 - loss: 0.6709 - acc: 0.5840
1088/4849 [=====>........................] - ETA: 4:58 - loss: 0.6688 - acc: 0.5873
1152/4849 [======>.......................] - ETA: 4:51 - loss: 0.6697 - acc: 0.5877
1216/4849 [======>.......................] - ETA: 4:46 - loss: 0.6665 - acc: 0.5946
1280/4849 [======>.......................] - ETA: 4:41 - loss: 0.6701 - acc: 0.5891
1344/4849 [=======>......................] - ETA: 4:36 - loss: 0.6721 - acc: 0.5863
1408/4849 [=======>......................] - ETA: 4:32 - loss: 0.6728 - acc: 0.5838
1472/4849 [========>.....................] - ETA: 4:26 - loss: 0.6732 - acc: 0.5836
1536/4849 [========>.....................] - ETA: 4:21 - loss: 0.6740 - acc: 0.5827
1600/4849 [========>.....................] - ETA: 4:16 - loss: 0.6736 - acc: 0.5844
1664/4849 [=========>....................] - ETA: 4:11 - loss: 0.6735 - acc: 0.5841
1728/4849 [=========>....................] - ETA: 4:06 - loss: 0.6737 - acc: 0.5839
1792/4849 [==========>...................] - ETA: 4:02 - loss: 0.6732 - acc: 0.5871
1856/4849 [==========>...................] - ETA: 3:58 - loss: 0.6739 - acc: 0.5857
1920/4849 [==========>...................] - ETA: 3:52 - loss: 0.6745 - acc: 0.5844
1984/4849 [===========>..................] - ETA: 3:48 - loss: 0.6765 - acc: 0.5832
2048/4849 [===========>..................] - ETA: 3:42 - loss: 0.6768 - acc: 0.5825
2112/4849 [============>.................] - ETA: 3:38 - loss: 0.6766 - acc: 0.5857
2176/4849 [============>.................] - ETA: 3:33 - loss: 0.6777 - acc: 0.5850
2240/4849 [============>.................] - ETA: 3:28 - loss: 0.6783 - acc: 0.5830
2304/4849 [=============>................] - ETA: 3:23 - loss: 0.6773 - acc: 0.5855
2368/4849 [=============>................] - ETA: 3:18 - loss: 0.6773 - acc: 0.5840
2432/4849 [==============>...............] - ETA: 3:13 - loss: 0.6766 - acc: 0.5843
2496/4849 [==============>...............] - ETA: 3:08 - loss: 0.6764 - acc: 0.5837
2560/4849 [==============>...............] - ETA: 3:03 - loss: 0.6761 - acc: 0.5844
2624/4849 [===============>..............] - ETA: 2:58 - loss: 0.6766 - acc: 0.5838
2688/4849 [===============>..............] - ETA: 2:52 - loss: 0.6769 - acc: 0.5830
2752/4849 [================>.............] - ETA: 2:47 - loss: 0.6769 - acc: 0.5821
2816/4849 [================>.............] - ETA: 2:42 - loss: 0.6769 - acc: 0.5803
2880/4849 [================>.............] - ETA: 2:37 - loss: 0.6772 - acc: 0.5799
2944/4849 [=================>............] - ETA: 2:32 - loss: 0.6765 - acc: 0.5822
3008/4849 [=================>............] - ETA: 2:27 - loss: 0.6758 - acc: 0.5844
3072/4849 [==================>...........] - ETA: 2:22 - loss: 0.6775 - acc: 0.5807
3136/4849 [==================>...........] - ETA: 2:17 - loss: 0.6761 - acc: 0.5820
3200/4849 [==================>...........] - ETA: 2:12 - loss: 0.6772 - acc: 0.5806
3264/4849 [===================>..........] - ETA: 2:07 - loss: 0.6766 - acc: 0.5824
3328/4849 [===================>..........] - ETA: 2:01 - loss: 0.6768 - acc: 0.5814
3392/4849 [===================>..........] - ETA: 1:56 - loss: 0.6769 - acc: 0.5817
3456/4849 [====================>.........] - ETA: 1:51 - loss: 0.6763 - acc: 0.5822
3520/4849 [====================>.........] - ETA: 1:46 - loss: 0.6761 - acc: 0.5815
3584/4849 [=====================>........] - ETA: 1:41 - loss: 0.6760 - acc: 0.5820
3648/4849 [=====================>........] - ETA: 1:36 - loss: 0.6753 - acc: 0.5839
3712/4849 [=====================>........] - ETA: 1:31 - loss: 0.6749 - acc: 0.5841
3776/4849 [======================>.......] - ETA: 1:26 - loss: 0.6754 - acc: 0.5824
3840/4849 [======================>.......] - ETA: 1:21 - loss: 0.6759 - acc: 0.5820
3904/4849 [=======================>......] - ETA: 1:15 - loss: 0.6753 - acc: 0.5838
3968/4849 [=======================>......] - ETA: 1:10 - loss: 0.6744 - acc: 0.5859
4032/4849 [=======================>......] - ETA: 1:05 - loss: 0.6753 - acc: 0.5836
4096/4849 [========================>.....] - ETA: 1:00 - loss: 0.6758 - acc: 0.5818
4160/4849 [========================>.....] - ETA: 55s - loss: 0.6757 - acc: 0.5825 
4224/4849 [=========================>....] - ETA: 50s - loss: 0.6755 - acc: 0.5829
4288/4849 [=========================>....] - ETA: 45s - loss: 0.6753 - acc: 0.5826
4352/4849 [=========================>....] - ETA: 39s - loss: 0.6752 - acc: 0.5827
4416/4849 [==========================>...] - ETA: 34s - loss: 0.6750 - acc: 0.5838
4480/4849 [==========================>...] - ETA: 29s - loss: 0.6752 - acc: 0.5830
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6748 - acc: 0.5838
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6762 - acc: 0.5827
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6772 - acc: 0.5811
4736/4849 [============================>.] - ETA: 8s - loss: 0.6776 - acc: 0.5804 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6774 - acc: 0.5802
4849/4849 [==============================] - 398s 82ms/step - loss: 0.6771 - acc: 0.5807 - val_loss: 0.7105 - val_acc: 0.5455

Epoch 00009: val_acc did not improve from 0.59184
Epoch 10/10

  64/4849 [..............................] - ETA: 5:29 - loss: 0.6439 - acc: 0.6562
 128/4849 [..............................] - ETA: 5:41 - loss: 0.6666 - acc: 0.6094
 192/4849 [>.............................] - ETA: 5:46 - loss: 0.6716 - acc: 0.5833
 256/4849 [>.............................] - ETA: 5:53 - loss: 0.6679 - acc: 0.5859
 320/4849 [>.............................] - ETA: 5:47 - loss: 0.6737 - acc: 0.5750
 384/4849 [=>............................] - ETA: 5:49 - loss: 0.6793 - acc: 0.5651
 448/4849 [=>............................] - ETA: 5:36 - loss: 0.6766 - acc: 0.5714
 512/4849 [==>...........................] - ETA: 5:32 - loss: 0.6772 - acc: 0.5742
 576/4849 [==>...........................] - ETA: 5:26 - loss: 0.6731 - acc: 0.5920
 640/4849 [==>...........................] - ETA: 5:22 - loss: 0.6761 - acc: 0.5891
 704/4849 [===>..........................] - ETA: 5:15 - loss: 0.6780 - acc: 0.5895
 768/4849 [===>..........................] - ETA: 5:13 - loss: 0.6775 - acc: 0.5911
 832/4849 [====>.........................] - ETA: 5:06 - loss: 0.6760 - acc: 0.5950
 896/4849 [====>.........................] - ETA: 5:00 - loss: 0.6730 - acc: 0.6004
 960/4849 [====>.........................] - ETA: 4:54 - loss: 0.6750 - acc: 0.5979
1024/4849 [=====>........................] - ETA: 4:49 - loss: 0.6751 - acc: 0.5957
1088/4849 [=====>........................] - ETA: 4:43 - loss: 0.6747 - acc: 0.5938
1152/4849 [======>.......................] - ETA: 4:37 - loss: 0.6742 - acc: 0.5946
1216/4849 [======>.......................] - ETA: 4:32 - loss: 0.6741 - acc: 0.5962
1280/4849 [======>.......................] - ETA: 4:27 - loss: 0.6740 - acc: 0.5961
1344/4849 [=======>......................] - ETA: 4:23 - loss: 0.6731 - acc: 0.5960
1408/4849 [=======>......................] - ETA: 4:18 - loss: 0.6718 - acc: 0.5973
1472/4849 [========>.....................] - ETA: 4:15 - loss: 0.6726 - acc: 0.5958
1536/4849 [========>.....................] - ETA: 4:10 - loss: 0.6731 - acc: 0.5944
1600/4849 [========>.....................] - ETA: 4:05 - loss: 0.6725 - acc: 0.5950
1664/4849 [=========>....................] - ETA: 4:01 - loss: 0.6716 - acc: 0.5980
1728/4849 [=========>....................] - ETA: 3:56 - loss: 0.6712 - acc: 0.5984
1792/4849 [==========>...................] - ETA: 3:51 - loss: 0.6728 - acc: 0.5938
1856/4849 [==========>...................] - ETA: 3:46 - loss: 0.6724 - acc: 0.5932
1920/4849 [==========>...................] - ETA: 3:39 - loss: 0.6707 - acc: 0.5969
1984/4849 [===========>..................] - ETA: 3:34 - loss: 0.6710 - acc: 0.5948
2048/4849 [===========>..................] - ETA: 3:29 - loss: 0.6718 - acc: 0.5928
2112/4849 [============>.................] - ETA: 3:25 - loss: 0.6719 - acc: 0.5942
2176/4849 [============>.................] - ETA: 3:21 - loss: 0.6731 - acc: 0.5905
2240/4849 [============>.................] - ETA: 3:16 - loss: 0.6731 - acc: 0.5920
2304/4849 [=============>................] - ETA: 3:11 - loss: 0.6727 - acc: 0.5916
2368/4849 [=============>................] - ETA: 3:05 - loss: 0.6729 - acc: 0.5908
2432/4849 [==============>...............] - ETA: 3:01 - loss: 0.6737 - acc: 0.5884
2496/4849 [==============>...............] - ETA: 2:56 - loss: 0.6740 - acc: 0.5877
2560/4849 [==============>...............] - ETA: 2:51 - loss: 0.6742 - acc: 0.5867
2624/4849 [===============>..............] - ETA: 2:47 - loss: 0.6733 - acc: 0.5899
2688/4849 [===============>..............] - ETA: 2:42 - loss: 0.6732 - acc: 0.5904
2752/4849 [================>.............] - ETA: 2:37 - loss: 0.6745 - acc: 0.5879
2816/4849 [================>.............] - ETA: 2:32 - loss: 0.6741 - acc: 0.5884
2880/4849 [================>.............] - ETA: 2:28 - loss: 0.6743 - acc: 0.5885
2944/4849 [=================>............] - ETA: 2:23 - loss: 0.6733 - acc: 0.5904
3008/4849 [=================>............] - ETA: 2:18 - loss: 0.6743 - acc: 0.5891
3072/4849 [==================>...........] - ETA: 2:14 - loss: 0.6754 - acc: 0.5879
3136/4849 [==================>...........] - ETA: 2:09 - loss: 0.6749 - acc: 0.5890
3200/4849 [==================>...........] - ETA: 2:04 - loss: 0.6748 - acc: 0.5900
3264/4849 [===================>..........] - ETA: 1:59 - loss: 0.6754 - acc: 0.5882
3328/4849 [===================>..........] - ETA: 1:54 - loss: 0.6761 - acc: 0.5862
3392/4849 [===================>..........] - ETA: 1:49 - loss: 0.6753 - acc: 0.5881
3456/4849 [====================>.........] - ETA: 1:44 - loss: 0.6752 - acc: 0.5885
3520/4849 [====================>.........] - ETA: 1:39 - loss: 0.6753 - acc: 0.5892
3584/4849 [=====================>........] - ETA: 1:34 - loss: 0.6752 - acc: 0.5893
3648/4849 [=====================>........] - ETA: 1:29 - loss: 0.6754 - acc: 0.5883
3712/4849 [=====================>........] - ETA: 1:24 - loss: 0.6746 - acc: 0.5900
3776/4849 [======================>.......] - ETA: 1:20 - loss: 0.6743 - acc: 0.5903
3840/4849 [======================>.......] - ETA: 1:15 - loss: 0.6743 - acc: 0.5909
3904/4849 [=======================>......] - ETA: 1:10 - loss: 0.6747 - acc: 0.5904
3968/4849 [=======================>......] - ETA: 1:05 - loss: 0.6741 - acc: 0.5912
4032/4849 [=======================>......] - ETA: 1:00 - loss: 0.6740 - acc: 0.5918
4096/4849 [========================>.....] - ETA: 56s - loss: 0.6736 - acc: 0.5923 
4160/4849 [========================>.....] - ETA: 51s - loss: 0.6735 - acc: 0.5930
4224/4849 [=========================>....] - ETA: 46s - loss: 0.6732 - acc: 0.5926
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6727 - acc: 0.5935
4352/4849 [=========================>....] - ETA: 37s - loss: 0.6732 - acc: 0.5926
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6737 - acc: 0.5922
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6738 - acc: 0.5917
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6741 - acc: 0.5900
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6742 - acc: 0.5890
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6743 - acc: 0.5886
4736/4849 [============================>.] - ETA: 8s - loss: 0.6740 - acc: 0.5889 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6742 - acc: 0.5885
4849/4849 [==============================] - 375s 77ms/step - loss: 0.6745 - acc: 0.5884 - val_loss: 0.7118 - val_acc: 0.5065

Epoch 00010: val_acc did not improve from 0.59184
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9600731a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9600731a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9600553d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9600553d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf84cb810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bf84cb810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f960026fdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f960026fdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9600220790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9600220790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c0d1950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c0d1950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f960026f290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f960026f290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c07d00d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c07d00d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f934039eb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f934039eb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9340393ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9340393ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c07df950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c07df950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93403b8c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93403b8c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93403860d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93403860d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95c0704f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95c0704f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95c04cded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95c04cded0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c04c8250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c04c8250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95c05c4090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95c05c4090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c0436410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c0436410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95c0203cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95c0203cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95c0162ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95c0162ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c02a5d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c02a5d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95c02cf110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95c02cf110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c0150f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95c0150f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95a064f750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95a064f750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95c00d8d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95c00d8d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95a0464990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95a0464990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95c0163550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95c0163550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95a040bc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95a040bc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95a0570250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95a0570250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95a02eeb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95a02eeb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95a0373f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95a0373f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95a05c3050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95a05c3050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95a0088950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95a0088950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95807e8190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95807e8190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95807d4bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95807d4bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95805d8850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95805d8850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95807e8ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95807e8ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95807c5110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95807c5110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95807d8650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95807d8650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f958039f790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f958039f790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95804f3d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95804f3d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95804da350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95804da350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9580514710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9580514710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9580181c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9580181c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f958019eed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f958019eed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95801a44d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95801a44d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9580089110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9580089110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95800b8f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95800b8f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f956866a950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f956866a950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f956865f490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f956865f490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95685fcc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95685fcc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95686fea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95686fea10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95685f3fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95685f3fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95683a38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95683a38d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95681be0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95681be0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9580112150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9580112150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f956864c5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f956864c5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9568132150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9568132150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9568214550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9568214550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f956805ced0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f956805ced0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9568058e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9568058e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95680dd350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95680dd350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f955c757290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f955c757290>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 2:22
 128/1348 [=>............................] - ETA: 1:23
 192/1348 [===>..........................] - ETA: 1:03
 256/1348 [====>.........................] - ETA: 51s 
 320/1348 [======>.......................] - ETA: 44s
 384/1348 [=======>......................] - ETA: 39s
 448/1348 [========>.....................] - ETA: 35s
 512/1348 [==========>...................] - ETA: 32s
 576/1348 [===========>..................] - ETA: 29s
 640/1348 [=============>................] - ETA: 25s
 704/1348 [==============>...............] - ETA: 22s
 768/1348 [================>.............] - ETA: 20s
 832/1348 [=================>............] - ETA: 17s
 896/1348 [==================>...........] - ETA: 15s
 960/1348 [====================>.........] - ETA: 13s
1024/1348 [=====================>........] - ETA: 10s
1088/1348 [=======================>......] - ETA: 8s 
1152/1348 [========================>.....] - ETA: 6s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 44s 33ms/step
loss: 0.6743292677650112
acc: 0.5875370919881305
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9284381e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9284381e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f94005d9dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f94005d9dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beb10d7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beb10d7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f940044ca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f940044ca90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f960066d190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f960066d190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f960048df90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f960048df90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f940044c410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f940044c410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f960038a290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f960038a290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9400404e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9400404e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9284218e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9284218e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9600456d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9600456d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9400404090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9400404090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f928420f310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f928420f310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92607e6c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92607e6c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f926067fcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f926067fcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92842f9250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92842f9250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92607e6c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92607e6c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92842fcad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92842fcad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92604c12d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92604c12d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9260526850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9260526850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f926042a7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f926042a7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92604c1390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92604c1390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92606ef6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92606ef6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92601cebd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92601cebd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f926014a190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f926014a190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92600d5410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92600d5410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92601ce090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92601ce090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92600c7150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92600c7150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f924070be90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f924070be90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f924055d690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f924055d690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f924070bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f924070bfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f926058df10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f926058df10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92405a5250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92405a5250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92406cf9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92406cf9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f924035d810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f924035d810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92405b0890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92405b0890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9240462410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9240462410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9240346cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9240346cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92207c7490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92207c7490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f924011df10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f924011df10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9220741dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9220741dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92207c7810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92207c7810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92205bc810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92205bc810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92207b4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92207b4fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f922072a850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f922072a850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92204df710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92204df710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9220732f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9220732f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f922037c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f922037c1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92203c8e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92203c8e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91402e2910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91402e2910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9260447150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9260447150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f914028f710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f914028f710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9140320d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9140320d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9120743a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9120743a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91401ddf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91401ddf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91207ab5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91207ab5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9120743710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9120743710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91206da950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91206da950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91204c7a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91204c7a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91203bc690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91203bc690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9120567690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9120567690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91206e1590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91206e1590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f912036bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f912036bc50>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 29:11 - loss: 0.7313 - acc: 0.5312
 128/4849 [..............................] - ETA: 17:48 - loss: 0.7297 - acc: 0.5312
 192/4849 [>.............................] - ETA: 13:50 - loss: 0.7154 - acc: 0.5365
 256/4849 [>.............................] - ETA: 11:51 - loss: 0.7191 - acc: 0.5352
 320/4849 [>.............................] - ETA: 10:38 - loss: 0.6989 - acc: 0.5594
 384/4849 [=>............................] - ETA: 9:45 - loss: 0.7024 - acc: 0.5521 
 448/4849 [=>............................] - ETA: 9:08 - loss: 0.6976 - acc: 0.5558
 512/4849 [==>...........................] - ETA: 8:38 - loss: 0.7040 - acc: 0.5508
 576/4849 [==>...........................] - ETA: 8:13 - loss: 0.7025 - acc: 0.5469
 640/4849 [==>...........................] - ETA: 7:55 - loss: 0.7000 - acc: 0.5531
 704/4849 [===>..........................] - ETA: 7:42 - loss: 0.7040 - acc: 0.5440
 768/4849 [===>..........................] - ETA: 7:24 - loss: 0.7080 - acc: 0.5391
 832/4849 [====>.........................] - ETA: 7:09 - loss: 0.7104 - acc: 0.5337
 896/4849 [====>.........................] - ETA: 6:57 - loss: 0.7126 - acc: 0.5301
 960/4849 [====>.........................] - ETA: 6:44 - loss: 0.7112 - acc: 0.5302
1024/4849 [=====>........................] - ETA: 6:33 - loss: 0.7114 - acc: 0.5322
1088/4849 [=====>........................] - ETA: 6:22 - loss: 0.7113 - acc: 0.5331
1152/4849 [======>.......................] - ETA: 6:12 - loss: 0.7140 - acc: 0.5312
1216/4849 [======>.......................] - ETA: 6:02 - loss: 0.7164 - acc: 0.5296
1280/4849 [======>.......................] - ETA: 5:52 - loss: 0.7166 - acc: 0.5281
1344/4849 [=======>......................] - ETA: 5:43 - loss: 0.7157 - acc: 0.5275
1408/4849 [=======>......................] - ETA: 5:35 - loss: 0.7167 - acc: 0.5291
1472/4849 [========>.....................] - ETA: 5:26 - loss: 0.7161 - acc: 0.5319
1536/4849 [========>.....................] - ETA: 5:18 - loss: 0.7145 - acc: 0.5339
1600/4849 [========>.....................] - ETA: 5:10 - loss: 0.7132 - acc: 0.5356
1664/4849 [=========>....................] - ETA: 5:03 - loss: 0.7139 - acc: 0.5361
1728/4849 [=========>....................] - ETA: 4:56 - loss: 0.7130 - acc: 0.5359
1792/4849 [==========>...................] - ETA: 4:49 - loss: 0.7120 - acc: 0.5379
1856/4849 [==========>...................] - ETA: 4:42 - loss: 0.7112 - acc: 0.5393
1920/4849 [==========>...................] - ETA: 4:35 - loss: 0.7097 - acc: 0.5417
1984/4849 [===========>..................] - ETA: 4:30 - loss: 0.7103 - acc: 0.5403
2048/4849 [===========>..................] - ETA: 4:23 - loss: 0.7088 - acc: 0.5425
2112/4849 [============>.................] - ETA: 4:16 - loss: 0.7107 - acc: 0.5402
2176/4849 [============>.................] - ETA: 4:11 - loss: 0.7109 - acc: 0.5418
2240/4849 [============>.................] - ETA: 4:04 - loss: 0.7118 - acc: 0.5402
2304/4849 [=============>................] - ETA: 3:57 - loss: 0.7125 - acc: 0.5395
2368/4849 [=============>................] - ETA: 3:51 - loss: 0.7141 - acc: 0.5367
2432/4849 [==============>...............] - ETA: 3:45 - loss: 0.7124 - acc: 0.5395
2496/4849 [==============>...............] - ETA: 3:38 - loss: 0.7137 - acc: 0.5345
2560/4849 [==============>...............] - ETA: 3:32 - loss: 0.7121 - acc: 0.5363
2624/4849 [===============>..............] - ETA: 3:26 - loss: 0.7122 - acc: 0.5354
2688/4849 [===============>..............] - ETA: 3:19 - loss: 0.7119 - acc: 0.5342
2752/4849 [================>.............] - ETA: 3:13 - loss: 0.7117 - acc: 0.5334
2816/4849 [================>.............] - ETA: 3:07 - loss: 0.7119 - acc: 0.5309
2880/4849 [================>.............] - ETA: 3:01 - loss: 0.7120 - acc: 0.5309
2944/4849 [=================>............] - ETA: 2:55 - loss: 0.7127 - acc: 0.5299
3008/4849 [=================>............] - ETA: 2:48 - loss: 0.7126 - acc: 0.5296
3072/4849 [==================>...........] - ETA: 2:42 - loss: 0.7114 - acc: 0.5309
3136/4849 [==================>...........] - ETA: 2:36 - loss: 0.7111 - acc: 0.5303
3200/4849 [==================>...........] - ETA: 2:30 - loss: 0.7115 - acc: 0.5294
3264/4849 [===================>..........] - ETA: 2:24 - loss: 0.7117 - acc: 0.5276
3328/4849 [===================>..........] - ETA: 2:18 - loss: 0.7123 - acc: 0.5240
3392/4849 [===================>..........] - ETA: 2:12 - loss: 0.7122 - acc: 0.5233
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.7128 - acc: 0.5220
3520/4849 [====================>.........] - ETA: 2:00 - loss: 0.7125 - acc: 0.5216
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.7112 - acc: 0.5240
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.7110 - acc: 0.5247
3712/4849 [=====================>........] - ETA: 1:42 - loss: 0.7110 - acc: 0.5242
3776/4849 [======================>.......] - ETA: 1:36 - loss: 0.7112 - acc: 0.5241
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.7110 - acc: 0.5242
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.7118 - acc: 0.5236
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.7116 - acc: 0.5234
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.7108 - acc: 0.5248
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.7114 - acc: 0.5229
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.7120 - acc: 0.5204
4224/4849 [=========================>....] - ETA: 55s - loss: 0.7116 - acc: 0.5215 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.7116 - acc: 0.5208
4352/4849 [=========================>....] - ETA: 44s - loss: 0.7120 - acc: 0.5205
4416/4849 [==========================>...] - ETA: 38s - loss: 0.7122 - acc: 0.5195
4480/4849 [==========================>...] - ETA: 32s - loss: 0.7121 - acc: 0.5199
4544/4849 [===========================>..] - ETA: 27s - loss: 0.7113 - acc: 0.5200
4608/4849 [===========================>..] - ETA: 21s - loss: 0.7112 - acc: 0.5191
4672/4849 [===========================>..] - ETA: 15s - loss: 0.7108 - acc: 0.5201
4736/4849 [============================>.] - ETA: 10s - loss: 0.7105 - acc: 0.5205
4800/4849 [============================>.] - ETA: 4s - loss: 0.7100 - acc: 0.5212 
4849/4849 [==============================] - 446s 92ms/step - loss: 0.7095 - acc: 0.5230 - val_loss: 0.6858 - val_acc: 0.5584

Epoch 00001: val_acc improved from -inf to 0.55844, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window19/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 7:00 - loss: 0.7034 - acc: 0.5156
 128/4849 [..............................] - ETA: 6:38 - loss: 0.6967 - acc: 0.5391
 192/4849 [>.............................] - ETA: 6:39 - loss: 0.7030 - acc: 0.5208
 256/4849 [>.............................] - ETA: 6:32 - loss: 0.7015 - acc: 0.5156
 320/4849 [>.............................] - ETA: 6:26 - loss: 0.6976 - acc: 0.5219
 384/4849 [=>............................] - ETA: 6:19 - loss: 0.7059 - acc: 0.5130
 448/4849 [=>............................] - ETA: 6:14 - loss: 0.7030 - acc: 0.5223
 512/4849 [==>...........................] - ETA: 6:09 - loss: 0.6964 - acc: 0.5371
 576/4849 [==>...........................] - ETA: 6:04 - loss: 0.6950 - acc: 0.5399
 640/4849 [==>...........................] - ETA: 5:57 - loss: 0.6899 - acc: 0.5516
 704/4849 [===>..........................] - ETA: 5:55 - loss: 0.6918 - acc: 0.5469
 768/4849 [===>..........................] - ETA: 5:46 - loss: 0.6915 - acc: 0.5456
 832/4849 [====>.........................] - ETA: 5:40 - loss: 0.6908 - acc: 0.5469
 896/4849 [====>.........................] - ETA: 5:35 - loss: 0.6895 - acc: 0.5502
 960/4849 [====>.........................] - ETA: 5:28 - loss: 0.6912 - acc: 0.5479
1024/4849 [=====>........................] - ETA: 5:23 - loss: 0.6879 - acc: 0.5576
1088/4849 [=====>........................] - ETA: 5:18 - loss: 0.6870 - acc: 0.5570
1152/4849 [======>.......................] - ETA: 5:12 - loss: 0.6866 - acc: 0.5582
1216/4849 [======>.......................] - ETA: 5:07 - loss: 0.6879 - acc: 0.5592
1280/4849 [======>.......................] - ETA: 5:01 - loss: 0.6865 - acc: 0.5633
1344/4849 [=======>......................] - ETA: 4:54 - loss: 0.6867 - acc: 0.5632
1408/4849 [=======>......................] - ETA: 4:49 - loss: 0.6877 - acc: 0.5604
1472/4849 [========>.....................] - ETA: 4:43 - loss: 0.6892 - acc: 0.5557
1536/4849 [========>.....................] - ETA: 4:39 - loss: 0.6905 - acc: 0.5534
1600/4849 [========>.....................] - ETA: 4:34 - loss: 0.6907 - acc: 0.5519
1664/4849 [=========>....................] - ETA: 4:30 - loss: 0.6917 - acc: 0.5523
1728/4849 [=========>....................] - ETA: 4:25 - loss: 0.6906 - acc: 0.5561
1792/4849 [==========>...................] - ETA: 4:19 - loss: 0.6903 - acc: 0.5575
1856/4849 [==========>...................] - ETA: 4:14 - loss: 0.6913 - acc: 0.5544
1920/4849 [==========>...................] - ETA: 4:09 - loss: 0.6901 - acc: 0.5568
1984/4849 [===========>..................] - ETA: 4:04 - loss: 0.6905 - acc: 0.5570
2048/4849 [===========>..................] - ETA: 3:59 - loss: 0.6916 - acc: 0.5542
2112/4849 [============>.................] - ETA: 3:54 - loss: 0.6923 - acc: 0.5526
2176/4849 [============>.................] - ETA: 3:49 - loss: 0.6919 - acc: 0.5515
2240/4849 [============>.................] - ETA: 3:44 - loss: 0.6922 - acc: 0.5500
2304/4849 [=============>................] - ETA: 3:38 - loss: 0.6922 - acc: 0.5495
2368/4849 [=============>................] - ETA: 3:33 - loss: 0.6920 - acc: 0.5503
2432/4849 [==============>...............] - ETA: 3:28 - loss: 0.6936 - acc: 0.5473
2496/4849 [==============>...............] - ETA: 3:22 - loss: 0.6939 - acc: 0.5465
2560/4849 [==============>...............] - ETA: 3:17 - loss: 0.6956 - acc: 0.5422
2624/4849 [===============>..............] - ETA: 3:11 - loss: 0.6955 - acc: 0.5427
2688/4849 [===============>..............] - ETA: 3:06 - loss: 0.6950 - acc: 0.5435
2752/4849 [================>.............] - ETA: 3:01 - loss: 0.6943 - acc: 0.5461
2816/4849 [================>.............] - ETA: 2:55 - loss: 0.6942 - acc: 0.5458
2880/4849 [================>.............] - ETA: 2:50 - loss: 0.6941 - acc: 0.5462
2944/4849 [=================>............] - ETA: 2:45 - loss: 0.6938 - acc: 0.5455
3008/4849 [=================>............] - ETA: 2:39 - loss: 0.6947 - acc: 0.5432
3072/4849 [==================>...........] - ETA: 2:34 - loss: 0.6950 - acc: 0.5423
3136/4849 [==================>...........] - ETA: 2:29 - loss: 0.6951 - acc: 0.5415
3200/4849 [==================>...........] - ETA: 2:23 - loss: 0.6950 - acc: 0.5422
3264/4849 [===================>..........] - ETA: 2:18 - loss: 0.6955 - acc: 0.5407
3328/4849 [===================>..........] - ETA: 2:12 - loss: 0.6957 - acc: 0.5403
3392/4849 [===================>..........] - ETA: 2:07 - loss: 0.6953 - acc: 0.5404
3456/4849 [====================>.........] - ETA: 2:02 - loss: 0.6945 - acc: 0.5425
3520/4849 [====================>.........] - ETA: 1:56 - loss: 0.6950 - acc: 0.5415
3584/4849 [=====================>........] - ETA: 1:51 - loss: 0.6948 - acc: 0.5419
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6949 - acc: 0.5414
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6948 - acc: 0.5415
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6950 - acc: 0.5410
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6948 - acc: 0.5411
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6945 - acc: 0.5412
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6940 - acc: 0.5416
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6936 - acc: 0.5429
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6934 - acc: 0.5430
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6926 - acc: 0.5437
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6928 - acc: 0.5429 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6923 - acc: 0.5438
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6921 - acc: 0.5441
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6921 - acc: 0.5442
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6923 - acc: 0.5446
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6921 - acc: 0.5447
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6919 - acc: 0.5449
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6923 - acc: 0.5445
4736/4849 [============================>.] - ETA: 10s - loss: 0.6927 - acc: 0.5441
4800/4849 [============================>.] - ETA: 4s - loss: 0.6927 - acc: 0.5446 
4849/4849 [==============================] - 457s 94ms/step - loss: 0.6929 - acc: 0.5438 - val_loss: 0.6859 - val_acc: 0.5603

Epoch 00002: val_acc improved from 0.55844 to 0.56030, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window19/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 3/10

  64/4849 [..............................] - ETA: 8:19 - loss: 0.6642 - acc: 0.6250
 128/4849 [..............................] - ETA: 8:04 - loss: 0.6747 - acc: 0.6172
 192/4849 [>.............................] - ETA: 8:06 - loss: 0.6717 - acc: 0.6094
 256/4849 [>.............................] - ETA: 7:51 - loss: 0.6676 - acc: 0.6055
 320/4849 [>.............................] - ETA: 7:43 - loss: 0.6663 - acc: 0.6094
 384/4849 [=>............................] - ETA: 7:37 - loss: 0.6761 - acc: 0.5833
 448/4849 [=>............................] - ETA: 7:34 - loss: 0.6843 - acc: 0.5692
 512/4849 [==>...........................] - ETA: 7:28 - loss: 0.6879 - acc: 0.5566
 576/4849 [==>...........................] - ETA: 7:21 - loss: 0.6890 - acc: 0.5538
 640/4849 [==>...........................] - ETA: 7:12 - loss: 0.6879 - acc: 0.5531
 704/4849 [===>..........................] - ETA: 7:03 - loss: 0.6846 - acc: 0.5597
 768/4849 [===>..........................] - ETA: 6:58 - loss: 0.6884 - acc: 0.5443
 832/4849 [====>.........................] - ETA: 6:51 - loss: 0.6900 - acc: 0.5409
 896/4849 [====>.........................] - ETA: 6:45 - loss: 0.6894 - acc: 0.5424
 960/4849 [====>.........................] - ETA: 6:41 - loss: 0.6890 - acc: 0.5427
1024/4849 [=====>........................] - ETA: 6:36 - loss: 0.6894 - acc: 0.5391
1088/4849 [=====>........................] - ETA: 6:31 - loss: 0.6914 - acc: 0.5368
1152/4849 [======>.......................] - ETA: 6:25 - loss: 0.6907 - acc: 0.5382
1216/4849 [======>.......................] - ETA: 6:18 - loss: 0.6911 - acc: 0.5345
1280/4849 [======>.......................] - ETA: 6:11 - loss: 0.6923 - acc: 0.5305
1344/4849 [=======>......................] - ETA: 6:04 - loss: 0.6912 - acc: 0.5335
1408/4849 [=======>......................] - ETA: 5:57 - loss: 0.6923 - acc: 0.5320
1472/4849 [========>.....................] - ETA: 5:50 - loss: 0.6930 - acc: 0.5292
1536/4849 [========>.....................] - ETA: 5:43 - loss: 0.6921 - acc: 0.5326
1600/4849 [========>.....................] - ETA: 5:37 - loss: 0.6939 - acc: 0.5306
1664/4849 [=========>....................] - ETA: 5:31 - loss: 0.6941 - acc: 0.5288
1728/4849 [=========>....................] - ETA: 5:24 - loss: 0.6941 - acc: 0.5301
1792/4849 [==========>...................] - ETA: 5:17 - loss: 0.6938 - acc: 0.5312
1856/4849 [==========>...................] - ETA: 5:10 - loss: 0.6935 - acc: 0.5345
1920/4849 [==========>...................] - ETA: 5:04 - loss: 0.6926 - acc: 0.5365
1984/4849 [===========>..................] - ETA: 4:57 - loss: 0.6918 - acc: 0.5403
2048/4849 [===========>..................] - ETA: 4:50 - loss: 0.6926 - acc: 0.5400
2112/4849 [============>.................] - ETA: 4:43 - loss: 0.6922 - acc: 0.5407
2176/4849 [============>.................] - ETA: 4:36 - loss: 0.6930 - acc: 0.5377
2240/4849 [============>.................] - ETA: 4:30 - loss: 0.6934 - acc: 0.5375
2304/4849 [=============>................] - ETA: 4:23 - loss: 0.6926 - acc: 0.5382
2368/4849 [=============>................] - ETA: 4:16 - loss: 0.6922 - acc: 0.5393
2432/4849 [==============>...............] - ETA: 4:09 - loss: 0.6915 - acc: 0.5407
2496/4849 [==============>...............] - ETA: 4:03 - loss: 0.6910 - acc: 0.5417
2560/4849 [==============>...............] - ETA: 3:56 - loss: 0.6916 - acc: 0.5398
2624/4849 [===============>..............] - ETA: 3:49 - loss: 0.6909 - acc: 0.5415
2688/4849 [===============>..............] - ETA: 3:43 - loss: 0.6911 - acc: 0.5398
2752/4849 [================>.............] - ETA: 3:36 - loss: 0.6905 - acc: 0.5403
2816/4849 [================>.............] - ETA: 3:30 - loss: 0.6906 - acc: 0.5391
2880/4849 [================>.............] - ETA: 3:23 - loss: 0.6907 - acc: 0.5389
2944/4849 [=================>............] - ETA: 3:16 - loss: 0.6902 - acc: 0.5404
3008/4849 [=================>............] - ETA: 3:10 - loss: 0.6903 - acc: 0.5389
3072/4849 [==================>...........] - ETA: 3:03 - loss: 0.6907 - acc: 0.5384
3136/4849 [==================>...........] - ETA: 2:56 - loss: 0.6901 - acc: 0.5408
3200/4849 [==================>...........] - ETA: 2:50 - loss: 0.6898 - acc: 0.5428
3264/4849 [===================>..........] - ETA: 2:43 - loss: 0.6904 - acc: 0.5429
3328/4849 [===================>..........] - ETA: 2:36 - loss: 0.6898 - acc: 0.5442
3392/4849 [===================>..........] - ETA: 2:30 - loss: 0.6892 - acc: 0.5460
3456/4849 [====================>.........] - ETA: 2:23 - loss: 0.6886 - acc: 0.5475
3520/4849 [====================>.........] - ETA: 2:16 - loss: 0.6884 - acc: 0.5474
3584/4849 [=====================>........] - ETA: 2:10 - loss: 0.6885 - acc: 0.5483
3648/4849 [=====================>........] - ETA: 2:03 - loss: 0.6884 - acc: 0.5480
3712/4849 [=====================>........] - ETA: 1:56 - loss: 0.6892 - acc: 0.5455
3776/4849 [======================>.......] - ETA: 1:50 - loss: 0.6888 - acc: 0.5458
3840/4849 [======================>.......] - ETA: 1:43 - loss: 0.6887 - acc: 0.5464
3904/4849 [=======================>......] - ETA: 1:37 - loss: 0.6887 - acc: 0.5461
3968/4849 [=======================>......] - ETA: 1:30 - loss: 0.6881 - acc: 0.5476
4032/4849 [=======================>......] - ETA: 1:23 - loss: 0.6881 - acc: 0.5484
4096/4849 [========================>.....] - ETA: 1:17 - loss: 0.6872 - acc: 0.5505
4160/4849 [========================>.....] - ETA: 1:10 - loss: 0.6871 - acc: 0.5514
4224/4849 [=========================>....] - ETA: 1:04 - loss: 0.6867 - acc: 0.5514
4288/4849 [=========================>....] - ETA: 57s - loss: 0.6869 - acc: 0.5508 
4352/4849 [=========================>....] - ETA: 50s - loss: 0.6868 - acc: 0.5499
4416/4849 [==========================>...] - ETA: 44s - loss: 0.6873 - acc: 0.5487
4480/4849 [==========================>...] - ETA: 37s - loss: 0.6877 - acc: 0.5482
4544/4849 [===========================>..] - ETA: 31s - loss: 0.6871 - acc: 0.5497
4608/4849 [===========================>..] - ETA: 24s - loss: 0.6872 - acc: 0.5497
4672/4849 [===========================>..] - ETA: 18s - loss: 0.6872 - acc: 0.5507
4736/4849 [============================>.] - ETA: 11s - loss: 0.6863 - acc: 0.5519
4800/4849 [============================>.] - ETA: 5s - loss: 0.6866 - acc: 0.5508 
4849/4849 [==============================] - 513s 106ms/step - loss: 0.6874 - acc: 0.5502 - val_loss: 0.7084 - val_acc: 0.5102

Epoch 00003: val_acc did not improve from 0.56030
Epoch 4/10

  64/4849 [..............................] - ETA: 7:35 - loss: 0.6849 - acc: 0.5156
 128/4849 [..............................] - ETA: 7:35 - loss: 0.6874 - acc: 0.5391
 192/4849 [>.............................] - ETA: 7:39 - loss: 0.6896 - acc: 0.5573
 256/4849 [>.............................] - ETA: 7:29 - loss: 0.6846 - acc: 0.5703
 320/4849 [>.............................] - ETA: 7:21 - loss: 0.6858 - acc: 0.5563
 384/4849 [=>............................] - ETA: 7:12 - loss: 0.6800 - acc: 0.5677
 448/4849 [=>............................] - ETA: 7:06 - loss: 0.6813 - acc: 0.5714
 512/4849 [==>...........................] - ETA: 6:59 - loss: 0.6785 - acc: 0.5781
 576/4849 [==>...........................] - ETA: 6:54 - loss: 0.6772 - acc: 0.5799
 640/4849 [==>...........................] - ETA: 6:48 - loss: 0.6759 - acc: 0.5844
 704/4849 [===>..........................] - ETA: 6:41 - loss: 0.6780 - acc: 0.5767
 768/4849 [===>..........................] - ETA: 6:37 - loss: 0.6806 - acc: 0.5781
 832/4849 [====>.........................] - ETA: 6:28 - loss: 0.6786 - acc: 0.5817
 896/4849 [====>.........................] - ETA: 6:22 - loss: 0.6797 - acc: 0.5770
 960/4849 [====>.........................] - ETA: 6:16 - loss: 0.6821 - acc: 0.5687
1024/4849 [=====>........................] - ETA: 6:09 - loss: 0.6834 - acc: 0.5664
1088/4849 [=====>........................] - ETA: 6:04 - loss: 0.6826 - acc: 0.5634
1152/4849 [======>.......................] - ETA: 5:58 - loss: 0.6823 - acc: 0.5634
1216/4849 [======>.......................] - ETA: 5:52 - loss: 0.6834 - acc: 0.5600
1280/4849 [======>.......................] - ETA: 5:45 - loss: 0.6844 - acc: 0.5570
1344/4849 [=======>......................] - ETA: 5:38 - loss: 0.6841 - acc: 0.5565
1408/4849 [=======>......................] - ETA: 5:32 - loss: 0.6848 - acc: 0.5547
1472/4849 [========>.....................] - ETA: 5:27 - loss: 0.6851 - acc: 0.5523
1536/4849 [========>.....................] - ETA: 5:20 - loss: 0.6849 - acc: 0.5547
1600/4849 [========>.....................] - ETA: 5:14 - loss: 0.6835 - acc: 0.5563
1664/4849 [=========>....................] - ETA: 5:08 - loss: 0.6843 - acc: 0.5529
1728/4849 [=========>....................] - ETA: 5:03 - loss: 0.6846 - acc: 0.5544
1792/4849 [==========>...................] - ETA: 4:56 - loss: 0.6848 - acc: 0.5513
1856/4849 [==========>...................] - ETA: 4:50 - loss: 0.6860 - acc: 0.5480
1920/4849 [==========>...................] - ETA: 4:43 - loss: 0.6866 - acc: 0.5469
1984/4849 [===========>..................] - ETA: 4:36 - loss: 0.6860 - acc: 0.5489
2048/4849 [===========>..................] - ETA: 4:30 - loss: 0.6852 - acc: 0.5513
2112/4849 [============>.................] - ETA: 4:24 - loss: 0.6852 - acc: 0.5483
2176/4849 [============>.................] - ETA: 4:17 - loss: 0.6858 - acc: 0.5483
2240/4849 [============>.................] - ETA: 4:11 - loss: 0.6867 - acc: 0.5473
2304/4849 [=============>................] - ETA: 4:04 - loss: 0.6867 - acc: 0.5477
2368/4849 [=============>................] - ETA: 3:57 - loss: 0.6868 - acc: 0.5481
2432/4849 [==============>...............] - ETA: 3:51 - loss: 0.6861 - acc: 0.5518
2496/4849 [==============>...............] - ETA: 3:45 - loss: 0.6866 - acc: 0.5509
2560/4849 [==============>...............] - ETA: 3:39 - loss: 0.6869 - acc: 0.5504
2624/4849 [===============>..............] - ETA: 3:33 - loss: 0.6867 - acc: 0.5518
2688/4849 [===============>..............] - ETA: 3:26 - loss: 0.6862 - acc: 0.5536
2752/4849 [================>.............] - ETA: 3:20 - loss: 0.6860 - acc: 0.5538
2816/4849 [================>.............] - ETA: 3:14 - loss: 0.6855 - acc: 0.5550
2880/4849 [================>.............] - ETA: 3:08 - loss: 0.6852 - acc: 0.5566
2944/4849 [=================>............] - ETA: 3:02 - loss: 0.6860 - acc: 0.5540
3008/4849 [=================>............] - ETA: 2:56 - loss: 0.6860 - acc: 0.5542
3072/4849 [==================>...........] - ETA: 2:49 - loss: 0.6856 - acc: 0.5563
3136/4849 [==================>...........] - ETA: 2:43 - loss: 0.6854 - acc: 0.5571
3200/4849 [==================>...........] - ETA: 2:37 - loss: 0.6858 - acc: 0.5559
3264/4849 [===================>..........] - ETA: 2:31 - loss: 0.6846 - acc: 0.5588
3328/4849 [===================>..........] - ETA: 2:25 - loss: 0.6846 - acc: 0.5589
3392/4849 [===================>..........] - ETA: 2:19 - loss: 0.6841 - acc: 0.5598
3456/4849 [====================>.........] - ETA: 2:12 - loss: 0.6837 - acc: 0.5599
3520/4849 [====================>.........] - ETA: 2:06 - loss: 0.6837 - acc: 0.5594
3584/4849 [=====================>........] - ETA: 2:00 - loss: 0.6836 - acc: 0.5594
3648/4849 [=====================>........] - ETA: 1:54 - loss: 0.6831 - acc: 0.5606
3712/4849 [=====================>........] - ETA: 1:48 - loss: 0.6831 - acc: 0.5598
3776/4849 [======================>.......] - ETA: 1:42 - loss: 0.6825 - acc: 0.5612
3840/4849 [======================>.......] - ETA: 1:36 - loss: 0.6825 - acc: 0.5609
3904/4849 [=======================>......] - ETA: 1:30 - loss: 0.6833 - acc: 0.5620
3968/4849 [=======================>......] - ETA: 1:24 - loss: 0.6835 - acc: 0.5610
4032/4849 [=======================>......] - ETA: 1:17 - loss: 0.6846 - acc: 0.5590
4096/4849 [========================>.....] - ETA: 1:11 - loss: 0.6850 - acc: 0.5586
4160/4849 [========================>.....] - ETA: 1:05 - loss: 0.6841 - acc: 0.5608
4224/4849 [=========================>....] - ETA: 59s - loss: 0.6843 - acc: 0.5592 
4288/4849 [=========================>....] - ETA: 53s - loss: 0.6840 - acc: 0.5592
4352/4849 [=========================>....] - ETA: 47s - loss: 0.6837 - acc: 0.5604
4416/4849 [==========================>...] - ETA: 41s - loss: 0.6832 - acc: 0.5616
4480/4849 [==========================>...] - ETA: 35s - loss: 0.6828 - acc: 0.5629
4544/4849 [===========================>..] - ETA: 29s - loss: 0.6834 - acc: 0.5627
4608/4849 [===========================>..] - ETA: 22s - loss: 0.6827 - acc: 0.5640
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6828 - acc: 0.5644
4736/4849 [============================>.] - ETA: 10s - loss: 0.6824 - acc: 0.5655
4800/4849 [============================>.] - ETA: 4s - loss: 0.6818 - acc: 0.5671 
4849/4849 [==============================] - 479s 99ms/step - loss: 0.6815 - acc: 0.5684 - val_loss: 0.6838 - val_acc: 0.5603

Epoch 00004: val_acc did not improve from 0.56030
Epoch 5/10

  64/4849 [..............................] - ETA: 6:42 - loss: 0.6743 - acc: 0.5625
 128/4849 [..............................] - ETA: 6:53 - loss: 0.6867 - acc: 0.5547
 192/4849 [>.............................] - ETA: 7:05 - loss: 0.6874 - acc: 0.5625
 256/4849 [>.............................] - ETA: 6:53 - loss: 0.6833 - acc: 0.5820
 320/4849 [>.............................] - ETA: 6:57 - loss: 0.6858 - acc: 0.5719
 384/4849 [=>............................] - ETA: 6:54 - loss: 0.6845 - acc: 0.5677
 448/4849 [=>............................] - ETA: 6:49 - loss: 0.6832 - acc: 0.5647
 512/4849 [==>...........................] - ETA: 6:43 - loss: 0.6808 - acc: 0.5684
 576/4849 [==>...........................] - ETA: 6:37 - loss: 0.6829 - acc: 0.5642
 640/4849 [==>...........................] - ETA: 6:33 - loss: 0.6895 - acc: 0.5516
 704/4849 [===>..........................] - ETA: 6:29 - loss: 0.6907 - acc: 0.5469
 768/4849 [===>..........................] - ETA: 6:24 - loss: 0.6906 - acc: 0.5469
 832/4849 [====>.........................] - ETA: 6:18 - loss: 0.6904 - acc: 0.5517
 896/4849 [====>.........................] - ETA: 6:13 - loss: 0.6925 - acc: 0.5480
 960/4849 [====>.........................] - ETA: 6:05 - loss: 0.6926 - acc: 0.5490
1024/4849 [=====>........................] - ETA: 5:58 - loss: 0.6904 - acc: 0.5527
1088/4849 [=====>........................] - ETA: 5:53 - loss: 0.6892 - acc: 0.5551
1152/4849 [======>.......................] - ETA: 5:47 - loss: 0.6879 - acc: 0.5582
1216/4849 [======>.......................] - ETA: 5:41 - loss: 0.6869 - acc: 0.5625
1280/4849 [======>.......................] - ETA: 5:35 - loss: 0.6861 - acc: 0.5625
1344/4849 [=======>......................] - ETA: 5:29 - loss: 0.6851 - acc: 0.5647
1408/4849 [=======>......................] - ETA: 5:22 - loss: 0.6857 - acc: 0.5618
1472/4849 [========>.....................] - ETA: 5:16 - loss: 0.6848 - acc: 0.5618
1536/4849 [========>.....................] - ETA: 5:09 - loss: 0.6843 - acc: 0.5625
1600/4849 [========>.....................] - ETA: 5:03 - loss: 0.6834 - acc: 0.5675
1664/4849 [=========>....................] - ETA: 4:57 - loss: 0.6868 - acc: 0.5619
1728/4849 [=========>....................] - ETA: 4:50 - loss: 0.6863 - acc: 0.5648
1792/4849 [==========>...................] - ETA: 4:44 - loss: 0.6852 - acc: 0.5653
1856/4849 [==========>...................] - ETA: 4:37 - loss: 0.6846 - acc: 0.5668
1920/4849 [==========>...................] - ETA: 4:31 - loss: 0.6849 - acc: 0.5656
1984/4849 [===========>..................] - ETA: 4:25 - loss: 0.6840 - acc: 0.5675
2048/4849 [===========>..................] - ETA: 4:18 - loss: 0.6846 - acc: 0.5654
2112/4849 [============>.................] - ETA: 4:12 - loss: 0.6853 - acc: 0.5639
2176/4849 [============>.................] - ETA: 4:06 - loss: 0.6866 - acc: 0.5611
2240/4849 [============>.................] - ETA: 3:59 - loss: 0.6868 - acc: 0.5612
2304/4849 [=============>................] - ETA: 3:53 - loss: 0.6856 - acc: 0.5625
2368/4849 [=============>................] - ETA: 3:47 - loss: 0.6851 - acc: 0.5642
2432/4849 [==============>...............] - ETA: 3:41 - loss: 0.6856 - acc: 0.5617
2496/4849 [==============>...............] - ETA: 3:35 - loss: 0.6857 - acc: 0.5625
2560/4849 [==============>...............] - ETA: 3:29 - loss: 0.6853 - acc: 0.5625
2624/4849 [===============>..............] - ETA: 3:23 - loss: 0.6851 - acc: 0.5629
2688/4849 [===============>..............] - ETA: 3:16 - loss: 0.6843 - acc: 0.5658
2752/4849 [================>.............] - ETA: 3:10 - loss: 0.6844 - acc: 0.5647
2816/4849 [================>.............] - ETA: 3:04 - loss: 0.6837 - acc: 0.5653
2880/4849 [================>.............] - ETA: 2:58 - loss: 0.6834 - acc: 0.5670
2944/4849 [=================>............] - ETA: 2:52 - loss: 0.6834 - acc: 0.5669
3008/4849 [=================>............] - ETA: 2:46 - loss: 0.6832 - acc: 0.5688
3072/4849 [==================>...........] - ETA: 2:40 - loss: 0.6829 - acc: 0.5687
3136/4849 [==================>...........] - ETA: 2:34 - loss: 0.6833 - acc: 0.5660
3200/4849 [==================>...........] - ETA: 2:28 - loss: 0.6829 - acc: 0.5669
3264/4849 [===================>..........] - ETA: 2:22 - loss: 0.6831 - acc: 0.5646
3328/4849 [===================>..........] - ETA: 2:16 - loss: 0.6830 - acc: 0.5640
3392/4849 [===================>..........] - ETA: 2:11 - loss: 0.6824 - acc: 0.5646
3456/4849 [====================>.........] - ETA: 2:05 - loss: 0.6831 - acc: 0.5637
3520/4849 [====================>.........] - ETA: 1:59 - loss: 0.6827 - acc: 0.5645
3584/4849 [=====================>........] - ETA: 1:53 - loss: 0.6824 - acc: 0.5647
3648/4849 [=====================>........] - ETA: 1:47 - loss: 0.6818 - acc: 0.5661
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6814 - acc: 0.5679
3776/4849 [======================>.......] - ETA: 1:36 - loss: 0.6813 - acc: 0.5691
3840/4849 [======================>.......] - ETA: 1:30 - loss: 0.6814 - acc: 0.5690
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6816 - acc: 0.5684
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6812 - acc: 0.5685
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6819 - acc: 0.5682
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6827 - acc: 0.5662
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6826 - acc: 0.5651
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6819 - acc: 0.5668 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6814 - acc: 0.5674
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6809 - acc: 0.5678
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6813 - acc: 0.5661
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6813 - acc: 0.5658
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6811 - acc: 0.5671
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6811 - acc: 0.5675
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6809 - acc: 0.5687
4736/4849 [============================>.] - ETA: 10s - loss: 0.6806 - acc: 0.5699
4800/4849 [============================>.] - ETA: 4s - loss: 0.6802 - acc: 0.5706 
4849/4849 [==============================] - 446s 92ms/step - loss: 0.6804 - acc: 0.5706 - val_loss: 0.7040 - val_acc: 0.5028

Epoch 00005: val_acc did not improve from 0.56030
Epoch 6/10

  64/4849 [..............................] - ETA: 6:47 - loss: 0.6978 - acc: 0.5625
 128/4849 [..............................] - ETA: 6:39 - loss: 0.6923 - acc: 0.5703
 192/4849 [>.............................] - ETA: 6:45 - loss: 0.6754 - acc: 0.6146
 256/4849 [>.............................] - ETA: 6:43 - loss: 0.6823 - acc: 0.5938
 320/4849 [>.............................] - ETA: 6:36 - loss: 0.6755 - acc: 0.6062
 384/4849 [=>............................] - ETA: 6:32 - loss: 0.6770 - acc: 0.5911
 448/4849 [=>............................] - ETA: 6:26 - loss: 0.6702 - acc: 0.6027
 512/4849 [==>...........................] - ETA: 6:21 - loss: 0.6772 - acc: 0.5879
 576/4849 [==>...........................] - ETA: 6:18 - loss: 0.6770 - acc: 0.5885
 640/4849 [==>...........................] - ETA: 6:14 - loss: 0.6797 - acc: 0.5813
 704/4849 [===>..........................] - ETA: 6:10 - loss: 0.6785 - acc: 0.5824
 768/4849 [===>..........................] - ETA: 6:06 - loss: 0.6778 - acc: 0.5846
 832/4849 [====>.........................] - ETA: 6:01 - loss: 0.6784 - acc: 0.5841
 896/4849 [====>.........................] - ETA: 5:57 - loss: 0.6780 - acc: 0.5882
 960/4849 [====>.........................] - ETA: 5:52 - loss: 0.6797 - acc: 0.5885
1024/4849 [=====>........................] - ETA: 5:47 - loss: 0.6772 - acc: 0.5928
1088/4849 [=====>........................] - ETA: 5:43 - loss: 0.6788 - acc: 0.5864
1152/4849 [======>.......................] - ETA: 5:37 - loss: 0.6766 - acc: 0.5894
1216/4849 [======>.......................] - ETA: 5:31 - loss: 0.6777 - acc: 0.5847
1280/4849 [======>.......................] - ETA: 5:25 - loss: 0.6740 - acc: 0.5883
1344/4849 [=======>......................] - ETA: 5:21 - loss: 0.6740 - acc: 0.5863
1408/4849 [=======>......................] - ETA: 5:16 - loss: 0.6745 - acc: 0.5874
1472/4849 [========>.....................] - ETA: 5:11 - loss: 0.6724 - acc: 0.5917
1536/4849 [========>.....................] - ETA: 5:05 - loss: 0.6726 - acc: 0.5898
1600/4849 [========>.....................] - ETA: 5:00 - loss: 0.6736 - acc: 0.5887
1664/4849 [=========>....................] - ETA: 4:55 - loss: 0.6739 - acc: 0.5883
1728/4849 [=========>....................] - ETA: 4:48 - loss: 0.6739 - acc: 0.5856
1792/4849 [==========>...................] - ETA: 4:43 - loss: 0.6740 - acc: 0.5854
1856/4849 [==========>...................] - ETA: 4:38 - loss: 0.6737 - acc: 0.5867
1920/4849 [==========>...................] - ETA: 4:32 - loss: 0.6771 - acc: 0.5807
1984/4849 [===========>..................] - ETA: 4:26 - loss: 0.6766 - acc: 0.5822
2048/4849 [===========>..................] - ETA: 4:21 - loss: 0.6764 - acc: 0.5835
2112/4849 [============>.................] - ETA: 4:15 - loss: 0.6764 - acc: 0.5819
2176/4849 [============>.................] - ETA: 4:09 - loss: 0.6758 - acc: 0.5827
2240/4849 [============>.................] - ETA: 4:03 - loss: 0.6755 - acc: 0.5835
2304/4849 [=============>................] - ETA: 3:58 - loss: 0.6747 - acc: 0.5859
2368/4849 [=============>................] - ETA: 3:52 - loss: 0.6751 - acc: 0.5845
2432/4849 [==============>...............] - ETA: 3:46 - loss: 0.6750 - acc: 0.5839
2496/4849 [==============>...............] - ETA: 3:41 - loss: 0.6761 - acc: 0.5801
2560/4849 [==============>...............] - ETA: 3:35 - loss: 0.6755 - acc: 0.5793
2624/4849 [===============>..............] - ETA: 3:29 - loss: 0.6756 - acc: 0.5793
2688/4849 [===============>..............] - ETA: 3:23 - loss: 0.6762 - acc: 0.5778
2752/4849 [================>.............] - ETA: 3:17 - loss: 0.6760 - acc: 0.5803
2816/4849 [================>.............] - ETA: 3:12 - loss: 0.6757 - acc: 0.5810
2880/4849 [================>.............] - ETA: 3:06 - loss: 0.6759 - acc: 0.5816
2944/4849 [=================>............] - ETA: 3:00 - loss: 0.6766 - acc: 0.5815
3008/4849 [=================>............] - ETA: 2:54 - loss: 0.6770 - acc: 0.5805
3072/4849 [==================>...........] - ETA: 2:48 - loss: 0.6767 - acc: 0.5807
3136/4849 [==================>...........] - ETA: 2:43 - loss: 0.6758 - acc: 0.5829
3200/4849 [==================>...........] - ETA: 2:36 - loss: 0.6758 - acc: 0.5819
3264/4849 [===================>..........] - ETA: 2:31 - loss: 0.6767 - acc: 0.5809
3328/4849 [===================>..........] - ETA: 2:25 - loss: 0.6763 - acc: 0.5805
3392/4849 [===================>..........] - ETA: 2:18 - loss: 0.6765 - acc: 0.5790
3456/4849 [====================>.........] - ETA: 2:12 - loss: 0.6757 - acc: 0.5810
3520/4849 [====================>.........] - ETA: 2:06 - loss: 0.6760 - acc: 0.5813
3584/4849 [=====================>........] - ETA: 2:00 - loss: 0.6761 - acc: 0.5809
3648/4849 [=====================>........] - ETA: 1:54 - loss: 0.6759 - acc: 0.5814
3712/4849 [=====================>........] - ETA: 1:48 - loss: 0.6753 - acc: 0.5816
3776/4849 [======================>.......] - ETA: 1:42 - loss: 0.6757 - acc: 0.5810
3840/4849 [======================>.......] - ETA: 1:36 - loss: 0.6754 - acc: 0.5813
3904/4849 [=======================>......] - ETA: 1:30 - loss: 0.6748 - acc: 0.5820
3968/4849 [=======================>......] - ETA: 1:24 - loss: 0.6747 - acc: 0.5822
4032/4849 [=======================>......] - ETA: 1:18 - loss: 0.6746 - acc: 0.5828
4096/4849 [========================>.....] - ETA: 1:12 - loss: 0.6747 - acc: 0.5813
4160/4849 [========================>.....] - ETA: 1:06 - loss: 0.6750 - acc: 0.5815
4224/4849 [=========================>....] - ETA: 1:00 - loss: 0.6754 - acc: 0.5807
4288/4849 [=========================>....] - ETA: 54s - loss: 0.6754 - acc: 0.5805 
4352/4849 [=========================>....] - ETA: 47s - loss: 0.6754 - acc: 0.5807
4416/4849 [==========================>...] - ETA: 41s - loss: 0.6762 - acc: 0.5786
4480/4849 [==========================>...] - ETA: 35s - loss: 0.6761 - acc: 0.5795
4544/4849 [===========================>..] - ETA: 29s - loss: 0.6763 - acc: 0.5797
4608/4849 [===========================>..] - ETA: 23s - loss: 0.6767 - acc: 0.5786
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6763 - acc: 0.5788
4736/4849 [============================>.] - ETA: 10s - loss: 0.6757 - acc: 0.5800
4800/4849 [============================>.] - ETA: 4s - loss: 0.6756 - acc: 0.5808 
4849/4849 [==============================] - 490s 101ms/step - loss: 0.6758 - acc: 0.5797 - val_loss: 0.6866 - val_acc: 0.5659

Epoch 00006: val_acc improved from 0.56030 to 0.56586, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window19/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 7/10

  64/4849 [..............................] - ETA: 8:28 - loss: 0.7440 - acc: 0.4531
 128/4849 [..............................] - ETA: 8:21 - loss: 0.7235 - acc: 0.4688
 192/4849 [>.............................] - ETA: 8:13 - loss: 0.7188 - acc: 0.4740
 256/4849 [>.............................] - ETA: 8:09 - loss: 0.7155 - acc: 0.4922
 320/4849 [>.............................] - ETA: 8:05 - loss: 0.7046 - acc: 0.5062
 384/4849 [=>............................] - ETA: 7:56 - loss: 0.6950 - acc: 0.5130
 448/4849 [=>............................] - ETA: 7:56 - loss: 0.6915 - acc: 0.5223
 512/4849 [==>...........................] - ETA: 7:46 - loss: 0.6948 - acc: 0.5156
 576/4849 [==>...........................] - ETA: 7:39 - loss: 0.6921 - acc: 0.5226
 640/4849 [==>...........................] - ETA: 7:31 - loss: 0.6911 - acc: 0.5250
 704/4849 [===>..........................] - ETA: 7:25 - loss: 0.6882 - acc: 0.5327
 768/4849 [===>..........................] - ETA: 7:22 - loss: 0.6874 - acc: 0.5378
 832/4849 [====>.........................] - ETA: 7:17 - loss: 0.6868 - acc: 0.5433
 896/4849 [====>.........................] - ETA: 7:10 - loss: 0.6832 - acc: 0.5525
 960/4849 [====>.........................] - ETA: 7:02 - loss: 0.6827 - acc: 0.5531
1024/4849 [=====>........................] - ETA: 6:55 - loss: 0.6814 - acc: 0.5566
1088/4849 [=====>........................] - ETA: 6:49 - loss: 0.6819 - acc: 0.5570
1152/4849 [======>.......................] - ETA: 6:45 - loss: 0.6830 - acc: 0.5547
1216/4849 [======>.......................] - ETA: 6:38 - loss: 0.6835 - acc: 0.5559
1280/4849 [======>.......................] - ETA: 6:30 - loss: 0.6824 - acc: 0.5594
1344/4849 [=======>......................] - ETA: 6:23 - loss: 0.6813 - acc: 0.5580
1408/4849 [=======>......................] - ETA: 6:16 - loss: 0.6809 - acc: 0.5618
1472/4849 [========>.....................] - ETA: 6:09 - loss: 0.6818 - acc: 0.5618
1536/4849 [========>.....................] - ETA: 6:01 - loss: 0.6808 - acc: 0.5632
1600/4849 [========>.....................] - ETA: 5:54 - loss: 0.6814 - acc: 0.5644
1664/4849 [=========>....................] - ETA: 5:46 - loss: 0.6808 - acc: 0.5673
1728/4849 [=========>....................] - ETA: 5:39 - loss: 0.6816 - acc: 0.5648
1792/4849 [==========>...................] - ETA: 5:31 - loss: 0.6817 - acc: 0.5625
1856/4849 [==========>...................] - ETA: 5:24 - loss: 0.6820 - acc: 0.5620
1920/4849 [==========>...................] - ETA: 5:17 - loss: 0.6826 - acc: 0.5599
1984/4849 [===========>..................] - ETA: 5:10 - loss: 0.6821 - acc: 0.5595
2048/4849 [===========>..................] - ETA: 5:03 - loss: 0.6812 - acc: 0.5615
2112/4849 [============>.................] - ETA: 4:56 - loss: 0.6816 - acc: 0.5597
2176/4849 [============>.................] - ETA: 4:48 - loss: 0.6821 - acc: 0.5584
2240/4849 [============>.................] - ETA: 4:42 - loss: 0.6823 - acc: 0.5567
2304/4849 [=============>................] - ETA: 4:35 - loss: 0.6812 - acc: 0.5573
2368/4849 [=============>................] - ETA: 4:28 - loss: 0.6812 - acc: 0.5587
2432/4849 [==============>...............] - ETA: 4:21 - loss: 0.6801 - acc: 0.5617
2496/4849 [==============>...............] - ETA: 4:14 - loss: 0.6793 - acc: 0.5641
2560/4849 [==============>...............] - ETA: 4:06 - loss: 0.6795 - acc: 0.5645
2624/4849 [===============>..............] - ETA: 4:00 - loss: 0.6788 - acc: 0.5671
2688/4849 [===============>..............] - ETA: 3:52 - loss: 0.6785 - acc: 0.5670
2752/4849 [================>.............] - ETA: 3:46 - loss: 0.6784 - acc: 0.5676
2816/4849 [================>.............] - ETA: 3:39 - loss: 0.6779 - acc: 0.5692
2880/4849 [================>.............] - ETA: 3:32 - loss: 0.6772 - acc: 0.5705
2944/4849 [=================>............] - ETA: 3:25 - loss: 0.6773 - acc: 0.5703
3008/4849 [=================>............] - ETA: 3:18 - loss: 0.6774 - acc: 0.5711
3072/4849 [==================>...........] - ETA: 3:11 - loss: 0.6775 - acc: 0.5703
3136/4849 [==================>...........] - ETA: 3:04 - loss: 0.6776 - acc: 0.5695
3200/4849 [==================>...........] - ETA: 2:57 - loss: 0.6778 - acc: 0.5697
3264/4849 [===================>..........] - ETA: 2:50 - loss: 0.6771 - acc: 0.5702
3328/4849 [===================>..........] - ETA: 2:43 - loss: 0.6758 - acc: 0.5712
3392/4849 [===================>..........] - ETA: 2:36 - loss: 0.6765 - acc: 0.5705
3456/4849 [====================>.........] - ETA: 2:29 - loss: 0.6778 - acc: 0.5697
3520/4849 [====================>.........] - ETA: 2:22 - loss: 0.6775 - acc: 0.5705
3584/4849 [=====================>........] - ETA: 2:15 - loss: 0.6779 - acc: 0.5706
3648/4849 [=====================>........] - ETA: 2:08 - loss: 0.6778 - acc: 0.5713
3712/4849 [=====================>........] - ETA: 2:01 - loss: 0.6777 - acc: 0.5714
3776/4849 [======================>.......] - ETA: 1:54 - loss: 0.6784 - acc: 0.5704
3840/4849 [======================>.......] - ETA: 1:47 - loss: 0.6784 - acc: 0.5703
3904/4849 [=======================>......] - ETA: 1:40 - loss: 0.6780 - acc: 0.5704
3968/4849 [=======================>......] - ETA: 1:34 - loss: 0.6773 - acc: 0.5716
4032/4849 [=======================>......] - ETA: 1:27 - loss: 0.6764 - acc: 0.5732
4096/4849 [========================>.....] - ETA: 1:20 - loss: 0.6775 - acc: 0.5708
4160/4849 [========================>.....] - ETA: 1:13 - loss: 0.6775 - acc: 0.5709
4224/4849 [=========================>....] - ETA: 1:06 - loss: 0.6779 - acc: 0.5703
4288/4849 [=========================>....] - ETA: 59s - loss: 0.6780 - acc: 0.5707 
4352/4849 [=========================>....] - ETA: 52s - loss: 0.6776 - acc: 0.5719
4416/4849 [==========================>...] - ETA: 46s - loss: 0.6773 - acc: 0.5734
4480/4849 [==========================>...] - ETA: 39s - loss: 0.6774 - acc: 0.5730
4544/4849 [===========================>..] - ETA: 32s - loss: 0.6774 - acc: 0.5733
4608/4849 [===========================>..] - ETA: 25s - loss: 0.6774 - acc: 0.5738
4672/4849 [===========================>..] - ETA: 18s - loss: 0.6775 - acc: 0.5732
4736/4849 [============================>.] - ETA: 12s - loss: 0.6769 - acc: 0.5743
4800/4849 [============================>.] - ETA: 5s - loss: 0.6775 - acc: 0.5735 
4849/4849 [==============================] - 534s 110ms/step - loss: 0.6774 - acc: 0.5737 - val_loss: 0.6972 - val_acc: 0.5436

Epoch 00007: val_acc did not improve from 0.56586
Epoch 8/10

  64/4849 [..............................] - ETA: 7:50 - loss: 0.6571 - acc: 0.5938
 128/4849 [..............................] - ETA: 7:50 - loss: 0.6492 - acc: 0.6328
 192/4849 [>.............................] - ETA: 7:33 - loss: 0.6577 - acc: 0.5990
 256/4849 [>.............................] - ETA: 7:32 - loss: 0.6721 - acc: 0.5742
 320/4849 [>.............................] - ETA: 7:31 - loss: 0.6658 - acc: 0.5938
 384/4849 [=>............................] - ETA: 7:26 - loss: 0.6602 - acc: 0.6120
 448/4849 [=>............................] - ETA: 7:22 - loss: 0.6654 - acc: 0.6049
 512/4849 [==>...........................] - ETA: 7:16 - loss: 0.6683 - acc: 0.5996
 576/4849 [==>...........................] - ETA: 7:11 - loss: 0.6695 - acc: 0.5920
 640/4849 [==>...........................] - ETA: 7:05 - loss: 0.6665 - acc: 0.5953
 704/4849 [===>..........................] - ETA: 6:59 - loss: 0.6651 - acc: 0.5966
 768/4849 [===>..........................] - ETA: 6:53 - loss: 0.6701 - acc: 0.5872
 832/4849 [====>.........................] - ETA: 6:49 - loss: 0.6722 - acc: 0.5853
 896/4849 [====>.........................] - ETA: 6:43 - loss: 0.6758 - acc: 0.5815
 960/4849 [====>.........................] - ETA: 6:36 - loss: 0.6743 - acc: 0.5844
1024/4849 [=====>........................] - ETA: 6:27 - loss: 0.6736 - acc: 0.5859
1088/4849 [=====>........................] - ETA: 6:21 - loss: 0.6760 - acc: 0.5818
1152/4849 [======>.......................] - ETA: 6:13 - loss: 0.6737 - acc: 0.5859
1216/4849 [======>.......................] - ETA: 6:06 - loss: 0.6705 - acc: 0.5921
1280/4849 [======>.......................] - ETA: 6:00 - loss: 0.6713 - acc: 0.5898
1344/4849 [=======>......................] - ETA: 5:53 - loss: 0.6704 - acc: 0.5908
1408/4849 [=======>......................] - ETA: 5:47 - loss: 0.6716 - acc: 0.5895
1472/4849 [========>.....................] - ETA: 5:39 - loss: 0.6717 - acc: 0.5883
1536/4849 [========>.....................] - ETA: 5:32 - loss: 0.6707 - acc: 0.5898
1600/4849 [========>.....................] - ETA: 5:25 - loss: 0.6699 - acc: 0.5900
1664/4849 [=========>....................] - ETA: 5:18 - loss: 0.6710 - acc: 0.5907
1728/4849 [=========>....................] - ETA: 5:10 - loss: 0.6738 - acc: 0.5851
1792/4849 [==========>...................] - ETA: 5:03 - loss: 0.6729 - acc: 0.5871
1856/4849 [==========>...................] - ETA: 4:58 - loss: 0.6750 - acc: 0.5846
1920/4849 [==========>...................] - ETA: 4:51 - loss: 0.6748 - acc: 0.5828
1984/4849 [===========>..................] - ETA: 4:44 - loss: 0.6755 - acc: 0.5801
2048/4849 [===========>..................] - ETA: 4:37 - loss: 0.6755 - acc: 0.5811
2112/4849 [============>.................] - ETA: 4:31 - loss: 0.6746 - acc: 0.5833
2176/4849 [============>.................] - ETA: 4:25 - loss: 0.6753 - acc: 0.5818
2240/4849 [============>.................] - ETA: 4:18 - loss: 0.6754 - acc: 0.5813
2304/4849 [=============>................] - ETA: 4:12 - loss: 0.6766 - acc: 0.5803
2368/4849 [=============>................] - ETA: 4:06 - loss: 0.6773 - acc: 0.5781
2432/4849 [==============>...............] - ETA: 3:59 - loss: 0.6778 - acc: 0.5777
2496/4849 [==============>...............] - ETA: 3:52 - loss: 0.6792 - acc: 0.5749
2560/4849 [==============>...............] - ETA: 3:45 - loss: 0.6794 - acc: 0.5750
2624/4849 [===============>..............] - ETA: 3:39 - loss: 0.6791 - acc: 0.5762
2688/4849 [===============>..............] - ETA: 3:32 - loss: 0.6791 - acc: 0.5755
2752/4849 [================>.............] - ETA: 3:25 - loss: 0.6792 - acc: 0.5752
2816/4849 [================>.............] - ETA: 3:19 - loss: 0.6791 - acc: 0.5760
2880/4849 [================>.............] - ETA: 3:12 - loss: 0.6784 - acc: 0.5771
2944/4849 [=================>............] - ETA: 3:06 - loss: 0.6779 - acc: 0.5778
3008/4849 [=================>............] - ETA: 2:59 - loss: 0.6778 - acc: 0.5788
3072/4849 [==================>...........] - ETA: 2:53 - loss: 0.6775 - acc: 0.5804
3136/4849 [==================>...........] - ETA: 2:47 - loss: 0.6767 - acc: 0.5823
3200/4849 [==================>...........] - ETA: 2:41 - loss: 0.6761 - acc: 0.5834
3264/4849 [===================>..........] - ETA: 2:35 - loss: 0.6765 - acc: 0.5824
3328/4849 [===================>..........] - ETA: 2:28 - loss: 0.6762 - acc: 0.5820
3392/4849 [===================>..........] - ETA: 2:22 - loss: 0.6759 - acc: 0.5820
3456/4849 [====================>.........] - ETA: 2:16 - loss: 0.6763 - acc: 0.5819
3520/4849 [====================>.........] - ETA: 2:09 - loss: 0.6764 - acc: 0.5821
3584/4849 [=====================>........] - ETA: 2:03 - loss: 0.6761 - acc: 0.5831
3648/4849 [=====================>........] - ETA: 1:57 - loss: 0.6758 - acc: 0.5844
3712/4849 [=====================>........] - ETA: 1:50 - loss: 0.6756 - acc: 0.5849
3776/4849 [======================>.......] - ETA: 1:44 - loss: 0.6758 - acc: 0.5853
3840/4849 [======================>.......] - ETA: 1:38 - loss: 0.6756 - acc: 0.5859
3904/4849 [=======================>......] - ETA: 1:32 - loss: 0.6750 - acc: 0.5873
3968/4849 [=======================>......] - ETA: 1:25 - loss: 0.6764 - acc: 0.5854
4032/4849 [=======================>......] - ETA: 1:19 - loss: 0.6771 - acc: 0.5848
4096/4849 [========================>.....] - ETA: 1:13 - loss: 0.6776 - acc: 0.5835
4160/4849 [========================>.....] - ETA: 1:07 - loss: 0.6776 - acc: 0.5841
4224/4849 [=========================>....] - ETA: 1:00 - loss: 0.6783 - acc: 0.5826
4288/4849 [=========================>....] - ETA: 54s - loss: 0.6785 - acc: 0.5823 
4352/4849 [=========================>....] - ETA: 48s - loss: 0.6783 - acc: 0.5832
4416/4849 [==========================>...] - ETA: 42s - loss: 0.6775 - acc: 0.5840
4480/4849 [==========================>...] - ETA: 35s - loss: 0.6772 - acc: 0.5839
4544/4849 [===========================>..] - ETA: 29s - loss: 0.6770 - acc: 0.5843
4608/4849 [===========================>..] - ETA: 23s - loss: 0.6770 - acc: 0.5846
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6774 - acc: 0.5837
4736/4849 [============================>.] - ETA: 11s - loss: 0.6773 - acc: 0.5847
4800/4849 [============================>.] - ETA: 4s - loss: 0.6774 - acc: 0.5842 
4849/4849 [==============================] - 492s 101ms/step - loss: 0.6775 - acc: 0.5840 - val_loss: 0.6754 - val_acc: 0.5881

Epoch 00008: val_acc improved from 0.56586 to 0.58813, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window19/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 9/10

  64/4849 [..............................] - ETA: 7:52 - loss: 0.6601 - acc: 0.5781
 128/4849 [..............................] - ETA: 7:50 - loss: 0.6626 - acc: 0.5703
 192/4849 [>.............................] - ETA: 7:43 - loss: 0.6667 - acc: 0.5833
 256/4849 [>.............................] - ETA: 7:38 - loss: 0.6710 - acc: 0.5742
 320/4849 [>.............................] - ETA: 7:31 - loss: 0.6727 - acc: 0.5656
 384/4849 [=>............................] - ETA: 7:23 - loss: 0.6782 - acc: 0.5573
 448/4849 [=>............................] - ETA: 7:13 - loss: 0.6744 - acc: 0.5737
 512/4849 [==>...........................] - ETA: 7:06 - loss: 0.6763 - acc: 0.5742
 576/4849 [==>...........................] - ETA: 7:00 - loss: 0.6737 - acc: 0.5833
 640/4849 [==>...........................] - ETA: 6:52 - loss: 0.6753 - acc: 0.5766
 704/4849 [===>..........................] - ETA: 6:45 - loss: 0.6735 - acc: 0.5810
 768/4849 [===>..........................] - ETA: 6:38 - loss: 0.6741 - acc: 0.5859
 832/4849 [====>.........................] - ETA: 6:30 - loss: 0.6763 - acc: 0.5853
 896/4849 [====>.........................] - ETA: 6:24 - loss: 0.6746 - acc: 0.5848
 960/4849 [====>.........................] - ETA: 6:18 - loss: 0.6741 - acc: 0.5833
1024/4849 [=====>........................] - ETA: 6:12 - loss: 0.6724 - acc: 0.5869
1088/4849 [=====>........................] - ETA: 6:06 - loss: 0.6740 - acc: 0.5846
1152/4849 [======>.......................] - ETA: 6:00 - loss: 0.6713 - acc: 0.5903
1216/4849 [======>.......................] - ETA: 5:54 - loss: 0.6721 - acc: 0.5905
1280/4849 [======>.......................] - ETA: 5:47 - loss: 0.6733 - acc: 0.5836
1344/4849 [=======>......................] - ETA: 5:41 - loss: 0.6728 - acc: 0.5841
1408/4849 [=======>......................] - ETA: 5:35 - loss: 0.6710 - acc: 0.5859
1472/4849 [========>.....................] - ETA: 5:30 - loss: 0.6714 - acc: 0.5870
1536/4849 [========>.....................] - ETA: 5:24 - loss: 0.6709 - acc: 0.5872
1600/4849 [========>.....................] - ETA: 5:18 - loss: 0.6718 - acc: 0.5856
1664/4849 [=========>....................] - ETA: 5:11 - loss: 0.6704 - acc: 0.5901
1728/4849 [=========>....................] - ETA: 5:06 - loss: 0.6687 - acc: 0.5926
1792/4849 [==========>...................] - ETA: 5:00 - loss: 0.6710 - acc: 0.5887
1856/4849 [==========>...................] - ETA: 4:55 - loss: 0.6731 - acc: 0.5841
1920/4849 [==========>...................] - ETA: 4:48 - loss: 0.6733 - acc: 0.5844
1984/4849 [===========>..................] - ETA: 4:42 - loss: 0.6730 - acc: 0.5837
2048/4849 [===========>..................] - ETA: 4:35 - loss: 0.6731 - acc: 0.5845
2112/4849 [============>.................] - ETA: 4:29 - loss: 0.6733 - acc: 0.5838
2176/4849 [============>.................] - ETA: 4:22 - loss: 0.6747 - acc: 0.5827
2240/4849 [============>.................] - ETA: 4:17 - loss: 0.6754 - acc: 0.5813
2304/4849 [=============>................] - ETA: 4:10 - loss: 0.6745 - acc: 0.5833
2368/4849 [=============>................] - ETA: 4:04 - loss: 0.6748 - acc: 0.5828
2432/4849 [==============>...............] - ETA: 3:57 - loss: 0.6741 - acc: 0.5843
2496/4849 [==============>...............] - ETA: 3:51 - loss: 0.6741 - acc: 0.5845
2560/4849 [==============>...............] - ETA: 3:45 - loss: 0.6733 - acc: 0.5859
2624/4849 [===============>..............] - ETA: 3:38 - loss: 0.6721 - acc: 0.5896
2688/4849 [===============>..............] - ETA: 3:32 - loss: 0.6724 - acc: 0.5897
2752/4849 [================>.............] - ETA: 3:26 - loss: 0.6722 - acc: 0.5887
2816/4849 [================>.............] - ETA: 3:19 - loss: 0.6712 - acc: 0.5909
2880/4849 [================>.............] - ETA: 3:13 - loss: 0.6708 - acc: 0.5903
2944/4849 [=================>............] - ETA: 3:07 - loss: 0.6704 - acc: 0.5904
3008/4849 [=================>............] - ETA: 3:00 - loss: 0.6718 - acc: 0.5874
3072/4849 [==================>...........] - ETA: 2:54 - loss: 0.6714 - acc: 0.5879
3136/4849 [==================>...........] - ETA: 2:48 - loss: 0.6710 - acc: 0.5880
3200/4849 [==================>...........] - ETA: 2:42 - loss: 0.6708 - acc: 0.5887
3264/4849 [===================>..........] - ETA: 2:36 - loss: 0.6720 - acc: 0.5867
3328/4849 [===================>..........] - ETA: 2:29 - loss: 0.6715 - acc: 0.5871
3392/4849 [===================>..........] - ETA: 2:23 - loss: 0.6725 - acc: 0.5849
3456/4849 [====================>.........] - ETA: 2:17 - loss: 0.6725 - acc: 0.5856
3520/4849 [====================>.........] - ETA: 2:11 - loss: 0.6716 - acc: 0.5869
3584/4849 [=====================>........] - ETA: 2:05 - loss: 0.6718 - acc: 0.5862
3648/4849 [=====================>........] - ETA: 1:58 - loss: 0.6713 - acc: 0.5861
3712/4849 [=====================>........] - ETA: 1:52 - loss: 0.6714 - acc: 0.5865
3776/4849 [======================>.......] - ETA: 1:46 - loss: 0.6715 - acc: 0.5866
3840/4849 [======================>.......] - ETA: 1:39 - loss: 0.6713 - acc: 0.5870
3904/4849 [=======================>......] - ETA: 1:33 - loss: 0.6718 - acc: 0.5853
3968/4849 [=======================>......] - ETA: 1:27 - loss: 0.6718 - acc: 0.5849
4032/4849 [=======================>......] - ETA: 1:20 - loss: 0.6713 - acc: 0.5858
4096/4849 [========================>.....] - ETA: 1:14 - loss: 0.6717 - acc: 0.5857
4160/4849 [========================>.....] - ETA: 1:08 - loss: 0.6716 - acc: 0.5863
4224/4849 [=========================>....] - ETA: 1:01 - loss: 0.6718 - acc: 0.5859
4288/4849 [=========================>....] - ETA: 55s - loss: 0.6713 - acc: 0.5868 
4352/4849 [=========================>....] - ETA: 49s - loss: 0.6715 - acc: 0.5864
4416/4849 [==========================>...] - ETA: 42s - loss: 0.6715 - acc: 0.5863
4480/4849 [==========================>...] - ETA: 36s - loss: 0.6714 - acc: 0.5866
4544/4849 [===========================>..] - ETA: 30s - loss: 0.6720 - acc: 0.5863
4608/4849 [===========================>..] - ETA: 23s - loss: 0.6720 - acc: 0.5862
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6719 - acc: 0.5865
4736/4849 [============================>.] - ETA: 11s - loss: 0.6718 - acc: 0.5868
4800/4849 [============================>.] - ETA: 4s - loss: 0.6713 - acc: 0.5885 
4849/4849 [==============================] - 500s 103ms/step - loss: 0.6712 - acc: 0.5884 - val_loss: 0.6840 - val_acc: 0.5547

Epoch 00009: val_acc did not improve from 0.58813
Epoch 10/10

  64/4849 [..............................] - ETA: 7:40 - loss: 0.6977 - acc: 0.5625
 128/4849 [..............................] - ETA: 7:51 - loss: 0.6738 - acc: 0.5625
 192/4849 [>.............................] - ETA: 7:40 - loss: 0.6727 - acc: 0.5833
 256/4849 [>.............................] - ETA: 7:39 - loss: 0.6733 - acc: 0.5859
 320/4849 [>.............................] - ETA: 7:30 - loss: 0.6764 - acc: 0.5719
 384/4849 [=>............................] - ETA: 7:24 - loss: 0.6776 - acc: 0.5599
 448/4849 [=>............................] - ETA: 7:15 - loss: 0.6750 - acc: 0.5670
 512/4849 [==>...........................] - ETA: 7:11 - loss: 0.6797 - acc: 0.5625
 576/4849 [==>...........................] - ETA: 7:07 - loss: 0.6754 - acc: 0.5642
 640/4849 [==>...........................] - ETA: 7:04 - loss: 0.6767 - acc: 0.5609
 704/4849 [===>..........................] - ETA: 6:58 - loss: 0.6747 - acc: 0.5696
 768/4849 [===>..........................] - ETA: 6:52 - loss: 0.6751 - acc: 0.5742
 832/4849 [====>.........................] - ETA: 6:43 - loss: 0.6739 - acc: 0.5769
 896/4849 [====>.........................] - ETA: 6:37 - loss: 0.6713 - acc: 0.5837
 960/4849 [====>.........................] - ETA: 6:31 - loss: 0.6751 - acc: 0.5802
1024/4849 [=====>........................] - ETA: 6:26 - loss: 0.6775 - acc: 0.5752
1088/4849 [=====>........................] - ETA: 6:21 - loss: 0.6758 - acc: 0.5772
1152/4849 [======>.......................] - ETA: 6:15 - loss: 0.6769 - acc: 0.5781
1216/4849 [======>.......................] - ETA: 6:07 - loss: 0.6765 - acc: 0.5814
1280/4849 [======>.......................] - ETA: 6:00 - loss: 0.6756 - acc: 0.5797
1344/4849 [=======>......................] - ETA: 5:53 - loss: 0.6743 - acc: 0.5841
1408/4849 [=======>......................] - ETA: 5:46 - loss: 0.6734 - acc: 0.5831
1472/4849 [========>.....................] - ETA: 5:39 - loss: 0.6732 - acc: 0.5842
1536/4849 [========>.....................] - ETA: 5:33 - loss: 0.6737 - acc: 0.5807
1600/4849 [========>.....................] - ETA: 5:26 - loss: 0.6742 - acc: 0.5794
1664/4849 [=========>....................] - ETA: 5:19 - loss: 0.6725 - acc: 0.5781
1728/4849 [=========>....................] - ETA: 5:14 - loss: 0.6737 - acc: 0.5793
1792/4849 [==========>...................] - ETA: 5:07 - loss: 0.6744 - acc: 0.5792
1856/4849 [==========>...................] - ETA: 5:01 - loss: 0.6743 - acc: 0.5808
1920/4849 [==========>...................] - ETA: 4:55 - loss: 0.6740 - acc: 0.5818
1984/4849 [===========>..................] - ETA: 4:49 - loss: 0.6736 - acc: 0.5811
2048/4849 [===========>..................] - ETA: 4:42 - loss: 0.6729 - acc: 0.5811
2112/4849 [============>.................] - ETA: 4:36 - loss: 0.6722 - acc: 0.5819
2176/4849 [============>.................] - ETA: 4:29 - loss: 0.6726 - acc: 0.5800
2240/4849 [============>.................] - ETA: 4:22 - loss: 0.6733 - acc: 0.5781
2304/4849 [=============>................] - ETA: 4:16 - loss: 0.6738 - acc: 0.5764
2368/4849 [=============>................] - ETA: 4:09 - loss: 0.6736 - acc: 0.5769
2432/4849 [==============>...............] - ETA: 4:03 - loss: 0.6742 - acc: 0.5769
2496/4849 [==============>...............] - ETA: 3:57 - loss: 0.6734 - acc: 0.5781
2560/4849 [==============>...............] - ETA: 3:50 - loss: 0.6742 - acc: 0.5766
2624/4849 [===============>..............] - ETA: 3:43 - loss: 0.6755 - acc: 0.5751
2688/4849 [===============>..............] - ETA: 3:37 - loss: 0.6753 - acc: 0.5763
2752/4849 [================>.............] - ETA: 3:30 - loss: 0.6754 - acc: 0.5774
2816/4849 [================>.............] - ETA: 3:23 - loss: 0.6749 - acc: 0.5774
2880/4849 [================>.............] - ETA: 3:17 - loss: 0.6743 - acc: 0.5785
2944/4849 [=================>............] - ETA: 3:10 - loss: 0.6743 - acc: 0.5788
3008/4849 [=================>............] - ETA: 3:04 - loss: 0.6749 - acc: 0.5775
3072/4849 [==================>...........] - ETA: 2:58 - loss: 0.6751 - acc: 0.5778
3136/4849 [==================>...........] - ETA: 2:51 - loss: 0.6754 - acc: 0.5768
3200/4849 [==================>...........] - ETA: 2:45 - loss: 0.6755 - acc: 0.5778
3264/4849 [===================>..........] - ETA: 2:38 - loss: 0.6751 - acc: 0.5794
3328/4849 [===================>..........] - ETA: 2:32 - loss: 0.6748 - acc: 0.5799
3392/4849 [===================>..........] - ETA: 2:25 - loss: 0.6747 - acc: 0.5790
3456/4849 [====================>.........] - ETA: 2:19 - loss: 0.6743 - acc: 0.5793
3520/4849 [====================>.........] - ETA: 2:12 - loss: 0.6741 - acc: 0.5807
3584/4849 [=====================>........] - ETA: 2:06 - loss: 0.6736 - acc: 0.5809
3648/4849 [=====================>........] - ETA: 1:59 - loss: 0.6739 - acc: 0.5798
3712/4849 [=====================>........] - ETA: 1:53 - loss: 0.6739 - acc: 0.5808
3776/4849 [======================>.......] - ETA: 1:46 - loss: 0.6734 - acc: 0.5818
3840/4849 [======================>.......] - ETA: 1:40 - loss: 0.6731 - acc: 0.5818
3904/4849 [=======================>......] - ETA: 1:34 - loss: 0.6731 - acc: 0.5820
3968/4849 [=======================>......] - ETA: 1:27 - loss: 0.6736 - acc: 0.5822
4032/4849 [=======================>......] - ETA: 1:21 - loss: 0.6735 - acc: 0.5826
4096/4849 [========================>.....] - ETA: 1:14 - loss: 0.6728 - acc: 0.5837
4160/4849 [========================>.....] - ETA: 1:08 - loss: 0.6731 - acc: 0.5837
4224/4849 [=========================>....] - ETA: 1:02 - loss: 0.6727 - acc: 0.5845
4288/4849 [=========================>....] - ETA: 55s - loss: 0.6729 - acc: 0.5844 
4352/4849 [=========================>....] - ETA: 49s - loss: 0.6729 - acc: 0.5841
4416/4849 [==========================>...] - ETA: 43s - loss: 0.6723 - acc: 0.5851
4480/4849 [==========================>...] - ETA: 36s - loss: 0.6726 - acc: 0.5850
4544/4849 [===========================>..] - ETA: 30s - loss: 0.6716 - acc: 0.5860
4608/4849 [===========================>..] - ETA: 23s - loss: 0.6718 - acc: 0.5857
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6707 - acc: 0.5875
4736/4849 [============================>.] - ETA: 11s - loss: 0.6712 - acc: 0.5872
4800/4849 [============================>.] - ETA: 4s - loss: 0.6706 - acc: 0.5883 
4849/4849 [==============================] - 498s 103ms/step - loss: 0.6708 - acc: 0.5886 - val_loss: 0.6970 - val_acc: 0.5622

Epoch 00010: val_acc did not improve from 0.58813
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f940062dd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f940062dd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f94002719d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f94002719d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beae76350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9beae76350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94001ca710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94001ca710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9400172450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9400172450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e07dcc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e07dcc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94001ca310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94001ca310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9400185090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9400185090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9400165f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9400165f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94001723d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94001723d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e060b050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e060b050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9400167a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9400167a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94000bd610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94000bd610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93e053db10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93e053db10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93e0523d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93e0523d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e07abb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e07abb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93e053de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93e053de90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e07a36d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e07a36d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93e0220310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93e0220310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93e0204c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93e0204c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e01beb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e01beb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93e0220710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93e0220710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e00e4a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e00e4a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93c0612210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93c0612210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93c057ead0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93c057ead0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e0204a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93e0204a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93c0612a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93c0612a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93c0500e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93c0500e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93c02c02d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93c02c02d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93c02ad4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93c02ad4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93c03b9fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93c03b9fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93c02c0150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93c02c0150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93c022c310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93c022c310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93c063c4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93c063c4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93a8701b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93a8701b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93a87ee250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93a87ee250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93c0214d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93c0214d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9090519850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9090519850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92842fca50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92842fca50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93a83a9910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93a83a9910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f955c6daa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f955c6daa90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92842fc410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92842fc410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92842fd990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92842fd990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93a8139190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93a8139190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93a81cc9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93a81cc9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93a8289910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93a8289910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93a8139690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93a8139690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93a809cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93a809cf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93806b8f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93806b8f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93a8098050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93a8098050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93806b7c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93806b7c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93a83ea290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93a83ea290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9380504150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9380504150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93802c4450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93802c4450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f938032e850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f938032e850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93802241d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93802241d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9380217050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9380217050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93802aa190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93802aa190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f938021cd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f938021cd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9380218310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9380218310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f936078d890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f936078d890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f938020f550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f938020f550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f936068abd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f936068abd0>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 2:08
 128/1348 [=>............................] - ETA: 1:13
 192/1348 [===>..........................] - ETA: 57s 
 256/1348 [====>.........................] - ETA: 46s
 320/1348 [======>.......................] - ETA: 38s
 384/1348 [=======>......................] - ETA: 33s
 448/1348 [========>.....................] - ETA: 29s
 512/1348 [==========>...................] - ETA: 26s
 576/1348 [===========>..................] - ETA: 23s
 640/1348 [=============>................] - ETA: 20s
 704/1348 [==============>...............] - ETA: 18s
 768/1348 [================>.............] - ETA: 15s
 832/1348 [=================>............] - ETA: 14s
 896/1348 [==================>...........] - ETA: 12s
 960/1348 [====================>.........] - ETA: 10s
1024/1348 [=====================>........] - ETA: 8s 
1088/1348 [=======================>......] - ETA: 6s
1152/1348 [========================>.....] - ETA: 5s
1216/1348 [==========================>...] - ETA: 3s
1280/1348 [===========================>..] - ETA: 1s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 34s 26ms/step
loss: 0.6781270394339406
acc: 0.5630563798219584
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f90904d0310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f90904d0310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f92105fcd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f92105fcd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92105fcf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92105fcf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9210645c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9210645c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f406afd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f406afd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9400242950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9400242950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9210645510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9210645510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94003438d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94003438d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f940053f4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f940053f4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90903eef10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90903eef10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94003c7150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94003c7150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f940053fb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f940053fb10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90904a5a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90904a5a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94003bc050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94003bc050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f909015df50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f909015df50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90901c7790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90901c7790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90903e9410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90903e9410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90903d6690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90903d6690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9080657310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9080657310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90806a7f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90806a7f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f908060e850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f908060e850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9090158150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9090158150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90803dd590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90803dd590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90805c5110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90805c5110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90802e9e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90802e9e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9080254950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9080254950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90805b7a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90805b7a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90802d9050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90802d9050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90447c5f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90447c5f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9044718a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9044718a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90447cd250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90447cd250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f908009bcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f908009bcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9080055f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9080055f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90444c9cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90444c9cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90445e3d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90445e3d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90444d3bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90444d3bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90444c9210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90444c9210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90442dfb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90442dfb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90441da2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90441da2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9044157e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9044157e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9044191310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9044191310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90441da090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90441da090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9044177ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9044177ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90441e2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90441e2a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90205bfd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90205bfd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9090206d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9090206d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90441e2750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90441e2750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f4063e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f4063e1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8f40640510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8f40640510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f40413f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f40413f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f4053fad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f4053fad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9020552ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9020552ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f404e8290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f404e8290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8f40413550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8f40413550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f207c4510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f207c4510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f401d7d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f401d7d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8f40413450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8f40413450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f4004e990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f4004e990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8f205ce350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8f205ce350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f205b3fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f205b3fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f4006c410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f4006c410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8f205cec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8f205cec90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f2051b0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f2051b0d0>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 39:07 - loss: 0.7532 - acc: 0.4688
 128/4849 [..............................] - ETA: 23:17 - loss: 0.7576 - acc: 0.5156
 192/4849 [>.............................] - ETA: 18:01 - loss: 0.7474 - acc: 0.5365
 256/4849 [>.............................] - ETA: 15:17 - loss: 0.7253 - acc: 0.5469
 320/4849 [>.............................] - ETA: 13:42 - loss: 0.7348 - acc: 0.5375
 384/4849 [=>............................] - ETA: 12:29 - loss: 0.7253 - acc: 0.5469
 448/4849 [=>............................] - ETA: 11:34 - loss: 0.7348 - acc: 0.5402
 512/4849 [==>...........................] - ETA: 10:52 - loss: 0.7359 - acc: 0.5449
 576/4849 [==>...........................] - ETA: 10:20 - loss: 0.7322 - acc: 0.5469
 640/4849 [==>...........................] - ETA: 9:50 - loss: 0.7297 - acc: 0.5484 
 704/4849 [===>..........................] - ETA: 9:26 - loss: 0.7254 - acc: 0.5497
 768/4849 [===>..........................] - ETA: 9:07 - loss: 0.7273 - acc: 0.5417
 832/4849 [====>.........................] - ETA: 8:48 - loss: 0.7343 - acc: 0.5385
 896/4849 [====>.........................] - ETA: 8:34 - loss: 0.7373 - acc: 0.5368
 960/4849 [====>.........................] - ETA: 8:18 - loss: 0.7379 - acc: 0.5333
1024/4849 [=====>........................] - ETA: 8:03 - loss: 0.7375 - acc: 0.5352
1088/4849 [=====>........................] - ETA: 7:48 - loss: 0.7412 - acc: 0.5303
1152/4849 [======>.......................] - ETA: 7:35 - loss: 0.7399 - acc: 0.5295
1216/4849 [======>.......................] - ETA: 7:24 - loss: 0.7432 - acc: 0.5206
1280/4849 [======>.......................] - ETA: 7:13 - loss: 0.7432 - acc: 0.5188
1344/4849 [=======>......................] - ETA: 7:03 - loss: 0.7415 - acc: 0.5201
1408/4849 [=======>......................] - ETA: 6:51 - loss: 0.7441 - acc: 0.5156
1472/4849 [========>.....................] - ETA: 6:41 - loss: 0.7405 - acc: 0.5204
1536/4849 [========>.....................] - ETA: 6:31 - loss: 0.7411 - acc: 0.5202
1600/4849 [========>.....................] - ETA: 6:21 - loss: 0.7415 - acc: 0.5175
1664/4849 [=========>....................] - ETA: 6:11 - loss: 0.7404 - acc: 0.5168
1728/4849 [=========>....................] - ETA: 6:02 - loss: 0.7391 - acc: 0.5203
1792/4849 [==========>...................] - ETA: 5:53 - loss: 0.7397 - acc: 0.5162
1856/4849 [==========>...................] - ETA: 5:45 - loss: 0.7376 - acc: 0.5183
1920/4849 [==========>...................] - ETA: 5:37 - loss: 0.7361 - acc: 0.5177
1984/4849 [===========>..................] - ETA: 5:28 - loss: 0.7351 - acc: 0.5171
2048/4849 [===========>..................] - ETA: 5:20 - loss: 0.7340 - acc: 0.5166
2112/4849 [============>.................] - ETA: 5:12 - loss: 0.7340 - acc: 0.5156
2176/4849 [============>.................] - ETA: 5:04 - loss: 0.7346 - acc: 0.5147
2240/4849 [============>.................] - ETA: 4:55 - loss: 0.7335 - acc: 0.5156
2304/4849 [=============>................] - ETA: 4:47 - loss: 0.7356 - acc: 0.5117
2368/4849 [=============>................] - ETA: 4:40 - loss: 0.7356 - acc: 0.5114
2432/4849 [==============>...............] - ETA: 4:32 - loss: 0.7349 - acc: 0.5127
2496/4849 [==============>...............] - ETA: 4:24 - loss: 0.7327 - acc: 0.5172
2560/4849 [==============>...............] - ETA: 4:16 - loss: 0.7312 - acc: 0.5184
2624/4849 [===============>..............] - ETA: 4:09 - loss: 0.7305 - acc: 0.5183
2688/4849 [===============>..............] - ETA: 4:01 - loss: 0.7283 - acc: 0.5212
2752/4849 [================>.............] - ETA: 3:53 - loss: 0.7280 - acc: 0.5225
2816/4849 [================>.............] - ETA: 3:46 - loss: 0.7279 - acc: 0.5220
2880/4849 [================>.............] - ETA: 3:38 - loss: 0.7278 - acc: 0.5219
2944/4849 [=================>............] - ETA: 3:30 - loss: 0.7267 - acc: 0.5228
3008/4849 [=================>............] - ETA: 3:23 - loss: 0.7259 - acc: 0.5239
3072/4849 [==================>...........] - ETA: 3:15 - loss: 0.7277 - acc: 0.5221
3136/4849 [==================>...........] - ETA: 3:08 - loss: 0.7281 - acc: 0.5220
3200/4849 [==================>...........] - ETA: 3:01 - loss: 0.7282 - acc: 0.5225
3264/4849 [===================>..........] - ETA: 2:54 - loss: 0.7275 - acc: 0.5224
3328/4849 [===================>..........] - ETA: 2:46 - loss: 0.7271 - acc: 0.5225
3392/4849 [===================>..........] - ETA: 2:39 - loss: 0.7271 - acc: 0.5224
3456/4849 [====================>.........] - ETA: 2:32 - loss: 0.7274 - acc: 0.5220
3520/4849 [====================>.........] - ETA: 2:25 - loss: 0.7268 - acc: 0.5222
3584/4849 [=====================>........] - ETA: 2:18 - loss: 0.7265 - acc: 0.5223
3648/4849 [=====================>........] - ETA: 2:11 - loss: 0.7258 - acc: 0.5228
3712/4849 [=====================>........] - ETA: 2:04 - loss: 0.7255 - acc: 0.5224
3776/4849 [======================>.......] - ETA: 1:56 - loss: 0.7259 - acc: 0.5217
3840/4849 [======================>.......] - ETA: 1:49 - loss: 0.7254 - acc: 0.5227
3904/4849 [=======================>......] - ETA: 1:42 - loss: 0.7249 - acc: 0.5231
3968/4849 [=======================>......] - ETA: 1:35 - loss: 0.7250 - acc: 0.5222
4032/4849 [=======================>......] - ETA: 1:28 - loss: 0.7239 - acc: 0.5236
4096/4849 [========================>.....] - ETA: 1:21 - loss: 0.7241 - acc: 0.5227
4160/4849 [========================>.....] - ETA: 1:14 - loss: 0.7234 - acc: 0.5233
4224/4849 [=========================>....] - ETA: 1:07 - loss: 0.7228 - acc: 0.5234
4288/4849 [=========================>....] - ETA: 1:00 - loss: 0.7224 - acc: 0.5236
4352/4849 [=========================>....] - ETA: 53s - loss: 0.7227 - acc: 0.5223 
4416/4849 [==========================>...] - ETA: 46s - loss: 0.7226 - acc: 0.5215
4480/4849 [==========================>...] - ETA: 39s - loss: 0.7224 - acc: 0.5219
4544/4849 [===========================>..] - ETA: 32s - loss: 0.7223 - acc: 0.5224
4608/4849 [===========================>..] - ETA: 26s - loss: 0.7215 - acc: 0.5237
4672/4849 [===========================>..] - ETA: 19s - loss: 0.7212 - acc: 0.5229
4736/4849 [============================>.] - ETA: 12s - loss: 0.7213 - acc: 0.5224
4800/4849 [============================>.] - ETA: 5s - loss: 0.7208 - acc: 0.5227 
4849/4849 [==============================] - 544s 112ms/step - loss: 0.7211 - acc: 0.5211 - val_loss: 0.6955 - val_acc: 0.5380

Epoch 00001: val_acc improved from -inf to 0.53803, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window20/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 8:13 - loss: 0.6934 - acc: 0.5312
 128/4849 [..............................] - ETA: 8:06 - loss: 0.6803 - acc: 0.5703
 192/4849 [>.............................] - ETA: 7:57 - loss: 0.6907 - acc: 0.5417
 256/4849 [>.............................] - ETA: 7:58 - loss: 0.6925 - acc: 0.5312
 320/4849 [>.............................] - ETA: 7:44 - loss: 0.6958 - acc: 0.5281
 384/4849 [=>............................] - ETA: 7:36 - loss: 0.6859 - acc: 0.5547
 448/4849 [=>............................] - ETA: 7:25 - loss: 0.6878 - acc: 0.5513
 512/4849 [==>...........................] - ETA: 7:20 - loss: 0.6920 - acc: 0.5527
 576/4849 [==>...........................] - ETA: 7:13 - loss: 0.7027 - acc: 0.5399
 640/4849 [==>...........................] - ETA: 7:06 - loss: 0.6988 - acc: 0.5453
 704/4849 [===>..........................] - ETA: 6:59 - loss: 0.6974 - acc: 0.5497
 768/4849 [===>..........................] - ETA: 6:55 - loss: 0.6974 - acc: 0.5469
 832/4849 [====>.........................] - ETA: 6:48 - loss: 0.6942 - acc: 0.5469
 896/4849 [====>.........................] - ETA: 6:46 - loss: 0.6973 - acc: 0.5413
 960/4849 [====>.........................] - ETA: 6:41 - loss: 0.6980 - acc: 0.5396
1024/4849 [=====>........................] - ETA: 6:35 - loss: 0.7042 - acc: 0.5342
1088/4849 [=====>........................] - ETA: 6:29 - loss: 0.7005 - acc: 0.5404
1152/4849 [======>.......................] - ETA: 6:21 - loss: 0.6995 - acc: 0.5382
1216/4849 [======>.......................] - ETA: 6:14 - loss: 0.6996 - acc: 0.5362
1280/4849 [======>.......................] - ETA: 6:08 - loss: 0.7004 - acc: 0.5336
1344/4849 [=======>......................] - ETA: 6:00 - loss: 0.6975 - acc: 0.5394
1408/4849 [=======>......................] - ETA: 5:53 - loss: 0.6970 - acc: 0.5391
1472/4849 [========>.....................] - ETA: 5:46 - loss: 0.6979 - acc: 0.5380
1536/4849 [========>.....................] - ETA: 5:40 - loss: 0.6970 - acc: 0.5404
1600/4849 [========>.....................] - ETA: 5:33 - loss: 0.6974 - acc: 0.5381
1664/4849 [=========>....................] - ETA: 5:27 - loss: 0.6970 - acc: 0.5385
1728/4849 [=========>....................] - ETA: 5:20 - loss: 0.6948 - acc: 0.5405
1792/4849 [==========>...................] - ETA: 5:13 - loss: 0.6937 - acc: 0.5441
1856/4849 [==========>...................] - ETA: 5:07 - loss: 0.6927 - acc: 0.5469
1920/4849 [==========>...................] - ETA: 5:00 - loss: 0.6927 - acc: 0.5458
1984/4849 [===========>..................] - ETA: 4:53 - loss: 0.6919 - acc: 0.5444
2048/4849 [===========>..................] - ETA: 4:46 - loss: 0.6931 - acc: 0.5444
2112/4849 [============>.................] - ETA: 4:39 - loss: 0.6924 - acc: 0.5473
2176/4849 [============>.................] - ETA: 4:32 - loss: 0.6908 - acc: 0.5478
2240/4849 [============>.................] - ETA: 4:25 - loss: 0.6897 - acc: 0.5509
2304/4849 [=============>................] - ETA: 4:19 - loss: 0.6908 - acc: 0.5490
2368/4849 [=============>................] - ETA: 4:13 - loss: 0.6918 - acc: 0.5452
2432/4849 [==============>...............] - ETA: 4:06 - loss: 0.6922 - acc: 0.5444
2496/4849 [==============>...............] - ETA: 4:00 - loss: 0.6929 - acc: 0.5441
2560/4849 [==============>...............] - ETA: 3:53 - loss: 0.6935 - acc: 0.5426
2624/4849 [===============>..............] - ETA: 3:46 - loss: 0.6938 - acc: 0.5408
2688/4849 [===============>..............] - ETA: 3:40 - loss: 0.6942 - acc: 0.5402
2752/4849 [================>.............] - ETA: 3:33 - loss: 0.6944 - acc: 0.5403
2816/4849 [================>.............] - ETA: 3:27 - loss: 0.6954 - acc: 0.5401
2880/4849 [================>.............] - ETA: 3:20 - loss: 0.6960 - acc: 0.5392
2944/4849 [=================>............] - ETA: 3:13 - loss: 0.6955 - acc: 0.5408
3008/4849 [=================>............] - ETA: 3:07 - loss: 0.6965 - acc: 0.5399
3072/4849 [==================>...........] - ETA: 3:00 - loss: 0.6973 - acc: 0.5371
3136/4849 [==================>...........] - ETA: 2:54 - loss: 0.6970 - acc: 0.5379
3200/4849 [==================>...........] - ETA: 2:47 - loss: 0.6968 - acc: 0.5387
3264/4849 [===================>..........] - ETA: 2:41 - loss: 0.6967 - acc: 0.5389
3328/4849 [===================>..........] - ETA: 2:34 - loss: 0.6966 - acc: 0.5397
3392/4849 [===================>..........] - ETA: 2:27 - loss: 0.6964 - acc: 0.5395
3456/4849 [====================>.........] - ETA: 2:21 - loss: 0.6963 - acc: 0.5388
3520/4849 [====================>.........] - ETA: 2:14 - loss: 0.6969 - acc: 0.5384
3584/4849 [=====================>........] - ETA: 2:08 - loss: 0.6961 - acc: 0.5410
3648/4849 [=====================>........] - ETA: 2:01 - loss: 0.6959 - acc: 0.5417
3712/4849 [=====================>........] - ETA: 1:55 - loss: 0.6961 - acc: 0.5412
3776/4849 [======================>.......] - ETA: 1:48 - loss: 0.6962 - acc: 0.5413
3840/4849 [======================>.......] - ETA: 1:42 - loss: 0.6951 - acc: 0.5440
3904/4849 [=======================>......] - ETA: 1:35 - loss: 0.6952 - acc: 0.5443
3968/4849 [=======================>......] - ETA: 1:29 - loss: 0.6950 - acc: 0.5449
4032/4849 [=======================>......] - ETA: 1:22 - loss: 0.6940 - acc: 0.5469
4096/4849 [========================>.....] - ETA: 1:16 - loss: 0.6937 - acc: 0.5466
4160/4849 [========================>.....] - ETA: 1:09 - loss: 0.6936 - acc: 0.5462
4224/4849 [=========================>....] - ETA: 1:03 - loss: 0.6937 - acc: 0.5459
4288/4849 [=========================>....] - ETA: 57s - loss: 0.6941 - acc: 0.5450 
4352/4849 [=========================>....] - ETA: 50s - loss: 0.6946 - acc: 0.5446
4416/4849 [==========================>...] - ETA: 44s - loss: 0.6942 - acc: 0.5453
4480/4849 [==========================>...] - ETA: 37s - loss: 0.6946 - acc: 0.5458
4544/4849 [===========================>..] - ETA: 31s - loss: 0.6947 - acc: 0.5456
4608/4849 [===========================>..] - ETA: 24s - loss: 0.6946 - acc: 0.5458
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6951 - acc: 0.5445
4736/4849 [============================>.] - ETA: 11s - loss: 0.6958 - acc: 0.5431
4800/4849 [============================>.] - ETA: 4s - loss: 0.6956 - acc: 0.5431 
4849/4849 [==============================] - 510s 105ms/step - loss: 0.6961 - acc: 0.5426 - val_loss: 0.6900 - val_acc: 0.5232

Epoch 00002: val_acc did not improve from 0.53803
Epoch 3/10

  64/4849 [..............................] - ETA: 8:41 - loss: 0.6613 - acc: 0.5781
 128/4849 [..............................] - ETA: 8:29 - loss: 0.6577 - acc: 0.5859
 192/4849 [>.............................] - ETA: 8:16 - loss: 0.6705 - acc: 0.5729
 256/4849 [>.............................] - ETA: 8:03 - loss: 0.6833 - acc: 0.5430
 320/4849 [>.............................] - ETA: 7:55 - loss: 0.6820 - acc: 0.5437
 384/4849 [=>............................] - ETA: 7:46 - loss: 0.6827 - acc: 0.5495
 448/4849 [=>............................] - ETA: 7:37 - loss: 0.6854 - acc: 0.5357
 512/4849 [==>...........................] - ETA: 7:29 - loss: 0.6834 - acc: 0.5508
 576/4849 [==>...........................] - ETA: 7:20 - loss: 0.6849 - acc: 0.5434
 640/4849 [==>...........................] - ETA: 7:13 - loss: 0.6824 - acc: 0.5453
 704/4849 [===>..........................] - ETA: 7:05 - loss: 0.6870 - acc: 0.5398
 768/4849 [===>..........................] - ETA: 6:57 - loss: 0.6884 - acc: 0.5417
 832/4849 [====>.........................] - ETA: 6:50 - loss: 0.6918 - acc: 0.5445
 896/4849 [====>.........................] - ETA: 6:44 - loss: 0.6928 - acc: 0.5413
 960/4849 [====>.........................] - ETA: 6:37 - loss: 0.6918 - acc: 0.5427
1024/4849 [=====>........................] - ETA: 6:28 - loss: 0.6900 - acc: 0.5459
1088/4849 [=====>........................] - ETA: 6:21 - loss: 0.6894 - acc: 0.5450
1152/4849 [======>.......................] - ETA: 6:14 - loss: 0.6891 - acc: 0.5451
1216/4849 [======>.......................] - ETA: 6:07 - loss: 0.6905 - acc: 0.5428
1280/4849 [======>.......................] - ETA: 6:02 - loss: 0.6876 - acc: 0.5508
1344/4849 [=======>......................] - ETA: 5:55 - loss: 0.6884 - acc: 0.5491
1408/4849 [=======>......................] - ETA: 5:48 - loss: 0.6872 - acc: 0.5504
1472/4849 [========>.....................] - ETA: 5:41 - loss: 0.6880 - acc: 0.5476
1536/4849 [========>.....................] - ETA: 5:34 - loss: 0.6875 - acc: 0.5495
1600/4849 [========>.....................] - ETA: 5:29 - loss: 0.6876 - acc: 0.5481
1664/4849 [=========>....................] - ETA: 5:22 - loss: 0.6878 - acc: 0.5487
1728/4849 [=========>....................] - ETA: 5:15 - loss: 0.6895 - acc: 0.5434
1792/4849 [==========>...................] - ETA: 5:08 - loss: 0.6897 - acc: 0.5441
1856/4849 [==========>...................] - ETA: 5:02 - loss: 0.6894 - acc: 0.5458
1920/4849 [==========>...................] - ETA: 4:55 - loss: 0.6889 - acc: 0.5474
1984/4849 [===========>..................] - ETA: 4:49 - loss: 0.6887 - acc: 0.5484
2048/4849 [===========>..................] - ETA: 4:42 - loss: 0.6890 - acc: 0.5474
2112/4849 [============>.................] - ETA: 4:35 - loss: 0.6898 - acc: 0.5469
2176/4849 [============>.................] - ETA: 4:29 - loss: 0.6899 - acc: 0.5464
2240/4849 [============>.................] - ETA: 4:22 - loss: 0.6913 - acc: 0.5429
2304/4849 [=============>................] - ETA: 4:16 - loss: 0.6919 - acc: 0.5421
2368/4849 [=============>................] - ETA: 4:09 - loss: 0.6922 - acc: 0.5405
2432/4849 [==============>...............] - ETA: 4:03 - loss: 0.6920 - acc: 0.5391
2496/4849 [==============>...............] - ETA: 3:56 - loss: 0.6916 - acc: 0.5405
2560/4849 [==============>...............] - ETA: 3:50 - loss: 0.6911 - acc: 0.5410
2624/4849 [===============>..............] - ETA: 3:43 - loss: 0.6900 - acc: 0.5434
2688/4849 [===============>..............] - ETA: 3:37 - loss: 0.6890 - acc: 0.5465
2752/4849 [================>.............] - ETA: 3:31 - loss: 0.6894 - acc: 0.5469
2816/4849 [================>.............] - ETA: 3:24 - loss: 0.6900 - acc: 0.5455
2880/4849 [================>.............] - ETA: 3:18 - loss: 0.6900 - acc: 0.5451
2944/4849 [=================>............] - ETA: 3:11 - loss: 0.6889 - acc: 0.5465
3008/4849 [=================>............] - ETA: 3:05 - loss: 0.6894 - acc: 0.5459
3072/4849 [==================>...........] - ETA: 2:58 - loss: 0.6892 - acc: 0.5462
3136/4849 [==================>...........] - ETA: 2:52 - loss: 0.6895 - acc: 0.5450
3200/4849 [==================>...........] - ETA: 2:45 - loss: 0.6899 - acc: 0.5444
3264/4849 [===================>..........] - ETA: 2:39 - loss: 0.6887 - acc: 0.5460
3328/4849 [===================>..........] - ETA: 2:33 - loss: 0.6880 - acc: 0.5472
3392/4849 [===================>..........] - ETA: 2:26 - loss: 0.6878 - acc: 0.5483
3456/4849 [====================>.........] - ETA: 2:20 - loss: 0.6882 - acc: 0.5480
3520/4849 [====================>.........] - ETA: 2:14 - loss: 0.6872 - acc: 0.5494
3584/4849 [=====================>........] - ETA: 2:07 - loss: 0.6874 - acc: 0.5483
3648/4849 [=====================>........] - ETA: 2:01 - loss: 0.6879 - acc: 0.5480
3712/4849 [=====================>........] - ETA: 1:54 - loss: 0.6880 - acc: 0.5485
3776/4849 [======================>.......] - ETA: 1:48 - loss: 0.6877 - acc: 0.5493
3840/4849 [======================>.......] - ETA: 1:41 - loss: 0.6870 - acc: 0.5518
3904/4849 [=======================>......] - ETA: 1:35 - loss: 0.6873 - acc: 0.5512
3968/4849 [=======================>......] - ETA: 1:28 - loss: 0.6879 - acc: 0.5496
4032/4849 [=======================>......] - ETA: 1:22 - loss: 0.6872 - acc: 0.5513
4096/4849 [========================>.....] - ETA: 1:15 - loss: 0.6869 - acc: 0.5522
4160/4849 [========================>.....] - ETA: 1:09 - loss: 0.6868 - acc: 0.5519
4224/4849 [=========================>....] - ETA: 1:02 - loss: 0.6882 - acc: 0.5485
4288/4849 [=========================>....] - ETA: 56s - loss: 0.6885 - acc: 0.5478 
4352/4849 [=========================>....] - ETA: 49s - loss: 0.6887 - acc: 0.5485
4416/4849 [==========================>...] - ETA: 43s - loss: 0.6884 - acc: 0.5489
4480/4849 [==========================>...] - ETA: 37s - loss: 0.6887 - acc: 0.5484
4544/4849 [===========================>..] - ETA: 30s - loss: 0.6889 - acc: 0.5484
4608/4849 [===========================>..] - ETA: 24s - loss: 0.6885 - acc: 0.5490
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6883 - acc: 0.5499
4736/4849 [============================>.] - ETA: 11s - loss: 0.6885 - acc: 0.5494
4800/4849 [============================>.] - ETA: 4s - loss: 0.6885 - acc: 0.5502 
4849/4849 [==============================] - 505s 104ms/step - loss: 0.6883 - acc: 0.5508 - val_loss: 0.6944 - val_acc: 0.5046

Epoch 00003: val_acc did not improve from 0.53803
Epoch 4/10

  64/4849 [..............................] - ETA: 8:05 - loss: 0.6465 - acc: 0.6250
 128/4849 [..............................] - ETA: 7:54 - loss: 0.6724 - acc: 0.5781
 192/4849 [>.............................] - ETA: 7:35 - loss: 0.6879 - acc: 0.5521
 256/4849 [>.............................] - ETA: 7:41 - loss: 0.6948 - acc: 0.5391
 320/4849 [>.............................] - ETA: 7:30 - loss: 0.6873 - acc: 0.5594
 384/4849 [=>............................] - ETA: 7:24 - loss: 0.6851 - acc: 0.5573
 448/4849 [=>............................] - ETA: 7:21 - loss: 0.6778 - acc: 0.5759
 512/4849 [==>...........................] - ETA: 7:10 - loss: 0.6865 - acc: 0.5547
 576/4849 [==>...........................] - ETA: 7:06 - loss: 0.6822 - acc: 0.5677
 640/4849 [==>...........................] - ETA: 6:57 - loss: 0.6825 - acc: 0.5672
 704/4849 [===>..........................] - ETA: 6:52 - loss: 0.6830 - acc: 0.5611
 768/4849 [===>..........................] - ETA: 6:47 - loss: 0.6820 - acc: 0.5638
 832/4849 [====>.........................] - ETA: 6:39 - loss: 0.6848 - acc: 0.5589
 896/4849 [====>.........................] - ETA: 6:34 - loss: 0.6857 - acc: 0.5592
 960/4849 [====>.........................] - ETA: 6:28 - loss: 0.6836 - acc: 0.5625
1024/4849 [=====>........................] - ETA: 6:21 - loss: 0.6847 - acc: 0.5625
1088/4849 [=====>........................] - ETA: 6:16 - loss: 0.6816 - acc: 0.5680
1152/4849 [======>.......................] - ETA: 6:08 - loss: 0.6796 - acc: 0.5712
1216/4849 [======>.......................] - ETA: 6:02 - loss: 0.6792 - acc: 0.5724
1280/4849 [======>.......................] - ETA: 5:55 - loss: 0.6796 - acc: 0.5711
1344/4849 [=======>......................] - ETA: 5:49 - loss: 0.6796 - acc: 0.5722
1408/4849 [=======>......................] - ETA: 5:42 - loss: 0.6810 - acc: 0.5696
1472/4849 [========>.....................] - ETA: 5:36 - loss: 0.6811 - acc: 0.5679
1536/4849 [========>.....................] - ETA: 5:30 - loss: 0.6827 - acc: 0.5658
1600/4849 [========>.....................] - ETA: 5:23 - loss: 0.6830 - acc: 0.5663
1664/4849 [=========>....................] - ETA: 5:16 - loss: 0.6818 - acc: 0.5667
1728/4849 [=========>....................] - ETA: 5:10 - loss: 0.6816 - acc: 0.5677
1792/4849 [==========>...................] - ETA: 5:03 - loss: 0.6814 - acc: 0.5686
1856/4849 [==========>...................] - ETA: 4:58 - loss: 0.6820 - acc: 0.5673
1920/4849 [==========>...................] - ETA: 4:51 - loss: 0.6820 - acc: 0.5703
1984/4849 [===========>..................] - ETA: 4:45 - loss: 0.6833 - acc: 0.5665
2048/4849 [===========>..................] - ETA: 4:38 - loss: 0.6846 - acc: 0.5615
2112/4849 [============>.................] - ETA: 4:32 - loss: 0.6831 - acc: 0.5625
2176/4849 [============>.................] - ETA: 4:26 - loss: 0.6842 - acc: 0.5616
2240/4849 [============>.................] - ETA: 4:19 - loss: 0.6828 - acc: 0.5647
2304/4849 [=============>................] - ETA: 4:13 - loss: 0.6830 - acc: 0.5634
2368/4849 [=============>................] - ETA: 4:07 - loss: 0.6835 - acc: 0.5625
2432/4849 [==============>...............] - ETA: 4:01 - loss: 0.6843 - acc: 0.5617
2496/4849 [==============>...............] - ETA: 3:55 - loss: 0.6838 - acc: 0.5641
2560/4849 [==============>...............] - ETA: 3:48 - loss: 0.6834 - acc: 0.5648
2624/4849 [===============>..............] - ETA: 3:42 - loss: 0.6825 - acc: 0.5655
2688/4849 [===============>..............] - ETA: 3:35 - loss: 0.6832 - acc: 0.5658
2752/4849 [================>.............] - ETA: 3:29 - loss: 0.6828 - acc: 0.5665
2816/4849 [================>.............] - ETA: 3:22 - loss: 0.6828 - acc: 0.5668
2880/4849 [================>.............] - ETA: 3:16 - loss: 0.6838 - acc: 0.5646
2944/4849 [=================>............] - ETA: 3:09 - loss: 0.6842 - acc: 0.5652
3008/4849 [=================>............] - ETA: 3:03 - loss: 0.6851 - acc: 0.5642
3072/4849 [==================>...........] - ETA: 2:57 - loss: 0.6851 - acc: 0.5651
3136/4849 [==================>...........] - ETA: 2:50 - loss: 0.6854 - acc: 0.5654
3200/4849 [==================>...........] - ETA: 2:44 - loss: 0.6854 - acc: 0.5656
3264/4849 [===================>..........] - ETA: 2:38 - loss: 0.6858 - acc: 0.5640
3328/4849 [===================>..........] - ETA: 2:31 - loss: 0.6850 - acc: 0.5649
3392/4849 [===================>..........] - ETA: 2:25 - loss: 0.6846 - acc: 0.5649
3456/4849 [====================>.........] - ETA: 2:18 - loss: 0.6851 - acc: 0.5639
3520/4849 [====================>.........] - ETA: 2:12 - loss: 0.6848 - acc: 0.5648
3584/4849 [=====================>........] - ETA: 2:06 - loss: 0.6849 - acc: 0.5631
3648/4849 [=====================>........] - ETA: 1:59 - loss: 0.6849 - acc: 0.5633
3712/4849 [=====================>........] - ETA: 1:53 - loss: 0.6851 - acc: 0.5628
3776/4849 [======================>.......] - ETA: 1:46 - loss: 0.6854 - acc: 0.5612
3840/4849 [======================>.......] - ETA: 1:40 - loss: 0.6857 - acc: 0.5602
3904/4849 [=======================>......] - ETA: 1:33 - loss: 0.6855 - acc: 0.5617
3968/4849 [=======================>......] - ETA: 1:27 - loss: 0.6857 - acc: 0.5612
4032/4849 [=======================>......] - ETA: 1:21 - loss: 0.6852 - acc: 0.5620
4096/4849 [========================>.....] - ETA: 1:14 - loss: 0.6847 - acc: 0.5630
4160/4849 [========================>.....] - ETA: 1:08 - loss: 0.6843 - acc: 0.5649
4224/4849 [=========================>....] - ETA: 1:01 - loss: 0.6848 - acc: 0.5642
4288/4849 [=========================>....] - ETA: 55s - loss: 0.6851 - acc: 0.5630 
4352/4849 [=========================>....] - ETA: 49s - loss: 0.6851 - acc: 0.5634
4416/4849 [==========================>...] - ETA: 43s - loss: 0.6856 - acc: 0.5627
4480/4849 [==========================>...] - ETA: 36s - loss: 0.6855 - acc: 0.5627
4544/4849 [===========================>..] - ETA: 30s - loss: 0.6849 - acc: 0.5638
4608/4849 [===========================>..] - ETA: 23s - loss: 0.6851 - acc: 0.5636
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6849 - acc: 0.5642
4736/4849 [============================>.] - ETA: 11s - loss: 0.6843 - acc: 0.5650
4800/4849 [============================>.] - ETA: 4s - loss: 0.6840 - acc: 0.5648 
4849/4849 [==============================] - 497s 103ms/step - loss: 0.6838 - acc: 0.5655 - val_loss: 0.6866 - val_acc: 0.5232

Epoch 00004: val_acc did not improve from 0.53803
Epoch 5/10

  64/4849 [..............................] - ETA: 7:19 - loss: 0.6775 - acc: 0.5469
 128/4849 [..............................] - ETA: 7:46 - loss: 0.6600 - acc: 0.6094
 192/4849 [>.............................] - ETA: 7:39 - loss: 0.6760 - acc: 0.5833
 256/4849 [>.............................] - ETA: 7:44 - loss: 0.6883 - acc: 0.5703
 320/4849 [>.............................] - ETA: 7:41 - loss: 0.6871 - acc: 0.5781
 384/4849 [=>............................] - ETA: 7:33 - loss: 0.6878 - acc: 0.5807
 448/4849 [=>............................] - ETA: 7:28 - loss: 0.6844 - acc: 0.5804
 512/4849 [==>...........................] - ETA: 7:19 - loss: 0.6919 - acc: 0.5684
 576/4849 [==>...........................] - ETA: 7:10 - loss: 0.6941 - acc: 0.5608
 640/4849 [==>...........................] - ETA: 7:03 - loss: 0.6899 - acc: 0.5656
 704/4849 [===>..........................] - ETA: 6:56 - loss: 0.6855 - acc: 0.5710
 768/4849 [===>..........................] - ETA: 6:50 - loss: 0.6862 - acc: 0.5690
 832/4849 [====>.........................] - ETA: 6:43 - loss: 0.6837 - acc: 0.5757
 896/4849 [====>.........................] - ETA: 6:35 - loss: 0.6808 - acc: 0.5804
 960/4849 [====>.........................] - ETA: 6:28 - loss: 0.6799 - acc: 0.5813
1024/4849 [=====>........................] - ETA: 6:23 - loss: 0.6801 - acc: 0.5820
1088/4849 [=====>........................] - ETA: 6:15 - loss: 0.6805 - acc: 0.5809
1152/4849 [======>.......................] - ETA: 6:09 - loss: 0.6801 - acc: 0.5807
1216/4849 [======>.......................] - ETA: 6:02 - loss: 0.6809 - acc: 0.5806
1280/4849 [======>.......................] - ETA: 5:55 - loss: 0.6812 - acc: 0.5820
1344/4849 [=======>......................] - ETA: 5:49 - loss: 0.6812 - acc: 0.5811
1408/4849 [=======>......................] - ETA: 5:41 - loss: 0.6839 - acc: 0.5774
1472/4849 [========>.....................] - ETA: 5:34 - loss: 0.6846 - acc: 0.5781
1536/4849 [========>.....................] - ETA: 5:29 - loss: 0.6822 - acc: 0.5801
1600/4849 [========>.....................] - ETA: 5:23 - loss: 0.6817 - acc: 0.5825
1664/4849 [=========>....................] - ETA: 5:16 - loss: 0.6825 - acc: 0.5799
1728/4849 [=========>....................] - ETA: 5:11 - loss: 0.6819 - acc: 0.5775
1792/4849 [==========>...................] - ETA: 5:04 - loss: 0.6824 - acc: 0.5765
1856/4849 [==========>...................] - ETA: 4:57 - loss: 0.6828 - acc: 0.5744
1920/4849 [==========>...................] - ETA: 4:50 - loss: 0.6824 - acc: 0.5750
1984/4849 [===========>..................] - ETA: 4:44 - loss: 0.6826 - acc: 0.5726
2048/4849 [===========>..................] - ETA: 4:37 - loss: 0.6811 - acc: 0.5747
2112/4849 [============>.................] - ETA: 4:31 - loss: 0.6805 - acc: 0.5734
2176/4849 [============>.................] - ETA: 4:24 - loss: 0.6802 - acc: 0.5731
2240/4849 [============>.................] - ETA: 4:18 - loss: 0.6799 - acc: 0.5728
2304/4849 [=============>................] - ETA: 4:11 - loss: 0.6798 - acc: 0.5734
2368/4849 [=============>................] - ETA: 4:05 - loss: 0.6801 - acc: 0.5718
2432/4849 [==============>...............] - ETA: 3:58 - loss: 0.6801 - acc: 0.5724
2496/4849 [==============>...............] - ETA: 3:52 - loss: 0.6807 - acc: 0.5717
2560/4849 [==============>...............] - ETA: 3:45 - loss: 0.6803 - acc: 0.5715
2624/4849 [===============>..............] - ETA: 3:39 - loss: 0.6798 - acc: 0.5697
2688/4849 [===============>..............] - ETA: 3:33 - loss: 0.6803 - acc: 0.5692
2752/4849 [================>.............] - ETA: 3:27 - loss: 0.6802 - acc: 0.5683
2816/4849 [================>.............] - ETA: 3:21 - loss: 0.6802 - acc: 0.5685
2880/4849 [================>.............] - ETA: 3:14 - loss: 0.6804 - acc: 0.5681
2944/4849 [=================>............] - ETA: 3:08 - loss: 0.6800 - acc: 0.5679
3008/4849 [=================>............] - ETA: 3:02 - loss: 0.6803 - acc: 0.5682
3072/4849 [==================>...........] - ETA: 2:56 - loss: 0.6805 - acc: 0.5687
3136/4849 [==================>...........] - ETA: 2:50 - loss: 0.6806 - acc: 0.5682
3200/4849 [==================>...........] - ETA: 2:44 - loss: 0.6801 - acc: 0.5681
3264/4849 [===================>..........] - ETA: 2:37 - loss: 0.6802 - acc: 0.5680
3328/4849 [===================>..........] - ETA: 2:31 - loss: 0.6804 - acc: 0.5691
3392/4849 [===================>..........] - ETA: 2:24 - loss: 0.6801 - acc: 0.5699
3456/4849 [====================>.........] - ETA: 2:18 - loss: 0.6789 - acc: 0.5720
3520/4849 [====================>.........] - ETA: 2:12 - loss: 0.6780 - acc: 0.5739
3584/4849 [=====================>........] - ETA: 2:05 - loss: 0.6777 - acc: 0.5748
3648/4849 [=====================>........] - ETA: 1:59 - loss: 0.6779 - acc: 0.5751
3712/4849 [=====================>........] - ETA: 1:52 - loss: 0.6783 - acc: 0.5741
3776/4849 [======================>.......] - ETA: 1:46 - loss: 0.6784 - acc: 0.5749
3840/4849 [======================>.......] - ETA: 1:39 - loss: 0.6786 - acc: 0.5745
3904/4849 [=======================>......] - ETA: 1:33 - loss: 0.6782 - acc: 0.5743
3968/4849 [=======================>......] - ETA: 1:27 - loss: 0.6781 - acc: 0.5751
4032/4849 [=======================>......] - ETA: 1:20 - loss: 0.6774 - acc: 0.5771
4096/4849 [========================>.....] - ETA: 1:14 - loss: 0.6774 - acc: 0.5764
4160/4849 [========================>.....] - ETA: 1:08 - loss: 0.6779 - acc: 0.5762
4224/4849 [=========================>....] - ETA: 1:01 - loss: 0.6779 - acc: 0.5765
4288/4849 [=========================>....] - ETA: 55s - loss: 0.6779 - acc: 0.5758 
4352/4849 [=========================>....] - ETA: 49s - loss: 0.6783 - acc: 0.5742
4416/4849 [==========================>...] - ETA: 42s - loss: 0.6782 - acc: 0.5761
4480/4849 [==========================>...] - ETA: 36s - loss: 0.6780 - acc: 0.5768
4544/4849 [===========================>..] - ETA: 30s - loss: 0.6786 - acc: 0.5757
4608/4849 [===========================>..] - ETA: 23s - loss: 0.6788 - acc: 0.5751
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6792 - acc: 0.5738
4736/4849 [============================>.] - ETA: 11s - loss: 0.6787 - acc: 0.5750
4800/4849 [============================>.] - ETA: 4s - loss: 0.6792 - acc: 0.5750 
4849/4849 [==============================] - 498s 103ms/step - loss: 0.6798 - acc: 0.5741 - val_loss: 0.6891 - val_acc: 0.5288

Epoch 00005: val_acc did not improve from 0.53803
Epoch 6/10

  64/4849 [..............................] - ETA: 7:40 - loss: 0.6665 - acc: 0.6250
 128/4849 [..............................] - ETA: 7:58 - loss: 0.6824 - acc: 0.5625
 192/4849 [>.............................] - ETA: 8:02 - loss: 0.6784 - acc: 0.5781
 256/4849 [>.............................] - ETA: 7:51 - loss: 0.6728 - acc: 0.5781
 320/4849 [>.............................] - ETA: 7:53 - loss: 0.6709 - acc: 0.5875
 384/4849 [=>............................] - ETA: 7:44 - loss: 0.6729 - acc: 0.5807
 448/4849 [=>............................] - ETA: 7:41 - loss: 0.6757 - acc: 0.5692
 512/4849 [==>...........................] - ETA: 7:31 - loss: 0.6771 - acc: 0.5703
 576/4849 [==>...........................] - ETA: 7:23 - loss: 0.6763 - acc: 0.5694
 640/4849 [==>...........................] - ETA: 7:15 - loss: 0.6769 - acc: 0.5687
 704/4849 [===>..........................] - ETA: 7:06 - loss: 0.6786 - acc: 0.5625
 768/4849 [===>..........................] - ETA: 7:00 - loss: 0.6817 - acc: 0.5586
 832/4849 [====>.........................] - ETA: 6:53 - loss: 0.6793 - acc: 0.5637
 896/4849 [====>.........................] - ETA: 6:47 - loss: 0.6816 - acc: 0.5636
 960/4849 [====>.........................] - ETA: 6:39 - loss: 0.6825 - acc: 0.5667
1024/4849 [=====>........................] - ETA: 6:30 - loss: 0.6804 - acc: 0.5664
1088/4849 [=====>........................] - ETA: 6:25 - loss: 0.6785 - acc: 0.5735
1152/4849 [======>.......................] - ETA: 6:17 - loss: 0.6779 - acc: 0.5755
1216/4849 [======>.......................] - ETA: 6:10 - loss: 0.6787 - acc: 0.5715
1280/4849 [======>.......................] - ETA: 6:02 - loss: 0.6809 - acc: 0.5680
1344/4849 [=======>......................] - ETA: 5:54 - loss: 0.6831 - acc: 0.5632
1408/4849 [=======>......................] - ETA: 5:49 - loss: 0.6852 - acc: 0.5618
1472/4849 [========>.....................] - ETA: 5:42 - loss: 0.6844 - acc: 0.5618
1536/4849 [========>.....................] - ETA: 5:35 - loss: 0.6853 - acc: 0.5605
1600/4849 [========>.....................] - ETA: 5:27 - loss: 0.6852 - acc: 0.5587
1664/4849 [=========>....................] - ETA: 5:20 - loss: 0.6843 - acc: 0.5601
1728/4849 [=========>....................] - ETA: 5:14 - loss: 0.6838 - acc: 0.5608
1792/4849 [==========>...................] - ETA: 5:08 - loss: 0.6834 - acc: 0.5619
1856/4849 [==========>...................] - ETA: 5:01 - loss: 0.6829 - acc: 0.5630
1920/4849 [==========>...................] - ETA: 4:54 - loss: 0.6818 - acc: 0.5677
1984/4849 [===========>..................] - ETA: 4:47 - loss: 0.6808 - acc: 0.5691
2048/4849 [===========>..................] - ETA: 4:40 - loss: 0.6822 - acc: 0.5659
2112/4849 [============>.................] - ETA: 4:34 - loss: 0.6818 - acc: 0.5658
2176/4849 [============>.................] - ETA: 4:27 - loss: 0.6819 - acc: 0.5648
2240/4849 [============>.................] - ETA: 4:21 - loss: 0.6826 - acc: 0.5638
2304/4849 [=============>................] - ETA: 4:14 - loss: 0.6820 - acc: 0.5660
2368/4849 [=============>................] - ETA: 4:07 - loss: 0.6823 - acc: 0.5659
2432/4849 [==============>...............] - ETA: 4:01 - loss: 0.6818 - acc: 0.5674
2496/4849 [==============>...............] - ETA: 3:55 - loss: 0.6828 - acc: 0.5669
2560/4849 [==============>...............] - ETA: 3:49 - loss: 0.6837 - acc: 0.5625
2624/4849 [===============>..............] - ETA: 3:42 - loss: 0.6833 - acc: 0.5621
2688/4849 [===============>..............] - ETA: 3:35 - loss: 0.6837 - acc: 0.5618
2752/4849 [================>.............] - ETA: 3:29 - loss: 0.6845 - acc: 0.5610
2816/4849 [================>.............] - ETA: 3:22 - loss: 0.6847 - acc: 0.5607
2880/4849 [================>.............] - ETA: 3:16 - loss: 0.6841 - acc: 0.5615
2944/4849 [=================>............] - ETA: 3:09 - loss: 0.6846 - acc: 0.5611
3008/4849 [=================>............] - ETA: 3:03 - loss: 0.6847 - acc: 0.5598
3072/4849 [==================>...........] - ETA: 2:56 - loss: 0.6841 - acc: 0.5609
3136/4849 [==================>...........] - ETA: 2:50 - loss: 0.6848 - acc: 0.5596
3200/4849 [==================>...........] - ETA: 2:43 - loss: 0.6844 - acc: 0.5606
3264/4849 [===================>..........] - ETA: 2:37 - loss: 0.6844 - acc: 0.5622
3328/4849 [===================>..........] - ETA: 2:31 - loss: 0.6842 - acc: 0.5628
3392/4849 [===================>..........] - ETA: 2:24 - loss: 0.6835 - acc: 0.5640
3456/4849 [====================>.........] - ETA: 2:18 - loss: 0.6834 - acc: 0.5637
3520/4849 [====================>.........] - ETA: 2:11 - loss: 0.6836 - acc: 0.5625
3584/4849 [=====================>........] - ETA: 2:05 - loss: 0.6837 - acc: 0.5617
3648/4849 [=====================>........] - ETA: 1:59 - loss: 0.6838 - acc: 0.5611
3712/4849 [=====================>........] - ETA: 1:52 - loss: 0.6831 - acc: 0.5628
3776/4849 [======================>.......] - ETA: 1:46 - loss: 0.6833 - acc: 0.5622
3840/4849 [======================>.......] - ETA: 1:40 - loss: 0.6826 - acc: 0.5651
3904/4849 [=======================>......] - ETA: 1:33 - loss: 0.6828 - acc: 0.5643
3968/4849 [=======================>......] - ETA: 1:27 - loss: 0.6826 - acc: 0.5648
4032/4849 [=======================>......] - ETA: 1:21 - loss: 0.6828 - acc: 0.5645
4096/4849 [========================>.....] - ETA: 1:14 - loss: 0.6832 - acc: 0.5632
4160/4849 [========================>.....] - ETA: 1:08 - loss: 0.6842 - acc: 0.5613
4224/4849 [=========================>....] - ETA: 1:02 - loss: 0.6831 - acc: 0.5642
4288/4849 [=========================>....] - ETA: 55s - loss: 0.6832 - acc: 0.5637 
4352/4849 [=========================>....] - ETA: 49s - loss: 0.6834 - acc: 0.5634
4416/4849 [==========================>...] - ETA: 42s - loss: 0.6834 - acc: 0.5632
4480/4849 [==========================>...] - ETA: 36s - loss: 0.6827 - acc: 0.5636
4544/4849 [===========================>..] - ETA: 30s - loss: 0.6828 - acc: 0.5643
4608/4849 [===========================>..] - ETA: 23s - loss: 0.6821 - acc: 0.5658
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6824 - acc: 0.5646
4736/4849 [============================>.] - ETA: 11s - loss: 0.6823 - acc: 0.5650
4800/4849 [============================>.] - ETA: 4s - loss: 0.6825 - acc: 0.5648 
4849/4849 [==============================] - 499s 103ms/step - loss: 0.6827 - acc: 0.5649 - val_loss: 0.6966 - val_acc: 0.5121

Epoch 00006: val_acc did not improve from 0.53803
Epoch 7/10

  64/4849 [..............................] - ETA: 8:34 - loss: 0.6580 - acc: 0.6562
 128/4849 [..............................] - ETA: 7:53 - loss: 0.6633 - acc: 0.6406
 192/4849 [>.............................] - ETA: 7:39 - loss: 0.6526 - acc: 0.6510
 256/4849 [>.............................] - ETA: 7:36 - loss: 0.6556 - acc: 0.6211
 320/4849 [>.............................] - ETA: 7:29 - loss: 0.6644 - acc: 0.5969
 384/4849 [=>............................] - ETA: 7:25 - loss: 0.6667 - acc: 0.5964
 448/4849 [=>............................] - ETA: 7:24 - loss: 0.6675 - acc: 0.5982
 512/4849 [==>...........................] - ETA: 7:16 - loss: 0.6693 - acc: 0.5957
 576/4849 [==>...........................] - ETA: 7:09 - loss: 0.6689 - acc: 0.5990
 640/4849 [==>...........................] - ETA: 7:01 - loss: 0.6664 - acc: 0.6016
 704/4849 [===>..........................] - ETA: 6:59 - loss: 0.6699 - acc: 0.5923
 768/4849 [===>..........................] - ETA: 6:51 - loss: 0.6698 - acc: 0.5911
 832/4849 [====>.........................] - ETA: 6:44 - loss: 0.6736 - acc: 0.5853
 896/4849 [====>.........................] - ETA: 6:37 - loss: 0.6725 - acc: 0.5826
 960/4849 [====>.........................] - ETA: 6:27 - loss: 0.6737 - acc: 0.5792
1024/4849 [=====>........................] - ETA: 6:19 - loss: 0.6712 - acc: 0.5840
1088/4849 [=====>........................] - ETA: 6:11 - loss: 0.6717 - acc: 0.5809
1152/4849 [======>.......................] - ETA: 6:03 - loss: 0.6716 - acc: 0.5816
1216/4849 [======>.......................] - ETA: 5:53 - loss: 0.6705 - acc: 0.5839
1280/4849 [======>.......................] - ETA: 5:48 - loss: 0.6722 - acc: 0.5789
1344/4849 [=======>......................] - ETA: 5:41 - loss: 0.6749 - acc: 0.5759
1408/4849 [=======>......................] - ETA: 5:33 - loss: 0.6737 - acc: 0.5781
1472/4849 [========>.....................] - ETA: 5:26 - loss: 0.6734 - acc: 0.5774
1536/4849 [========>.....................] - ETA: 5:18 - loss: 0.6712 - acc: 0.5820
1600/4849 [========>.....................] - ETA: 5:12 - loss: 0.6699 - acc: 0.5837
1664/4849 [=========>....................] - ETA: 5:05 - loss: 0.6685 - acc: 0.5871
1728/4849 [=========>....................] - ETA: 4:59 - loss: 0.6684 - acc: 0.5880
1792/4849 [==========>...................] - ETA: 4:53 - loss: 0.6689 - acc: 0.5865
1856/4849 [==========>...................] - ETA: 4:46 - loss: 0.6697 - acc: 0.5857
1920/4849 [==========>...................] - ETA: 4:40 - loss: 0.6695 - acc: 0.5854
1984/4849 [===========>..................] - ETA: 4:34 - loss: 0.6715 - acc: 0.5822
2048/4849 [===========>..................] - ETA: 4:27 - loss: 0.6707 - acc: 0.5815
2112/4849 [============>.................] - ETA: 4:21 - loss: 0.6708 - acc: 0.5819
2176/4849 [============>.................] - ETA: 4:15 - loss: 0.6700 - acc: 0.5832
2240/4849 [============>.................] - ETA: 4:10 - loss: 0.6697 - acc: 0.5826
2304/4849 [=============>................] - ETA: 4:04 - loss: 0.6698 - acc: 0.5829
2368/4849 [=============>................] - ETA: 3:58 - loss: 0.6695 - acc: 0.5832
2432/4849 [==============>...............] - ETA: 3:52 - loss: 0.6704 - acc: 0.5802
2496/4849 [==============>...............] - ETA: 3:46 - loss: 0.6700 - acc: 0.5809
2560/4849 [==============>...............] - ETA: 3:40 - loss: 0.6696 - acc: 0.5816
2624/4849 [===============>..............] - ETA: 3:35 - loss: 0.6696 - acc: 0.5827
2688/4849 [===============>..............] - ETA: 3:29 - loss: 0.6699 - acc: 0.5822
2752/4849 [================>.............] - ETA: 3:23 - loss: 0.6711 - acc: 0.5818
2816/4849 [================>.............] - ETA: 3:17 - loss: 0.6714 - acc: 0.5803
2880/4849 [================>.............] - ETA: 3:11 - loss: 0.6720 - acc: 0.5785
2944/4849 [=================>............] - ETA: 3:05 - loss: 0.6736 - acc: 0.5751
3008/4849 [=================>............] - ETA: 2:59 - loss: 0.6736 - acc: 0.5741
3072/4849 [==================>...........] - ETA: 2:53 - loss: 0.6739 - acc: 0.5739
3136/4849 [==================>...........] - ETA: 2:47 - loss: 0.6736 - acc: 0.5749
3200/4849 [==================>...........] - ETA: 2:40 - loss: 0.6728 - acc: 0.5766
3264/4849 [===================>..........] - ETA: 2:34 - loss: 0.6737 - acc: 0.5744
3328/4849 [===================>..........] - ETA: 2:28 - loss: 0.6728 - acc: 0.5763
3392/4849 [===================>..........] - ETA: 2:21 - loss: 0.6732 - acc: 0.5755
3456/4849 [====================>.........] - ETA: 2:15 - loss: 0.6731 - acc: 0.5761
3520/4849 [====================>.........] - ETA: 2:09 - loss: 0.6729 - acc: 0.5773
3584/4849 [=====================>........] - ETA: 2:03 - loss: 0.6726 - acc: 0.5781
3648/4849 [=====================>........] - ETA: 1:57 - loss: 0.6723 - acc: 0.5792
3712/4849 [=====================>........] - ETA: 1:50 - loss: 0.6722 - acc: 0.5792
3776/4849 [======================>.......] - ETA: 1:44 - loss: 0.6719 - acc: 0.5797
3840/4849 [======================>.......] - ETA: 1:38 - loss: 0.6719 - acc: 0.5799
3904/4849 [=======================>......] - ETA: 1:32 - loss: 0.6719 - acc: 0.5799
3968/4849 [=======================>......] - ETA: 1:25 - loss: 0.6721 - acc: 0.5804
4032/4849 [=======================>......] - ETA: 1:19 - loss: 0.6714 - acc: 0.5821
4096/4849 [========================>.....] - ETA: 1:13 - loss: 0.6715 - acc: 0.5818
4160/4849 [========================>.....] - ETA: 1:06 - loss: 0.6718 - acc: 0.5820
4224/4849 [=========================>....] - ETA: 1:00 - loss: 0.6724 - acc: 0.5807
4288/4849 [=========================>....] - ETA: 54s - loss: 0.6724 - acc: 0.5800 
4352/4849 [=========================>....] - ETA: 48s - loss: 0.6726 - acc: 0.5795
4416/4849 [==========================>...] - ETA: 42s - loss: 0.6732 - acc: 0.5788
4480/4849 [==========================>...] - ETA: 35s - loss: 0.6729 - acc: 0.5795
4544/4849 [===========================>..] - ETA: 29s - loss: 0.6731 - acc: 0.5790
4608/4849 [===========================>..] - ETA: 23s - loss: 0.6726 - acc: 0.5801
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6728 - acc: 0.5798
4736/4849 [============================>.] - ETA: 10s - loss: 0.6724 - acc: 0.5804
4800/4849 [============================>.] - ETA: 4s - loss: 0.6721 - acc: 0.5802 
4849/4849 [==============================] - 487s 101ms/step - loss: 0.6718 - acc: 0.5803 - val_loss: 0.6882 - val_acc: 0.5455

Epoch 00007: val_acc improved from 0.53803 to 0.54545, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window20/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 8/10

  64/4849 [..............................] - ETA: 5:52 - loss: 0.6623 - acc: 0.5938
 128/4849 [..............................] - ETA: 6:06 - loss: 0.6786 - acc: 0.5625
 192/4849 [>.............................] - ETA: 6:19 - loss: 0.6577 - acc: 0.6198
 256/4849 [>.............................] - ETA: 6:19 - loss: 0.6606 - acc: 0.6133
 320/4849 [>.............................] - ETA: 6:09 - loss: 0.6690 - acc: 0.5969
 384/4849 [=>............................] - ETA: 6:11 - loss: 0.6619 - acc: 0.6068
 448/4849 [=>............................] - ETA: 6:00 - loss: 0.6643 - acc: 0.6071
 512/4849 [==>...........................] - ETA: 5:54 - loss: 0.6671 - acc: 0.6035
 576/4849 [==>...........................] - ETA: 5:52 - loss: 0.6711 - acc: 0.5903
 640/4849 [==>...........................] - ETA: 5:52 - loss: 0.6729 - acc: 0.5828
 704/4849 [===>..........................] - ETA: 5:49 - loss: 0.6711 - acc: 0.5781
 768/4849 [===>..........................] - ETA: 5:44 - loss: 0.6726 - acc: 0.5716
 832/4849 [====>.........................] - ETA: 5:40 - loss: 0.6751 - acc: 0.5685
 896/4849 [====>.........................] - ETA: 5:35 - loss: 0.6770 - acc: 0.5737
 960/4849 [====>.........................] - ETA: 5:29 - loss: 0.6740 - acc: 0.5813
1024/4849 [=====>........................] - ETA: 5:24 - loss: 0.6742 - acc: 0.5840
1088/4849 [=====>........................] - ETA: 5:20 - loss: 0.6737 - acc: 0.5846
1152/4849 [======>.......................] - ETA: 5:14 - loss: 0.6743 - acc: 0.5842
1216/4849 [======>.......................] - ETA: 5:10 - loss: 0.6759 - acc: 0.5831
1280/4849 [======>.......................] - ETA: 5:04 - loss: 0.6731 - acc: 0.5875
1344/4849 [=======>......................] - ETA: 5:02 - loss: 0.6753 - acc: 0.5856
1408/4849 [=======>......................] - ETA: 4:55 - loss: 0.6761 - acc: 0.5866
1472/4849 [========>.....................] - ETA: 4:50 - loss: 0.6778 - acc: 0.5822
1536/4849 [========>.....................] - ETA: 4:46 - loss: 0.6773 - acc: 0.5820
1600/4849 [========>.....................] - ETA: 4:41 - loss: 0.6785 - acc: 0.5806
1664/4849 [=========>....................] - ETA: 4:35 - loss: 0.6787 - acc: 0.5799
1728/4849 [=========>....................] - ETA: 4:29 - loss: 0.6797 - acc: 0.5787
1792/4849 [==========>...................] - ETA: 4:24 - loss: 0.6778 - acc: 0.5809
1856/4849 [==========>...................] - ETA: 4:18 - loss: 0.6776 - acc: 0.5803
1920/4849 [==========>...................] - ETA: 4:14 - loss: 0.6772 - acc: 0.5818
1984/4849 [===========>..................] - ETA: 4:08 - loss: 0.6758 - acc: 0.5847
2048/4849 [===========>..................] - ETA: 4:03 - loss: 0.6742 - acc: 0.5879
2112/4849 [============>.................] - ETA: 3:57 - loss: 0.6750 - acc: 0.5852
2176/4849 [============>.................] - ETA: 3:52 - loss: 0.6770 - acc: 0.5818
2240/4849 [============>.................] - ETA: 3:46 - loss: 0.6780 - acc: 0.5804
2304/4849 [=============>................] - ETA: 3:41 - loss: 0.6776 - acc: 0.5812
2368/4849 [=============>................] - ETA: 3:36 - loss: 0.6777 - acc: 0.5811
2432/4849 [==============>...............] - ETA: 3:30 - loss: 0.6775 - acc: 0.5818
2496/4849 [==============>...............] - ETA: 3:25 - loss: 0.6766 - acc: 0.5825
2560/4849 [==============>...............] - ETA: 3:19 - loss: 0.6777 - acc: 0.5805
2624/4849 [===============>..............] - ETA: 3:14 - loss: 0.6780 - acc: 0.5800
2688/4849 [===============>..............] - ETA: 3:08 - loss: 0.6774 - acc: 0.5811
2752/4849 [================>.............] - ETA: 3:03 - loss: 0.6766 - acc: 0.5821
2816/4849 [================>.............] - ETA: 2:57 - loss: 0.6776 - acc: 0.5813
2880/4849 [================>.............] - ETA: 2:52 - loss: 0.6777 - acc: 0.5823
2944/4849 [=================>............] - ETA: 2:46 - loss: 0.6777 - acc: 0.5822
3008/4849 [=================>............] - ETA: 2:40 - loss: 0.6768 - acc: 0.5854
3072/4849 [==================>...........] - ETA: 2:35 - loss: 0.6767 - acc: 0.5859
3136/4849 [==================>...........] - ETA: 2:29 - loss: 0.6768 - acc: 0.5851
3200/4849 [==================>...........] - ETA: 2:24 - loss: 0.6770 - acc: 0.5841
3264/4849 [===================>..........] - ETA: 2:18 - loss: 0.6765 - acc: 0.5855
3328/4849 [===================>..........] - ETA: 2:13 - loss: 0.6764 - acc: 0.5850
3392/4849 [===================>..........] - ETA: 2:07 - loss: 0.6753 - acc: 0.5867
3456/4849 [====================>.........] - ETA: 2:01 - loss: 0.6759 - acc: 0.5856
3520/4849 [====================>.........] - ETA: 1:56 - loss: 0.6760 - acc: 0.5864
3584/4849 [=====================>........] - ETA: 1:50 - loss: 0.6759 - acc: 0.5857
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6751 - acc: 0.5874
3712/4849 [=====================>........] - ETA: 1:39 - loss: 0.6754 - acc: 0.5878
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6758 - acc: 0.5871
3840/4849 [======================>.......] - ETA: 1:28 - loss: 0.6762 - acc: 0.5862
3904/4849 [=======================>......] - ETA: 1:22 - loss: 0.6766 - acc: 0.5853
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6767 - acc: 0.5857
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6762 - acc: 0.5863
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6763 - acc: 0.5850
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6770 - acc: 0.5846
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6770 - acc: 0.5845 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6771 - acc: 0.5847
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6776 - acc: 0.5846
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6771 - acc: 0.5856
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6768 - acc: 0.5859
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6769 - acc: 0.5856
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6774 - acc: 0.5859
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6774 - acc: 0.5854
4736/4849 [============================>.] - ETA: 9s - loss: 0.6769 - acc: 0.5866 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6767 - acc: 0.5865
4849/4849 [==============================] - 443s 91ms/step - loss: 0.6769 - acc: 0.5857 - val_loss: 0.6884 - val_acc: 0.5306

Epoch 00008: val_acc did not improve from 0.54545
Epoch 9/10

  64/4849 [..............................] - ETA: 7:26 - loss: 0.6924 - acc: 0.5312
 128/4849 [..............................] - ETA: 6:44 - loss: 0.6752 - acc: 0.5703
 192/4849 [>.............................] - ETA: 6:35 - loss: 0.6819 - acc: 0.5521
 256/4849 [>.............................] - ETA: 6:29 - loss: 0.6747 - acc: 0.5781
 320/4849 [>.............................] - ETA: 6:23 - loss: 0.6779 - acc: 0.5781
 384/4849 [=>............................] - ETA: 6:25 - loss: 0.6882 - acc: 0.5599
 448/4849 [=>............................] - ETA: 6:23 - loss: 0.6821 - acc: 0.5737
 512/4849 [==>...........................] - ETA: 6:22 - loss: 0.6770 - acc: 0.5801
 576/4849 [==>...........................] - ETA: 6:17 - loss: 0.6736 - acc: 0.5799
 640/4849 [==>...........................] - ETA: 6:11 - loss: 0.6781 - acc: 0.5734
 704/4849 [===>..........................] - ETA: 6:02 - loss: 0.6729 - acc: 0.5795
 768/4849 [===>..........................] - ETA: 5:59 - loss: 0.6733 - acc: 0.5729
 832/4849 [====>.........................] - ETA: 5:54 - loss: 0.6717 - acc: 0.5769
 896/4849 [====>.........................] - ETA: 5:47 - loss: 0.6705 - acc: 0.5781
 960/4849 [====>.........................] - ETA: 5:41 - loss: 0.6715 - acc: 0.5698
1024/4849 [=====>........................] - ETA: 5:36 - loss: 0.6696 - acc: 0.5771
1088/4849 [=====>........................] - ETA: 5:31 - loss: 0.6713 - acc: 0.5708
1152/4849 [======>.......................] - ETA: 5:24 - loss: 0.6710 - acc: 0.5720
1216/4849 [======>.......................] - ETA: 5:18 - loss: 0.6701 - acc: 0.5740
1280/4849 [======>.......................] - ETA: 5:13 - loss: 0.6696 - acc: 0.5758
1344/4849 [=======>......................] - ETA: 5:07 - loss: 0.6689 - acc: 0.5804
1408/4849 [=======>......................] - ETA: 5:01 - loss: 0.6707 - acc: 0.5788
1472/4849 [========>.....................] - ETA: 4:57 - loss: 0.6699 - acc: 0.5802
1536/4849 [========>.....................] - ETA: 4:51 - loss: 0.6684 - acc: 0.5833
1600/4849 [========>.....................] - ETA: 4:45 - loss: 0.6679 - acc: 0.5850
1664/4849 [=========>....................] - ETA: 4:39 - loss: 0.6687 - acc: 0.5835
1728/4849 [=========>....................] - ETA: 4:34 - loss: 0.6660 - acc: 0.5862
1792/4849 [==========>...................] - ETA: 4:29 - loss: 0.6658 - acc: 0.5865
1856/4849 [==========>...................] - ETA: 4:23 - loss: 0.6674 - acc: 0.5819
1920/4849 [==========>...................] - ETA: 4:17 - loss: 0.6690 - acc: 0.5781
1984/4849 [===========>..................] - ETA: 4:12 - loss: 0.6666 - acc: 0.5817
2048/4849 [===========>..................] - ETA: 4:06 - loss: 0.6682 - acc: 0.5801
2112/4849 [============>.................] - ETA: 4:00 - loss: 0.6702 - acc: 0.5781
2176/4849 [============>.................] - ETA: 3:56 - loss: 0.6701 - acc: 0.5790
2240/4849 [============>.................] - ETA: 3:49 - loss: 0.6707 - acc: 0.5790
2304/4849 [=============>................] - ETA: 3:44 - loss: 0.6711 - acc: 0.5781
2368/4849 [=============>................] - ETA: 3:38 - loss: 0.6705 - acc: 0.5790
2432/4849 [==============>...............] - ETA: 3:32 - loss: 0.6715 - acc: 0.5773
2496/4849 [==============>...............] - ETA: 3:27 - loss: 0.6714 - acc: 0.5777
2560/4849 [==============>...............] - ETA: 3:21 - loss: 0.6714 - acc: 0.5766
2624/4849 [===============>..............] - ETA: 3:15 - loss: 0.6716 - acc: 0.5781
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.6710 - acc: 0.5804
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.6714 - acc: 0.5807
2816/4849 [================>.............] - ETA: 2:58 - loss: 0.6712 - acc: 0.5803
2880/4849 [================>.............] - ETA: 2:54 - loss: 0.6710 - acc: 0.5819
2944/4849 [=================>............] - ETA: 2:48 - loss: 0.6711 - acc: 0.5822
3008/4849 [=================>............] - ETA: 2:42 - loss: 0.6718 - acc: 0.5805
3072/4849 [==================>...........] - ETA: 2:36 - loss: 0.6715 - acc: 0.5804
3136/4849 [==================>...........] - ETA: 2:31 - loss: 0.6708 - acc: 0.5820
3200/4849 [==================>...........] - ETA: 2:25 - loss: 0.6706 - acc: 0.5834
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6713 - acc: 0.5836
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6706 - acc: 0.5838
3392/4849 [===================>..........] - ETA: 2:08 - loss: 0.6708 - acc: 0.5837
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6702 - acc: 0.5842
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6701 - acc: 0.5855
3584/4849 [=====================>........] - ETA: 1:51 - loss: 0.6700 - acc: 0.5862
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6700 - acc: 0.5874
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6707 - acc: 0.5867
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6710 - acc: 0.5861
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6713 - acc: 0.5854
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6704 - acc: 0.5873
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6705 - acc: 0.5882
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6705 - acc: 0.5873
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6712 - acc: 0.5859
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6714 - acc: 0.5851
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6713 - acc: 0.5852 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6712 - acc: 0.5854
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6706 - acc: 0.5869
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6712 - acc: 0.5863
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6708 - acc: 0.5871
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6715 - acc: 0.5852
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6718 - acc: 0.5842
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6718 - acc: 0.5841
4736/4849 [============================>.] - ETA: 9s - loss: 0.6719 - acc: 0.5830 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6716 - acc: 0.5833
4849/4849 [==============================] - 443s 91ms/step - loss: 0.6720 - acc: 0.5822 - val_loss: 0.7081 - val_acc: 0.5158

Epoch 00009: val_acc did not improve from 0.54545
Epoch 10/10

  64/4849 [..............................] - ETA: 6:33 - loss: 0.6574 - acc: 0.6406
 128/4849 [..............................] - ETA: 6:33 - loss: 0.6639 - acc: 0.6250
 192/4849 [>.............................] - ETA: 6:56 - loss: 0.6594 - acc: 0.6250
 256/4849 [>.............................] - ETA: 6:44 - loss: 0.6650 - acc: 0.6055
 320/4849 [>.............................] - ETA: 6:42 - loss: 0.6752 - acc: 0.5875
 384/4849 [=>............................] - ETA: 6:29 - loss: 0.6780 - acc: 0.5859
 448/4849 [=>............................] - ETA: 6:25 - loss: 0.6747 - acc: 0.5893
 512/4849 [==>...........................] - ETA: 6:16 - loss: 0.6692 - acc: 0.5957
 576/4849 [==>...........................] - ETA: 6:10 - loss: 0.6681 - acc: 0.5990
 640/4849 [==>...........................] - ETA: 6:03 - loss: 0.6632 - acc: 0.6078
 704/4849 [===>..........................] - ETA: 5:58 - loss: 0.6647 - acc: 0.6065
 768/4849 [===>..........................] - ETA: 5:50 - loss: 0.6613 - acc: 0.6042
 832/4849 [====>.........................] - ETA: 5:44 - loss: 0.6639 - acc: 0.6010
 896/4849 [====>.........................] - ETA: 5:40 - loss: 0.6631 - acc: 0.6027
 960/4849 [====>.........................] - ETA: 5:34 - loss: 0.6628 - acc: 0.6073
1024/4849 [=====>........................] - ETA: 5:27 - loss: 0.6671 - acc: 0.6064
1088/4849 [=====>........................] - ETA: 5:23 - loss: 0.6687 - acc: 0.5993
1152/4849 [======>.......................] - ETA: 5:17 - loss: 0.6682 - acc: 0.6033
1216/4849 [======>.......................] - ETA: 5:11 - loss: 0.6682 - acc: 0.6044
1280/4849 [======>.......................] - ETA: 5:07 - loss: 0.6670 - acc: 0.6070
1344/4849 [=======>......................] - ETA: 5:02 - loss: 0.6677 - acc: 0.6049
1408/4849 [=======>......................] - ETA: 4:57 - loss: 0.6682 - acc: 0.6016
1472/4849 [========>.....................] - ETA: 4:52 - loss: 0.6668 - acc: 0.6039
1536/4849 [========>.....................] - ETA: 4:47 - loss: 0.6680 - acc: 0.6009
1600/4849 [========>.....................] - ETA: 4:42 - loss: 0.6669 - acc: 0.6025
1664/4849 [=========>....................] - ETA: 4:37 - loss: 0.6686 - acc: 0.5980
1728/4849 [=========>....................] - ETA: 4:30 - loss: 0.6682 - acc: 0.5990
1792/4849 [==========>...................] - ETA: 4:26 - loss: 0.6681 - acc: 0.5988
1856/4849 [==========>...................] - ETA: 4:20 - loss: 0.6690 - acc: 0.5970
1920/4849 [==========>...................] - ETA: 4:14 - loss: 0.6690 - acc: 0.5943
1984/4849 [===========>..................] - ETA: 4:09 - loss: 0.6688 - acc: 0.5953
2048/4849 [===========>..................] - ETA: 4:03 - loss: 0.6688 - acc: 0.5942
2112/4849 [============>.................] - ETA: 3:58 - loss: 0.6685 - acc: 0.5952
2176/4849 [============>.................] - ETA: 3:52 - loss: 0.6692 - acc: 0.5928
2240/4849 [============>.................] - ETA: 3:47 - loss: 0.6687 - acc: 0.5969
2304/4849 [=============>................] - ETA: 3:41 - loss: 0.6676 - acc: 0.5972
2368/4849 [=============>................] - ETA: 3:36 - loss: 0.6683 - acc: 0.5959
2432/4849 [==============>...............] - ETA: 3:30 - loss: 0.6677 - acc: 0.5966
2496/4849 [==============>...............] - ETA: 3:25 - loss: 0.6680 - acc: 0.5974
2560/4849 [==============>...............] - ETA: 3:19 - loss: 0.6681 - acc: 0.5980
2624/4849 [===============>..............] - ETA: 3:14 - loss: 0.6697 - acc: 0.5949
2688/4849 [===============>..............] - ETA: 3:08 - loss: 0.6688 - acc: 0.5960
2752/4849 [================>.............] - ETA: 3:02 - loss: 0.6679 - acc: 0.5985
2816/4849 [================>.............] - ETA: 2:57 - loss: 0.6689 - acc: 0.5969
2880/4849 [================>.............] - ETA: 2:51 - loss: 0.6684 - acc: 0.5990
2944/4849 [=================>............] - ETA: 2:46 - loss: 0.6675 - acc: 0.5999
3008/4849 [=================>............] - ETA: 2:40 - loss: 0.6665 - acc: 0.6017
3072/4849 [==================>...........] - ETA: 2:34 - loss: 0.6663 - acc: 0.6019
3136/4849 [==================>...........] - ETA: 2:28 - loss: 0.6680 - acc: 0.5998
3200/4849 [==================>...........] - ETA: 2:23 - loss: 0.6681 - acc: 0.5994
3264/4849 [===================>..........] - ETA: 2:17 - loss: 0.6685 - acc: 0.6002
3328/4849 [===================>..........] - ETA: 2:12 - loss: 0.6687 - acc: 0.5995
3392/4849 [===================>..........] - ETA: 2:06 - loss: 0.6690 - acc: 0.5988
3456/4849 [====================>.........] - ETA: 2:01 - loss: 0.6696 - acc: 0.5981
3520/4849 [====================>.........] - ETA: 1:55 - loss: 0.6694 - acc: 0.5989
3584/4849 [=====================>........] - ETA: 1:50 - loss: 0.6689 - acc: 0.5999
3648/4849 [=====================>........] - ETA: 1:44 - loss: 0.6700 - acc: 0.5973
3712/4849 [=====================>........] - ETA: 1:38 - loss: 0.6697 - acc: 0.5981
3776/4849 [======================>.......] - ETA: 1:33 - loss: 0.6701 - acc: 0.5975
3840/4849 [======================>.......] - ETA: 1:27 - loss: 0.6700 - acc: 0.5966
3904/4849 [=======================>......] - ETA: 1:22 - loss: 0.6702 - acc: 0.5961
3968/4849 [=======================>......] - ETA: 1:16 - loss: 0.6703 - acc: 0.5953
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6700 - acc: 0.5970
4096/4849 [========================>.....] - ETA: 1:05 - loss: 0.6706 - acc: 0.5969
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6711 - acc: 0.5962 
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6712 - acc: 0.5961
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6706 - acc: 0.5977
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6711 - acc: 0.5965
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6711 - acc: 0.5956
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6721 - acc: 0.5929
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6721 - acc: 0.5926
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6721 - acc: 0.5924
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6722 - acc: 0.5923
4736/4849 [============================>.] - ETA: 9s - loss: 0.6723 - acc: 0.5918 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6722 - acc: 0.5923
4849/4849 [==============================] - 440s 91ms/step - loss: 0.6725 - acc: 0.5925 - val_loss: 0.6973 - val_acc: 0.5436

Epoch 00010: val_acc did not improve from 0.54545
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f92200f8bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f92200f8bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9210480c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9210480c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9210480e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9210480e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9210470510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9210470510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92103f2cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92103f2cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92103851d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92103851d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9210470410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9210470410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92103c8450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92103c8450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8f4077af10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8f4077af10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f921025eed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f921025eed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92101cc050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92101cc050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8f4077ac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8f4077ac90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9210107450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9210107450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9120112a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9120112a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91e46541d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91e46541d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f921010b490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f921010b490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9120112a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9120112a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91e4619cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91e4619cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91e4433310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91e4433310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91e456c150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91e456c150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91e44299d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91e44299d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9120082d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9120082d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91e4219fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91e4219fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91e42c0ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91e42c0ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91e4043590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91e4043590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c869e850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c869e850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91e42c0a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91e42c0a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c87f0150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c87f0150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91e40be910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91e40be910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91c8445650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91c8445650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c858e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c858e550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91c87d7710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91c87d7710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c845cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c845cf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91c8251050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91c8251050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91c848e290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91c848e290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c824c650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c824c650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91c87dadd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91c87dadd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c87daad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c87daad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91a474d210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91a474d210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91a45d9950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91a45d9950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c81b2b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91c81b2b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91a473eb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91a473eb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91a47430d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91a47430d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91a4470250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91a4470250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91a461b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91a461b8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91a4353350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91a4353350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91a4602b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91a4602b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91a42eb210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91a42eb210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91a407b190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91a407b190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9180753e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9180753e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91a4061390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91a4061390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91a407b210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91a407b210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9180795f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9180795f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9180564cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9180564cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f918043a050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f918043a050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9180445e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9180445e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f918078bdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f918078bdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91802f9a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91802f9a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9180257ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9180257ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f918011f450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f918011f450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9180281110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9180281110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9180257110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9180257110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f918078dc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f918078dc10>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 4:59
 128/1348 [=>............................] - ETA: 2:39
 192/1348 [===>..........................] - ETA: 1:50
 256/1348 [====>.........................] - ETA: 1:25
 320/1348 [======>.......................] - ETA: 1:09
 384/1348 [=======>......................] - ETA: 57s 
 448/1348 [========>.....................] - ETA: 49s
 512/1348 [==========>...................] - ETA: 42s
 576/1348 [===========>..................] - ETA: 37s
 640/1348 [=============>................] - ETA: 32s
 704/1348 [==============>...............] - ETA: 28s
 768/1348 [================>.............] - ETA: 24s
 832/1348 [=================>............] - ETA: 21s
 896/1348 [==================>...........] - ETA: 18s
 960/1348 [====================>.........] - ETA: 15s
1024/1348 [=====================>........] - ETA: 12s
1088/1348 [=======================>......] - ETA: 9s 
1152/1348 [========================>.....] - ETA: 7s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 48s 35ms/step
loss: 0.6736126186585921
acc: 0.5816023738872403
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f8e986d37d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f8e986d37d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f8e9868ee90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f8e9868ee90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9400404110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9400404110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90200817d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90200817d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f902014ed90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f902014ed90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9020191750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9020191750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9020081690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9020081690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92104d50d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92104d50d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9210699690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9210699690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e98593dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e98593dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90201b3050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90201b3050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f921070b150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f921070b150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e98626f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e98626f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e9842d150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e9842d150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e982df150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e982df150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f922006fad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f922006fad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e9842d110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e9842d110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e982f4290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e982f4290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e9828abd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e9828abd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e98303610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e98303610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e847e7210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e847e7210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e98085a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e98085a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e84756490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e84756490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e844fb1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e844fb1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e84500090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e84500090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e8473d690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e8473d690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e847ded50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e847ded50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e845a8a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e845a8a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e84224290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e84224290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e8420e110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e8420e110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e84470d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e84470d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e980b4150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e980b4150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e84244ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e84244ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e841f6d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e841f6d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e58675a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e58675a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e84752e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e84752e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e58726e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e58726e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e58577390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e58577390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e583ad7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e583ad7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e58245590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e58245590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e583984d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e583984d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e585f0790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e585f0790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e58256bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e58256bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e58285dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8e58285dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e3c771350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e3c771350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db0755510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db0755510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e58285a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e58285a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e3c722050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e3c722050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8db074cc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8db074cc90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8db054fe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8db054fe90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db05da3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db05da3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e580a0a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e580a0a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db046c210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db046c210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8db03a19d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8db03a19d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8db05d8610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8db05d8610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e84479cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e84479cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8db026fdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8db026fdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db02f25d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db02f25d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da87ef950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da87ef950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da8754790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da8754790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e8445b090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e8445b090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e3c7074d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8e3c7074d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da86cf6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da86cf6d0>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 35:15 - loss: 0.6985 - acc: 0.5781
 128/4849 [..............................] - ETA: 20:09 - loss: 0.7281 - acc: 0.5391
 192/4849 [>.............................] - ETA: 15:11 - loss: 0.7400 - acc: 0.5156
 256/4849 [>.............................] - ETA: 12:40 - loss: 0.7423 - acc: 0.4883
 320/4849 [>.............................] - ETA: 11:03 - loss: 0.7357 - acc: 0.5000
 384/4849 [=>............................] - ETA: 10:00 - loss: 0.7208 - acc: 0.5182
 448/4849 [=>............................] - ETA: 9:10 - loss: 0.7299 - acc: 0.5134 
 512/4849 [==>...........................] - ETA: 8:35 - loss: 0.7284 - acc: 0.5078
 576/4849 [==>...........................] - ETA: 8:07 - loss: 0.7326 - acc: 0.5087
 640/4849 [==>...........................] - ETA: 7:41 - loss: 0.7335 - acc: 0.5141
 704/4849 [===>..........................] - ETA: 7:21 - loss: 0.7317 - acc: 0.5241
 768/4849 [===>..........................] - ETA: 7:01 - loss: 0.7325 - acc: 0.5234
 832/4849 [====>.........................] - ETA: 6:44 - loss: 0.7313 - acc: 0.5228
 896/4849 [====>.........................] - ETA: 6:31 - loss: 0.7340 - acc: 0.5179
 960/4849 [====>.........................] - ETA: 6:17 - loss: 0.7334 - acc: 0.5156
1024/4849 [=====>........................] - ETA: 6:05 - loss: 0.7360 - acc: 0.5088
1088/4849 [=====>........................] - ETA: 5:53 - loss: 0.7342 - acc: 0.5110
1152/4849 [======>.......................] - ETA: 5:42 - loss: 0.7357 - acc: 0.5113
1216/4849 [======>.......................] - ETA: 5:33 - loss: 0.7357 - acc: 0.5107
1280/4849 [======>.......................] - ETA: 5:23 - loss: 0.7355 - acc: 0.5086
1344/4849 [=======>......................] - ETA: 5:14 - loss: 0.7364 - acc: 0.5067
1408/4849 [=======>......................] - ETA: 5:06 - loss: 0.7365 - acc: 0.5064
1472/4849 [========>.....................] - ETA: 4:57 - loss: 0.7352 - acc: 0.5061
1536/4849 [========>.....................] - ETA: 4:49 - loss: 0.7336 - acc: 0.5098
1600/4849 [========>.....................] - ETA: 4:41 - loss: 0.7330 - acc: 0.5088
1664/4849 [=========>....................] - ETA: 4:34 - loss: 0.7337 - acc: 0.5048
1728/4849 [=========>....................] - ETA: 4:28 - loss: 0.7323 - acc: 0.5069
1792/4849 [==========>...................] - ETA: 4:21 - loss: 0.7302 - acc: 0.5100
1856/4849 [==========>...................] - ETA: 4:14 - loss: 0.7306 - acc: 0.5108
1920/4849 [==========>...................] - ETA: 4:08 - loss: 0.7278 - acc: 0.5161
1984/4849 [===========>..................] - ETA: 4:01 - loss: 0.7262 - acc: 0.5186
2048/4849 [===========>..................] - ETA: 3:55 - loss: 0.7270 - acc: 0.5181
2112/4849 [============>.................] - ETA: 3:48 - loss: 0.7268 - acc: 0.5170
2176/4849 [============>.................] - ETA: 3:42 - loss: 0.7256 - acc: 0.5179
2240/4849 [============>.................] - ETA: 3:36 - loss: 0.7244 - acc: 0.5201
2304/4849 [=============>................] - ETA: 3:30 - loss: 0.7226 - acc: 0.5208
2368/4849 [=============>................] - ETA: 3:23 - loss: 0.7232 - acc: 0.5194
2432/4849 [==============>...............] - ETA: 3:18 - loss: 0.7218 - acc: 0.5214
2496/4849 [==============>...............] - ETA: 3:12 - loss: 0.7229 - acc: 0.5204
2560/4849 [==============>...............] - ETA: 3:06 - loss: 0.7219 - acc: 0.5227
2624/4849 [===============>..............] - ETA: 3:00 - loss: 0.7226 - acc: 0.5217
2688/4849 [===============>..............] - ETA: 2:54 - loss: 0.7218 - acc: 0.5223
2752/4849 [================>.............] - ETA: 2:49 - loss: 0.7210 - acc: 0.5233
2816/4849 [================>.............] - ETA: 2:43 - loss: 0.7210 - acc: 0.5245
2880/4849 [================>.............] - ETA: 2:38 - loss: 0.7222 - acc: 0.5222
2944/4849 [=================>............] - ETA: 2:32 - loss: 0.7219 - acc: 0.5200
3008/4849 [=================>............] - ETA: 2:27 - loss: 0.7213 - acc: 0.5213
3072/4849 [==================>...........] - ETA: 2:21 - loss: 0.7194 - acc: 0.5241
3136/4849 [==================>...........] - ETA: 2:16 - loss: 0.7198 - acc: 0.5239
3200/4849 [==================>...........] - ETA: 2:10 - loss: 0.7195 - acc: 0.5234
3264/4849 [===================>..........] - ETA: 2:05 - loss: 0.7190 - acc: 0.5230
3328/4849 [===================>..........] - ETA: 2:00 - loss: 0.7193 - acc: 0.5216
3392/4849 [===================>..........] - ETA: 1:54 - loss: 0.7195 - acc: 0.5209
3456/4849 [====================>.........] - ETA: 1:49 - loss: 0.7190 - acc: 0.5217
3520/4849 [====================>.........] - ETA: 1:44 - loss: 0.7184 - acc: 0.5219
3584/4849 [=====================>........] - ETA: 1:39 - loss: 0.7183 - acc: 0.5206
3648/4849 [=====================>........] - ETA: 1:33 - loss: 0.7176 - acc: 0.5219
3712/4849 [=====================>........] - ETA: 1:28 - loss: 0.7175 - acc: 0.5213
3776/4849 [======================>.......] - ETA: 1:23 - loss: 0.7185 - acc: 0.5207
3840/4849 [======================>.......] - ETA: 1:18 - loss: 0.7179 - acc: 0.5221
3904/4849 [=======================>......] - ETA: 1:13 - loss: 0.7184 - acc: 0.5213
3968/4849 [=======================>......] - ETA: 1:08 - loss: 0.7183 - acc: 0.5209
4032/4849 [=======================>......] - ETA: 1:03 - loss: 0.7178 - acc: 0.5223
4096/4849 [========================>.....] - ETA: 58s - loss: 0.7185 - acc: 0.5208 
4160/4849 [========================>.....] - ETA: 53s - loss: 0.7196 - acc: 0.5190
4224/4849 [=========================>....] - ETA: 48s - loss: 0.7196 - acc: 0.5185
4288/4849 [=========================>....] - ETA: 43s - loss: 0.7190 - acc: 0.5196
4352/4849 [=========================>....] - ETA: 38s - loss: 0.7188 - acc: 0.5186
4416/4849 [==========================>...] - ETA: 33s - loss: 0.7183 - acc: 0.5192
4480/4849 [==========================>...] - ETA: 28s - loss: 0.7179 - acc: 0.5199
4544/4849 [===========================>..] - ETA: 23s - loss: 0.7176 - acc: 0.5194
4608/4849 [===========================>..] - ETA: 18s - loss: 0.7174 - acc: 0.5195
4672/4849 [===========================>..] - ETA: 13s - loss: 0.7174 - acc: 0.5193
4736/4849 [============================>.] - ETA: 8s - loss: 0.7171 - acc: 0.5190 
4800/4849 [============================>.] - ETA: 3s - loss: 0.7169 - acc: 0.5185
4849/4849 [==============================] - 388s 80ms/step - loss: 0.7169 - acc: 0.5180 - val_loss: 0.7068 - val_acc: 0.4972

Epoch 00001: val_acc improved from -inf to 0.49722, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window21/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 5:19 - loss: 0.6741 - acc: 0.5781
 128/4849 [..............................] - ETA: 5:26 - loss: 0.6883 - acc: 0.5625
 192/4849 [>.............................] - ETA: 5:17 - loss: 0.6858 - acc: 0.5365
 256/4849 [>.............................] - ETA: 5:15 - loss: 0.6832 - acc: 0.5352
 320/4849 [>.............................] - ETA: 5:11 - loss: 0.6862 - acc: 0.5312
 384/4849 [=>............................] - ETA: 5:07 - loss: 0.6853 - acc: 0.5443
 448/4849 [=>............................] - ETA: 5:01 - loss: 0.6850 - acc: 0.5469
 512/4849 [==>...........................] - ETA: 4:56 - loss: 0.6864 - acc: 0.5469
 576/4849 [==>...........................] - ETA: 4:51 - loss: 0.6864 - acc: 0.5486
 640/4849 [==>...........................] - ETA: 4:46 - loss: 0.6844 - acc: 0.5547
 704/4849 [===>..........................] - ETA: 4:42 - loss: 0.6857 - acc: 0.5483
 768/4849 [===>..........................] - ETA: 4:36 - loss: 0.6866 - acc: 0.5443
 832/4849 [====>.........................] - ETA: 4:33 - loss: 0.6892 - acc: 0.5445
 896/4849 [====>.........................] - ETA: 4:28 - loss: 0.6861 - acc: 0.5480
 960/4849 [====>.........................] - ETA: 4:25 - loss: 0.6862 - acc: 0.5490
1024/4849 [=====>........................] - ETA: 4:21 - loss: 0.6845 - acc: 0.5547
1088/4849 [=====>........................] - ETA: 4:17 - loss: 0.6879 - acc: 0.5450
1152/4849 [======>.......................] - ETA: 4:12 - loss: 0.6877 - acc: 0.5443
1216/4849 [======>.......................] - ETA: 4:09 - loss: 0.6875 - acc: 0.5444
1280/4849 [======>.......................] - ETA: 4:06 - loss: 0.6903 - acc: 0.5430
1344/4849 [=======>......................] - ETA: 4:02 - loss: 0.6920 - acc: 0.5402
1408/4849 [=======>......................] - ETA: 3:59 - loss: 0.6932 - acc: 0.5384
1472/4849 [========>.....................] - ETA: 3:55 - loss: 0.6907 - acc: 0.5435
1536/4849 [========>.....................] - ETA: 3:50 - loss: 0.6924 - acc: 0.5410
1600/4849 [========>.....................] - ETA: 3:46 - loss: 0.6931 - acc: 0.5394
1664/4849 [=========>....................] - ETA: 3:42 - loss: 0.6929 - acc: 0.5379
1728/4849 [=========>....................] - ETA: 3:37 - loss: 0.6944 - acc: 0.5359
1792/4849 [==========>...................] - ETA: 3:33 - loss: 0.6963 - acc: 0.5335
1856/4849 [==========>...................] - ETA: 3:29 - loss: 0.6972 - acc: 0.5318
1920/4849 [==========>...................] - ETA: 3:25 - loss: 0.6981 - acc: 0.5312
1984/4849 [===========>..................] - ETA: 3:21 - loss: 0.6985 - acc: 0.5312
2048/4849 [===========>..................] - ETA: 3:16 - loss: 0.6990 - acc: 0.5298
2112/4849 [============>.................] - ETA: 3:12 - loss: 0.6985 - acc: 0.5308
2176/4849 [============>.................] - ETA: 3:07 - loss: 0.6991 - acc: 0.5294
2240/4849 [============>.................] - ETA: 3:02 - loss: 0.6984 - acc: 0.5299
2304/4849 [=============>................] - ETA: 2:57 - loss: 0.6988 - acc: 0.5282
2368/4849 [=============>................] - ETA: 2:53 - loss: 0.7003 - acc: 0.5249
2432/4849 [==============>...............] - ETA: 2:48 - loss: 0.6999 - acc: 0.5267
2496/4849 [==============>...............] - ETA: 2:44 - loss: 0.7003 - acc: 0.5268
2560/4849 [==============>...............] - ETA: 2:39 - loss: 0.7011 - acc: 0.5250
2624/4849 [===============>..............] - ETA: 2:35 - loss: 0.7024 - acc: 0.5221
2688/4849 [===============>..............] - ETA: 2:30 - loss: 0.7030 - acc: 0.5212
2752/4849 [================>.............] - ETA: 2:26 - loss: 0.7031 - acc: 0.5203
2816/4849 [================>.............] - ETA: 2:21 - loss: 0.7015 - acc: 0.5238
2880/4849 [================>.............] - ETA: 2:17 - loss: 0.7013 - acc: 0.5229
2944/4849 [=================>............] - ETA: 2:13 - loss: 0.7004 - acc: 0.5238
3008/4849 [=================>............] - ETA: 2:08 - loss: 0.7008 - acc: 0.5226
3072/4849 [==================>...........] - ETA: 2:04 - loss: 0.7004 - acc: 0.5234
3136/4849 [==================>...........] - ETA: 1:59 - loss: 0.6999 - acc: 0.5249
3200/4849 [==================>...........] - ETA: 1:54 - loss: 0.6999 - acc: 0.5259
3264/4849 [===================>..........] - ETA: 1:50 - loss: 0.6998 - acc: 0.5267
3328/4849 [===================>..........] - ETA: 1:45 - loss: 0.7006 - acc: 0.5246
3392/4849 [===================>..........] - ETA: 1:41 - loss: 0.7013 - acc: 0.5236
3456/4849 [====================>.........] - ETA: 1:36 - loss: 0.7027 - acc: 0.5220
3520/4849 [====================>.........] - ETA: 1:32 - loss: 0.7028 - acc: 0.5210
3584/4849 [=====================>........] - ETA: 1:27 - loss: 0.7031 - acc: 0.5220
3648/4849 [=====================>........] - ETA: 1:22 - loss: 0.7028 - acc: 0.5217
3712/4849 [=====================>........] - ETA: 1:18 - loss: 0.7030 - acc: 0.5207
3776/4849 [======================>.......] - ETA: 1:14 - loss: 0.7037 - acc: 0.5199
3840/4849 [======================>.......] - ETA: 1:09 - loss: 0.7042 - acc: 0.5188
3904/4849 [=======================>......] - ETA: 1:05 - loss: 0.7041 - acc: 0.5179
3968/4849 [=======================>......] - ETA: 1:00 - loss: 0.7042 - acc: 0.5179
4032/4849 [=======================>......] - ETA: 56s - loss: 0.7038 - acc: 0.5191 
4096/4849 [========================>.....] - ETA: 51s - loss: 0.7039 - acc: 0.5193
4160/4849 [========================>.....] - ETA: 47s - loss: 0.7035 - acc: 0.5197
4224/4849 [=========================>....] - ETA: 43s - loss: 0.7036 - acc: 0.5182
4288/4849 [=========================>....] - ETA: 38s - loss: 0.7039 - acc: 0.5170
4352/4849 [=========================>....] - ETA: 34s - loss: 0.7045 - acc: 0.5165
4416/4849 [==========================>...] - ETA: 29s - loss: 0.7041 - acc: 0.5172
4480/4849 [==========================>...] - ETA: 25s - loss: 0.7042 - acc: 0.5165
4544/4849 [===========================>..] - ETA: 21s - loss: 0.7046 - acc: 0.5158
4608/4849 [===========================>..] - ETA: 16s - loss: 0.7046 - acc: 0.5158
4672/4849 [===========================>..] - ETA: 12s - loss: 0.7045 - acc: 0.5167
4736/4849 [============================>.] - ETA: 7s - loss: 0.7050 - acc: 0.5150 
4800/4849 [============================>.] - ETA: 3s - loss: 0.7049 - acc: 0.5158
4849/4849 [==============================] - 349s 72ms/step - loss: 0.7052 - acc: 0.5147 - val_loss: 0.6900 - val_acc: 0.5529

Epoch 00002: val_acc improved from 0.49722 to 0.55288, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window21/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 3/10

  64/4849 [..............................] - ETA: 5:35 - loss: 0.6901 - acc: 0.5000
 128/4849 [..............................] - ETA: 5:30 - loss: 0.6873 - acc: 0.5391
 192/4849 [>.............................] - ETA: 5:29 - loss: 0.6800 - acc: 0.5625
 256/4849 [>.............................] - ETA: 5:20 - loss: 0.6840 - acc: 0.5508
 320/4849 [>.............................] - ETA: 5:20 - loss: 0.6891 - acc: 0.5375
 384/4849 [=>............................] - ETA: 5:12 - loss: 0.6918 - acc: 0.5182
 448/4849 [=>............................] - ETA: 5:10 - loss: 0.6943 - acc: 0.5201
 512/4849 [==>...........................] - ETA: 5:02 - loss: 0.6953 - acc: 0.5195
 576/4849 [==>...........................] - ETA: 4:58 - loss: 0.6964 - acc: 0.5243
 640/4849 [==>...........................] - ETA: 4:51 - loss: 0.6930 - acc: 0.5312
 704/4849 [===>..........................] - ETA: 4:48 - loss: 0.6947 - acc: 0.5341
 768/4849 [===>..........................] - ETA: 4:43 - loss: 0.6997 - acc: 0.5247
 832/4849 [====>.........................] - ETA: 4:39 - loss: 0.6990 - acc: 0.5240
 896/4849 [====>.........................] - ETA: 4:34 - loss: 0.6997 - acc: 0.5190
 960/4849 [====>.........................] - ETA: 4:29 - loss: 0.6997 - acc: 0.5208
1024/4849 [=====>........................] - ETA: 4:25 - loss: 0.7000 - acc: 0.5205
1088/4849 [=====>........................] - ETA: 4:20 - loss: 0.6988 - acc: 0.5257
1152/4849 [======>.......................] - ETA: 4:16 - loss: 0.7007 - acc: 0.5200
1216/4849 [======>.......................] - ETA: 4:11 - loss: 0.6994 - acc: 0.5255
1280/4849 [======>.......................] - ETA: 4:06 - loss: 0.6991 - acc: 0.5281
1344/4849 [=======>......................] - ETA: 4:01 - loss: 0.6993 - acc: 0.5283
1408/4849 [=======>......................] - ETA: 3:57 - loss: 0.7001 - acc: 0.5277
1472/4849 [========>.....................] - ETA: 3:52 - loss: 0.6991 - acc: 0.5299
1536/4849 [========>.....................] - ETA: 3:48 - loss: 0.6978 - acc: 0.5312
1600/4849 [========>.....................] - ETA: 3:44 - loss: 0.6977 - acc: 0.5294
1664/4849 [=========>....................] - ETA: 3:40 - loss: 0.6965 - acc: 0.5343
1728/4849 [=========>....................] - ETA: 3:35 - loss: 0.6966 - acc: 0.5336
1792/4849 [==========>...................] - ETA: 3:31 - loss: 0.6964 - acc: 0.5340
1856/4849 [==========>...................] - ETA: 3:27 - loss: 0.6966 - acc: 0.5366
1920/4849 [==========>...................] - ETA: 3:23 - loss: 0.6964 - acc: 0.5365
1984/4849 [===========>..................] - ETA: 3:18 - loss: 0.6967 - acc: 0.5333
2048/4849 [===========>..................] - ETA: 3:14 - loss: 0.6964 - acc: 0.5332
2112/4849 [============>.................] - ETA: 3:10 - loss: 0.6952 - acc: 0.5350
2176/4849 [============>.................] - ETA: 3:05 - loss: 0.6949 - acc: 0.5354
2240/4849 [============>.................] - ETA: 3:01 - loss: 0.6947 - acc: 0.5371
2304/4849 [=============>................] - ETA: 2:57 - loss: 0.6958 - acc: 0.5330
2368/4849 [=============>................] - ETA: 2:52 - loss: 0.6960 - acc: 0.5329
2432/4849 [==============>...............] - ETA: 2:48 - loss: 0.6960 - acc: 0.5321
2496/4849 [==============>...............] - ETA: 2:43 - loss: 0.6964 - acc: 0.5329
2560/4849 [==============>...............] - ETA: 2:39 - loss: 0.6965 - acc: 0.5340
2624/4849 [===============>..............] - ETA: 2:34 - loss: 0.6971 - acc: 0.5347
2688/4849 [===============>..............] - ETA: 2:30 - loss: 0.6971 - acc: 0.5346
2752/4849 [================>.............] - ETA: 2:26 - loss: 0.6969 - acc: 0.5363
2816/4849 [================>.............] - ETA: 2:21 - loss: 0.6974 - acc: 0.5355
2880/4849 [================>.............] - ETA: 2:17 - loss: 0.6969 - acc: 0.5351
2944/4849 [=================>............] - ETA: 2:12 - loss: 0.6976 - acc: 0.5329
3008/4849 [=================>............] - ETA: 2:08 - loss: 0.6980 - acc: 0.5319
3072/4849 [==================>...........] - ETA: 2:04 - loss: 0.6978 - acc: 0.5329
3136/4849 [==================>...........] - ETA: 1:59 - loss: 0.6976 - acc: 0.5328
3200/4849 [==================>...........] - ETA: 1:55 - loss: 0.6971 - acc: 0.5347
3264/4849 [===================>..........] - ETA: 1:50 - loss: 0.6978 - acc: 0.5309
3328/4849 [===================>..........] - ETA: 1:46 - loss: 0.6978 - acc: 0.5306
3392/4849 [===================>..........] - ETA: 1:41 - loss: 0.6979 - acc: 0.5301
3456/4849 [====================>.........] - ETA: 1:37 - loss: 0.6983 - acc: 0.5281
3520/4849 [====================>.........] - ETA: 1:33 - loss: 0.6980 - acc: 0.5287
3584/4849 [=====================>........] - ETA: 1:28 - loss: 0.6984 - acc: 0.5285
3648/4849 [=====================>........] - ETA: 1:24 - loss: 0.6982 - acc: 0.5282
3712/4849 [=====================>........] - ETA: 1:19 - loss: 0.6973 - acc: 0.5296
3776/4849 [======================>.......] - ETA: 1:15 - loss: 0.6974 - acc: 0.5305
3840/4849 [======================>.......] - ETA: 1:10 - loss: 0.6977 - acc: 0.5297
3904/4849 [=======================>......] - ETA: 1:06 - loss: 0.6980 - acc: 0.5284
3968/4849 [=======================>......] - ETA: 1:01 - loss: 0.6988 - acc: 0.5277
4032/4849 [=======================>......] - ETA: 57s - loss: 0.6985 - acc: 0.5275 
4096/4849 [========================>.....] - ETA: 52s - loss: 0.6983 - acc: 0.5271
4160/4849 [========================>.....] - ETA: 48s - loss: 0.6981 - acc: 0.5272
4224/4849 [=========================>....] - ETA: 43s - loss: 0.6983 - acc: 0.5268
4288/4849 [=========================>....] - ETA: 39s - loss: 0.6980 - acc: 0.5273
4352/4849 [=========================>....] - ETA: 34s - loss: 0.6982 - acc: 0.5264
4416/4849 [==========================>...] - ETA: 30s - loss: 0.6979 - acc: 0.5276
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6973 - acc: 0.5299
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6971 - acc: 0.5295
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6976 - acc: 0.5280
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6977 - acc: 0.5274
4736/4849 [============================>.] - ETA: 7s - loss: 0.6973 - acc: 0.5281 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6973 - acc: 0.5281
4849/4849 [==============================] - 352s 73ms/step - loss: 0.6974 - acc: 0.5271 - val_loss: 0.6932 - val_acc: 0.5195

Epoch 00003: val_acc did not improve from 0.55288
Epoch 4/10

  64/4849 [..............................] - ETA: 4:57 - loss: 0.6912 - acc: 0.5469
 128/4849 [..............................] - ETA: 5:02 - loss: 0.7156 - acc: 0.4609
 192/4849 [>.............................] - ETA: 4:49 - loss: 0.7117 - acc: 0.4896
 256/4849 [>.............................] - ETA: 4:46 - loss: 0.7166 - acc: 0.4727
 320/4849 [>.............................] - ETA: 4:37 - loss: 0.7097 - acc: 0.5000
 384/4849 [=>............................] - ETA: 4:36 - loss: 0.7031 - acc: 0.5208
 448/4849 [=>............................] - ETA: 4:32 - loss: 0.7024 - acc: 0.5223
 512/4849 [==>...........................] - ETA: 4:30 - loss: 0.7007 - acc: 0.5215
 576/4849 [==>...........................] - ETA: 4:24 - loss: 0.6989 - acc: 0.5208
 640/4849 [==>...........................] - ETA: 4:21 - loss: 0.6972 - acc: 0.5250
 704/4849 [===>..........................] - ETA: 4:17 - loss: 0.6972 - acc: 0.5241
 768/4849 [===>..........................] - ETA: 4:14 - loss: 0.6970 - acc: 0.5221
 832/4849 [====>.........................] - ETA: 4:10 - loss: 0.6930 - acc: 0.5325
 896/4849 [====>.........................] - ETA: 4:06 - loss: 0.6932 - acc: 0.5301
 960/4849 [====>.........................] - ETA: 4:03 - loss: 0.6932 - acc: 0.5312
1024/4849 [=====>........................] - ETA: 4:00 - loss: 0.6926 - acc: 0.5361
1088/4849 [=====>........................] - ETA: 3:56 - loss: 0.6932 - acc: 0.5349
1152/4849 [======>.......................] - ETA: 3:53 - loss: 0.6933 - acc: 0.5391
1216/4849 [======>.......................] - ETA: 3:51 - loss: 0.6919 - acc: 0.5411
1280/4849 [======>.......................] - ETA: 3:47 - loss: 0.6912 - acc: 0.5406
1344/4849 [=======>......................] - ETA: 3:43 - loss: 0.6908 - acc: 0.5424
1408/4849 [=======>......................] - ETA: 3:40 - loss: 0.6896 - acc: 0.5455
1472/4849 [========>.....................] - ETA: 3:36 - loss: 0.6889 - acc: 0.5442
1536/4849 [========>.....................] - ETA: 3:33 - loss: 0.6881 - acc: 0.5469
1600/4849 [========>.....................] - ETA: 3:28 - loss: 0.6878 - acc: 0.5463
1664/4849 [=========>....................] - ETA: 3:25 - loss: 0.6873 - acc: 0.5469
1728/4849 [=========>....................] - ETA: 3:21 - loss: 0.6874 - acc: 0.5480
1792/4849 [==========>...................] - ETA: 3:18 - loss: 0.6877 - acc: 0.5491
1856/4849 [==========>...................] - ETA: 3:14 - loss: 0.6864 - acc: 0.5528
1920/4849 [==========>...................] - ETA: 3:10 - loss: 0.6874 - acc: 0.5490
1984/4849 [===========>..................] - ETA: 3:06 - loss: 0.6908 - acc: 0.5439
2048/4849 [===========>..................] - ETA: 3:02 - loss: 0.6909 - acc: 0.5425
2112/4849 [============>.................] - ETA: 2:58 - loss: 0.6910 - acc: 0.5440
2176/4849 [============>.................] - ETA: 2:54 - loss: 0.6918 - acc: 0.5418
2240/4849 [============>.................] - ETA: 2:50 - loss: 0.6916 - acc: 0.5442
2304/4849 [=============>................] - ETA: 2:46 - loss: 0.6914 - acc: 0.5456
2368/4849 [=============>................] - ETA: 2:42 - loss: 0.6917 - acc: 0.5456
2432/4849 [==============>...............] - ETA: 2:38 - loss: 0.6917 - acc: 0.5465
2496/4849 [==============>...............] - ETA: 2:34 - loss: 0.6915 - acc: 0.5465
2560/4849 [==============>...............] - ETA: 2:30 - loss: 0.6915 - acc: 0.5477
2624/4849 [===============>..............] - ETA: 2:26 - loss: 0.6920 - acc: 0.5457
2688/4849 [===============>..............] - ETA: 2:22 - loss: 0.6928 - acc: 0.5439
2752/4849 [================>.............] - ETA: 2:18 - loss: 0.6932 - acc: 0.5403
2816/4849 [================>.............] - ETA: 2:14 - loss: 0.6943 - acc: 0.5362
2880/4849 [================>.............] - ETA: 2:10 - loss: 0.6936 - acc: 0.5375
2944/4849 [=================>............] - ETA: 2:06 - loss: 0.6931 - acc: 0.5384
3008/4849 [=================>............] - ETA: 2:02 - loss: 0.6928 - acc: 0.5392
3072/4849 [==================>...........] - ETA: 1:58 - loss: 0.6934 - acc: 0.5371
3136/4849 [==================>...........] - ETA: 1:54 - loss: 0.6940 - acc: 0.5364
3200/4849 [==================>...........] - ETA: 1:50 - loss: 0.6938 - acc: 0.5369
3264/4849 [===================>..........] - ETA: 1:46 - loss: 0.6938 - acc: 0.5365
3328/4849 [===================>..........] - ETA: 1:41 - loss: 0.6939 - acc: 0.5349
3392/4849 [===================>..........] - ETA: 1:37 - loss: 0.6937 - acc: 0.5357
3456/4849 [====================>.........] - ETA: 1:33 - loss: 0.6938 - acc: 0.5353
3520/4849 [====================>.........] - ETA: 1:29 - loss: 0.6934 - acc: 0.5378
3584/4849 [=====================>........] - ETA: 1:24 - loss: 0.6932 - acc: 0.5385
3648/4849 [=====================>........] - ETA: 1:20 - loss: 0.6927 - acc: 0.5395
3712/4849 [=====================>........] - ETA: 1:16 - loss: 0.6925 - acc: 0.5404
3776/4849 [======================>.......] - ETA: 1:12 - loss: 0.6931 - acc: 0.5376
3840/4849 [======================>.......] - ETA: 1:07 - loss: 0.6936 - acc: 0.5365
3904/4849 [=======================>......] - ETA: 1:03 - loss: 0.6933 - acc: 0.5374
3968/4849 [=======================>......] - ETA: 59s - loss: 0.6932 - acc: 0.5388 
4032/4849 [=======================>......] - ETA: 55s - loss: 0.6932 - acc: 0.5382
4096/4849 [========================>.....] - ETA: 50s - loss: 0.6934 - acc: 0.5371
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6931 - acc: 0.5382
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6932 - acc: 0.5374
4288/4849 [=========================>....] - ETA: 37s - loss: 0.6934 - acc: 0.5373
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6932 - acc: 0.5375
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6931 - acc: 0.5383
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6928 - acc: 0.5391
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6924 - acc: 0.5409
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6917 - acc: 0.5425
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6920 - acc: 0.5422
4736/4849 [============================>.] - ETA: 7s - loss: 0.6918 - acc: 0.5427 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6926 - acc: 0.5419
4849/4849 [==============================] - 342s 71ms/step - loss: 0.6926 - acc: 0.5420 - val_loss: 0.6897 - val_acc: 0.5288

Epoch 00004: val_acc did not improve from 0.55288
Epoch 5/10

  64/4849 [..............................] - ETA: 5:13 - loss: 0.7359 - acc: 0.4688
 128/4849 [..............................] - ETA: 5:17 - loss: 0.7198 - acc: 0.4844
 192/4849 [>.............................] - ETA: 5:12 - loss: 0.7201 - acc: 0.4844
 256/4849 [>.............................] - ETA: 5:12 - loss: 0.7163 - acc: 0.5000
 320/4849 [>.............................] - ETA: 5:02 - loss: 0.7107 - acc: 0.5000
 384/4849 [=>............................] - ETA: 5:00 - loss: 0.7046 - acc: 0.5104
 448/4849 [=>............................] - ETA: 4:53 - loss: 0.6964 - acc: 0.5312
 512/4849 [==>...........................] - ETA: 4:50 - loss: 0.6983 - acc: 0.5254
 576/4849 [==>...........................] - ETA: 4:43 - loss: 0.6970 - acc: 0.5260
 640/4849 [==>...........................] - ETA: 4:42 - loss: 0.6934 - acc: 0.5344
 704/4849 [===>..........................] - ETA: 4:39 - loss: 0.6935 - acc: 0.5298
 768/4849 [===>..........................] - ETA: 4:35 - loss: 0.6916 - acc: 0.5378
 832/4849 [====>.........................] - ETA: 4:29 - loss: 0.6902 - acc: 0.5445
 896/4849 [====>.........................] - ETA: 4:25 - loss: 0.6881 - acc: 0.5525
 960/4849 [====>.........................] - ETA: 4:20 - loss: 0.6891 - acc: 0.5531
1024/4849 [=====>........................] - ETA: 4:17 - loss: 0.6869 - acc: 0.5527
1088/4849 [=====>........................] - ETA: 4:12 - loss: 0.6877 - acc: 0.5515
1152/4849 [======>.......................] - ETA: 4:10 - loss: 0.6861 - acc: 0.5573
1216/4849 [======>.......................] - ETA: 4:06 - loss: 0.6893 - acc: 0.5510
1280/4849 [======>.......................] - ETA: 4:02 - loss: 0.6872 - acc: 0.5547
1344/4849 [=======>......................] - ETA: 3:57 - loss: 0.6881 - acc: 0.5506
1408/4849 [=======>......................] - ETA: 3:53 - loss: 0.6887 - acc: 0.5526
1472/4849 [========>.....................] - ETA: 3:48 - loss: 0.6891 - acc: 0.5537
1536/4849 [========>.....................] - ETA: 3:44 - loss: 0.6883 - acc: 0.5566
1600/4849 [========>.....................] - ETA: 3:40 - loss: 0.6898 - acc: 0.5531
1664/4849 [=========>....................] - ETA: 3:36 - loss: 0.6895 - acc: 0.5541
1728/4849 [=========>....................] - ETA: 3:31 - loss: 0.6904 - acc: 0.5515
1792/4849 [==========>...................] - ETA: 3:27 - loss: 0.6904 - acc: 0.5508
1856/4849 [==========>...................] - ETA: 3:23 - loss: 0.6925 - acc: 0.5474
1920/4849 [==========>...................] - ETA: 3:18 - loss: 0.6937 - acc: 0.5427
1984/4849 [===========>..................] - ETA: 3:14 - loss: 0.6940 - acc: 0.5408
2048/4849 [===========>..................] - ETA: 3:10 - loss: 0.6942 - acc: 0.5386
2112/4849 [============>.................] - ETA: 3:06 - loss: 0.6928 - acc: 0.5417
2176/4849 [============>.................] - ETA: 3:01 - loss: 0.6929 - acc: 0.5404
2240/4849 [============>.................] - ETA: 2:57 - loss: 0.6935 - acc: 0.5384
2304/4849 [=============>................] - ETA: 2:53 - loss: 0.6938 - acc: 0.5365
2368/4849 [=============>................] - ETA: 2:49 - loss: 0.6939 - acc: 0.5355
2432/4849 [==============>...............] - ETA: 2:44 - loss: 0.6934 - acc: 0.5354
2496/4849 [==============>...............] - ETA: 2:40 - loss: 0.6942 - acc: 0.5337
2560/4849 [==============>...............] - ETA: 2:36 - loss: 0.6925 - acc: 0.5375
2624/4849 [===============>..............] - ETA: 2:31 - loss: 0.6933 - acc: 0.5354
2688/4849 [===============>..............] - ETA: 2:27 - loss: 0.6932 - acc: 0.5372
2752/4849 [================>.............] - ETA: 2:23 - loss: 0.6928 - acc: 0.5374
2816/4849 [================>.............] - ETA: 2:19 - loss: 0.6930 - acc: 0.5380
2880/4849 [================>.............] - ETA: 2:14 - loss: 0.6926 - acc: 0.5389
2944/4849 [=================>............] - ETA: 2:10 - loss: 0.6919 - acc: 0.5408
3008/4849 [=================>............] - ETA: 2:06 - loss: 0.6919 - acc: 0.5419
3072/4849 [==================>...........] - ETA: 2:01 - loss: 0.6914 - acc: 0.5443
3136/4849 [==================>...........] - ETA: 1:57 - loss: 0.6911 - acc: 0.5443
3200/4849 [==================>...........] - ETA: 1:52 - loss: 0.6912 - acc: 0.5444
3264/4849 [===================>..........] - ETA: 1:48 - loss: 0.6909 - acc: 0.5444
3328/4849 [===================>..........] - ETA: 1:44 - loss: 0.6911 - acc: 0.5421
3392/4849 [===================>..........] - ETA: 1:39 - loss: 0.6903 - acc: 0.5439
3456/4849 [====================>.........] - ETA: 1:35 - loss: 0.6902 - acc: 0.5437
3520/4849 [====================>.........] - ETA: 1:31 - loss: 0.6900 - acc: 0.5443
3584/4849 [=====================>........] - ETA: 1:26 - loss: 0.6897 - acc: 0.5441
3648/4849 [=====================>........] - ETA: 1:22 - loss: 0.6894 - acc: 0.5441
3712/4849 [=====================>........] - ETA: 1:17 - loss: 0.6895 - acc: 0.5434
3776/4849 [======================>.......] - ETA: 1:13 - loss: 0.6893 - acc: 0.5434
3840/4849 [======================>.......] - ETA: 1:09 - loss: 0.6887 - acc: 0.5443
3904/4849 [=======================>......] - ETA: 1:04 - loss: 0.6886 - acc: 0.5453
3968/4849 [=======================>......] - ETA: 1:00 - loss: 0.6889 - acc: 0.5444
4032/4849 [=======================>......] - ETA: 56s - loss: 0.6890 - acc: 0.5451 
4096/4849 [========================>.....] - ETA: 51s - loss: 0.6890 - acc: 0.5461
4160/4849 [========================>.....] - ETA: 47s - loss: 0.6892 - acc: 0.5447
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6892 - acc: 0.5447
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6885 - acc: 0.5457
4352/4849 [=========================>....] - ETA: 34s - loss: 0.6882 - acc: 0.5462
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6882 - acc: 0.5462
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6890 - acc: 0.5451
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6891 - acc: 0.5456
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6895 - acc: 0.5447
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6895 - acc: 0.5449
4736/4849 [============================>.] - ETA: 7s - loss: 0.6890 - acc: 0.5465 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6891 - acc: 0.5467
4849/4849 [==============================] - 347s 72ms/step - loss: 0.6890 - acc: 0.5465 - val_loss: 0.7015 - val_acc: 0.5158

Epoch 00005: val_acc did not improve from 0.55288
Epoch 6/10

  64/4849 [..............................] - ETA: 5:32 - loss: 0.6587 - acc: 0.6406
 128/4849 [..............................] - ETA: 5:25 - loss: 0.6774 - acc: 0.5781
 192/4849 [>.............................] - ETA: 5:31 - loss: 0.6808 - acc: 0.5729
 256/4849 [>.............................] - ETA: 5:25 - loss: 0.6847 - acc: 0.5430
 320/4849 [>.............................] - ETA: 5:19 - loss: 0.6839 - acc: 0.5531
 384/4849 [=>............................] - ETA: 5:13 - loss: 0.6809 - acc: 0.5599
 448/4849 [=>............................] - ETA: 5:09 - loss: 0.6833 - acc: 0.5558
 512/4849 [==>...........................] - ETA: 5:04 - loss: 0.6824 - acc: 0.5586
 576/4849 [==>...........................] - ETA: 5:00 - loss: 0.6859 - acc: 0.5573
 640/4849 [==>...........................] - ETA: 4:55 - loss: 0.6875 - acc: 0.5563
 704/4849 [===>..........................] - ETA: 4:51 - loss: 0.6889 - acc: 0.5526
 768/4849 [===>..........................] - ETA: 4:49 - loss: 0.6889 - acc: 0.5482
 832/4849 [====>.........................] - ETA: 4:44 - loss: 0.6875 - acc: 0.5493
 896/4849 [====>.........................] - ETA: 4:39 - loss: 0.6877 - acc: 0.5469
 960/4849 [====>.........................] - ETA: 4:34 - loss: 0.6867 - acc: 0.5510
1024/4849 [=====>........................] - ETA: 4:29 - loss: 0.6889 - acc: 0.5479
1088/4849 [=====>........................] - ETA: 4:24 - loss: 0.6891 - acc: 0.5496
1152/4849 [======>.......................] - ETA: 4:20 - loss: 0.6880 - acc: 0.5530
1216/4849 [======>.......................] - ETA: 4:14 - loss: 0.6875 - acc: 0.5535
1280/4849 [======>.......................] - ETA: 4:10 - loss: 0.6881 - acc: 0.5523
1344/4849 [=======>......................] - ETA: 4:05 - loss: 0.6885 - acc: 0.5506
1408/4849 [=======>......................] - ETA: 4:00 - loss: 0.6891 - acc: 0.5518
1472/4849 [========>.....................] - ETA: 3:55 - loss: 0.6881 - acc: 0.5523
1536/4849 [========>.....................] - ETA: 3:51 - loss: 0.6884 - acc: 0.5534
1600/4849 [========>.....................] - ETA: 3:46 - loss: 0.6889 - acc: 0.5519
1664/4849 [=========>....................] - ETA: 3:42 - loss: 0.6893 - acc: 0.5517
1728/4849 [=========>....................] - ETA: 3:38 - loss: 0.6911 - acc: 0.5515
1792/4849 [==========>...................] - ETA: 3:33 - loss: 0.6917 - acc: 0.5480
1856/4849 [==========>...................] - ETA: 3:29 - loss: 0.6911 - acc: 0.5506
1920/4849 [==========>...................] - ETA: 3:24 - loss: 0.6908 - acc: 0.5521
1984/4849 [===========>..................] - ETA: 3:19 - loss: 0.6901 - acc: 0.5519
2048/4849 [===========>..................] - ETA: 3:14 - loss: 0.6889 - acc: 0.5547
2112/4849 [============>.................] - ETA: 3:10 - loss: 0.6878 - acc: 0.5597
2176/4849 [============>.................] - ETA: 3:05 - loss: 0.6885 - acc: 0.5579
2240/4849 [============>.................] - ETA: 3:01 - loss: 0.6895 - acc: 0.5567
2304/4849 [=============>................] - ETA: 2:56 - loss: 0.6884 - acc: 0.5582
2368/4849 [=============>................] - ETA: 2:52 - loss: 0.6878 - acc: 0.5600
2432/4849 [==============>...............] - ETA: 2:47 - loss: 0.6873 - acc: 0.5609
2496/4849 [==============>...............] - ETA: 2:43 - loss: 0.6877 - acc: 0.5597
2560/4849 [==============>...............] - ETA: 2:38 - loss: 0.6888 - acc: 0.5574
2624/4849 [===============>..............] - ETA: 2:34 - loss: 0.6886 - acc: 0.5575
2688/4849 [===============>..............] - ETA: 2:30 - loss: 0.6885 - acc: 0.5577
2752/4849 [================>.............] - ETA: 2:25 - loss: 0.6887 - acc: 0.5567
2816/4849 [================>.............] - ETA: 2:21 - loss: 0.6888 - acc: 0.5575
2880/4849 [================>.............] - ETA: 2:16 - loss: 0.6883 - acc: 0.5583
2944/4849 [=================>............] - ETA: 2:12 - loss: 0.6874 - acc: 0.5608
3008/4849 [=================>............] - ETA: 2:07 - loss: 0.6883 - acc: 0.5598
3072/4849 [==================>...........] - ETA: 2:03 - loss: 0.6890 - acc: 0.5586
3136/4849 [==================>...........] - ETA: 1:58 - loss: 0.6890 - acc: 0.5596
3200/4849 [==================>...........] - ETA: 1:54 - loss: 0.6892 - acc: 0.5584
3264/4849 [===================>..........] - ETA: 1:50 - loss: 0.6883 - acc: 0.5610
3328/4849 [===================>..........] - ETA: 1:45 - loss: 0.6879 - acc: 0.5616
3392/4849 [===================>..........] - ETA: 1:41 - loss: 0.6877 - acc: 0.5631
3456/4849 [====================>.........] - ETA: 1:36 - loss: 0.6877 - acc: 0.5616
3520/4849 [====================>.........] - ETA: 1:32 - loss: 0.6878 - acc: 0.5608
3584/4849 [=====================>........] - ETA: 1:27 - loss: 0.6880 - acc: 0.5608
3648/4849 [=====================>........] - ETA: 1:23 - loss: 0.6872 - acc: 0.5620
3712/4849 [=====================>........] - ETA: 1:18 - loss: 0.6873 - acc: 0.5612
3776/4849 [======================>.......] - ETA: 1:14 - loss: 0.6866 - acc: 0.5625
3840/4849 [======================>.......] - ETA: 1:10 - loss: 0.6864 - acc: 0.5617
3904/4849 [=======================>......] - ETA: 1:05 - loss: 0.6860 - acc: 0.5628
3968/4849 [=======================>......] - ETA: 1:01 - loss: 0.6854 - acc: 0.5640
4032/4849 [=======================>......] - ETA: 56s - loss: 0.6852 - acc: 0.5637 
4096/4849 [========================>.....] - ETA: 52s - loss: 0.6860 - acc: 0.5615
4160/4849 [========================>.....] - ETA: 47s - loss: 0.6873 - acc: 0.5584
4224/4849 [=========================>....] - ETA: 43s - loss: 0.6867 - acc: 0.5589
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6860 - acc: 0.5606
4352/4849 [=========================>....] - ETA: 34s - loss: 0.6863 - acc: 0.5588
4416/4849 [==========================>...] - ETA: 30s - loss: 0.6858 - acc: 0.5600
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6860 - acc: 0.5596
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6860 - acc: 0.5594
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6855 - acc: 0.5603
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6851 - acc: 0.5612
4736/4849 [============================>.] - ETA: 7s - loss: 0.6856 - acc: 0.5600 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6860 - acc: 0.5596
4849/4849 [==============================] - 349s 72ms/step - loss: 0.6859 - acc: 0.5593 - val_loss: 0.6951 - val_acc: 0.5473

Epoch 00006: val_acc did not improve from 0.55288
Epoch 7/10

  64/4849 [..............................] - ETA: 5:14 - loss: 0.6834 - acc: 0.5156
 128/4849 [..............................] - ETA: 5:23 - loss: 0.6887 - acc: 0.5547
 192/4849 [>.............................] - ETA: 5:15 - loss: 0.6829 - acc: 0.5573
 256/4849 [>.............................] - ETA: 5:14 - loss: 0.6801 - acc: 0.5625
 320/4849 [>.............................] - ETA: 5:10 - loss: 0.6808 - acc: 0.5625
 384/4849 [=>............................] - ETA: 5:05 - loss: 0.6800 - acc: 0.5651
 448/4849 [=>............................] - ETA: 5:00 - loss: 0.6770 - acc: 0.5692
 512/4849 [==>...........................] - ETA: 4:55 - loss: 0.6766 - acc: 0.5703
 576/4849 [==>...........................] - ETA: 4:54 - loss: 0.6748 - acc: 0.5712
 640/4849 [==>...........................] - ETA: 4:49 - loss: 0.6759 - acc: 0.5656
 704/4849 [===>..........................] - ETA: 4:46 - loss: 0.6754 - acc: 0.5696
 768/4849 [===>..........................] - ETA: 4:41 - loss: 0.6795 - acc: 0.5638
 832/4849 [====>.........................] - ETA: 4:38 - loss: 0.6791 - acc: 0.5673
 896/4849 [====>.........................] - ETA: 4:33 - loss: 0.6793 - acc: 0.5658
 960/4849 [====>.........................] - ETA: 4:29 - loss: 0.6823 - acc: 0.5563
1024/4849 [=====>........................] - ETA: 4:25 - loss: 0.6795 - acc: 0.5645
1088/4849 [=====>........................] - ETA: 4:21 - loss: 0.6798 - acc: 0.5634
1152/4849 [======>.......................] - ETA: 4:15 - loss: 0.6798 - acc: 0.5616
1216/4849 [======>.......................] - ETA: 4:11 - loss: 0.6819 - acc: 0.5576
1280/4849 [======>.......................] - ETA: 4:06 - loss: 0.6814 - acc: 0.5563
1344/4849 [=======>......................] - ETA: 4:01 - loss: 0.6814 - acc: 0.5573
1408/4849 [=======>......................] - ETA: 3:57 - loss: 0.6811 - acc: 0.5568
1472/4849 [========>.....................] - ETA: 3:53 - loss: 0.6805 - acc: 0.5591
1536/4849 [========>.....................] - ETA: 3:49 - loss: 0.6814 - acc: 0.5566
1600/4849 [========>.....................] - ETA: 3:45 - loss: 0.6810 - acc: 0.5581
1664/4849 [=========>....................] - ETA: 3:40 - loss: 0.6821 - acc: 0.5553
1728/4849 [=========>....................] - ETA: 3:36 - loss: 0.6830 - acc: 0.5521
1792/4849 [==========>...................] - ETA: 3:31 - loss: 0.6818 - acc: 0.5519
1856/4849 [==========>...................] - ETA: 3:26 - loss: 0.6818 - acc: 0.5544
1920/4849 [==========>...................] - ETA: 3:22 - loss: 0.6809 - acc: 0.5568
1984/4849 [===========>..................] - ETA: 3:17 - loss: 0.6817 - acc: 0.5565
2048/4849 [===========>..................] - ETA: 3:13 - loss: 0.6818 - acc: 0.5576
2112/4849 [============>.................] - ETA: 3:08 - loss: 0.6813 - acc: 0.5601
2176/4849 [============>.................] - ETA: 3:03 - loss: 0.6810 - acc: 0.5597
2240/4849 [============>.................] - ETA: 2:59 - loss: 0.6797 - acc: 0.5616
2304/4849 [=============>................] - ETA: 2:55 - loss: 0.6795 - acc: 0.5621
2368/4849 [=============>................] - ETA: 2:50 - loss: 0.6793 - acc: 0.5633
2432/4849 [==============>...............] - ETA: 2:46 - loss: 0.6790 - acc: 0.5646
2496/4849 [==============>...............] - ETA: 2:41 - loss: 0.6781 - acc: 0.5661
2560/4849 [==============>...............] - ETA: 2:37 - loss: 0.6773 - acc: 0.5676
2624/4849 [===============>..............] - ETA: 2:33 - loss: 0.6781 - acc: 0.5667
2688/4849 [===============>..............] - ETA: 2:28 - loss: 0.6783 - acc: 0.5651
2752/4849 [================>.............] - ETA: 2:24 - loss: 0.6783 - acc: 0.5640
2816/4849 [================>.............] - ETA: 2:19 - loss: 0.6790 - acc: 0.5643
2880/4849 [================>.............] - ETA: 2:15 - loss: 0.6785 - acc: 0.5653
2944/4849 [=================>............] - ETA: 2:11 - loss: 0.6791 - acc: 0.5632
3008/4849 [=================>............] - ETA: 2:06 - loss: 0.6791 - acc: 0.5628
3072/4849 [==================>...........] - ETA: 2:02 - loss: 0.6795 - acc: 0.5632
3136/4849 [==================>...........] - ETA: 1:57 - loss: 0.6792 - acc: 0.5651
3200/4849 [==================>...........] - ETA: 1:53 - loss: 0.6792 - acc: 0.5653
3264/4849 [===================>..........] - ETA: 1:49 - loss: 0.6798 - acc: 0.5640
3328/4849 [===================>..........] - ETA: 1:44 - loss: 0.6794 - acc: 0.5637
3392/4849 [===================>..........] - ETA: 1:40 - loss: 0.6800 - acc: 0.5628
3456/4849 [====================>.........] - ETA: 1:35 - loss: 0.6800 - acc: 0.5634
3520/4849 [====================>.........] - ETA: 1:31 - loss: 0.6802 - acc: 0.5642
3584/4849 [=====================>........] - ETA: 1:26 - loss: 0.6802 - acc: 0.5650
3648/4849 [=====================>........] - ETA: 1:22 - loss: 0.6802 - acc: 0.5658
3712/4849 [=====================>........] - ETA: 1:18 - loss: 0.6808 - acc: 0.5652
3776/4849 [======================>.......] - ETA: 1:13 - loss: 0.6810 - acc: 0.5636
3840/4849 [======================>.......] - ETA: 1:09 - loss: 0.6814 - acc: 0.5617
3904/4849 [=======================>......] - ETA: 1:04 - loss: 0.6811 - acc: 0.5620
3968/4849 [=======================>......] - ETA: 1:00 - loss: 0.6806 - acc: 0.5630
4032/4849 [=======================>......] - ETA: 56s - loss: 0.6805 - acc: 0.5637 
4096/4849 [========================>.....] - ETA: 51s - loss: 0.6799 - acc: 0.5662
4160/4849 [========================>.....] - ETA: 47s - loss: 0.6796 - acc: 0.5668
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6800 - acc: 0.5658
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6797 - acc: 0.5662
4352/4849 [=========================>....] - ETA: 34s - loss: 0.6802 - acc: 0.5657
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6803 - acc: 0.5654
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6801 - acc: 0.5661
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6801 - acc: 0.5660
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6800 - acc: 0.5668
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6806 - acc: 0.5657
4736/4849 [============================>.] - ETA: 7s - loss: 0.6801 - acc: 0.5665 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6798 - acc: 0.5673
4849/4849 [==============================] - 345s 71ms/step - loss: 0.6796 - acc: 0.5680 - val_loss: 0.7169 - val_acc: 0.5213

Epoch 00007: val_acc did not improve from 0.55288
Epoch 8/10

  64/4849 [..............................] - ETA: 5:35 - loss: 0.7006 - acc: 0.4688
 128/4849 [..............................] - ETA: 5:23 - loss: 0.6937 - acc: 0.5156
 192/4849 [>.............................] - ETA: 5:18 - loss: 0.6861 - acc: 0.5417
 256/4849 [>.............................] - ETA: 5:10 - loss: 0.6871 - acc: 0.5391
 320/4849 [>.............................] - ETA: 5:08 - loss: 0.6899 - acc: 0.5344
 384/4849 [=>............................] - ETA: 5:01 - loss: 0.7017 - acc: 0.5130
 448/4849 [=>............................] - ETA: 4:59 - loss: 0.6894 - acc: 0.5402
 512/4849 [==>...........................] - ETA: 4:52 - loss: 0.6829 - acc: 0.5566
 576/4849 [==>...........................] - ETA: 4:49 - loss: 0.6778 - acc: 0.5660
 640/4849 [==>...........................] - ETA: 4:43 - loss: 0.6752 - acc: 0.5703
 704/4849 [===>..........................] - ETA: 4:40 - loss: 0.6731 - acc: 0.5739
 768/4849 [===>..........................] - ETA: 4:34 - loss: 0.6715 - acc: 0.5781
 832/4849 [====>.........................] - ETA: 4:31 - loss: 0.6724 - acc: 0.5757
 896/4849 [====>.........................] - ETA: 4:25 - loss: 0.6731 - acc: 0.5759
 960/4849 [====>.........................] - ETA: 4:21 - loss: 0.6730 - acc: 0.5802
1024/4849 [=====>........................] - ETA: 4:17 - loss: 0.6735 - acc: 0.5801
1088/4849 [=====>........................] - ETA: 4:13 - loss: 0.6714 - acc: 0.5846
1152/4849 [======>.......................] - ETA: 4:09 - loss: 0.6728 - acc: 0.5799
1216/4849 [======>.......................] - ETA: 4:05 - loss: 0.6756 - acc: 0.5773
1280/4849 [======>.......................] - ETA: 4:01 - loss: 0.6771 - acc: 0.5727
1344/4849 [=======>......................] - ETA: 3:57 - loss: 0.6769 - acc: 0.5729
1408/4849 [=======>......................] - ETA: 3:52 - loss: 0.6777 - acc: 0.5732
1472/4849 [========>.....................] - ETA: 3:48 - loss: 0.6805 - acc: 0.5679
1536/4849 [========>.....................] - ETA: 3:44 - loss: 0.6825 - acc: 0.5664
1600/4849 [========>.....................] - ETA: 3:40 - loss: 0.6808 - acc: 0.5731
1664/4849 [=========>....................] - ETA: 3:36 - loss: 0.6814 - acc: 0.5721
1728/4849 [=========>....................] - ETA: 3:32 - loss: 0.6807 - acc: 0.5747
1792/4849 [==========>...................] - ETA: 3:28 - loss: 0.6805 - acc: 0.5742
1856/4849 [==========>...................] - ETA: 3:23 - loss: 0.6802 - acc: 0.5733
1920/4849 [==========>...................] - ETA: 3:19 - loss: 0.6801 - acc: 0.5734
1984/4849 [===========>..................] - ETA: 3:14 - loss: 0.6800 - acc: 0.5751
2048/4849 [===========>..................] - ETA: 3:10 - loss: 0.6785 - acc: 0.5801
2112/4849 [============>.................] - ETA: 3:06 - loss: 0.6790 - acc: 0.5781
2176/4849 [============>.................] - ETA: 3:02 - loss: 0.6797 - acc: 0.5767
2240/4849 [============>.................] - ETA: 2:57 - loss: 0.6802 - acc: 0.5754
2304/4849 [=============>................] - ETA: 2:53 - loss: 0.6800 - acc: 0.5751
2368/4849 [=============>................] - ETA: 2:48 - loss: 0.6802 - acc: 0.5747
2432/4849 [==============>...............] - ETA: 2:44 - loss: 0.6803 - acc: 0.5748
2496/4849 [==============>...............] - ETA: 2:40 - loss: 0.6809 - acc: 0.5729
2560/4849 [==============>...............] - ETA: 2:35 - loss: 0.6805 - acc: 0.5750
2624/4849 [===============>..............] - ETA: 2:31 - loss: 0.6806 - acc: 0.5736
2688/4849 [===============>..............] - ETA: 2:27 - loss: 0.6809 - acc: 0.5729
2752/4849 [================>.............] - ETA: 2:22 - loss: 0.6803 - acc: 0.5741
2816/4849 [================>.............] - ETA: 2:18 - loss: 0.6811 - acc: 0.5721
2880/4849 [================>.............] - ETA: 2:13 - loss: 0.6805 - acc: 0.5736
2944/4849 [=================>............] - ETA: 2:09 - loss: 0.6802 - acc: 0.5744
3008/4849 [=================>............] - ETA: 2:05 - loss: 0.6800 - acc: 0.5735
3072/4849 [==================>...........] - ETA: 2:01 - loss: 0.6793 - acc: 0.5752
3136/4849 [==================>...........] - ETA: 1:56 - loss: 0.6795 - acc: 0.5746
3200/4849 [==================>...........] - ETA: 1:52 - loss: 0.6795 - acc: 0.5731
3264/4849 [===================>..........] - ETA: 1:48 - loss: 0.6790 - acc: 0.5738
3328/4849 [===================>..........] - ETA: 1:43 - loss: 0.6785 - acc: 0.5760
3392/4849 [===================>..........] - ETA: 1:39 - loss: 0.6788 - acc: 0.5746
3456/4849 [====================>.........] - ETA: 1:35 - loss: 0.6796 - acc: 0.5726
3520/4849 [====================>.........] - ETA: 1:30 - loss: 0.6786 - acc: 0.5741
3584/4849 [=====================>........] - ETA: 1:26 - loss: 0.6788 - acc: 0.5742
3648/4849 [=====================>........] - ETA: 1:22 - loss: 0.6785 - acc: 0.5743
3712/4849 [=====================>........] - ETA: 1:17 - loss: 0.6785 - acc: 0.5735
3776/4849 [======================>.......] - ETA: 1:13 - loss: 0.6780 - acc: 0.5739
3840/4849 [======================>.......] - ETA: 1:08 - loss: 0.6783 - acc: 0.5737
3904/4849 [=======================>......] - ETA: 1:04 - loss: 0.6784 - acc: 0.5733
3968/4849 [=======================>......] - ETA: 1:00 - loss: 0.6789 - acc: 0.5726
4032/4849 [=======================>......] - ETA: 55s - loss: 0.6801 - acc: 0.5712 
4096/4849 [========================>.....] - ETA: 51s - loss: 0.6815 - acc: 0.5681
4160/4849 [========================>.....] - ETA: 47s - loss: 0.6824 - acc: 0.5661
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6825 - acc: 0.5658
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6822 - acc: 0.5667
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6823 - acc: 0.5666
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6824 - acc: 0.5663
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6824 - acc: 0.5658
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6821 - acc: 0.5665
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6820 - acc: 0.5662
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6823 - acc: 0.5657
4736/4849 [============================>.] - ETA: 7s - loss: 0.6829 - acc: 0.5642 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6831 - acc: 0.5631
4849/4849 [==============================] - 344s 71ms/step - loss: 0.6832 - acc: 0.5622 - val_loss: 0.7342 - val_acc: 0.4898

Epoch 00008: val_acc did not improve from 0.55288
Epoch 9/10

  64/4849 [..............................] - ETA: 5:39 - loss: 0.7038 - acc: 0.5625
 128/4849 [..............................] - ETA: 5:20 - loss: 0.6888 - acc: 0.5859
 192/4849 [>.............................] - ETA: 5:17 - loss: 0.6802 - acc: 0.6042
 256/4849 [>.............................] - ETA: 5:09 - loss: 0.6760 - acc: 0.6016
 320/4849 [>.............................] - ETA: 5:09 - loss: 0.6821 - acc: 0.5813
 384/4849 [=>............................] - ETA: 5:01 - loss: 0.6847 - acc: 0.5781
 448/4849 [=>............................] - ETA: 4:59 - loss: 0.6865 - acc: 0.5737
 512/4849 [==>...........................] - ETA: 4:55 - loss: 0.6878 - acc: 0.5586
 576/4849 [==>...........................] - ETA: 4:52 - loss: 0.6886 - acc: 0.5538
 640/4849 [==>...........................] - ETA: 4:47 - loss: 0.6913 - acc: 0.5547
 704/4849 [===>..........................] - ETA: 4:44 - loss: 0.6892 - acc: 0.5511
 768/4849 [===>..........................] - ETA: 4:38 - loss: 0.6874 - acc: 0.5521
 832/4849 [====>.........................] - ETA: 4:35 - loss: 0.6878 - acc: 0.5517
 896/4849 [====>.........................] - ETA: 4:30 - loss: 0.6881 - acc: 0.5480
 960/4849 [====>.........................] - ETA: 4:26 - loss: 0.6862 - acc: 0.5521
1024/4849 [=====>........................] - ETA: 4:20 - loss: 0.6828 - acc: 0.5586
1088/4849 [=====>........................] - ETA: 4:16 - loss: 0.6848 - acc: 0.5561
1152/4849 [======>.......................] - ETA: 4:11 - loss: 0.6858 - acc: 0.5521
1216/4849 [======>.......................] - ETA: 4:08 - loss: 0.6840 - acc: 0.5584
1280/4849 [======>.......................] - ETA: 4:03 - loss: 0.6862 - acc: 0.5594
1344/4849 [=======>......................] - ETA: 3:59 - loss: 0.6851 - acc: 0.5625
1408/4849 [=======>......................] - ETA: 3:54 - loss: 0.6849 - acc: 0.5661
1472/4849 [========>.....................] - ETA: 3:50 - loss: 0.6870 - acc: 0.5645
1536/4849 [========>.....................] - ETA: 3:46 - loss: 0.6878 - acc: 0.5638
1600/4849 [========>.....................] - ETA: 3:42 - loss: 0.6881 - acc: 0.5631
1664/4849 [=========>....................] - ETA: 3:38 - loss: 0.6891 - acc: 0.5601
1728/4849 [=========>....................] - ETA: 3:34 - loss: 0.6886 - acc: 0.5613
1792/4849 [==========>...................] - ETA: 3:30 - loss: 0.6889 - acc: 0.5597
1856/4849 [==========>...................] - ETA: 3:26 - loss: 0.6879 - acc: 0.5625
1920/4849 [==========>...................] - ETA: 3:22 - loss: 0.6885 - acc: 0.5609
1984/4849 [===========>..................] - ETA: 3:17 - loss: 0.6882 - acc: 0.5580
2048/4849 [===========>..................] - ETA: 3:13 - loss: 0.6881 - acc: 0.5581
2112/4849 [============>.................] - ETA: 3:09 - loss: 0.6879 - acc: 0.5573
2176/4849 [============>.................] - ETA: 3:05 - loss: 0.6873 - acc: 0.5602
2240/4849 [============>.................] - ETA: 3:00 - loss: 0.6882 - acc: 0.5580
2304/4849 [=============>................] - ETA: 2:56 - loss: 0.6882 - acc: 0.5569
2368/4849 [=============>................] - ETA: 2:52 - loss: 0.6878 - acc: 0.5549
2432/4849 [==============>...............] - ETA: 2:48 - loss: 0.6883 - acc: 0.5543
2496/4849 [==============>...............] - ETA: 2:43 - loss: 0.6893 - acc: 0.5541
2560/4849 [==============>...............] - ETA: 2:39 - loss: 0.6893 - acc: 0.5547
2624/4849 [===============>..............] - ETA: 2:34 - loss: 0.6885 - acc: 0.5579
2688/4849 [===============>..............] - ETA: 2:30 - loss: 0.6884 - acc: 0.5580
2752/4849 [================>.............] - ETA: 2:26 - loss: 0.6881 - acc: 0.5581
2816/4849 [================>.............] - ETA: 2:21 - loss: 0.6875 - acc: 0.5575
2880/4849 [================>.............] - ETA: 2:17 - loss: 0.6875 - acc: 0.5580
2944/4849 [=================>............] - ETA: 2:12 - loss: 0.6861 - acc: 0.5618
3008/4849 [=================>............] - ETA: 2:08 - loss: 0.6858 - acc: 0.5608
3072/4849 [==================>...........] - ETA: 2:03 - loss: 0.6855 - acc: 0.5609
3136/4849 [==================>...........] - ETA: 1:58 - loss: 0.6857 - acc: 0.5596
3200/4849 [==================>...........] - ETA: 1:54 - loss: 0.6856 - acc: 0.5594
3264/4849 [===================>..........] - ETA: 1:49 - loss: 0.6854 - acc: 0.5594
3328/4849 [===================>..........] - ETA: 1:45 - loss: 0.6853 - acc: 0.5598
3392/4849 [===================>..........] - ETA: 1:40 - loss: 0.6850 - acc: 0.5604
3456/4849 [====================>.........] - ETA: 1:36 - loss: 0.6841 - acc: 0.5622
3520/4849 [====================>.........] - ETA: 1:32 - loss: 0.6834 - acc: 0.5639
3584/4849 [=====================>........] - ETA: 1:27 - loss: 0.6842 - acc: 0.5617
3648/4849 [=====================>........] - ETA: 1:23 - loss: 0.6836 - acc: 0.5630
3712/4849 [=====================>........] - ETA: 1:18 - loss: 0.6835 - acc: 0.5633
3776/4849 [======================>.......] - ETA: 1:14 - loss: 0.6834 - acc: 0.5630
3840/4849 [======================>.......] - ETA: 1:09 - loss: 0.6835 - acc: 0.5641
3904/4849 [=======================>......] - ETA: 1:05 - loss: 0.6831 - acc: 0.5643
3968/4849 [=======================>......] - ETA: 1:01 - loss: 0.6830 - acc: 0.5655
4032/4849 [=======================>......] - ETA: 56s - loss: 0.6827 - acc: 0.5660 
4096/4849 [========================>.....] - ETA: 52s - loss: 0.6831 - acc: 0.5654
4160/4849 [========================>.....] - ETA: 47s - loss: 0.6830 - acc: 0.5661
4224/4849 [=========================>....] - ETA: 43s - loss: 0.6829 - acc: 0.5675
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6825 - acc: 0.5679
4352/4849 [=========================>....] - ETA: 34s - loss: 0.6825 - acc: 0.5682
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6830 - acc: 0.5677
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6829 - acc: 0.5679
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6825 - acc: 0.5680
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6820 - acc: 0.5684
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6817 - acc: 0.5683
4736/4849 [============================>.] - ETA: 7s - loss: 0.6819 - acc: 0.5676 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6817 - acc: 0.5683
4849/4849 [==============================] - 347s 71ms/step - loss: 0.6818 - acc: 0.5680 - val_loss: 0.6986 - val_acc: 0.5584

Epoch 00009: val_acc improved from 0.55288 to 0.55844, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window21/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 10/10

  64/4849 [..............................] - ETA: 5:00 - loss: 0.6897 - acc: 0.5469
 128/4849 [..............................] - ETA: 5:10 - loss: 0.6589 - acc: 0.6016
 192/4849 [>.............................] - ETA: 5:15 - loss: 0.6613 - acc: 0.5990
 256/4849 [>.............................] - ETA: 5:14 - loss: 0.6551 - acc: 0.6211
 320/4849 [>.............................] - ETA: 5:07 - loss: 0.6549 - acc: 0.6156
 384/4849 [=>............................] - ETA: 5:05 - loss: 0.6647 - acc: 0.6068
 448/4849 [=>............................] - ETA: 5:00 - loss: 0.6644 - acc: 0.6094
 512/4849 [==>...........................] - ETA: 4:57 - loss: 0.6694 - acc: 0.6016
 576/4849 [==>...........................] - ETA: 4:50 - loss: 0.6708 - acc: 0.5990
 640/4849 [==>...........................] - ETA: 4:45 - loss: 0.6750 - acc: 0.5906
 704/4849 [===>..........................] - ETA: 4:40 - loss: 0.6745 - acc: 0.5966
 768/4849 [===>..........................] - ETA: 4:36 - loss: 0.6741 - acc: 0.5964
 832/4849 [====>.........................] - ETA: 4:33 - loss: 0.6787 - acc: 0.5865
 896/4849 [====>.........................] - ETA: 4:29 - loss: 0.6818 - acc: 0.5759
 960/4849 [====>.........................] - ETA: 4:25 - loss: 0.6802 - acc: 0.5771
1024/4849 [=====>........................] - ETA: 4:21 - loss: 0.6825 - acc: 0.5752
1088/4849 [=====>........................] - ETA: 4:16 - loss: 0.6837 - acc: 0.5699
1152/4849 [======>.......................] - ETA: 4:12 - loss: 0.6830 - acc: 0.5686
1216/4849 [======>.......................] - ETA: 4:08 - loss: 0.6827 - acc: 0.5691
1280/4849 [======>.......................] - ETA: 4:03 - loss: 0.6864 - acc: 0.5602
1344/4849 [=======>......................] - ETA: 3:59 - loss: 0.6861 - acc: 0.5580
1408/4849 [=======>......................] - ETA: 3:56 - loss: 0.6861 - acc: 0.5575
1472/4849 [========>.....................] - ETA: 3:52 - loss: 0.6857 - acc: 0.5605
1536/4849 [========>.....................] - ETA: 3:47 - loss: 0.6869 - acc: 0.5560
1600/4849 [========>.....................] - ETA: 3:43 - loss: 0.6868 - acc: 0.5556
1664/4849 [=========>....................] - ETA: 3:38 - loss: 0.6860 - acc: 0.5553
1728/4849 [=========>....................] - ETA: 3:34 - loss: 0.6865 - acc: 0.5538
1792/4849 [==========>...................] - ETA: 3:30 - loss: 0.6868 - acc: 0.5547
1856/4849 [==========>...................] - ETA: 3:26 - loss: 0.6852 - acc: 0.5571
1920/4849 [==========>...................] - ETA: 3:21 - loss: 0.6855 - acc: 0.5547
1984/4849 [===========>..................] - ETA: 3:17 - loss: 0.6858 - acc: 0.5534
2048/4849 [===========>..................] - ETA: 3:12 - loss: 0.6851 - acc: 0.5537
2112/4849 [============>.................] - ETA: 3:08 - loss: 0.6858 - acc: 0.5507
2176/4849 [============>.................] - ETA: 3:03 - loss: 0.6861 - acc: 0.5478
2240/4849 [============>.................] - ETA: 2:59 - loss: 0.6856 - acc: 0.5487
2304/4849 [=============>................] - ETA: 2:55 - loss: 0.6851 - acc: 0.5477
2368/4849 [=============>................] - ETA: 2:50 - loss: 0.6850 - acc: 0.5490
2432/4849 [==============>...............] - ETA: 2:46 - loss: 0.6846 - acc: 0.5489
2496/4849 [==============>...............] - ETA: 2:41 - loss: 0.6841 - acc: 0.5497
2560/4849 [==============>...............] - ETA: 2:37 - loss: 0.6835 - acc: 0.5516
2624/4849 [===============>..............] - ETA: 2:33 - loss: 0.6839 - acc: 0.5507
2688/4849 [===============>..............] - ETA: 2:29 - loss: 0.6844 - acc: 0.5510
2752/4849 [================>.............] - ETA: 2:24 - loss: 0.6841 - acc: 0.5520
2816/4849 [================>.............] - ETA: 2:20 - loss: 0.6835 - acc: 0.5540
2880/4849 [================>.............] - ETA: 2:15 - loss: 0.6835 - acc: 0.5542
2944/4849 [=================>............] - ETA: 2:11 - loss: 0.6836 - acc: 0.5523
3008/4849 [=================>............] - ETA: 2:07 - loss: 0.6830 - acc: 0.5532
3072/4849 [==================>...........] - ETA: 2:02 - loss: 0.6827 - acc: 0.5544
3136/4849 [==================>...........] - ETA: 1:58 - loss: 0.6829 - acc: 0.5548
3200/4849 [==================>...........] - ETA: 1:54 - loss: 0.6824 - acc: 0.5550
3264/4849 [===================>..........] - ETA: 1:49 - loss: 0.6810 - acc: 0.5582
3328/4849 [===================>..........] - ETA: 1:45 - loss: 0.6811 - acc: 0.5589
3392/4849 [===================>..........] - ETA: 1:40 - loss: 0.6815 - acc: 0.5584
3456/4849 [====================>.........] - ETA: 1:36 - loss: 0.6807 - acc: 0.5605
3520/4849 [====================>.........] - ETA: 1:32 - loss: 0.6819 - acc: 0.5571
3584/4849 [=====================>........] - ETA: 1:27 - loss: 0.6820 - acc: 0.5566
3648/4849 [=====================>........] - ETA: 1:23 - loss: 0.6815 - acc: 0.5570
3712/4849 [=====================>........] - ETA: 1:18 - loss: 0.6811 - acc: 0.5585
3776/4849 [======================>.......] - ETA: 1:14 - loss: 0.6810 - acc: 0.5591
3840/4849 [======================>.......] - ETA: 1:09 - loss: 0.6807 - acc: 0.5599
3904/4849 [=======================>......] - ETA: 1:05 - loss: 0.6806 - acc: 0.5597
3968/4849 [=======================>......] - ETA: 1:01 - loss: 0.6800 - acc: 0.5615
4032/4849 [=======================>......] - ETA: 56s - loss: 0.6797 - acc: 0.5625 
4096/4849 [========================>.....] - ETA: 52s - loss: 0.6794 - acc: 0.5635
4160/4849 [========================>.....] - ETA: 47s - loss: 0.6796 - acc: 0.5632
4224/4849 [=========================>....] - ETA: 43s - loss: 0.6795 - acc: 0.5632
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6792 - acc: 0.5630
4352/4849 [=========================>....] - ETA: 34s - loss: 0.6789 - acc: 0.5634
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6786 - acc: 0.5652
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6786 - acc: 0.5654
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6792 - acc: 0.5636
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6791 - acc: 0.5638
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6787 - acc: 0.5646
4736/4849 [============================>.] - ETA: 7s - loss: 0.6786 - acc: 0.5655 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6784 - acc: 0.5658
4849/4849 [==============================] - 349s 72ms/step - loss: 0.6786 - acc: 0.5655 - val_loss: 0.6942 - val_acc: 0.5492

Epoch 00010: val_acc did not improve from 0.55844
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f90202d5790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f90202d5790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f90047cbe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f90047cbe90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90045cf090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90045cf090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90045cf690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90045cf690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e985bba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8e985bba90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90045cfe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90045cfe10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90045cf7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90045cf7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f900438afd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f900438afd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fe0338550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fe0338550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90043718d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90043718d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90045030d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90045030d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90044cb590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90044cb590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9004503e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9004503e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9004143510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9004143510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90040f5710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90040f5710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f900448d350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f900448d350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fa4531d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fa4531d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e987454d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8e987454d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fe0604490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fe0604490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fe0549450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fe0549450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9178640dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9178640dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fe07d5350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fe07d5350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fe0602b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fe0602b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fe0346210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fe0346210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fe01d7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fe01d7290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fe03415d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fe03415d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fe03eb990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fe03eb990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fe02e9150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fe02e9150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fc0750cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fc0750cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fc06c51d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fc06c51d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fe01d7390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fe01d7390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fc07501d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fc07501d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fc06bbd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fc06bbd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fc03fe050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fc03fe050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fc03e0390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fc03e0390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fc03ed790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fc03ed790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fc03fee50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fc03fee50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fc063e5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fc063e5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fc020d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fc020d8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fc0120c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fc0120c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa47bfb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa47bfb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fc00fad50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fc00fad50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fc030ec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fc030ec90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fa4669b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fa4669b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fa4570a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fa4570a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa4619d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa4619d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fa4669050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fa4669050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa4499a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa4499a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fa4268c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fa4268c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fa41b2190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fa41b2190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa406f190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa406f190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fa45f3890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8fa45f3890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa4078a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa4078a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fa44a7f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8fa44a7f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fa424b4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8fa424b4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f9063b3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f9063b3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8f906b5190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8f906b5190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa427d090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8fa427d090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8f906a27d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8f906a27d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f903c6f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f903c6f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f90615390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f90615390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8f906a2210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8f906a2210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f902c63d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8f902c63d0>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 3:29
 128/1348 [=>............................] - ETA: 1:54
 192/1348 [===>..........................] - ETA: 1:21
 256/1348 [====>.........................] - ETA: 1:03
 320/1348 [======>.......................] - ETA: 53s 
 384/1348 [=======>......................] - ETA: 45s
 448/1348 [========>.....................] - ETA: 39s
 512/1348 [==========>...................] - ETA: 35s
 576/1348 [===========>..................] - ETA: 30s
 640/1348 [=============>................] - ETA: 27s
 704/1348 [==============>...............] - ETA: 24s
 768/1348 [================>.............] - ETA: 20s
 832/1348 [=================>............] - ETA: 18s
 896/1348 [==================>...........] - ETA: 15s
 960/1348 [====================>.........] - ETA: 13s
1024/1348 [=====================>........] - ETA: 10s
1088/1348 [=======================>......] - ETA: 8s 
1152/1348 [========================>.....] - ETA: 6s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 43s 32ms/step
loss: 0.6719595763379106
acc: 0.6001483679525222
