nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2694
样本个数 5388
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd8849cdd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd8849cdd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd8eb078050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd8eb078050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eaed77d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eaed77d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8eae8db50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8eae8db50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8eae17710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8eae17710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd88496a3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd88496a3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8848df550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8848df550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87c898110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87c898110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd87c70f850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd87c70f850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd87c66d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd87c66d390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd88490ddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd88490ddd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8848df390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8848df390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87c66de50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87c66de50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd87c6ece10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd87c6ece10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd87c2a1510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd87c2a1510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87c39e150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87c39e150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd87c691cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd87c691cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87c2a81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87c2a81d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd86c0c7610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd86c0c7610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd86bf6f950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd86bf6f950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd86c09ebd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd86c09ebd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd87c0e5350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd87c0e5350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd88496d590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd88496d590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd88495a3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd88495a3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd86bc17d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd86bc17d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd86bb8ca10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd86bb8ca10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd86be66410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd86be66410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd86bb47c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd86bb47c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd87c0f72d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd87c0f72d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd86b941d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd86b941d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd86b9783d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd86b9783d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd86bd3c610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd86bd3c610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd863820c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd863820c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd86b929d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd86b929d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd86369bf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd86369bf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd86b963fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd86b963fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd86ba1ea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd86ba1ea10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8636894d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8636894d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd86367f050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd86367f050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8632e7950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8632e7950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8631e1550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8631e1550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd863446990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd863446990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8633242d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8633242d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8630ef390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8630ef390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd85af9ce10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd85af9ce10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8631fecd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8631fecd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8630ef9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8630ef9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85afd1510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85afd1510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd85ae24050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd85ae24050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd85b03bb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd85b03bb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85aeed350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85aeed350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd85ae24890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd85ae24890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85b0615d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85b0615d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8636456d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8636456d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd85a9caed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd85a9caed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85aab3590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85aab3590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd85ac0e590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd85ac0e590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85ab24bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85ab24bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8527a4e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8527a4e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd85266bd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd85266bd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8528beb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8528beb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8528a5790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8528a5790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85265f390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85265f390>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-26 19:02:46.713738: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-26 19:02:46.800731: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-26 19:02:46.940760: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560a40d6c7b0 executing computations on platform Host. Devices:
2022-11-26 19:02:46.940835: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-26 19:02:48.290744: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 25:06 - loss: 0.7126 - acc: 0.5156
 128/4849 [..............................] - ETA: 19:40 - loss: 0.7149 - acc: 0.4844
 192/4849 [>.............................] - ETA: 15:56 - loss: 0.7094 - acc: 0.5052
 256/4849 [>.............................] - ETA: 13:54 - loss: 0.7050 - acc: 0.5195
 320/4849 [>.............................] - ETA: 12:51 - loss: 0.7049 - acc: 0.5250
 384/4849 [=>............................] - ETA: 11:56 - loss: 0.7047 - acc: 0.5339
 448/4849 [=>............................] - ETA: 11:03 - loss: 0.7098 - acc: 0.5312
 512/4849 [==>...........................] - ETA: 10:35 - loss: 0.7267 - acc: 0.5234
 576/4849 [==>...........................] - ETA: 10:03 - loss: 0.7317 - acc: 0.5226
 640/4849 [==>...........................] - ETA: 9:39 - loss: 0.7333 - acc: 0.5234 
 704/4849 [===>..........................] - ETA: 9:14 - loss: 0.7359 - acc: 0.5256
 768/4849 [===>..........................] - ETA: 8:52 - loss: 0.7373 - acc: 0.5169
 832/4849 [====>.........................] - ETA: 8:32 - loss: 0.7359 - acc: 0.5168
 896/4849 [====>.........................] - ETA: 8:15 - loss: 0.7317 - acc: 0.5279
 960/4849 [====>.........................] - ETA: 8:00 - loss: 0.7342 - acc: 0.5219
1024/4849 [=====>........................] - ETA: 7:46 - loss: 0.7309 - acc: 0.5244
1088/4849 [=====>........................] - ETA: 7:33 - loss: 0.7294 - acc: 0.5239
1152/4849 [======>.......................] - ETA: 7:20 - loss: 0.7305 - acc: 0.5208
1216/4849 [======>.......................] - ETA: 7:19 - loss: 0.7310 - acc: 0.5189
1280/4849 [======>.......................] - ETA: 7:09 - loss: 0.7313 - acc: 0.5164
1344/4849 [=======>......................] - ETA: 7:00 - loss: 0.7307 - acc: 0.5171
1408/4849 [=======>......................] - ETA: 6:49 - loss: 0.7308 - acc: 0.5135
1472/4849 [========>.....................] - ETA: 6:39 - loss: 0.7275 - acc: 0.5183
1536/4849 [========>.....................] - ETA: 6:30 - loss: 0.7249 - acc: 0.5234
1600/4849 [========>.....................] - ETA: 6:19 - loss: 0.7234 - acc: 0.5244
1664/4849 [=========>....................] - ETA: 6:10 - loss: 0.7228 - acc: 0.5252
1728/4849 [=========>....................] - ETA: 6:01 - loss: 0.7225 - acc: 0.5255
1792/4849 [==========>...................] - ETA: 5:53 - loss: 0.7215 - acc: 0.5273
1856/4849 [==========>...................] - ETA: 5:44 - loss: 0.7201 - acc: 0.5296
1920/4849 [==========>...................] - ETA: 5:35 - loss: 0.7196 - acc: 0.5292
1984/4849 [===========>..................] - ETA: 5:26 - loss: 0.7214 - acc: 0.5282
2048/4849 [===========>..................] - ETA: 5:17 - loss: 0.7208 - acc: 0.5283
2112/4849 [============>.................] - ETA: 5:08 - loss: 0.7206 - acc: 0.5303
2176/4849 [============>.................] - ETA: 5:00 - loss: 0.7185 - acc: 0.5331
2240/4849 [============>.................] - ETA: 4:52 - loss: 0.7170 - acc: 0.5330
2304/4849 [=============>................] - ETA: 4:43 - loss: 0.7148 - acc: 0.5360
2368/4849 [=============>................] - ETA: 4:35 - loss: 0.7144 - acc: 0.5363
2432/4849 [==============>...............] - ETA: 4:28 - loss: 0.7142 - acc: 0.5358
2496/4849 [==============>...............] - ETA: 4:20 - loss: 0.7155 - acc: 0.5333
2560/4849 [==============>...............] - ETA: 4:12 - loss: 0.7159 - acc: 0.5324
2624/4849 [===============>..............] - ETA: 4:04 - loss: 0.7151 - acc: 0.5328
2688/4849 [===============>..............] - ETA: 3:56 - loss: 0.7142 - acc: 0.5342
2752/4849 [================>.............] - ETA: 3:49 - loss: 0.7135 - acc: 0.5349
2816/4849 [================>.............] - ETA: 3:41 - loss: 0.7128 - acc: 0.5359
2880/4849 [================>.............] - ETA: 3:34 - loss: 0.7121 - acc: 0.5351
2944/4849 [=================>............] - ETA: 3:27 - loss: 0.7122 - acc: 0.5343
3008/4849 [=================>............] - ETA: 3:20 - loss: 0.7114 - acc: 0.5352
3072/4849 [==================>...........] - ETA: 3:12 - loss: 0.7123 - acc: 0.5342
3136/4849 [==================>...........] - ETA: 3:05 - loss: 0.7118 - acc: 0.5348
3200/4849 [==================>...........] - ETA: 2:58 - loss: 0.7113 - acc: 0.5353
3264/4849 [===================>..........] - ETA: 2:51 - loss: 0.7102 - acc: 0.5365
3328/4849 [===================>..........] - ETA: 2:43 - loss: 0.7094 - acc: 0.5376
3392/4849 [===================>..........] - ETA: 2:36 - loss: 0.7088 - acc: 0.5380
3456/4849 [====================>.........] - ETA: 2:29 - loss: 0.7085 - acc: 0.5394
3520/4849 [====================>.........] - ETA: 2:22 - loss: 0.7077 - acc: 0.5418
3584/4849 [=====================>........] - ETA: 2:15 - loss: 0.7072 - acc: 0.5427
3648/4849 [=====================>........] - ETA: 2:08 - loss: 0.7072 - acc: 0.5411
3712/4849 [=====================>........] - ETA: 2:01 - loss: 0.7062 - acc: 0.5428
3776/4849 [======================>.......] - ETA: 1:54 - loss: 0.7056 - acc: 0.5432
3840/4849 [======================>.......] - ETA: 1:47 - loss: 0.7050 - acc: 0.5448
3904/4849 [=======================>......] - ETA: 1:40 - loss: 0.7047 - acc: 0.5451
3968/4849 [=======================>......] - ETA: 1:33 - loss: 0.7044 - acc: 0.5464
4032/4849 [=======================>......] - ETA: 1:26 - loss: 0.7045 - acc: 0.5464
4096/4849 [========================>.....] - ETA: 1:19 - loss: 0.7040 - acc: 0.5474
4160/4849 [========================>.....] - ETA: 1:12 - loss: 0.7028 - acc: 0.5483
4224/4849 [=========================>....] - ETA: 1:06 - loss: 0.7026 - acc: 0.5490
4288/4849 [=========================>....] - ETA: 59s - loss: 0.7024 - acc: 0.5494 
4352/4849 [=========================>....] - ETA: 52s - loss: 0.7018 - acc: 0.5499
4416/4849 [==========================>...] - ETA: 45s - loss: 0.7011 - acc: 0.5512
4480/4849 [==========================>...] - ETA: 38s - loss: 0.7014 - acc: 0.5507
4544/4849 [===========================>..] - ETA: 32s - loss: 0.7009 - acc: 0.5508
4608/4849 [===========================>..] - ETA: 25s - loss: 0.6999 - acc: 0.5523
4672/4849 [===========================>..] - ETA: 18s - loss: 0.6993 - acc: 0.5524
4736/4849 [============================>.] - ETA: 11s - loss: 0.6994 - acc: 0.5532
4800/4849 [============================>.] - ETA: 5s - loss: 0.6985 - acc: 0.5537 
4849/4849 [==============================] - 528s 109ms/step - loss: 0.6982 - acc: 0.5543 - val_loss: 0.6737 - val_acc: 0.5807

Epoch 00001: val_acc improved from -inf to 0.58071, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window11/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 7:41 - loss: 0.7357 - acc: 0.5312
 128/4849 [..............................] - ETA: 7:36 - loss: 0.7124 - acc: 0.5547
 192/4849 [>.............................] - ETA: 7:45 - loss: 0.7002 - acc: 0.5625
 256/4849 [>.............................] - ETA: 7:38 - loss: 0.7011 - acc: 0.5469
 320/4849 [>.............................] - ETA: 7:30 - loss: 0.6936 - acc: 0.5563
 384/4849 [=>............................] - ETA: 7:24 - loss: 0.6848 - acc: 0.5781
 448/4849 [=>............................] - ETA: 7:20 - loss: 0.6746 - acc: 0.6049
 512/4849 [==>...........................] - ETA: 7:13 - loss: 0.6695 - acc: 0.6055
 576/4849 [==>...........................] - ETA: 7:05 - loss: 0.6727 - acc: 0.5903
 640/4849 [==>...........................] - ETA: 6:59 - loss: 0.6789 - acc: 0.5750
 704/4849 [===>..........................] - ETA: 6:52 - loss: 0.6792 - acc: 0.5767
 768/4849 [===>..........................] - ETA: 6:43 - loss: 0.6760 - acc: 0.5833
 832/4849 [====>.........................] - ETA: 6:33 - loss: 0.6753 - acc: 0.5865
 896/4849 [====>.........................] - ETA: 6:20 - loss: 0.6775 - acc: 0.5804
 960/4849 [====>.........................] - ETA: 6:11 - loss: 0.6774 - acc: 0.5771
1024/4849 [=====>........................] - ETA: 6:03 - loss: 0.6750 - acc: 0.5791
1088/4849 [=====>........................] - ETA: 5:54 - loss: 0.6715 - acc: 0.5809
1152/4849 [======>.......................] - ETA: 5:48 - loss: 0.6734 - acc: 0.5799
1216/4849 [======>.......................] - ETA: 5:42 - loss: 0.6720 - acc: 0.5847
1280/4849 [======>.......................] - ETA: 5:33 - loss: 0.6713 - acc: 0.5852
1344/4849 [=======>......................] - ETA: 5:26 - loss: 0.6694 - acc: 0.5863
1408/4849 [=======>......................] - ETA: 5:18 - loss: 0.6707 - acc: 0.5874
1472/4849 [========>.....................] - ETA: 5:12 - loss: 0.6695 - acc: 0.5890
1536/4849 [========>.....................] - ETA: 5:07 - loss: 0.6721 - acc: 0.5853
1600/4849 [========>.....................] - ETA: 5:02 - loss: 0.6684 - acc: 0.5906
1664/4849 [=========>....................] - ETA: 4:56 - loss: 0.6654 - acc: 0.5950
1728/4849 [=========>....................] - ETA: 4:50 - loss: 0.6664 - acc: 0.5943
1792/4849 [==========>...................] - ETA: 4:45 - loss: 0.6655 - acc: 0.5960
1856/4849 [==========>...................] - ETA: 4:39 - loss: 0.6662 - acc: 0.5954
1920/4849 [==========>...................] - ETA: 4:32 - loss: 0.6662 - acc: 0.5969
1984/4849 [===========>..................] - ETA: 4:26 - loss: 0.6636 - acc: 0.5988
2048/4849 [===========>..................] - ETA: 4:19 - loss: 0.6630 - acc: 0.6016
2112/4849 [============>.................] - ETA: 4:13 - loss: 0.6641 - acc: 0.6009
2176/4849 [============>.................] - ETA: 4:06 - loss: 0.6628 - acc: 0.6025
2240/4849 [============>.................] - ETA: 4:00 - loss: 0.6626 - acc: 0.6022
2304/4849 [=============>................] - ETA: 3:53 - loss: 0.6634 - acc: 0.6020
2368/4849 [=============>................] - ETA: 3:47 - loss: 0.6645 - acc: 0.6022
2432/4849 [==============>...............] - ETA: 3:41 - loss: 0.6635 - acc: 0.6032
2496/4849 [==============>...............] - ETA: 3:35 - loss: 0.6630 - acc: 0.6054
2560/4849 [==============>...............] - ETA: 3:29 - loss: 0.6631 - acc: 0.6059
2624/4849 [===============>..............] - ETA: 3:23 - loss: 0.6630 - acc: 0.6044
2688/4849 [===============>..............] - ETA: 3:17 - loss: 0.6626 - acc: 0.6042
2752/4849 [================>.............] - ETA: 3:11 - loss: 0.6636 - acc: 0.6047
2816/4849 [================>.............] - ETA: 3:05 - loss: 0.6648 - acc: 0.6026
2880/4849 [================>.............] - ETA: 2:59 - loss: 0.6644 - acc: 0.6045
2944/4849 [=================>............] - ETA: 2:53 - loss: 0.6642 - acc: 0.6046
3008/4849 [=================>............] - ETA: 2:47 - loss: 0.6649 - acc: 0.6034
3072/4849 [==================>...........] - ETA: 2:41 - loss: 0.6645 - acc: 0.6048
3136/4849 [==================>...........] - ETA: 2:36 - loss: 0.6646 - acc: 0.6046
3200/4849 [==================>...........] - ETA: 2:30 - loss: 0.6641 - acc: 0.6059
3264/4849 [===================>..........] - ETA: 2:24 - loss: 0.6659 - acc: 0.6032
3328/4849 [===================>..........] - ETA: 2:18 - loss: 0.6657 - acc: 0.6028
3392/4849 [===================>..........] - ETA: 2:12 - loss: 0.6666 - acc: 0.6023
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.6677 - acc: 0.6016
3520/4849 [====================>.........] - ETA: 2:00 - loss: 0.6670 - acc: 0.6026
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6671 - acc: 0.6013
3648/4849 [=====================>........] - ETA: 1:49 - loss: 0.6668 - acc: 0.6022
3712/4849 [=====================>........] - ETA: 1:43 - loss: 0.6662 - acc: 0.6021
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6664 - acc: 0.6017
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6664 - acc: 0.6016
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6667 - acc: 0.6009
3968/4849 [=======================>......] - ETA: 1:20 - loss: 0.6666 - acc: 0.6011
4032/4849 [=======================>......] - ETA: 1:14 - loss: 0.6679 - acc: 0.5992
4096/4849 [========================>.....] - ETA: 1:08 - loss: 0.6681 - acc: 0.5984
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6679 - acc: 0.5993
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6670 - acc: 0.6011 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6673 - acc: 0.6012
4352/4849 [=========================>....] - ETA: 45s - loss: 0.6666 - acc: 0.6027
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6655 - acc: 0.6035
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6656 - acc: 0.6029
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6657 - acc: 0.6028
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6653 - acc: 0.6029
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6647 - acc: 0.6038
4736/4849 [============================>.] - ETA: 10s - loss: 0.6652 - acc: 0.6039
4800/4849 [============================>.] - ETA: 4s - loss: 0.6648 - acc: 0.6048 
4849/4849 [==============================] - 456s 94ms/step - loss: 0.6658 - acc: 0.6038 - val_loss: 0.6986 - val_acc: 0.5603

Epoch 00002: val_acc did not improve from 0.58071
Epoch 3/10

  64/4849 [..............................] - ETA: 6:31 - loss: 0.5566 - acc: 0.7500
 128/4849 [..............................] - ETA: 6:51 - loss: 0.6031 - acc: 0.6797
 192/4849 [>.............................] - ETA: 6:42 - loss: 0.6094 - acc: 0.6771
 256/4849 [>.............................] - ETA: 6:33 - loss: 0.6202 - acc: 0.6758
 320/4849 [>.............................] - ETA: 6:32 - loss: 0.6300 - acc: 0.6562
 384/4849 [=>............................] - ETA: 6:26 - loss: 0.6314 - acc: 0.6562
 448/4849 [=>............................] - ETA: 6:23 - loss: 0.6415 - acc: 0.6406
 512/4849 [==>...........................] - ETA: 6:22 - loss: 0.6430 - acc: 0.6406
 576/4849 [==>...........................] - ETA: 6:16 - loss: 0.6500 - acc: 0.6337
 640/4849 [==>...........................] - ETA: 6:09 - loss: 0.6447 - acc: 0.6438
 704/4849 [===>..........................] - ETA: 6:04 - loss: 0.6519 - acc: 0.6378
 768/4849 [===>..........................] - ETA: 6:02 - loss: 0.6517 - acc: 0.6341
 832/4849 [====>.........................] - ETA: 5:53 - loss: 0.6483 - acc: 0.6406
 896/4849 [====>.........................] - ETA: 5:47 - loss: 0.6514 - acc: 0.6350
 960/4849 [====>.........................] - ETA: 5:42 - loss: 0.6550 - acc: 0.6312
1024/4849 [=====>........................] - ETA: 5:37 - loss: 0.6518 - acc: 0.6338
1088/4849 [=====>........................] - ETA: 5:32 - loss: 0.6534 - acc: 0.6333
1152/4849 [======>.......................] - ETA: 5:27 - loss: 0.6522 - acc: 0.6337
1216/4849 [======>.......................] - ETA: 5:21 - loss: 0.6524 - acc: 0.6340
1280/4849 [======>.......................] - ETA: 5:16 - loss: 0.6535 - acc: 0.6359
1344/4849 [=======>......................] - ETA: 5:10 - loss: 0.6541 - acc: 0.6369
1408/4849 [=======>......................] - ETA: 5:05 - loss: 0.6513 - acc: 0.6385
1472/4849 [========>.....................] - ETA: 4:58 - loss: 0.6492 - acc: 0.6393
1536/4849 [========>.....................] - ETA: 4:53 - loss: 0.6496 - acc: 0.6393
1600/4849 [========>.....................] - ETA: 4:47 - loss: 0.6481 - acc: 0.6412
1664/4849 [=========>....................] - ETA: 4:40 - loss: 0.6485 - acc: 0.6424
1728/4849 [=========>....................] - ETA: 4:35 - loss: 0.6504 - acc: 0.6389
1792/4849 [==========>...................] - ETA: 4:30 - loss: 0.6514 - acc: 0.6356
1856/4849 [==========>...................] - ETA: 4:24 - loss: 0.6530 - acc: 0.6336
1920/4849 [==========>...................] - ETA: 4:18 - loss: 0.6545 - acc: 0.6312
1984/4849 [===========>..................] - ETA: 4:13 - loss: 0.6554 - acc: 0.6300
2048/4849 [===========>..................] - ETA: 4:08 - loss: 0.6559 - acc: 0.6289
2112/4849 [============>.................] - ETA: 4:02 - loss: 0.6553 - acc: 0.6283
2176/4849 [============>.................] - ETA: 3:56 - loss: 0.6551 - acc: 0.6282
2240/4849 [============>.................] - ETA: 3:50 - loss: 0.6570 - acc: 0.6259
2304/4849 [=============>................] - ETA: 3:44 - loss: 0.6569 - acc: 0.6250
2368/4849 [=============>................] - ETA: 3:38 - loss: 0.6569 - acc: 0.6250
2432/4849 [==============>...............] - ETA: 3:32 - loss: 0.6571 - acc: 0.6238
2496/4849 [==============>...............] - ETA: 3:27 - loss: 0.6564 - acc: 0.6254
2560/4849 [==============>...............] - ETA: 3:21 - loss: 0.6577 - acc: 0.6227
2624/4849 [===============>..............] - ETA: 3:15 - loss: 0.6579 - acc: 0.6231
2688/4849 [===============>..............] - ETA: 3:09 - loss: 0.6578 - acc: 0.6235
2752/4849 [================>.............] - ETA: 3:03 - loss: 0.6577 - acc: 0.6232
2816/4849 [================>.............] - ETA: 2:58 - loss: 0.6562 - acc: 0.6246
2880/4849 [================>.............] - ETA: 2:52 - loss: 0.6561 - acc: 0.6250
2944/4849 [=================>............] - ETA: 2:46 - loss: 0.6553 - acc: 0.6260
3008/4849 [=================>............] - ETA: 2:40 - loss: 0.6552 - acc: 0.6270
3072/4849 [==================>...........] - ETA: 2:35 - loss: 0.6557 - acc: 0.6257
3136/4849 [==================>...........] - ETA: 2:29 - loss: 0.6565 - acc: 0.6240
3200/4849 [==================>...........] - ETA: 2:24 - loss: 0.6563 - acc: 0.6250
3264/4849 [===================>..........] - ETA: 2:18 - loss: 0.6565 - acc: 0.6259
3328/4849 [===================>..........] - ETA: 2:13 - loss: 0.6560 - acc: 0.6262
3392/4849 [===================>..........] - ETA: 2:07 - loss: 0.6560 - acc: 0.6274
3456/4849 [====================>.........] - ETA: 2:01 - loss: 0.6560 - acc: 0.6276
3520/4849 [====================>.........] - ETA: 1:56 - loss: 0.6546 - acc: 0.6287
3584/4849 [=====================>........] - ETA: 1:50 - loss: 0.6544 - acc: 0.6295
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6539 - acc: 0.6291
3712/4849 [=====================>........] - ETA: 1:39 - loss: 0.6543 - acc: 0.6274
3776/4849 [======================>.......] - ETA: 1:33 - loss: 0.6533 - acc: 0.6282
3840/4849 [======================>.......] - ETA: 1:28 - loss: 0.6530 - acc: 0.6286
3904/4849 [=======================>......] - ETA: 1:22 - loss: 0.6523 - acc: 0.6294
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6527 - acc: 0.6278
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6525 - acc: 0.6285
4096/4849 [========================>.....] - ETA: 1:05 - loss: 0.6535 - acc: 0.6272
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6528 - acc: 0.6281
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6523 - acc: 0.6283 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6527 - acc: 0.6278
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6525 - acc: 0.6280
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6522 - acc: 0.6291
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6526 - acc: 0.6288
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6526 - acc: 0.6283
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6523 - acc: 0.6280
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6526 - acc: 0.6276
4736/4849 [============================>.] - ETA: 9s - loss: 0.6536 - acc: 0.6265 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6531 - acc: 0.6273
4849/4849 [==============================] - 444s 92ms/step - loss: 0.6529 - acc: 0.6271 - val_loss: 0.6728 - val_acc: 0.5918

Epoch 00003: val_acc improved from 0.58071 to 0.59184, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window11/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 4/10

  64/4849 [..............................] - ETA: 7:05 - loss: 0.6557 - acc: 0.5781
 128/4849 [..............................] - ETA: 7:07 - loss: 0.6732 - acc: 0.5469
 192/4849 [>.............................] - ETA: 6:57 - loss: 0.6373 - acc: 0.6198
 256/4849 [>.............................] - ETA: 6:54 - loss: 0.6568 - acc: 0.5938
 320/4849 [>.............................] - ETA: 6:42 - loss: 0.6526 - acc: 0.6062
 384/4849 [=>............................] - ETA: 6:38 - loss: 0.6492 - acc: 0.6042
 448/4849 [=>............................] - ETA: 6:27 - loss: 0.6502 - acc: 0.6027
 512/4849 [==>...........................] - ETA: 6:22 - loss: 0.6531 - acc: 0.5996
 576/4849 [==>...........................] - ETA: 6:22 - loss: 0.6550 - acc: 0.6024
 640/4849 [==>...........................] - ETA: 6:14 - loss: 0.6615 - acc: 0.6016
 704/4849 [===>..........................] - ETA: 6:07 - loss: 0.6593 - acc: 0.6009
 768/4849 [===>..........................] - ETA: 6:03 - loss: 0.6556 - acc: 0.6042
 832/4849 [====>.........................] - ETA: 5:54 - loss: 0.6567 - acc: 0.6034
 896/4849 [====>.........................] - ETA: 5:49 - loss: 0.6589 - acc: 0.6004
 960/4849 [====>.........................] - ETA: 5:45 - loss: 0.6593 - acc: 0.5969
1024/4849 [=====>........................] - ETA: 5:39 - loss: 0.6558 - acc: 0.6035
1088/4849 [=====>........................] - ETA: 5:31 - loss: 0.6538 - acc: 0.6057
1152/4849 [======>.......................] - ETA: 5:25 - loss: 0.6508 - acc: 0.6111
1216/4849 [======>.......................] - ETA: 5:19 - loss: 0.6487 - acc: 0.6135
1280/4849 [======>.......................] - ETA: 5:14 - loss: 0.6489 - acc: 0.6133
1344/4849 [=======>......................] - ETA: 5:10 - loss: 0.6506 - acc: 0.6094
1408/4849 [=======>......................] - ETA: 5:04 - loss: 0.6498 - acc: 0.6080
1472/4849 [========>.....................] - ETA: 4:58 - loss: 0.6467 - acc: 0.6135
1536/4849 [========>.....................] - ETA: 4:52 - loss: 0.6475 - acc: 0.6126
1600/4849 [========>.....................] - ETA: 4:47 - loss: 0.6474 - acc: 0.6156
1664/4849 [=========>....................] - ETA: 4:41 - loss: 0.6470 - acc: 0.6172
1728/4849 [=========>....................] - ETA: 4:35 - loss: 0.6450 - acc: 0.6181
1792/4849 [==========>...................] - ETA: 4:29 - loss: 0.6434 - acc: 0.6205
1856/4849 [==========>...................] - ETA: 4:23 - loss: 0.6436 - acc: 0.6207
1920/4849 [==========>...................] - ETA: 4:17 - loss: 0.6460 - acc: 0.6177
1984/4849 [===========>..................] - ETA: 4:12 - loss: 0.6468 - acc: 0.6169
2048/4849 [===========>..................] - ETA: 4:07 - loss: 0.6446 - acc: 0.6191
2112/4849 [============>.................] - ETA: 4:01 - loss: 0.6458 - acc: 0.6184
2176/4849 [============>.................] - ETA: 3:55 - loss: 0.6474 - acc: 0.6186
2240/4849 [============>.................] - ETA: 3:50 - loss: 0.6464 - acc: 0.6188
2304/4849 [=============>................] - ETA: 3:44 - loss: 0.6480 - acc: 0.6155
2368/4849 [=============>................] - ETA: 3:37 - loss: 0.6499 - acc: 0.6128
2432/4849 [==============>...............] - ETA: 3:31 - loss: 0.6491 - acc: 0.6139
2496/4849 [==============>...............] - ETA: 3:26 - loss: 0.6495 - acc: 0.6122
2560/4849 [==============>...............] - ETA: 3:21 - loss: 0.6500 - acc: 0.6113
2624/4849 [===============>..............] - ETA: 3:15 - loss: 0.6490 - acc: 0.6132
2688/4849 [===============>..............] - ETA: 3:09 - loss: 0.6498 - acc: 0.6120
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.6504 - acc: 0.6123
2816/4849 [================>.............] - ETA: 2:59 - loss: 0.6510 - acc: 0.6112
2880/4849 [================>.............] - ETA: 2:53 - loss: 0.6511 - acc: 0.6104
2944/4849 [=================>............] - ETA: 2:47 - loss: 0.6525 - acc: 0.6067
3008/4849 [=================>............] - ETA: 2:42 - loss: 0.6518 - acc: 0.6080
3072/4849 [==================>...........] - ETA: 2:36 - loss: 0.6523 - acc: 0.6068
3136/4849 [==================>...........] - ETA: 2:30 - loss: 0.6519 - acc: 0.6087
3200/4849 [==================>...........] - ETA: 2:25 - loss: 0.6516 - acc: 0.6097
3264/4849 [===================>..........] - ETA: 2:19 - loss: 0.6509 - acc: 0.6106
3328/4849 [===================>..........] - ETA: 2:13 - loss: 0.6499 - acc: 0.6127
3392/4849 [===================>..........] - ETA: 2:08 - loss: 0.6499 - acc: 0.6126
3456/4849 [====================>.........] - ETA: 2:02 - loss: 0.6499 - acc: 0.6123
3520/4849 [====================>.........] - ETA: 1:56 - loss: 0.6499 - acc: 0.6145
3584/4849 [=====================>........] - ETA: 1:50 - loss: 0.6493 - acc: 0.6150
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6509 - acc: 0.6143
3712/4849 [=====================>........] - ETA: 1:39 - loss: 0.6504 - acc: 0.6164
3776/4849 [======================>.......] - ETA: 1:33 - loss: 0.6501 - acc: 0.6178
3840/4849 [======================>.......] - ETA: 1:28 - loss: 0.6502 - acc: 0.6180
3904/4849 [=======================>......] - ETA: 1:22 - loss: 0.6497 - acc: 0.6196
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6487 - acc: 0.6212
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6476 - acc: 0.6225
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6476 - acc: 0.6228
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6484 - acc: 0.6219
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6476 - acc: 0.6236 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6473 - acc: 0.6229
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6468 - acc: 0.6239
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6462 - acc: 0.6245
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6456 - acc: 0.6252
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6469 - acc: 0.6239
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6462 - acc: 0.6241
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6471 - acc: 0.6235
4736/4849 [============================>.] - ETA: 9s - loss: 0.6470 - acc: 0.6237 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6475 - acc: 0.6231
4849/4849 [==============================] - 445s 92ms/step - loss: 0.6472 - acc: 0.6232 - val_loss: 0.6671 - val_acc: 0.5974

Epoch 00004: val_acc improved from 0.59184 to 0.59740, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window11/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 5/10

  64/4849 [..............................] - ETA: 7:15 - loss: 0.6263 - acc: 0.6719
 128/4849 [..............................] - ETA: 6:53 - loss: 0.6613 - acc: 0.6172
 192/4849 [>.............................] - ETA: 6:51 - loss: 0.6409 - acc: 0.6302
 256/4849 [>.............................] - ETA: 6:44 - loss: 0.6355 - acc: 0.6250
 320/4849 [>.............................] - ETA: 6:43 - loss: 0.6411 - acc: 0.6156
 384/4849 [=>............................] - ETA: 6:36 - loss: 0.6449 - acc: 0.6146
 448/4849 [=>............................] - ETA: 6:33 - loss: 0.6446 - acc: 0.6138
 512/4849 [==>...........................] - ETA: 6:25 - loss: 0.6479 - acc: 0.6094
 576/4849 [==>...........................] - ETA: 6:20 - loss: 0.6497 - acc: 0.6042
 640/4849 [==>...........................] - ETA: 6:16 - loss: 0.6412 - acc: 0.6156
 704/4849 [===>..........................] - ETA: 6:08 - loss: 0.6332 - acc: 0.6293
 768/4849 [===>..........................] - ETA: 6:03 - loss: 0.6395 - acc: 0.6185
 832/4849 [====>.........................] - ETA: 5:57 - loss: 0.6373 - acc: 0.6262
 896/4849 [====>.........................] - ETA: 5:52 - loss: 0.6371 - acc: 0.6261
 960/4849 [====>.........................] - ETA: 5:48 - loss: 0.6370 - acc: 0.6302
1024/4849 [=====>........................] - ETA: 5:43 - loss: 0.6346 - acc: 0.6338
1088/4849 [=====>........................] - ETA: 5:37 - loss: 0.6347 - acc: 0.6305
1152/4849 [======>.......................] - ETA: 5:29 - loss: 0.6356 - acc: 0.6311
1216/4849 [======>.......................] - ETA: 5:24 - loss: 0.6381 - acc: 0.6291
1280/4849 [======>.......................] - ETA: 5:19 - loss: 0.6380 - acc: 0.6305
1344/4849 [=======>......................] - ETA: 5:13 - loss: 0.6378 - acc: 0.6302
1408/4849 [=======>......................] - ETA: 5:06 - loss: 0.6384 - acc: 0.6307
1472/4849 [========>.....................] - ETA: 5:00 - loss: 0.6408 - acc: 0.6264
1536/4849 [========>.....................] - ETA: 4:55 - loss: 0.6415 - acc: 0.6283
1600/4849 [========>.....................] - ETA: 4:50 - loss: 0.6428 - acc: 0.6269
1664/4849 [=========>....................] - ETA: 4:43 - loss: 0.6423 - acc: 0.6280
1728/4849 [=========>....................] - ETA: 4:38 - loss: 0.6426 - acc: 0.6279
1792/4849 [==========>...................] - ETA: 4:31 - loss: 0.6420 - acc: 0.6289
1856/4849 [==========>...................] - ETA: 4:26 - loss: 0.6395 - acc: 0.6304
1920/4849 [==========>...................] - ETA: 4:20 - loss: 0.6393 - acc: 0.6318
1984/4849 [===========>..................] - ETA: 4:14 - loss: 0.6385 - acc: 0.6336
2048/4849 [===========>..................] - ETA: 4:08 - loss: 0.6383 - acc: 0.6328
2112/4849 [============>.................] - ETA: 4:03 - loss: 0.6375 - acc: 0.6345
2176/4849 [============>.................] - ETA: 3:57 - loss: 0.6355 - acc: 0.6369
2240/4849 [============>.................] - ETA: 3:51 - loss: 0.6351 - acc: 0.6371
2304/4849 [=============>................] - ETA: 3:46 - loss: 0.6360 - acc: 0.6376
2368/4849 [=============>................] - ETA: 3:40 - loss: 0.6367 - acc: 0.6377
2432/4849 [==============>...............] - ETA: 3:34 - loss: 0.6369 - acc: 0.6369
2496/4849 [==============>...............] - ETA: 3:28 - loss: 0.6363 - acc: 0.6374
2560/4849 [==============>...............] - ETA: 3:23 - loss: 0.6344 - acc: 0.6398
2624/4849 [===============>..............] - ETA: 3:17 - loss: 0.6351 - acc: 0.6376
2688/4849 [===============>..............] - ETA: 3:11 - loss: 0.6369 - acc: 0.6365
2752/4849 [================>.............] - ETA: 3:05 - loss: 0.6371 - acc: 0.6363
2816/4849 [================>.............] - ETA: 2:59 - loss: 0.6375 - acc: 0.6357
2880/4849 [================>.............] - ETA: 2:54 - loss: 0.6380 - acc: 0.6354
2944/4849 [=================>............] - ETA: 2:48 - loss: 0.6371 - acc: 0.6362
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6382 - acc: 0.6330
3072/4849 [==================>...........] - ETA: 2:37 - loss: 0.6383 - acc: 0.6325
3136/4849 [==================>...........] - ETA: 2:31 - loss: 0.6384 - acc: 0.6336
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6377 - acc: 0.6347
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6376 - acc: 0.6357
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6385 - acc: 0.6343
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6391 - acc: 0.6335
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6389 - acc: 0.6351
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6384 - acc: 0.6358
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6392 - acc: 0.6345
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6393 - acc: 0.6343
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6390 - acc: 0.6339
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6397 - acc: 0.6335
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6386 - acc: 0.6354
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6389 - acc: 0.6337
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6396 - acc: 0.6326
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6393 - acc: 0.6327
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6409 - acc: 0.6299
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6405 - acc: 0.6305
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6399 - acc: 0.6312 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6408 - acc: 0.6297
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6407 - acc: 0.6301
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6403 - acc: 0.6304
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6405 - acc: 0.6304
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6398 - acc: 0.6314
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6402 - acc: 0.6309
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6400 - acc: 0.6306
4736/4849 [============================>.] - ETA: 9s - loss: 0.6406 - acc: 0.6299 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6397 - acc: 0.6308
4849/4849 [==============================] - 446s 92ms/step - loss: 0.6391 - acc: 0.6315 - val_loss: 0.7081 - val_acc: 0.5751

Epoch 00005: val_acc did not improve from 0.59740
Epoch 6/10

  64/4849 [..............................] - ETA: 6:48 - loss: 0.6016 - acc: 0.6562
 128/4849 [..............................] - ETA: 7:06 - loss: 0.6008 - acc: 0.6953
 192/4849 [>.............................] - ETA: 6:51 - loss: 0.5858 - acc: 0.7031
 256/4849 [>.............................] - ETA: 6:55 - loss: 0.6167 - acc: 0.6562
 320/4849 [>.............................] - ETA: 6:47 - loss: 0.6264 - acc: 0.6344
 384/4849 [=>............................] - ETA: 6:35 - loss: 0.6322 - acc: 0.6302
 448/4849 [=>............................] - ETA: 6:25 - loss: 0.6333 - acc: 0.6362
 512/4849 [==>...........................] - ETA: 6:20 - loss: 0.6275 - acc: 0.6426
 576/4849 [==>...........................] - ETA: 6:12 - loss: 0.6246 - acc: 0.6528
 640/4849 [==>...........................] - ETA: 6:07 - loss: 0.6371 - acc: 0.6375
 704/4849 [===>..........................] - ETA: 6:02 - loss: 0.6318 - acc: 0.6364
 768/4849 [===>..........................] - ETA: 5:58 - loss: 0.6292 - acc: 0.6380
 832/4849 [====>.........................] - ETA: 5:52 - loss: 0.6276 - acc: 0.6382
 896/4849 [====>.........................] - ETA: 5:46 - loss: 0.6234 - acc: 0.6417
 960/4849 [====>.........................] - ETA: 5:39 - loss: 0.6249 - acc: 0.6448
1024/4849 [=====>........................] - ETA: 5:33 - loss: 0.6266 - acc: 0.6465
1088/4849 [=====>........................] - ETA: 5:27 - loss: 0.6292 - acc: 0.6471
1152/4849 [======>.......................] - ETA: 5:21 - loss: 0.6296 - acc: 0.6441
1216/4849 [======>.......................] - ETA: 5:16 - loss: 0.6380 - acc: 0.6365
1280/4849 [======>.......................] - ETA: 5:10 - loss: 0.6377 - acc: 0.6375
1344/4849 [=======>......................] - ETA: 5:04 - loss: 0.6376 - acc: 0.6362
1408/4849 [=======>......................] - ETA: 4:59 - loss: 0.6370 - acc: 0.6349
1472/4849 [========>.....................] - ETA: 4:53 - loss: 0.6328 - acc: 0.6372
1536/4849 [========>.....................] - ETA: 4:48 - loss: 0.6354 - acc: 0.6328
1600/4849 [========>.....................] - ETA: 4:43 - loss: 0.6342 - acc: 0.6362
1664/4849 [=========>....................] - ETA: 4:38 - loss: 0.6359 - acc: 0.6346
1728/4849 [=========>....................] - ETA: 4:30 - loss: 0.6378 - acc: 0.6319
1792/4849 [==========>...................] - ETA: 4:25 - loss: 0.6368 - acc: 0.6339
1856/4849 [==========>...................] - ETA: 4:19 - loss: 0.6382 - acc: 0.6342
1920/4849 [==========>...................] - ETA: 4:13 - loss: 0.6391 - acc: 0.6333
1984/4849 [===========>..................] - ETA: 4:08 - loss: 0.6385 - acc: 0.6326
2048/4849 [===========>..................] - ETA: 4:04 - loss: 0.6369 - acc: 0.6348
2112/4849 [============>.................] - ETA: 3:58 - loss: 0.6375 - acc: 0.6330
2176/4849 [============>.................] - ETA: 3:53 - loss: 0.6354 - acc: 0.6351
2240/4849 [============>.................] - ETA: 3:48 - loss: 0.6378 - acc: 0.6330
2304/4849 [=============>................] - ETA: 3:43 - loss: 0.6379 - acc: 0.6337
2368/4849 [=============>................] - ETA: 3:37 - loss: 0.6374 - acc: 0.6347
2432/4849 [==============>...............] - ETA: 3:32 - loss: 0.6361 - acc: 0.6369
2496/4849 [==============>...............] - ETA: 3:26 - loss: 0.6338 - acc: 0.6386
2560/4849 [==============>...............] - ETA: 3:21 - loss: 0.6330 - acc: 0.6402
2624/4849 [===============>..............] - ETA: 3:15 - loss: 0.6323 - acc: 0.6410
2688/4849 [===============>..............] - ETA: 3:09 - loss: 0.6333 - acc: 0.6414
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.6340 - acc: 0.6395
2816/4849 [================>.............] - ETA: 2:58 - loss: 0.6350 - acc: 0.6378
2880/4849 [================>.............] - ETA: 2:52 - loss: 0.6361 - acc: 0.6365
2944/4849 [=================>............] - ETA: 2:46 - loss: 0.6362 - acc: 0.6359
3008/4849 [=================>............] - ETA: 2:41 - loss: 0.6360 - acc: 0.6353
3072/4849 [==================>...........] - ETA: 2:36 - loss: 0.6359 - acc: 0.6364
3136/4849 [==================>...........] - ETA: 2:30 - loss: 0.6350 - acc: 0.6378
3200/4849 [==================>...........] - ETA: 2:24 - loss: 0.6370 - acc: 0.6356
3264/4849 [===================>..........] - ETA: 2:18 - loss: 0.6358 - acc: 0.6366
3328/4849 [===================>..........] - ETA: 2:13 - loss: 0.6358 - acc: 0.6367
3392/4849 [===================>..........] - ETA: 2:07 - loss: 0.6359 - acc: 0.6353
3456/4849 [====================>.........] - ETA: 2:02 - loss: 0.6350 - acc: 0.6351
3520/4849 [====================>.........] - ETA: 1:56 - loss: 0.6335 - acc: 0.6366
3584/4849 [=====================>........] - ETA: 1:50 - loss: 0.6343 - acc: 0.6370
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6337 - acc: 0.6382
3712/4849 [=====================>........] - ETA: 1:39 - loss: 0.6341 - acc: 0.6379
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6337 - acc: 0.6380
3840/4849 [======================>.......] - ETA: 1:28 - loss: 0.6339 - acc: 0.6385
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6353 - acc: 0.6363
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6357 - acc: 0.6371
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6354 - acc: 0.6374
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6366 - acc: 0.6355
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6366 - acc: 0.6353
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6372 - acc: 0.6345 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6389 - acc: 0.6327
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6390 - acc: 0.6324
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6391 - acc: 0.6327
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6392 - acc: 0.6333
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6401 - acc: 0.6325
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6408 - acc: 0.6315
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6406 - acc: 0.6314
4736/4849 [============================>.] - ETA: 9s - loss: 0.6403 - acc: 0.6313 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6401 - acc: 0.6317
4849/4849 [==============================] - 446s 92ms/step - loss: 0.6403 - acc: 0.6317 - val_loss: 0.6599 - val_acc: 0.6067

Epoch 00006: val_acc improved from 0.59740 to 0.60668, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window11/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 7/10

  64/4849 [..............................] - ETA: 7:01 - loss: 0.6522 - acc: 0.5938
 128/4849 [..............................] - ETA: 7:27 - loss: 0.6654 - acc: 0.5547
 192/4849 [>.............................] - ETA: 7:04 - loss: 0.6584 - acc: 0.6042
 256/4849 [>.............................] - ETA: 6:44 - loss: 0.6654 - acc: 0.5820
 320/4849 [>.............................] - ETA: 6:40 - loss: 0.6539 - acc: 0.6125
 384/4849 [=>............................] - ETA: 6:33 - loss: 0.6476 - acc: 0.6224
 448/4849 [=>............................] - ETA: 6:24 - loss: 0.6428 - acc: 0.6317
 512/4849 [==>...........................] - ETA: 6:20 - loss: 0.6390 - acc: 0.6309
 576/4849 [==>...........................] - ETA: 6:14 - loss: 0.6473 - acc: 0.6128
 640/4849 [==>...........................] - ETA: 6:04 - loss: 0.6464 - acc: 0.6156
 704/4849 [===>..........................] - ETA: 5:58 - loss: 0.6495 - acc: 0.6108
 768/4849 [===>..........................] - ETA: 5:54 - loss: 0.6492 - acc: 0.6094
 832/4849 [====>.........................] - ETA: 5:46 - loss: 0.6460 - acc: 0.6118
 896/4849 [====>.........................] - ETA: 5:41 - loss: 0.6412 - acc: 0.6217
 960/4849 [====>.........................] - ETA: 5:35 - loss: 0.6408 - acc: 0.6219
1024/4849 [=====>........................] - ETA: 5:29 - loss: 0.6427 - acc: 0.6240
1088/4849 [=====>........................] - ETA: 5:25 - loss: 0.6425 - acc: 0.6241
1152/4849 [======>.......................] - ETA: 5:21 - loss: 0.6411 - acc: 0.6241
1216/4849 [======>.......................] - ETA: 5:14 - loss: 0.6414 - acc: 0.6266
1280/4849 [======>.......................] - ETA: 5:08 - loss: 0.6389 - acc: 0.6297
1344/4849 [=======>......................] - ETA: 5:01 - loss: 0.6409 - acc: 0.6287
1408/4849 [=======>......................] - ETA: 4:55 - loss: 0.6397 - acc: 0.6307
1472/4849 [========>.....................] - ETA: 4:51 - loss: 0.6375 - acc: 0.6332
1536/4849 [========>.....................] - ETA: 4:48 - loss: 0.6351 - acc: 0.6354
1600/4849 [========>.....................] - ETA: 4:42 - loss: 0.6366 - acc: 0.6331
1664/4849 [=========>....................] - ETA: 4:38 - loss: 0.6370 - acc: 0.6334
1728/4849 [=========>....................] - ETA: 4:32 - loss: 0.6352 - acc: 0.6360
1792/4849 [==========>...................] - ETA: 4:26 - loss: 0.6328 - acc: 0.6412
1856/4849 [==========>...................] - ETA: 4:22 - loss: 0.6360 - acc: 0.6390
1920/4849 [==========>...................] - ETA: 4:16 - loss: 0.6364 - acc: 0.6385
1984/4849 [===========>..................] - ETA: 4:10 - loss: 0.6364 - acc: 0.6376
2048/4849 [===========>..................] - ETA: 4:05 - loss: 0.6376 - acc: 0.6362
2112/4849 [============>.................] - ETA: 4:00 - loss: 0.6398 - acc: 0.6340
2176/4849 [============>.................] - ETA: 3:54 - loss: 0.6391 - acc: 0.6347
2240/4849 [============>.................] - ETA: 3:49 - loss: 0.6393 - acc: 0.6344
2304/4849 [=============>................] - ETA: 3:43 - loss: 0.6383 - acc: 0.6345
2368/4849 [=============>................] - ETA: 3:38 - loss: 0.6377 - acc: 0.6364
2432/4849 [==============>...............] - ETA: 3:32 - loss: 0.6370 - acc: 0.6353
2496/4849 [==============>...............] - ETA: 3:27 - loss: 0.6366 - acc: 0.6350
2560/4849 [==============>...............] - ETA: 3:22 - loss: 0.6367 - acc: 0.6344
2624/4849 [===============>..............] - ETA: 3:16 - loss: 0.6376 - acc: 0.6338
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.6387 - acc: 0.6332
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.6421 - acc: 0.6301
2816/4849 [================>.............] - ETA: 2:59 - loss: 0.6407 - acc: 0.6314
2880/4849 [================>.............] - ETA: 2:53 - loss: 0.6411 - acc: 0.6309
2944/4849 [=================>............] - ETA: 2:48 - loss: 0.6409 - acc: 0.6304
3008/4849 [=================>............] - ETA: 2:42 - loss: 0.6410 - acc: 0.6307
3072/4849 [==================>...........] - ETA: 2:36 - loss: 0.6409 - acc: 0.6305
3136/4849 [==================>...........] - ETA: 2:31 - loss: 0.6406 - acc: 0.6307
3200/4849 [==================>...........] - ETA: 2:25 - loss: 0.6405 - acc: 0.6312
3264/4849 [===================>..........] - ETA: 2:19 - loss: 0.6402 - acc: 0.6308
3328/4849 [===================>..........] - ETA: 2:13 - loss: 0.6408 - acc: 0.6304
3392/4849 [===================>..........] - ETA: 2:08 - loss: 0.6413 - acc: 0.6297
3456/4849 [====================>.........] - ETA: 2:02 - loss: 0.6410 - acc: 0.6302
3520/4849 [====================>.........] - ETA: 1:56 - loss: 0.6415 - acc: 0.6295
3584/4849 [=====================>........] - ETA: 1:51 - loss: 0.6421 - acc: 0.6286
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6426 - acc: 0.6283
3712/4849 [=====================>........] - ETA: 1:39 - loss: 0.6415 - acc: 0.6298
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6418 - acc: 0.6300
3840/4849 [======================>.......] - ETA: 1:28 - loss: 0.6411 - acc: 0.6310
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6412 - acc: 0.6304
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6415 - acc: 0.6305
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6408 - acc: 0.6322
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6401 - acc: 0.6326
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6394 - acc: 0.6327
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6398 - acc: 0.6319 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6384 - acc: 0.6339
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6387 - acc: 0.6330
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6388 - acc: 0.6325
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6386 - acc: 0.6326
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6377 - acc: 0.6329
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6374 - acc: 0.6335
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6372 - acc: 0.6333
4736/4849 [============================>.] - ETA: 9s - loss: 0.6369 - acc: 0.6341 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6368 - acc: 0.6342
4849/4849 [==============================] - 444s 92ms/step - loss: 0.6356 - acc: 0.6354 - val_loss: 0.6600 - val_acc: 0.6141

Epoch 00007: val_acc improved from 0.60668 to 0.61410, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window11/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 8/10

  64/4849 [..............................] - ETA: 6:17 - loss: 0.6218 - acc: 0.5625
 128/4849 [..............................] - ETA: 6:40 - loss: 0.6303 - acc: 0.5781
 192/4849 [>.............................] - ETA: 6:40 - loss: 0.6173 - acc: 0.6094
 256/4849 [>.............................] - ETA: 6:43 - loss: 0.6303 - acc: 0.6172
 320/4849 [>.............................] - ETA: 6:37 - loss: 0.6255 - acc: 0.6219
 384/4849 [=>............................] - ETA: 6:29 - loss: 0.6142 - acc: 0.6484
 448/4849 [=>............................] - ETA: 6:30 - loss: 0.6144 - acc: 0.6585
 512/4849 [==>...........................] - ETA: 6:22 - loss: 0.6178 - acc: 0.6523
 576/4849 [==>...........................] - ETA: 6:17 - loss: 0.6205 - acc: 0.6510
 640/4849 [==>...........................] - ETA: 6:12 - loss: 0.6238 - acc: 0.6453
 704/4849 [===>..........................] - ETA: 6:04 - loss: 0.6293 - acc: 0.6449
 768/4849 [===>..........................] - ETA: 5:59 - loss: 0.6213 - acc: 0.6510
 832/4849 [====>.........................] - ETA: 5:51 - loss: 0.6187 - acc: 0.6514
 896/4849 [====>.........................] - ETA: 5:46 - loss: 0.6140 - acc: 0.6540
 960/4849 [====>.........................] - ETA: 5:43 - loss: 0.6214 - acc: 0.6490
1024/4849 [=====>........................] - ETA: 5:36 - loss: 0.6217 - acc: 0.6514
1088/4849 [=====>........................] - ETA: 5:29 - loss: 0.6199 - acc: 0.6535
1152/4849 [======>.......................] - ETA: 5:24 - loss: 0.6208 - acc: 0.6536
1216/4849 [======>.......................] - ETA: 5:19 - loss: 0.6262 - acc: 0.6505
1280/4849 [======>.......................] - ETA: 5:12 - loss: 0.6261 - acc: 0.6508
1344/4849 [=======>......................] - ETA: 5:07 - loss: 0.6244 - acc: 0.6510
1408/4849 [=======>......................] - ETA: 5:02 - loss: 0.6253 - acc: 0.6499
1472/4849 [========>.....................] - ETA: 4:57 - loss: 0.6266 - acc: 0.6488
1536/4849 [========>.....................] - ETA: 4:50 - loss: 0.6251 - acc: 0.6484
1600/4849 [========>.....................] - ETA: 4:45 - loss: 0.6298 - acc: 0.6419
1664/4849 [=========>....................] - ETA: 4:39 - loss: 0.6298 - acc: 0.6430
1728/4849 [=========>....................] - ETA: 4:34 - loss: 0.6305 - acc: 0.6406
1792/4849 [==========>...................] - ETA: 4:27 - loss: 0.6297 - acc: 0.6412
1856/4849 [==========>...................] - ETA: 4:21 - loss: 0.6287 - acc: 0.6401
1920/4849 [==========>...................] - ETA: 4:16 - loss: 0.6302 - acc: 0.6396
1984/4849 [===========>..................] - ETA: 4:10 - loss: 0.6295 - acc: 0.6391
2048/4849 [===========>..................] - ETA: 4:04 - loss: 0.6314 - acc: 0.6382
2112/4849 [============>.................] - ETA: 3:59 - loss: 0.6310 - acc: 0.6383
2176/4849 [============>.................] - ETA: 3:54 - loss: 0.6286 - acc: 0.6406
2240/4849 [============>.................] - ETA: 3:48 - loss: 0.6281 - acc: 0.6420
2304/4849 [=============>................] - ETA: 3:43 - loss: 0.6288 - acc: 0.6406
2368/4849 [=============>................] - ETA: 3:37 - loss: 0.6286 - acc: 0.6410
2432/4849 [==============>...............] - ETA: 3:31 - loss: 0.6295 - acc: 0.6394
2496/4849 [==============>...............] - ETA: 3:26 - loss: 0.6288 - acc: 0.6406
2560/4849 [==============>...............] - ETA: 3:20 - loss: 0.6277 - acc: 0.6434
2624/4849 [===============>..............] - ETA: 3:14 - loss: 0.6285 - acc: 0.6421
2688/4849 [===============>..............] - ETA: 3:09 - loss: 0.6282 - acc: 0.6429
2752/4849 [================>.............] - ETA: 3:03 - loss: 0.6306 - acc: 0.6403
2816/4849 [================>.............] - ETA: 2:58 - loss: 0.6326 - acc: 0.6374
2880/4849 [================>.............] - ETA: 2:53 - loss: 0.6324 - acc: 0.6361
2944/4849 [=================>............] - ETA: 2:47 - loss: 0.6320 - acc: 0.6365
3008/4849 [=================>............] - ETA: 2:41 - loss: 0.6312 - acc: 0.6386
3072/4849 [==================>...........] - ETA: 2:36 - loss: 0.6305 - acc: 0.6396
3136/4849 [==================>...........] - ETA: 2:30 - loss: 0.6304 - acc: 0.6400
3200/4849 [==================>...........] - ETA: 2:24 - loss: 0.6301 - acc: 0.6412
3264/4849 [===================>..........] - ETA: 2:19 - loss: 0.6291 - acc: 0.6425
3328/4849 [===================>..........] - ETA: 2:13 - loss: 0.6306 - acc: 0.6397
3392/4849 [===================>..........] - ETA: 2:07 - loss: 0.6295 - acc: 0.6403
3456/4849 [====================>.........] - ETA: 2:01 - loss: 0.6310 - acc: 0.6398
3520/4849 [====================>.........] - ETA: 1:56 - loss: 0.6318 - acc: 0.6398
3584/4849 [=====================>........] - ETA: 1:50 - loss: 0.6309 - acc: 0.6406
3648/4849 [=====================>........] - ETA: 1:45 - loss: 0.6319 - acc: 0.6404
3712/4849 [=====================>........] - ETA: 1:39 - loss: 0.6316 - acc: 0.6409
3776/4849 [======================>.......] - ETA: 1:33 - loss: 0.6315 - acc: 0.6409
3840/4849 [======================>.......] - ETA: 1:28 - loss: 0.6313 - acc: 0.6406
3904/4849 [=======================>......] - ETA: 1:22 - loss: 0.6309 - acc: 0.6409
3968/4849 [=======================>......] - ETA: 1:17 - loss: 0.6313 - acc: 0.6406
4032/4849 [=======================>......] - ETA: 1:11 - loss: 0.6325 - acc: 0.6379
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6324 - acc: 0.6387
4160/4849 [========================>.....] - ETA: 1:00 - loss: 0.6329 - acc: 0.6380
4224/4849 [=========================>....] - ETA: 54s - loss: 0.6326 - acc: 0.6385 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6326 - acc: 0.6376
4352/4849 [=========================>....] - ETA: 43s - loss: 0.6316 - acc: 0.6392
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6312 - acc: 0.6399
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6300 - acc: 0.6408
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6310 - acc: 0.6404
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6293 - acc: 0.6417
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6289 - acc: 0.6426
4736/4849 [============================>.] - ETA: 9s - loss: 0.6287 - acc: 0.6429 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6299 - acc: 0.6417
4849/4849 [==============================] - 444s 92ms/step - loss: 0.6291 - acc: 0.6424 - val_loss: 0.6608 - val_acc: 0.6197

Epoch 00008: val_acc improved from 0.61410 to 0.61967, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window11/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 9/10

  64/4849 [..............................] - ETA: 7:21 - loss: 0.6337 - acc: 0.6562
 128/4849 [..............................] - ETA: 6:50 - loss: 0.6427 - acc: 0.6250
 192/4849 [>.............................] - ETA: 6:38 - loss: 0.6341 - acc: 0.6510
 256/4849 [>.............................] - ETA: 6:37 - loss: 0.6052 - acc: 0.6836
 320/4849 [>.............................] - ETA: 6:23 - loss: 0.5903 - acc: 0.6937
 384/4849 [=>............................] - ETA: 6:27 - loss: 0.5877 - acc: 0.6953
 448/4849 [=>............................] - ETA: 6:23 - loss: 0.5865 - acc: 0.6942
 512/4849 [==>...........................] - ETA: 6:15 - loss: 0.5918 - acc: 0.6836
 576/4849 [==>...........................] - ETA: 6:10 - loss: 0.5885 - acc: 0.6858
 640/4849 [==>...........................] - ETA: 6:04 - loss: 0.5911 - acc: 0.6828
 704/4849 [===>..........................] - ETA: 5:57 - loss: 0.5966 - acc: 0.6761
 768/4849 [===>..........................] - ETA: 5:54 - loss: 0.5992 - acc: 0.6758
 832/4849 [====>.........................] - ETA: 5:49 - loss: 0.5940 - acc: 0.6791
 896/4849 [====>.........................] - ETA: 5:43 - loss: 0.5936 - acc: 0.6797
 960/4849 [====>.........................] - ETA: 5:37 - loss: 0.5943 - acc: 0.6760
1024/4849 [=====>........................] - ETA: 5:34 - loss: 0.6061 - acc: 0.6660
1088/4849 [=====>........................] - ETA: 5:28 - loss: 0.6088 - acc: 0.6645
1152/4849 [======>.......................] - ETA: 5:21 - loss: 0.6099 - acc: 0.6641
1216/4849 [======>.......................] - ETA: 5:15 - loss: 0.6167 - acc: 0.6579
1280/4849 [======>.......................] - ETA: 5:08 - loss: 0.6201 - acc: 0.6555
1344/4849 [=======>......................] - ETA: 5:03 - loss: 0.6157 - acc: 0.6607
1408/4849 [=======>......................] - ETA: 4:58 - loss: 0.6170 - acc: 0.6562
1472/4849 [========>.....................] - ETA: 4:53 - loss: 0.6179 - acc: 0.6562
1536/4849 [========>.....................] - ETA: 4:46 - loss: 0.6229 - acc: 0.6491
1600/4849 [========>.....................] - ETA: 4:41 - loss: 0.6228 - acc: 0.6500
1664/4849 [=========>....................] - ETA: 4:34 - loss: 0.6235 - acc: 0.6484
1728/4849 [=========>....................] - ETA: 4:29 - loss: 0.6215 - acc: 0.6499
1792/4849 [==========>...................] - ETA: 4:23 - loss: 0.6211 - acc: 0.6507
1856/4849 [==========>...................] - ETA: 4:17 - loss: 0.6217 - acc: 0.6503
1920/4849 [==========>...................] - ETA: 4:11 - loss: 0.6219 - acc: 0.6490
1984/4849 [===========>..................] - ETA: 4:06 - loss: 0.6215 - acc: 0.6502
2048/4849 [===========>..................] - ETA: 3:59 - loss: 0.6216 - acc: 0.6489
2112/4849 [============>.................] - ETA: 3:54 - loss: 0.6232 - acc: 0.6458
2176/4849 [============>.................] - ETA: 3:49 - loss: 0.6251 - acc: 0.6448
2240/4849 [============>.................] - ETA: 3:44 - loss: 0.6250 - acc: 0.6442
2304/4849 [=============>................] - ETA: 3:38 - loss: 0.6280 - acc: 0.6419
2368/4849 [=============>................] - ETA: 3:33 - loss: 0.6275 - acc: 0.6427
2432/4849 [==============>...............] - ETA: 3:27 - loss: 0.6272 - acc: 0.6431
2496/4849 [==============>...............] - ETA: 3:22 - loss: 0.6274 - acc: 0.6438
2560/4849 [==============>...............] - ETA: 3:17 - loss: 0.6277 - acc: 0.6434
2624/4849 [===============>..............] - ETA: 3:11 - loss: 0.6278 - acc: 0.6433
2688/4849 [===============>..............] - ETA: 3:06 - loss: 0.6288 - acc: 0.6410
2752/4849 [================>.............] - ETA: 3:00 - loss: 0.6290 - acc: 0.6406
2816/4849 [================>.............] - ETA: 2:55 - loss: 0.6301 - acc: 0.6392
2880/4849 [================>.............] - ETA: 2:50 - loss: 0.6302 - acc: 0.6399
2944/4849 [=================>............] - ETA: 2:44 - loss: 0.6317 - acc: 0.6379
3008/4849 [=================>............] - ETA: 2:39 - loss: 0.6304 - acc: 0.6396
3072/4849 [==================>...........] - ETA: 2:33 - loss: 0.6302 - acc: 0.6413
3136/4849 [==================>...........] - ETA: 2:28 - loss: 0.6316 - acc: 0.6381
3200/4849 [==================>...........] - ETA: 2:22 - loss: 0.6316 - acc: 0.6388
3264/4849 [===================>..........] - ETA: 2:17 - loss: 0.6315 - acc: 0.6391
3328/4849 [===================>..........] - ETA: 2:11 - loss: 0.6315 - acc: 0.6391
3392/4849 [===================>..........] - ETA: 2:06 - loss: 0.6309 - acc: 0.6392
3456/4849 [====================>.........] - ETA: 2:00 - loss: 0.6308 - acc: 0.6406
3520/4849 [====================>.........] - ETA: 1:55 - loss: 0.6302 - acc: 0.6412
3584/4849 [=====================>........] - ETA: 1:49 - loss: 0.6311 - acc: 0.6401
3648/4849 [=====================>........] - ETA: 1:43 - loss: 0.6305 - acc: 0.6409
3712/4849 [=====================>........] - ETA: 1:38 - loss: 0.6308 - acc: 0.6417
3776/4849 [======================>.......] - ETA: 1:32 - loss: 0.6317 - acc: 0.6404
3840/4849 [======================>.......] - ETA: 1:27 - loss: 0.6314 - acc: 0.6404
3904/4849 [=======================>......] - ETA: 1:21 - loss: 0.6315 - acc: 0.6401
3968/4849 [=======================>......] - ETA: 1:16 - loss: 0.6328 - acc: 0.6396
4032/4849 [=======================>......] - ETA: 1:10 - loss: 0.6331 - acc: 0.6401
4096/4849 [========================>.....] - ETA: 1:05 - loss: 0.6322 - acc: 0.6421
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6316 - acc: 0.6428 
4224/4849 [=========================>....] - ETA: 53s - loss: 0.6322 - acc: 0.6423
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6324 - acc: 0.6416
4352/4849 [=========================>....] - ETA: 42s - loss: 0.6320 - acc: 0.6425
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6326 - acc: 0.6424
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6326 - acc: 0.6417
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6324 - acc: 0.6422
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6315 - acc: 0.6434
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6315 - acc: 0.6430
4736/4849 [============================>.] - ETA: 9s - loss: 0.6311 - acc: 0.6436 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6305 - acc: 0.6440
4849/4849 [==============================] - 434s 89ms/step - loss: 0.6290 - acc: 0.6455 - val_loss: 0.7126 - val_acc: 0.6085

Epoch 00009: val_acc did not improve from 0.61967
Epoch 10/10

  64/4849 [..............................] - ETA: 6:26 - loss: 0.7044 - acc: 0.5938
 128/4849 [..............................] - ETA: 6:32 - loss: 0.6659 - acc: 0.6328
 192/4849 [>.............................] - ETA: 6:28 - loss: 0.6684 - acc: 0.6302
 256/4849 [>.............................] - ETA: 6:21 - loss: 0.6727 - acc: 0.6250
 320/4849 [>.............................] - ETA: 6:14 - loss: 0.6819 - acc: 0.6062
 384/4849 [=>............................] - ETA: 6:11 - loss: 0.6632 - acc: 0.6172
 448/4849 [=>............................] - ETA: 6:07 - loss: 0.6600 - acc: 0.6205
 512/4849 [==>...........................] - ETA: 6:05 - loss: 0.6665 - acc: 0.6133
 576/4849 [==>...........................] - ETA: 6:07 - loss: 0.6685 - acc: 0.6059
 640/4849 [==>...........................] - ETA: 6:03 - loss: 0.6636 - acc: 0.6141
 704/4849 [===>..........................] - ETA: 5:56 - loss: 0.6576 - acc: 0.6236
 768/4849 [===>..........................] - ETA: 5:52 - loss: 0.6529 - acc: 0.6250
 832/4849 [====>.........................] - ETA: 5:50 - loss: 0.6498 - acc: 0.6262
 896/4849 [====>.........................] - ETA: 5:45 - loss: 0.6486 - acc: 0.6272
 960/4849 [====>.........................] - ETA: 5:40 - loss: 0.6455 - acc: 0.6271
1024/4849 [=====>........................] - ETA: 5:34 - loss: 0.6478 - acc: 0.6230
1088/4849 [=====>........................] - ETA: 5:27 - loss: 0.6456 - acc: 0.6204
1152/4849 [======>.......................] - ETA: 5:21 - loss: 0.6431 - acc: 0.6233
1216/4849 [======>.......................] - ETA: 5:17 - loss: 0.6455 - acc: 0.6209
1280/4849 [======>.......................] - ETA: 5:11 - loss: 0.6467 - acc: 0.6203
1344/4849 [=======>......................] - ETA: 5:05 - loss: 0.6460 - acc: 0.6213
1408/4849 [=======>......................] - ETA: 5:00 - loss: 0.6480 - acc: 0.6222
1472/4849 [========>.....................] - ETA: 4:54 - loss: 0.6439 - acc: 0.6277
1536/4849 [========>.....................] - ETA: 4:48 - loss: 0.6395 - acc: 0.6302
1600/4849 [========>.....................] - ETA: 4:43 - loss: 0.6408 - acc: 0.6275
1664/4849 [=========>....................] - ETA: 4:37 - loss: 0.6432 - acc: 0.6250
1728/4849 [=========>....................] - ETA: 4:31 - loss: 0.6422 - acc: 0.6204
1792/4849 [==========>...................] - ETA: 4:25 - loss: 0.6420 - acc: 0.6233
1856/4849 [==========>...................] - ETA: 4:21 - loss: 0.6413 - acc: 0.6228
1920/4849 [==========>...................] - ETA: 4:15 - loss: 0.6411 - acc: 0.6245
1984/4849 [===========>..................] - ETA: 4:09 - loss: 0.6396 - acc: 0.6265
2048/4849 [===========>..................] - ETA: 4:04 - loss: 0.6403 - acc: 0.6240
2112/4849 [============>.................] - ETA: 3:58 - loss: 0.6388 - acc: 0.6259
2176/4849 [============>.................] - ETA: 3:52 - loss: 0.6372 - acc: 0.6296
2240/4849 [============>.................] - ETA: 3:47 - loss: 0.6361 - acc: 0.6330
2304/4849 [=============>................] - ETA: 3:41 - loss: 0.6358 - acc: 0.6332
2368/4849 [=============>................] - ETA: 3:35 - loss: 0.6341 - acc: 0.6356
2432/4849 [==============>...............] - ETA: 3:30 - loss: 0.6323 - acc: 0.6382
2496/4849 [==============>...............] - ETA: 3:24 - loss: 0.6314 - acc: 0.6406
2560/4849 [==============>...............] - ETA: 3:18 - loss: 0.6308 - acc: 0.6422
2624/4849 [===============>..............] - ETA: 3:12 - loss: 0.6305 - acc: 0.6418
2688/4849 [===============>..............] - ETA: 3:07 - loss: 0.6311 - acc: 0.6421
2752/4849 [================>.............] - ETA: 3:02 - loss: 0.6305 - acc: 0.6417
2816/4849 [================>.............] - ETA: 2:56 - loss: 0.6321 - acc: 0.6406
2880/4849 [================>.............] - ETA: 2:51 - loss: 0.6329 - acc: 0.6399
2944/4849 [=================>............] - ETA: 2:45 - loss: 0.6337 - acc: 0.6396
3008/4849 [=================>............] - ETA: 2:40 - loss: 0.6336 - acc: 0.6400
3072/4849 [==================>...........] - ETA: 2:34 - loss: 0.6317 - acc: 0.6423
3136/4849 [==================>...........] - ETA: 2:28 - loss: 0.6330 - acc: 0.6413
3200/4849 [==================>...........] - ETA: 2:23 - loss: 0.6327 - acc: 0.6416
3264/4849 [===================>..........] - ETA: 2:17 - loss: 0.6322 - acc: 0.6419
3328/4849 [===================>..........] - ETA: 2:12 - loss: 0.6340 - acc: 0.6397
3392/4849 [===================>..........] - ETA: 2:06 - loss: 0.6317 - acc: 0.6424
3456/4849 [====================>.........] - ETA: 2:01 - loss: 0.6315 - acc: 0.6427
3520/4849 [====================>.........] - ETA: 1:55 - loss: 0.6312 - acc: 0.6432
3584/4849 [=====================>........] - ETA: 1:49 - loss: 0.6328 - acc: 0.6409
3648/4849 [=====================>........] - ETA: 1:44 - loss: 0.6328 - acc: 0.6404
3712/4849 [=====================>........] - ETA: 1:38 - loss: 0.6319 - acc: 0.6406
3776/4849 [======================>.......] - ETA: 1:33 - loss: 0.6311 - acc: 0.6425
3840/4849 [======================>.......] - ETA: 1:27 - loss: 0.6310 - acc: 0.6422
3904/4849 [=======================>......] - ETA: 1:21 - loss: 0.6329 - acc: 0.6401
3968/4849 [=======================>......] - ETA: 1:16 - loss: 0.6325 - acc: 0.6406
4032/4849 [=======================>......] - ETA: 1:10 - loss: 0.6326 - acc: 0.6406
4096/4849 [========================>.....] - ETA: 1:04 - loss: 0.6324 - acc: 0.6416
4160/4849 [========================>.....] - ETA: 59s - loss: 0.6329 - acc: 0.6406 
4224/4849 [=========================>....] - ETA: 53s - loss: 0.6330 - acc: 0.6404
4288/4849 [=========================>....] - ETA: 48s - loss: 0.6334 - acc: 0.6388
4352/4849 [=========================>....] - ETA: 42s - loss: 0.6323 - acc: 0.6402
4416/4849 [==========================>...] - ETA: 37s - loss: 0.6320 - acc: 0.6409
4480/4849 [==========================>...] - ETA: 31s - loss: 0.6311 - acc: 0.6422
4544/4849 [===========================>..] - ETA: 26s - loss: 0.6307 - acc: 0.6422
4608/4849 [===========================>..] - ETA: 20s - loss: 0.6316 - acc: 0.6402
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6320 - acc: 0.6396
4736/4849 [============================>.] - ETA: 9s - loss: 0.6317 - acc: 0.6408 
4800/4849 [============================>.] - ETA: 4s - loss: 0.6320 - acc: 0.6404
4849/4849 [==============================] - 433s 89ms/step - loss: 0.6327 - acc: 0.6395 - val_loss: 0.6929 - val_acc: 0.5751

Epoch 00010: val_acc did not improve from 0.61967
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd8eb274510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd8eb274510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd8eae82250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd8eae82250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eadd1690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eadd1690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8ead72410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8ead72410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8ead33610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8ead33610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eab9e2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eab9e2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8ead72590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8ead72590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eab8c090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eab8c090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd884a1dcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd884a1dcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8eacf8fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8eacf8fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884b39f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884b39f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd884a1d210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd884a1d210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8ea97bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8ea97bc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8ea8f0e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8ea8f0e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8e27b5310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8e27b5310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e26cf110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e26cf110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2f40c8f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2f40c8f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e28b9210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e28b9210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8e25d1bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8e25d1bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8e2483910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8e2483910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85ad84c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd85ad84c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8e27a0a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8e27a0a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e237c990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e237c990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8e2699cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8e2699cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8e216bd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8e216bd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e24751d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e24751d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8e22fdad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8e22fdad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e21e6f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e21e6f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8d2084690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8d2084690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8d1dfeb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8d1dfeb50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8d1ea4490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8d1ea4490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8e21e6e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8e21e6e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8d1cfbb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8d1cfbb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8d1bfecd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8d1bfecd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8d1b78110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8d1b78110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8d1a0d310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8d1a0d310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8d1e3ed10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8d1e3ed10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8d1be7a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8d1be7a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8c98a6c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8c98a6c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8c9808b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8c9808b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8c96c0090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8c96c0090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8d18f0ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8d18f0ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8c98a6e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8c98a6e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8c98ab2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8c98ab2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8c958fe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8c958fe90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8c95e2ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8c95e2ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8c989b490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8c989b490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8c9530d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8c9530d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8c94b2510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8c94b2510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8c94b6590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8c94b6590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b9081510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b9081510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8c94b2b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8c94b2b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b9079490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b9079490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8b8fd5e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8b8fd5e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8c91ce050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8c91ce050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b8ef8a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b8ef8a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8c9307a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8c9307a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8c91ce150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8c91ce150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8b8c78a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8b8c78a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b8b4ebd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b8b4ebd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b8d96450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b8d96450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8b8e27150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8b8e27150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b8b1cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b8b1cf90>>: AttributeError: module 'gast' has no attribute 'Str'
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 1:49
 128/1348 [=>............................] - ETA: 1:11
 192/1348 [===>..........................] - ETA: 56s 
 256/1348 [====>.........................] - ETA: 48s
 320/1348 [======>.......................] - ETA: 41s
 384/1348 [=======>......................] - ETA: 37s
 448/1348 [========>.....................] - ETA: 34s
 512/1348 [==========>...................] - ETA: 31s
 576/1348 [===========>..................] - ETA: 28s
 640/1348 [=============>................] - ETA: 25s
 704/1348 [==============>...............] - ETA: 23s
 768/1348 [================>.............] - ETA: 20s
 832/1348 [=================>............] - ETA: 18s
 896/1348 [==================>...........] - ETA: 15s
 960/1348 [====================>.........] - ETA: 13s
1024/1348 [=====================>........] - ETA: 11s
1088/1348 [=======================>......] - ETA: 8s 
1152/1348 [========================>.....] - ETA: 6s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 46s 34ms/step
loss: 0.6332313726139351
acc: 0.6416913946587537
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd2f4105810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd2f4105810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd8eb078910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd8eb078910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2f40cf110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2f40cf110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8af968f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8af968f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1341755d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1341755d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eb0a7510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eb0a7510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8af968a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8af968a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eaf95190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8eaf95190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8eb286750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8eb286750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8eb192f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8eb192f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d46ee250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d46ee250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8eb0f1110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8eb0f1110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d46923d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d46923d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2d46c0f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2d46c0f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2d44d72d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2d44d72d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d4608e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d4608e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2d46dc5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2d46dc5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d45bf450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d45bf450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2d4490590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2d4490590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2d41b6250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2d41b6250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d417c690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d417c690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2d438f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2d438f5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d41ab090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2d41ab090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8eaef2490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8eaef2490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2b461ad10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2b461ad10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2b475b950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2b475b950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2d4313490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2d4313490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2b44fb650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2b44fb650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2b44b0110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2b44b0110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2b44fb350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2b44fb350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2b422a0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2b422a0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2d4074e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2d4074e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2b445a410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2b445a410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2b414ec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2b414ec90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2b417bd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2b417bd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b8c22f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b8c22f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2b414e690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2b414e690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2947e1550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2947e1550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd294591210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd294591210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2944a8dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2944a8dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2943a4690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2943a4690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd294685890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd294685890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2945efe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2945efe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2945d8410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2945d8410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd29414ea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd29414ea90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2b41461d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2b41461d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2943a4650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2943a4650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd12477eb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd12477eb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2941b6410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2941b6410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1245f9550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd1245f9550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd29419b050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd29419b050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd29418d4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd29418d4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd12452a810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd12452a810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd124423550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd124423550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd124417c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd124417c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1246fd050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1246fd050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd124512fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd124512fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1244760d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd1244760d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1242c6150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd1242c6150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd0f47a2b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd0f47a2b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd0f469e850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd0f469e850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd124449390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd124449390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd124114d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd124114d90>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 19:48 - loss: 0.7087 - acc: 0.5469
 128/4849 [..............................] - ETA: 13:50 - loss: 0.7807 - acc: 0.5078
 192/4849 [>.............................] - ETA: 11:39 - loss: 0.7634 - acc: 0.5000
 256/4849 [>.............................] - ETA: 10:20 - loss: 0.7586 - acc: 0.5078
 320/4849 [>.............................] - ETA: 9:35 - loss: 0.7462 - acc: 0.5094 
 384/4849 [=>............................] - ETA: 9:06 - loss: 0.7503 - acc: 0.5078
 448/4849 [=>............................] - ETA: 8:48 - loss: 0.7569 - acc: 0.5045
 512/4849 [==>...........................] - ETA: 8:33 - loss: 0.7538 - acc: 0.5078
 576/4849 [==>...........................] - ETA: 8:14 - loss: 0.7510 - acc: 0.5035
 640/4849 [==>...........................] - ETA: 8:02 - loss: 0.7450 - acc: 0.5141
 704/4849 [===>..........................] - ETA: 7:48 - loss: 0.7396 - acc: 0.5199
 768/4849 [===>..........................] - ETA: 7:36 - loss: 0.7415 - acc: 0.5169
 832/4849 [====>.........................] - ETA: 7:27 - loss: 0.7375 - acc: 0.5216
 896/4849 [====>.........................] - ETA: 7:18 - loss: 0.7322 - acc: 0.5246
 960/4849 [====>.........................] - ETA: 7:08 - loss: 0.7322 - acc: 0.5250
1024/4849 [=====>........................] - ETA: 7:00 - loss: 0.7352 - acc: 0.5176
1088/4849 [=====>........................] - ETA: 6:52 - loss: 0.7393 - acc: 0.5110
1152/4849 [======>.......................] - ETA: 6:43 - loss: 0.7378 - acc: 0.5122
1216/4849 [======>.......................] - ETA: 6:34 - loss: 0.7371 - acc: 0.5082
1280/4849 [======>.......................] - ETA: 6:27 - loss: 0.7374 - acc: 0.5078
1344/4849 [=======>......................] - ETA: 6:18 - loss: 0.7373 - acc: 0.5074
1408/4849 [=======>......................] - ETA: 6:10 - loss: 0.7384 - acc: 0.5021
1472/4849 [========>.....................] - ETA: 6:02 - loss: 0.7388 - acc: 0.5000
1536/4849 [========>.....................] - ETA: 5:56 - loss: 0.7368 - acc: 0.5007
1600/4849 [========>.....................] - ETA: 5:48 - loss: 0.7352 - acc: 0.5025
1664/4849 [=========>....................] - ETA: 5:40 - loss: 0.7320 - acc: 0.5060
1728/4849 [=========>....................] - ETA: 5:33 - loss: 0.7319 - acc: 0.5052
1792/4849 [==========>...................] - ETA: 5:26 - loss: 0.7337 - acc: 0.5022
1856/4849 [==========>...................] - ETA: 5:19 - loss: 0.7325 - acc: 0.5043
1920/4849 [==========>...................] - ETA: 5:12 - loss: 0.7319 - acc: 0.5042
1984/4849 [===========>..................] - ETA: 5:04 - loss: 0.7312 - acc: 0.5035
2048/4849 [===========>..................] - ETA: 4:57 - loss: 0.7312 - acc: 0.5024
2112/4849 [============>.................] - ETA: 4:50 - loss: 0.7314 - acc: 0.5005
2176/4849 [============>.................] - ETA: 4:42 - loss: 0.7312 - acc: 0.5000
2240/4849 [============>.................] - ETA: 4:35 - loss: 0.7299 - acc: 0.5040
2304/4849 [=============>................] - ETA: 4:28 - loss: 0.7285 - acc: 0.5052
2368/4849 [=============>................] - ETA: 4:21 - loss: 0.7281 - acc: 0.5055
2432/4849 [==============>...............] - ETA: 4:14 - loss: 0.7280 - acc: 0.5058
2496/4849 [==============>...............] - ETA: 4:07 - loss: 0.7267 - acc: 0.5080
2560/4849 [==============>...............] - ETA: 4:01 - loss: 0.7276 - acc: 0.5066
2624/4849 [===============>..............] - ETA: 3:54 - loss: 0.7280 - acc: 0.5050
2688/4849 [===============>..............] - ETA: 3:47 - loss: 0.7278 - acc: 0.5052
2752/4849 [================>.............] - ETA: 3:41 - loss: 0.7273 - acc: 0.5047
2816/4849 [================>.............] - ETA: 3:34 - loss: 0.7263 - acc: 0.5053
2880/4849 [================>.............] - ETA: 3:28 - loss: 0.7263 - acc: 0.5035
2944/4849 [=================>............] - ETA: 3:21 - loss: 0.7254 - acc: 0.5051
3008/4849 [=================>............] - ETA: 3:14 - loss: 0.7248 - acc: 0.5040
3072/4849 [==================>...........] - ETA: 3:08 - loss: 0.7244 - acc: 0.5029
3136/4849 [==================>...........] - ETA: 3:01 - loss: 0.7235 - acc: 0.5035
3200/4849 [==================>...........] - ETA: 2:54 - loss: 0.7230 - acc: 0.5041
3264/4849 [===================>..........] - ETA: 2:47 - loss: 0.7233 - acc: 0.5028
3328/4849 [===================>..........] - ETA: 2:40 - loss: 0.7232 - acc: 0.5024
3392/4849 [===================>..........] - ETA: 2:33 - loss: 0.7222 - acc: 0.5047
3456/4849 [====================>.........] - ETA: 2:26 - loss: 0.7217 - acc: 0.5069
3520/4849 [====================>.........] - ETA: 2:19 - loss: 0.7211 - acc: 0.5071
3584/4849 [=====================>........] - ETA: 2:12 - loss: 0.7209 - acc: 0.5078
3648/4849 [=====================>........] - ETA: 2:06 - loss: 0.7195 - acc: 0.5104
3712/4849 [=====================>........] - ETA: 1:59 - loss: 0.7191 - acc: 0.5116
3776/4849 [======================>.......] - ETA: 1:52 - loss: 0.7193 - acc: 0.5109
3840/4849 [======================>.......] - ETA: 1:45 - loss: 0.7189 - acc: 0.5112
3904/4849 [=======================>......] - ETA: 1:38 - loss: 0.7180 - acc: 0.5113
3968/4849 [=======================>......] - ETA: 1:32 - loss: 0.7181 - acc: 0.5098
4032/4849 [=======================>......] - ETA: 1:25 - loss: 0.7173 - acc: 0.5114
4096/4849 [========================>.....] - ETA: 1:18 - loss: 0.7174 - acc: 0.5110
4160/4849 [========================>.....] - ETA: 1:11 - loss: 0.7168 - acc: 0.5132
4224/4849 [=========================>....] - ETA: 1:05 - loss: 0.7159 - acc: 0.5142
4288/4849 [=========================>....] - ETA: 58s - loss: 0.7170 - acc: 0.5126 
4352/4849 [=========================>....] - ETA: 51s - loss: 0.7170 - acc: 0.5115
4416/4849 [==========================>...] - ETA: 45s - loss: 0.7167 - acc: 0.5115
4480/4849 [==========================>...] - ETA: 38s - loss: 0.7158 - acc: 0.5127
4544/4849 [===========================>..] - ETA: 31s - loss: 0.7153 - acc: 0.5139
4608/4849 [===========================>..] - ETA: 25s - loss: 0.7162 - acc: 0.5122
4672/4849 [===========================>..] - ETA: 18s - loss: 0.7166 - acc: 0.5113
4736/4849 [============================>.] - ETA: 11s - loss: 0.7169 - acc: 0.5103
4800/4849 [============================>.] - ETA: 5s - loss: 0.7171 - acc: 0.5098 
4849/4849 [==============================] - 519s 107ms/step - loss: 0.7172 - acc: 0.5098 - val_loss: 0.6905 - val_acc: 0.5473

Epoch 00001: val_acc improved from -inf to 0.54731, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window12/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 7:10 - loss: 0.7329 - acc: 0.4688
 128/4849 [..............................] - ETA: 7:05 - loss: 0.7262 - acc: 0.5234
 192/4849 [>.............................] - ETA: 6:53 - loss: 0.7184 - acc: 0.5260
 256/4849 [>.............................] - ETA: 6:34 - loss: 0.7179 - acc: 0.5195
 320/4849 [>.............................] - ETA: 6:33 - loss: 0.7159 - acc: 0.5125
 384/4849 [=>............................] - ETA: 6:25 - loss: 0.7225 - acc: 0.5000
 448/4849 [=>............................] - ETA: 6:23 - loss: 0.7137 - acc: 0.5089
 512/4849 [==>...........................] - ETA: 6:14 - loss: 0.7084 - acc: 0.5117
 576/4849 [==>...........................] - ETA: 6:08 - loss: 0.7077 - acc: 0.5191
 640/4849 [==>...........................] - ETA: 6:06 - loss: 0.7058 - acc: 0.5250
 704/4849 [===>..........................] - ETA: 6:03 - loss: 0.7003 - acc: 0.5369
 768/4849 [===>..........................] - ETA: 5:58 - loss: 0.7014 - acc: 0.5312
 832/4849 [====>.........................] - ETA: 5:51 - loss: 0.7030 - acc: 0.5288
 896/4849 [====>.........................] - ETA: 5:45 - loss: 0.7027 - acc: 0.5290
 960/4849 [====>.........................] - ETA: 5:41 - loss: 0.7007 - acc: 0.5333
1024/4849 [=====>........................] - ETA: 5:37 - loss: 0.6960 - acc: 0.5410
1088/4849 [=====>........................] - ETA: 5:31 - loss: 0.6948 - acc: 0.5487
1152/4849 [======>.......................] - ETA: 5:26 - loss: 0.6964 - acc: 0.5443
1216/4849 [======>.......................] - ETA: 5:21 - loss: 0.6947 - acc: 0.5469
1280/4849 [======>.......................] - ETA: 5:17 - loss: 0.6952 - acc: 0.5406
1344/4849 [=======>......................] - ETA: 5:11 - loss: 0.6957 - acc: 0.5394
1408/4849 [=======>......................] - ETA: 5:04 - loss: 0.6958 - acc: 0.5376
1472/4849 [========>.....................] - ETA: 5:00 - loss: 0.6977 - acc: 0.5333
1536/4849 [========>.....................] - ETA: 4:55 - loss: 0.6971 - acc: 0.5365
1600/4849 [========>.....................] - ETA: 4:50 - loss: 0.6960 - acc: 0.5406
1664/4849 [=========>....................] - ETA: 4:45 - loss: 0.6945 - acc: 0.5451
1728/4849 [=========>....................] - ETA: 4:39 - loss: 0.6923 - acc: 0.5498
1792/4849 [==========>...................] - ETA: 4:33 - loss: 0.6933 - acc: 0.5469
1856/4849 [==========>...................] - ETA: 4:28 - loss: 0.6929 - acc: 0.5453
1920/4849 [==========>...................] - ETA: 4:22 - loss: 0.6942 - acc: 0.5443
1984/4849 [===========>..................] - ETA: 4:17 - loss: 0.6951 - acc: 0.5433
2048/4849 [===========>..................] - ETA: 4:11 - loss: 0.6941 - acc: 0.5439
2112/4849 [============>.................] - ETA: 4:05 - loss: 0.6937 - acc: 0.5450
2176/4849 [============>.................] - ETA: 4:00 - loss: 0.6937 - acc: 0.5446
2240/4849 [============>.................] - ETA: 3:54 - loss: 0.6948 - acc: 0.5446
2304/4849 [=============>................] - ETA: 3:48 - loss: 0.6936 - acc: 0.5460
2368/4849 [=============>................] - ETA: 3:42 - loss: 0.6930 - acc: 0.5460
2432/4849 [==============>...............] - ETA: 3:37 - loss: 0.6943 - acc: 0.5432
2496/4849 [==============>...............] - ETA: 3:31 - loss: 0.6942 - acc: 0.5445
2560/4849 [==============>...............] - ETA: 3:25 - loss: 0.6944 - acc: 0.5445
2624/4849 [===============>..............] - ETA: 3:20 - loss: 0.6947 - acc: 0.5434
2688/4849 [===============>..............] - ETA: 3:14 - loss: 0.6938 - acc: 0.5443
2752/4849 [================>.............] - ETA: 3:09 - loss: 0.6935 - acc: 0.5451
2816/4849 [================>.............] - ETA: 3:02 - loss: 0.6928 - acc: 0.5465
2880/4849 [================>.............] - ETA: 2:57 - loss: 0.6924 - acc: 0.5458
2944/4849 [=================>............] - ETA: 2:51 - loss: 0.6930 - acc: 0.5442
3008/4849 [=================>............] - ETA: 2:45 - loss: 0.6935 - acc: 0.5432
3072/4849 [==================>...........] - ETA: 2:40 - loss: 0.6939 - acc: 0.5430
3136/4849 [==================>...........] - ETA: 2:34 - loss: 0.6950 - acc: 0.5418
3200/4849 [==================>...........] - ETA: 2:29 - loss: 0.6943 - acc: 0.5437
3264/4849 [===================>..........] - ETA: 2:23 - loss: 0.6949 - acc: 0.5420
3328/4849 [===================>..........] - ETA: 2:17 - loss: 0.6947 - acc: 0.5424
3392/4849 [===================>..........] - ETA: 2:11 - loss: 0.6950 - acc: 0.5419
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.6950 - acc: 0.5425
3520/4849 [====================>.........] - ETA: 2:00 - loss: 0.6949 - acc: 0.5420
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6951 - acc: 0.5416
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.6945 - acc: 0.5419
3712/4849 [=====================>........] - ETA: 1:42 - loss: 0.6953 - acc: 0.5409
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6967 - acc: 0.5389
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6972 - acc: 0.5385
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6966 - acc: 0.5392
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6964 - acc: 0.5396
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6963 - acc: 0.5397
4096/4849 [========================>.....] - ETA: 1:08 - loss: 0.6961 - acc: 0.5400
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6956 - acc: 0.5416
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6954 - acc: 0.5419 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6949 - acc: 0.5420
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6940 - acc: 0.5434
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6926 - acc: 0.5460
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6920 - acc: 0.5464
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6928 - acc: 0.5456
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6931 - acc: 0.5451
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6932 - acc: 0.5443
4736/4849 [============================>.] - ETA: 10s - loss: 0.6939 - acc: 0.5435
4800/4849 [============================>.] - ETA: 4s - loss: 0.6941 - acc: 0.5440 
4849/4849 [==============================] - 456s 94ms/step - loss: 0.6935 - acc: 0.5444 - val_loss: 0.6836 - val_acc: 0.5622

Epoch 00002: val_acc improved from 0.54731 to 0.56215, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window12/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 3/10

  64/4849 [..............................] - ETA: 6:54 - loss: 0.6649 - acc: 0.5625
 128/4849 [..............................] - ETA: 6:47 - loss: 0.7024 - acc: 0.5312
 192/4849 [>.............................] - ETA: 6:49 - loss: 0.6853 - acc: 0.5938
 256/4849 [>.............................] - ETA: 6:43 - loss: 0.6854 - acc: 0.5938
 320/4849 [>.............................] - ETA: 6:44 - loss: 0.6771 - acc: 0.6000
 384/4849 [=>............................] - ETA: 6:35 - loss: 0.6752 - acc: 0.5859
 448/4849 [=>............................] - ETA: 6:22 - loss: 0.6777 - acc: 0.5781
 512/4849 [==>...........................] - ETA: 6:14 - loss: 0.6788 - acc: 0.5684
 576/4849 [==>...........................] - ETA: 6:12 - loss: 0.6812 - acc: 0.5642
 640/4849 [==>...........................] - ETA: 6:08 - loss: 0.6782 - acc: 0.5687
 704/4849 [===>..........................] - ETA: 6:06 - loss: 0.6774 - acc: 0.5682
 768/4849 [===>..........................] - ETA: 5:59 - loss: 0.6795 - acc: 0.5703
 832/4849 [====>.........................] - ETA: 5:55 - loss: 0.6819 - acc: 0.5697
 896/4849 [====>.........................] - ETA: 5:48 - loss: 0.6821 - acc: 0.5692
 960/4849 [====>.........................] - ETA: 5:43 - loss: 0.6785 - acc: 0.5740
1024/4849 [=====>........................] - ETA: 5:39 - loss: 0.6767 - acc: 0.5762
1088/4849 [=====>........................] - ETA: 5:34 - loss: 0.6792 - acc: 0.5699
1152/4849 [======>.......................] - ETA: 5:28 - loss: 0.6762 - acc: 0.5764
1216/4849 [======>.......................] - ETA: 5:22 - loss: 0.6770 - acc: 0.5740
1280/4849 [======>.......................] - ETA: 5:18 - loss: 0.6754 - acc: 0.5781
1344/4849 [=======>......................] - ETA: 5:13 - loss: 0.6756 - acc: 0.5804
1408/4849 [=======>......................] - ETA: 5:07 - loss: 0.6765 - acc: 0.5803
1472/4849 [========>.....................] - ETA: 5:01 - loss: 0.6767 - acc: 0.5788
1536/4849 [========>.....................] - ETA: 4:56 - loss: 0.6770 - acc: 0.5768
1600/4849 [========>.....................] - ETA: 4:51 - loss: 0.6755 - acc: 0.5787
1664/4849 [=========>....................] - ETA: 4:44 - loss: 0.6805 - acc: 0.5733
1728/4849 [=========>....................] - ETA: 4:38 - loss: 0.6803 - acc: 0.5718
1792/4849 [==========>...................] - ETA: 4:33 - loss: 0.6797 - acc: 0.5742
1856/4849 [==========>...................] - ETA: 4:26 - loss: 0.6794 - acc: 0.5760
1920/4849 [==========>...................] - ETA: 4:20 - loss: 0.6795 - acc: 0.5781
1984/4849 [===========>..................] - ETA: 4:15 - loss: 0.6805 - acc: 0.5756
2048/4849 [===========>..................] - ETA: 4:10 - loss: 0.6825 - acc: 0.5752
2112/4849 [============>.................] - ETA: 4:04 - loss: 0.6829 - acc: 0.5748
2176/4849 [============>.................] - ETA: 3:59 - loss: 0.6826 - acc: 0.5754
2240/4849 [============>.................] - ETA: 3:53 - loss: 0.6827 - acc: 0.5737
2304/4849 [=============>................] - ETA: 3:47 - loss: 0.6826 - acc: 0.5738
2368/4849 [=============>................] - ETA: 3:41 - loss: 0.6814 - acc: 0.5743
2432/4849 [==============>...............] - ETA: 3:35 - loss: 0.6828 - acc: 0.5732
2496/4849 [==============>...............] - ETA: 3:30 - loss: 0.6833 - acc: 0.5713
2560/4849 [==============>...............] - ETA: 3:25 - loss: 0.6837 - acc: 0.5703
2624/4849 [===============>..............] - ETA: 3:19 - loss: 0.6820 - acc: 0.5736
2688/4849 [===============>..............] - ETA: 3:13 - loss: 0.6819 - acc: 0.5744
2752/4849 [================>.............] - ETA: 3:08 - loss: 0.6820 - acc: 0.5734
2816/4849 [================>.............] - ETA: 3:02 - loss: 0.6820 - acc: 0.5728
2880/4849 [================>.............] - ETA: 2:56 - loss: 0.6828 - acc: 0.5726
2944/4849 [=================>............] - ETA: 2:50 - loss: 0.6831 - acc: 0.5717
3008/4849 [=================>............] - ETA: 2:45 - loss: 0.6835 - acc: 0.5688
3072/4849 [==================>...........] - ETA: 2:39 - loss: 0.6828 - acc: 0.5706
3136/4849 [==================>...........] - ETA: 2:33 - loss: 0.6830 - acc: 0.5702
3200/4849 [==================>...........] - ETA: 2:27 - loss: 0.6828 - acc: 0.5713
3264/4849 [===================>..........] - ETA: 2:21 - loss: 0.6832 - acc: 0.5714
3328/4849 [===================>..........] - ETA: 2:16 - loss: 0.6832 - acc: 0.5712
3392/4849 [===================>..........] - ETA: 2:10 - loss: 0.6829 - acc: 0.5725
3456/4849 [====================>.........] - ETA: 2:04 - loss: 0.6825 - acc: 0.5744
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6827 - acc: 0.5736
3584/4849 [=====================>........] - ETA: 1:53 - loss: 0.6821 - acc: 0.5759
3648/4849 [=====================>........] - ETA: 1:47 - loss: 0.6823 - acc: 0.5757
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6829 - acc: 0.5744
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6833 - acc: 0.5728
3840/4849 [======================>.......] - ETA: 1:30 - loss: 0.6833 - acc: 0.5729
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6839 - acc: 0.5725
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6842 - acc: 0.5731
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6846 - acc: 0.5722
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6848 - acc: 0.5710
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6851 - acc: 0.5702
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6846 - acc: 0.5717 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6848 - acc: 0.5716
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6857 - acc: 0.5689
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6852 - acc: 0.5704
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6844 - acc: 0.5721
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6847 - acc: 0.5713
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6849 - acc: 0.5712
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6857 - acc: 0.5689
4736/4849 [============================>.] - ETA: 10s - loss: 0.6852 - acc: 0.5697
4800/4849 [============================>.] - ETA: 4s - loss: 0.6849 - acc: 0.5698 
4849/4849 [==============================] - 449s 93ms/step - loss: 0.6852 - acc: 0.5686 - val_loss: 0.6861 - val_acc: 0.5417

Epoch 00003: val_acc did not improve from 0.56215
Epoch 4/10

  64/4849 [..............................] - ETA: 7:32 - loss: 0.6902 - acc: 0.5469
 128/4849 [..............................] - ETA: 7:07 - loss: 0.6889 - acc: 0.5625
 192/4849 [>.............................] - ETA: 7:15 - loss: 0.6857 - acc: 0.5990
 256/4849 [>.............................] - ETA: 7:05 - loss: 0.6857 - acc: 0.5859
 320/4849 [>.............................] - ETA: 7:02 - loss: 0.6928 - acc: 0.5625
 384/4849 [=>............................] - ETA: 6:56 - loss: 0.6835 - acc: 0.5703
 448/4849 [=>............................] - ETA: 6:44 - loss: 0.6773 - acc: 0.5848
 512/4849 [==>...........................] - ETA: 6:34 - loss: 0.6808 - acc: 0.5801
 576/4849 [==>...........................] - ETA: 6:28 - loss: 0.6813 - acc: 0.5764
 640/4849 [==>...........................] - ETA: 6:22 - loss: 0.6730 - acc: 0.5891
 704/4849 [===>..........................] - ETA: 6:16 - loss: 0.6721 - acc: 0.5938
 768/4849 [===>..........................] - ETA: 6:10 - loss: 0.6713 - acc: 0.5911
 832/4849 [====>.........................] - ETA: 6:03 - loss: 0.6742 - acc: 0.5901
 896/4849 [====>.........................] - ETA: 5:56 - loss: 0.6743 - acc: 0.5882
 960/4849 [====>.........................] - ETA: 5:50 - loss: 0.6777 - acc: 0.5802
1024/4849 [=====>........................] - ETA: 5:45 - loss: 0.6757 - acc: 0.5850
1088/4849 [=====>........................] - ETA: 5:39 - loss: 0.6766 - acc: 0.5846
1152/4849 [======>.......................] - ETA: 5:33 - loss: 0.6788 - acc: 0.5825
1216/4849 [======>.......................] - ETA: 5:26 - loss: 0.6783 - acc: 0.5831
1280/4849 [======>.......................] - ETA: 5:19 - loss: 0.6799 - acc: 0.5773
1344/4849 [=======>......................] - ETA: 5:12 - loss: 0.6805 - acc: 0.5744
1408/4849 [=======>......................] - ETA: 5:08 - loss: 0.6793 - acc: 0.5746
1472/4849 [========>.....................] - ETA: 5:01 - loss: 0.6801 - acc: 0.5727
1536/4849 [========>.....................] - ETA: 4:56 - loss: 0.6811 - acc: 0.5716
1600/4849 [========>.....................] - ETA: 4:50 - loss: 0.6808 - acc: 0.5719
1664/4849 [=========>....................] - ETA: 4:45 - loss: 0.6810 - acc: 0.5715
1728/4849 [=========>....................] - ETA: 4:39 - loss: 0.6806 - acc: 0.5741
1792/4849 [==========>...................] - ETA: 4:34 - loss: 0.6795 - acc: 0.5770
1856/4849 [==========>...................] - ETA: 4:28 - loss: 0.6804 - acc: 0.5738
1920/4849 [==========>...................] - ETA: 4:21 - loss: 0.6812 - acc: 0.5724
1984/4849 [===========>..................] - ETA: 4:16 - loss: 0.6810 - acc: 0.5751
2048/4849 [===========>..................] - ETA: 4:10 - loss: 0.6799 - acc: 0.5762
2112/4849 [============>.................] - ETA: 4:04 - loss: 0.6804 - acc: 0.5748
2176/4849 [============>.................] - ETA: 3:58 - loss: 0.6796 - acc: 0.5767
2240/4849 [============>.................] - ETA: 3:52 - loss: 0.6794 - acc: 0.5768
2304/4849 [=============>................] - ETA: 3:46 - loss: 0.6791 - acc: 0.5773
2368/4849 [=============>................] - ETA: 3:41 - loss: 0.6786 - acc: 0.5790
2432/4849 [==============>...............] - ETA: 3:35 - loss: 0.6780 - acc: 0.5794
2496/4849 [==============>...............] - ETA: 3:29 - loss: 0.6783 - acc: 0.5797
2560/4849 [==============>...............] - ETA: 3:23 - loss: 0.6789 - acc: 0.5793
2624/4849 [===============>..............] - ETA: 3:18 - loss: 0.6783 - acc: 0.5823
2688/4849 [===============>..............] - ETA: 3:12 - loss: 0.6786 - acc: 0.5818
2752/4849 [================>.............] - ETA: 3:06 - loss: 0.6795 - acc: 0.5796
2816/4849 [================>.............] - ETA: 3:00 - loss: 0.6800 - acc: 0.5792
2880/4849 [================>.............] - ETA: 2:55 - loss: 0.6804 - acc: 0.5781
2944/4849 [=================>............] - ETA: 2:49 - loss: 0.6810 - acc: 0.5774
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6809 - acc: 0.5775
3072/4849 [==================>...........] - ETA: 2:37 - loss: 0.6811 - acc: 0.5771
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6809 - acc: 0.5762
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6812 - acc: 0.5753
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6810 - acc: 0.5754
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6813 - acc: 0.5748
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6815 - acc: 0.5725
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6813 - acc: 0.5732
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6809 - acc: 0.5727
3584/4849 [=====================>........] - ETA: 1:52 - loss: 0.6804 - acc: 0.5742
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6802 - acc: 0.5737
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6807 - acc: 0.5727
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6803 - acc: 0.5744
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6800 - acc: 0.5753
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6790 - acc: 0.5774
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6793 - acc: 0.5771
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6796 - acc: 0.5774
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6801 - acc: 0.5764
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6798 - acc: 0.5760
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6798 - acc: 0.5765 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6799 - acc: 0.5772
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6799 - acc: 0.5765
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6794 - acc: 0.5774
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6795 - acc: 0.5777
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6792 - acc: 0.5786
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6799 - acc: 0.5770
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6800 - acc: 0.5762
4736/4849 [============================>.] - ETA: 10s - loss: 0.6795 - acc: 0.5773
4800/4849 [============================>.] - ETA: 4s - loss: 0.6802 - acc: 0.5752 
4849/4849 [==============================] - 448s 92ms/step - loss: 0.6802 - acc: 0.5754 - val_loss: 0.7192 - val_acc: 0.5046

Epoch 00004: val_acc did not improve from 0.56215
Epoch 5/10

  64/4849 [..............................] - ETA: 6:28 - loss: 0.6762 - acc: 0.5625
 128/4849 [..............................] - ETA: 6:48 - loss: 0.6739 - acc: 0.5469
 192/4849 [>.............................] - ETA: 6:41 - loss: 0.6867 - acc: 0.5104
 256/4849 [>.............................] - ETA: 6:31 - loss: 0.6752 - acc: 0.5469
 320/4849 [>.............................] - ETA: 6:30 - loss: 0.6691 - acc: 0.5687
 384/4849 [=>............................] - ETA: 6:26 - loss: 0.6743 - acc: 0.5495
 448/4849 [=>............................] - ETA: 6:22 - loss: 0.6671 - acc: 0.5670
 512/4849 [==>...........................] - ETA: 6:19 - loss: 0.6694 - acc: 0.5664
 576/4849 [==>...........................] - ETA: 6:15 - loss: 0.6714 - acc: 0.5642
 640/4849 [==>...........................] - ETA: 6:10 - loss: 0.6712 - acc: 0.5687
 704/4849 [===>..........................] - ETA: 6:04 - loss: 0.6709 - acc: 0.5696
 768/4849 [===>..........................] - ETA: 5:56 - loss: 0.6706 - acc: 0.5729
 832/4849 [====>.........................] - ETA: 5:50 - loss: 0.6732 - acc: 0.5649
 896/4849 [====>.........................] - ETA: 5:46 - loss: 0.6756 - acc: 0.5647
 960/4849 [====>.........................] - ETA: 5:39 - loss: 0.6790 - acc: 0.5604
1024/4849 [=====>........................] - ETA: 5:33 - loss: 0.6775 - acc: 0.5615
1088/4849 [=====>........................] - ETA: 5:30 - loss: 0.6770 - acc: 0.5634
1152/4849 [======>.......................] - ETA: 5:25 - loss: 0.6757 - acc: 0.5660
1216/4849 [======>.......................] - ETA: 5:17 - loss: 0.6753 - acc: 0.5674
1280/4849 [======>.......................] - ETA: 5:07 - loss: 0.6765 - acc: 0.5664
1344/4849 [=======>......................] - ETA: 4:59 - loss: 0.6756 - acc: 0.5685
1408/4849 [=======>......................] - ETA: 4:51 - loss: 0.6754 - acc: 0.5703
1472/4849 [========>.....................] - ETA: 4:44 - loss: 0.6739 - acc: 0.5740
1536/4849 [========>.....................] - ETA: 4:38 - loss: 0.6732 - acc: 0.5762
1600/4849 [========>.....................] - ETA: 4:34 - loss: 0.6728 - acc: 0.5787
1664/4849 [=========>....................] - ETA: 4:28 - loss: 0.6716 - acc: 0.5817
1728/4849 [=========>....................] - ETA: 4:21 - loss: 0.6714 - acc: 0.5833
1792/4849 [==========>...................] - ETA: 4:15 - loss: 0.6708 - acc: 0.5843
1856/4849 [==========>...................] - ETA: 4:08 - loss: 0.6716 - acc: 0.5830
1920/4849 [==========>...................] - ETA: 4:01 - loss: 0.6717 - acc: 0.5823
1984/4849 [===========>..................] - ETA: 3:55 - loss: 0.6722 - acc: 0.5796
2048/4849 [===========>..................] - ETA: 3:49 - loss: 0.6705 - acc: 0.5830
2112/4849 [============>.................] - ETA: 3:43 - loss: 0.6707 - acc: 0.5824
2176/4849 [============>.................] - ETA: 3:37 - loss: 0.6689 - acc: 0.5873
2240/4849 [============>.................] - ETA: 3:31 - loss: 0.6679 - acc: 0.5897
2304/4849 [=============>................] - ETA: 3:25 - loss: 0.6678 - acc: 0.5920
2368/4849 [=============>................] - ETA: 3:20 - loss: 0.6677 - acc: 0.5921
2432/4849 [==============>...............] - ETA: 3:15 - loss: 0.6679 - acc: 0.5933
2496/4849 [==============>...............] - ETA: 3:10 - loss: 0.6679 - acc: 0.5933
2560/4849 [==============>...............] - ETA: 3:04 - loss: 0.6670 - acc: 0.5945
2624/4849 [===============>..............] - ETA: 2:58 - loss: 0.6677 - acc: 0.5930
2688/4849 [===============>..............] - ETA: 2:53 - loss: 0.6667 - acc: 0.5938
2752/4849 [================>.............] - ETA: 2:48 - loss: 0.6664 - acc: 0.5963
2816/4849 [================>.............] - ETA: 2:43 - loss: 0.6671 - acc: 0.5955
2880/4849 [================>.............] - ETA: 2:37 - loss: 0.6670 - acc: 0.5965
2944/4849 [=================>............] - ETA: 2:33 - loss: 0.6672 - acc: 0.5965
3008/4849 [=================>............] - ETA: 2:28 - loss: 0.6694 - acc: 0.5931
3072/4849 [==================>...........] - ETA: 2:22 - loss: 0.6688 - acc: 0.5941
3136/4849 [==================>...........] - ETA: 2:17 - loss: 0.6688 - acc: 0.5938
3200/4849 [==================>...........] - ETA: 2:12 - loss: 0.6685 - acc: 0.5947
3264/4849 [===================>..........] - ETA: 2:07 - loss: 0.6684 - acc: 0.5950
3328/4849 [===================>..........] - ETA: 2:01 - loss: 0.6684 - acc: 0.5953
3392/4849 [===================>..........] - ETA: 1:56 - loss: 0.6678 - acc: 0.5955
3456/4849 [====================>.........] - ETA: 1:51 - loss: 0.6687 - acc: 0.5932
3520/4849 [====================>.........] - ETA: 1:46 - loss: 0.6697 - acc: 0.5915
3584/4849 [=====================>........] - ETA: 1:40 - loss: 0.6691 - acc: 0.5926
3648/4849 [=====================>........] - ETA: 1:35 - loss: 0.6690 - acc: 0.5938
3712/4849 [=====================>........] - ETA: 1:30 - loss: 0.6682 - acc: 0.5951
3776/4849 [======================>.......] - ETA: 1:25 - loss: 0.6671 - acc: 0.5969
3840/4849 [======================>.......] - ETA: 1:20 - loss: 0.6670 - acc: 0.5974
3904/4849 [=======================>......] - ETA: 1:15 - loss: 0.6672 - acc: 0.5966
3968/4849 [=======================>......] - ETA: 1:10 - loss: 0.6674 - acc: 0.5960
4032/4849 [=======================>......] - ETA: 1:04 - loss: 0.6668 - acc: 0.5967
4096/4849 [========================>.....] - ETA: 59s - loss: 0.6673 - acc: 0.5957 
4160/4849 [========================>.....] - ETA: 54s - loss: 0.6676 - acc: 0.5947
4224/4849 [=========================>....] - ETA: 49s - loss: 0.6682 - acc: 0.5938
4288/4849 [=========================>....] - ETA: 44s - loss: 0.6684 - acc: 0.5931
4352/4849 [=========================>....] - ETA: 39s - loss: 0.6687 - acc: 0.5921
4416/4849 [==========================>...] - ETA: 34s - loss: 0.6675 - acc: 0.5940
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6685 - acc: 0.5913
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6697 - acc: 0.5891
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6698 - acc: 0.5890
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6697 - acc: 0.5895
4736/4849 [============================>.] - ETA: 8s - loss: 0.6699 - acc: 0.5889 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6694 - acc: 0.5902
4849/4849 [==============================] - 393s 81ms/step - loss: 0.6690 - acc: 0.5908 - val_loss: 0.7112 - val_acc: 0.5269

Epoch 00005: val_acc did not improve from 0.56215
Epoch 6/10

  64/4849 [..............................] - ETA: 6:00 - loss: 0.6991 - acc: 0.5000
 128/4849 [..............................] - ETA: 5:40 - loss: 0.6806 - acc: 0.5625
 192/4849 [>.............................] - ETA: 5:35 - loss: 0.6819 - acc: 0.5677
 256/4849 [>.............................] - ETA: 5:41 - loss: 0.6691 - acc: 0.6172
 320/4849 [>.............................] - ETA: 5:38 - loss: 0.6718 - acc: 0.6062
 384/4849 [=>............................] - ETA: 5:28 - loss: 0.6665 - acc: 0.6172
 448/4849 [=>............................] - ETA: 5:24 - loss: 0.6690 - acc: 0.6138
 512/4849 [==>...........................] - ETA: 5:22 - loss: 0.6651 - acc: 0.6172
 576/4849 [==>...........................] - ETA: 5:14 - loss: 0.6677 - acc: 0.6094
 640/4849 [==>...........................] - ETA: 5:10 - loss: 0.6718 - acc: 0.6062
 704/4849 [===>..........................] - ETA: 5:06 - loss: 0.6678 - acc: 0.6065
 768/4849 [===>..........................] - ETA: 5:01 - loss: 0.6627 - acc: 0.6120
 832/4849 [====>.........................] - ETA: 4:53 - loss: 0.6636 - acc: 0.6094
 896/4849 [====>.........................] - ETA: 4:49 - loss: 0.6619 - acc: 0.6116
 960/4849 [====>.........................] - ETA: 4:45 - loss: 0.6617 - acc: 0.6104
1024/4849 [=====>........................] - ETA: 4:39 - loss: 0.6595 - acc: 0.6162
1088/4849 [=====>........................] - ETA: 4:37 - loss: 0.6606 - acc: 0.6149
1152/4849 [======>.......................] - ETA: 4:33 - loss: 0.6600 - acc: 0.6111
1216/4849 [======>.......................] - ETA: 4:26 - loss: 0.6626 - acc: 0.6118
1280/4849 [======>.......................] - ETA: 4:21 - loss: 0.6605 - acc: 0.6133
1344/4849 [=======>......................] - ETA: 4:18 - loss: 0.6604 - acc: 0.6153
1408/4849 [=======>......................] - ETA: 4:13 - loss: 0.6640 - acc: 0.6129
1472/4849 [========>.....................] - ETA: 4:08 - loss: 0.6662 - acc: 0.6128
1536/4849 [========>.....................] - ETA: 4:04 - loss: 0.6676 - acc: 0.6120
1600/4849 [========>.....................] - ETA: 3:59 - loss: 0.6688 - acc: 0.6088
1664/4849 [=========>....................] - ETA: 3:54 - loss: 0.6723 - acc: 0.6028
1728/4849 [=========>....................] - ETA: 3:49 - loss: 0.6706 - acc: 0.6042
1792/4849 [==========>...................] - ETA: 3:44 - loss: 0.6717 - acc: 0.6027
1856/4849 [==========>...................] - ETA: 3:41 - loss: 0.6722 - acc: 0.5991
1920/4849 [==========>...................] - ETA: 3:37 - loss: 0.6718 - acc: 0.6000
1984/4849 [===========>..................] - ETA: 3:32 - loss: 0.6710 - acc: 0.6013
2048/4849 [===========>..................] - ETA: 3:28 - loss: 0.6715 - acc: 0.5991
2112/4849 [============>.................] - ETA: 3:23 - loss: 0.6717 - acc: 0.5994
2176/4849 [============>.................] - ETA: 3:18 - loss: 0.6730 - acc: 0.5960
2240/4849 [============>.................] - ETA: 3:14 - loss: 0.6729 - acc: 0.5973
2304/4849 [=============>................] - ETA: 3:09 - loss: 0.6738 - acc: 0.5946
2368/4849 [=============>................] - ETA: 3:03 - loss: 0.6747 - acc: 0.5916
2432/4849 [==============>...............] - ETA: 2:57 - loss: 0.6742 - acc: 0.5909
2496/4849 [==============>...............] - ETA: 2:51 - loss: 0.6729 - acc: 0.5917
2560/4849 [==============>...............] - ETA: 2:46 - loss: 0.6736 - acc: 0.5898
2624/4849 [===============>..............] - ETA: 2:41 - loss: 0.6736 - acc: 0.5888
2688/4849 [===============>..............] - ETA: 2:35 - loss: 0.6737 - acc: 0.5897
2752/4849 [================>.............] - ETA: 2:30 - loss: 0.6729 - acc: 0.5901
2816/4849 [================>.............] - ETA: 2:24 - loss: 0.6737 - acc: 0.5888
2880/4849 [================>.............] - ETA: 2:19 - loss: 0.6737 - acc: 0.5892
2944/4849 [=================>............] - ETA: 2:14 - loss: 0.6737 - acc: 0.5887
3008/4849 [=================>............] - ETA: 2:09 - loss: 0.6731 - acc: 0.5888
3072/4849 [==================>...........] - ETA: 2:04 - loss: 0.6734 - acc: 0.5876
3136/4849 [==================>...........] - ETA: 1:59 - loss: 0.6725 - acc: 0.5880
3200/4849 [==================>...........] - ETA: 1:55 - loss: 0.6725 - acc: 0.5891
3264/4849 [===================>..........] - ETA: 1:50 - loss: 0.6710 - acc: 0.5925
3328/4849 [===================>..........] - ETA: 1:45 - loss: 0.6710 - acc: 0.5922
3392/4849 [===================>..........] - ETA: 1:40 - loss: 0.6725 - acc: 0.5896
3456/4849 [====================>.........] - ETA: 1:36 - loss: 0.6727 - acc: 0.5888
3520/4849 [====================>.........] - ETA: 1:31 - loss: 0.6726 - acc: 0.5895
3584/4849 [=====================>........] - ETA: 1:27 - loss: 0.6730 - acc: 0.5890
3648/4849 [=====================>........] - ETA: 1:23 - loss: 0.6729 - acc: 0.5891
3712/4849 [=====================>........] - ETA: 1:18 - loss: 0.6728 - acc: 0.5892
3776/4849 [======================>.......] - ETA: 1:13 - loss: 0.6735 - acc: 0.5885
3840/4849 [======================>.......] - ETA: 1:09 - loss: 0.6739 - acc: 0.5875
3904/4849 [=======================>......] - ETA: 1:04 - loss: 0.6735 - acc: 0.5881
3968/4849 [=======================>......] - ETA: 1:00 - loss: 0.6730 - acc: 0.5882
4032/4849 [=======================>......] - ETA: 55s - loss: 0.6724 - acc: 0.5890 
4096/4849 [========================>.....] - ETA: 51s - loss: 0.6728 - acc: 0.5879
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6728 - acc: 0.5877
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6724 - acc: 0.5888
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6724 - acc: 0.5889
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6723 - acc: 0.5898
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6717 - acc: 0.5908
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6718 - acc: 0.5908
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6715 - acc: 0.5911
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6721 - acc: 0.5901
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6721 - acc: 0.5905
4736/4849 [============================>.] - ETA: 7s - loss: 0.6719 - acc: 0.5908 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6724 - acc: 0.5902
4849/4849 [==============================] - 336s 69ms/step - loss: 0.6728 - acc: 0.5904 - val_loss: 0.6924 - val_acc: 0.5603

Epoch 00006: val_acc did not improve from 0.56215
Epoch 7/10

  64/4849 [..............................] - ETA: 4:58 - loss: 0.7256 - acc: 0.4844
 128/4849 [..............................] - ETA: 5:01 - loss: 0.6770 - acc: 0.5781
 192/4849 [>.............................] - ETA: 4:52 - loss: 0.6889 - acc: 0.5521
 256/4849 [>.............................] - ETA: 4:38 - loss: 0.6809 - acc: 0.5664
 320/4849 [>.............................] - ETA: 4:30 - loss: 0.6834 - acc: 0.5656
 384/4849 [=>............................] - ETA: 4:25 - loss: 0.6710 - acc: 0.5833
 448/4849 [=>............................] - ETA: 4:22 - loss: 0.6783 - acc: 0.5759
 512/4849 [==>...........................] - ETA: 4:18 - loss: 0.6818 - acc: 0.5664
 576/4849 [==>...........................] - ETA: 4:16 - loss: 0.6791 - acc: 0.5694
 640/4849 [==>...........................] - ETA: 4:12 - loss: 0.6737 - acc: 0.5750
 704/4849 [===>..........................] - ETA: 4:07 - loss: 0.6751 - acc: 0.5739
 768/4849 [===>..........................] - ETA: 4:03 - loss: 0.6718 - acc: 0.5820
 832/4849 [====>.........................] - ETA: 3:58 - loss: 0.6701 - acc: 0.5865
 896/4849 [====>.........................] - ETA: 3:53 - loss: 0.6719 - acc: 0.5859
 960/4849 [====>.........................] - ETA: 3:50 - loss: 0.6718 - acc: 0.5885
1024/4849 [=====>........................] - ETA: 3:46 - loss: 0.6737 - acc: 0.5869
1088/4849 [=====>........................] - ETA: 3:44 - loss: 0.6745 - acc: 0.5892
1152/4849 [======>.......................] - ETA: 3:41 - loss: 0.6731 - acc: 0.5929
1216/4849 [======>.......................] - ETA: 3:38 - loss: 0.6733 - acc: 0.5921
1280/4849 [======>.......................] - ETA: 3:34 - loss: 0.6736 - acc: 0.5930
1344/4849 [=======>......................] - ETA: 3:30 - loss: 0.6739 - acc: 0.5900
1408/4849 [=======>......................] - ETA: 3:26 - loss: 0.6751 - acc: 0.5888
1472/4849 [========>.....................] - ETA: 3:21 - loss: 0.6751 - acc: 0.5890
1536/4849 [========>.....................] - ETA: 3:18 - loss: 0.6755 - acc: 0.5892
1600/4849 [========>.....................] - ETA: 3:14 - loss: 0.6759 - acc: 0.5869
1664/4849 [=========>....................] - ETA: 3:10 - loss: 0.6760 - acc: 0.5841
1728/4849 [=========>....................] - ETA: 3:06 - loss: 0.6733 - acc: 0.5891
1792/4849 [==========>...................] - ETA: 3:01 - loss: 0.6723 - acc: 0.5893
1856/4849 [==========>...................] - ETA: 2:58 - loss: 0.6725 - acc: 0.5884
1920/4849 [==========>...................] - ETA: 2:54 - loss: 0.6712 - acc: 0.5906
1984/4849 [===========>..................] - ETA: 2:50 - loss: 0.6715 - acc: 0.5897
2048/4849 [===========>..................] - ETA: 2:46 - loss: 0.6728 - acc: 0.5869
2112/4849 [============>.................] - ETA: 2:43 - loss: 0.6716 - acc: 0.5871
2176/4849 [============>.................] - ETA: 2:39 - loss: 0.6717 - acc: 0.5878
2240/4849 [============>.................] - ETA: 2:35 - loss: 0.6712 - acc: 0.5879
2304/4849 [=============>................] - ETA: 2:32 - loss: 0.6704 - acc: 0.5903
2368/4849 [=============>................] - ETA: 2:28 - loss: 0.6711 - acc: 0.5870
2432/4849 [==============>...............] - ETA: 2:24 - loss: 0.6704 - acc: 0.5880
2496/4849 [==============>...............] - ETA: 2:20 - loss: 0.6709 - acc: 0.5873
2560/4849 [==============>...............] - ETA: 2:16 - loss: 0.6706 - acc: 0.5883
2624/4849 [===============>..............] - ETA: 2:12 - loss: 0.6709 - acc: 0.5888
2688/4849 [===============>..............] - ETA: 2:08 - loss: 0.6718 - acc: 0.5859
2752/4849 [================>.............] - ETA: 2:04 - loss: 0.6712 - acc: 0.5876
2816/4849 [================>.............] - ETA: 2:01 - loss: 0.6705 - acc: 0.5881
2880/4849 [================>.............] - ETA: 1:57 - loss: 0.6710 - acc: 0.5854
2944/4849 [=================>............] - ETA: 1:53 - loss: 0.6709 - acc: 0.5863
3008/4849 [=================>............] - ETA: 1:49 - loss: 0.6713 - acc: 0.5854
3072/4849 [==================>...........] - ETA: 1:46 - loss: 0.6730 - acc: 0.5837
3136/4849 [==================>...........] - ETA: 1:42 - loss: 0.6726 - acc: 0.5848
3200/4849 [==================>...........] - ETA: 1:38 - loss: 0.6730 - acc: 0.5841
3264/4849 [===================>..........] - ETA: 1:34 - loss: 0.6725 - acc: 0.5843
3328/4849 [===================>..........] - ETA: 1:30 - loss: 0.6718 - acc: 0.5865
3392/4849 [===================>..........] - ETA: 1:26 - loss: 0.6719 - acc: 0.5873
3456/4849 [====================>.........] - ETA: 1:23 - loss: 0.6719 - acc: 0.5874
3520/4849 [====================>.........] - ETA: 1:18 - loss: 0.6722 - acc: 0.5861
3584/4849 [=====================>........] - ETA: 1:15 - loss: 0.6726 - acc: 0.5851
3648/4849 [=====================>........] - ETA: 1:11 - loss: 0.6725 - acc: 0.5853
3712/4849 [=====================>........] - ETA: 1:07 - loss: 0.6717 - acc: 0.5859
3776/4849 [======================>.......] - ETA: 1:04 - loss: 0.6713 - acc: 0.5863
3840/4849 [======================>.......] - ETA: 1:00 - loss: 0.6709 - acc: 0.5872
3904/4849 [=======================>......] - ETA: 56s - loss: 0.6705 - acc: 0.5876 
3968/4849 [=======================>......] - ETA: 52s - loss: 0.6703 - acc: 0.5880
4032/4849 [=======================>......] - ETA: 48s - loss: 0.6708 - acc: 0.5873
4096/4849 [========================>.....] - ETA: 44s - loss: 0.6707 - acc: 0.5874
4160/4849 [========================>.....] - ETA: 40s - loss: 0.6697 - acc: 0.5897
4224/4849 [=========================>....] - ETA: 37s - loss: 0.6698 - acc: 0.5897
4288/4849 [=========================>....] - ETA: 33s - loss: 0.6694 - acc: 0.5914
4352/4849 [=========================>....] - ETA: 29s - loss: 0.6683 - acc: 0.5933
4416/4849 [==========================>...] - ETA: 25s - loss: 0.6682 - acc: 0.5938
4480/4849 [==========================>...] - ETA: 21s - loss: 0.6676 - acc: 0.5955
4544/4849 [===========================>..] - ETA: 18s - loss: 0.6672 - acc: 0.5960
4608/4849 [===========================>..] - ETA: 14s - loss: 0.6676 - acc: 0.5951
4672/4849 [===========================>..] - ETA: 10s - loss: 0.6665 - acc: 0.5965
4736/4849 [============================>.] - ETA: 6s - loss: 0.6663 - acc: 0.5959 
4800/4849 [============================>.] - ETA: 2s - loss: 0.6668 - acc: 0.5946
4849/4849 [==============================] - 300s 62ms/step - loss: 0.6660 - acc: 0.5958 - val_loss: 0.6998 - val_acc: 0.5325

Epoch 00007: val_acc did not improve from 0.56215
Epoch 8/10

  64/4849 [..............................] - ETA: 5:30 - loss: 0.6260 - acc: 0.6562
 128/4849 [..............................] - ETA: 5:09 - loss: 0.6121 - acc: 0.6797
 192/4849 [>.............................] - ETA: 4:45 - loss: 0.6375 - acc: 0.6458
 256/4849 [>.............................] - ETA: 4:33 - loss: 0.6460 - acc: 0.6445
 320/4849 [>.............................] - ETA: 4:30 - loss: 0.6382 - acc: 0.6687
 384/4849 [=>............................] - ETA: 4:31 - loss: 0.6393 - acc: 0.6693
 448/4849 [=>............................] - ETA: 4:23 - loss: 0.6407 - acc: 0.6652
 512/4849 [==>...........................] - ETA: 4:21 - loss: 0.6443 - acc: 0.6543
 576/4849 [==>...........................] - ETA: 4:15 - loss: 0.6555 - acc: 0.6389
 640/4849 [==>...........................] - ETA: 4:12 - loss: 0.6594 - acc: 0.6359
 704/4849 [===>..........................] - ETA: 4:05 - loss: 0.6633 - acc: 0.6349
 768/4849 [===>..........................] - ETA: 4:00 - loss: 0.6615 - acc: 0.6341
 832/4849 [====>.........................] - ETA: 3:59 - loss: 0.6637 - acc: 0.6310
 896/4849 [====>.........................] - ETA: 3:54 - loss: 0.6649 - acc: 0.6250
 960/4849 [====>.........................] - ETA: 3:49 - loss: 0.6621 - acc: 0.6281
1024/4849 [=====>........................] - ETA: 3:45 - loss: 0.6603 - acc: 0.6289
1088/4849 [=====>........................] - ETA: 3:44 - loss: 0.6604 - acc: 0.6278
1152/4849 [======>.......................] - ETA: 3:42 - loss: 0.6572 - acc: 0.6319
1216/4849 [======>.......................] - ETA: 3:39 - loss: 0.6553 - acc: 0.6349
1280/4849 [======>.......................] - ETA: 3:35 - loss: 0.6578 - acc: 0.6305
1344/4849 [=======>......................] - ETA: 3:30 - loss: 0.6612 - acc: 0.6265
1408/4849 [=======>......................] - ETA: 3:26 - loss: 0.6600 - acc: 0.6264
1472/4849 [========>.....................] - ETA: 3:22 - loss: 0.6601 - acc: 0.6250
1536/4849 [========>.....................] - ETA: 3:19 - loss: 0.6625 - acc: 0.6211
1600/4849 [========>.....................] - ETA: 3:15 - loss: 0.6643 - acc: 0.6181
1664/4849 [=========>....................] - ETA: 3:12 - loss: 0.6651 - acc: 0.6184
1728/4849 [=========>....................] - ETA: 3:08 - loss: 0.6659 - acc: 0.6169
1792/4849 [==========>...................] - ETA: 3:04 - loss: 0.6673 - acc: 0.6138
1856/4849 [==========>...................] - ETA: 3:00 - loss: 0.6683 - acc: 0.6121
1920/4849 [==========>...................] - ETA: 2:56 - loss: 0.6688 - acc: 0.6089
1984/4849 [===========>..................] - ETA: 2:52 - loss: 0.6674 - acc: 0.6094
2048/4849 [===========>..................] - ETA: 2:47 - loss: 0.6658 - acc: 0.6123
2112/4849 [============>.................] - ETA: 2:44 - loss: 0.6639 - acc: 0.6132
2176/4849 [============>.................] - ETA: 2:40 - loss: 0.6623 - acc: 0.6149
2240/4849 [============>.................] - ETA: 2:36 - loss: 0.6634 - acc: 0.6138
2304/4849 [=============>................] - ETA: 2:32 - loss: 0.6646 - acc: 0.6107
2368/4849 [=============>................] - ETA: 2:28 - loss: 0.6643 - acc: 0.6123
2432/4849 [==============>...............] - ETA: 2:24 - loss: 0.6668 - acc: 0.6077
2496/4849 [==============>...............] - ETA: 2:21 - loss: 0.6663 - acc: 0.6078
2560/4849 [==============>...............] - ETA: 2:17 - loss: 0.6669 - acc: 0.6055
2624/4849 [===============>..............] - ETA: 2:13 - loss: 0.6672 - acc: 0.6052
2688/4849 [===============>..............] - ETA: 2:09 - loss: 0.6677 - acc: 0.6053
2752/4849 [================>.............] - ETA: 2:05 - loss: 0.6680 - acc: 0.6043
2816/4849 [================>.............] - ETA: 2:01 - loss: 0.6679 - acc: 0.6055
2880/4849 [================>.............] - ETA: 1:57 - loss: 0.6684 - acc: 0.6045
2944/4849 [=================>............] - ETA: 1:54 - loss: 0.6696 - acc: 0.6009
3008/4849 [=================>............] - ETA: 1:50 - loss: 0.6693 - acc: 0.6014
3072/4849 [==================>...........] - ETA: 1:46 - loss: 0.6702 - acc: 0.5990
3136/4849 [==================>...........] - ETA: 1:42 - loss: 0.6707 - acc: 0.5979
3200/4849 [==================>...........] - ETA: 1:38 - loss: 0.6705 - acc: 0.5988
3264/4849 [===================>..........] - ETA: 1:34 - loss: 0.6709 - acc: 0.5971
3328/4849 [===================>..........] - ETA: 1:30 - loss: 0.6713 - acc: 0.5968
3392/4849 [===================>..........] - ETA: 1:26 - loss: 0.6713 - acc: 0.5970
3456/4849 [====================>.........] - ETA: 1:23 - loss: 0.6712 - acc: 0.5966
3520/4849 [====================>.........] - ETA: 1:19 - loss: 0.6714 - acc: 0.5960
3584/4849 [=====================>........] - ETA: 1:15 - loss: 0.6720 - acc: 0.5949
3648/4849 [=====================>........] - ETA: 1:11 - loss: 0.6712 - acc: 0.5965
3712/4849 [=====================>........] - ETA: 1:07 - loss: 0.6713 - acc: 0.5956
3776/4849 [======================>.......] - ETA: 1:04 - loss: 0.6711 - acc: 0.5964
3840/4849 [======================>.......] - ETA: 1:00 - loss: 0.6700 - acc: 0.5982
3904/4849 [=======================>......] - ETA: 56s - loss: 0.6701 - acc: 0.5989 
3968/4849 [=======================>......] - ETA: 52s - loss: 0.6699 - acc: 0.5993
4032/4849 [=======================>......] - ETA: 49s - loss: 0.6694 - acc: 0.5995
4096/4849 [========================>.....] - ETA: 45s - loss: 0.6694 - acc: 0.5996
4160/4849 [========================>.....] - ETA: 41s - loss: 0.6690 - acc: 0.5995
4224/4849 [=========================>....] - ETA: 37s - loss: 0.6695 - acc: 0.5987
4288/4849 [=========================>....] - ETA: 33s - loss: 0.6699 - acc: 0.5986
4352/4849 [=========================>....] - ETA: 29s - loss: 0.6689 - acc: 0.6004
4416/4849 [==========================>...] - ETA: 26s - loss: 0.6686 - acc: 0.6008
4480/4849 [==========================>...] - ETA: 22s - loss: 0.6677 - acc: 0.6018
4544/4849 [===========================>..] - ETA: 18s - loss: 0.6676 - acc: 0.6017
4608/4849 [===========================>..] - ETA: 14s - loss: 0.6678 - acc: 0.6016
4672/4849 [===========================>..] - ETA: 10s - loss: 0.6674 - acc: 0.6019
4736/4849 [============================>.] - ETA: 6s - loss: 0.6676 - acc: 0.6018 
4800/4849 [============================>.] - ETA: 2s - loss: 0.6675 - acc: 0.6019
4849/4849 [==============================] - 306s 63ms/step - loss: 0.6675 - acc: 0.6016 - val_loss: 0.6951 - val_acc: 0.5547

Epoch 00008: val_acc did not improve from 0.56215
Epoch 9/10

  64/4849 [..............................] - ETA: 5:17 - loss: 0.6326 - acc: 0.6406
 128/4849 [..............................] - ETA: 4:56 - loss: 0.6595 - acc: 0.5859
 192/4849 [>.............................] - ETA: 4:48 - loss: 0.6694 - acc: 0.5729
 256/4849 [>.............................] - ETA: 4:44 - loss: 0.6839 - acc: 0.5703
 320/4849 [>.............................] - ETA: 4:33 - loss: 0.6958 - acc: 0.5531
 384/4849 [=>............................] - ETA: 4:25 - loss: 0.6939 - acc: 0.5651
 448/4849 [=>............................] - ETA: 4:27 - loss: 0.6988 - acc: 0.5580
 512/4849 [==>...........................] - ETA: 4:28 - loss: 0.6962 - acc: 0.5645
 576/4849 [==>...........................] - ETA: 4:23 - loss: 0.6919 - acc: 0.5660
 640/4849 [==>...........................] - ETA: 4:21 - loss: 0.6939 - acc: 0.5625
 704/4849 [===>..........................] - ETA: 4:19 - loss: 0.6923 - acc: 0.5625
 768/4849 [===>..........................] - ETA: 4:12 - loss: 0.6858 - acc: 0.5794
 832/4849 [====>.........................] - ETA: 4:05 - loss: 0.6843 - acc: 0.5757
 896/4849 [====>.........................] - ETA: 4:00 - loss: 0.6835 - acc: 0.5759
 960/4849 [====>.........................] - ETA: 3:58 - loss: 0.6817 - acc: 0.5740
1024/4849 [=====>........................] - ETA: 3:56 - loss: 0.6782 - acc: 0.5811
1088/4849 [=====>........................] - ETA: 3:50 - loss: 0.6772 - acc: 0.5827
1152/4849 [======>.......................] - ETA: 3:44 - loss: 0.6750 - acc: 0.5842
1216/4849 [======>.......................] - ETA: 3:39 - loss: 0.6743 - acc: 0.5847
1280/4849 [======>.......................] - ETA: 3:34 - loss: 0.6741 - acc: 0.5859
1344/4849 [=======>......................] - ETA: 3:30 - loss: 0.6734 - acc: 0.5863
1408/4849 [=======>......................] - ETA: 3:26 - loss: 0.6740 - acc: 0.5874
1472/4849 [========>.....................] - ETA: 3:22 - loss: 0.6739 - acc: 0.5876
1536/4849 [========>.....................] - ETA: 3:18 - loss: 0.6741 - acc: 0.5905
1600/4849 [========>.....................] - ETA: 3:14 - loss: 0.6745 - acc: 0.5906
1664/4849 [=========>....................] - ETA: 3:10 - loss: 0.6735 - acc: 0.5950
1728/4849 [=========>....................] - ETA: 3:06 - loss: 0.6712 - acc: 0.5990
1792/4849 [==========>...................] - ETA: 3:03 - loss: 0.6707 - acc: 0.5982
1856/4849 [==========>...................] - ETA: 2:59 - loss: 0.6689 - acc: 0.6018
1920/4849 [==========>...................] - ETA: 2:55 - loss: 0.6700 - acc: 0.5974
1984/4849 [===========>..................] - ETA: 2:51 - loss: 0.6687 - acc: 0.5993
2048/4849 [===========>..................] - ETA: 2:47 - loss: 0.6689 - acc: 0.5986
2112/4849 [============>.................] - ETA: 2:44 - loss: 0.6689 - acc: 0.5994
2176/4849 [============>.................] - ETA: 2:40 - loss: 0.6676 - acc: 0.6006
2240/4849 [============>.................] - ETA: 2:36 - loss: 0.6681 - acc: 0.6000
2304/4849 [=============>................] - ETA: 2:33 - loss: 0.6683 - acc: 0.5994
2368/4849 [=============>................] - ETA: 2:29 - loss: 0.6664 - acc: 0.6018
2432/4849 [==============>...............] - ETA: 2:26 - loss: 0.6665 - acc: 0.6007
2496/4849 [==============>...............] - ETA: 2:23 - loss: 0.6666 - acc: 0.5998
2560/4849 [==============>...............] - ETA: 2:19 - loss: 0.6651 - acc: 0.6023
2624/4849 [===============>..............] - ETA: 2:15 - loss: 0.6662 - acc: 0.6010
2688/4849 [===============>..............] - ETA: 2:11 - loss: 0.6667 - acc: 0.6001
2752/4849 [================>.............] - ETA: 2:07 - loss: 0.6676 - acc: 0.5996
2816/4849 [================>.............] - ETA: 2:03 - loss: 0.6678 - acc: 0.5994
2880/4849 [================>.............] - ETA: 1:59 - loss: 0.6673 - acc: 0.6003
2944/4849 [=================>............] - ETA: 1:55 - loss: 0.6677 - acc: 0.5999
3008/4849 [=================>............] - ETA: 1:52 - loss: 0.6661 - acc: 0.6017
3072/4849 [==================>...........] - ETA: 1:48 - loss: 0.6665 - acc: 0.6003
3136/4849 [==================>...........] - ETA: 1:44 - loss: 0.6647 - acc: 0.6030
3200/4849 [==================>...........] - ETA: 1:40 - loss: 0.6650 - acc: 0.6016
3264/4849 [===================>..........] - ETA: 1:36 - loss: 0.6652 - acc: 0.6008
3328/4849 [===================>..........] - ETA: 1:32 - loss: 0.6652 - acc: 0.6010
3392/4849 [===================>..........] - ETA: 1:28 - loss: 0.6654 - acc: 0.6011
3456/4849 [====================>.........] - ETA: 1:25 - loss: 0.6654 - acc: 0.6013
3520/4849 [====================>.........] - ETA: 1:21 - loss: 0.6661 - acc: 0.6000
3584/4849 [=====================>........] - ETA: 1:17 - loss: 0.6659 - acc: 0.5999
3648/4849 [=====================>........] - ETA: 1:13 - loss: 0.6654 - acc: 0.6006
3712/4849 [=====================>........] - ETA: 1:09 - loss: 0.6642 - acc: 0.6018
3776/4849 [======================>.......] - ETA: 1:05 - loss: 0.6647 - acc: 0.6012
3840/4849 [======================>.......] - ETA: 1:01 - loss: 0.6653 - acc: 0.6005
3904/4849 [=======================>......] - ETA: 57s - loss: 0.6650 - acc: 0.6009 
3968/4849 [=======================>......] - ETA: 53s - loss: 0.6656 - acc: 0.6003
4032/4849 [=======================>......] - ETA: 49s - loss: 0.6654 - acc: 0.6017
4096/4849 [========================>.....] - ETA: 46s - loss: 0.6652 - acc: 0.6013
4160/4849 [========================>.....] - ETA: 42s - loss: 0.6654 - acc: 0.6000
4224/4849 [=========================>....] - ETA: 38s - loss: 0.6654 - acc: 0.5987
4288/4849 [=========================>....] - ETA: 34s - loss: 0.6642 - acc: 0.6000
4352/4849 [=========================>....] - ETA: 30s - loss: 0.6641 - acc: 0.6000
4416/4849 [==========================>...] - ETA: 26s - loss: 0.6635 - acc: 0.6005
4480/4849 [==========================>...] - ETA: 22s - loss: 0.6634 - acc: 0.6011
4544/4849 [===========================>..] - ETA: 18s - loss: 0.6632 - acc: 0.6015
4608/4849 [===========================>..] - ETA: 14s - loss: 0.6634 - acc: 0.6011
4672/4849 [===========================>..] - ETA: 10s - loss: 0.6633 - acc: 0.6017
4736/4849 [============================>.] - ETA: 6s - loss: 0.6634 - acc: 0.6014 
4800/4849 [============================>.] - ETA: 2s - loss: 0.6631 - acc: 0.6025
4849/4849 [==============================] - 308s 64ms/step - loss: 0.6630 - acc: 0.6032 - val_loss: 0.7070 - val_acc: 0.5788

Epoch 00009: val_acc improved from 0.56215 to 0.57885, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window12/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 10/10

  64/4849 [..............................] - ETA: 5:12 - loss: 0.6525 - acc: 0.6094
 128/4849 [..............................] - ETA: 4:49 - loss: 0.6419 - acc: 0.6172
 192/4849 [>.............................] - ETA: 4:51 - loss: 0.6392 - acc: 0.6198
 256/4849 [>.............................] - ETA: 4:38 - loss: 0.6574 - acc: 0.5938
 320/4849 [>.............................] - ETA: 4:34 - loss: 0.6564 - acc: 0.6000
 384/4849 [=>............................] - ETA: 4:27 - loss: 0.6566 - acc: 0.6016
 448/4849 [=>............................] - ETA: 4:22 - loss: 0.6567 - acc: 0.6027
 512/4849 [==>...........................] - ETA: 4:24 - loss: 0.6545 - acc: 0.6055
 576/4849 [==>...........................] - ETA: 4:24 - loss: 0.6536 - acc: 0.6076
 640/4849 [==>...........................] - ETA: 4:18 - loss: 0.6535 - acc: 0.6141
 704/4849 [===>..........................] - ETA: 4:12 - loss: 0.6515 - acc: 0.6236
 768/4849 [===>..........................] - ETA: 4:09 - loss: 0.6440 - acc: 0.6315
 832/4849 [====>.........................] - ETA: 4:04 - loss: 0.6404 - acc: 0.6346
 896/4849 [====>.........................] - ETA: 3:58 - loss: 0.6400 - acc: 0.6373
 960/4849 [====>.........................] - ETA: 3:54 - loss: 0.6377 - acc: 0.6396
1024/4849 [=====>........................] - ETA: 3:50 - loss: 0.6376 - acc: 0.6387
1088/4849 [=====>........................] - ETA: 3:46 - loss: 0.6405 - acc: 0.6333
1152/4849 [======>.......................] - ETA: 3:42 - loss: 0.6457 - acc: 0.6293
1216/4849 [======>.......................] - ETA: 3:38 - loss: 0.6457 - acc: 0.6266
1280/4849 [======>.......................] - ETA: 3:33 - loss: 0.6467 - acc: 0.6242
1344/4849 [=======>......................] - ETA: 3:29 - loss: 0.6483 - acc: 0.6220
1408/4849 [=======>......................] - ETA: 3:24 - loss: 0.6493 - acc: 0.6193
1472/4849 [========>.....................] - ETA: 3:21 - loss: 0.6502 - acc: 0.6162
1536/4849 [========>.....................] - ETA: 3:16 - loss: 0.6504 - acc: 0.6159
1600/4849 [========>.....................] - ETA: 3:12 - loss: 0.6524 - acc: 0.6138
1664/4849 [=========>....................] - ETA: 3:07 - loss: 0.6558 - acc: 0.6082
1728/4849 [=========>....................] - ETA: 3:03 - loss: 0.6577 - acc: 0.6059
1792/4849 [==========>...................] - ETA: 2:58 - loss: 0.6577 - acc: 0.6055
1856/4849 [==========>...................] - ETA: 2:55 - loss: 0.6580 - acc: 0.6056
1920/4849 [==========>...................] - ETA: 2:50 - loss: 0.6579 - acc: 0.6078
1984/4849 [===========>..................] - ETA: 2:47 - loss: 0.6580 - acc: 0.6084
2048/4849 [===========>..................] - ETA: 2:44 - loss: 0.6572 - acc: 0.6099
2112/4849 [============>.................] - ETA: 2:40 - loss: 0.6569 - acc: 0.6094
2176/4849 [============>.................] - ETA: 2:35 - loss: 0.6555 - acc: 0.6117
2240/4849 [============>.................] - ETA: 2:31 - loss: 0.6559 - acc: 0.6098
2304/4849 [=============>................] - ETA: 2:27 - loss: 0.6556 - acc: 0.6102
2368/4849 [=============>................] - ETA: 2:23 - loss: 0.6555 - acc: 0.6098
2432/4849 [==============>...............] - ETA: 2:19 - loss: 0.6545 - acc: 0.6114
2496/4849 [==============>...............] - ETA: 2:16 - loss: 0.6548 - acc: 0.6114
2560/4849 [==============>...............] - ETA: 2:12 - loss: 0.6548 - acc: 0.6117
2624/4849 [===============>..............] - ETA: 2:09 - loss: 0.6545 - acc: 0.6113
2688/4849 [===============>..............] - ETA: 2:05 - loss: 0.6547 - acc: 0.6112
2752/4849 [================>.............] - ETA: 2:01 - loss: 0.6558 - acc: 0.6097
2816/4849 [================>.............] - ETA: 1:57 - loss: 0.6557 - acc: 0.6108
2880/4849 [================>.............] - ETA: 1:54 - loss: 0.6569 - acc: 0.6097
2944/4849 [=================>............] - ETA: 1:50 - loss: 0.6566 - acc: 0.6104
3008/4849 [=================>............] - ETA: 1:46 - loss: 0.6571 - acc: 0.6107
3072/4849 [==================>...........] - ETA: 1:43 - loss: 0.6567 - acc: 0.6104
3136/4849 [==================>...........] - ETA: 1:39 - loss: 0.6563 - acc: 0.6097
3200/4849 [==================>...........] - ETA: 1:35 - loss: 0.6580 - acc: 0.6072
3264/4849 [===================>..........] - ETA: 1:32 - loss: 0.6592 - acc: 0.6048
3328/4849 [===================>..........] - ETA: 1:28 - loss: 0.6592 - acc: 0.6028
3392/4849 [===================>..........] - ETA: 1:24 - loss: 0.6577 - acc: 0.6058
3456/4849 [====================>.........] - ETA: 1:20 - loss: 0.6584 - acc: 0.6047
3520/4849 [====================>.........] - ETA: 1:16 - loss: 0.6590 - acc: 0.6040
3584/4849 [=====================>........] - ETA: 1:13 - loss: 0.6588 - acc: 0.6046
3648/4849 [=====================>........] - ETA: 1:09 - loss: 0.6590 - acc: 0.6039
3712/4849 [=====================>........] - ETA: 1:05 - loss: 0.6592 - acc: 0.6040
3776/4849 [======================>.......] - ETA: 1:02 - loss: 0.6597 - acc: 0.6028
3840/4849 [======================>.......] - ETA: 58s - loss: 0.6602 - acc: 0.6021 
3904/4849 [=======================>......] - ETA: 54s - loss: 0.6592 - acc: 0.6040
3968/4849 [=======================>......] - ETA: 50s - loss: 0.6598 - acc: 0.6031
4032/4849 [=======================>......] - ETA: 47s - loss: 0.6603 - acc: 0.6017
4096/4849 [========================>.....] - ETA: 43s - loss: 0.6606 - acc: 0.6025
4160/4849 [========================>.....] - ETA: 39s - loss: 0.6599 - acc: 0.6038
4224/4849 [=========================>....] - ETA: 35s - loss: 0.6600 - acc: 0.6032
4288/4849 [=========================>....] - ETA: 32s - loss: 0.6607 - acc: 0.6028
4352/4849 [=========================>....] - ETA: 28s - loss: 0.6607 - acc: 0.6027
4416/4849 [==========================>...] - ETA: 24s - loss: 0.6603 - acc: 0.6030
4480/4849 [==========================>...] - ETA: 21s - loss: 0.6603 - acc: 0.6025
4544/4849 [===========================>..] - ETA: 17s - loss: 0.6602 - acc: 0.6028
4608/4849 [===========================>..] - ETA: 13s - loss: 0.6605 - acc: 0.6018
4672/4849 [===========================>..] - ETA: 10s - loss: 0.6601 - acc: 0.6021
4736/4849 [============================>.] - ETA: 6s - loss: 0.6603 - acc: 0.6018 
4800/4849 [============================>.] - ETA: 2s - loss: 0.6603 - acc: 0.6006
4849/4849 [==============================] - 291s 60ms/step - loss: 0.6606 - acc: 0.5999 - val_loss: 0.6960 - val_acc: 0.5751

Epoch 00010: val_acc did not improve from 0.57885
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd8afb88450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd8afb88450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd8afb26d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd8afb26d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2f40a4e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2f40a4e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8856e3f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8856e3f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd88570b0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd88570b0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8856ce250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8856ce250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8856e3b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8856e3b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd0f459fcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd0f459fcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd88548a290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd88548a290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd884f535d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd884f535d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8853b2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8853b2390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd885544f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd885544f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e26bbbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8e26bbbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd885369e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd885369e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8851d61d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8851d61d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd88505e450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd88505e450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8e2671650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8e2671650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8850553d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8850553d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd885101550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd885101550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd884f5bd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd884f5bd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884e3f750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884e3f750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd884e9f8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd884e9f8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884fc3850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884fc3850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd884c5ae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd884c5ae50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd884aa3f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd884aa3f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd840995150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd840995150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd884c5a9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd884c5a9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884e8c350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884e8c350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd5b9b31290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd5b9b31290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2940ebf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2940ebf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2747a89d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2747a89d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd840991cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd840991cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884ade650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884ade650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd274697e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd274697e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd274622750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd274622750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2746d3b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2746d3b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd274697390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd274697390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884af0290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd884af0290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd27435a150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd27435a150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2746c9dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2746c9dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd274172250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd274172250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd27435acd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd27435acd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2741617d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2741617d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd27403d150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd27403d150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd274078590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd274078590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd25477bb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd25477bb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd27403d0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd27403d0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd274053b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd274053b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8850d4650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8850d4650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2544da050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2544da050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd254525c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd254525c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2544ca750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2544ca750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2542f3890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd2542f3890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2541d9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd2541d9e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2541f0110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2541f0110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd254446790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd254446790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2541d9890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd2541d9890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd23473ea50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd23473ea50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd23466d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd23466d8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2345d5dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd2345d5dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd25420e390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd25420e390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd23466de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd23466de90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd25421c310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd25421c310>>: AttributeError: module 'gast' has no attribute 'Str'
Traceback (most recent call last):
  File "03window_select.py", line 272, in <module>
    X_seq_test = [extract_single_features(tokenizer, x['sequence']) for x in sequenceList]
  File "03window_select.py", line 272, in <listcomp>
    X_seq_test = [extract_single_features(tokenizer, x['sequence']) for x in sequenceList]
  File "03window_select.py", line 187, in extract_single_features
    indices, segments = tokenizer.encode(first=text, max_len=max_len)
  File "/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras_bert/tokenizer.py", line 73, in encode
    first_tokens = self._tokenize(first)
  File "03window_select.py", line 177, in _tokenize
    elif self._is_space(tmp):
  File "/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras_bert/tokenizer.py", line 165, in _is_space
    unicodedata.category(ch) == 'Zs'
TypeError: category() argument must be a unicode character, not str
nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 2694
样本个数 5388
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7bc2444050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7bc2444050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7bc2487f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7bc2487f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bcc2cf690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bcc2cf690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc2487ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc2487ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bc2630190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bc2630190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b43cd2710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b43cd2710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3bc83550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3bc83550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc2630250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc2630250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3badd890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3badd890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b43d61890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b43d61890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc22da8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc22da8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3badd3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3badd3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b810a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b810a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3b7e9c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3b7e9c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3b63e210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3b63e210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b684710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b684710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3b7e9810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3b7e9810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b526a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b526a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3b71ca50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3b71ca50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3b526750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3b526750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b489610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b489610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3b71c5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3b71c5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b24d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b24d8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3baf2610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3baf2610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3b165410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3b165410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b244510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b244510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3b1dab10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3b1dab10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b8bba10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b8bba10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3b394d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3b394d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3ad15d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3ad15d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b1bbf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3b1bbf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3b15af50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3b15af50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3ad66090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3ad66090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3ab25510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3ab25510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3aa32a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3aa32a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a9fe650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a9fe650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3ad6e650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3ad6e650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a8d49d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a8d49d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3b842310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3b842310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3a6cfd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3a6cfd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a82f450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a82f450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3b1bbbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3b1bbbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a83aa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a83aa10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3a63b4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3a63b4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3a3bff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3a3bff10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a4dd3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a4dd3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3a63b790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3a63b790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a2b24d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a2b24d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3a1d6810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3a1d6810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3a214a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b3a214a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a1540d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a1540d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3a214e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3a214e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a8111d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b3a8111d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3a429090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b3a429090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b39dab690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b39dab690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b39eea690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b39eea690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b39f2add0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b39f2add0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b39cecfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b39cecfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b39d8f050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b39d8f050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b39d8f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b39d8f590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b39dfc190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b39dfc190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3aa32ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b3aa32ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b29b21f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b29b21f10>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-27 13:18:00.781324: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-27 13:18:00.883347: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-27 13:18:00.980475: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563b6befd240 executing computations on platform Host. Devices:
2022-11-27 13:18:00.980648: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-27 13:18:01.881462: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 27:51 - loss: 0.7906 - acc: 0.4062
 128/4849 [..............................] - ETA: 19:19 - loss: 0.7321 - acc: 0.5078
 192/4849 [>.............................] - ETA: 16:16 - loss: 0.7429 - acc: 0.5000
 256/4849 [>.............................] - ETA: 14:29 - loss: 0.7250 - acc: 0.5156
 320/4849 [>.............................] - ETA: 13:37 - loss: 0.7188 - acc: 0.5094
 384/4849 [=>............................] - ETA: 12:53 - loss: 0.7184 - acc: 0.5156
 448/4849 [=>............................] - ETA: 12:13 - loss: 0.7281 - acc: 0.5089
 512/4849 [==>...........................] - ETA: 11:42 - loss: 0.7287 - acc: 0.4980
 576/4849 [==>...........................] - ETA: 11:18 - loss: 0.7304 - acc: 0.4965
 640/4849 [==>...........................] - ETA: 10:58 - loss: 0.7313 - acc: 0.4953
 704/4849 [===>..........................] - ETA: 10:36 - loss: 0.7259 - acc: 0.5014
 768/4849 [===>..........................] - ETA: 10:17 - loss: 0.7287 - acc: 0.5013
 832/4849 [====>.........................] - ETA: 9:57 - loss: 0.7304 - acc: 0.4952 
 896/4849 [====>.........................] - ETA: 9:42 - loss: 0.7284 - acc: 0.4967
 960/4849 [====>.........................] - ETA: 9:39 - loss: 0.7274 - acc: 0.4969
1024/4849 [=====>........................] - ETA: 9:30 - loss: 0.7262 - acc: 0.5029
1088/4849 [=====>........................] - ETA: 9:14 - loss: 0.7244 - acc: 0.5055
1152/4849 [======>.......................] - ETA: 9:04 - loss: 0.7228 - acc: 0.5087
1216/4849 [======>.......................] - ETA: 8:50 - loss: 0.7211 - acc: 0.5115
1280/4849 [======>.......................] - ETA: 8:40 - loss: 0.7203 - acc: 0.5125
1344/4849 [=======>......................] - ETA: 8:27 - loss: 0.7198 - acc: 0.5119
1408/4849 [=======>......................] - ETA: 8:14 - loss: 0.7216 - acc: 0.5099
1472/4849 [========>.....................] - ETA: 8:01 - loss: 0.7213 - acc: 0.5075
1536/4849 [========>.....................] - ETA: 7:52 - loss: 0.7217 - acc: 0.5052
1600/4849 [========>.....................] - ETA: 7:41 - loss: 0.7209 - acc: 0.5050
1664/4849 [=========>....................] - ETA: 7:31 - loss: 0.7203 - acc: 0.5078
1728/4849 [=========>....................] - ETA: 7:19 - loss: 0.7192 - acc: 0.5104
1792/4849 [==========>...................] - ETA: 7:08 - loss: 0.7200 - acc: 0.5095
1856/4849 [==========>...................] - ETA: 6:57 - loss: 0.7189 - acc: 0.5097
1920/4849 [==========>...................] - ETA: 6:46 - loss: 0.7185 - acc: 0.5104
1984/4849 [===========>..................] - ETA: 6:36 - loss: 0.7182 - acc: 0.5121
2048/4849 [===========>..................] - ETA: 6:25 - loss: 0.7195 - acc: 0.5117
2112/4849 [============>.................] - ETA: 6:15 - loss: 0.7178 - acc: 0.5137
2176/4849 [============>.................] - ETA: 6:05 - loss: 0.7179 - acc: 0.5133
2240/4849 [============>.................] - ETA: 5:56 - loss: 0.7174 - acc: 0.5134
2304/4849 [=============>................] - ETA: 5:46 - loss: 0.7169 - acc: 0.5139
2368/4849 [=============>................] - ETA: 5:36 - loss: 0.7163 - acc: 0.5160
2432/4849 [==============>...............] - ETA: 5:27 - loss: 0.7157 - acc: 0.5169
2496/4849 [==============>...............] - ETA: 5:18 - loss: 0.7151 - acc: 0.5184
2560/4849 [==============>...............] - ETA: 5:08 - loss: 0.7143 - acc: 0.5195
2624/4849 [===============>..............] - ETA: 4:59 - loss: 0.7134 - acc: 0.5210
2688/4849 [===============>..............] - ETA: 4:50 - loss: 0.7150 - acc: 0.5208
2752/4849 [================>.............] - ETA: 4:41 - loss: 0.7154 - acc: 0.5196
2816/4849 [================>.............] - ETA: 4:33 - loss: 0.7142 - acc: 0.5217
2880/4849 [================>.............] - ETA: 4:24 - loss: 0.7138 - acc: 0.5215
2944/4849 [=================>............] - ETA: 4:14 - loss: 0.7137 - acc: 0.5224
3008/4849 [=================>............] - ETA: 4:06 - loss: 0.7126 - acc: 0.5256
3072/4849 [==================>...........] - ETA: 3:57 - loss: 0.7122 - acc: 0.5260
3136/4849 [==================>...........] - ETA: 3:48 - loss: 0.7116 - acc: 0.5261
3200/4849 [==================>...........] - ETA: 3:39 - loss: 0.7116 - acc: 0.5272
3264/4849 [===================>..........] - ETA: 3:30 - loss: 0.7117 - acc: 0.5279
3328/4849 [===================>..........] - ETA: 3:21 - loss: 0.7113 - acc: 0.5279
3392/4849 [===================>..........] - ETA: 3:12 - loss: 0.7103 - acc: 0.5295
3456/4849 [====================>.........] - ETA: 3:03 - loss: 0.7110 - acc: 0.5289
3520/4849 [====================>.........] - ETA: 2:55 - loss: 0.7106 - acc: 0.5287
3584/4849 [=====================>........] - ETA: 2:46 - loss: 0.7109 - acc: 0.5279
3648/4849 [=====================>........] - ETA: 2:38 - loss: 0.7120 - acc: 0.5260
3712/4849 [=====================>........] - ETA: 2:30 - loss: 0.7122 - acc: 0.5267
3776/4849 [======================>.......] - ETA: 2:22 - loss: 0.7115 - acc: 0.5275
3840/4849 [======================>.......] - ETA: 2:13 - loss: 0.7116 - acc: 0.5271
3904/4849 [=======================>......] - ETA: 2:04 - loss: 0.7119 - acc: 0.5272
3968/4849 [=======================>......] - ETA: 1:56 - loss: 0.7112 - acc: 0.5292
4032/4849 [=======================>......] - ETA: 1:47 - loss: 0.7110 - acc: 0.5293
4096/4849 [========================>.....] - ETA: 1:39 - loss: 0.7107 - acc: 0.5298
4160/4849 [========================>.....] - ETA: 1:30 - loss: 0.7095 - acc: 0.5320
4224/4849 [=========================>....] - ETA: 1:22 - loss: 0.7095 - acc: 0.5308
4288/4849 [=========================>....] - ETA: 1:13 - loss: 0.7098 - acc: 0.5296
4352/4849 [=========================>....] - ETA: 1:05 - loss: 0.7089 - acc: 0.5317
4416/4849 [==========================>...] - ETA: 56s - loss: 0.7085 - acc: 0.5322 
4480/4849 [==========================>...] - ETA: 48s - loss: 0.7086 - acc: 0.5328
4544/4849 [===========================>..] - ETA: 39s - loss: 0.7077 - acc: 0.5343
4608/4849 [===========================>..] - ETA: 31s - loss: 0.7078 - acc: 0.5345
4672/4849 [===========================>..] - ETA: 23s - loss: 0.7073 - acc: 0.5351
4736/4849 [============================>.] - ETA: 14s - loss: 0.7070 - acc: 0.5351
4800/4849 [============================>.] - ETA: 6s - loss: 0.7068 - acc: 0.5346 
4849/4849 [==============================] - 656s 135ms/step - loss: 0.7070 - acc: 0.5347 - val_loss: 0.6940 - val_acc: 0.5250

Epoch 00001: val_acc improved from -inf to 0.52505, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window12/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 9:28 - loss: 0.7035 - acc: 0.5312
 128/4849 [..............................] - ETA: 9:25 - loss: 0.6907 - acc: 0.5391
 192/4849 [>.............................] - ETA: 9:21 - loss: 0.6685 - acc: 0.5781
 256/4849 [>.............................] - ETA: 9:12 - loss: 0.6770 - acc: 0.5703
 320/4849 [>.............................] - ETA: 9:05 - loss: 0.6870 - acc: 0.5500
 384/4849 [=>............................] - ETA: 8:58 - loss: 0.6858 - acc: 0.5599
 448/4849 [=>............................] - ETA: 8:54 - loss: 0.6898 - acc: 0.5558
 512/4849 [==>...........................] - ETA: 8:46 - loss: 0.6976 - acc: 0.5391
 576/4849 [==>...........................] - ETA: 8:35 - loss: 0.6935 - acc: 0.5451
 640/4849 [==>...........................] - ETA: 8:24 - loss: 0.6867 - acc: 0.5531
 704/4849 [===>..........................] - ETA: 8:15 - loss: 0.6850 - acc: 0.5611
 768/4849 [===>..........................] - ETA: 8:07 - loss: 0.6856 - acc: 0.5586
 832/4849 [====>.........................] - ETA: 8:00 - loss: 0.6845 - acc: 0.5565
 896/4849 [====>.........................] - ETA: 7:51 - loss: 0.6824 - acc: 0.5614
 960/4849 [====>.........................] - ETA: 7:43 - loss: 0.6819 - acc: 0.5667
1024/4849 [=====>........................] - ETA: 7:34 - loss: 0.6841 - acc: 0.5664
1088/4849 [=====>........................] - ETA: 7:28 - loss: 0.6822 - acc: 0.5689
1152/4849 [======>.......................] - ETA: 7:20 - loss: 0.6810 - acc: 0.5720
1216/4849 [======>.......................] - ETA: 7:11 - loss: 0.6837 - acc: 0.5666
1280/4849 [======>.......................] - ETA: 7:03 - loss: 0.6858 - acc: 0.5625
1344/4849 [=======>......................] - ETA: 6:55 - loss: 0.6865 - acc: 0.5588
1408/4849 [=======>......................] - ETA: 6:47 - loss: 0.6855 - acc: 0.5597
1472/4849 [========>.....................] - ETA: 6:39 - loss: 0.6866 - acc: 0.5571
1536/4849 [========>.....................] - ETA: 6:30 - loss: 0.6855 - acc: 0.5579
1600/4849 [========>.....................] - ETA: 6:22 - loss: 0.6845 - acc: 0.5625
1664/4849 [=========>....................] - ETA: 6:15 - loss: 0.6856 - acc: 0.5607
1728/4849 [=========>....................] - ETA: 6:07 - loss: 0.6846 - acc: 0.5608
1792/4849 [==========>...................] - ETA: 5:58 - loss: 0.6851 - acc: 0.5580
1856/4849 [==========>...................] - ETA: 5:50 - loss: 0.6839 - acc: 0.5587
1920/4849 [==========>...................] - ETA: 5:42 - loss: 0.6840 - acc: 0.5568
1984/4849 [===========>..................] - ETA: 5:34 - loss: 0.6843 - acc: 0.5585
2048/4849 [===========>..................] - ETA: 5:26 - loss: 0.6853 - acc: 0.5562
2112/4849 [============>.................] - ETA: 5:19 - loss: 0.6850 - acc: 0.5563
2176/4849 [============>.................] - ETA: 5:12 - loss: 0.6857 - acc: 0.5556
2240/4849 [============>.................] - ETA: 5:04 - loss: 0.6842 - acc: 0.5585
2304/4849 [=============>................] - ETA: 4:57 - loss: 0.6830 - acc: 0.5599
2368/4849 [=============>................] - ETA: 4:49 - loss: 0.6837 - acc: 0.5579
2432/4849 [==============>...............] - ETA: 4:42 - loss: 0.6843 - acc: 0.5580
2496/4849 [==============>...............] - ETA: 4:34 - loss: 0.6842 - acc: 0.5609
2560/4849 [==============>...............] - ETA: 4:26 - loss: 0.6835 - acc: 0.5625
2624/4849 [===============>..............] - ETA: 4:19 - loss: 0.6831 - acc: 0.5648
2688/4849 [===============>..............] - ETA: 4:11 - loss: 0.6829 - acc: 0.5673
2752/4849 [================>.............] - ETA: 4:03 - loss: 0.6829 - acc: 0.5661
2816/4849 [================>.............] - ETA: 3:56 - loss: 0.6847 - acc: 0.5643
2880/4849 [================>.............] - ETA: 3:48 - loss: 0.6853 - acc: 0.5632
2944/4849 [=================>............] - ETA: 3:40 - loss: 0.6853 - acc: 0.5628
3008/4849 [=================>............] - ETA: 3:33 - loss: 0.6847 - acc: 0.5638
3072/4849 [==================>...........] - ETA: 3:25 - loss: 0.6844 - acc: 0.5632
3136/4849 [==================>...........] - ETA: 3:18 - loss: 0.6864 - acc: 0.5612
3200/4849 [==================>...........] - ETA: 3:10 - loss: 0.6874 - acc: 0.5600
3264/4849 [===================>..........] - ETA: 3:02 - loss: 0.6868 - acc: 0.5616
3328/4849 [===================>..........] - ETA: 2:55 - loss: 0.6862 - acc: 0.5625
3392/4849 [===================>..........] - ETA: 2:47 - loss: 0.6865 - acc: 0.5616
3456/4849 [====================>.........] - ETA: 2:40 - loss: 0.6864 - acc: 0.5613
3520/4849 [====================>.........] - ETA: 2:32 - loss: 0.6858 - acc: 0.5631
3584/4849 [=====================>........] - ETA: 2:25 - loss: 0.6854 - acc: 0.5631
3648/4849 [=====================>........] - ETA: 2:18 - loss: 0.6856 - acc: 0.5625
3712/4849 [=====================>........] - ETA: 2:10 - loss: 0.6853 - acc: 0.5628
3776/4849 [======================>.......] - ETA: 2:03 - loss: 0.6860 - acc: 0.5622
3840/4849 [======================>.......] - ETA: 1:55 - loss: 0.6862 - acc: 0.5617
3904/4849 [=======================>......] - ETA: 1:48 - loss: 0.6861 - acc: 0.5625
3968/4849 [=======================>......] - ETA: 1:40 - loss: 0.6863 - acc: 0.5620
4032/4849 [=======================>......] - ETA: 1:33 - loss: 0.6855 - acc: 0.5620
4096/4849 [========================>.....] - ETA: 1:26 - loss: 0.6864 - acc: 0.5610
4160/4849 [========================>.....] - ETA: 1:18 - loss: 0.6863 - acc: 0.5613
4224/4849 [=========================>....] - ETA: 1:11 - loss: 0.6867 - acc: 0.5613
4288/4849 [=========================>....] - ETA: 1:04 - loss: 0.6865 - acc: 0.5618
4352/4849 [=========================>....] - ETA: 56s - loss: 0.6861 - acc: 0.5614 
4416/4849 [==========================>...] - ETA: 49s - loss: 0.6862 - acc: 0.5611
4480/4849 [==========================>...] - ETA: 42s - loss: 0.6858 - acc: 0.5621
4544/4849 [===========================>..] - ETA: 34s - loss: 0.6857 - acc: 0.5627
4608/4849 [===========================>..] - ETA: 27s - loss: 0.6855 - acc: 0.5638
4672/4849 [===========================>..] - ETA: 20s - loss: 0.6860 - acc: 0.5623
4736/4849 [============================>.] - ETA: 12s - loss: 0.6865 - acc: 0.5612
4800/4849 [============================>.] - ETA: 5s - loss: 0.6868 - acc: 0.5602 
4849/4849 [==============================] - 566s 117ms/step - loss: 0.6868 - acc: 0.5607 - val_loss: 0.7047 - val_acc: 0.5158

Epoch 00002: val_acc did not improve from 0.52505
Epoch 3/10

  64/4849 [..............................] - ETA: 7:54 - loss: 0.6922 - acc: 0.5156
 128/4849 [..............................] - ETA: 7:55 - loss: 0.6884 - acc: 0.5312
 192/4849 [>.............................] - ETA: 7:48 - loss: 0.7062 - acc: 0.5000
 256/4849 [>.............................] - ETA: 7:36 - loss: 0.7034 - acc: 0.5000
 320/4849 [>.............................] - ETA: 7:27 - loss: 0.6946 - acc: 0.5250
 384/4849 [=>............................] - ETA: 7:17 - loss: 0.6939 - acc: 0.5391
 448/4849 [=>............................] - ETA: 7:09 - loss: 0.6906 - acc: 0.5357
 512/4849 [==>...........................] - ETA: 6:59 - loss: 0.6900 - acc: 0.5430
 576/4849 [==>...........................] - ETA: 6:51 - loss: 0.6849 - acc: 0.5486
 640/4849 [==>...........................] - ETA: 6:44 - loss: 0.6857 - acc: 0.5422
 704/4849 [===>..........................] - ETA: 6:36 - loss: 0.6877 - acc: 0.5455
 768/4849 [===>..........................] - ETA: 6:33 - loss: 0.6841 - acc: 0.5482
 832/4849 [====>.........................] - ETA: 6:25 - loss: 0.6874 - acc: 0.5445
 896/4849 [====>.........................] - ETA: 6:19 - loss: 0.6895 - acc: 0.5480
 960/4849 [====>.........................] - ETA: 6:12 - loss: 0.6948 - acc: 0.5437
1024/4849 [=====>........................] - ETA: 6:06 - loss: 0.6905 - acc: 0.5488
1088/4849 [=====>........................] - ETA: 5:59 - loss: 0.6901 - acc: 0.5533
1152/4849 [======>.......................] - ETA: 5:51 - loss: 0.6877 - acc: 0.5538
1216/4849 [======>.......................] - ETA: 5:46 - loss: 0.6893 - acc: 0.5510
1280/4849 [======>.......................] - ETA: 5:39 - loss: 0.6912 - acc: 0.5469
1344/4849 [=======>......................] - ETA: 5:33 - loss: 0.6917 - acc: 0.5439
1408/4849 [=======>......................] - ETA: 5:27 - loss: 0.6928 - acc: 0.5469
1472/4849 [========>.....................] - ETA: 5:21 - loss: 0.6910 - acc: 0.5496
1536/4849 [========>.....................] - ETA: 5:14 - loss: 0.6898 - acc: 0.5527
1600/4849 [========>.....................] - ETA: 5:09 - loss: 0.6878 - acc: 0.5556
1664/4849 [=========>....................] - ETA: 5:04 - loss: 0.6892 - acc: 0.5535
1728/4849 [=========>....................] - ETA: 4:58 - loss: 0.6887 - acc: 0.5550
1792/4849 [==========>...................] - ETA: 4:51 - loss: 0.6888 - acc: 0.5536
1856/4849 [==========>...................] - ETA: 4:46 - loss: 0.6898 - acc: 0.5523
1920/4849 [==========>...................] - ETA: 4:41 - loss: 0.6884 - acc: 0.5557
1984/4849 [===========>..................] - ETA: 4:35 - loss: 0.6860 - acc: 0.5600
2048/4849 [===========>..................] - ETA: 4:31 - loss: 0.6848 - acc: 0.5601
2112/4849 [============>.................] - ETA: 4:25 - loss: 0.6844 - acc: 0.5616
2176/4849 [============>.................] - ETA: 4:20 - loss: 0.6855 - acc: 0.5602
2240/4849 [============>.................] - ETA: 4:14 - loss: 0.6853 - acc: 0.5612
2304/4849 [=============>................] - ETA: 4:09 - loss: 0.6845 - acc: 0.5638
2368/4849 [=============>................] - ETA: 4:03 - loss: 0.6854 - acc: 0.5604
2432/4849 [==============>...............] - ETA: 3:58 - loss: 0.6839 - acc: 0.5633
2496/4849 [==============>...............] - ETA: 3:52 - loss: 0.6832 - acc: 0.5649
2560/4849 [==============>...............] - ETA: 3:46 - loss: 0.6827 - acc: 0.5668
2624/4849 [===============>..............] - ETA: 3:41 - loss: 0.6829 - acc: 0.5663
2688/4849 [===============>..............] - ETA: 3:35 - loss: 0.6830 - acc: 0.5666
2752/4849 [================>.............] - ETA: 3:28 - loss: 0.6817 - acc: 0.5683
2816/4849 [================>.............] - ETA: 3:22 - loss: 0.6809 - acc: 0.5700
2880/4849 [================>.............] - ETA: 3:17 - loss: 0.6805 - acc: 0.5705
2944/4849 [=================>............] - ETA: 3:11 - loss: 0.6800 - acc: 0.5713
3008/4849 [=================>............] - ETA: 3:05 - loss: 0.6800 - acc: 0.5715
3072/4849 [==================>...........] - ETA: 2:58 - loss: 0.6790 - acc: 0.5736
3136/4849 [==================>...........] - ETA: 2:52 - loss: 0.6788 - acc: 0.5749
3200/4849 [==================>...........] - ETA: 2:46 - loss: 0.6792 - acc: 0.5750
3264/4849 [===================>..........] - ETA: 2:40 - loss: 0.6791 - acc: 0.5754
3328/4849 [===================>..........] - ETA: 2:34 - loss: 0.6781 - acc: 0.5772
3392/4849 [===================>..........] - ETA: 2:28 - loss: 0.6770 - acc: 0.5781
3456/4849 [====================>.........] - ETA: 2:21 - loss: 0.6776 - acc: 0.5764
3520/4849 [====================>.........] - ETA: 2:15 - loss: 0.6773 - acc: 0.5770
3584/4849 [=====================>........] - ETA: 2:09 - loss: 0.6771 - acc: 0.5773
3648/4849 [=====================>........] - ETA: 2:02 - loss: 0.6772 - acc: 0.5773
3712/4849 [=====================>........] - ETA: 1:56 - loss: 0.6780 - acc: 0.5770
3776/4849 [======================>.......] - ETA: 1:49 - loss: 0.6767 - acc: 0.5784
3840/4849 [======================>.......] - ETA: 1:43 - loss: 0.6777 - acc: 0.5779
3904/4849 [=======================>......] - ETA: 1:37 - loss: 0.6773 - acc: 0.5786
3968/4849 [=======================>......] - ETA: 1:30 - loss: 0.6772 - acc: 0.5784
4032/4849 [=======================>......] - ETA: 1:24 - loss: 0.6781 - acc: 0.5766
4096/4849 [========================>.....] - ETA: 1:17 - loss: 0.6785 - acc: 0.5767
4160/4849 [========================>.....] - ETA: 1:11 - loss: 0.6787 - acc: 0.5762
4224/4849 [=========================>....] - ETA: 1:04 - loss: 0.6784 - acc: 0.5772
4288/4849 [=========================>....] - ETA: 58s - loss: 0.6783 - acc: 0.5772 
4352/4849 [=========================>....] - ETA: 51s - loss: 0.6791 - acc: 0.5758
4416/4849 [==========================>...] - ETA: 45s - loss: 0.6784 - acc: 0.5759
4480/4849 [==========================>...] - ETA: 38s - loss: 0.6784 - acc: 0.5766
4544/4849 [===========================>..] - ETA: 31s - loss: 0.6790 - acc: 0.5750
4608/4849 [===========================>..] - ETA: 25s - loss: 0.6793 - acc: 0.5749
4672/4849 [===========================>..] - ETA: 18s - loss: 0.6788 - acc: 0.5753
4736/4849 [============================>.] - ETA: 11s - loss: 0.6788 - acc: 0.5747
4800/4849 [============================>.] - ETA: 5s - loss: 0.6787 - acc: 0.5752 
4849/4849 [==============================] - 528s 109ms/step - loss: 0.6791 - acc: 0.5741 - val_loss: 0.7202 - val_acc: 0.5083

Epoch 00003: val_acc did not improve from 0.52505
Epoch 4/10

  64/4849 [..............................] - ETA: 8:28 - loss: 0.6962 - acc: 0.5000
 128/4849 [..............................] - ETA: 8:26 - loss: 0.6793 - acc: 0.5703
 192/4849 [>.............................] - ETA: 8:24 - loss: 0.6640 - acc: 0.5938
 256/4849 [>.............................] - ETA: 8:21 - loss: 0.6730 - acc: 0.5820
 320/4849 [>.............................] - ETA: 8:29 - loss: 0.6833 - acc: 0.5625
 384/4849 [=>............................] - ETA: 8:34 - loss: 0.6837 - acc: 0.5651
 448/4849 [=>............................] - ETA: 8:33 - loss: 0.6792 - acc: 0.5737
 512/4849 [==>...........................] - ETA: 8:32 - loss: 0.6804 - acc: 0.5762
 576/4849 [==>...........................] - ETA: 8:26 - loss: 0.6732 - acc: 0.5885
 640/4849 [==>...........................] - ETA: 8:17 - loss: 0.6773 - acc: 0.5828
 704/4849 [===>..........................] - ETA: 8:09 - loss: 0.6749 - acc: 0.5866
 768/4849 [===>..........................] - ETA: 8:01 - loss: 0.6759 - acc: 0.5846
 832/4849 [====>.........................] - ETA: 7:56 - loss: 0.6731 - acc: 0.5877
 896/4849 [====>.........................] - ETA: 7:50 - loss: 0.6767 - acc: 0.5815
 960/4849 [====>.........................] - ETA: 7:42 - loss: 0.6765 - acc: 0.5813
1024/4849 [=====>........................] - ETA: 7:36 - loss: 0.6761 - acc: 0.5801
1088/4849 [=====>........................] - ETA: 7:31 - loss: 0.6748 - acc: 0.5827
1152/4849 [======>.......................] - ETA: 7:23 - loss: 0.6750 - acc: 0.5851
1216/4849 [======>.......................] - ETA: 7:15 - loss: 0.6744 - acc: 0.5839
1280/4849 [======>.......................] - ETA: 7:07 - loss: 0.6737 - acc: 0.5805
1344/4849 [=======>......................] - ETA: 7:00 - loss: 0.6745 - acc: 0.5811
1408/4849 [=======>......................] - ETA: 6:52 - loss: 0.6726 - acc: 0.5852
1472/4849 [========>.....................] - ETA: 6:45 - loss: 0.6735 - acc: 0.5842
1536/4849 [========>.....................] - ETA: 6:38 - loss: 0.6741 - acc: 0.5820
1600/4849 [========>.....................] - ETA: 6:30 - loss: 0.6742 - acc: 0.5813
1664/4849 [=========>....................] - ETA: 6:23 - loss: 0.6737 - acc: 0.5823
1728/4849 [=========>....................] - ETA: 6:15 - loss: 0.6747 - acc: 0.5822
1792/4849 [==========>...................] - ETA: 6:08 - loss: 0.6740 - acc: 0.5837
1856/4849 [==========>...................] - ETA: 6:01 - loss: 0.6729 - acc: 0.5857
1920/4849 [==========>...................] - ETA: 5:53 - loss: 0.6712 - acc: 0.5896
1984/4849 [===========>..................] - ETA: 5:46 - loss: 0.6715 - acc: 0.5877
2048/4849 [===========>..................] - ETA: 5:38 - loss: 0.6709 - acc: 0.5889
2112/4849 [============>.................] - ETA: 5:31 - loss: 0.6699 - acc: 0.5914
2176/4849 [============>.................] - ETA: 5:24 - loss: 0.6691 - acc: 0.5915
2240/4849 [============>.................] - ETA: 5:17 - loss: 0.6700 - acc: 0.5902
2304/4849 [=============>................] - ETA: 5:09 - loss: 0.6689 - acc: 0.5898
2368/4849 [=============>................] - ETA: 5:01 - loss: 0.6696 - acc: 0.5883
2432/4849 [==============>...............] - ETA: 4:53 - loss: 0.6687 - acc: 0.5896
2496/4849 [==============>...............] - ETA: 4:45 - loss: 0.6693 - acc: 0.5889
2560/4849 [==============>...............] - ETA: 4:37 - loss: 0.6674 - acc: 0.5922
2624/4849 [===============>..............] - ETA: 4:29 - loss: 0.6674 - acc: 0.5918
2688/4849 [===============>..............] - ETA: 4:21 - loss: 0.6670 - acc: 0.5923
2752/4849 [================>.............] - ETA: 4:14 - loss: 0.6662 - acc: 0.5938
2816/4849 [================>.............] - ETA: 4:06 - loss: 0.6653 - acc: 0.5969
2880/4849 [================>.............] - ETA: 3:58 - loss: 0.6656 - acc: 0.5969
2944/4849 [=================>............] - ETA: 3:50 - loss: 0.6669 - acc: 0.5961
3008/4849 [=================>............] - ETA: 3:42 - loss: 0.6672 - acc: 0.5951
3072/4849 [==================>...........] - ETA: 3:34 - loss: 0.6678 - acc: 0.5938
3136/4849 [==================>...........] - ETA: 3:27 - loss: 0.6684 - acc: 0.5928
3200/4849 [==================>...........] - ETA: 3:19 - loss: 0.6689 - acc: 0.5916
3264/4849 [===================>..........] - ETA: 3:11 - loss: 0.6695 - acc: 0.5907
3328/4849 [===================>..........] - ETA: 3:03 - loss: 0.6693 - acc: 0.5907
3392/4849 [===================>..........] - ETA: 2:55 - loss: 0.6692 - acc: 0.5914
3456/4849 [====================>.........] - ETA: 2:47 - loss: 0.6691 - acc: 0.5917
3520/4849 [====================>.........] - ETA: 2:40 - loss: 0.6697 - acc: 0.5909
3584/4849 [=====================>........] - ETA: 2:32 - loss: 0.6696 - acc: 0.5907
3648/4849 [=====================>........] - ETA: 2:24 - loss: 0.6695 - acc: 0.5910
3712/4849 [=====================>........] - ETA: 2:17 - loss: 0.6702 - acc: 0.5886
3776/4849 [======================>.......] - ETA: 2:09 - loss: 0.6701 - acc: 0.5882
3840/4849 [======================>.......] - ETA: 2:01 - loss: 0.6702 - acc: 0.5885
3904/4849 [=======================>......] - ETA: 1:53 - loss: 0.6708 - acc: 0.5881
3968/4849 [=======================>......] - ETA: 1:46 - loss: 0.6709 - acc: 0.5880
4032/4849 [=======================>......] - ETA: 1:38 - loss: 0.6707 - acc: 0.5878
4096/4849 [========================>.....] - ETA: 1:30 - loss: 0.6700 - acc: 0.5881
4160/4849 [========================>.....] - ETA: 1:22 - loss: 0.6700 - acc: 0.5880
4224/4849 [=========================>....] - ETA: 1:15 - loss: 0.6695 - acc: 0.5890
4288/4849 [=========================>....] - ETA: 1:07 - loss: 0.6696 - acc: 0.5889
4352/4849 [=========================>....] - ETA: 59s - loss: 0.6699 - acc: 0.5885 
4416/4849 [==========================>...] - ETA: 52s - loss: 0.6715 - acc: 0.5865
4480/4849 [==========================>...] - ETA: 44s - loss: 0.6724 - acc: 0.5853
4544/4849 [===========================>..] - ETA: 36s - loss: 0.6731 - acc: 0.5838
4608/4849 [===========================>..] - ETA: 28s - loss: 0.6726 - acc: 0.5846
4672/4849 [===========================>..] - ETA: 21s - loss: 0.6726 - acc: 0.5848
4736/4849 [============================>.] - ETA: 13s - loss: 0.6723 - acc: 0.5853
4800/4849 [============================>.] - ETA: 5s - loss: 0.6726 - acc: 0.5852 
4849/4849 [==============================] - 602s 124ms/step - loss: 0.6725 - acc: 0.5857 - val_loss: 0.6891 - val_acc: 0.5566

Epoch 00004: val_acc improved from 0.52505 to 0.55659, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window12/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 5/10

  64/4849 [..............................] - ETA: 8:10 - loss: 0.6063 - acc: 0.7188
 128/4849 [..............................] - ETA: 8:22 - loss: 0.6356 - acc: 0.6484
 192/4849 [>.............................] - ETA: 8:01 - loss: 0.6478 - acc: 0.6198
 256/4849 [>.............................] - ETA: 7:53 - loss: 0.6596 - acc: 0.6055
 320/4849 [>.............................] - ETA: 7:56 - loss: 0.6616 - acc: 0.5969
 384/4849 [=>............................] - ETA: 7:48 - loss: 0.6741 - acc: 0.5729
 448/4849 [=>............................] - ETA: 7:42 - loss: 0.6782 - acc: 0.5737
 512/4849 [==>...........................] - ETA: 7:37 - loss: 0.6805 - acc: 0.5762
 576/4849 [==>...........................] - ETA: 7:30 - loss: 0.6830 - acc: 0.5747
 640/4849 [==>...........................] - ETA: 7:25 - loss: 0.6824 - acc: 0.5781
 704/4849 [===>..........................] - ETA: 7:22 - loss: 0.6834 - acc: 0.5795
 768/4849 [===>..........................] - ETA: 7:14 - loss: 0.6857 - acc: 0.5755
 832/4849 [====>.........................] - ETA: 7:09 - loss: 0.6837 - acc: 0.5745
 896/4849 [====>.........................] - ETA: 7:04 - loss: 0.6837 - acc: 0.5714
 960/4849 [====>.........................] - ETA: 6:57 - loss: 0.6837 - acc: 0.5708
1024/4849 [=====>........................] - ETA: 6:52 - loss: 0.6801 - acc: 0.5762
1088/4849 [=====>........................] - ETA: 6:45 - loss: 0.6820 - acc: 0.5689
1152/4849 [======>.......................] - ETA: 6:39 - loss: 0.6813 - acc: 0.5747
1216/4849 [======>.......................] - ETA: 6:31 - loss: 0.6809 - acc: 0.5748
1280/4849 [======>.......................] - ETA: 6:24 - loss: 0.6807 - acc: 0.5742
1344/4849 [=======>......................] - ETA: 6:18 - loss: 0.6822 - acc: 0.5707
1408/4849 [=======>......................] - ETA: 6:11 - loss: 0.6793 - acc: 0.5767
1472/4849 [========>.....................] - ETA: 6:03 - loss: 0.6790 - acc: 0.5802
1536/4849 [========>.....................] - ETA: 5:56 - loss: 0.6771 - acc: 0.5827
1600/4849 [========>.....................] - ETA: 5:49 - loss: 0.6761 - acc: 0.5831
1664/4849 [=========>....................] - ETA: 5:42 - loss: 0.6759 - acc: 0.5835
1728/4849 [=========>....................] - ETA: 5:35 - loss: 0.6750 - acc: 0.5868
1792/4849 [==========>...................] - ETA: 5:28 - loss: 0.6758 - acc: 0.5854
1856/4849 [==========>...................] - ETA: 5:21 - loss: 0.6760 - acc: 0.5846
1920/4849 [==========>...................] - ETA: 5:14 - loss: 0.6766 - acc: 0.5833
1984/4849 [===========>..................] - ETA: 5:07 - loss: 0.6762 - acc: 0.5832
2048/4849 [===========>..................] - ETA: 5:00 - loss: 0.6755 - acc: 0.5830
2112/4849 [============>.................] - ETA: 4:54 - loss: 0.6747 - acc: 0.5848
2176/4849 [============>.................] - ETA: 4:47 - loss: 0.6741 - acc: 0.5878
2240/4849 [============>.................] - ETA: 4:40 - loss: 0.6740 - acc: 0.5871
2304/4849 [=============>................] - ETA: 4:33 - loss: 0.6724 - acc: 0.5885
2368/4849 [=============>................] - ETA: 4:26 - loss: 0.6732 - acc: 0.5878
2432/4849 [==============>...............] - ETA: 4:19 - loss: 0.6722 - acc: 0.5905
2496/4849 [==============>...............] - ETA: 4:12 - loss: 0.6701 - acc: 0.5929
2560/4849 [==============>...............] - ETA: 4:04 - loss: 0.6687 - acc: 0.5949
2624/4849 [===============>..............] - ETA: 3:57 - loss: 0.6693 - acc: 0.5930
2688/4849 [===============>..............] - ETA: 3:50 - loss: 0.6679 - acc: 0.5956
2752/4849 [================>.............] - ETA: 3:43 - loss: 0.6684 - acc: 0.5956
2816/4849 [================>.............] - ETA: 3:36 - loss: 0.6681 - acc: 0.5945
2880/4849 [================>.............] - ETA: 3:29 - loss: 0.6694 - acc: 0.5920
2944/4849 [=================>............] - ETA: 3:21 - loss: 0.6684 - acc: 0.5948
3008/4849 [=================>............] - ETA: 3:14 - loss: 0.6691 - acc: 0.5934
3072/4849 [==================>...........] - ETA: 3:07 - loss: 0.6697 - acc: 0.5938
3136/4849 [==================>...........] - ETA: 3:01 - loss: 0.6701 - acc: 0.5925
3200/4849 [==================>...........] - ETA: 2:53 - loss: 0.6704 - acc: 0.5931
3264/4849 [===================>..........] - ETA: 2:46 - loss: 0.6704 - acc: 0.5928
3328/4849 [===================>..........] - ETA: 2:40 - loss: 0.6710 - acc: 0.5922
3392/4849 [===================>..........] - ETA: 2:33 - loss: 0.6714 - acc: 0.5914
3456/4849 [====================>.........] - ETA: 2:26 - loss: 0.6713 - acc: 0.5906
3520/4849 [====================>.........] - ETA: 2:19 - loss: 0.6722 - acc: 0.5886
3584/4849 [=====================>........] - ETA: 2:12 - loss: 0.6722 - acc: 0.5884
3648/4849 [=====================>........] - ETA: 2:05 - loss: 0.6713 - acc: 0.5902
3712/4849 [=====================>........] - ETA: 1:58 - loss: 0.6707 - acc: 0.5916
3776/4849 [======================>.......] - ETA: 1:52 - loss: 0.6706 - acc: 0.5916
3840/4849 [======================>.......] - ETA: 1:45 - loss: 0.6704 - acc: 0.5914
3904/4849 [=======================>......] - ETA: 1:38 - loss: 0.6701 - acc: 0.5912
3968/4849 [=======================>......] - ETA: 1:31 - loss: 0.6696 - acc: 0.5922
4032/4849 [=======================>......] - ETA: 1:24 - loss: 0.6690 - acc: 0.5933
4096/4849 [========================>.....] - ETA: 1:18 - loss: 0.6690 - acc: 0.5933
4160/4849 [========================>.....] - ETA: 1:11 - loss: 0.6684 - acc: 0.5942
4224/4849 [=========================>....] - ETA: 1:04 - loss: 0.6694 - acc: 0.5926
4288/4849 [=========================>....] - ETA: 58s - loss: 0.6694 - acc: 0.5926 
4352/4849 [=========================>....] - ETA: 51s - loss: 0.6692 - acc: 0.5933
4416/4849 [==========================>...] - ETA: 44s - loss: 0.6690 - acc: 0.5942
4480/4849 [==========================>...] - ETA: 38s - loss: 0.6702 - acc: 0.5931
4544/4849 [===========================>..] - ETA: 31s - loss: 0.6700 - acc: 0.5933
4608/4849 [===========================>..] - ETA: 24s - loss: 0.6695 - acc: 0.5938
4672/4849 [===========================>..] - ETA: 18s - loss: 0.6696 - acc: 0.5935
4736/4849 [============================>.] - ETA: 11s - loss: 0.6699 - acc: 0.5935
4800/4849 [============================>.] - ETA: 5s - loss: 0.6696 - acc: 0.5938 
4849/4849 [==============================] - 518s 107ms/step - loss: 0.6695 - acc: 0.5943 - val_loss: 0.6907 - val_acc: 0.5640

Epoch 00005: val_acc improved from 0.55659 to 0.56401, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window12/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 6/10

  64/4849 [..............................] - ETA: 7:52 - loss: 0.6668 - acc: 0.6562
 128/4849 [..............................] - ETA: 7:34 - loss: 0.6488 - acc: 0.6484
 192/4849 [>.............................] - ETA: 7:33 - loss: 0.6708 - acc: 0.6146
 256/4849 [>.............................] - ETA: 7:30 - loss: 0.6682 - acc: 0.6055
 320/4849 [>.............................] - ETA: 7:19 - loss: 0.6596 - acc: 0.6156
 384/4849 [=>............................] - ETA: 7:08 - loss: 0.6600 - acc: 0.6250
 448/4849 [=>............................] - ETA: 7:07 - loss: 0.6621 - acc: 0.6183
 512/4849 [==>...........................] - ETA: 7:02 - loss: 0.6632 - acc: 0.6133
 576/4849 [==>...........................] - ETA: 6:57 - loss: 0.6632 - acc: 0.6128
 640/4849 [==>...........................] - ETA: 6:52 - loss: 0.6622 - acc: 0.6109
 704/4849 [===>..........................] - ETA: 6:48 - loss: 0.6669 - acc: 0.6051
 768/4849 [===>..........................] - ETA: 6:42 - loss: 0.6668 - acc: 0.6016
 832/4849 [====>.........................] - ETA: 6:35 - loss: 0.6650 - acc: 0.6022
 896/4849 [====>.........................] - ETA: 6:30 - loss: 0.6668 - acc: 0.5982
 960/4849 [====>.........................] - ETA: 6:26 - loss: 0.6621 - acc: 0.6062
1024/4849 [=====>........................] - ETA: 6:20 - loss: 0.6644 - acc: 0.6045
1088/4849 [=====>........................] - ETA: 6:14 - loss: 0.6638 - acc: 0.6048
1152/4849 [======>.......................] - ETA: 6:07 - loss: 0.6685 - acc: 0.5990
1216/4849 [======>.......................] - ETA: 6:01 - loss: 0.6704 - acc: 0.5921
1280/4849 [======>.......................] - ETA: 5:56 - loss: 0.6699 - acc: 0.5891
1344/4849 [=======>......................] - ETA: 5:49 - loss: 0.6711 - acc: 0.5885
1408/4849 [=======>......................] - ETA: 5:43 - loss: 0.6711 - acc: 0.5888
1472/4849 [========>.....................] - ETA: 5:36 - loss: 0.6708 - acc: 0.5897
1536/4849 [========>.....................] - ETA: 5:31 - loss: 0.6722 - acc: 0.5898
1600/4849 [========>.....................] - ETA: 5:24 - loss: 0.6724 - acc: 0.5881
1664/4849 [=========>....................] - ETA: 5:17 - loss: 0.6710 - acc: 0.5919
1728/4849 [=========>....................] - ETA: 5:10 - loss: 0.6720 - acc: 0.5885
1792/4849 [==========>...................] - ETA: 5:03 - loss: 0.6712 - acc: 0.5904
1856/4849 [==========>...................] - ETA: 4:56 - loss: 0.6706 - acc: 0.5900
1920/4849 [==========>...................] - ETA: 4:50 - loss: 0.6706 - acc: 0.5891
1984/4849 [===========>..................] - ETA: 4:44 - loss: 0.6690 - acc: 0.5912
2048/4849 [===========>..................] - ETA: 4:38 - loss: 0.6696 - acc: 0.5913
2112/4849 [============>.................] - ETA: 4:31 - loss: 0.6701 - acc: 0.5914
2176/4849 [============>.................] - ETA: 4:25 - loss: 0.6697 - acc: 0.5938
2240/4849 [============>.................] - ETA: 4:19 - loss: 0.6685 - acc: 0.5960
2304/4849 [=============>................] - ETA: 4:12 - loss: 0.6699 - acc: 0.5942
2368/4849 [=============>................] - ETA: 4:06 - loss: 0.6698 - acc: 0.5938
2432/4849 [==============>...............] - ETA: 3:59 - loss: 0.6676 - acc: 0.5966
2496/4849 [==============>...............] - ETA: 3:53 - loss: 0.6662 - acc: 0.5982
2560/4849 [==============>...............] - ETA: 3:47 - loss: 0.6658 - acc: 0.5980
2624/4849 [===============>..............] - ETA: 3:40 - loss: 0.6650 - acc: 0.5983
2688/4849 [===============>..............] - ETA: 3:34 - loss: 0.6651 - acc: 0.5986
2752/4849 [================>.............] - ETA: 3:28 - loss: 0.6644 - acc: 0.5999
2816/4849 [================>.............] - ETA: 3:21 - loss: 0.6660 - acc: 0.5980
2880/4849 [================>.............] - ETA: 3:15 - loss: 0.6655 - acc: 0.5983
2944/4849 [=================>............] - ETA: 3:09 - loss: 0.6665 - acc: 0.5958
3008/4849 [=================>............] - ETA: 3:02 - loss: 0.6661 - acc: 0.5957
3072/4849 [==================>...........] - ETA: 2:56 - loss: 0.6663 - acc: 0.5951
3136/4849 [==================>...........] - ETA: 2:50 - loss: 0.6653 - acc: 0.5957
3200/4849 [==================>...........] - ETA: 2:43 - loss: 0.6634 - acc: 0.5981
3264/4849 [===================>..........] - ETA: 2:37 - loss: 0.6625 - acc: 0.6008
3328/4849 [===================>..........] - ETA: 2:30 - loss: 0.6624 - acc: 0.6022
3392/4849 [===================>..........] - ETA: 2:24 - loss: 0.6631 - acc: 0.6005
3456/4849 [====================>.........] - ETA: 2:18 - loss: 0.6631 - acc: 0.6013
3520/4849 [====================>.........] - ETA: 2:12 - loss: 0.6631 - acc: 0.6026
3584/4849 [=====================>........] - ETA: 2:05 - loss: 0.6642 - acc: 0.6013
3648/4849 [=====================>........] - ETA: 1:59 - loss: 0.6641 - acc: 0.6014
3712/4849 [=====================>........] - ETA: 1:53 - loss: 0.6647 - acc: 0.6016
3776/4849 [======================>.......] - ETA: 1:46 - loss: 0.6652 - acc: 0.6006
3840/4849 [======================>.......] - ETA: 1:40 - loss: 0.6651 - acc: 0.6005
3904/4849 [=======================>......] - ETA: 1:34 - loss: 0.6654 - acc: 0.6007
3968/4849 [=======================>......] - ETA: 1:27 - loss: 0.6648 - acc: 0.6011
4032/4849 [=======================>......] - ETA: 1:21 - loss: 0.6652 - acc: 0.6012
4096/4849 [========================>.....] - ETA: 1:14 - loss: 0.6649 - acc: 0.6021
4160/4849 [========================>.....] - ETA: 1:08 - loss: 0.6643 - acc: 0.6038
4224/4849 [=========================>....] - ETA: 1:02 - loss: 0.6647 - acc: 0.6037
4288/4849 [=========================>....] - ETA: 55s - loss: 0.6638 - acc: 0.6056 
4352/4849 [=========================>....] - ETA: 49s - loss: 0.6642 - acc: 0.6057
4416/4849 [==========================>...] - ETA: 43s - loss: 0.6645 - acc: 0.6058
4480/4849 [==========================>...] - ETA: 36s - loss: 0.6650 - acc: 0.6042
4544/4849 [===========================>..] - ETA: 30s - loss: 0.6646 - acc: 0.6050
4608/4849 [===========================>..] - ETA: 24s - loss: 0.6657 - acc: 0.6031
4672/4849 [===========================>..] - ETA: 17s - loss: 0.6655 - acc: 0.6032
4736/4849 [============================>.] - ETA: 11s - loss: 0.6653 - acc: 0.6035
4800/4849 [============================>.] - ETA: 4s - loss: 0.6655 - acc: 0.6025 
4849/4849 [==============================] - 501s 103ms/step - loss: 0.6658 - acc: 0.6020 - val_loss: 0.6879 - val_acc: 0.5566

Epoch 00006: val_acc did not improve from 0.56401
Epoch 7/10

  64/4849 [..............................] - ETA: 7:56 - loss: 0.6705 - acc: 0.5312
 128/4849 [..............................] - ETA: 8:02 - loss: 0.6629 - acc: 0.5547
 192/4849 [>.............................] - ETA: 7:51 - loss: 0.6608 - acc: 0.5833
 256/4849 [>.............................] - ETA: 7:45 - loss: 0.6680 - acc: 0.5703
 320/4849 [>.............................] - ETA: 7:41 - loss: 0.6689 - acc: 0.5625
 384/4849 [=>............................] - ETA: 7:37 - loss: 0.6634 - acc: 0.5807
 448/4849 [=>............................] - ETA: 7:28 - loss: 0.6625 - acc: 0.5826
 512/4849 [==>...........................] - ETA: 7:18 - loss: 0.6658 - acc: 0.5781
 576/4849 [==>...........................] - ETA: 7:13 - loss: 0.6652 - acc: 0.5747
 640/4849 [==>...........................] - ETA: 7:09 - loss: 0.6677 - acc: 0.5719
 704/4849 [===>..........................] - ETA: 7:03 - loss: 0.6794 - acc: 0.5582
 768/4849 [===>..........................] - ETA: 6:57 - loss: 0.6795 - acc: 0.5599
 832/4849 [====>.........................] - ETA: 6:50 - loss: 0.6777 - acc: 0.5649
 896/4849 [====>.........................] - ETA: 6:44 - loss: 0.6781 - acc: 0.5636
 960/4849 [====>.........................] - ETA: 6:38 - loss: 0.6795 - acc: 0.5594
1024/4849 [=====>........................] - ETA: 6:31 - loss: 0.6766 - acc: 0.5664
1088/4849 [=====>........................] - ETA: 6:22 - loss: 0.6762 - acc: 0.5680
1152/4849 [======>.......................] - ETA: 6:16 - loss: 0.6742 - acc: 0.5720
1216/4849 [======>.......................] - ETA: 6:09 - loss: 0.6735 - acc: 0.5715
1280/4849 [======>.......................] - ETA: 6:02 - loss: 0.6739 - acc: 0.5734
1344/4849 [=======>......................] - ETA: 5:55 - loss: 0.6750 - acc: 0.5722
1408/4849 [=======>......................] - ETA: 5:49 - loss: 0.6760 - acc: 0.5682
1472/4849 [========>.....................] - ETA: 5:43 - loss: 0.6750 - acc: 0.5686
1536/4849 [========>.....................] - ETA: 5:35 - loss: 0.6731 - acc: 0.5729
1600/4849 [========>.....................] - ETA: 5:29 - loss: 0.6738 - acc: 0.5713
1664/4849 [=========>....................] - ETA: 5:23 - loss: 0.6748 - acc: 0.5697
1728/4849 [=========>....................] - ETA: 5:16 - loss: 0.6764 - acc: 0.5660
1792/4849 [==========>...................] - ETA: 5:10 - loss: 0.6754 - acc: 0.5664
1856/4849 [==========>...................] - ETA: 5:03 - loss: 0.6752 - acc: 0.5679
1920/4849 [==========>...................] - ETA: 4:56 - loss: 0.6739 - acc: 0.5714
1984/4849 [===========>..................] - ETA: 4:50 - loss: 0.6746 - acc: 0.5706
2048/4849 [===========>..................] - ETA: 4:43 - loss: 0.6753 - acc: 0.5703
2112/4849 [============>.................] - ETA: 4:37 - loss: 0.6756 - acc: 0.5705
2176/4849 [============>.................] - ETA: 4:31 - loss: 0.6773 - acc: 0.5680
2240/4849 [============>.................] - ETA: 4:25 - loss: 0.6758 - acc: 0.5719
2304/4849 [=============>................] - ETA: 4:19 - loss: 0.6759 - acc: 0.5729
2368/4849 [=============>................] - ETA: 4:12 - loss: 0.6764 - acc: 0.5709
2432/4849 [==============>...............] - ETA: 4:06 - loss: 0.6757 - acc: 0.5732
2496/4849 [==============>...............] - ETA: 3:59 - loss: 0.6763 - acc: 0.5713
2560/4849 [==============>...............] - ETA: 3:53 - loss: 0.6754 - acc: 0.5719
2624/4849 [===============>..............] - ETA: 3:46 - loss: 0.6749 - acc: 0.5724
2688/4849 [===============>..............] - ETA: 3:40 - loss: 0.6738 - acc: 0.5759
2752/4849 [================>.............] - ETA: 3:33 - loss: 0.6739 - acc: 0.5756
2816/4849 [================>.............] - ETA: 3:27 - loss: 0.6739 - acc: 0.5763
2880/4849 [================>.............] - ETA: 3:20 - loss: 0.6735 - acc: 0.5764
2944/4849 [=================>............] - ETA: 3:14 - loss: 0.6730 - acc: 0.5778
3008/4849 [=================>............] - ETA: 3:07 - loss: 0.6728 - acc: 0.5788
3072/4849 [==================>...........] - ETA: 3:01 - loss: 0.6728 - acc: 0.5794
3136/4849 [==================>...........] - ETA: 2:54 - loss: 0.6726 - acc: 0.5800
3200/4849 [==================>...........] - ETA: 2:47 - loss: 0.6728 - acc: 0.5791
3264/4849 [===================>..........] - ETA: 2:41 - loss: 0.6714 - acc: 0.5824
3328/4849 [===================>..........] - ETA: 2:35 - loss: 0.6708 - acc: 0.5841
3392/4849 [===================>..........] - ETA: 2:28 - loss: 0.6705 - acc: 0.5846
3456/4849 [====================>.........] - ETA: 2:22 - loss: 0.6694 - acc: 0.5862
3520/4849 [====================>.........] - ETA: 2:15 - loss: 0.6689 - acc: 0.5881
3584/4849 [=====================>........] - ETA: 2:09 - loss: 0.6691 - acc: 0.5873
3648/4849 [=====================>........] - ETA: 2:03 - loss: 0.6688 - acc: 0.5885
3712/4849 [=====================>........] - ETA: 1:57 - loss: 0.6685 - acc: 0.5894
3776/4849 [======================>.......] - ETA: 1:50 - loss: 0.6674 - acc: 0.5919
3840/4849 [======================>.......] - ETA: 1:44 - loss: 0.6662 - acc: 0.5932
3904/4849 [=======================>......] - ETA: 1:37 - loss: 0.6664 - acc: 0.5930
3968/4849 [=======================>......] - ETA: 1:30 - loss: 0.6662 - acc: 0.5938
4032/4849 [=======================>......] - ETA: 1:24 - loss: 0.6662 - acc: 0.5935
4096/4849 [========================>.....] - ETA: 1:17 - loss: 0.6655 - acc: 0.5942
4160/4849 [========================>.....] - ETA: 1:10 - loss: 0.6647 - acc: 0.5952
4224/4849 [=========================>....] - ETA: 1:04 - loss: 0.6654 - acc: 0.5945
4288/4849 [=========================>....] - ETA: 57s - loss: 0.6653 - acc: 0.5951 
4352/4849 [=========================>....] - ETA: 51s - loss: 0.6655 - acc: 0.5956
4416/4849 [==========================>...] - ETA: 44s - loss: 0.6646 - acc: 0.5962
4480/4849 [==========================>...] - ETA: 38s - loss: 0.6634 - acc: 0.5980
4544/4849 [===========================>..] - ETA: 31s - loss: 0.6640 - acc: 0.5975
4608/4849 [===========================>..] - ETA: 24s - loss: 0.6631 - acc: 0.5985
4672/4849 [===========================>..] - ETA: 18s - loss: 0.6631 - acc: 0.5985
4736/4849 [============================>.] - ETA: 11s - loss: 0.6632 - acc: 0.5990
4800/4849 [============================>.] - ETA: 5s - loss: 0.6631 - acc: 0.5990 
4849/4849 [==============================] - 520s 107ms/step - loss: 0.6622 - acc: 0.5999 - val_loss: 0.7407 - val_acc: 0.5436

Epoch 00007: val_acc did not improve from 0.56401
Epoch 8/10

  64/4849 [..............................] - ETA: 7:39 - loss: 0.6417 - acc: 0.6250
 128/4849 [..............................] - ETA: 7:41 - loss: 0.6829 - acc: 0.5391
 192/4849 [>.............................] - ETA: 7:46 - loss: 0.6778 - acc: 0.5677
 256/4849 [>.............................] - ETA: 7:43 - loss: 0.6594 - acc: 0.5898
 320/4849 [>.............................] - ETA: 7:41 - loss: 0.6631 - acc: 0.5906
 384/4849 [=>............................] - ETA: 7:33 - loss: 0.6641 - acc: 0.5885
 448/4849 [=>............................] - ETA: 7:32 - loss: 0.6541 - acc: 0.6071
 512/4849 [==>...........................] - ETA: 7:28 - loss: 0.6590 - acc: 0.6074
 576/4849 [==>...........................] - ETA: 7:19 - loss: 0.6544 - acc: 0.6163
 640/4849 [==>...........................] - ETA: 7:11 - loss: 0.6550 - acc: 0.6141
 704/4849 [===>..........................] - ETA: 7:04 - loss: 0.6563 - acc: 0.6151
 768/4849 [===>..........................] - ETA: 7:00 - loss: 0.6530 - acc: 0.6198
 832/4849 [====>.........................] - ETA: 6:53 - loss: 0.6561 - acc: 0.6106
 896/4849 [====>.........................] - ETA: 6:45 - loss: 0.6543 - acc: 0.6138
 960/4849 [====>.........................] - ETA: 6:38 - loss: 0.6500 - acc: 0.6229
1024/4849 [=====>........................] - ETA: 6:31 - loss: 0.6494 - acc: 0.6240
1088/4849 [=====>........................] - ETA: 6:23 - loss: 0.6458 - acc: 0.6268
1152/4849 [======>.......................] - ETA: 6:17 - loss: 0.6448 - acc: 0.6276
1216/4849 [======>.......................] - ETA: 6:10 - loss: 0.6471 - acc: 0.6258
1280/4849 [======>.......................] - ETA: 6:04 - loss: 0.6460 - acc: 0.6258
1344/4849 [=======>......................] - ETA: 5:58 - loss: 0.6474 - acc: 0.6235
1408/4849 [=======>......................] - ETA: 5:51 - loss: 0.6475 - acc: 0.6207
1472/4849 [========>.....................] - ETA: 5:44 - loss: 0.6502 - acc: 0.6175
1536/4849 [========>.....................] - ETA: 5:39 - loss: 0.6501 - acc: 0.6165
1600/4849 [========>.....................] - ETA: 5:32 - loss: 0.6507 - acc: 0.6150
1664/4849 [=========>....................] - ETA: 5:26 - loss: 0.6513 - acc: 0.6148
1728/4849 [=========>....................] - ETA: 5:19 - loss: 0.6520 - acc: 0.6128
1792/4849 [==========>...................] - ETA: 5:12 - loss: 0.6523 - acc: 0.6138
1856/4849 [==========>...................] - ETA: 5:06 - loss: 0.6519 - acc: 0.6148
1920/4849 [==========>...................] - ETA: 4:59 - loss: 0.6501 - acc: 0.6188
1984/4849 [===========>..................] - ETA: 4:52 - loss: 0.6515 - acc: 0.6184
2048/4849 [===========>..................] - ETA: 4:46 - loss: 0.6536 - acc: 0.6147
2112/4849 [============>.................] - ETA: 4:39 - loss: 0.6536 - acc: 0.6160
2176/4849 [============>.................] - ETA: 4:33 - loss: 0.6537 - acc: 0.6144
2240/4849 [============>.................] - ETA: 4:26 - loss: 0.6537 - acc: 0.6138
2304/4849 [=============>................] - ETA: 4:19 - loss: 0.6537 - acc: 0.6150
2368/4849 [=============>................] - ETA: 4:13 - loss: 0.6558 - acc: 0.6111
2432/4849 [==============>...............] - ETA: 4:07 - loss: 0.6567 - acc: 0.6086
2496/4849 [==============>...............] - ETA: 4:01 - loss: 0.6552 - acc: 0.6098
2560/4849 [==============>...............] - ETA: 3:54 - loss: 0.6551 - acc: 0.6102
2624/4849 [===============>..............] - ETA: 3:47 - loss: 0.6564 - acc: 0.6090
2688/4849 [===============>..............] - ETA: 3:41 - loss: 0.6570 - acc: 0.6112
2752/4849 [================>.............] - ETA: 3:35 - loss: 0.6560 - acc: 0.6126
2816/4849 [================>.............] - ETA: 3:28 - loss: 0.6558 - acc: 0.6126
2880/4849 [================>.............] - ETA: 3:22 - loss: 0.6564 - acc: 0.6115
2944/4849 [=================>............] - ETA: 3:15 - loss: 0.6562 - acc: 0.6114
3008/4849 [=================>............] - ETA: 3:08 - loss: 0.6565 - acc: 0.6120
3072/4849 [==================>...........] - ETA: 3:02 - loss: 0.6565 - acc: 0.6113
3136/4849 [==================>...........] - ETA: 2:55 - loss: 0.6555 - acc: 0.6129
3200/4849 [==================>...........] - ETA: 2:49 - loss: 0.6553 - acc: 0.6138
3264/4849 [===================>..........] - ETA: 2:42 - loss: 0.6565 - acc: 0.6121
3328/4849 [===================>..........] - ETA: 2:36 - loss: 0.6567 - acc: 0.6118
3392/4849 [===================>..........] - ETA: 2:30 - loss: 0.6561 - acc: 0.6117
3456/4849 [====================>.........] - ETA: 2:23 - loss: 0.6565 - acc: 0.6117
3520/4849 [====================>.........] - ETA: 2:16 - loss: 0.6568 - acc: 0.6114
3584/4849 [=====================>........] - ETA: 2:10 - loss: 0.6578 - acc: 0.6102
3648/4849 [=====================>........] - ETA: 2:03 - loss: 0.6584 - acc: 0.6091
3712/4849 [=====================>........] - ETA: 1:57 - loss: 0.6576 - acc: 0.6113
3776/4849 [======================>.......] - ETA: 1:50 - loss: 0.6580 - acc: 0.6099
3840/4849 [======================>.......] - ETA: 1:44 - loss: 0.6584 - acc: 0.6086
3904/4849 [=======================>......] - ETA: 1:38 - loss: 0.6584 - acc: 0.6094
3968/4849 [=======================>......] - ETA: 1:31 - loss: 0.6586 - acc: 0.6091
4032/4849 [=======================>......] - ETA: 1:24 - loss: 0.6588 - acc: 0.6086
4096/4849 [========================>.....] - ETA: 1:18 - loss: 0.6587 - acc: 0.6089
4160/4849 [========================>.....] - ETA: 1:11 - loss: 0.6595 - acc: 0.6079
4224/4849 [=========================>....] - ETA: 1:04 - loss: 0.6599 - acc: 0.6072
4288/4849 [=========================>....] - ETA: 58s - loss: 0.6598 - acc: 0.6082 
4352/4849 [=========================>....] - ETA: 51s - loss: 0.6595 - acc: 0.6091
4416/4849 [==========================>...] - ETA: 44s - loss: 0.6589 - acc: 0.6101
4480/4849 [==========================>...] - ETA: 38s - loss: 0.6587 - acc: 0.6098
4544/4849 [===========================>..] - ETA: 31s - loss: 0.6594 - acc: 0.6087
4608/4849 [===========================>..] - ETA: 24s - loss: 0.6598 - acc: 0.6087
4672/4849 [===========================>..] - ETA: 18s - loss: 0.6606 - acc: 0.6085
4736/4849 [============================>.] - ETA: 11s - loss: 0.6611 - acc: 0.6077
4800/4849 [============================>.] - ETA: 5s - loss: 0.6610 - acc: 0.6081 
4849/4849 [==============================] - 520s 107ms/step - loss: 0.6610 - acc: 0.6071 - val_loss: 0.6987 - val_acc: 0.5455

Epoch 00008: val_acc did not improve from 0.56401
Epoch 9/10

  64/4849 [..............................] - ETA: 8:17 - loss: 0.6474 - acc: 0.6094
 128/4849 [..............................] - ETA: 7:58 - loss: 0.6360 - acc: 0.6406
 192/4849 [>.............................] - ETA: 8:05 - loss: 0.6483 - acc: 0.6094
 256/4849 [>.............................] - ETA: 7:55 - loss: 0.6551 - acc: 0.5938
 320/4849 [>.............................] - ETA: 7:45 - loss: 0.6500 - acc: 0.6031
 384/4849 [=>............................] - ETA: 7:36 - loss: 0.6488 - acc: 0.6068
 448/4849 [=>............................] - ETA: 7:28 - loss: 0.6451 - acc: 0.6049
 512/4849 [==>...........................] - ETA: 7:24 - loss: 0.6430 - acc: 0.6035
 576/4849 [==>...........................] - ETA: 7:20 - loss: 0.6454 - acc: 0.5990
 640/4849 [==>...........................] - ETA: 7:13 - loss: 0.6469 - acc: 0.6094
 704/4849 [===>..........................] - ETA: 7:07 - loss: 0.6486 - acc: 0.6080
 768/4849 [===>..........................] - ETA: 6:59 - loss: 0.6476 - acc: 0.6159
 832/4849 [====>.........................] - ETA: 6:53 - loss: 0.6478 - acc: 0.6190
 896/4849 [====>.........................] - ETA: 6:46 - loss: 0.6509 - acc: 0.6138
 960/4849 [====>.........................] - ETA: 6:38 - loss: 0.6503 - acc: 0.6115
1024/4849 [=====>........................] - ETA: 6:31 - loss: 0.6523 - acc: 0.6084
1088/4849 [=====>........................] - ETA: 6:22 - loss: 0.6543 - acc: 0.6085
1152/4849 [======>.......................] - ETA: 6:15 - loss: 0.6580 - acc: 0.6042
1216/4849 [======>.......................] - ETA: 6:07 - loss: 0.6603 - acc: 0.6012
1280/4849 [======>.......................] - ETA: 5:59 - loss: 0.6598 - acc: 0.6000
1344/4849 [=======>......................] - ETA: 5:51 - loss: 0.6610 - acc: 0.6004
1408/4849 [=======>......................] - ETA: 5:42 - loss: 0.6601 - acc: 0.6030
1472/4849 [========>.....................] - ETA: 5:34 - loss: 0.6601 - acc: 0.6039
1536/4849 [========>.....................] - ETA: 5:25 - loss: 0.6610 - acc: 0.6022
1600/4849 [========>.....................] - ETA: 5:17 - loss: 0.6597 - acc: 0.6044
1664/4849 [=========>....................] - ETA: 5:08 - loss: 0.6607 - acc: 0.6052
1728/4849 [=========>....................] - ETA: 5:01 - loss: 0.6602 - acc: 0.6059
1792/4849 [==========>...................] - ETA: 4:53 - loss: 0.6589 - acc: 0.6071
1856/4849 [==========>...................] - ETA: 4:45 - loss: 0.6592 - acc: 0.6061
1920/4849 [==========>...................] - ETA: 4:37 - loss: 0.6601 - acc: 0.6057
1984/4849 [===========>..................] - ETA: 4:29 - loss: 0.6607 - acc: 0.6058
2048/4849 [===========>..................] - ETA: 4:22 - loss: 0.6610 - acc: 0.6060
2112/4849 [============>.................] - ETA: 4:15 - loss: 0.6616 - acc: 0.6056
2176/4849 [============>.................] - ETA: 4:07 - loss: 0.6594 - acc: 0.6089
2240/4849 [============>.................] - ETA: 4:00 - loss: 0.6597 - acc: 0.6062
2304/4849 [=============>................] - ETA: 3:53 - loss: 0.6601 - acc: 0.6063
2368/4849 [=============>................] - ETA: 3:45 - loss: 0.6607 - acc: 0.6056
2432/4849 [==============>...............] - ETA: 3:38 - loss: 0.6598 - acc: 0.6049
2496/4849 [==============>...............] - ETA: 3:31 - loss: 0.6616 - acc: 0.6030
2560/4849 [==============>...............] - ETA: 3:25 - loss: 0.6609 - acc: 0.6031
2624/4849 [===============>..............] - ETA: 3:18 - loss: 0.6615 - acc: 0.6021
2688/4849 [===============>..............] - ETA: 3:12 - loss: 0.6619 - acc: 0.5997
2752/4849 [================>.............] - ETA: 3:06 - loss: 0.6601 - acc: 0.6028
2816/4849 [================>.............] - ETA: 3:00 - loss: 0.6604 - acc: 0.6026
2880/4849 [================>.............] - ETA: 2:53 - loss: 0.6601 - acc: 0.6024
2944/4849 [=================>............] - ETA: 2:47 - loss: 0.6600 - acc: 0.6022
3008/4849 [=================>............] - ETA: 2:41 - loss: 0.6596 - acc: 0.6044
3072/4849 [==================>...........] - ETA: 2:35 - loss: 0.6590 - acc: 0.6042
3136/4849 [==================>...........] - ETA: 2:30 - loss: 0.6601 - acc: 0.6017
3200/4849 [==================>...........] - ETA: 2:24 - loss: 0.6607 - acc: 0.6006
3264/4849 [===================>..........] - ETA: 2:18 - loss: 0.6609 - acc: 0.6014
3328/4849 [===================>..........] - ETA: 2:11 - loss: 0.6612 - acc: 0.6010
3392/4849 [===================>..........] - ETA: 2:05 - loss: 0.6619 - acc: 0.6002
3456/4849 [====================>.........] - ETA: 1:59 - loss: 0.6625 - acc: 0.5992
3520/4849 [====================>.........] - ETA: 1:53 - loss: 0.6629 - acc: 0.5989
3584/4849 [=====================>........] - ETA: 1:48 - loss: 0.6626 - acc: 0.5988
3648/4849 [=====================>........] - ETA: 1:42 - loss: 0.6627 - acc: 0.5987
3712/4849 [=====================>........] - ETA: 1:36 - loss: 0.6623 - acc: 0.5999
3776/4849 [======================>.......] - ETA: 1:30 - loss: 0.6615 - acc: 0.6017
3840/4849 [======================>.......] - ETA: 1:25 - loss: 0.6611 - acc: 0.6023
3904/4849 [=======================>......] - ETA: 1:19 - loss: 0.6609 - acc: 0.6027
3968/4849 [=======================>......] - ETA: 1:13 - loss: 0.6610 - acc: 0.6018
4032/4849 [=======================>......] - ETA: 1:08 - loss: 0.6614 - acc: 0.6017
4096/4849 [========================>.....] - ETA: 1:02 - loss: 0.6612 - acc: 0.6018
4160/4849 [========================>.....] - ETA: 57s - loss: 0.6613 - acc: 0.6012 
4224/4849 [=========================>....] - ETA: 51s - loss: 0.6613 - acc: 0.6013
4288/4849 [=========================>....] - ETA: 46s - loss: 0.6611 - acc: 0.6010
4352/4849 [=========================>....] - ETA: 41s - loss: 0.6604 - acc: 0.6018
4416/4849 [==========================>...] - ETA: 35s - loss: 0.6600 - acc: 0.6021
4480/4849 [==========================>...] - ETA: 30s - loss: 0.6597 - acc: 0.6027
4544/4849 [===========================>..] - ETA: 25s - loss: 0.6601 - acc: 0.6021
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6602 - acc: 0.6013
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6595 - acc: 0.6019
4736/4849 [============================>.] - ETA: 9s - loss: 0.6588 - acc: 0.6033 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6593 - acc: 0.6025
4849/4849 [==============================] - 406s 84ms/step - loss: 0.6595 - acc: 0.6024 - val_loss: 0.7338 - val_acc: 0.5362

Epoch 00009: val_acc did not improve from 0.56401
Epoch 10/10

  64/4849 [..............................] - ETA: 5:49 - loss: 0.6246 - acc: 0.6406
 128/4849 [..............................] - ETA: 5:37 - loss: 0.6458 - acc: 0.5938
 192/4849 [>.............................] - ETA: 5:21 - loss: 0.6580 - acc: 0.6146
 256/4849 [>.............................] - ETA: 5:16 - loss: 0.6441 - acc: 0.6445
 320/4849 [>.............................] - ETA: 5:07 - loss: 0.6495 - acc: 0.6406
 384/4849 [=>............................] - ETA: 5:03 - loss: 0.6535 - acc: 0.6276
 448/4849 [=>............................] - ETA: 4:57 - loss: 0.6540 - acc: 0.6272
 512/4849 [==>...........................] - ETA: 4:57 - loss: 0.6494 - acc: 0.6250
 576/4849 [==>...........................] - ETA: 4:53 - loss: 0.6508 - acc: 0.6215
 640/4849 [==>...........................] - ETA: 4:50 - loss: 0.6484 - acc: 0.6266
 704/4849 [===>..........................] - ETA: 4:46 - loss: 0.6491 - acc: 0.6264
 768/4849 [===>..........................] - ETA: 4:42 - loss: 0.6517 - acc: 0.6211
 832/4849 [====>.........................] - ETA: 4:37 - loss: 0.6523 - acc: 0.6154
 896/4849 [====>.........................] - ETA: 4:33 - loss: 0.6550 - acc: 0.6083
 960/4849 [====>.........................] - ETA: 4:30 - loss: 0.6504 - acc: 0.6177
1024/4849 [=====>........................] - ETA: 4:25 - loss: 0.6504 - acc: 0.6182
1088/4849 [=====>........................] - ETA: 4:19 - loss: 0.6498 - acc: 0.6213
1152/4849 [======>.......................] - ETA: 4:16 - loss: 0.6501 - acc: 0.6207
1216/4849 [======>.......................] - ETA: 4:09 - loss: 0.6474 - acc: 0.6250
1280/4849 [======>.......................] - ETA: 4:05 - loss: 0.6462 - acc: 0.6242
1344/4849 [=======>......................] - ETA: 4:00 - loss: 0.6471 - acc: 0.6243
1408/4849 [=======>......................] - ETA: 3:55 - loss: 0.6454 - acc: 0.6250
1472/4849 [========>.....................] - ETA: 3:50 - loss: 0.6458 - acc: 0.6250
1536/4849 [========>.....................] - ETA: 3:45 - loss: 0.6475 - acc: 0.6204
1600/4849 [========>.....................] - ETA: 3:39 - loss: 0.6511 - acc: 0.6162
1664/4849 [=========>....................] - ETA: 3:35 - loss: 0.6503 - acc: 0.6184
1728/4849 [=========>....................] - ETA: 3:31 - loss: 0.6498 - acc: 0.6186
1792/4849 [==========>...................] - ETA: 3:27 - loss: 0.6512 - acc: 0.6183
1856/4849 [==========>...................] - ETA: 3:22 - loss: 0.6522 - acc: 0.6180
1920/4849 [==========>...................] - ETA: 3:18 - loss: 0.6533 - acc: 0.6172
1984/4849 [===========>..................] - ETA: 3:14 - loss: 0.6531 - acc: 0.6174
2048/4849 [===========>..................] - ETA: 3:10 - loss: 0.6537 - acc: 0.6162
2112/4849 [============>.................] - ETA: 3:05 - loss: 0.6531 - acc: 0.6151
2176/4849 [============>.................] - ETA: 3:01 - loss: 0.6527 - acc: 0.6158
2240/4849 [============>.................] - ETA: 2:56 - loss: 0.6524 - acc: 0.6165
2304/4849 [=============>................] - ETA: 2:52 - loss: 0.6532 - acc: 0.6159
2368/4849 [=============>................] - ETA: 2:47 - loss: 0.6530 - acc: 0.6153
2432/4849 [==============>...............] - ETA: 2:43 - loss: 0.6545 - acc: 0.6139
2496/4849 [==============>...............] - ETA: 2:38 - loss: 0.6546 - acc: 0.6146
2560/4849 [==============>...............] - ETA: 2:34 - loss: 0.6544 - acc: 0.6152
2624/4849 [===============>..............] - ETA: 2:30 - loss: 0.6541 - acc: 0.6159
2688/4849 [===============>..............] - ETA: 2:25 - loss: 0.6530 - acc: 0.6164
2752/4849 [================>.............] - ETA: 2:21 - loss: 0.6531 - acc: 0.6174
2816/4849 [================>.............] - ETA: 2:17 - loss: 0.6532 - acc: 0.6179
2880/4849 [================>.............] - ETA: 2:12 - loss: 0.6531 - acc: 0.6181
2944/4849 [=================>............] - ETA: 2:08 - loss: 0.6535 - acc: 0.6172
3008/4849 [=================>............] - ETA: 2:04 - loss: 0.6530 - acc: 0.6187
3072/4849 [==================>...........] - ETA: 1:59 - loss: 0.6518 - acc: 0.6191
3136/4849 [==================>...........] - ETA: 1:55 - loss: 0.6515 - acc: 0.6189
3200/4849 [==================>...........] - ETA: 1:51 - loss: 0.6521 - acc: 0.6188
3264/4849 [===================>..........] - ETA: 1:47 - loss: 0.6520 - acc: 0.6189
3328/4849 [===================>..........] - ETA: 1:43 - loss: 0.6529 - acc: 0.6175
3392/4849 [===================>..........] - ETA: 1:38 - loss: 0.6524 - acc: 0.6188
3456/4849 [====================>.........] - ETA: 1:34 - loss: 0.6526 - acc: 0.6189
3520/4849 [====================>.........] - ETA: 1:30 - loss: 0.6522 - acc: 0.6202
3584/4849 [=====================>........] - ETA: 1:25 - loss: 0.6536 - acc: 0.6177
3648/4849 [=====================>........] - ETA: 1:21 - loss: 0.6544 - acc: 0.6165
3712/4849 [=====================>........] - ETA: 1:16 - loss: 0.6538 - acc: 0.6161
3776/4849 [======================>.......] - ETA: 1:12 - loss: 0.6544 - acc: 0.6160
3840/4849 [======================>.......] - ETA: 1:08 - loss: 0.6549 - acc: 0.6151
3904/4849 [=======================>......] - ETA: 1:04 - loss: 0.6555 - acc: 0.6145
3968/4849 [=======================>......] - ETA: 59s - loss: 0.6553 - acc: 0.6159 
4032/4849 [=======================>......] - ETA: 55s - loss: 0.6562 - acc: 0.6141
4096/4849 [========================>.....] - ETA: 51s - loss: 0.6565 - acc: 0.6140
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6562 - acc: 0.6142
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6566 - acc: 0.6141
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6563 - acc: 0.6147
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6570 - acc: 0.6147
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6569 - acc: 0.6155
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6566 - acc: 0.6163
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6565 - acc: 0.6166
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6564 - acc: 0.6176
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6561 - acc: 0.6179
4736/4849 [============================>.] - ETA: 7s - loss: 0.6565 - acc: 0.6172 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6566 - acc: 0.6177
4849/4849 [==============================] - 340s 70ms/step - loss: 0.6563 - acc: 0.6183 - val_loss: 0.7145 - val_acc: 0.5547

Epoch 00010: val_acc did not improve from 0.56401
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7bc26707d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7bc26707d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7bc248ccd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7bc248ccd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc22dd090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc22dd090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc20bae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc20bae50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bc1fee6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bc1fee6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc19bf510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc19bf510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc19bf3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc19bf3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc1eb3290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc1eb3290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc1b54f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc1b54f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bc1f434d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bc1f434d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc1f97890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc1f97890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc20bafd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc20bafd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc2155410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc2155410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc1d003d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc1d003d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bc1c5fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bc1c5fe10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc1f3a110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc1f3a110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc19dff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc19dff10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7550517f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7550517f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc1998050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc1998050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f75504cef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f75504cef90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc187f890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc187f890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc1998c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc1998c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc183b050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc183b050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc1793e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc1793e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bc155bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bc155bc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc18fc9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc18fc9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc169dd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc169dd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb9406d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb9406d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc152aa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc152aa50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb91e5f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb91e5f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc1689190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc1689190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc15c0a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bc15c0a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb9293610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb9293610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bb9032190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bb9032190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb9038750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb9038750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb9232310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb9232310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bb9032750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bb9032750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb8d8db10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb8d8db10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bb8cfea50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bb8cfea50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb8bae590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb8bae590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb8cd87d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb8cd87d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bb8d54490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bb8d54490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb8e009d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb8e009d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bb8c8d110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bb8c8d110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb8868bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb8868bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb89fc8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb89fc8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bb8a98bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bb8a98bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb897af10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb897af10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7550567bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7550567bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb856e710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb856e710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb88493d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb88493d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7550537350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7550537350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb85ba650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb85ba650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bb843efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bb843efd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb8223d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb8223d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb85c00d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb85c00d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bb843ed10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bb843ed10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb810b390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb810b390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bb8027110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bb8027110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb7f376d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bb7f376d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb7f34190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb7f34190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bb8027710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bb8027710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb7f79390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bb7f79390>>: AttributeError: module 'gast' has no attribute 'Str'
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 1:25
 128/1348 [=>............................] - ETA: 56s 
 192/1348 [===>..........................] - ETA: 47s
 256/1348 [====>.........................] - ETA: 42s
 320/1348 [======>.......................] - ETA: 37s
 384/1348 [=======>......................] - ETA: 34s
 448/1348 [========>.....................] - ETA: 30s
 512/1348 [==========>...................] - ETA: 28s
 576/1348 [===========>..................] - ETA: 25s
 640/1348 [=============>................] - ETA: 23s
 704/1348 [==============>...............] - ETA: 20s
 768/1348 [================>.............] - ETA: 18s
 832/1348 [=================>............] - ETA: 16s
 896/1348 [==================>...........] - ETA: 14s
 960/1348 [====================>.........] - ETA: 12s
1024/1348 [=====================>........] - ETA: 10s
1088/1348 [=======================>......] - ETA: 8s 
1152/1348 [========================>.....] - ETA: 6s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 42s 31ms/step
loss: 0.6654181202724355
acc: 0.607566765578635
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f75e8584f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f75e8584f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f75504c93d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f75504c93d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86d2b990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86d2b990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b86e20410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b86e20410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b86d5e810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b86d5e810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86e20750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86e20750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b86e20210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b86e20210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc24e16d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc24e16d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc25d8890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bc25d8890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f75503d4190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f75503d4190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc24c10d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc24c10d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b86df4b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b86df4b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7550245690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7550245690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7550210190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7550210190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f75500d1650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f75500d1650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f755022bdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f755022bdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7550210a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7550210a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7550108f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7550108f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7530689a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7530689a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f75305af750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f75305af750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7530653e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7530653e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f75306898d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f75306898d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7530664a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7530664a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f753068e990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f753068e990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f753024e310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f753024e310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f753037d110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f753037d110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f75303d3910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f75303d3910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f753024a890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f753024a890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f75300e9c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f75300e9c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f753003ce90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f753003ce90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f753009a190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f753009a190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f75303cacd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f75303cacd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f75106213d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f75106213d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f75105255d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f75105255d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7510629c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7510629c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f753004f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f753004f610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7510525210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7510525210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f75104aefd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f75104aefd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f75101d0190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f75101d0190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f75100ea0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f75100ea0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f75102ce650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f75102ce650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7510403b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7510403b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f751014b050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f751014b050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74f063c850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74f063c850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74f06b53d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74f06b53d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f75101d7d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f75101d7d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f75104199d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f75104199d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7510123090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7510123090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74f0596150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74f0596150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73943a6090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73943a6090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73942ad3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73942ad3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f751054e790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f751054e790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73941622d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73941622d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73942b2810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73942b2810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f739408c290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f739408c290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73942b8750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73942b8750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f73707f1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f73707f1c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73705c4290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73705c4290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73704c9190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73704c9190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73703e2210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73703e2210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86dac2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86dac2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f73704c97d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f73704c97d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73702e4450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73702e4450>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 25:33 - loss: 0.8144 - acc: 0.3906
 128/4849 [..............................] - ETA: 15:57 - loss: 0.7696 - acc: 0.4766
 192/4849 [>.............................] - ETA: 12:21 - loss: 0.7567 - acc: 0.5000
 256/4849 [>.............................] - ETA: 10:40 - loss: 0.7715 - acc: 0.4844
 320/4849 [>.............................] - ETA: 9:42 - loss: 0.7715 - acc: 0.4688 
 384/4849 [=>............................] - ETA: 8:58 - loss: 0.7648 - acc: 0.4870
 448/4849 [=>............................] - ETA: 8:21 - loss: 0.7588 - acc: 0.4978
 512/4849 [==>...........................] - ETA: 7:55 - loss: 0.7603 - acc: 0.4961
 576/4849 [==>...........................] - ETA: 7:33 - loss: 0.7502 - acc: 0.5052
 640/4849 [==>...........................] - ETA: 7:15 - loss: 0.7490 - acc: 0.5016
 704/4849 [===>..........................] - ETA: 7:00 - loss: 0.7449 - acc: 0.5085
 768/4849 [===>..........................] - ETA: 6:46 - loss: 0.7412 - acc: 0.5169
 832/4849 [====>.........................] - ETA: 6:34 - loss: 0.7427 - acc: 0.5144
 896/4849 [====>.........................] - ETA: 6:25 - loss: 0.7438 - acc: 0.5089
 960/4849 [====>.........................] - ETA: 6:13 - loss: 0.7409 - acc: 0.5104
1024/4849 [=====>........................] - ETA: 6:04 - loss: 0.7368 - acc: 0.5215
1088/4849 [=====>........................] - ETA: 5:54 - loss: 0.7351 - acc: 0.5248
1152/4849 [======>.......................] - ETA: 5:45 - loss: 0.7311 - acc: 0.5295
1216/4849 [======>.......................] - ETA: 5:37 - loss: 0.7313 - acc: 0.5329
1280/4849 [======>.......................] - ETA: 5:29 - loss: 0.7352 - acc: 0.5289
1344/4849 [=======>......................] - ETA: 5:20 - loss: 0.7337 - acc: 0.5275
1408/4849 [=======>......................] - ETA: 5:12 - loss: 0.7355 - acc: 0.5234
1472/4849 [========>.....................] - ETA: 5:04 - loss: 0.7381 - acc: 0.5245
1536/4849 [========>.....................] - ETA: 4:57 - loss: 0.7363 - acc: 0.5260
1600/4849 [========>.....................] - ETA: 4:50 - loss: 0.7365 - acc: 0.5250
1664/4849 [=========>....................] - ETA: 4:42 - loss: 0.7346 - acc: 0.5228
1728/4849 [=========>....................] - ETA: 4:35 - loss: 0.7329 - acc: 0.5220
1792/4849 [==========>...................] - ETA: 4:28 - loss: 0.7312 - acc: 0.5229
1856/4849 [==========>...................] - ETA: 4:21 - loss: 0.7324 - acc: 0.5205
1920/4849 [==========>...................] - ETA: 4:14 - loss: 0.7353 - acc: 0.5188
1984/4849 [===========>..................] - ETA: 4:08 - loss: 0.7351 - acc: 0.5207
2048/4849 [===========>..................] - ETA: 4:02 - loss: 0.7348 - acc: 0.5190
2112/4849 [============>.................] - ETA: 3:56 - loss: 0.7356 - acc: 0.5166
2176/4849 [============>.................] - ETA: 3:49 - loss: 0.7344 - acc: 0.5184
2240/4849 [============>.................] - ETA: 3:43 - loss: 0.7334 - acc: 0.5214
2304/4849 [=============>................] - ETA: 3:37 - loss: 0.7338 - acc: 0.5200
2368/4849 [=============>................] - ETA: 3:31 - loss: 0.7336 - acc: 0.5198
2432/4849 [==============>...............] - ETA: 3:25 - loss: 0.7310 - acc: 0.5226
2496/4849 [==============>...............] - ETA: 3:19 - loss: 0.7323 - acc: 0.5204
2560/4849 [==============>...............] - ETA: 3:14 - loss: 0.7324 - acc: 0.5199
2624/4849 [===============>..............] - ETA: 3:08 - loss: 0.7335 - acc: 0.5198
2688/4849 [===============>..............] - ETA: 3:02 - loss: 0.7320 - acc: 0.5231
2752/4849 [================>.............] - ETA: 2:57 - loss: 0.7327 - acc: 0.5211
2816/4849 [================>.............] - ETA: 2:51 - loss: 0.7313 - acc: 0.5234
2880/4849 [================>.............] - ETA: 2:46 - loss: 0.7315 - acc: 0.5229
2944/4849 [=================>............] - ETA: 2:40 - loss: 0.7309 - acc: 0.5231
3008/4849 [=================>............] - ETA: 2:34 - loss: 0.7314 - acc: 0.5206
3072/4849 [==================>...........] - ETA: 2:29 - loss: 0.7311 - acc: 0.5212
3136/4849 [==================>...........] - ETA: 2:23 - loss: 0.7298 - acc: 0.5214
3200/4849 [==================>...........] - ETA: 2:18 - loss: 0.7298 - acc: 0.5212
3264/4849 [===================>..........] - ETA: 2:12 - loss: 0.7292 - acc: 0.5221
3328/4849 [===================>..........] - ETA: 2:07 - loss: 0.7286 - acc: 0.5231
3392/4849 [===================>..........] - ETA: 2:01 - loss: 0.7266 - acc: 0.5256
3456/4849 [====================>.........] - ETA: 1:56 - loss: 0.7264 - acc: 0.5260
3520/4849 [====================>.........] - ETA: 1:51 - loss: 0.7264 - acc: 0.5259
3584/4849 [=====================>........] - ETA: 1:45 - loss: 0.7271 - acc: 0.5240
3648/4849 [=====================>........] - ETA: 1:40 - loss: 0.7271 - acc: 0.5236
3712/4849 [=====================>........] - ETA: 1:34 - loss: 0.7266 - acc: 0.5248
3776/4849 [======================>.......] - ETA: 1:29 - loss: 0.7262 - acc: 0.5249
3840/4849 [======================>.......] - ETA: 1:24 - loss: 0.7258 - acc: 0.5245
3904/4849 [=======================>......] - ETA: 1:19 - loss: 0.7261 - acc: 0.5228
3968/4849 [=======================>......] - ETA: 1:14 - loss: 0.7257 - acc: 0.5229
4032/4849 [=======================>......] - ETA: 1:08 - loss: 0.7257 - acc: 0.5213
4096/4849 [========================>.....] - ETA: 1:03 - loss: 0.7245 - acc: 0.5232
4160/4849 [========================>.....] - ETA: 58s - loss: 0.7247 - acc: 0.5214 
4224/4849 [=========================>....] - ETA: 52s - loss: 0.7242 - acc: 0.5213
4288/4849 [=========================>....] - ETA: 47s - loss: 0.7234 - acc: 0.5224
4352/4849 [=========================>....] - ETA: 42s - loss: 0.7230 - acc: 0.5223
4416/4849 [==========================>...] - ETA: 36s - loss: 0.7223 - acc: 0.5240
4480/4849 [==========================>...] - ETA: 31s - loss: 0.7220 - acc: 0.5241
4544/4849 [===========================>..] - ETA: 25s - loss: 0.7213 - acc: 0.5246
4608/4849 [===========================>..] - ETA: 20s - loss: 0.7212 - acc: 0.5241
4672/4849 [===========================>..] - ETA: 15s - loss: 0.7209 - acc: 0.5242
4736/4849 [============================>.] - ETA: 9s - loss: 0.7209 - acc: 0.5245 
4800/4849 [============================>.] - ETA: 4s - loss: 0.7211 - acc: 0.5240
4849/4849 [==============================] - 435s 90ms/step - loss: 0.7209 - acc: 0.5238 - val_loss: 0.6858 - val_acc: 0.5696

Epoch 00001: val_acc improved from -inf to 0.56957, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window13/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 6:58 - loss: 0.6837 - acc: 0.5469
 128/4849 [..............................] - ETA: 7:11 - loss: 0.6722 - acc: 0.5547
 192/4849 [>.............................] - ETA: 7:05 - loss: 0.6944 - acc: 0.5469
 256/4849 [>.............................] - ETA: 6:59 - loss: 0.6774 - acc: 0.5664
 320/4849 [>.............................] - ETA: 6:49 - loss: 0.6773 - acc: 0.5594
 384/4849 [=>............................] - ETA: 6:44 - loss: 0.6778 - acc: 0.5625
 448/4849 [=>............................] - ETA: 6:39 - loss: 0.6846 - acc: 0.5580
 512/4849 [==>...........................] - ETA: 6:34 - loss: 0.6924 - acc: 0.5469
 576/4849 [==>...........................] - ETA: 6:29 - loss: 0.6896 - acc: 0.5590
 640/4849 [==>...........................] - ETA: 6:22 - loss: 0.6934 - acc: 0.5500
 704/4849 [===>..........................] - ETA: 6:17 - loss: 0.6931 - acc: 0.5497
 768/4849 [===>..........................] - ETA: 6:10 - loss: 0.6900 - acc: 0.5547
 832/4849 [====>.........................] - ETA: 6:05 - loss: 0.6952 - acc: 0.5541
 896/4849 [====>.........................] - ETA: 6:02 - loss: 0.6957 - acc: 0.5547
 960/4849 [====>.........................] - ETA: 5:55 - loss: 0.6964 - acc: 0.5563
1024/4849 [=====>........................] - ETA: 5:49 - loss: 0.6952 - acc: 0.5576
1088/4849 [=====>........................] - ETA: 5:43 - loss: 0.6922 - acc: 0.5634
1152/4849 [======>.......................] - ETA: 5:37 - loss: 0.6923 - acc: 0.5625
1216/4849 [======>.......................] - ETA: 5:30 - loss: 0.6916 - acc: 0.5600
1280/4849 [======>.......................] - ETA: 5:25 - loss: 0.6900 - acc: 0.5641
1344/4849 [=======>......................] - ETA: 5:18 - loss: 0.6892 - acc: 0.5640
1408/4849 [=======>......................] - ETA: 5:13 - loss: 0.6886 - acc: 0.5618
1472/4849 [========>.....................] - ETA: 5:06 - loss: 0.6891 - acc: 0.5618
1536/4849 [========>.....................] - ETA: 5:01 - loss: 0.6883 - acc: 0.5625
1600/4849 [========>.....................] - ETA: 4:55 - loss: 0.6890 - acc: 0.5613
1664/4849 [=========>....................] - ETA: 4:49 - loss: 0.6881 - acc: 0.5631
1728/4849 [=========>....................] - ETA: 4:44 - loss: 0.6873 - acc: 0.5642
1792/4849 [==========>...................] - ETA: 4:37 - loss: 0.6876 - acc: 0.5636
1856/4849 [==========>...................] - ETA: 4:31 - loss: 0.6874 - acc: 0.5630
1920/4849 [==========>...................] - ETA: 4:25 - loss: 0.6868 - acc: 0.5651
1984/4849 [===========>..................] - ETA: 4:20 - loss: 0.6862 - acc: 0.5645
2048/4849 [===========>..................] - ETA: 4:14 - loss: 0.6885 - acc: 0.5601
2112/4849 [============>.................] - ETA: 4:08 - loss: 0.6884 - acc: 0.5597
2176/4849 [============>.................] - ETA: 4:02 - loss: 0.6895 - acc: 0.5579
2240/4849 [============>.................] - ETA: 3:57 - loss: 0.6903 - acc: 0.5567
2304/4849 [=============>................] - ETA: 3:50 - loss: 0.6893 - acc: 0.5577
2368/4849 [=============>................] - ETA: 3:45 - loss: 0.6899 - acc: 0.5553
2432/4849 [==============>...............] - ETA: 3:39 - loss: 0.6891 - acc: 0.5576
2496/4849 [==============>...............] - ETA: 3:33 - loss: 0.6900 - acc: 0.5565
2560/4849 [==============>...............] - ETA: 3:27 - loss: 0.6909 - acc: 0.5551
2624/4849 [===============>..............] - ETA: 3:21 - loss: 0.6907 - acc: 0.5553
2688/4849 [===============>..............] - ETA: 3:16 - loss: 0.6904 - acc: 0.5543
2752/4849 [================>.............] - ETA: 3:09 - loss: 0.6907 - acc: 0.5516
2816/4849 [================>.............] - ETA: 3:04 - loss: 0.6914 - acc: 0.5501
2880/4849 [================>.............] - ETA: 2:58 - loss: 0.6917 - acc: 0.5483
2944/4849 [=================>............] - ETA: 2:52 - loss: 0.6918 - acc: 0.5496
3008/4849 [=================>............] - ETA: 2:46 - loss: 0.6915 - acc: 0.5502
3072/4849 [==================>...........] - ETA: 2:41 - loss: 0.6914 - acc: 0.5498
3136/4849 [==================>...........] - ETA: 2:35 - loss: 0.6932 - acc: 0.5466
3200/4849 [==================>...........] - ETA: 2:29 - loss: 0.6936 - acc: 0.5463
3264/4849 [===================>..........] - ETA: 2:23 - loss: 0.6954 - acc: 0.5441
3328/4849 [===================>..........] - ETA: 2:17 - loss: 0.6949 - acc: 0.5451
3392/4849 [===================>..........] - ETA: 2:12 - loss: 0.6954 - acc: 0.5436
3456/4849 [====================>.........] - ETA: 2:06 - loss: 0.6952 - acc: 0.5440
3520/4849 [====================>.........] - ETA: 2:00 - loss: 0.6953 - acc: 0.5446
3584/4849 [=====================>........] - ETA: 1:54 - loss: 0.6956 - acc: 0.5435
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.6955 - acc: 0.5444
3712/4849 [=====================>........] - ETA: 1:43 - loss: 0.6958 - acc: 0.5434
3776/4849 [======================>.......] - ETA: 1:37 - loss: 0.6967 - acc: 0.5429
3840/4849 [======================>.......] - ETA: 1:31 - loss: 0.6966 - acc: 0.5430
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6970 - acc: 0.5412
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6974 - acc: 0.5406
4032/4849 [=======================>......] - ETA: 1:14 - loss: 0.6976 - acc: 0.5392
4096/4849 [========================>.....] - ETA: 1:08 - loss: 0.6973 - acc: 0.5408
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6966 - acc: 0.5423
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6959 - acc: 0.5440 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6953 - acc: 0.5450
4352/4849 [=========================>....] - ETA: 45s - loss: 0.6948 - acc: 0.5462
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6943 - acc: 0.5478
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6943 - acc: 0.5475
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6937 - acc: 0.5480
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6939 - acc: 0.5477
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6945 - acc: 0.5475
4736/4849 [============================>.] - ETA: 10s - loss: 0.6943 - acc: 0.5473
4800/4849 [============================>.] - ETA: 4s - loss: 0.6938 - acc: 0.5483 
4849/4849 [==============================] - 457s 94ms/step - loss: 0.6942 - acc: 0.5477 - val_loss: 0.6931 - val_acc: 0.5083

Epoch 00002: val_acc did not improve from 0.56957
Epoch 3/10

  64/4849 [..............................] - ETA: 7:05 - loss: 0.6446 - acc: 0.6406
 128/4849 [..............................] - ETA: 7:03 - loss: 0.6628 - acc: 0.5938
 192/4849 [>.............................] - ETA: 6:52 - loss: 0.6472 - acc: 0.6406
 256/4849 [>.............................] - ETA: 6:52 - loss: 0.6553 - acc: 0.6172
 320/4849 [>.............................] - ETA: 6:38 - loss: 0.6533 - acc: 0.6188
 384/4849 [=>............................] - ETA: 6:35 - loss: 0.6505 - acc: 0.6354
 448/4849 [=>............................] - ETA: 6:30 - loss: 0.6659 - acc: 0.6027
 512/4849 [==>...........................] - ETA: 6:24 - loss: 0.6770 - acc: 0.5879
 576/4849 [==>...........................] - ETA: 6:16 - loss: 0.6825 - acc: 0.5799
 640/4849 [==>...........................] - ETA: 6:12 - loss: 0.6840 - acc: 0.5750
 704/4849 [===>..........................] - ETA: 6:07 - loss: 0.6873 - acc: 0.5639
 768/4849 [===>..........................] - ETA: 6:01 - loss: 0.6883 - acc: 0.5612
 832/4849 [====>.........................] - ETA: 5:56 - loss: 0.6877 - acc: 0.5601
 896/4849 [====>.........................] - ETA: 5:52 - loss: 0.6878 - acc: 0.5625
 960/4849 [====>.........................] - ETA: 5:46 - loss: 0.6855 - acc: 0.5677
1024/4849 [=====>........................] - ETA: 5:39 - loss: 0.6869 - acc: 0.5674
1088/4849 [=====>........................] - ETA: 5:33 - loss: 0.6877 - acc: 0.5625
1152/4849 [======>.......................] - ETA: 5:26 - loss: 0.6877 - acc: 0.5608
1216/4849 [======>.......................] - ETA: 5:21 - loss: 0.6869 - acc: 0.5617
1280/4849 [======>.......................] - ETA: 5:16 - loss: 0.6871 - acc: 0.5617
1344/4849 [=======>......................] - ETA: 5:10 - loss: 0.6862 - acc: 0.5625
1408/4849 [=======>......................] - ETA: 5:04 - loss: 0.6860 - acc: 0.5625
1472/4849 [========>.....................] - ETA: 4:58 - loss: 0.6871 - acc: 0.5584
1536/4849 [========>.....................] - ETA: 4:53 - loss: 0.6867 - acc: 0.5618
1600/4849 [========>.....................] - ETA: 4:47 - loss: 0.6864 - acc: 0.5625
1664/4849 [=========>....................] - ETA: 4:41 - loss: 0.6863 - acc: 0.5637
1728/4849 [=========>....................] - ETA: 4:35 - loss: 0.6864 - acc: 0.5631
1792/4849 [==========>...................] - ETA: 4:31 - loss: 0.6858 - acc: 0.5653
1856/4849 [==========>...................] - ETA: 4:24 - loss: 0.6861 - acc: 0.5641
1920/4849 [==========>...................] - ETA: 4:19 - loss: 0.6854 - acc: 0.5656
1984/4849 [===========>..................] - ETA: 4:13 - loss: 0.6849 - acc: 0.5665
2048/4849 [===========>..................] - ETA: 4:07 - loss: 0.6853 - acc: 0.5645
2112/4849 [============>.................] - ETA: 4:02 - loss: 0.6843 - acc: 0.5672
2176/4849 [============>.................] - ETA: 3:56 - loss: 0.6853 - acc: 0.5643
2240/4849 [============>.................] - ETA: 3:50 - loss: 0.6865 - acc: 0.5621
2304/4849 [=============>................] - ETA: 3:45 - loss: 0.6875 - acc: 0.5625
2368/4849 [=============>................] - ETA: 3:39 - loss: 0.6872 - acc: 0.5638
2432/4849 [==============>...............] - ETA: 3:33 - loss: 0.6869 - acc: 0.5650
2496/4849 [==============>...............] - ETA: 3:28 - loss: 0.6863 - acc: 0.5673
2560/4849 [==============>...............] - ETA: 3:22 - loss: 0.6869 - acc: 0.5684
2624/4849 [===============>..............] - ETA: 3:16 - loss: 0.6868 - acc: 0.5682
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.6872 - acc: 0.5673
2752/4849 [================>.............] - ETA: 3:05 - loss: 0.6884 - acc: 0.5647
2816/4849 [================>.............] - ETA: 3:00 - loss: 0.6887 - acc: 0.5653
2880/4849 [================>.............] - ETA: 2:54 - loss: 0.6891 - acc: 0.5639
2944/4849 [=================>............] - ETA: 2:48 - loss: 0.6889 - acc: 0.5645
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6892 - acc: 0.5628
3072/4849 [==================>...........] - ETA: 2:37 - loss: 0.6895 - acc: 0.5618
3136/4849 [==================>...........] - ETA: 2:31 - loss: 0.6896 - acc: 0.5606
3200/4849 [==================>...........] - ETA: 2:25 - loss: 0.6890 - acc: 0.5619
3264/4849 [===================>..........] - ETA: 2:20 - loss: 0.6891 - acc: 0.5616
3328/4849 [===================>..........] - ETA: 2:14 - loss: 0.6882 - acc: 0.5625
3392/4849 [===================>..........] - ETA: 2:09 - loss: 0.6883 - acc: 0.5625
3456/4849 [====================>.........] - ETA: 2:03 - loss: 0.6882 - acc: 0.5622
3520/4849 [====================>.........] - ETA: 1:57 - loss: 0.6882 - acc: 0.5625
3584/4849 [=====================>........] - ETA: 1:51 - loss: 0.6881 - acc: 0.5619
3648/4849 [=====================>........] - ETA: 1:46 - loss: 0.6882 - acc: 0.5611
3712/4849 [=====================>........] - ETA: 1:40 - loss: 0.6884 - acc: 0.5601
3776/4849 [======================>.......] - ETA: 1:34 - loss: 0.6884 - acc: 0.5612
3840/4849 [======================>.......] - ETA: 1:29 - loss: 0.6886 - acc: 0.5620
3904/4849 [=======================>......] - ETA: 1:23 - loss: 0.6887 - acc: 0.5617
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6883 - acc: 0.5625
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6886 - acc: 0.5615
4096/4849 [========================>.....] - ETA: 1:06 - loss: 0.6877 - acc: 0.5635
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6876 - acc: 0.5639
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6882 - acc: 0.5630 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6879 - acc: 0.5644
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6878 - acc: 0.5646
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6883 - acc: 0.5641
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6878 - acc: 0.5650
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6877 - acc: 0.5645
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6873 - acc: 0.5645
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6871 - acc: 0.5657
4736/4849 [============================>.] - ETA: 10s - loss: 0.6869 - acc: 0.5655
4800/4849 [============================>.] - ETA: 4s - loss: 0.6864 - acc: 0.5671 
4849/4849 [==============================] - 452s 93ms/step - loss: 0.6865 - acc: 0.5673 - val_loss: 0.6963 - val_acc: 0.5362

Epoch 00003: val_acc did not improve from 0.56957
Epoch 4/10

  64/4849 [..............................] - ETA: 7:00 - loss: 0.7085 - acc: 0.5469
 128/4849 [..............................] - ETA: 6:56 - loss: 0.6997 - acc: 0.5078
 192/4849 [>.............................] - ETA: 7:04 - loss: 0.7049 - acc: 0.5208
 256/4849 [>.............................] - ETA: 6:59 - loss: 0.6818 - acc: 0.5742
 320/4849 [>.............................] - ETA: 6:53 - loss: 0.6731 - acc: 0.5875
 384/4849 [=>............................] - ETA: 6:49 - loss: 0.6705 - acc: 0.5964
 448/4849 [=>............................] - ETA: 6:45 - loss: 0.6714 - acc: 0.5893
 512/4849 [==>...........................] - ETA: 6:41 - loss: 0.6735 - acc: 0.5879
 576/4849 [==>...........................] - ETA: 6:34 - loss: 0.6702 - acc: 0.5903
 640/4849 [==>...........................] - ETA: 6:31 - loss: 0.6699 - acc: 0.5984
 704/4849 [===>..........................] - ETA: 6:28 - loss: 0.6709 - acc: 0.5994
 768/4849 [===>..........................] - ETA: 6:20 - loss: 0.6704 - acc: 0.6055
 832/4849 [====>.........................] - ETA: 6:15 - loss: 0.6756 - acc: 0.5974
 896/4849 [====>.........................] - ETA: 6:10 - loss: 0.6725 - acc: 0.6004
 960/4849 [====>.........................] - ETA: 6:04 - loss: 0.6744 - acc: 0.6000
1024/4849 [=====>........................] - ETA: 5:56 - loss: 0.6773 - acc: 0.5938
1088/4849 [=====>........................] - ETA: 5:50 - loss: 0.6787 - acc: 0.5892
1152/4849 [======>.......................] - ETA: 5:43 - loss: 0.6794 - acc: 0.5894
1216/4849 [======>.......................] - ETA: 5:38 - loss: 0.6775 - acc: 0.5913
1280/4849 [======>.......................] - ETA: 5:32 - loss: 0.6769 - acc: 0.5953
1344/4849 [=======>......................] - ETA: 5:27 - loss: 0.6752 - acc: 0.5960
1408/4849 [=======>......................] - ETA: 5:22 - loss: 0.6759 - acc: 0.5938
1472/4849 [========>.....................] - ETA: 5:14 - loss: 0.6783 - acc: 0.5904
1536/4849 [========>.....................] - ETA: 5:09 - loss: 0.6783 - acc: 0.5911
1600/4849 [========>.....................] - ETA: 5:03 - loss: 0.6787 - acc: 0.5925
1664/4849 [=========>....................] - ETA: 4:57 - loss: 0.6789 - acc: 0.5901
1728/4849 [=========>....................] - ETA: 4:50 - loss: 0.6803 - acc: 0.5880
1792/4849 [==========>...................] - ETA: 4:44 - loss: 0.6792 - acc: 0.5882
1856/4849 [==========>...................] - ETA: 4:38 - loss: 0.6779 - acc: 0.5916
1920/4849 [==========>...................] - ETA: 4:32 - loss: 0.6792 - acc: 0.5885
1984/4849 [===========>..................] - ETA: 4:27 - loss: 0.6796 - acc: 0.5867
2048/4849 [===========>..................] - ETA: 4:21 - loss: 0.6793 - acc: 0.5884
2112/4849 [============>.................] - ETA: 4:15 - loss: 0.6791 - acc: 0.5876
2176/4849 [============>.................] - ETA: 4:09 - loss: 0.6781 - acc: 0.5882
2240/4849 [============>.................] - ETA: 4:04 - loss: 0.6780 - acc: 0.5875
2304/4849 [=============>................] - ETA: 3:57 - loss: 0.6800 - acc: 0.5829
2368/4849 [=============>................] - ETA: 3:51 - loss: 0.6809 - acc: 0.5819
2432/4849 [==============>...............] - ETA: 3:45 - loss: 0.6792 - acc: 0.5859
2496/4849 [==============>...............] - ETA: 3:39 - loss: 0.6787 - acc: 0.5873
2560/4849 [==============>...............] - ETA: 3:33 - loss: 0.6787 - acc: 0.5871
2624/4849 [===============>..............] - ETA: 3:27 - loss: 0.6776 - acc: 0.5884
2688/4849 [===============>..............] - ETA: 3:21 - loss: 0.6779 - acc: 0.5874
2752/4849 [================>.............] - ETA: 3:15 - loss: 0.6780 - acc: 0.5876
2816/4849 [================>.............] - ETA: 3:09 - loss: 0.6780 - acc: 0.5874
2880/4849 [================>.............] - ETA: 3:03 - loss: 0.6787 - acc: 0.5865
2944/4849 [=================>............] - ETA: 2:57 - loss: 0.6791 - acc: 0.5853
3008/4849 [=================>............] - ETA: 2:51 - loss: 0.6797 - acc: 0.5834
3072/4849 [==================>...........] - ETA: 2:45 - loss: 0.6806 - acc: 0.5804
3136/4849 [==================>...........] - ETA: 2:39 - loss: 0.6805 - acc: 0.5810
3200/4849 [==================>...........] - ETA: 2:33 - loss: 0.6813 - acc: 0.5803
3264/4849 [===================>..........] - ETA: 2:27 - loss: 0.6816 - acc: 0.5784
3328/4849 [===================>..........] - ETA: 2:21 - loss: 0.6806 - acc: 0.5799
3392/4849 [===================>..........] - ETA: 2:15 - loss: 0.6810 - acc: 0.5787
3456/4849 [====================>.........] - ETA: 2:09 - loss: 0.6812 - acc: 0.5787
3520/4849 [====================>.........] - ETA: 2:03 - loss: 0.6811 - acc: 0.5778
3584/4849 [=====================>........] - ETA: 1:57 - loss: 0.6816 - acc: 0.5773
3648/4849 [=====================>........] - ETA: 1:51 - loss: 0.6811 - acc: 0.5773
3712/4849 [=====================>........] - ETA: 1:45 - loss: 0.6807 - acc: 0.5773
3776/4849 [======================>.......] - ETA: 1:39 - loss: 0.6810 - acc: 0.5757
3840/4849 [======================>.......] - ETA: 1:33 - loss: 0.6812 - acc: 0.5755
3904/4849 [=======================>......] - ETA: 1:27 - loss: 0.6810 - acc: 0.5751
3968/4849 [=======================>......] - ETA: 1:21 - loss: 0.6815 - acc: 0.5741
4032/4849 [=======================>......] - ETA: 1:15 - loss: 0.6818 - acc: 0.5742
4096/4849 [========================>.....] - ETA: 1:09 - loss: 0.6817 - acc: 0.5742
4160/4849 [========================>.....] - ETA: 1:03 - loss: 0.6822 - acc: 0.5731
4224/4849 [=========================>....] - ETA: 58s - loss: 0.6824 - acc: 0.5727 
4288/4849 [=========================>....] - ETA: 52s - loss: 0.6828 - acc: 0.5723
4352/4849 [=========================>....] - ETA: 46s - loss: 0.6830 - acc: 0.5715
4416/4849 [==========================>...] - ETA: 40s - loss: 0.6834 - acc: 0.5707
4480/4849 [==========================>...] - ETA: 34s - loss: 0.6832 - acc: 0.5719
4544/4849 [===========================>..] - ETA: 28s - loss: 0.6832 - acc: 0.5709
4608/4849 [===========================>..] - ETA: 22s - loss: 0.6835 - acc: 0.5699
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6833 - acc: 0.5700
4736/4849 [============================>.] - ETA: 10s - loss: 0.6833 - acc: 0.5707
4800/4849 [============================>.] - ETA: 4s - loss: 0.6830 - acc: 0.5721 
4849/4849 [==============================] - 467s 96ms/step - loss: 0.6832 - acc: 0.5717 - val_loss: 0.6913 - val_acc: 0.5547

Epoch 00004: val_acc did not improve from 0.56957
Epoch 5/10

  64/4849 [..............................] - ETA: 7:23 - loss: 0.6910 - acc: 0.5625
 128/4849 [..............................] - ETA: 7:31 - loss: 0.7003 - acc: 0.5469
 192/4849 [>.............................] - ETA: 7:14 - loss: 0.6904 - acc: 0.5573
 256/4849 [>.............................] - ETA: 7:15 - loss: 0.6873 - acc: 0.5586
 320/4849 [>.............................] - ETA: 7:14 - loss: 0.6878 - acc: 0.5406
 384/4849 [=>............................] - ETA: 7:16 - loss: 0.6872 - acc: 0.5391
 448/4849 [=>............................] - ETA: 7:06 - loss: 0.6868 - acc: 0.5446
 512/4849 [==>...........................] - ETA: 6:59 - loss: 0.6835 - acc: 0.5586
 576/4849 [==>...........................] - ETA: 6:49 - loss: 0.6825 - acc: 0.5625
 640/4849 [==>...........................] - ETA: 6:39 - loss: 0.6846 - acc: 0.5578
 704/4849 [===>..........................] - ETA: 6:30 - loss: 0.6844 - acc: 0.5597
 768/4849 [===>..........................] - ETA: 6:19 - loss: 0.6818 - acc: 0.5677
 832/4849 [====>.........................] - ETA: 6:14 - loss: 0.6814 - acc: 0.5673
 896/4849 [====>.........................] - ETA: 6:05 - loss: 0.6795 - acc: 0.5692
 960/4849 [====>.........................] - ETA: 5:58 - loss: 0.6774 - acc: 0.5719
1024/4849 [=====>........................] - ETA: 5:49 - loss: 0.6767 - acc: 0.5732
1088/4849 [=====>........................] - ETA: 5:43 - loss: 0.6787 - acc: 0.5671
1152/4849 [======>.......................] - ETA: 5:36 - loss: 0.6762 - acc: 0.5738
1216/4849 [======>.......................] - ETA: 5:30 - loss: 0.6768 - acc: 0.5748
1280/4849 [======>.......................] - ETA: 5:25 - loss: 0.6780 - acc: 0.5687
1344/4849 [=======>......................] - ETA: 5:18 - loss: 0.6785 - acc: 0.5677
1408/4849 [=======>......................] - ETA: 5:11 - loss: 0.6774 - acc: 0.5689
1472/4849 [========>.....................] - ETA: 5:06 - loss: 0.6785 - acc: 0.5652
1536/4849 [========>.....................] - ETA: 4:59 - loss: 0.6800 - acc: 0.5612
1600/4849 [========>.....................] - ETA: 4:53 - loss: 0.6803 - acc: 0.5637
1664/4849 [=========>....................] - ETA: 4:47 - loss: 0.6803 - acc: 0.5637
1728/4849 [=========>....................] - ETA: 4:41 - loss: 0.6797 - acc: 0.5625
1792/4849 [==========>...................] - ETA: 4:35 - loss: 0.6791 - acc: 0.5642
1856/4849 [==========>...................] - ETA: 4:29 - loss: 0.6782 - acc: 0.5673
1920/4849 [==========>...................] - ETA: 4:23 - loss: 0.6778 - acc: 0.5687
1984/4849 [===========>..................] - ETA: 4:17 - loss: 0.6785 - acc: 0.5691
2048/4849 [===========>..................] - ETA: 4:12 - loss: 0.6784 - acc: 0.5703
2112/4849 [============>.................] - ETA: 4:06 - loss: 0.6779 - acc: 0.5729
2176/4849 [============>.................] - ETA: 4:00 - loss: 0.6785 - acc: 0.5717
2240/4849 [============>.................] - ETA: 3:54 - loss: 0.6789 - acc: 0.5705
2304/4849 [=============>................] - ETA: 3:48 - loss: 0.6784 - acc: 0.5725
2368/4849 [=============>................] - ETA: 3:42 - loss: 0.6779 - acc: 0.5747
2432/4849 [==============>...............] - ETA: 3:37 - loss: 0.6783 - acc: 0.5736
2496/4849 [==============>...............] - ETA: 3:31 - loss: 0.6784 - acc: 0.5721
2560/4849 [==============>...............] - ETA: 3:25 - loss: 0.6782 - acc: 0.5715
2624/4849 [===============>..............] - ETA: 3:19 - loss: 0.6786 - acc: 0.5701
2688/4849 [===============>..............] - ETA: 3:13 - loss: 0.6786 - acc: 0.5703
2752/4849 [================>.............] - ETA: 3:08 - loss: 0.6782 - acc: 0.5716
2816/4849 [================>.............] - ETA: 3:02 - loss: 0.6780 - acc: 0.5735
2880/4849 [================>.............] - ETA: 2:56 - loss: 0.6777 - acc: 0.5757
2944/4849 [=================>............] - ETA: 2:50 - loss: 0.6783 - acc: 0.5747
3008/4849 [=================>............] - ETA: 2:45 - loss: 0.6787 - acc: 0.5748
3072/4849 [==================>...........] - ETA: 2:39 - loss: 0.6793 - acc: 0.5742
3136/4849 [==================>...........] - ETA: 2:33 - loss: 0.6799 - acc: 0.5737
3200/4849 [==================>...........] - ETA: 2:27 - loss: 0.6799 - acc: 0.5719
3264/4849 [===================>..........] - ETA: 2:22 - loss: 0.6800 - acc: 0.5717
3328/4849 [===================>..........] - ETA: 2:16 - loss: 0.6788 - acc: 0.5739
3392/4849 [===================>..........] - ETA: 2:10 - loss: 0.6795 - acc: 0.5722
3456/4849 [====================>.........] - ETA: 2:04 - loss: 0.6795 - acc: 0.5718
3520/4849 [====================>.........] - ETA: 1:59 - loss: 0.6796 - acc: 0.5719
3584/4849 [=====================>........] - ETA: 1:53 - loss: 0.6800 - acc: 0.5700
3648/4849 [=====================>........] - ETA: 1:47 - loss: 0.6789 - acc: 0.5718
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6786 - acc: 0.5725
3776/4849 [======================>.......] - ETA: 1:35 - loss: 0.6786 - acc: 0.5723
3840/4849 [======================>.......] - ETA: 1:30 - loss: 0.6797 - acc: 0.5701
3904/4849 [=======================>......] - ETA: 1:24 - loss: 0.6793 - acc: 0.5704
3968/4849 [=======================>......] - ETA: 1:18 - loss: 0.6795 - acc: 0.5696
4032/4849 [=======================>......] - ETA: 1:12 - loss: 0.6791 - acc: 0.5699
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6796 - acc: 0.5671
4160/4849 [========================>.....] - ETA: 1:01 - loss: 0.6792 - acc: 0.5671
4224/4849 [=========================>....] - ETA: 55s - loss: 0.6786 - acc: 0.5684 
4288/4849 [=========================>....] - ETA: 49s - loss: 0.6774 - acc: 0.5709
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6781 - acc: 0.5694
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6778 - acc: 0.5693
4480/4849 [==========================>...] - ETA: 32s - loss: 0.6769 - acc: 0.5710
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6771 - acc: 0.5698
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6778 - acc: 0.5686
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6778 - acc: 0.5693
4736/4849 [============================>.] - ETA: 10s - loss: 0.6779 - acc: 0.5693
4800/4849 [============================>.] - ETA: 4s - loss: 0.6776 - acc: 0.5700 
4849/4849 [==============================] - 447s 92ms/step - loss: 0.6773 - acc: 0.5706 - val_loss: 0.6906 - val_acc: 0.5696

Epoch 00005: val_acc did not improve from 0.56957
Epoch 6/10

  64/4849 [..............................] - ETA: 6:33 - loss: 0.6880 - acc: 0.5625
 128/4849 [..............................] - ETA: 6:04 - loss: 0.7156 - acc: 0.5078
 192/4849 [>.............................] - ETA: 6:02 - loss: 0.6947 - acc: 0.5417
 256/4849 [>.............................] - ETA: 6:01 - loss: 0.7041 - acc: 0.5117
 320/4849 [>.............................] - ETA: 5:57 - loss: 0.7014 - acc: 0.5125
 384/4849 [=>............................] - ETA: 5:55 - loss: 0.6959 - acc: 0.5260
 448/4849 [=>............................] - ETA: 5:46 - loss: 0.6966 - acc: 0.5246
 512/4849 [==>...........................] - ETA: 5:45 - loss: 0.6926 - acc: 0.5332
 576/4849 [==>...........................] - ETA: 5:36 - loss: 0.6894 - acc: 0.5399
 640/4849 [==>...........................] - ETA: 5:32 - loss: 0.6859 - acc: 0.5500
 704/4849 [===>..........................] - ETA: 5:22 - loss: 0.6846 - acc: 0.5568
 768/4849 [===>..........................] - ETA: 5:20 - loss: 0.6831 - acc: 0.5638
 832/4849 [====>.........................] - ETA: 5:13 - loss: 0.6809 - acc: 0.5637
 896/4849 [====>.........................] - ETA: 5:10 - loss: 0.6787 - acc: 0.5692
 960/4849 [====>.........................] - ETA: 5:05 - loss: 0.6803 - acc: 0.5708
1024/4849 [=====>........................] - ETA: 5:01 - loss: 0.6790 - acc: 0.5732
1088/4849 [=====>........................] - ETA: 4:57 - loss: 0.6811 - acc: 0.5708
1152/4849 [======>.......................] - ETA: 4:53 - loss: 0.6828 - acc: 0.5694
1216/4849 [======>.......................] - ETA: 4:47 - loss: 0.6803 - acc: 0.5724
1280/4849 [======>.......................] - ETA: 4:42 - loss: 0.6789 - acc: 0.5742
1344/4849 [=======>......................] - ETA: 4:36 - loss: 0.6779 - acc: 0.5744
1408/4849 [=======>......................] - ETA: 4:30 - loss: 0.6778 - acc: 0.5788
1472/4849 [========>.....................] - ETA: 4:25 - loss: 0.6758 - acc: 0.5836
1536/4849 [========>.....................] - ETA: 4:20 - loss: 0.6742 - acc: 0.5866
1600/4849 [========>.....................] - ETA: 4:16 - loss: 0.6731 - acc: 0.5869
1664/4849 [=========>....................] - ETA: 4:10 - loss: 0.6734 - acc: 0.5859
1728/4849 [=========>....................] - ETA: 4:05 - loss: 0.6776 - acc: 0.5799
1792/4849 [==========>...................] - ETA: 3:59 - loss: 0.6794 - acc: 0.5776
1856/4849 [==========>...................] - ETA: 3:54 - loss: 0.6786 - acc: 0.5787
1920/4849 [==========>...................] - ETA: 3:49 - loss: 0.6805 - acc: 0.5740
1984/4849 [===========>..................] - ETA: 3:44 - loss: 0.6800 - acc: 0.5756
2048/4849 [===========>..................] - ETA: 3:38 - loss: 0.6802 - acc: 0.5737
2112/4849 [============>.................] - ETA: 3:33 - loss: 0.6793 - acc: 0.5762
2176/4849 [============>.................] - ETA: 3:28 - loss: 0.6799 - acc: 0.5744
2240/4849 [============>.................] - ETA: 3:23 - loss: 0.6790 - acc: 0.5754
2304/4849 [=============>................] - ETA: 3:19 - loss: 0.6789 - acc: 0.5755
2368/4849 [=============>................] - ETA: 3:13 - loss: 0.6779 - acc: 0.5785
2432/4849 [==============>...............] - ETA: 3:08 - loss: 0.6798 - acc: 0.5769
2496/4849 [==============>...............] - ETA: 3:03 - loss: 0.6805 - acc: 0.5765
2560/4849 [==============>...............] - ETA: 2:58 - loss: 0.6796 - acc: 0.5781
2624/4849 [===============>..............] - ETA: 2:53 - loss: 0.6791 - acc: 0.5789
2688/4849 [===============>..............] - ETA: 2:48 - loss: 0.6784 - acc: 0.5796
2752/4849 [================>.............] - ETA: 2:43 - loss: 0.6792 - acc: 0.5781
2816/4849 [================>.............] - ETA: 2:38 - loss: 0.6789 - acc: 0.5778
2880/4849 [================>.............] - ETA: 2:33 - loss: 0.6788 - acc: 0.5788
2944/4849 [=================>............] - ETA: 2:28 - loss: 0.6786 - acc: 0.5798
3008/4849 [=================>............] - ETA: 2:23 - loss: 0.6791 - acc: 0.5788
3072/4849 [==================>...........] - ETA: 2:18 - loss: 0.6793 - acc: 0.5788
3136/4849 [==================>...........] - ETA: 2:13 - loss: 0.6794 - acc: 0.5788
3200/4849 [==================>...........] - ETA: 2:07 - loss: 0.6788 - acc: 0.5784
3264/4849 [===================>..........] - ETA: 2:02 - loss: 0.6786 - acc: 0.5790
3328/4849 [===================>..........] - ETA: 1:57 - loss: 0.6784 - acc: 0.5781
3392/4849 [===================>..........] - ETA: 1:52 - loss: 0.6777 - acc: 0.5793
3456/4849 [====================>.........] - ETA: 1:47 - loss: 0.6787 - acc: 0.5775
3520/4849 [====================>.........] - ETA: 1:43 - loss: 0.6785 - acc: 0.5770
3584/4849 [=====================>........] - ETA: 1:38 - loss: 0.6786 - acc: 0.5767
3648/4849 [=====================>........] - ETA: 1:32 - loss: 0.6781 - acc: 0.5773
3712/4849 [=====================>........] - ETA: 1:28 - loss: 0.6773 - acc: 0.5792
3776/4849 [======================>.......] - ETA: 1:23 - loss: 0.6773 - acc: 0.5787
3840/4849 [======================>.......] - ETA: 1:18 - loss: 0.6773 - acc: 0.5781
3904/4849 [=======================>......] - ETA: 1:13 - loss: 0.6780 - acc: 0.5768
3968/4849 [=======================>......] - ETA: 1:08 - loss: 0.6774 - acc: 0.5784
4032/4849 [=======================>......] - ETA: 1:03 - loss: 0.6769 - acc: 0.5794
4096/4849 [========================>.....] - ETA: 58s - loss: 0.6759 - acc: 0.5803 
4160/4849 [========================>.....] - ETA: 53s - loss: 0.6761 - acc: 0.5803
4224/4849 [=========================>....] - ETA: 48s - loss: 0.6755 - acc: 0.5810
4288/4849 [=========================>....] - ETA: 43s - loss: 0.6748 - acc: 0.5819
4352/4849 [=========================>....] - ETA: 38s - loss: 0.6742 - acc: 0.5834
4416/4849 [==========================>...] - ETA: 33s - loss: 0.6738 - acc: 0.5838
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6736 - acc: 0.5839
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6733 - acc: 0.5843
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6739 - acc: 0.5829
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6735 - acc: 0.5839
4736/4849 [============================>.] - ETA: 8s - loss: 0.6739 - acc: 0.5836 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6744 - acc: 0.5825
4849/4849 [==============================] - 390s 80ms/step - loss: 0.6742 - acc: 0.5830 - val_loss: 0.6916 - val_acc: 0.5584

Epoch 00006: val_acc did not improve from 0.56957
Epoch 7/10

  64/4849 [..............................] - ETA: 6:36 - loss: 0.6246 - acc: 0.6562
 128/4849 [..............................] - ETA: 5:50 - loss: 0.6823 - acc: 0.6016
 192/4849 [>.............................] - ETA: 6:06 - loss: 0.6735 - acc: 0.6042
 256/4849 [>.............................] - ETA: 5:55 - loss: 0.6652 - acc: 0.6055
 320/4849 [>.............................] - ETA: 6:00 - loss: 0.6687 - acc: 0.5906
 384/4849 [=>............................] - ETA: 5:52 - loss: 0.6772 - acc: 0.5755
 448/4849 [=>............................] - ETA: 5:48 - loss: 0.6760 - acc: 0.5893
 512/4849 [==>...........................] - ETA: 5:43 - loss: 0.6706 - acc: 0.5957
 576/4849 [==>...........................] - ETA: 5:39 - loss: 0.6737 - acc: 0.5938
 640/4849 [==>...........................] - ETA: 5:34 - loss: 0.6749 - acc: 0.5859
 704/4849 [===>..........................] - ETA: 5:27 - loss: 0.6731 - acc: 0.5966
 768/4849 [===>..........................] - ETA: 5:21 - loss: 0.6722 - acc: 0.6016
 832/4849 [====>.........................] - ETA: 5:15 - loss: 0.6710 - acc: 0.6034
 896/4849 [====>.........................] - ETA: 5:10 - loss: 0.6765 - acc: 0.5960
 960/4849 [====>.........................] - ETA: 5:04 - loss: 0.6782 - acc: 0.5948
1024/4849 [=====>........................] - ETA: 4:59 - loss: 0.6741 - acc: 0.5986
1088/4849 [=====>........................] - ETA: 4:53 - loss: 0.6741 - acc: 0.5993
1152/4849 [======>.......................] - ETA: 4:50 - loss: 0.6736 - acc: 0.5990
1216/4849 [======>.......................] - ETA: 4:42 - loss: 0.6729 - acc: 0.5995
1280/4849 [======>.......................] - ETA: 4:38 - loss: 0.6739 - acc: 0.5977
1344/4849 [=======>......................] - ETA: 4:32 - loss: 0.6731 - acc: 0.6019
1408/4849 [=======>......................] - ETA: 4:28 - loss: 0.6739 - acc: 0.6037
1472/4849 [========>.....................] - ETA: 4:22 - loss: 0.6728 - acc: 0.6046
1536/4849 [========>.....................] - ETA: 4:18 - loss: 0.6724 - acc: 0.6061
1600/4849 [========>.....................] - ETA: 4:13 - loss: 0.6726 - acc: 0.6056
1664/4849 [=========>....................] - ETA: 4:08 - loss: 0.6728 - acc: 0.6028
1728/4849 [=========>....................] - ETA: 4:03 - loss: 0.6742 - acc: 0.5972
1792/4849 [==========>...................] - ETA: 3:58 - loss: 0.6754 - acc: 0.5949
1856/4849 [==========>...................] - ETA: 3:53 - loss: 0.6759 - acc: 0.5921
1920/4849 [==========>...................] - ETA: 3:48 - loss: 0.6751 - acc: 0.5922
1984/4849 [===========>..................] - ETA: 3:43 - loss: 0.6760 - acc: 0.5902
2048/4849 [===========>..................] - ETA: 3:38 - loss: 0.6767 - acc: 0.5889
2112/4849 [============>.................] - ETA: 3:33 - loss: 0.6755 - acc: 0.5909
2176/4849 [============>.................] - ETA: 3:27 - loss: 0.6752 - acc: 0.5928
2240/4849 [============>.................] - ETA: 3:22 - loss: 0.6750 - acc: 0.5911
2304/4849 [=============>................] - ETA: 3:17 - loss: 0.6740 - acc: 0.5920
2368/4849 [=============>................] - ETA: 3:12 - loss: 0.6745 - acc: 0.5921
2432/4849 [==============>...............] - ETA: 3:07 - loss: 0.6740 - acc: 0.5938
2496/4849 [==============>...............] - ETA: 3:03 - loss: 0.6742 - acc: 0.5921
2560/4849 [==============>...............] - ETA: 2:57 - loss: 0.6734 - acc: 0.5945
2624/4849 [===============>..............] - ETA: 2:52 - loss: 0.6732 - acc: 0.5934
2688/4849 [===============>..............] - ETA: 2:47 - loss: 0.6736 - acc: 0.5949
2752/4849 [================>.............] - ETA: 2:41 - loss: 0.6731 - acc: 0.5952
2816/4849 [================>.............] - ETA: 2:36 - loss: 0.6738 - acc: 0.5945
2880/4849 [================>.............] - ETA: 2:31 - loss: 0.6740 - acc: 0.5941
2944/4849 [=================>............] - ETA: 2:26 - loss: 0.6759 - acc: 0.5914
3008/4849 [=================>............] - ETA: 2:21 - loss: 0.6766 - acc: 0.5894
3072/4849 [==================>...........] - ETA: 2:16 - loss: 0.6772 - acc: 0.5879
3136/4849 [==================>...........] - ETA: 2:11 - loss: 0.6764 - acc: 0.5893
3200/4849 [==================>...........] - ETA: 2:06 - loss: 0.6779 - acc: 0.5863
3264/4849 [===================>..........] - ETA: 2:01 - loss: 0.6781 - acc: 0.5852
3328/4849 [===================>..........] - ETA: 1:56 - loss: 0.6775 - acc: 0.5853
3392/4849 [===================>..........] - ETA: 1:51 - loss: 0.6773 - acc: 0.5861
3456/4849 [====================>.........] - ETA: 1:46 - loss: 0.6769 - acc: 0.5865
3520/4849 [====================>.........] - ETA: 1:41 - loss: 0.6774 - acc: 0.5858
3584/4849 [=====================>........] - ETA: 1:36 - loss: 0.6780 - acc: 0.5851
3648/4849 [=====================>........] - ETA: 1:31 - loss: 0.6772 - acc: 0.5863
3712/4849 [=====================>........] - ETA: 1:26 - loss: 0.6773 - acc: 0.5849
3776/4849 [======================>.......] - ETA: 1:21 - loss: 0.6770 - acc: 0.5850
3840/4849 [======================>.......] - ETA: 1:17 - loss: 0.6778 - acc: 0.5831
3904/4849 [=======================>......] - ETA: 1:12 - loss: 0.6766 - acc: 0.5856
3968/4849 [=======================>......] - ETA: 1:07 - loss: 0.6765 - acc: 0.5852
4032/4849 [=======================>......] - ETA: 1:02 - loss: 0.6778 - acc: 0.5833
4096/4849 [========================>.....] - ETA: 57s - loss: 0.6773 - acc: 0.5840 
4160/4849 [========================>.....] - ETA: 52s - loss: 0.6771 - acc: 0.5849
4224/4849 [=========================>....] - ETA: 47s - loss: 0.6763 - acc: 0.5869
4288/4849 [=========================>....] - ETA: 42s - loss: 0.6760 - acc: 0.5870
4352/4849 [=========================>....] - ETA: 37s - loss: 0.6766 - acc: 0.5862
4416/4849 [==========================>...] - ETA: 33s - loss: 0.6764 - acc: 0.5867
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6763 - acc: 0.5868
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6765 - acc: 0.5858
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6766 - acc: 0.5846
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6769 - acc: 0.5841
4736/4849 [============================>.] - ETA: 8s - loss: 0.6781 - acc: 0.5832 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6775 - acc: 0.5840
4849/4849 [==============================] - 384s 79ms/step - loss: 0.6775 - acc: 0.5849 - val_loss: 0.6909 - val_acc: 0.5659

Epoch 00007: val_acc did not improve from 0.56957
Epoch 8/10

  64/4849 [..............................] - ETA: 6:12 - loss: 0.6646 - acc: 0.6250
 128/4849 [..............................] - ETA: 5:53 - loss: 0.6728 - acc: 0.6250
 192/4849 [>.............................] - ETA: 5:54 - loss: 0.6809 - acc: 0.6198
 256/4849 [>.............................] - ETA: 5:42 - loss: 0.6783 - acc: 0.6250
 320/4849 [>.............................] - ETA: 5:36 - loss: 0.6764 - acc: 0.6281
 384/4849 [=>............................] - ETA: 5:36 - loss: 0.6755 - acc: 0.6146
 448/4849 [=>............................] - ETA: 5:31 - loss: 0.6698 - acc: 0.6295
 512/4849 [==>...........................] - ETA: 5:25 - loss: 0.6657 - acc: 0.6250
 576/4849 [==>...........................] - ETA: 5:18 - loss: 0.6653 - acc: 0.6285
 640/4849 [==>...........................] - ETA: 5:15 - loss: 0.6657 - acc: 0.6281
 704/4849 [===>..........................] - ETA: 5:05 - loss: 0.6666 - acc: 0.6236
 768/4849 [===>..........................] - ETA: 5:00 - loss: 0.6660 - acc: 0.6211
 832/4849 [====>.........................] - ETA: 4:56 - loss: 0.6675 - acc: 0.6190
 896/4849 [====>.........................] - ETA: 4:50 - loss: 0.6672 - acc: 0.6138
 960/4849 [====>.........................] - ETA: 4:46 - loss: 0.6666 - acc: 0.6156
1024/4849 [=====>........................] - ETA: 4:40 - loss: 0.6683 - acc: 0.6084
1088/4849 [=====>........................] - ETA: 4:35 - loss: 0.6680 - acc: 0.6085
1152/4849 [======>.......................] - ETA: 4:30 - loss: 0.6711 - acc: 0.6059
1216/4849 [======>.......................] - ETA: 4:26 - loss: 0.6706 - acc: 0.6053
1280/4849 [======>.......................] - ETA: 4:20 - loss: 0.6686 - acc: 0.6047
1344/4849 [=======>......................] - ETA: 4:17 - loss: 0.6721 - acc: 0.6012
1408/4849 [=======>......................] - ETA: 4:12 - loss: 0.6712 - acc: 0.6023
1472/4849 [========>.....................] - ETA: 4:06 - loss: 0.6731 - acc: 0.5978
1536/4849 [========>.....................] - ETA: 4:01 - loss: 0.6721 - acc: 0.6003
1600/4849 [========>.....................] - ETA: 3:57 - loss: 0.6739 - acc: 0.5938
1664/4849 [=========>....................] - ETA: 3:53 - loss: 0.6737 - acc: 0.5919
1728/4849 [=========>....................] - ETA: 3:48 - loss: 0.6746 - acc: 0.5885
1792/4849 [==========>...................] - ETA: 3:44 - loss: 0.6723 - acc: 0.5926
1856/4849 [==========>...................] - ETA: 3:39 - loss: 0.6729 - acc: 0.5921
1920/4849 [==========>...................] - ETA: 3:35 - loss: 0.6724 - acc: 0.5927
1984/4849 [===========>..................] - ETA: 3:30 - loss: 0.6718 - acc: 0.5932
2048/4849 [===========>..................] - ETA: 3:25 - loss: 0.6728 - acc: 0.5894
2112/4849 [============>.................] - ETA: 3:21 - loss: 0.6739 - acc: 0.5885
2176/4849 [============>.................] - ETA: 3:16 - loss: 0.6745 - acc: 0.5869
2240/4849 [============>.................] - ETA: 3:12 - loss: 0.6745 - acc: 0.5875
2304/4849 [=============>................] - ETA: 3:07 - loss: 0.6734 - acc: 0.5894
2368/4849 [=============>................] - ETA: 3:02 - loss: 0.6728 - acc: 0.5912
2432/4849 [==============>...............] - ETA: 2:57 - loss: 0.6735 - acc: 0.5892
2496/4849 [==============>...............] - ETA: 2:53 - loss: 0.6725 - acc: 0.5893
2560/4849 [==============>...............] - ETA: 2:47 - loss: 0.6715 - acc: 0.5914
2624/4849 [===============>..............] - ETA: 2:43 - loss: 0.6710 - acc: 0.5941
2688/4849 [===============>..............] - ETA: 2:38 - loss: 0.6722 - acc: 0.5926
2752/4849 [================>.............] - ETA: 2:33 - loss: 0.6720 - acc: 0.5905
2816/4849 [================>.............] - ETA: 2:29 - loss: 0.6728 - acc: 0.5895
2880/4849 [================>.............] - ETA: 2:24 - loss: 0.6737 - acc: 0.5889
2944/4849 [=================>............] - ETA: 2:19 - loss: 0.6733 - acc: 0.5890
3008/4849 [=================>............] - ETA: 2:14 - loss: 0.6728 - acc: 0.5894
3072/4849 [==================>...........] - ETA: 2:10 - loss: 0.6740 - acc: 0.5869
3136/4849 [==================>...........] - ETA: 2:05 - loss: 0.6750 - acc: 0.5867
3200/4849 [==================>...........] - ETA: 2:00 - loss: 0.6746 - acc: 0.5869
3264/4849 [===================>..........] - ETA: 1:56 - loss: 0.6740 - acc: 0.5879
3328/4849 [===================>..........] - ETA: 1:51 - loss: 0.6739 - acc: 0.5892
3392/4849 [===================>..........] - ETA: 1:46 - loss: 0.6742 - acc: 0.5890
3456/4849 [====================>.........] - ETA: 1:42 - loss: 0.6744 - acc: 0.5891
3520/4849 [====================>.........] - ETA: 1:37 - loss: 0.6744 - acc: 0.5889
3584/4849 [=====================>........] - ETA: 1:32 - loss: 0.6743 - acc: 0.5896
3648/4849 [=====================>........] - ETA: 1:28 - loss: 0.6748 - acc: 0.5896
3712/4849 [=====================>........] - ETA: 1:23 - loss: 0.6742 - acc: 0.5911
3776/4849 [======================>.......] - ETA: 1:18 - loss: 0.6751 - acc: 0.5898
3840/4849 [======================>.......] - ETA: 1:14 - loss: 0.6753 - acc: 0.5896
3904/4849 [=======================>......] - ETA: 1:09 - loss: 0.6756 - acc: 0.5886
3968/4849 [=======================>......] - ETA: 1:04 - loss: 0.6753 - acc: 0.5897
4032/4849 [=======================>......] - ETA: 59s - loss: 0.6748 - acc: 0.5908 
4096/4849 [========================>.....] - ETA: 55s - loss: 0.6743 - acc: 0.5920
4160/4849 [========================>.....] - ETA: 50s - loss: 0.6737 - acc: 0.5933
4224/4849 [=========================>....] - ETA: 45s - loss: 0.6741 - acc: 0.5926
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6739 - acc: 0.5931
4352/4849 [=========================>....] - ETA: 36s - loss: 0.6741 - acc: 0.5926
4416/4849 [==========================>...] - ETA: 31s - loss: 0.6739 - acc: 0.5924
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6733 - acc: 0.5933
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6731 - acc: 0.5944
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6736 - acc: 0.5942
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6735 - acc: 0.5942
4736/4849 [============================>.] - ETA: 8s - loss: 0.6727 - acc: 0.5954 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6729 - acc: 0.5956
4849/4849 [==============================] - 370s 76ms/step - loss: 0.6730 - acc: 0.5952 - val_loss: 0.6882 - val_acc: 0.5566

Epoch 00008: val_acc did not improve from 0.56957
Epoch 9/10

  64/4849 [..............................] - ETA: 6:07 - loss: 0.6401 - acc: 0.6094
 128/4849 [..............................] - ETA: 5:51 - loss: 0.6629 - acc: 0.5781
 192/4849 [>.............................] - ETA: 5:55 - loss: 0.6616 - acc: 0.5885
 256/4849 [>.............................] - ETA: 5:40 - loss: 0.6712 - acc: 0.5938
 320/4849 [>.............................] - ETA: 5:34 - loss: 0.6692 - acc: 0.5969
 384/4849 [=>............................] - ETA: 5:32 - loss: 0.6755 - acc: 0.5885
 448/4849 [=>............................] - ETA: 5:31 - loss: 0.6751 - acc: 0.5804
 512/4849 [==>...........................] - ETA: 5:27 - loss: 0.6714 - acc: 0.5918
 576/4849 [==>...........................] - ETA: 5:20 - loss: 0.6682 - acc: 0.5990
 640/4849 [==>...........................] - ETA: 5:18 - loss: 0.6704 - acc: 0.5922
 704/4849 [===>..........................] - ETA: 5:14 - loss: 0.6691 - acc: 0.5966
 768/4849 [===>..........................] - ETA: 5:10 - loss: 0.6692 - acc: 0.5924
 832/4849 [====>.........................] - ETA: 5:06 - loss: 0.6729 - acc: 0.5841
 896/4849 [====>.........................] - ETA: 5:02 - loss: 0.6761 - acc: 0.5781
 960/4849 [====>.........................] - ETA: 4:57 - loss: 0.6793 - acc: 0.5750
1024/4849 [=====>........................] - ETA: 4:56 - loss: 0.6787 - acc: 0.5762
1088/4849 [=====>........................] - ETA: 4:52 - loss: 0.6751 - acc: 0.5818
1152/4849 [======>.......................] - ETA: 4:51 - loss: 0.6751 - acc: 0.5833
1216/4849 [======>.......................] - ETA: 4:48 - loss: 0.6766 - acc: 0.5822
1280/4849 [======>.......................] - ETA: 4:45 - loss: 0.6759 - acc: 0.5844
1344/4849 [=======>......................] - ETA: 4:42 - loss: 0.6748 - acc: 0.5848
1408/4849 [=======>......................] - ETA: 4:39 - loss: 0.6753 - acc: 0.5838
1472/4849 [========>.....................] - ETA: 4:36 - loss: 0.6748 - acc: 0.5842
1536/4849 [========>.....................] - ETA: 4:33 - loss: 0.6753 - acc: 0.5840
1600/4849 [========>.....................] - ETA: 4:29 - loss: 0.6761 - acc: 0.5806
1664/4849 [=========>....................] - ETA: 4:25 - loss: 0.6751 - acc: 0.5829
1728/4849 [=========>....................] - ETA: 4:21 - loss: 0.6744 - acc: 0.5856
1792/4849 [==========>...................] - ETA: 4:16 - loss: 0.6753 - acc: 0.5843
1856/4849 [==========>...................] - ETA: 4:12 - loss: 0.6746 - acc: 0.5857
1920/4849 [==========>...................] - ETA: 4:08 - loss: 0.6733 - acc: 0.5885
1984/4849 [===========>..................] - ETA: 4:03 - loss: 0.6728 - acc: 0.5907
2048/4849 [===========>..................] - ETA: 3:58 - loss: 0.6720 - acc: 0.5918
2112/4849 [============>.................] - ETA: 3:54 - loss: 0.6717 - acc: 0.5914
2176/4849 [============>.................] - ETA: 3:49 - loss: 0.6716 - acc: 0.5915
2240/4849 [============>.................] - ETA: 3:44 - loss: 0.6717 - acc: 0.5911
2304/4849 [=============>................] - ETA: 3:40 - loss: 0.6729 - acc: 0.5885
2368/4849 [=============>................] - ETA: 3:34 - loss: 0.6728 - acc: 0.5895
2432/4849 [==============>...............] - ETA: 3:30 - loss: 0.6733 - acc: 0.5900
2496/4849 [==============>...............] - ETA: 3:25 - loss: 0.6749 - acc: 0.5869
2560/4849 [==============>...............] - ETA: 3:20 - loss: 0.6744 - acc: 0.5867
2624/4849 [===============>..............] - ETA: 3:15 - loss: 0.6739 - acc: 0.5869
2688/4849 [===============>..............] - ETA: 3:10 - loss: 0.6730 - acc: 0.5889
2752/4849 [================>.............] - ETA: 3:04 - loss: 0.6726 - acc: 0.5905
2816/4849 [================>.............] - ETA: 2:59 - loss: 0.6735 - acc: 0.5888
2880/4849 [================>.............] - ETA: 2:54 - loss: 0.6724 - acc: 0.5906
2944/4849 [=================>............] - ETA: 2:48 - loss: 0.6730 - acc: 0.5897
3008/4849 [=================>............] - ETA: 2:43 - loss: 0.6726 - acc: 0.5921
3072/4849 [==================>...........] - ETA: 2:38 - loss: 0.6727 - acc: 0.5924
3136/4849 [==================>...........] - ETA: 2:32 - loss: 0.6735 - acc: 0.5906
3200/4849 [==================>...........] - ETA: 2:26 - loss: 0.6727 - acc: 0.5922
3264/4849 [===================>..........] - ETA: 2:21 - loss: 0.6737 - acc: 0.5901
3328/4849 [===================>..........] - ETA: 2:15 - loss: 0.6733 - acc: 0.5916
3392/4849 [===================>..........] - ETA: 2:10 - loss: 0.6730 - acc: 0.5929
3456/4849 [====================>.........] - ETA: 2:04 - loss: 0.6719 - acc: 0.5958
3520/4849 [====================>.........] - ETA: 1:58 - loss: 0.6728 - acc: 0.5943
3584/4849 [=====================>........] - ETA: 1:53 - loss: 0.6727 - acc: 0.5949
3648/4849 [=====================>........] - ETA: 1:47 - loss: 0.6729 - acc: 0.5946
3712/4849 [=====================>........] - ETA: 1:41 - loss: 0.6732 - acc: 0.5951
3776/4849 [======================>.......] - ETA: 1:36 - loss: 0.6736 - acc: 0.5935
3840/4849 [======================>.......] - ETA: 1:30 - loss: 0.6740 - acc: 0.5927
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6740 - acc: 0.5927
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6740 - acc: 0.5922
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6730 - acc: 0.5933
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6735 - acc: 0.5925
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6733 - acc: 0.5928
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6736 - acc: 0.5919 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6732 - acc: 0.5921
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6737 - acc: 0.5908
4416/4849 [==========================>...] - ETA: 39s - loss: 0.6739 - acc: 0.5899
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6745 - acc: 0.5884
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6745 - acc: 0.5885
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6748 - acc: 0.5872
4672/4849 [===========================>..] - ETA: 16s - loss: 0.6747 - acc: 0.5867
4736/4849 [============================>.] - ETA: 10s - loss: 0.6748 - acc: 0.5866
4800/4849 [============================>.] - ETA: 4s - loss: 0.6751 - acc: 0.5858 
4849/4849 [==============================] - 458s 94ms/step - loss: 0.6749 - acc: 0.5859 - val_loss: 0.6919 - val_acc: 0.5417

Epoch 00009: val_acc did not improve from 0.56957
Epoch 10/10

  64/4849 [..............................] - ETA: 7:15 - loss: 0.6740 - acc: 0.6250
 128/4849 [..............................] - ETA: 7:17 - loss: 0.6908 - acc: 0.5703
 192/4849 [>.............................] - ETA: 7:11 - loss: 0.6920 - acc: 0.5573
 256/4849 [>.............................] - ETA: 7:04 - loss: 0.6809 - acc: 0.5898
 320/4849 [>.............................] - ETA: 6:54 - loss: 0.6795 - acc: 0.5875
 384/4849 [=>............................] - ETA: 6:49 - loss: 0.6764 - acc: 0.5833
 448/4849 [=>............................] - ETA: 6:44 - loss: 0.6691 - acc: 0.6004
 512/4849 [==>...........................] - ETA: 6:37 - loss: 0.6693 - acc: 0.6035
 576/4849 [==>...........................] - ETA: 6:31 - loss: 0.6696 - acc: 0.5938
 640/4849 [==>...........................] - ETA: 6:25 - loss: 0.6697 - acc: 0.5922
 704/4849 [===>..........................] - ETA: 6:19 - loss: 0.6704 - acc: 0.6023
 768/4849 [===>..........................] - ETA: 6:12 - loss: 0.6686 - acc: 0.6003
 832/4849 [====>.........................] - ETA: 6:06 - loss: 0.6688 - acc: 0.6010
 896/4849 [====>.........................] - ETA: 6:00 - loss: 0.6699 - acc: 0.5971
 960/4849 [====>.........................] - ETA: 5:54 - loss: 0.6685 - acc: 0.5990
1024/4849 [=====>........................] - ETA: 5:47 - loss: 0.6665 - acc: 0.6055
1088/4849 [=====>........................] - ETA: 5:41 - loss: 0.6684 - acc: 0.5993
1152/4849 [======>.......................] - ETA: 5:35 - loss: 0.6687 - acc: 0.5990
1216/4849 [======>.......................] - ETA: 5:30 - loss: 0.6662 - acc: 0.6036
1280/4849 [======>.......................] - ETA: 5:24 - loss: 0.6666 - acc: 0.6039
1344/4849 [=======>......................] - ETA: 5:18 - loss: 0.6637 - acc: 0.6101
1408/4849 [=======>......................] - ETA: 5:12 - loss: 0.6646 - acc: 0.6087
1472/4849 [========>.....................] - ETA: 5:06 - loss: 0.6656 - acc: 0.6060
1536/4849 [========>.....................] - ETA: 5:00 - loss: 0.6661 - acc: 0.6055
1600/4849 [========>.....................] - ETA: 4:54 - loss: 0.6642 - acc: 0.6069
1664/4849 [=========>....................] - ETA: 4:48 - loss: 0.6638 - acc: 0.6088
1728/4849 [=========>....................] - ETA: 4:42 - loss: 0.6641 - acc: 0.6076
1792/4849 [==========>...................] - ETA: 4:37 - loss: 0.6640 - acc: 0.6083
1856/4849 [==========>...................] - ETA: 4:31 - loss: 0.6648 - acc: 0.6061
1920/4849 [==========>...................] - ETA: 4:25 - loss: 0.6646 - acc: 0.6068
1984/4849 [===========>..................] - ETA: 4:19 - loss: 0.6646 - acc: 0.6069
2048/4849 [===========>..................] - ETA: 4:13 - loss: 0.6655 - acc: 0.6045
2112/4849 [============>.................] - ETA: 4:07 - loss: 0.6650 - acc: 0.6042
2176/4849 [============>.................] - ETA: 4:01 - loss: 0.6650 - acc: 0.6034
2240/4849 [============>.................] - ETA: 3:55 - loss: 0.6638 - acc: 0.6062
2304/4849 [=============>................] - ETA: 3:49 - loss: 0.6649 - acc: 0.6037
2368/4849 [=============>................] - ETA: 3:43 - loss: 0.6655 - acc: 0.6030
2432/4849 [==============>...............] - ETA: 3:37 - loss: 0.6650 - acc: 0.6049
2496/4849 [==============>...............] - ETA: 3:32 - loss: 0.6653 - acc: 0.6038
2560/4849 [==============>...............] - ETA: 3:26 - loss: 0.6651 - acc: 0.6043
2624/4849 [===============>..............] - ETA: 3:21 - loss: 0.6646 - acc: 0.6040
2688/4849 [===============>..............] - ETA: 3:14 - loss: 0.6642 - acc: 0.6042
2752/4849 [================>.............] - ETA: 3:09 - loss: 0.6632 - acc: 0.6054
2816/4849 [================>.............] - ETA: 3:03 - loss: 0.6639 - acc: 0.6044
2880/4849 [================>.............] - ETA: 2:57 - loss: 0.6634 - acc: 0.6056
2944/4849 [=================>............] - ETA: 2:51 - loss: 0.6631 - acc: 0.6063
3008/4849 [=================>............] - ETA: 2:46 - loss: 0.6626 - acc: 0.6074
3072/4849 [==================>...........] - ETA: 2:40 - loss: 0.6622 - acc: 0.6074
3136/4849 [==================>...........] - ETA: 2:34 - loss: 0.6625 - acc: 0.6065
3200/4849 [==================>...........] - ETA: 2:28 - loss: 0.6623 - acc: 0.6081
3264/4849 [===================>..........] - ETA: 2:22 - loss: 0.6623 - acc: 0.6078
3328/4849 [===================>..........] - ETA: 2:16 - loss: 0.6632 - acc: 0.6064
3392/4849 [===================>..........] - ETA: 2:11 - loss: 0.6640 - acc: 0.6058
3456/4849 [====================>.........] - ETA: 2:05 - loss: 0.6643 - acc: 0.6062
3520/4849 [====================>.........] - ETA: 1:59 - loss: 0.6657 - acc: 0.6051
3584/4849 [=====================>........] - ETA: 1:53 - loss: 0.6655 - acc: 0.6049
3648/4849 [=====================>........] - ETA: 1:48 - loss: 0.6663 - acc: 0.6039
3712/4849 [=====================>........] - ETA: 1:42 - loss: 0.6673 - acc: 0.6026
3776/4849 [======================>.......] - ETA: 1:36 - loss: 0.6685 - acc: 0.6006
3840/4849 [======================>.......] - ETA: 1:30 - loss: 0.6683 - acc: 0.6008
3904/4849 [=======================>......] - ETA: 1:25 - loss: 0.6689 - acc: 0.6007
3968/4849 [=======================>......] - ETA: 1:19 - loss: 0.6692 - acc: 0.6003
4032/4849 [=======================>......] - ETA: 1:13 - loss: 0.6686 - acc: 0.6014
4096/4849 [========================>.....] - ETA: 1:07 - loss: 0.6683 - acc: 0.6011
4160/4849 [========================>.....] - ETA: 1:02 - loss: 0.6678 - acc: 0.6019
4224/4849 [=========================>....] - ETA: 56s - loss: 0.6678 - acc: 0.6020 
4288/4849 [=========================>....] - ETA: 50s - loss: 0.6679 - acc: 0.6019
4352/4849 [=========================>....] - ETA: 44s - loss: 0.6682 - acc: 0.6009
4416/4849 [==========================>...] - ETA: 38s - loss: 0.6677 - acc: 0.6021
4480/4849 [==========================>...] - ETA: 33s - loss: 0.6674 - acc: 0.6029
4544/4849 [===========================>..] - ETA: 27s - loss: 0.6679 - acc: 0.6023
4608/4849 [===========================>..] - ETA: 21s - loss: 0.6684 - acc: 0.6016
4672/4849 [===========================>..] - ETA: 15s - loss: 0.6689 - acc: 0.6012
4736/4849 [============================>.] - ETA: 10s - loss: 0.6696 - acc: 0.6001
4800/4849 [============================>.] - ETA: 4s - loss: 0.6701 - acc: 0.5990 
4849/4849 [==============================] - 453s 93ms/step - loss: 0.6703 - acc: 0.5991 - val_loss: 0.6978 - val_acc: 0.5176

Epoch 00010: val_acc did not improve from 0.56957
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7b86f71c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7b86f71c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7b86e26890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7b86e26890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86e79a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86e79a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b44b10410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b44b10410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b44769950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b44769950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b44aa9750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b44aa9750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b44b10710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b44b10710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73940871d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73940871d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73946e2b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73946e2b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b4485a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b4485a1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b4496c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b4496c710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b44774810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b44774810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f75e85e3950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f75e85e3950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f735068ee90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f735068ee90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b4466bf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b4466bf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73946f4810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73946f4810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f735068e310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f735068e310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b4466b710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b4466b710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b443a0d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b443a0d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b4422e350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b4422e350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b4427b410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b4427b410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b4436f2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b4436f2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b4412e590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b4412e590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b4426e950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7b4426e950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b43e8ec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b43e8ec90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b43fc09d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b43fc09d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b4426ecd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7b4426ecd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b43ed4750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b43ed4750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7798e96bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7798e96bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b43edba10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b43edba10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b07d24c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b07d24c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7798e96550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7798e96550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7798e25710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7798e25710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74f02d68d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74f02d68d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74f018ec10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74f018ec10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7798dbf4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7798dbf4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74f02d61d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74f02d61d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74f0303f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74f0303f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74f01f1810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74f01f1810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74d86738d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74d86738d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d87cad90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d87cad90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74f01f1090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74f01f1090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d86b7510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d86b7510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74d87b0a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74d87b0a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74d831ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74d831ded0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d8407850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d8407850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74d8664750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74d8664750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d8314210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d8314210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74d83146d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74d83146d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74d810ea50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74d810ea50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d8358950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d8358950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74d83dea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74d83dea10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d8165810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74d8165810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74b4579bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74b4579bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74b449ac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74b449ac90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74b44ff410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74b44ff410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74b45797d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74b45797d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74b4391ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74b4391ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74b4389610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74b4389610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74b413da10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74b413da10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74b42bf350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74b42bf350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74b42c3b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74b42c3b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74b42aac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74b42aac90>>: AttributeError: module 'gast' has no attribute 'Str'
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 2:06
 128/1348 [=>............................] - ETA: 1:16
 192/1348 [===>..........................] - ETA: 56s 
 256/1348 [====>.........................] - ETA: 46s
 320/1348 [======>.......................] - ETA: 40s
 384/1348 [=======>......................] - ETA: 35s
 448/1348 [========>.....................] - ETA: 31s
 512/1348 [==========>...................] - ETA: 27s
 576/1348 [===========>..................] - ETA: 24s
 640/1348 [=============>................] - ETA: 22s
 704/1348 [==============>...............] - ETA: 19s
 768/1348 [================>.............] - ETA: 17s
 832/1348 [=================>............] - ETA: 15s
 896/1348 [==================>...........] - ETA: 13s
 960/1348 [====================>.........] - ETA: 11s
1024/1348 [=====================>........] - ETA: 9s 
1088/1348 [=======================>......] - ETA: 7s
1152/1348 [========================>.....] - ETA: 5s
1216/1348 [==========================>...] - ETA: 3s
1280/1348 [===========================>..] - ETA: 1s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 37s 28ms/step
loss: 0.6824774264581833
acc: 0.5541543026706232
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f72f06177d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f72f06177d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f747826ff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f747826ff10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f05cd890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f05cd890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7458701f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7458701f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b86cd58d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7b86cd58d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72a860a050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72a860a050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f747816e290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f747816e290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86cfd350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86cfd350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7478147cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7478147cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72f051df10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72f051df10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86d46750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7b86d46750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72f04c9bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72f04c9bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f04cc110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f04cc110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f051d850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f051d850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72f01e7b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72f01e7b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f0315990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f0315990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72f051ddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72f051ddd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f01e4110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f01e4110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72d47a7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72d47a7290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72d464ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72d464ded0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d47a35d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d47a35d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72f032ad90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72f032ad90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d45a7190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d45a7190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72d4492750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72d4492750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72d43bbc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72d43bbc90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d45298d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d45298d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72d45b15d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72d45b15d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d43a8250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d43a8250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72d4144c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72d4144c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72b07c4790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72b07c4790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d4160350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d4160350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72d444ec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72d444ec90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72b0680d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72b0680d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72b06262d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72b06262d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72b058db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72b058db90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72b0654950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72b0654950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72b0626890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72b0626890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72b0542190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72b0542190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72b053e890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72b053e890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72b018ff50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72b018ff50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72b0332b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72b0332b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72b053e190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72b053e190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72b02e9250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72b02e9250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72a87850d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72a87850d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72a865dd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72a865dd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72a87e12d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72a87e12d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72b00f0250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72b00f0250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72a87e1c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72a87e1c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f71cc5a7ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f71cc5a7ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f71cc443310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f71cc443310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71cc4876d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71cc4876d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f71cc5a76d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f71cc5a76d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71cc495c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71cc495c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f71cc2e2f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f71cc2e2f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f71cc15d410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f71cc15d410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71cc2a08d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71cc2a08d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f71cc2e2850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f71cc2e2850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71947bf3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71947bf3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f71947e9cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f71947e9cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f71945bad10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f71945bad10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7194719b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7194719b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f71947e9150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f71947e9150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71944bd190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71944bd190>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 23:50 - loss: 0.7601 - acc: 0.4219
 128/4849 [..............................] - ETA: 14:49 - loss: 0.8846 - acc: 0.4219
 192/4849 [>.............................] - ETA: 11:51 - loss: 0.8258 - acc: 0.4635
 256/4849 [>.............................] - ETA: 10:15 - loss: 0.8156 - acc: 0.4648
 320/4849 [>.............................] - ETA: 9:17 - loss: 0.8045 - acc: 0.4750 
 384/4849 [=>............................] - ETA: 8:35 - loss: 0.7999 - acc: 0.4792
 448/4849 [=>............................] - ETA: 8:02 - loss: 0.7952 - acc: 0.4688
 512/4849 [==>...........................] - ETA: 7:45 - loss: 0.7833 - acc: 0.4785
 576/4849 [==>...........................] - ETA: 7:25 - loss: 0.7780 - acc: 0.4792
 640/4849 [==>...........................] - ETA: 7:09 - loss: 0.7753 - acc: 0.4766
 704/4849 [===>..........................] - ETA: 6:53 - loss: 0.7686 - acc: 0.4858
 768/4849 [===>..........................] - ETA: 6:40 - loss: 0.7637 - acc: 0.4896
 832/4849 [====>.........................] - ETA: 6:29 - loss: 0.7577 - acc: 0.4988
 896/4849 [====>.........................] - ETA: 6:18 - loss: 0.7566 - acc: 0.4967
 960/4849 [====>.........................] - ETA: 6:07 - loss: 0.7594 - acc: 0.4906
1024/4849 [=====>........................] - ETA: 6:00 - loss: 0.7545 - acc: 0.4961
1088/4849 [=====>........................] - ETA: 5:51 - loss: 0.7515 - acc: 0.5000
1152/4849 [======>.......................] - ETA: 5:42 - loss: 0.7513 - acc: 0.5000
1216/4849 [======>.......................] - ETA: 5:34 - loss: 0.7487 - acc: 0.5008
1280/4849 [======>.......................] - ETA: 5:26 - loss: 0.7461 - acc: 0.5039
1344/4849 [=======>......................] - ETA: 5:18 - loss: 0.7411 - acc: 0.5126
1408/4849 [=======>......................] - ETA: 5:11 - loss: 0.7382 - acc: 0.5156
1472/4849 [========>.....................] - ETA: 5:04 - loss: 0.7362 - acc: 0.5170
1536/4849 [========>.....................] - ETA: 4:57 - loss: 0.7360 - acc: 0.5176
1600/4849 [========>.....................] - ETA: 4:51 - loss: 0.7365 - acc: 0.5200
1664/4849 [=========>....................] - ETA: 4:45 - loss: 0.7338 - acc: 0.5222
1728/4849 [=========>....................] - ETA: 4:38 - loss: 0.7336 - acc: 0.5214
1792/4849 [==========>...................] - ETA: 4:31 - loss: 0.7330 - acc: 0.5223
1856/4849 [==========>...................] - ETA: 4:24 - loss: 0.7305 - acc: 0.5248
1920/4849 [==========>...................] - ETA: 4:18 - loss: 0.7296 - acc: 0.5266
1984/4849 [===========>..................] - ETA: 4:11 - loss: 0.7274 - acc: 0.5282
2048/4849 [===========>..................] - ETA: 4:05 - loss: 0.7267 - acc: 0.5283
2112/4849 [============>.................] - ETA: 3:59 - loss: 0.7271 - acc: 0.5256
2176/4849 [============>.................] - ETA: 3:53 - loss: 0.7251 - acc: 0.5262
2240/4849 [============>.................] - ETA: 3:47 - loss: 0.7236 - acc: 0.5286
2304/4849 [=============>................] - ETA: 3:41 - loss: 0.7232 - acc: 0.5273
2368/4849 [=============>................] - ETA: 3:35 - loss: 0.7228 - acc: 0.5287
2432/4849 [==============>...............] - ETA: 3:29 - loss: 0.7224 - acc: 0.5275
2496/4849 [==============>...............] - ETA: 3:23 - loss: 0.7202 - acc: 0.5300
2560/4849 [==============>...............] - ETA: 3:17 - loss: 0.7212 - acc: 0.5277
2624/4849 [===============>..............] - ETA: 3:11 - loss: 0.7206 - acc: 0.5274
2688/4849 [===============>..............] - ETA: 3:05 - loss: 0.7204 - acc: 0.5272
2752/4849 [================>.............] - ETA: 3:00 - loss: 0.7194 - acc: 0.5291
2816/4849 [================>.............] - ETA: 2:54 - loss: 0.7188 - acc: 0.5305
2880/4849 [================>.............] - ETA: 2:49 - loss: 0.7180 - acc: 0.5319
2944/4849 [=================>............] - ETA: 2:43 - loss: 0.7170 - acc: 0.5319
3008/4849 [=================>............] - ETA: 2:37 - loss: 0.7170 - acc: 0.5306
3072/4849 [==================>...........] - ETA: 2:31 - loss: 0.7165 - acc: 0.5322
3136/4849 [==================>...........] - ETA: 2:26 - loss: 0.7175 - acc: 0.5309
3200/4849 [==================>...........] - ETA: 2:20 - loss: 0.7180 - acc: 0.5303
3264/4849 [===================>..........] - ETA: 2:15 - loss: 0.7174 - acc: 0.5325
3328/4849 [===================>..........] - ETA: 2:09 - loss: 0.7164 - acc: 0.5334
3392/4849 [===================>..........] - ETA: 2:04 - loss: 0.7167 - acc: 0.5321
3456/4849 [====================>.........] - ETA: 1:58 - loss: 0.7167 - acc: 0.5321
3520/4849 [====================>.........] - ETA: 1:52 - loss: 0.7166 - acc: 0.5324
3584/4849 [=====================>........] - ETA: 1:47 - loss: 0.7165 - acc: 0.5315
3648/4849 [=====================>........] - ETA: 1:41 - loss: 0.7160 - acc: 0.5310
3712/4849 [=====================>........] - ETA: 1:36 - loss: 0.7153 - acc: 0.5329
3776/4849 [======================>.......] - ETA: 1:30 - loss: 0.7159 - acc: 0.5307
3840/4849 [======================>.......] - ETA: 1:25 - loss: 0.7154 - acc: 0.5318
3904/4849 [=======================>......] - ETA: 1:19 - loss: 0.7163 - acc: 0.5292
3968/4849 [=======================>......] - ETA: 1:14 - loss: 0.7164 - acc: 0.5282
4032/4849 [=======================>......] - ETA: 1:09 - loss: 0.7165 - acc: 0.5285
4096/4849 [========================>.....] - ETA: 1:03 - loss: 0.7164 - acc: 0.5293
4160/4849 [========================>.....] - ETA: 58s - loss: 0.7160 - acc: 0.5298 
4224/4849 [=========================>....] - ETA: 52s - loss: 0.7161 - acc: 0.5294
4288/4849 [=========================>....] - ETA: 47s - loss: 0.7158 - acc: 0.5301
4352/4849 [=========================>....] - ETA: 41s - loss: 0.7160 - acc: 0.5303
4416/4849 [==========================>...] - ETA: 36s - loss: 0.7157 - acc: 0.5301
4480/4849 [==========================>...] - ETA: 30s - loss: 0.7157 - acc: 0.5301
4544/4849 [===========================>..] - ETA: 25s - loss: 0.7154 - acc: 0.5299
4608/4849 [===========================>..] - ETA: 20s - loss: 0.7154 - acc: 0.5293
4672/4849 [===========================>..] - ETA: 14s - loss: 0.7153 - acc: 0.5295
4736/4849 [============================>.] - ETA: 9s - loss: 0.7155 - acc: 0.5298 
4800/4849 [============================>.] - ETA: 4s - loss: 0.7147 - acc: 0.5308
4849/4849 [==============================] - 425s 88ms/step - loss: 0.7144 - acc: 0.5308 - val_loss: 0.6927 - val_acc: 0.5492

Epoch 00001: val_acc improved from -inf to 0.54917, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window14/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 6:07 - loss: 0.7187 - acc: 0.4531
 128/4849 [..............................] - ETA: 6:14 - loss: 0.6996 - acc: 0.4844
 192/4849 [>.............................] - ETA: 6:04 - loss: 0.7101 - acc: 0.4792
 256/4849 [>.............................] - ETA: 5:59 - loss: 0.6992 - acc: 0.5195
 320/4849 [>.............................] - ETA: 5:49 - loss: 0.7052 - acc: 0.5250
 384/4849 [=>............................] - ETA: 5:51 - loss: 0.7018 - acc: 0.5312
 448/4849 [=>............................] - ETA: 5:44 - loss: 0.7006 - acc: 0.5290
 512/4849 [==>...........................] - ETA: 5:38 - loss: 0.7047 - acc: 0.5176
 576/4849 [==>...........................] - ETA: 5:34 - loss: 0.6986 - acc: 0.5295
 640/4849 [==>...........................] - ETA: 5:30 - loss: 0.6995 - acc: 0.5359
 704/4849 [===>..........................] - ETA: 5:23 - loss: 0.6965 - acc: 0.5440
 768/4849 [===>..........................] - ETA: 5:17 - loss: 0.7043 - acc: 0.5326
 832/4849 [====>.........................] - ETA: 5:09 - loss: 0.7043 - acc: 0.5312
 896/4849 [====>.........................] - ETA: 5:02 - loss: 0.7034 - acc: 0.5335
 960/4849 [====>.........................] - ETA: 4:54 - loss: 0.6993 - acc: 0.5385
1024/4849 [=====>........................] - ETA: 4:49 - loss: 0.7012 - acc: 0.5371
1088/4849 [=====>........................] - ETA: 4:42 - loss: 0.7012 - acc: 0.5368
1152/4849 [======>.......................] - ETA: 4:35 - loss: 0.7001 - acc: 0.5382
1216/4849 [======>.......................] - ETA: 4:30 - loss: 0.7009 - acc: 0.5362
1280/4849 [======>.......................] - ETA: 4:24 - loss: 0.6993 - acc: 0.5359
1344/4849 [=======>......................] - ETA: 4:19 - loss: 0.7000 - acc: 0.5327
1408/4849 [=======>......................] - ETA: 4:13 - loss: 0.7005 - acc: 0.5291
1472/4849 [========>.....................] - ETA: 4:09 - loss: 0.7013 - acc: 0.5272
1536/4849 [========>.....................] - ETA: 4:03 - loss: 0.7007 - acc: 0.5273
1600/4849 [========>.....................] - ETA: 3:58 - loss: 0.7016 - acc: 0.5231
1664/4849 [=========>....................] - ETA: 3:53 - loss: 0.7015 - acc: 0.5246
1728/4849 [=========>....................] - ETA: 3:49 - loss: 0.6995 - acc: 0.5278
1792/4849 [==========>...................] - ETA: 3:44 - loss: 0.7003 - acc: 0.5257
1856/4849 [==========>...................] - ETA: 3:38 - loss: 0.7018 - acc: 0.5242
1920/4849 [==========>...................] - ETA: 3:33 - loss: 0.7027 - acc: 0.5214
1984/4849 [===========>..................] - ETA: 3:29 - loss: 0.7005 - acc: 0.5257
2048/4849 [===========>..................] - ETA: 3:24 - loss: 0.7001 - acc: 0.5269
2112/4849 [============>.................] - ETA: 3:19 - loss: 0.6989 - acc: 0.5298
2176/4849 [============>.................] - ETA: 3:14 - loss: 0.6980 - acc: 0.5322
2240/4849 [============>.................] - ETA: 3:09 - loss: 0.6980 - acc: 0.5339
2304/4849 [=============>................] - ETA: 3:04 - loss: 0.6981 - acc: 0.5343
2368/4849 [=============>................] - ETA: 2:59 - loss: 0.6970 - acc: 0.5363
2432/4849 [==============>...............] - ETA: 2:54 - loss: 0.6965 - acc: 0.5366
2496/4849 [==============>...............] - ETA: 2:49 - loss: 0.6978 - acc: 0.5337
2560/4849 [==============>...............] - ETA: 2:45 - loss: 0.6988 - acc: 0.5324
2624/4849 [===============>..............] - ETA: 2:40 - loss: 0.6975 - acc: 0.5347
2688/4849 [===============>..............] - ETA: 2:35 - loss: 0.6972 - acc: 0.5357
2752/4849 [================>.............] - ETA: 2:31 - loss: 0.6976 - acc: 0.5356
2816/4849 [================>.............] - ETA: 2:26 - loss: 0.6975 - acc: 0.5359
2880/4849 [================>.............] - ETA: 2:21 - loss: 0.6971 - acc: 0.5375
2944/4849 [=================>............] - ETA: 2:16 - loss: 0.6967 - acc: 0.5394
3008/4849 [=================>............] - ETA: 2:12 - loss: 0.6965 - acc: 0.5396
3072/4849 [==================>...........] - ETA: 2:07 - loss: 0.6958 - acc: 0.5413
3136/4849 [==================>...........] - ETA: 2:03 - loss: 0.6959 - acc: 0.5411
3200/4849 [==================>...........] - ETA: 1:58 - loss: 0.6963 - acc: 0.5416
3264/4849 [===================>..........] - ETA: 1:53 - loss: 0.6964 - acc: 0.5407
3328/4849 [===================>..........] - ETA: 1:48 - loss: 0.6961 - acc: 0.5406
3392/4849 [===================>..........] - ETA: 1:44 - loss: 0.6963 - acc: 0.5401
3456/4849 [====================>.........] - ETA: 1:39 - loss: 0.6962 - acc: 0.5399
3520/4849 [====================>.........] - ETA: 1:35 - loss: 0.6959 - acc: 0.5418
3584/4849 [=====================>........] - ETA: 1:30 - loss: 0.6955 - acc: 0.5432
3648/4849 [=====================>........] - ETA: 1:25 - loss: 0.6958 - acc: 0.5439
3712/4849 [=====================>........] - ETA: 1:21 - loss: 0.6961 - acc: 0.5436
3776/4849 [======================>.......] - ETA: 1:16 - loss: 0.6958 - acc: 0.5440
3840/4849 [======================>.......] - ETA: 1:12 - loss: 0.6966 - acc: 0.5427
3904/4849 [=======================>......] - ETA: 1:07 - loss: 0.6960 - acc: 0.5441
3968/4849 [=======================>......] - ETA: 1:02 - loss: 0.6960 - acc: 0.5433
4032/4849 [=======================>......] - ETA: 58s - loss: 0.6955 - acc: 0.5441 
4096/4849 [========================>.....] - ETA: 53s - loss: 0.6953 - acc: 0.5439
4160/4849 [========================>.....] - ETA: 49s - loss: 0.6951 - acc: 0.5442
4224/4849 [=========================>....] - ETA: 44s - loss: 0.6953 - acc: 0.5436
4288/4849 [=========================>....] - ETA: 40s - loss: 0.6953 - acc: 0.5427
4352/4849 [=========================>....] - ETA: 35s - loss: 0.6951 - acc: 0.5427
4416/4849 [==========================>...] - ETA: 30s - loss: 0.6952 - acc: 0.5421
4480/4849 [==========================>...] - ETA: 26s - loss: 0.6953 - acc: 0.5411
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6954 - acc: 0.5414
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6955 - acc: 0.5412
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6951 - acc: 0.5420
4736/4849 [============================>.] - ETA: 8s - loss: 0.6950 - acc: 0.5420 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6953 - acc: 0.5413
4849/4849 [==============================] - 361s 74ms/step - loss: 0.6951 - acc: 0.5409 - val_loss: 0.7070 - val_acc: 0.5158

Epoch 00002: val_acc did not improve from 0.54917
Epoch 3/10

  64/4849 [..............................] - ETA: 4:56 - loss: 0.7549 - acc: 0.4062
 128/4849 [..............................] - ETA: 5:05 - loss: 0.7295 - acc: 0.4531
 192/4849 [>.............................] - ETA: 4:58 - loss: 0.7178 - acc: 0.4844
 256/4849 [>.............................] - ETA: 4:57 - loss: 0.7152 - acc: 0.4766
 320/4849 [>.............................] - ETA: 4:52 - loss: 0.7156 - acc: 0.4750
 384/4849 [=>............................] - ETA: 4:54 - loss: 0.7107 - acc: 0.4870
 448/4849 [=>............................] - ETA: 4:51 - loss: 0.7054 - acc: 0.4978
 512/4849 [==>...........................] - ETA: 4:44 - loss: 0.7014 - acc: 0.5059
 576/4849 [==>...........................] - ETA: 4:43 - loss: 0.6992 - acc: 0.5052
 640/4849 [==>...........................] - ETA: 4:38 - loss: 0.7000 - acc: 0.5016
 704/4849 [===>..........................] - ETA: 4:35 - loss: 0.6994 - acc: 0.5028
 768/4849 [===>..........................] - ETA: 4:32 - loss: 0.7026 - acc: 0.5000
 832/4849 [====>.........................] - ETA: 4:28 - loss: 0.6989 - acc: 0.5072
 896/4849 [====>.........................] - ETA: 4:23 - loss: 0.6950 - acc: 0.5100
 960/4849 [====>.........................] - ETA: 4:18 - loss: 0.6922 - acc: 0.5208
1024/4849 [=====>........................] - ETA: 4:15 - loss: 0.6915 - acc: 0.5215
1088/4849 [=====>........................] - ETA: 4:11 - loss: 0.6906 - acc: 0.5221
1152/4849 [======>.......................] - ETA: 4:06 - loss: 0.6922 - acc: 0.5243
1216/4849 [======>.......................] - ETA: 4:02 - loss: 0.6928 - acc: 0.5255
1280/4849 [======>.......................] - ETA: 3:59 - loss: 0.6939 - acc: 0.5250
1344/4849 [=======>......................] - ETA: 3:54 - loss: 0.6931 - acc: 0.5327
1408/4849 [=======>......................] - ETA: 3:50 - loss: 0.6928 - acc: 0.5341
1472/4849 [========>.....................] - ETA: 3:46 - loss: 0.6934 - acc: 0.5312
1536/4849 [========>.....................] - ETA: 3:41 - loss: 0.6923 - acc: 0.5352
1600/4849 [========>.....................] - ETA: 3:37 - loss: 0.6913 - acc: 0.5387
1664/4849 [=========>....................] - ETA: 3:32 - loss: 0.6908 - acc: 0.5409
1728/4849 [=========>....................] - ETA: 3:28 - loss: 0.6916 - acc: 0.5417
1792/4849 [==========>...................] - ETA: 3:23 - loss: 0.6891 - acc: 0.5463
1856/4849 [==========>...................] - ETA: 3:20 - loss: 0.6886 - acc: 0.5474
1920/4849 [==========>...................] - ETA: 3:14 - loss: 0.6874 - acc: 0.5490
1984/4849 [===========>..................] - ETA: 3:10 - loss: 0.6882 - acc: 0.5464
2048/4849 [===========>..................] - ETA: 3:06 - loss: 0.6873 - acc: 0.5493
2112/4849 [============>.................] - ETA: 3:01 - loss: 0.6870 - acc: 0.5492
2176/4849 [============>.................] - ETA: 2:57 - loss: 0.6877 - acc: 0.5483
2240/4849 [============>.................] - ETA: 2:52 - loss: 0.6875 - acc: 0.5487
2304/4849 [=============>................] - ETA: 2:48 - loss: 0.6880 - acc: 0.5469
2368/4849 [=============>................] - ETA: 2:44 - loss: 0.6876 - acc: 0.5490
2432/4849 [==============>...............] - ETA: 2:39 - loss: 0.6869 - acc: 0.5502
2496/4849 [==============>...............] - ETA: 2:35 - loss: 0.6877 - acc: 0.5473
2560/4849 [==============>...............] - ETA: 2:30 - loss: 0.6877 - acc: 0.5480
2624/4849 [===============>..............] - ETA: 2:26 - loss: 0.6866 - acc: 0.5511
2688/4849 [===============>..............] - ETA: 2:21 - loss: 0.6862 - acc: 0.5521
2752/4849 [================>.............] - ETA: 2:17 - loss: 0.6854 - acc: 0.5538
2816/4849 [================>.............] - ETA: 2:13 - loss: 0.6855 - acc: 0.5540
2880/4849 [================>.............] - ETA: 2:09 - loss: 0.6846 - acc: 0.5563
2944/4849 [=================>............] - ETA: 2:05 - loss: 0.6843 - acc: 0.5577
3008/4849 [=================>............] - ETA: 2:00 - loss: 0.6846 - acc: 0.5582
3072/4849 [==================>...........] - ETA: 1:56 - loss: 0.6846 - acc: 0.5586
3136/4849 [==================>...........] - ETA: 1:52 - loss: 0.6849 - acc: 0.5590
3200/4849 [==================>...........] - ETA: 1:47 - loss: 0.6855 - acc: 0.5578
3264/4849 [===================>..........] - ETA: 1:43 - loss: 0.6855 - acc: 0.5597
3328/4849 [===================>..........] - ETA: 1:39 - loss: 0.6856 - acc: 0.5604
3392/4849 [===================>..........] - ETA: 1:35 - loss: 0.6854 - acc: 0.5601
3456/4849 [====================>.........] - ETA: 1:31 - loss: 0.6851 - acc: 0.5608
3520/4849 [====================>.........] - ETA: 1:27 - loss: 0.6856 - acc: 0.5591
3584/4849 [=====================>........] - ETA: 1:22 - loss: 0.6856 - acc: 0.5592
3648/4849 [=====================>........] - ETA: 1:18 - loss: 0.6848 - acc: 0.5617
3712/4849 [=====================>........] - ETA: 1:14 - loss: 0.6851 - acc: 0.5628
3776/4849 [======================>.......] - ETA: 1:10 - loss: 0.6846 - acc: 0.5630
3840/4849 [======================>.......] - ETA: 1:06 - loss: 0.6845 - acc: 0.5635
3904/4849 [=======================>......] - ETA: 1:01 - loss: 0.6843 - acc: 0.5651
3968/4849 [=======================>......] - ETA: 57s - loss: 0.6837 - acc: 0.5663 
4032/4849 [=======================>......] - ETA: 53s - loss: 0.6848 - acc: 0.5647
4096/4849 [========================>.....] - ETA: 49s - loss: 0.6850 - acc: 0.5652
4160/4849 [========================>.....] - ETA: 45s - loss: 0.6842 - acc: 0.5668
4224/4849 [=========================>....] - ETA: 40s - loss: 0.6842 - acc: 0.5675
4288/4849 [=========================>....] - ETA: 36s - loss: 0.6841 - acc: 0.5679
4352/4849 [=========================>....] - ETA: 32s - loss: 0.6836 - acc: 0.5685
4416/4849 [==========================>...] - ETA: 28s - loss: 0.6836 - acc: 0.5686
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6839 - acc: 0.5690
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6843 - acc: 0.5676
4608/4849 [===========================>..] - ETA: 15s - loss: 0.6844 - acc: 0.5677
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6843 - acc: 0.5672
4736/4849 [============================>.] - ETA: 7s - loss: 0.6839 - acc: 0.5680 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6836 - acc: 0.5683
4849/4849 [==============================] - 332s 69ms/step - loss: 0.6841 - acc: 0.5684 - val_loss: 0.6868 - val_acc: 0.5640

Epoch 00003: val_acc improved from 0.54917 to 0.56401, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window14/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 4/10

  64/4849 [..............................] - ETA: 5:32 - loss: 0.7125 - acc: 0.5469
 128/4849 [..............................] - ETA: 5:38 - loss: 0.7128 - acc: 0.5312
 192/4849 [>.............................] - ETA: 5:33 - loss: 0.6960 - acc: 0.5417
 256/4849 [>.............................] - ETA: 5:17 - loss: 0.6843 - acc: 0.5664
 320/4849 [>.............................] - ETA: 5:15 - loss: 0.6765 - acc: 0.5750
 384/4849 [=>............................] - ETA: 5:06 - loss: 0.6810 - acc: 0.5521
 448/4849 [=>............................] - ETA: 5:03 - loss: 0.6796 - acc: 0.5603
 512/4849 [==>...........................] - ETA: 4:59 - loss: 0.6766 - acc: 0.5645
 576/4849 [==>...........................] - ETA: 4:54 - loss: 0.6744 - acc: 0.5729
 640/4849 [==>...........................] - ETA: 4:50 - loss: 0.6689 - acc: 0.5859
 704/4849 [===>..........................] - ETA: 4:44 - loss: 0.6671 - acc: 0.5909
 768/4849 [===>..........................] - ETA: 4:39 - loss: 0.6722 - acc: 0.5781
 832/4849 [====>.........................] - ETA: 4:33 - loss: 0.6747 - acc: 0.5781
 896/4849 [====>.........................] - ETA: 4:31 - loss: 0.6818 - acc: 0.5670
 960/4849 [====>.........................] - ETA: 4:26 - loss: 0.6824 - acc: 0.5677
1024/4849 [=====>........................] - ETA: 4:21 - loss: 0.6807 - acc: 0.5674
1088/4849 [=====>........................] - ETA: 4:17 - loss: 0.6824 - acc: 0.5653
1152/4849 [======>.......................] - ETA: 4:12 - loss: 0.6818 - acc: 0.5651
1216/4849 [======>.......................] - ETA: 4:08 - loss: 0.6811 - acc: 0.5658
1280/4849 [======>.......................] - ETA: 4:03 - loss: 0.6838 - acc: 0.5633
1344/4849 [=======>......................] - ETA: 3:58 - loss: 0.6814 - acc: 0.5692
1408/4849 [=======>......................] - ETA: 3:53 - loss: 0.6818 - acc: 0.5682
1472/4849 [========>.....................] - ETA: 3:48 - loss: 0.6825 - acc: 0.5679
1536/4849 [========>.....................] - ETA: 3:43 - loss: 0.6829 - acc: 0.5671
1600/4849 [========>.....................] - ETA: 3:39 - loss: 0.6819 - acc: 0.5687
1664/4849 [=========>....................] - ETA: 3:35 - loss: 0.6825 - acc: 0.5673
1728/4849 [=========>....................] - ETA: 3:30 - loss: 0.6825 - acc: 0.5671
1792/4849 [==========>...................] - ETA: 3:26 - loss: 0.6821 - acc: 0.5675
1856/4849 [==========>...................] - ETA: 3:21 - loss: 0.6815 - acc: 0.5684
1920/4849 [==========>...................] - ETA: 3:17 - loss: 0.6808 - acc: 0.5698
1984/4849 [===========>..................] - ETA: 3:12 - loss: 0.6815 - acc: 0.5691
2048/4849 [===========>..................] - ETA: 3:08 - loss: 0.6817 - acc: 0.5688
2112/4849 [============>.................] - ETA: 3:04 - loss: 0.6816 - acc: 0.5701
2176/4849 [============>.................] - ETA: 3:00 - loss: 0.6807 - acc: 0.5726
2240/4849 [============>.................] - ETA: 2:55 - loss: 0.6815 - acc: 0.5701
2304/4849 [=============>................] - ETA: 2:51 - loss: 0.6812 - acc: 0.5703
2368/4849 [=============>................] - ETA: 2:47 - loss: 0.6816 - acc: 0.5722
2432/4849 [==============>...............] - ETA: 2:42 - loss: 0.6813 - acc: 0.5720
2496/4849 [==============>...............] - ETA: 2:38 - loss: 0.6797 - acc: 0.5757
2560/4849 [==============>...............] - ETA: 2:33 - loss: 0.6801 - acc: 0.5770
2624/4849 [===============>..............] - ETA: 2:29 - loss: 0.6798 - acc: 0.5774
2688/4849 [===============>..............] - ETA: 2:24 - loss: 0.6800 - acc: 0.5770
2752/4849 [================>.............] - ETA: 2:20 - loss: 0.6792 - acc: 0.5774
2816/4849 [================>.............] - ETA: 2:16 - loss: 0.6785 - acc: 0.5788
2880/4849 [================>.............] - ETA: 2:12 - loss: 0.6797 - acc: 0.5774
2944/4849 [=================>............] - ETA: 2:08 - loss: 0.6792 - acc: 0.5785
3008/4849 [=================>............] - ETA: 2:03 - loss: 0.6794 - acc: 0.5768
3072/4849 [==================>...........] - ETA: 1:59 - loss: 0.6806 - acc: 0.5745
3136/4849 [==================>...........] - ETA: 1:54 - loss: 0.6808 - acc: 0.5743
3200/4849 [==================>...........] - ETA: 1:50 - loss: 0.6811 - acc: 0.5741
3264/4849 [===================>..........] - ETA: 1:46 - loss: 0.6814 - acc: 0.5741
3328/4849 [===================>..........] - ETA: 1:42 - loss: 0.6824 - acc: 0.5724
3392/4849 [===================>..........] - ETA: 1:37 - loss: 0.6826 - acc: 0.5728
3456/4849 [====================>.........] - ETA: 1:33 - loss: 0.6827 - acc: 0.5723
3520/4849 [====================>.........] - ETA: 1:29 - loss: 0.6834 - acc: 0.5710
3584/4849 [=====================>........] - ETA: 1:24 - loss: 0.6832 - acc: 0.5711
3648/4849 [=====================>........] - ETA: 1:20 - loss: 0.6832 - acc: 0.5721
3712/4849 [=====================>........] - ETA: 1:16 - loss: 0.6832 - acc: 0.5722
3776/4849 [======================>.......] - ETA: 1:11 - loss: 0.6829 - acc: 0.5720
3840/4849 [======================>.......] - ETA: 1:07 - loss: 0.6830 - acc: 0.5719
3904/4849 [=======================>......] - ETA: 1:03 - loss: 0.6824 - acc: 0.5733
3968/4849 [=======================>......] - ETA: 59s - loss: 0.6819 - acc: 0.5751 
4032/4849 [=======================>......] - ETA: 54s - loss: 0.6823 - acc: 0.5747
4096/4849 [========================>.....] - ETA: 50s - loss: 0.6826 - acc: 0.5742
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6834 - acc: 0.5728
4224/4849 [=========================>....] - ETA: 41s - loss: 0.6829 - acc: 0.5736
4288/4849 [=========================>....] - ETA: 37s - loss: 0.6827 - acc: 0.5742
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6825 - acc: 0.5742
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6824 - acc: 0.5747
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6830 - acc: 0.5730
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6827 - acc: 0.5733
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6834 - acc: 0.5727
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6830 - acc: 0.5721
4736/4849 [============================>.] - ETA: 7s - loss: 0.6831 - acc: 0.5718 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6831 - acc: 0.5719
4849/4849 [==============================] - 338s 70ms/step - loss: 0.6829 - acc: 0.5725 - val_loss: 0.7012 - val_acc: 0.5325

Epoch 00004: val_acc did not improve from 0.56401
Epoch 5/10

  64/4849 [..............................] - ETA: 5:31 - loss: 0.7351 - acc: 0.3906
 128/4849 [..............................] - ETA: 5:13 - loss: 0.6877 - acc: 0.5312
 192/4849 [>.............................] - ETA: 5:17 - loss: 0.7018 - acc: 0.5156
 256/4849 [>.............................] - ETA: 5:04 - loss: 0.6930 - acc: 0.5312
 320/4849 [>.............................] - ETA: 5:04 - loss: 0.6984 - acc: 0.5156
 384/4849 [=>............................] - ETA: 4:59 - loss: 0.6971 - acc: 0.5182
 448/4849 [=>............................] - ETA: 4:53 - loss: 0.6944 - acc: 0.5246
 512/4849 [==>...........................] - ETA: 4:49 - loss: 0.6921 - acc: 0.5273
 576/4849 [==>...........................] - ETA: 4:45 - loss: 0.6907 - acc: 0.5347
 640/4849 [==>...........................] - ETA: 4:41 - loss: 0.6888 - acc: 0.5344
 704/4849 [===>..........................] - ETA: 4:37 - loss: 0.6903 - acc: 0.5284
 768/4849 [===>..........................] - ETA: 4:33 - loss: 0.6907 - acc: 0.5286
 832/4849 [====>.........................] - ETA: 4:28 - loss: 0.6914 - acc: 0.5312
 896/4849 [====>.........................] - ETA: 4:25 - loss: 0.6920 - acc: 0.5346
 960/4849 [====>.........................] - ETA: 4:20 - loss: 0.6900 - acc: 0.5427
1024/4849 [=====>........................] - ETA: 4:16 - loss: 0.6894 - acc: 0.5439
1088/4849 [=====>........................] - ETA: 4:13 - loss: 0.6885 - acc: 0.5460
1152/4849 [======>.......................] - ETA: 4:08 - loss: 0.6899 - acc: 0.5417
1216/4849 [======>.......................] - ETA: 4:04 - loss: 0.6887 - acc: 0.5428
1280/4849 [======>.......................] - ETA: 3:58 - loss: 0.6885 - acc: 0.5430
1344/4849 [=======>......................] - ETA: 3:56 - loss: 0.6883 - acc: 0.5469
1408/4849 [=======>......................] - ETA: 3:51 - loss: 0.6868 - acc: 0.5504
1472/4849 [========>.....................] - ETA: 3:47 - loss: 0.6855 - acc: 0.5510
1536/4849 [========>.....................] - ETA: 3:43 - loss: 0.6847 - acc: 0.5521
1600/4849 [========>.....................] - ETA: 3:38 - loss: 0.6834 - acc: 0.5537
1664/4849 [=========>....................] - ETA: 3:34 - loss: 0.6835 - acc: 0.5547
1728/4849 [=========>....................] - ETA: 3:29 - loss: 0.6843 - acc: 0.5544
1792/4849 [==========>...................] - ETA: 3:25 - loss: 0.6841 - acc: 0.5536
1856/4849 [==========>...................] - ETA: 3:21 - loss: 0.6845 - acc: 0.5523
1920/4849 [==========>...................] - ETA: 3:16 - loss: 0.6831 - acc: 0.5563
1984/4849 [===========>..................] - ETA: 3:12 - loss: 0.6828 - acc: 0.5575
2048/4849 [===========>..................] - ETA: 3:07 - loss: 0.6812 - acc: 0.5610
2112/4849 [============>.................] - ETA: 3:03 - loss: 0.6809 - acc: 0.5611
2176/4849 [============>.................] - ETA: 2:58 - loss: 0.6811 - acc: 0.5611
2240/4849 [============>.................] - ETA: 2:55 - loss: 0.6808 - acc: 0.5625
2304/4849 [=============>................] - ETA: 2:50 - loss: 0.6795 - acc: 0.5642
2368/4849 [=============>................] - ETA: 2:46 - loss: 0.6797 - acc: 0.5633
2432/4849 [==============>...............] - ETA: 2:41 - loss: 0.6787 - acc: 0.5654
2496/4849 [==============>...............] - ETA: 2:37 - loss: 0.6796 - acc: 0.5649
2560/4849 [==============>...............] - ETA: 2:33 - loss: 0.6782 - acc: 0.5684
2624/4849 [===============>..............] - ETA: 2:29 - loss: 0.6779 - acc: 0.5713
2688/4849 [===============>..............] - ETA: 2:25 - loss: 0.6775 - acc: 0.5718
2752/4849 [================>.............] - ETA: 2:20 - loss: 0.6763 - acc: 0.5752
2816/4849 [================>.............] - ETA: 2:16 - loss: 0.6751 - acc: 0.5774
2880/4849 [================>.............] - ETA: 2:12 - loss: 0.6746 - acc: 0.5795
2944/4849 [=================>............] - ETA: 2:07 - loss: 0.6745 - acc: 0.5791
3008/4849 [=================>............] - ETA: 2:03 - loss: 0.6748 - acc: 0.5765
3072/4849 [==================>...........] - ETA: 1:58 - loss: 0.6754 - acc: 0.5758
3136/4849 [==================>...........] - ETA: 1:55 - loss: 0.6754 - acc: 0.5765
3200/4849 [==================>...........] - ETA: 1:50 - loss: 0.6761 - acc: 0.5753
3264/4849 [===================>..........] - ETA: 1:46 - loss: 0.6764 - acc: 0.5754
3328/4849 [===================>..........] - ETA: 1:42 - loss: 0.6760 - acc: 0.5769
3392/4849 [===================>..........] - ETA: 1:37 - loss: 0.6761 - acc: 0.5764
3456/4849 [====================>.........] - ETA: 1:33 - loss: 0.6757 - acc: 0.5773
3520/4849 [====================>.........] - ETA: 1:29 - loss: 0.6761 - acc: 0.5773
3584/4849 [=====================>........] - ETA: 1:24 - loss: 0.6752 - acc: 0.5787
3648/4849 [=====================>........] - ETA: 1:20 - loss: 0.6745 - acc: 0.5800
3712/4849 [=====================>........] - ETA: 1:16 - loss: 0.6754 - acc: 0.5797
3776/4849 [======================>.......] - ETA: 1:12 - loss: 0.6751 - acc: 0.5800
3840/4849 [======================>.......] - ETA: 1:07 - loss: 0.6756 - acc: 0.5784
3904/4849 [=======================>......] - ETA: 1:03 - loss: 0.6750 - acc: 0.5786
3968/4849 [=======================>......] - ETA: 59s - loss: 0.6745 - acc: 0.5786 
4032/4849 [=======================>......] - ETA: 54s - loss: 0.6743 - acc: 0.5786
4096/4849 [========================>.....] - ETA: 50s - loss: 0.6738 - acc: 0.5798
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6744 - acc: 0.5796
4224/4849 [=========================>....] - ETA: 41s - loss: 0.6745 - acc: 0.5803
4288/4849 [=========================>....] - ETA: 37s - loss: 0.6753 - acc: 0.5784
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6749 - acc: 0.5793
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6746 - acc: 0.5806
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6743 - acc: 0.5810
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6744 - acc: 0.5808
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6735 - acc: 0.5822
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6738 - acc: 0.5813
4736/4849 [============================>.] - ETA: 7s - loss: 0.6737 - acc: 0.5811 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6736 - acc: 0.5815
4849/4849 [==============================] - 340s 70ms/step - loss: 0.6737 - acc: 0.5805 - val_loss: 0.6992 - val_acc: 0.5343

Epoch 00005: val_acc did not improve from 0.56401
Epoch 6/10

  64/4849 [..............................] - ETA: 5:27 - loss: 0.6837 - acc: 0.5469
 128/4849 [..............................] - ETA: 5:30 - loss: 0.6948 - acc: 0.5156
 192/4849 [>.............................] - ETA: 5:28 - loss: 0.6860 - acc: 0.5417
 256/4849 [>.............................] - ETA: 5:16 - loss: 0.6927 - acc: 0.5156
 320/4849 [>.............................] - ETA: 5:11 - loss: 0.6928 - acc: 0.5219
 384/4849 [=>............................] - ETA: 5:09 - loss: 0.6921 - acc: 0.5260
 448/4849 [=>............................] - ETA: 5:01 - loss: 0.6923 - acc: 0.5402
 512/4849 [==>...........................] - ETA: 4:56 - loss: 0.6928 - acc: 0.5508
 576/4849 [==>...........................] - ETA: 4:50 - loss: 0.6909 - acc: 0.5538
 640/4849 [==>...........................] - ETA: 4:46 - loss: 0.6888 - acc: 0.5609
 704/4849 [===>..........................] - ETA: 4:40 - loss: 0.6881 - acc: 0.5611
 768/4849 [===>..........................] - ETA: 4:36 - loss: 0.6860 - acc: 0.5664
 832/4849 [====>.........................] - ETA: 4:32 - loss: 0.6827 - acc: 0.5757
 896/4849 [====>.........................] - ETA: 4:27 - loss: 0.6806 - acc: 0.5759
 960/4849 [====>.........................] - ETA: 4:23 - loss: 0.6820 - acc: 0.5719
1024/4849 [=====>........................] - ETA: 4:18 - loss: 0.6820 - acc: 0.5693
1088/4849 [=====>........................] - ETA: 4:15 - loss: 0.6826 - acc: 0.5680
1152/4849 [======>.......................] - ETA: 4:10 - loss: 0.6819 - acc: 0.5720
1216/4849 [======>.......................] - ETA: 4:05 - loss: 0.6805 - acc: 0.5740
1280/4849 [======>.......................] - ETA: 4:00 - loss: 0.6815 - acc: 0.5758
1344/4849 [=======>......................] - ETA: 3:57 - loss: 0.6821 - acc: 0.5744
1408/4849 [=======>......................] - ETA: 3:53 - loss: 0.6811 - acc: 0.5774
1472/4849 [========>.....................] - ETA: 3:48 - loss: 0.6831 - acc: 0.5720
1536/4849 [========>.....................] - ETA: 3:44 - loss: 0.6829 - acc: 0.5723
1600/4849 [========>.....................] - ETA: 3:40 - loss: 0.6828 - acc: 0.5725
1664/4849 [=========>....................] - ETA: 3:35 - loss: 0.6829 - acc: 0.5715
1728/4849 [=========>....................] - ETA: 3:31 - loss: 0.6814 - acc: 0.5735
1792/4849 [==========>...................] - ETA: 3:26 - loss: 0.6791 - acc: 0.5770
1856/4849 [==========>...................] - ETA: 3:22 - loss: 0.6797 - acc: 0.5760
1920/4849 [==========>...................] - ETA: 3:18 - loss: 0.6798 - acc: 0.5750
1984/4849 [===========>..................] - ETA: 3:14 - loss: 0.6781 - acc: 0.5791
2048/4849 [===========>..................] - ETA: 3:09 - loss: 0.6781 - acc: 0.5781
2112/4849 [============>.................] - ETA: 3:05 - loss: 0.6794 - acc: 0.5781
2176/4849 [============>.................] - ETA: 3:00 - loss: 0.6785 - acc: 0.5800
2240/4849 [============>.................] - ETA: 2:56 - loss: 0.6780 - acc: 0.5777
2304/4849 [=============>................] - ETA: 2:52 - loss: 0.6759 - acc: 0.5825
2368/4849 [=============>................] - ETA: 2:48 - loss: 0.6744 - acc: 0.5849
2432/4849 [==============>...............] - ETA: 2:43 - loss: 0.6736 - acc: 0.5880
2496/4849 [==============>...............] - ETA: 2:39 - loss: 0.6724 - acc: 0.5909
2560/4849 [==============>...............] - ETA: 2:34 - loss: 0.6743 - acc: 0.5871
2624/4849 [===============>..............] - ETA: 2:30 - loss: 0.6745 - acc: 0.5865
2688/4849 [===============>..............] - ETA: 2:26 - loss: 0.6758 - acc: 0.5844
2752/4849 [================>.............] - ETA: 2:21 - loss: 0.6762 - acc: 0.5847
2816/4849 [================>.............] - ETA: 2:17 - loss: 0.6763 - acc: 0.5859
2880/4849 [================>.............] - ETA: 2:13 - loss: 0.6749 - acc: 0.5878
2944/4849 [=================>............] - ETA: 2:09 - loss: 0.6747 - acc: 0.5883
3008/4849 [=================>............] - ETA: 2:04 - loss: 0.6745 - acc: 0.5891
3072/4849 [==================>...........] - ETA: 2:00 - loss: 0.6756 - acc: 0.5866
3136/4849 [==================>...........] - ETA: 1:56 - loss: 0.6759 - acc: 0.5880
3200/4849 [==================>...........] - ETA: 1:51 - loss: 0.6777 - acc: 0.5866
3264/4849 [===================>..........] - ETA: 1:47 - loss: 0.6770 - acc: 0.5885
3328/4849 [===================>..........] - ETA: 1:42 - loss: 0.6772 - acc: 0.5862
3392/4849 [===================>..........] - ETA: 1:38 - loss: 0.6762 - acc: 0.5873
3456/4849 [====================>.........] - ETA: 1:34 - loss: 0.6757 - acc: 0.5891
3520/4849 [====================>.........] - ETA: 1:29 - loss: 0.6749 - acc: 0.5906
3584/4849 [=====================>........] - ETA: 1:25 - loss: 0.6748 - acc: 0.5901
3648/4849 [=====================>........] - ETA: 1:21 - loss: 0.6747 - acc: 0.5899
3712/4849 [=====================>........] - ETA: 1:16 - loss: 0.6741 - acc: 0.5911
3776/4849 [======================>.......] - ETA: 1:12 - loss: 0.6749 - acc: 0.5895
3840/4849 [======================>.......] - ETA: 1:08 - loss: 0.6745 - acc: 0.5906
3904/4849 [=======================>......] - ETA: 1:03 - loss: 0.6746 - acc: 0.5899
3968/4849 [=======================>......] - ETA: 59s - loss: 0.6741 - acc: 0.5912 
4032/4849 [=======================>......] - ETA: 55s - loss: 0.6743 - acc: 0.5905
4096/4849 [========================>.....] - ETA: 50s - loss: 0.6746 - acc: 0.5901
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6754 - acc: 0.5885
4224/4849 [=========================>....] - ETA: 41s - loss: 0.6747 - acc: 0.5890
4288/4849 [=========================>....] - ETA: 37s - loss: 0.6742 - acc: 0.5891
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6747 - acc: 0.5882
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6742 - acc: 0.5892
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6744 - acc: 0.5886
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6747 - acc: 0.5878
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6750 - acc: 0.5866
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6752 - acc: 0.5867
4736/4849 [============================>.] - ETA: 7s - loss: 0.6756 - acc: 0.5870 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6753 - acc: 0.5879
4849/4849 [==============================] - 340s 70ms/step - loss: 0.6749 - acc: 0.5894 - val_loss: 0.6954 - val_acc: 0.5622

Epoch 00006: val_acc did not improve from 0.56401
Epoch 7/10

  64/4849 [..............................] - ETA: 5:23 - loss: 0.6824 - acc: 0.5469
 128/4849 [..............................] - ETA: 5:16 - loss: 0.6852 - acc: 0.5312
 192/4849 [>.............................] - ETA: 5:03 - loss: 0.6927 - acc: 0.5417
 256/4849 [>.............................] - ETA: 5:01 - loss: 0.6848 - acc: 0.5547
 320/4849 [>.............................] - ETA: 5:00 - loss: 0.6766 - acc: 0.5625
 384/4849 [=>............................] - ETA: 5:01 - loss: 0.6795 - acc: 0.5547
 448/4849 [=>............................] - ETA: 4:53 - loss: 0.6754 - acc: 0.5625
 512/4849 [==>...........................] - ETA: 4:55 - loss: 0.6775 - acc: 0.5645
 576/4849 [==>...........................] - ETA: 4:51 - loss: 0.6778 - acc: 0.5660
 640/4849 [==>...........................] - ETA: 4:46 - loss: 0.6738 - acc: 0.5703
 704/4849 [===>..........................] - ETA: 4:43 - loss: 0.6709 - acc: 0.5795
 768/4849 [===>..........................] - ETA: 4:41 - loss: 0.6707 - acc: 0.5820
 832/4849 [====>.........................] - ETA: 4:37 - loss: 0.6702 - acc: 0.5817
 896/4849 [====>.........................] - ETA: 4:34 - loss: 0.6674 - acc: 0.5871
 960/4849 [====>.........................] - ETA: 4:30 - loss: 0.6677 - acc: 0.5865
1024/4849 [=====>........................] - ETA: 4:23 - loss: 0.6695 - acc: 0.5830
1088/4849 [=====>........................] - ETA: 4:19 - loss: 0.6710 - acc: 0.5790
1152/4849 [======>.......................] - ETA: 4:13 - loss: 0.6709 - acc: 0.5807
1216/4849 [======>.......................] - ETA: 4:09 - loss: 0.6711 - acc: 0.5789
1280/4849 [======>.......................] - ETA: 4:04 - loss: 0.6713 - acc: 0.5742
1344/4849 [=======>......................] - ETA: 3:59 - loss: 0.6730 - acc: 0.5699
1408/4849 [=======>......................] - ETA: 3:54 - loss: 0.6727 - acc: 0.5724
1472/4849 [========>.....................] - ETA: 3:50 - loss: 0.6720 - acc: 0.5754
1536/4849 [========>.....................] - ETA: 3:45 - loss: 0.6704 - acc: 0.5768
1600/4849 [========>.....................] - ETA: 3:41 - loss: 0.6698 - acc: 0.5763
1664/4849 [=========>....................] - ETA: 3:37 - loss: 0.6691 - acc: 0.5787
1728/4849 [=========>....................] - ETA: 3:32 - loss: 0.6690 - acc: 0.5775
1792/4849 [==========>...................] - ETA: 3:28 - loss: 0.6697 - acc: 0.5742
1856/4849 [==========>...................] - ETA: 3:23 - loss: 0.6700 - acc: 0.5749
1920/4849 [==========>...................] - ETA: 3:20 - loss: 0.6705 - acc: 0.5734
1984/4849 [===========>..................] - ETA: 3:15 - loss: 0.6702 - acc: 0.5741
2048/4849 [===========>..................] - ETA: 3:11 - loss: 0.6681 - acc: 0.5776
2112/4849 [============>.................] - ETA: 3:07 - loss: 0.6668 - acc: 0.5810
2176/4849 [============>.................] - ETA: 3:03 - loss: 0.6672 - acc: 0.5827
2240/4849 [============>.................] - ETA: 2:58 - loss: 0.6675 - acc: 0.5808
2304/4849 [=============>................] - ETA: 2:54 - loss: 0.6692 - acc: 0.5773
2368/4849 [=============>................] - ETA: 2:50 - loss: 0.6699 - acc: 0.5769
2432/4849 [==============>...............] - ETA: 2:45 - loss: 0.6691 - acc: 0.5773
2496/4849 [==============>...............] - ETA: 2:41 - loss: 0.6699 - acc: 0.5773
2560/4849 [==============>...............] - ETA: 2:36 - loss: 0.6704 - acc: 0.5773
2624/4849 [===============>..............] - ETA: 2:32 - loss: 0.6701 - acc: 0.5789
2688/4849 [===============>..............] - ETA: 2:28 - loss: 0.6707 - acc: 0.5785
2752/4849 [================>.............] - ETA: 2:23 - loss: 0.6699 - acc: 0.5799
2816/4849 [================>.............] - ETA: 2:19 - loss: 0.6707 - acc: 0.5788
2880/4849 [================>.............] - ETA: 2:15 - loss: 0.6705 - acc: 0.5795
2944/4849 [=================>............] - ETA: 2:10 - loss: 0.6708 - acc: 0.5788
3008/4849 [=================>............] - ETA: 2:05 - loss: 0.6708 - acc: 0.5798
3072/4849 [==================>...........] - ETA: 2:01 - loss: 0.6707 - acc: 0.5801
3136/4849 [==================>...........] - ETA: 1:57 - loss: 0.6698 - acc: 0.5813
3200/4849 [==================>...........] - ETA: 1:52 - loss: 0.6697 - acc: 0.5816
3264/4849 [===================>..........] - ETA: 1:48 - loss: 0.6705 - acc: 0.5812
3328/4849 [===================>..........] - ETA: 1:44 - loss: 0.6707 - acc: 0.5808
3392/4849 [===================>..........] - ETA: 1:39 - loss: 0.6703 - acc: 0.5808
3456/4849 [====================>.........] - ETA: 1:35 - loss: 0.6698 - acc: 0.5816
3520/4849 [====================>.........] - ETA: 1:30 - loss: 0.6694 - acc: 0.5821
3584/4849 [=====================>........] - ETA: 1:26 - loss: 0.6709 - acc: 0.5806
3648/4849 [=====================>........] - ETA: 1:22 - loss: 0.6713 - acc: 0.5800
3712/4849 [=====================>........] - ETA: 1:17 - loss: 0.6719 - acc: 0.5784
3776/4849 [======================>.......] - ETA: 1:13 - loss: 0.6720 - acc: 0.5784
3840/4849 [======================>.......] - ETA: 1:09 - loss: 0.6727 - acc: 0.5781
3904/4849 [=======================>......] - ETA: 1:04 - loss: 0.6723 - acc: 0.5791
3968/4849 [=======================>......] - ETA: 1:00 - loss: 0.6721 - acc: 0.5789
4032/4849 [=======================>......] - ETA: 55s - loss: 0.6716 - acc: 0.5806 
4096/4849 [========================>.....] - ETA: 51s - loss: 0.6710 - acc: 0.5823
4160/4849 [========================>.....] - ETA: 47s - loss: 0.6711 - acc: 0.5825
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6712 - acc: 0.5824
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6718 - acc: 0.5812
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6721 - acc: 0.5809
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6715 - acc: 0.5824
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6709 - acc: 0.5835
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6712 - acc: 0.5830
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6703 - acc: 0.5844
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6698 - acc: 0.5856
4736/4849 [============================>.] - ETA: 7s - loss: 0.6696 - acc: 0.5864 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6692 - acc: 0.5871
4849/4849 [==============================] - 344s 71ms/step - loss: 0.6697 - acc: 0.5867 - val_loss: 0.6823 - val_acc: 0.5826

Epoch 00007: val_acc improved from 0.56401 to 0.58256, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window14/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 8/10

  64/4849 [..............................] - ETA: 5:07 - loss: 0.6490 - acc: 0.5938
 128/4849 [..............................] - ETA: 5:10 - loss: 0.6372 - acc: 0.6250
 192/4849 [>.............................] - ETA: 5:04 - loss: 0.6346 - acc: 0.6250
 256/4849 [>.............................] - ETA: 4:51 - loss: 0.6474 - acc: 0.6172
 320/4849 [>.............................] - ETA: 4:53 - loss: 0.6680 - acc: 0.5906
 384/4849 [=>............................] - ETA: 4:45 - loss: 0.6743 - acc: 0.5911
 448/4849 [=>............................] - ETA: 4:43 - loss: 0.6623 - acc: 0.6116
 512/4849 [==>...........................] - ETA: 4:37 - loss: 0.6695 - acc: 0.5977
 576/4849 [==>...........................] - ETA: 4:34 - loss: 0.6691 - acc: 0.5972
 640/4849 [==>...........................] - ETA: 4:32 - loss: 0.6675 - acc: 0.6016
 704/4849 [===>..........................] - ETA: 4:26 - loss: 0.6721 - acc: 0.5952
 768/4849 [===>..........................] - ETA: 4:23 - loss: 0.6689 - acc: 0.6003
 832/4849 [====>.........................] - ETA: 4:20 - loss: 0.6707 - acc: 0.5962
 896/4849 [====>.........................] - ETA: 4:17 - loss: 0.6699 - acc: 0.5971
 960/4849 [====>.........................] - ETA: 4:12 - loss: 0.6702 - acc: 0.5927
1024/4849 [=====>........................] - ETA: 4:09 - loss: 0.6709 - acc: 0.5898
1088/4849 [=====>........................] - ETA: 4:04 - loss: 0.6718 - acc: 0.5901
1152/4849 [======>.......................] - ETA: 4:00 - loss: 0.6730 - acc: 0.5868
1216/4849 [======>.......................] - ETA: 3:56 - loss: 0.6733 - acc: 0.5855
1280/4849 [======>.......................] - ETA: 3:52 - loss: 0.6725 - acc: 0.5852
1344/4849 [=======>......................] - ETA: 3:49 - loss: 0.6719 - acc: 0.5826
1408/4849 [=======>......................] - ETA: 3:45 - loss: 0.6723 - acc: 0.5810
1472/4849 [========>.....................] - ETA: 3:42 - loss: 0.6714 - acc: 0.5849
1536/4849 [========>.....................] - ETA: 3:38 - loss: 0.6707 - acc: 0.5833
1600/4849 [========>.....................] - ETA: 3:34 - loss: 0.6727 - acc: 0.5775
1664/4849 [=========>....................] - ETA: 3:31 - loss: 0.6715 - acc: 0.5787
1728/4849 [=========>....................] - ETA: 3:26 - loss: 0.6709 - acc: 0.5793
1792/4849 [==========>...................] - ETA: 3:22 - loss: 0.6704 - acc: 0.5781
1856/4849 [==========>...................] - ETA: 3:18 - loss: 0.6699 - acc: 0.5765
1920/4849 [==========>...................] - ETA: 3:14 - loss: 0.6696 - acc: 0.5781
1984/4849 [===========>..................] - ETA: 3:10 - loss: 0.6705 - acc: 0.5776
2048/4849 [===========>..................] - ETA: 3:06 - loss: 0.6693 - acc: 0.5801
2112/4849 [============>.................] - ETA: 3:01 - loss: 0.6689 - acc: 0.5800
2176/4849 [============>.................] - ETA: 2:57 - loss: 0.6698 - acc: 0.5795
2240/4849 [============>.................] - ETA: 2:53 - loss: 0.6709 - acc: 0.5790
2304/4849 [=============>................] - ETA: 2:49 - loss: 0.6704 - acc: 0.5812
2368/4849 [=============>................] - ETA: 2:45 - loss: 0.6709 - acc: 0.5802
2432/4849 [==============>...............] - ETA: 2:41 - loss: 0.6714 - acc: 0.5818
2496/4849 [==============>...............] - ETA: 2:36 - loss: 0.6716 - acc: 0.5817
2560/4849 [==============>...............] - ETA: 2:32 - loss: 0.6717 - acc: 0.5828
2624/4849 [===============>..............] - ETA: 2:28 - loss: 0.6734 - acc: 0.5816
2688/4849 [===============>..............] - ETA: 2:24 - loss: 0.6747 - acc: 0.5800
2752/4849 [================>.............] - ETA: 2:20 - loss: 0.6754 - acc: 0.5803
2816/4849 [================>.............] - ETA: 2:15 - loss: 0.6757 - acc: 0.5803
2880/4849 [================>.............] - ETA: 2:11 - loss: 0.6753 - acc: 0.5806
2944/4849 [=================>............] - ETA: 2:07 - loss: 0.6761 - acc: 0.5808
3008/4849 [=================>............] - ETA: 2:03 - loss: 0.6759 - acc: 0.5814
3072/4849 [==================>...........] - ETA: 1:59 - loss: 0.6758 - acc: 0.5811
3136/4849 [==================>...........] - ETA: 1:54 - loss: 0.6762 - acc: 0.5794
3200/4849 [==================>...........] - ETA: 1:50 - loss: 0.6759 - acc: 0.5803
3264/4849 [===================>..........] - ETA: 1:46 - loss: 0.6771 - acc: 0.5787
3328/4849 [===================>..........] - ETA: 1:42 - loss: 0.6769 - acc: 0.5781
3392/4849 [===================>..........] - ETA: 1:37 - loss: 0.6771 - acc: 0.5775
3456/4849 [====================>.........] - ETA: 1:33 - loss: 0.6768 - acc: 0.5787
3520/4849 [====================>.........] - ETA: 1:29 - loss: 0.6771 - acc: 0.5778
3584/4849 [=====================>........] - ETA: 1:24 - loss: 0.6777 - acc: 0.5765
3648/4849 [=====================>........] - ETA: 1:20 - loss: 0.6782 - acc: 0.5751
3712/4849 [=====================>........] - ETA: 1:16 - loss: 0.6779 - acc: 0.5765
3776/4849 [======================>.......] - ETA: 1:11 - loss: 0.6783 - acc: 0.5744
3840/4849 [======================>.......] - ETA: 1:07 - loss: 0.6778 - acc: 0.5750
3904/4849 [=======================>......] - ETA: 1:03 - loss: 0.6774 - acc: 0.5763
3968/4849 [=======================>......] - ETA: 59s - loss: 0.6772 - acc: 0.5764 
4032/4849 [=======================>......] - ETA: 54s - loss: 0.6769 - acc: 0.5771
4096/4849 [========================>.....] - ETA: 50s - loss: 0.6766 - acc: 0.5774
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6766 - acc: 0.5776
4224/4849 [=========================>....] - ETA: 41s - loss: 0.6766 - acc: 0.5767
4288/4849 [=========================>....] - ETA: 37s - loss: 0.6768 - acc: 0.5765
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6770 - acc: 0.5758
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6778 - acc: 0.5745
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6776 - acc: 0.5750
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6776 - acc: 0.5744
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6781 - acc: 0.5736
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6780 - acc: 0.5734
4736/4849 [============================>.] - ETA: 7s - loss: 0.6773 - acc: 0.5754 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6774 - acc: 0.5758
4849/4849 [==============================] - 340s 70ms/step - loss: 0.6776 - acc: 0.5752 - val_loss: 0.6897 - val_acc: 0.5640

Epoch 00008: val_acc did not improve from 0.58256
Epoch 9/10

  64/4849 [..............................] - ETA: 5:16 - loss: 0.6730 - acc: 0.6094
 128/4849 [..............................] - ETA: 5:09 - loss: 0.6739 - acc: 0.6016
 192/4849 [>.............................] - ETA: 5:13 - loss: 0.6658 - acc: 0.5990
 256/4849 [>.............................] - ETA: 5:09 - loss: 0.6643 - acc: 0.6016
 320/4849 [>.............................] - ETA: 5:03 - loss: 0.6693 - acc: 0.6062
 384/4849 [=>............................] - ETA: 4:56 - loss: 0.6734 - acc: 0.5938
 448/4849 [=>............................] - ETA: 4:55 - loss: 0.6749 - acc: 0.5759
 512/4849 [==>...........................] - ETA: 4:48 - loss: 0.6702 - acc: 0.5879
 576/4849 [==>...........................] - ETA: 4:45 - loss: 0.6678 - acc: 0.5868
 640/4849 [==>...........................] - ETA: 4:41 - loss: 0.6712 - acc: 0.5750
 704/4849 [===>..........................] - ETA: 4:37 - loss: 0.6704 - acc: 0.5781
 768/4849 [===>..........................] - ETA: 4:33 - loss: 0.6691 - acc: 0.5820
 832/4849 [====>.........................] - ETA: 4:31 - loss: 0.6659 - acc: 0.5913
 896/4849 [====>.........................] - ETA: 4:27 - loss: 0.6689 - acc: 0.5837
 960/4849 [====>.........................] - ETA: 4:22 - loss: 0.6716 - acc: 0.5781
1024/4849 [=====>........................] - ETA: 4:18 - loss: 0.6705 - acc: 0.5781
1088/4849 [=====>........................] - ETA: 4:13 - loss: 0.6689 - acc: 0.5827
1152/4849 [======>.......................] - ETA: 4:09 - loss: 0.6693 - acc: 0.5842
1216/4849 [======>.......................] - ETA: 4:04 - loss: 0.6708 - acc: 0.5806
1280/4849 [======>.......................] - ETA: 3:59 - loss: 0.6702 - acc: 0.5820
1344/4849 [=======>......................] - ETA: 3:56 - loss: 0.6732 - acc: 0.5774
1408/4849 [=======>......................] - ETA: 3:51 - loss: 0.6726 - acc: 0.5788
1472/4849 [========>.....................] - ETA: 3:48 - loss: 0.6724 - acc: 0.5815
1536/4849 [========>.....................] - ETA: 3:42 - loss: 0.6722 - acc: 0.5814
1600/4849 [========>.....................] - ETA: 3:38 - loss: 0.6709 - acc: 0.5856
1664/4849 [=========>....................] - ETA: 3:34 - loss: 0.6695 - acc: 0.5901
1728/4849 [=========>....................] - ETA: 3:30 - loss: 0.6689 - acc: 0.5920
1792/4849 [==========>...................] - ETA: 3:26 - loss: 0.6679 - acc: 0.5926
1856/4849 [==========>...................] - ETA: 3:23 - loss: 0.6678 - acc: 0.5948
1920/4849 [==========>...................] - ETA: 3:18 - loss: 0.6659 - acc: 0.5990
1984/4849 [===========>..................] - ETA: 3:14 - loss: 0.6653 - acc: 0.6003
2048/4849 [===========>..................] - ETA: 3:10 - loss: 0.6661 - acc: 0.5986
2112/4849 [============>.................] - ETA: 3:06 - loss: 0.6648 - acc: 0.6023
2176/4849 [============>.................] - ETA: 3:02 - loss: 0.6636 - acc: 0.6034
2240/4849 [============>.................] - ETA: 2:57 - loss: 0.6637 - acc: 0.6013
2304/4849 [=============>................] - ETA: 2:52 - loss: 0.6652 - acc: 0.5985
2368/4849 [=============>................] - ETA: 2:48 - loss: 0.6658 - acc: 0.5976
2432/4849 [==============>...............] - ETA: 2:43 - loss: 0.6652 - acc: 0.5983
2496/4849 [==============>...............] - ETA: 2:39 - loss: 0.6649 - acc: 0.5986
2560/4849 [==============>...............] - ETA: 2:35 - loss: 0.6632 - acc: 0.6012
2624/4849 [===============>..............] - ETA: 2:31 - loss: 0.6650 - acc: 0.5972
2688/4849 [===============>..............] - ETA: 2:26 - loss: 0.6659 - acc: 0.5956
2752/4849 [================>.............] - ETA: 2:22 - loss: 0.6663 - acc: 0.5967
2816/4849 [================>.............] - ETA: 2:17 - loss: 0.6649 - acc: 0.6001
2880/4849 [================>.............] - ETA: 2:12 - loss: 0.6647 - acc: 0.6010
2944/4849 [=================>............] - ETA: 2:07 - loss: 0.6665 - acc: 0.5985
3008/4849 [=================>............] - ETA: 2:02 - loss: 0.6666 - acc: 0.5984
3072/4849 [==================>...........] - ETA: 1:58 - loss: 0.6663 - acc: 0.5980
3136/4849 [==================>...........] - ETA: 1:53 - loss: 0.6667 - acc: 0.5979
3200/4849 [==================>...........] - ETA: 1:48 - loss: 0.6676 - acc: 0.5972
3264/4849 [===================>..........] - ETA: 1:44 - loss: 0.6682 - acc: 0.5962
3328/4849 [===================>..........] - ETA: 1:39 - loss: 0.6679 - acc: 0.5968
3392/4849 [===================>..........] - ETA: 1:34 - loss: 0.6681 - acc: 0.5970
3456/4849 [====================>.........] - ETA: 1:30 - loss: 0.6682 - acc: 0.5969
3520/4849 [====================>.........] - ETA: 1:25 - loss: 0.6686 - acc: 0.5966
3584/4849 [=====================>........] - ETA: 1:21 - loss: 0.6690 - acc: 0.5963
3648/4849 [=====================>........] - ETA: 1:16 - loss: 0.6696 - acc: 0.5965
3712/4849 [=====================>........] - ETA: 1:12 - loss: 0.6701 - acc: 0.5956
3776/4849 [======================>.......] - ETA: 1:08 - loss: 0.6698 - acc: 0.5967
3840/4849 [======================>.......] - ETA: 1:03 - loss: 0.6698 - acc: 0.5971
3904/4849 [=======================>......] - ETA: 59s - loss: 0.6702 - acc: 0.5971 
3968/4849 [=======================>......] - ETA: 55s - loss: 0.6702 - acc: 0.5970
4032/4849 [=======================>......] - ETA: 51s - loss: 0.6701 - acc: 0.5970
4096/4849 [========================>.....] - ETA: 47s - loss: 0.6703 - acc: 0.5972
4160/4849 [========================>.....] - ETA: 43s - loss: 0.6702 - acc: 0.5978
4224/4849 [=========================>....] - ETA: 39s - loss: 0.6694 - acc: 0.5987
4288/4849 [=========================>....] - ETA: 35s - loss: 0.6692 - acc: 0.5991
4352/4849 [=========================>....] - ETA: 30s - loss: 0.6689 - acc: 0.5997
4416/4849 [==========================>...] - ETA: 26s - loss: 0.6688 - acc: 0.6010
4480/4849 [==========================>...] - ETA: 22s - loss: 0.6683 - acc: 0.6013
4544/4849 [===========================>..] - ETA: 18s - loss: 0.6687 - acc: 0.6012
4608/4849 [===========================>..] - ETA: 14s - loss: 0.6687 - acc: 0.6011
4672/4849 [===========================>..] - ETA: 10s - loss: 0.6688 - acc: 0.6019
4736/4849 [============================>.] - ETA: 6s - loss: 0.6680 - acc: 0.6028 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6677 - acc: 0.6035
4849/4849 [==============================] - 309s 64ms/step - loss: 0.6675 - acc: 0.6036 - val_loss: 0.7137 - val_acc: 0.5584

Epoch 00009: val_acc did not improve from 0.58256
Epoch 10/10

  64/4849 [..............................] - ETA: 4:00 - loss: 0.6787 - acc: 0.5938
 128/4849 [..............................] - ETA: 3:57 - loss: 0.6986 - acc: 0.5312
 192/4849 [>.............................] - ETA: 3:55 - loss: 0.6986 - acc: 0.5312
 256/4849 [>.............................] - ETA: 4:03 - loss: 0.6749 - acc: 0.5664
 320/4849 [>.............................] - ETA: 4:02 - loss: 0.6647 - acc: 0.5875
 384/4849 [=>............................] - ETA: 3:57 - loss: 0.6657 - acc: 0.5990
 448/4849 [=>............................] - ETA: 3:53 - loss: 0.6757 - acc: 0.5781
 512/4849 [==>...........................] - ETA: 3:54 - loss: 0.6705 - acc: 0.5938
 576/4849 [==>...........................] - ETA: 3:48 - loss: 0.6720 - acc: 0.5903
 640/4849 [==>...........................] - ETA: 3:45 - loss: 0.6722 - acc: 0.5844
 704/4849 [===>..........................] - ETA: 3:40 - loss: 0.6691 - acc: 0.5923
 768/4849 [===>..........................] - ETA: 3:36 - loss: 0.6690 - acc: 0.5951
 832/4849 [====>.........................] - ETA: 3:32 - loss: 0.6662 - acc: 0.6034
 896/4849 [====>.........................] - ETA: 3:28 - loss: 0.6661 - acc: 0.6049
 960/4849 [====>.........................] - ETA: 3:24 - loss: 0.6647 - acc: 0.6031
1024/4849 [=====>........................] - ETA: 3:20 - loss: 0.6641 - acc: 0.6064
1088/4849 [=====>........................] - ETA: 3:18 - loss: 0.6675 - acc: 0.6029
1152/4849 [======>.......................] - ETA: 3:15 - loss: 0.6673 - acc: 0.6050
1216/4849 [======>.......................] - ETA: 3:11 - loss: 0.6648 - acc: 0.6110
1280/4849 [======>.......................] - ETA: 3:07 - loss: 0.6664 - acc: 0.6094
1344/4849 [=======>......................] - ETA: 3:03 - loss: 0.6660 - acc: 0.6109
1408/4849 [=======>......................] - ETA: 2:59 - loss: 0.6661 - acc: 0.6101
1472/4849 [========>.....................] - ETA: 2:56 - loss: 0.6641 - acc: 0.6141
1536/4849 [========>.....................] - ETA: 2:53 - loss: 0.6626 - acc: 0.6152
1600/4849 [========>.....................] - ETA: 2:49 - loss: 0.6626 - acc: 0.6144
1664/4849 [=========>....................] - ETA: 2:47 - loss: 0.6629 - acc: 0.6148
1728/4849 [=========>....................] - ETA: 2:44 - loss: 0.6639 - acc: 0.6128
1792/4849 [==========>...................] - ETA: 2:41 - loss: 0.6654 - acc: 0.6105
1856/4849 [==========>...................] - ETA: 2:37 - loss: 0.6668 - acc: 0.6110
1920/4849 [==========>...................] - ETA: 2:34 - loss: 0.6667 - acc: 0.6125
1984/4849 [===========>..................] - ETA: 2:31 - loss: 0.6653 - acc: 0.6134
2048/4849 [===========>..................] - ETA: 2:27 - loss: 0.6645 - acc: 0.6138
2112/4849 [============>.................] - ETA: 2:24 - loss: 0.6641 - acc: 0.6136
2176/4849 [============>.................] - ETA: 2:20 - loss: 0.6638 - acc: 0.6149
2240/4849 [============>.................] - ETA: 2:17 - loss: 0.6647 - acc: 0.6138
2304/4849 [=============>................] - ETA: 2:13 - loss: 0.6651 - acc: 0.6133
2368/4849 [=============>................] - ETA: 2:11 - loss: 0.6649 - acc: 0.6136
2432/4849 [==============>...............] - ETA: 2:07 - loss: 0.6654 - acc: 0.6123
2496/4849 [==============>...............] - ETA: 2:04 - loss: 0.6642 - acc: 0.6138
2560/4849 [==============>...............] - ETA: 2:01 - loss: 0.6632 - acc: 0.6152
2624/4849 [===============>..............] - ETA: 1:57 - loss: 0.6633 - acc: 0.6151
2688/4849 [===============>..............] - ETA: 1:54 - loss: 0.6633 - acc: 0.6153
2752/4849 [================>.............] - ETA: 1:50 - loss: 0.6636 - acc: 0.6145
2816/4849 [================>.............] - ETA: 1:47 - loss: 0.6649 - acc: 0.6129
2880/4849 [================>.............] - ETA: 1:44 - loss: 0.6650 - acc: 0.6146
2944/4849 [=================>............] - ETA: 1:40 - loss: 0.6647 - acc: 0.6148
3008/4849 [=================>............] - ETA: 1:37 - loss: 0.6641 - acc: 0.6154
3072/4849 [==================>...........] - ETA: 1:33 - loss: 0.6638 - acc: 0.6149
3136/4849 [==================>...........] - ETA: 1:30 - loss: 0.6638 - acc: 0.6151
3200/4849 [==================>...........] - ETA: 1:26 - loss: 0.6630 - acc: 0.6166
3264/4849 [===================>..........] - ETA: 1:23 - loss: 0.6638 - acc: 0.6152
3328/4849 [===================>..........] - ETA: 1:20 - loss: 0.6641 - acc: 0.6148
3392/4849 [===================>..........] - ETA: 1:16 - loss: 0.6637 - acc: 0.6153
3456/4849 [====================>.........] - ETA: 1:13 - loss: 0.6633 - acc: 0.6155
3520/4849 [====================>.........] - ETA: 1:10 - loss: 0.6634 - acc: 0.6151
3584/4849 [=====================>........] - ETA: 1:06 - loss: 0.6643 - acc: 0.6136
3648/4849 [=====================>........] - ETA: 1:03 - loss: 0.6643 - acc: 0.6138
3712/4849 [=====================>........] - ETA: 1:00 - loss: 0.6647 - acc: 0.6126
3776/4849 [======================>.......] - ETA: 56s - loss: 0.6653 - acc: 0.6120 
3840/4849 [======================>.......] - ETA: 53s - loss: 0.6654 - acc: 0.6115
3904/4849 [=======================>......] - ETA: 49s - loss: 0.6656 - acc: 0.6109
3968/4849 [=======================>......] - ETA: 46s - loss: 0.6659 - acc: 0.6109
4032/4849 [=======================>......] - ETA: 43s - loss: 0.6659 - acc: 0.6101
4096/4849 [========================>.....] - ETA: 39s - loss: 0.6650 - acc: 0.6113
4160/4849 [========================>.....] - ETA: 36s - loss: 0.6650 - acc: 0.6115
4224/4849 [=========================>....] - ETA: 33s - loss: 0.6652 - acc: 0.6110
4288/4849 [=========================>....] - ETA: 29s - loss: 0.6663 - acc: 0.6091
4352/4849 [=========================>....] - ETA: 26s - loss: 0.6666 - acc: 0.6082
4416/4849 [==========================>...] - ETA: 22s - loss: 0.6670 - acc: 0.6073
4480/4849 [==========================>...] - ETA: 19s - loss: 0.6665 - acc: 0.6085
4544/4849 [===========================>..] - ETA: 16s - loss: 0.6664 - acc: 0.6087
4608/4849 [===========================>..] - ETA: 12s - loss: 0.6677 - acc: 0.6053
4672/4849 [===========================>..] - ETA: 9s - loss: 0.6675 - acc: 0.6060 
4736/4849 [============================>.] - ETA: 5s - loss: 0.6677 - acc: 0.6058
4800/4849 [============================>.] - ETA: 2s - loss: 0.6688 - acc: 0.6033
4849/4849 [==============================] - 267s 55ms/step - loss: 0.6689 - acc: 0.6028 - val_loss: 0.7011 - val_acc: 0.5343

Epoch 00010: val_acc did not improve from 0.58256
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f747833d9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f747833d9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7478232b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7478232b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc21b8f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bc21b8f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7458733750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7458733750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74584ee950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74584ee950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7458579410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7458579410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7458733490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7458733490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74585e4550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74585e4550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f745861f090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f745861f090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7458409510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7458409510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74585c36d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74585c36d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74584550d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74584550d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74582c3a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74582c3a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7458260ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7458260ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74580ea0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74580ea0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74582cb050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74582cb050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7458409810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7458409810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74580e8d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74580e8d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f745067e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f745067e8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f745054c850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f745054c850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d40baa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72d40baa10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7450598050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7450598050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74580bc190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74580bc190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f745035f6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f745035f6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74502455d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74502455d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f745060e510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f745060e510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f745035fc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f745035fc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74502aadd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74502aadd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f745035a050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f745035a050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74107accd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74107accd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f745008de50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f745008de50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7450077e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7450077e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74106cf2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74106cf2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7410719d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7410719d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f741044f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f741044f590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74103c4d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74103c4d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74107192d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74107192d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74103f3410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74103f3410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74101ede10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74101ede10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74100e2dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f74100e2dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74105371d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74105371d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74101ed3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74101ed3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7410167190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7410167190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73f07ab050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73f07ab050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73f04fd0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73f04fd0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73f073f910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73f073f910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74105d6390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f74105d6390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73f03ff550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73f03ff550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73f03a6090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73f03a6090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73f0272e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73f0272e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73f05b2950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73f05b2950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f73f054f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f73f054f5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73f06dd610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73f06dd610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73f008ea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73f008ea10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73e86d0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73e86d0e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74584419d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74584419d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f73f0178750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f73f0178750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73e854e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73e854e8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73e851ddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f73e851ddd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73e83a2cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f73e83a2cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73e8481750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73e8481750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f73e86c6050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f73e86c6050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73e83e4590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f73e83e4590>>: AttributeError: module 'gast' has no attribute 'Str'
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 2:49
 128/1348 [=>............................] - ETA: 1:34
 192/1348 [===>..........................] - ETA: 1:08
 256/1348 [====>.........................] - ETA: 55s 
 320/1348 [======>.......................] - ETA: 47s
 384/1348 [=======>......................] - ETA: 40s
 448/1348 [========>.....................] - ETA: 35s
 512/1348 [==========>...................] - ETA: 31s
 576/1348 [===========>..................] - ETA: 28s
 640/1348 [=============>................] - ETA: 24s
 704/1348 [==============>...............] - ETA: 21s
 768/1348 [================>.............] - ETA: 19s
 832/1348 [=================>............] - ETA: 16s
 896/1348 [==================>...........] - ETA: 14s
 960/1348 [====================>.........] - ETA: 12s
1024/1348 [=====================>........] - ETA: 9s 
1088/1348 [=======================>......] - ETA: 7s
1152/1348 [========================>.....] - ETA: 5s
1216/1348 [==========================>...] - ETA: 3s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 40s 30ms/step
loss: 0.6692432013039066
acc: 0.5890207715133531
样本个数 2694
样本个数 5388
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f71700f14d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f71700f14d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f70c8050910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f70c8050910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72a817f950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72a817f950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72a817fd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72a817fd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f727872f6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f727872f6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74780e6410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74780e6410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72a817f990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72a817f990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74587e3a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f74587e3a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74782e9b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f74782e9b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7478144b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7478144b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fd419bf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fd419bf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7478360c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7478360c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7478140bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7478140bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70f44d2f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70f44d2f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70f451e890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70f451e890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7478214f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7478214f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70f44d2190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70f44d2190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70f444df10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70f444df10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70f442e510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70f442e510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70f40e6110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70f40e6110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71140406d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f71140406d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70f43c6050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70f43c6050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d470ed90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d470ed90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70d46c9490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70d46c9490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70d45421d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70d45421d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d4540190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d4540190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70f4141350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70f4141350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d4590690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d4590690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70d4351ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70d4351ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70d425ec10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70d425ec10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d430d250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d430d250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70d43e51d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70d43e51d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d42b1fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d42b1fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70d415aa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70d415aa50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70d407c2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70d407c2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d4340b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d4340b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70d43b8590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70d43b8590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70c861aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70c861aa90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70c862ed90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70c862ed90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70c83dbf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70c83dbf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d40b1a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70d40b1a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70d43e3390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70d43e3390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70c83dd450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70c83dd450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70c8543710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70c8543710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70c807f410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f70c807f410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70c822a490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f70c822a490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70c8429dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70c8429dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fd407fb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fd407fb10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70c807be50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70c807be50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6fb065bdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6fb065bdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb0736310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb0736310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6fb07addd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6fb07addd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb05a9850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb05a9850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70c8413f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70c8413f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6fb034b0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6fb034b0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb0650110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb0650110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6fb071f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6fb071f5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb01ddc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb01ddc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6fb06501d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6fb06501d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6f907d7d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6f907d7d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb005d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb005d710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6fb033c490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6fb033c490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6f906cda10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6f906cda10>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 4849 samples, validate on 539 samples
Epoch 1/10

  64/4849 [..............................] - ETA: 47:12 - loss: 0.7256 - acc: 0.4688
 128/4849 [..............................] - ETA: 25:58 - loss: 0.8117 - acc: 0.4766
 192/4849 [>.............................] - ETA: 18:49 - loss: 0.7932 - acc: 0.4479
 256/4849 [>.............................] - ETA: 15:16 - loss: 0.7831 - acc: 0.4453
 320/4849 [>.............................] - ETA: 13:29 - loss: 0.7925 - acc: 0.4437
 384/4849 [=>............................] - ETA: 12:15 - loss: 0.7959 - acc: 0.4323
 448/4849 [=>............................] - ETA: 11:18 - loss: 0.7875 - acc: 0.4397
 512/4849 [==>...........................] - ETA: 10:29 - loss: 0.7784 - acc: 0.4531
 576/4849 [==>...........................] - ETA: 9:53 - loss: 0.7745 - acc: 0.4601 
 640/4849 [==>...........................] - ETA: 9:21 - loss: 0.7664 - acc: 0.4719
 704/4849 [===>..........................] - ETA: 8:54 - loss: 0.7589 - acc: 0.4759
 768/4849 [===>..........................] - ETA: 8:30 - loss: 0.7645 - acc: 0.4779
 832/4849 [====>.........................] - ETA: 8:12 - loss: 0.7742 - acc: 0.4663
 896/4849 [====>.........................] - ETA: 7:54 - loss: 0.7729 - acc: 0.4710
 960/4849 [====>.........................] - ETA: 7:39 - loss: 0.7687 - acc: 0.4750
1024/4849 [=====>........................] - ETA: 7:22 - loss: 0.7643 - acc: 0.4805
1088/4849 [=====>........................] - ETA: 7:08 - loss: 0.7631 - acc: 0.4835
1152/4849 [======>.......................] - ETA: 6:55 - loss: 0.7585 - acc: 0.4905
1216/4849 [======>.......................] - ETA: 6:43 - loss: 0.7493 - acc: 0.5049
1280/4849 [======>.......................] - ETA: 6:32 - loss: 0.7524 - acc: 0.5023
1344/4849 [=======>......................] - ETA: 6:22 - loss: 0.7482 - acc: 0.5082
1408/4849 [=======>......................] - ETA: 6:12 - loss: 0.7455 - acc: 0.5128
1472/4849 [========>.....................] - ETA: 6:02 - loss: 0.7445 - acc: 0.5143
1536/4849 [========>.....................] - ETA: 5:52 - loss: 0.7436 - acc: 0.5130
1600/4849 [========>.....................] - ETA: 5:42 - loss: 0.7402 - acc: 0.5144
1664/4849 [=========>....................] - ETA: 5:34 - loss: 0.7403 - acc: 0.5102
1728/4849 [=========>....................] - ETA: 5:25 - loss: 0.7400 - acc: 0.5110
1792/4849 [==========>...................] - ETA: 5:17 - loss: 0.7412 - acc: 0.5067
1856/4849 [==========>...................] - ETA: 5:08 - loss: 0.7406 - acc: 0.5059
1920/4849 [==========>...................] - ETA: 5:00 - loss: 0.7390 - acc: 0.5073
1984/4849 [===========>..................] - ETA: 4:52 - loss: 0.7396 - acc: 0.5055
2048/4849 [===========>..................] - ETA: 4:44 - loss: 0.7392 - acc: 0.5029
2112/4849 [============>.................] - ETA: 4:36 - loss: 0.7402 - acc: 0.5019
2176/4849 [============>.................] - ETA: 4:29 - loss: 0.7389 - acc: 0.5023
2240/4849 [============>.................] - ETA: 4:22 - loss: 0.7384 - acc: 0.5000
2304/4849 [=============>................] - ETA: 4:14 - loss: 0.7375 - acc: 0.4996
2368/4849 [=============>................] - ETA: 4:07 - loss: 0.7379 - acc: 0.4979
2432/4849 [==============>...............] - ETA: 4:00 - loss: 0.7384 - acc: 0.4955
2496/4849 [==============>...............] - ETA: 3:53 - loss: 0.7382 - acc: 0.4972
2560/4849 [==============>...............] - ETA: 3:46 - loss: 0.7356 - acc: 0.5008
2624/4849 [===============>..............] - ETA: 3:39 - loss: 0.7344 - acc: 0.5034
2688/4849 [===============>..............] - ETA: 3:32 - loss: 0.7325 - acc: 0.5060
2752/4849 [================>.............] - ETA: 3:25 - loss: 0.7309 - acc: 0.5076
2816/4849 [================>.............] - ETA: 3:18 - loss: 0.7303 - acc: 0.5085
2880/4849 [================>.............] - ETA: 3:11 - loss: 0.7312 - acc: 0.5073
2944/4849 [=================>............] - ETA: 3:04 - loss: 0.7313 - acc: 0.5082
3008/4849 [=================>............] - ETA: 2:58 - loss: 0.7308 - acc: 0.5086
3072/4849 [==================>...........] - ETA: 2:51 - loss: 0.7299 - acc: 0.5088
3136/4849 [==================>...........] - ETA: 2:45 - loss: 0.7288 - acc: 0.5102
3200/4849 [==================>...........] - ETA: 2:38 - loss: 0.7272 - acc: 0.5112
3264/4849 [===================>..........] - ETA: 2:31 - loss: 0.7260 - acc: 0.5123
3328/4849 [===================>..........] - ETA: 2:25 - loss: 0.7259 - acc: 0.5126
3392/4849 [===================>..........] - ETA: 2:18 - loss: 0.7263 - acc: 0.5127
3456/4849 [====================>.........] - ETA: 2:12 - loss: 0.7257 - acc: 0.5127
3520/4849 [====================>.........] - ETA: 2:06 - loss: 0.7261 - acc: 0.5116
3584/4849 [=====================>........] - ETA: 1:59 - loss: 0.7258 - acc: 0.5117
3648/4849 [=====================>........] - ETA: 1:53 - loss: 0.7256 - acc: 0.5115
3712/4849 [=====================>........] - ETA: 1:47 - loss: 0.7255 - acc: 0.5110
3776/4849 [======================>.......] - ETA: 1:41 - loss: 0.7256 - acc: 0.5106
3840/4849 [======================>.......] - ETA: 1:34 - loss: 0.7247 - acc: 0.5117
3904/4849 [=======================>......] - ETA: 1:28 - loss: 0.7243 - acc: 0.5133
3968/4849 [=======================>......] - ETA: 1:22 - loss: 0.7233 - acc: 0.5151
4032/4849 [=======================>......] - ETA: 1:16 - loss: 0.7232 - acc: 0.5151
4096/4849 [========================>.....] - ETA: 1:10 - loss: 0.7229 - acc: 0.5151
4160/4849 [========================>.....] - ETA: 1:03 - loss: 0.7224 - acc: 0.5151
4224/4849 [=========================>....] - ETA: 57s - loss: 0.7214 - acc: 0.5175 
4288/4849 [=========================>....] - ETA: 51s - loss: 0.7203 - acc: 0.5189
4352/4849 [=========================>....] - ETA: 45s - loss: 0.7198 - acc: 0.5198
4416/4849 [==========================>...] - ETA: 39s - loss: 0.7189 - acc: 0.5213
4480/4849 [==========================>...] - ETA: 34s - loss: 0.7186 - acc: 0.5212
4544/4849 [===========================>..] - ETA: 28s - loss: 0.7186 - acc: 0.5213
4608/4849 [===========================>..] - ETA: 22s - loss: 0.7180 - acc: 0.5215
4672/4849 [===========================>..] - ETA: 16s - loss: 0.7172 - acc: 0.5229
4736/4849 [============================>.] - ETA: 10s - loss: 0.7168 - acc: 0.5232
4800/4849 [============================>.] - ETA: 4s - loss: 0.7168 - acc: 0.5219 
4849/4849 [==============================] - 463s 96ms/step - loss: 0.7172 - acc: 0.5207 - val_loss: 0.6929 - val_acc: 0.5399

Epoch 00001: val_acc improved from -inf to 0.53989, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window15/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 2/10

  64/4849 [..............................] - ETA: 6:12 - loss: 0.6948 - acc: 0.5312
 128/4849 [..............................] - ETA: 6:05 - loss: 0.7073 - acc: 0.5078
 192/4849 [>.............................] - ETA: 6:01 - loss: 0.6916 - acc: 0.5469
 256/4849 [>.............................] - ETA: 6:01 - loss: 0.7021 - acc: 0.5391
 320/4849 [>.............................] - ETA: 6:00 - loss: 0.7112 - acc: 0.5219
 384/4849 [=>............................] - ETA: 5:55 - loss: 0.7063 - acc: 0.5260
 448/4849 [=>............................] - ETA: 5:42 - loss: 0.7046 - acc: 0.5268
 512/4849 [==>...........................] - ETA: 5:33 - loss: 0.7039 - acc: 0.5293
 576/4849 [==>...........................] - ETA: 5:26 - loss: 0.7002 - acc: 0.5347
 640/4849 [==>...........................] - ETA: 5:14 - loss: 0.7052 - acc: 0.5219
 704/4849 [===>..........................] - ETA: 5:07 - loss: 0.7023 - acc: 0.5270
 768/4849 [===>..........................] - ETA: 5:00 - loss: 0.7007 - acc: 0.5299
 832/4849 [====>.........................] - ETA: 4:55 - loss: 0.6984 - acc: 0.5312
 896/4849 [====>.........................] - ETA: 4:51 - loss: 0.6987 - acc: 0.5335
 960/4849 [====>.........................] - ETA: 4:48 - loss: 0.7016 - acc: 0.5292
1024/4849 [=====>........................] - ETA: 4:46 - loss: 0.6978 - acc: 0.5381
1088/4849 [=====>........................] - ETA: 4:42 - loss: 0.6976 - acc: 0.5386
1152/4849 [======>.......................] - ETA: 4:39 - loss: 0.6960 - acc: 0.5373
1216/4849 [======>.......................] - ETA: 4:32 - loss: 0.6970 - acc: 0.5362
1280/4849 [======>.......................] - ETA: 4:28 - loss: 0.6972 - acc: 0.5336
1344/4849 [=======>......................] - ETA: 4:20 - loss: 0.6987 - acc: 0.5320
1408/4849 [=======>......................] - ETA: 4:15 - loss: 0.6986 - acc: 0.5320
1472/4849 [========>.....................] - ETA: 4:10 - loss: 0.6976 - acc: 0.5299
1536/4849 [========>.....................] - ETA: 4:04 - loss: 0.6962 - acc: 0.5312
1600/4849 [========>.....................] - ETA: 4:01 - loss: 0.6935 - acc: 0.5356
1664/4849 [=========>....................] - ETA: 3:56 - loss: 0.6950 - acc: 0.5343
1728/4849 [=========>....................] - ETA: 3:52 - loss: 0.6961 - acc: 0.5312
1792/4849 [==========>...................] - ETA: 3:48 - loss: 0.6961 - acc: 0.5301
1856/4849 [==========>...................] - ETA: 3:44 - loss: 0.6958 - acc: 0.5318
1920/4849 [==========>...................] - ETA: 3:39 - loss: 0.6971 - acc: 0.5276
1984/4849 [===========>..................] - ETA: 3:35 - loss: 0.6976 - acc: 0.5272
2048/4849 [===========>..................] - ETA: 3:29 - loss: 0.6986 - acc: 0.5269
2112/4849 [============>.................] - ETA: 3:24 - loss: 0.6986 - acc: 0.5260
2176/4849 [============>.................] - ETA: 3:19 - loss: 0.6982 - acc: 0.5267
2240/4849 [============>.................] - ETA: 3:14 - loss: 0.6975 - acc: 0.5281
2304/4849 [=============>................] - ETA: 3:09 - loss: 0.6980 - acc: 0.5282
2368/4849 [=============>................] - ETA: 3:05 - loss: 0.6982 - acc: 0.5279
2432/4849 [==============>...............] - ETA: 3:01 - loss: 0.6986 - acc: 0.5259
2496/4849 [==============>...............] - ETA: 2:57 - loss: 0.6992 - acc: 0.5240
2560/4849 [==============>...............] - ETA: 2:52 - loss: 0.6987 - acc: 0.5262
2624/4849 [===============>..............] - ETA: 2:48 - loss: 0.6994 - acc: 0.5248
2688/4849 [===============>..............] - ETA: 2:43 - loss: 0.6995 - acc: 0.5242
2752/4849 [================>.............] - ETA: 2:38 - loss: 0.6988 - acc: 0.5269
2816/4849 [================>.............] - ETA: 2:33 - loss: 0.6982 - acc: 0.5284
2880/4849 [================>.............] - ETA: 2:28 - loss: 0.6975 - acc: 0.5302
2944/4849 [=================>............] - ETA: 2:23 - loss: 0.6975 - acc: 0.5299
3008/4849 [=================>............] - ETA: 2:18 - loss: 0.6972 - acc: 0.5316
3072/4849 [==================>...........] - ETA: 2:13 - loss: 0.6960 - acc: 0.5335
3136/4849 [==================>...........] - ETA: 2:08 - loss: 0.6956 - acc: 0.5351
3200/4849 [==================>...........] - ETA: 2:04 - loss: 0.6956 - acc: 0.5366
3264/4849 [===================>..........] - ETA: 1:59 - loss: 0.6961 - acc: 0.5358
3328/4849 [===================>..........] - ETA: 1:54 - loss: 0.6963 - acc: 0.5361
3392/4849 [===================>..........] - ETA: 1:50 - loss: 0.6968 - acc: 0.5357
3456/4849 [====================>.........] - ETA: 1:45 - loss: 0.6961 - acc: 0.5367
3520/4849 [====================>.........] - ETA: 1:40 - loss: 0.6965 - acc: 0.5358
3584/4849 [=====================>........] - ETA: 1:35 - loss: 0.6967 - acc: 0.5357
3648/4849 [=====================>........] - ETA: 1:30 - loss: 0.6967 - acc: 0.5362
3712/4849 [=====================>........] - ETA: 1:25 - loss: 0.6970 - acc: 0.5353
3776/4849 [======================>.......] - ETA: 1:20 - loss: 0.6968 - acc: 0.5360
3840/4849 [======================>.......] - ETA: 1:16 - loss: 0.6972 - acc: 0.5352
3904/4849 [=======================>......] - ETA: 1:11 - loss: 0.6979 - acc: 0.5330
3968/4849 [=======================>......] - ETA: 1:06 - loss: 0.6975 - acc: 0.5343
4032/4849 [=======================>......] - ETA: 1:02 - loss: 0.6976 - acc: 0.5327
4096/4849 [========================>.....] - ETA: 57s - loss: 0.6973 - acc: 0.5332 
4160/4849 [========================>.....] - ETA: 52s - loss: 0.6976 - acc: 0.5320
4224/4849 [=========================>....] - ETA: 47s - loss: 0.6975 - acc: 0.5322
4288/4849 [=========================>....] - ETA: 43s - loss: 0.6971 - acc: 0.5331
4352/4849 [=========================>....] - ETA: 38s - loss: 0.6965 - acc: 0.5356
4416/4849 [==========================>...] - ETA: 33s - loss: 0.6964 - acc: 0.5365
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6962 - acc: 0.5364
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6960 - acc: 0.5365
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6959 - acc: 0.5369
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6955 - acc: 0.5377
4736/4849 [============================>.] - ETA: 8s - loss: 0.6952 - acc: 0.5380 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6946 - acc: 0.5394
4849/4849 [==============================] - 392s 81ms/step - loss: 0.6951 - acc: 0.5385 - val_loss: 0.6933 - val_acc: 0.5529

Epoch 00002: val_acc improved from 0.53989 to 0.55288, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window15/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 3/10

  64/4849 [..............................] - ETA: 6:21 - loss: 0.6902 - acc: 0.6250
 128/4849 [..............................] - ETA: 6:18 - loss: 0.6755 - acc: 0.6016
 192/4849 [>.............................] - ETA: 6:13 - loss: 0.6798 - acc: 0.5781
 256/4849 [>.............................] - ETA: 6:12 - loss: 0.6755 - acc: 0.5938
 320/4849 [>.............................] - ETA: 6:15 - loss: 0.6734 - acc: 0.5844
 384/4849 [=>............................] - ETA: 6:08 - loss: 0.6729 - acc: 0.5833
 448/4849 [=>............................] - ETA: 6:03 - loss: 0.6789 - acc: 0.5737
 512/4849 [==>...........................] - ETA: 5:57 - loss: 0.6815 - acc: 0.5762
 576/4849 [==>...........................] - ETA: 5:54 - loss: 0.6780 - acc: 0.5799
 640/4849 [==>...........................] - ETA: 5:49 - loss: 0.6753 - acc: 0.5828
 704/4849 [===>..........................] - ETA: 5:45 - loss: 0.6774 - acc: 0.5767
 768/4849 [===>..........................] - ETA: 5:42 - loss: 0.6807 - acc: 0.5729
 832/4849 [====>.........................] - ETA: 5:37 - loss: 0.6818 - acc: 0.5721
 896/4849 [====>.........................] - ETA: 5:31 - loss: 0.6832 - acc: 0.5714
 960/4849 [====>.........................] - ETA: 5:25 - loss: 0.6826 - acc: 0.5771
1024/4849 [=====>........................] - ETA: 5:19 - loss: 0.6821 - acc: 0.5801
1088/4849 [=====>........................] - ETA: 5:14 - loss: 0.6819 - acc: 0.5809
1152/4849 [======>.......................] - ETA: 5:09 - loss: 0.6807 - acc: 0.5833
1216/4849 [======>.......................] - ETA: 5:04 - loss: 0.6827 - acc: 0.5740
1280/4849 [======>.......................] - ETA: 4:59 - loss: 0.6849 - acc: 0.5680
1344/4849 [=======>......................] - ETA: 4:53 - loss: 0.6869 - acc: 0.5625
1408/4849 [=======>......................] - ETA: 4:49 - loss: 0.6884 - acc: 0.5604
1472/4849 [========>.....................] - ETA: 4:43 - loss: 0.6878 - acc: 0.5618
1536/4849 [========>.....................] - ETA: 4:38 - loss: 0.6888 - acc: 0.5586
1600/4849 [========>.....................] - ETA: 4:33 - loss: 0.6895 - acc: 0.5556
1664/4849 [=========>....................] - ETA: 4:27 - loss: 0.6911 - acc: 0.5511
1728/4849 [=========>....................] - ETA: 4:22 - loss: 0.6903 - acc: 0.5521
1792/4849 [==========>...................] - ETA: 4:16 - loss: 0.6902 - acc: 0.5530
1856/4849 [==========>...................] - ETA: 4:11 - loss: 0.6905 - acc: 0.5512
1920/4849 [==========>...................] - ETA: 4:06 - loss: 0.6909 - acc: 0.5479
1984/4849 [===========>..................] - ETA: 4:00 - loss: 0.6903 - acc: 0.5489
2048/4849 [===========>..................] - ETA: 3:54 - loss: 0.6901 - acc: 0.5488
2112/4849 [============>.................] - ETA: 3:48 - loss: 0.6910 - acc: 0.5455
2176/4849 [============>.................] - ETA: 3:42 - loss: 0.6901 - acc: 0.5478
2240/4849 [============>.................] - ETA: 3:37 - loss: 0.6899 - acc: 0.5487
2304/4849 [=============>................] - ETA: 3:32 - loss: 0.6898 - acc: 0.5495
2368/4849 [=============>................] - ETA: 3:26 - loss: 0.6911 - acc: 0.5473
2432/4849 [==============>...............] - ETA: 3:20 - loss: 0.6921 - acc: 0.5461
2496/4849 [==============>...............] - ETA: 3:15 - loss: 0.6936 - acc: 0.5433
2560/4849 [==============>...............] - ETA: 3:09 - loss: 0.6933 - acc: 0.5437
2624/4849 [===============>..............] - ETA: 3:04 - loss: 0.6925 - acc: 0.5450
2688/4849 [===============>..............] - ETA: 2:58 - loss: 0.6923 - acc: 0.5435
2752/4849 [================>.............] - ETA: 2:53 - loss: 0.6923 - acc: 0.5432
2816/4849 [================>.............] - ETA: 2:47 - loss: 0.6924 - acc: 0.5433
2880/4849 [================>.............] - ETA: 2:42 - loss: 0.6930 - acc: 0.5427
2944/4849 [=================>............] - ETA: 2:37 - loss: 0.6933 - acc: 0.5411
3008/4849 [=================>............] - ETA: 2:31 - loss: 0.6941 - acc: 0.5406
3072/4849 [==================>...........] - ETA: 2:26 - loss: 0.6937 - acc: 0.5423
3136/4849 [==================>...........] - ETA: 2:20 - loss: 0.6929 - acc: 0.5453
3200/4849 [==================>...........] - ETA: 2:15 - loss: 0.6927 - acc: 0.5453
3264/4849 [===================>..........] - ETA: 2:09 - loss: 0.6927 - acc: 0.5456
3328/4849 [===================>..........] - ETA: 2:04 - loss: 0.6934 - acc: 0.5433
3392/4849 [===================>..........] - ETA: 1:59 - loss: 0.6936 - acc: 0.5427
3456/4849 [====================>.........] - ETA: 1:54 - loss: 0.6935 - acc: 0.5434
3520/4849 [====================>.........] - ETA: 1:48 - loss: 0.6930 - acc: 0.5449
3584/4849 [=====================>........] - ETA: 1:43 - loss: 0.6930 - acc: 0.5446
3648/4849 [=====================>........] - ETA: 1:38 - loss: 0.6922 - acc: 0.5466
3712/4849 [=====================>........] - ETA: 1:32 - loss: 0.6921 - acc: 0.5471
3776/4849 [======================>.......] - ETA: 1:27 - loss: 0.6921 - acc: 0.5456
3840/4849 [======================>.......] - ETA: 1:22 - loss: 0.6930 - acc: 0.5435
3904/4849 [=======================>......] - ETA: 1:16 - loss: 0.6934 - acc: 0.5420
3968/4849 [=======================>......] - ETA: 1:11 - loss: 0.6935 - acc: 0.5426
4032/4849 [=======================>......] - ETA: 1:06 - loss: 0.6927 - acc: 0.5446
4096/4849 [========================>.....] - ETA: 1:01 - loss: 0.6928 - acc: 0.5442
4160/4849 [========================>.....] - ETA: 56s - loss: 0.6929 - acc: 0.5437 
4224/4849 [=========================>....] - ETA: 50s - loss: 0.6926 - acc: 0.5452
4288/4849 [=========================>....] - ETA: 45s - loss: 0.6919 - acc: 0.5459
4352/4849 [=========================>....] - ETA: 40s - loss: 0.6915 - acc: 0.5464
4416/4849 [==========================>...] - ETA: 35s - loss: 0.6911 - acc: 0.5471
4480/4849 [==========================>...] - ETA: 30s - loss: 0.6913 - acc: 0.5471
4544/4849 [===========================>..] - ETA: 24s - loss: 0.6911 - acc: 0.5478
4608/4849 [===========================>..] - ETA: 19s - loss: 0.6909 - acc: 0.5484
4672/4849 [===========================>..] - ETA: 14s - loss: 0.6914 - acc: 0.5477
4736/4849 [============================>.] - ETA: 9s - loss: 0.6914 - acc: 0.5469 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6914 - acc: 0.5481
4849/4849 [==============================] - 405s 84ms/step - loss: 0.6910 - acc: 0.5488 - val_loss: 0.6898 - val_acc: 0.5492

Epoch 00003: val_acc did not improve from 0.55288
Epoch 4/10

  64/4849 [..............................] - ETA: 6:53 - loss: 0.6460 - acc: 0.6562
 128/4849 [..............................] - ETA: 6:43 - loss: 0.6456 - acc: 0.6484
 192/4849 [>.............................] - ETA: 6:34 - loss: 0.6596 - acc: 0.6042
 256/4849 [>.............................] - ETA: 6:09 - loss: 0.6782 - acc: 0.5859
 320/4849 [>.............................] - ETA: 5:58 - loss: 0.6858 - acc: 0.5813
 384/4849 [=>............................] - ETA: 5:47 - loss: 0.6888 - acc: 0.5651
 448/4849 [=>............................] - ETA: 5:35 - loss: 0.6836 - acc: 0.5647
 512/4849 [==>...........................] - ETA: 5:27 - loss: 0.6814 - acc: 0.5703
 576/4849 [==>...........................] - ETA: 5:18 - loss: 0.6821 - acc: 0.5677
 640/4849 [==>...........................] - ETA: 5:19 - loss: 0.6816 - acc: 0.5672
 704/4849 [===>..........................] - ETA: 5:17 - loss: 0.6799 - acc: 0.5682
 768/4849 [===>..........................] - ETA: 5:15 - loss: 0.6768 - acc: 0.5729
 832/4849 [====>.........................] - ETA: 5:10 - loss: 0.6767 - acc: 0.5733
 896/4849 [====>.........................] - ETA: 5:07 - loss: 0.6751 - acc: 0.5737
 960/4849 [====>.........................] - ETA: 5:00 - loss: 0.6762 - acc: 0.5729
1024/4849 [=====>........................] - ETA: 4:54 - loss: 0.6752 - acc: 0.5732
1088/4849 [=====>........................] - ETA: 4:47 - loss: 0.6773 - acc: 0.5689
1152/4849 [======>.......................] - ETA: 4:40 - loss: 0.6788 - acc: 0.5712
1216/4849 [======>.......................] - ETA: 4:39 - loss: 0.6831 - acc: 0.5683
1280/4849 [======>.......................] - ETA: 4:34 - loss: 0.6830 - acc: 0.5656
1344/4849 [=======>......................] - ETA: 4:30 - loss: 0.6818 - acc: 0.5699
1408/4849 [=======>......................] - ETA: 4:26 - loss: 0.6824 - acc: 0.5689
1472/4849 [========>.....................] - ETA: 4:22 - loss: 0.6832 - acc: 0.5645
1536/4849 [========>.....................] - ETA: 4:17 - loss: 0.6830 - acc: 0.5671
1600/4849 [========>.....................] - ETA: 4:14 - loss: 0.6828 - acc: 0.5663
1664/4849 [=========>....................] - ETA: 4:09 - loss: 0.6836 - acc: 0.5661
1728/4849 [=========>....................] - ETA: 4:05 - loss: 0.6832 - acc: 0.5671
1792/4849 [==========>...................] - ETA: 3:59 - loss: 0.6834 - acc: 0.5664
1856/4849 [==========>...................] - ETA: 3:54 - loss: 0.6848 - acc: 0.5641
1920/4849 [==========>...................] - ETA: 3:49 - loss: 0.6850 - acc: 0.5646
1984/4849 [===========>..................] - ETA: 3:43 - loss: 0.6856 - acc: 0.5625
2048/4849 [===========>..................] - ETA: 3:38 - loss: 0.6859 - acc: 0.5610
2112/4849 [============>.................] - ETA: 3:33 - loss: 0.6864 - acc: 0.5601
2176/4849 [============>.................] - ETA: 3:28 - loss: 0.6858 - acc: 0.5616
2240/4849 [============>.................] - ETA: 3:23 - loss: 0.6854 - acc: 0.5607
2304/4849 [=============>................] - ETA: 3:18 - loss: 0.6853 - acc: 0.5595
2368/4849 [=============>................] - ETA: 3:12 - loss: 0.6854 - acc: 0.5583
2432/4849 [==============>...............] - ETA: 3:08 - loss: 0.6855 - acc: 0.5580
2496/4849 [==============>...............] - ETA: 3:02 - loss: 0.6855 - acc: 0.5585
2560/4849 [==============>...............] - ETA: 2:57 - loss: 0.6844 - acc: 0.5617
2624/4849 [===============>..............] - ETA: 2:52 - loss: 0.6848 - acc: 0.5629
2688/4849 [===============>..............] - ETA: 2:47 - loss: 0.6848 - acc: 0.5629
2752/4849 [================>.............] - ETA: 2:42 - loss: 0.6853 - acc: 0.5618
2816/4849 [================>.............] - ETA: 2:37 - loss: 0.6857 - acc: 0.5621
2880/4849 [================>.............] - ETA: 2:32 - loss: 0.6856 - acc: 0.5628
2944/4849 [=================>............] - ETA: 2:27 - loss: 0.6856 - acc: 0.5628
3008/4849 [=================>............] - ETA: 2:22 - loss: 0.6852 - acc: 0.5648
3072/4849 [==================>...........] - ETA: 2:17 - loss: 0.6851 - acc: 0.5641
3136/4849 [==================>...........] - ETA: 2:12 - loss: 0.6849 - acc: 0.5654
3200/4849 [==================>...........] - ETA: 2:07 - loss: 0.6842 - acc: 0.5672
3264/4849 [===================>..........] - ETA: 2:02 - loss: 0.6841 - acc: 0.5665
3328/4849 [===================>..........] - ETA: 1:57 - loss: 0.6842 - acc: 0.5676
3392/4849 [===================>..........] - ETA: 1:53 - loss: 0.6843 - acc: 0.5681
3456/4849 [====================>.........] - ETA: 1:48 - loss: 0.6840 - acc: 0.5692
3520/4849 [====================>.........] - ETA: 1:43 - loss: 0.6841 - acc: 0.5693
3584/4849 [=====================>........] - ETA: 1:38 - loss: 0.6836 - acc: 0.5692
3648/4849 [=====================>........] - ETA: 1:33 - loss: 0.6833 - acc: 0.5685
3712/4849 [=====================>........] - ETA: 1:28 - loss: 0.6831 - acc: 0.5682
3776/4849 [======================>.......] - ETA: 1:23 - loss: 0.6821 - acc: 0.5702
3840/4849 [======================>.......] - ETA: 1:18 - loss: 0.6820 - acc: 0.5706
3904/4849 [=======================>......] - ETA: 1:13 - loss: 0.6819 - acc: 0.5710
3968/4849 [=======================>......] - ETA: 1:08 - loss: 0.6814 - acc: 0.5718
4032/4849 [=======================>......] - ETA: 1:03 - loss: 0.6813 - acc: 0.5717
4096/4849 [========================>.....] - ETA: 58s - loss: 0.6814 - acc: 0.5720 
4160/4849 [========================>.....] - ETA: 53s - loss: 0.6815 - acc: 0.5714
4224/4849 [=========================>....] - ETA: 48s - loss: 0.6817 - acc: 0.5710
4288/4849 [=========================>....] - ETA: 43s - loss: 0.6816 - acc: 0.5711
4352/4849 [=========================>....] - ETA: 38s - loss: 0.6811 - acc: 0.5722
4416/4849 [==========================>...] - ETA: 33s - loss: 0.6817 - acc: 0.5704
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6814 - acc: 0.5708
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6812 - acc: 0.5713
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6815 - acc: 0.5707
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6819 - acc: 0.5706
4736/4849 [============================>.] - ETA: 8s - loss: 0.6814 - acc: 0.5714 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6820 - acc: 0.5713
4849/4849 [==============================] - 392s 81ms/step - loss: 0.6823 - acc: 0.5700 - val_loss: 0.6906 - val_acc: 0.5547

Epoch 00004: val_acc improved from 0.55288 to 0.55473, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window15/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 5/10

  64/4849 [..............................] - ETA: 6:32 - loss: 0.7256 - acc: 0.5156
 128/4849 [..............................] - ETA: 6:13 - loss: 0.6970 - acc: 0.5547
 192/4849 [>.............................] - ETA: 6:11 - loss: 0.6913 - acc: 0.5677
 256/4849 [>.............................] - ETA: 6:02 - loss: 0.6955 - acc: 0.5625
 320/4849 [>.............................] - ETA: 5:55 - loss: 0.6850 - acc: 0.5719
 384/4849 [=>............................] - ETA: 5:48 - loss: 0.6820 - acc: 0.5755
 448/4849 [=>............................] - ETA: 5:41 - loss: 0.6859 - acc: 0.5670
 512/4849 [==>...........................] - ETA: 5:38 - loss: 0.6888 - acc: 0.5586
 576/4849 [==>...........................] - ETA: 5:33 - loss: 0.6874 - acc: 0.5642
 640/4849 [==>...........................] - ETA: 5:26 - loss: 0.6844 - acc: 0.5609
 704/4849 [===>..........................] - ETA: 5:19 - loss: 0.6860 - acc: 0.5554
 768/4849 [===>..........................] - ETA: 5:15 - loss: 0.6875 - acc: 0.5534
 832/4849 [====>.........................] - ETA: 5:09 - loss: 0.6888 - acc: 0.5529
 896/4849 [====>.........................] - ETA: 5:04 - loss: 0.6906 - acc: 0.5480
 960/4849 [====>.........................] - ETA: 4:56 - loss: 0.6892 - acc: 0.5500
1024/4849 [=====>........................] - ETA: 4:51 - loss: 0.6896 - acc: 0.5449
1088/4849 [=====>........................] - ETA: 4:46 - loss: 0.6867 - acc: 0.5496
1152/4849 [======>.......................] - ETA: 4:41 - loss: 0.6858 - acc: 0.5512
1216/4849 [======>.......................] - ETA: 4:37 - loss: 0.6847 - acc: 0.5535
1280/4849 [======>.......................] - ETA: 4:32 - loss: 0.6854 - acc: 0.5547
1344/4849 [=======>......................] - ETA: 4:27 - loss: 0.6843 - acc: 0.5588
1408/4849 [=======>......................] - ETA: 4:22 - loss: 0.6854 - acc: 0.5582
1472/4849 [========>.....................] - ETA: 4:18 - loss: 0.6854 - acc: 0.5550
1536/4849 [========>.....................] - ETA: 4:12 - loss: 0.6854 - acc: 0.5540
1600/4849 [========>.....................] - ETA: 4:08 - loss: 0.6860 - acc: 0.5544
1664/4849 [=========>....................] - ETA: 4:03 - loss: 0.6850 - acc: 0.5595
1728/4849 [=========>....................] - ETA: 3:58 - loss: 0.6851 - acc: 0.5584
1792/4849 [==========>...................] - ETA: 3:53 - loss: 0.6844 - acc: 0.5603
1856/4849 [==========>...................] - ETA: 3:47 - loss: 0.6837 - acc: 0.5625
1920/4849 [==========>...................] - ETA: 3:43 - loss: 0.6837 - acc: 0.5635
1984/4849 [===========>..................] - ETA: 3:38 - loss: 0.6844 - acc: 0.5640
2048/4849 [===========>..................] - ETA: 3:34 - loss: 0.6855 - acc: 0.5635
2112/4849 [============>.................] - ETA: 3:29 - loss: 0.6869 - acc: 0.5616
2176/4849 [============>.................] - ETA: 3:24 - loss: 0.6862 - acc: 0.5630
2240/4849 [============>.................] - ETA: 3:19 - loss: 0.6854 - acc: 0.5652
2304/4849 [=============>................] - ETA: 3:14 - loss: 0.6850 - acc: 0.5642
2368/4849 [=============>................] - ETA: 3:10 - loss: 0.6857 - acc: 0.5638
2432/4849 [==============>...............] - ETA: 3:05 - loss: 0.6842 - acc: 0.5666
2496/4849 [==============>...............] - ETA: 3:00 - loss: 0.6841 - acc: 0.5657
2560/4849 [==============>...............] - ETA: 2:55 - loss: 0.6839 - acc: 0.5676
2624/4849 [===============>..............] - ETA: 2:50 - loss: 0.6838 - acc: 0.5671
2688/4849 [===============>..............] - ETA: 2:45 - loss: 0.6839 - acc: 0.5662
2752/4849 [================>.............] - ETA: 2:40 - loss: 0.6829 - acc: 0.5672
2816/4849 [================>.............] - ETA: 2:35 - loss: 0.6829 - acc: 0.5668
2880/4849 [================>.............] - ETA: 2:30 - loss: 0.6825 - acc: 0.5663
2944/4849 [=================>............] - ETA: 2:25 - loss: 0.6839 - acc: 0.5639
3008/4849 [=================>............] - ETA: 2:20 - loss: 0.6843 - acc: 0.5628
3072/4849 [==================>...........] - ETA: 2:15 - loss: 0.6842 - acc: 0.5625
3136/4849 [==================>...........] - ETA: 2:10 - loss: 0.6837 - acc: 0.5635
3200/4849 [==================>...........] - ETA: 2:06 - loss: 0.6845 - acc: 0.5609
3264/4849 [===================>..........] - ETA: 2:01 - loss: 0.6841 - acc: 0.5610
3328/4849 [===================>..........] - ETA: 1:56 - loss: 0.6837 - acc: 0.5613
3392/4849 [===================>..........] - ETA: 1:51 - loss: 0.6844 - acc: 0.5604
3456/4849 [====================>.........] - ETA: 1:46 - loss: 0.6847 - acc: 0.5602
3520/4849 [====================>.........] - ETA: 1:41 - loss: 0.6839 - acc: 0.5634
3584/4849 [=====================>........] - ETA: 1:36 - loss: 0.6842 - acc: 0.5633
3648/4849 [=====================>........] - ETA: 1:31 - loss: 0.6841 - acc: 0.5641
3712/4849 [=====================>........] - ETA: 1:26 - loss: 0.6840 - acc: 0.5641
3776/4849 [======================>.......] - ETA: 1:22 - loss: 0.6837 - acc: 0.5646
3840/4849 [======================>.......] - ETA: 1:17 - loss: 0.6841 - acc: 0.5635
3904/4849 [=======================>......] - ETA: 1:12 - loss: 0.6838 - acc: 0.5628
3968/4849 [=======================>......] - ETA: 1:07 - loss: 0.6841 - acc: 0.5622
4032/4849 [=======================>......] - ETA: 1:02 - loss: 0.6845 - acc: 0.5613
4096/4849 [========================>.....] - ETA: 57s - loss: 0.6843 - acc: 0.5618 
4160/4849 [========================>.....] - ETA: 52s - loss: 0.6847 - acc: 0.5613
4224/4849 [=========================>....] - ETA: 47s - loss: 0.6843 - acc: 0.5620
4288/4849 [=========================>....] - ETA: 42s - loss: 0.6834 - acc: 0.5637
4352/4849 [=========================>....] - ETA: 37s - loss: 0.6830 - acc: 0.5646
4416/4849 [==========================>...] - ETA: 33s - loss: 0.6827 - acc: 0.5643
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6827 - acc: 0.5650
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6826 - acc: 0.5654
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6825 - acc: 0.5658
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6823 - acc: 0.5666
4736/4849 [============================>.] - ETA: 8s - loss: 0.6813 - acc: 0.5682 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6808 - acc: 0.5683
4849/4849 [==============================] - 387s 80ms/step - loss: 0.6804 - acc: 0.5692 - val_loss: 0.6918 - val_acc: 0.5677

Epoch 00005: val_acc improved from 0.55473 to 0.56772, saving model to /data/lyli/Pancreas/Pancreas-model/Seq_feature_exact/window15/checkpoints/final_seq_model/pancreas_seq_model_2_mer.hdf5
Epoch 6/10

  64/4849 [..............................] - ETA: 6:20 - loss: 0.7478 - acc: 0.4219
 128/4849 [..............................] - ETA: 6:08 - loss: 0.7199 - acc: 0.5234
 192/4849 [>.............................] - ETA: 5:54 - loss: 0.7051 - acc: 0.5365
 256/4849 [>.............................] - ETA: 5:54 - loss: 0.6966 - acc: 0.5430
 320/4849 [>.............................] - ETA: 5:48 - loss: 0.7032 - acc: 0.5250
 384/4849 [=>............................] - ETA: 5:48 - loss: 0.6995 - acc: 0.5339
 448/4849 [=>............................] - ETA: 5:48 - loss: 0.7022 - acc: 0.5290
 512/4849 [==>...........................] - ETA: 5:46 - loss: 0.6954 - acc: 0.5391
 576/4849 [==>...........................] - ETA: 5:40 - loss: 0.6904 - acc: 0.5469
 640/4849 [==>...........................] - ETA: 5:36 - loss: 0.6884 - acc: 0.5484
 704/4849 [===>..........................] - ETA: 5:28 - loss: 0.6888 - acc: 0.5582
 768/4849 [===>..........................] - ETA: 5:22 - loss: 0.6894 - acc: 0.5534
 832/4849 [====>.........................] - ETA: 5:16 - loss: 0.6889 - acc: 0.5589
 896/4849 [====>.........................] - ETA: 5:11 - loss: 0.6864 - acc: 0.5580
 960/4849 [====>.........................] - ETA: 5:05 - loss: 0.6850 - acc: 0.5573
1024/4849 [=====>........................] - ETA: 5:00 - loss: 0.6879 - acc: 0.5479
1088/4849 [=====>........................] - ETA: 4:56 - loss: 0.6887 - acc: 0.5506
1152/4849 [======>.......................] - ETA: 4:51 - loss: 0.6896 - acc: 0.5512
1216/4849 [======>.......................] - ETA: 4:46 - loss: 0.6866 - acc: 0.5576
1280/4849 [======>.......................] - ETA: 4:41 - loss: 0.6853 - acc: 0.5641
1344/4849 [=======>......................] - ETA: 4:35 - loss: 0.6830 - acc: 0.5677
1408/4849 [=======>......................] - ETA: 4:30 - loss: 0.6849 - acc: 0.5639
1472/4849 [========>.....................] - ETA: 4:26 - loss: 0.6837 - acc: 0.5659
1536/4849 [========>.....................] - ETA: 4:21 - loss: 0.6835 - acc: 0.5645
1600/4849 [========>.....................] - ETA: 4:16 - loss: 0.6837 - acc: 0.5637
1664/4849 [=========>....................] - ETA: 4:12 - loss: 0.6813 - acc: 0.5697
1728/4849 [=========>....................] - ETA: 4:07 - loss: 0.6808 - acc: 0.5694
1792/4849 [==========>...................] - ETA: 4:01 - loss: 0.6806 - acc: 0.5686
1856/4849 [==========>...................] - ETA: 3:55 - loss: 0.6803 - acc: 0.5700
1920/4849 [==========>...................] - ETA: 3:50 - loss: 0.6801 - acc: 0.5698
1984/4849 [===========>..................] - ETA: 3:45 - loss: 0.6799 - acc: 0.5706
2048/4849 [===========>..................] - ETA: 3:40 - loss: 0.6806 - acc: 0.5679
2112/4849 [============>.................] - ETA: 3:34 - loss: 0.6806 - acc: 0.5682
2176/4849 [============>.................] - ETA: 3:29 - loss: 0.6781 - acc: 0.5722
2240/4849 [============>.................] - ETA: 3:24 - loss: 0.6785 - acc: 0.5692
2304/4849 [=============>................] - ETA: 3:19 - loss: 0.6784 - acc: 0.5699
2368/4849 [=============>................] - ETA: 3:14 - loss: 0.6787 - acc: 0.5697
2432/4849 [==============>...............] - ETA: 3:08 - loss: 0.6792 - acc: 0.5678
2496/4849 [==============>...............] - ETA: 3:03 - loss: 0.6795 - acc: 0.5669
2560/4849 [==============>...............] - ETA: 2:58 - loss: 0.6797 - acc: 0.5672
2624/4849 [===============>..............] - ETA: 2:53 - loss: 0.6792 - acc: 0.5690
2688/4849 [===============>..............] - ETA: 2:48 - loss: 0.6789 - acc: 0.5703
2752/4849 [================>.............] - ETA: 2:43 - loss: 0.6789 - acc: 0.5705
2816/4849 [================>.............] - ETA: 2:38 - loss: 0.6794 - acc: 0.5700
2880/4849 [================>.............] - ETA: 2:33 - loss: 0.6803 - acc: 0.5701
2944/4849 [=================>............] - ETA: 2:28 - loss: 0.6806 - acc: 0.5686
3008/4849 [=================>............] - ETA: 2:23 - loss: 0.6799 - acc: 0.5701
3072/4849 [==================>...........] - ETA: 2:18 - loss: 0.6795 - acc: 0.5700
3136/4849 [==================>...........] - ETA: 2:13 - loss: 0.6789 - acc: 0.5721
3200/4849 [==================>...........] - ETA: 2:08 - loss: 0.6792 - acc: 0.5703
3264/4849 [===================>..........] - ETA: 2:03 - loss: 0.6791 - acc: 0.5695
3328/4849 [===================>..........] - ETA: 1:58 - loss: 0.6783 - acc: 0.5706
3392/4849 [===================>..........] - ETA: 1:53 - loss: 0.6794 - acc: 0.5681
3456/4849 [====================>.........] - ETA: 1:48 - loss: 0.6795 - acc: 0.5677
3520/4849 [====================>.........] - ETA: 1:43 - loss: 0.6782 - acc: 0.5705
3584/4849 [=====================>........] - ETA: 1:38 - loss: 0.6776 - acc: 0.5717
3648/4849 [=====================>........] - ETA: 1:33 - loss: 0.6775 - acc: 0.5710
3712/4849 [=====================>........] - ETA: 1:28 - loss: 0.6768 - acc: 0.5725
3776/4849 [======================>.......] - ETA: 1:23 - loss: 0.6765 - acc: 0.5731
3840/4849 [======================>.......] - ETA: 1:18 - loss: 0.6764 - acc: 0.5734
3904/4849 [=======================>......] - ETA: 1:13 - loss: 0.6770 - acc: 0.5720
3968/4849 [=======================>......] - ETA: 1:08 - loss: 0.6766 - acc: 0.5723
4032/4849 [=======================>......] - ETA: 1:03 - loss: 0.6757 - acc: 0.5744
4096/4849 [========================>.....] - ETA: 58s - loss: 0.6760 - acc: 0.5735 
4160/4849 [========================>.....] - ETA: 53s - loss: 0.6760 - acc: 0.5745
4224/4849 [=========================>....] - ETA: 48s - loss: 0.6763 - acc: 0.5748
4288/4849 [=========================>....] - ETA: 43s - loss: 0.6764 - acc: 0.5763
4352/4849 [=========================>....] - ETA: 38s - loss: 0.6761 - acc: 0.5756
4416/4849 [==========================>...] - ETA: 33s - loss: 0.6767 - acc: 0.5745
4480/4849 [==========================>...] - ETA: 28s - loss: 0.6768 - acc: 0.5746
4544/4849 [===========================>..] - ETA: 23s - loss: 0.6770 - acc: 0.5742
4608/4849 [===========================>..] - ETA: 18s - loss: 0.6764 - acc: 0.5747
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6761 - acc: 0.5760
4736/4849 [============================>.] - ETA: 8s - loss: 0.6765 - acc: 0.5752 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6764 - acc: 0.5750
4849/4849 [==============================] - 390s 80ms/step - loss: 0.6766 - acc: 0.5750 - val_loss: 0.7049 - val_acc: 0.5121

Epoch 00006: val_acc did not improve from 0.56772
Epoch 7/10

  64/4849 [..............................] - ETA: 5:12 - loss: 0.7090 - acc: 0.4531
 128/4849 [..............................] - ETA: 5:25 - loss: 0.7196 - acc: 0.4766
 192/4849 [>.............................] - ETA: 5:26 - loss: 0.7080 - acc: 0.5156
 256/4849 [>.............................] - ETA: 5:19 - loss: 0.6999 - acc: 0.5430
 320/4849 [>.............................] - ETA: 5:17 - loss: 0.6838 - acc: 0.5719
 384/4849 [=>............................] - ETA: 5:12 - loss: 0.6822 - acc: 0.5781
 448/4849 [=>............................] - ETA: 5:07 - loss: 0.6834 - acc: 0.5759
 512/4849 [==>...........................] - ETA: 5:01 - loss: 0.6819 - acc: 0.5742
 576/4849 [==>...........................] - ETA: 4:56 - loss: 0.6801 - acc: 0.5694
 640/4849 [==>...........................] - ETA: 4:53 - loss: 0.6792 - acc: 0.5687
 704/4849 [===>..........................] - ETA: 4:47 - loss: 0.6772 - acc: 0.5739
 768/4849 [===>..........................] - ETA: 4:46 - loss: 0.6797 - acc: 0.5664
 832/4849 [====>.........................] - ETA: 4:42 - loss: 0.6792 - acc: 0.5709
 896/4849 [====>.........................] - ETA: 4:38 - loss: 0.6755 - acc: 0.5804
 960/4849 [====>.........................] - ETA: 4:34 - loss: 0.6738 - acc: 0.5813
1024/4849 [=====>........................] - ETA: 4:31 - loss: 0.6739 - acc: 0.5840
1088/4849 [=====>........................] - ETA: 4:26 - loss: 0.6759 - acc: 0.5754
1152/4849 [======>.......................] - ETA: 4:20 - loss: 0.6758 - acc: 0.5773
1216/4849 [======>.......................] - ETA: 4:16 - loss: 0.6771 - acc: 0.5773
1280/4849 [======>.......................] - ETA: 4:11 - loss: 0.6778 - acc: 0.5758
1344/4849 [=======>......................] - ETA: 4:06 - loss: 0.6746 - acc: 0.5804
1408/4849 [=======>......................] - ETA: 4:01 - loss: 0.6758 - acc: 0.5803
1472/4849 [========>.....................] - ETA: 3:57 - loss: 0.6753 - acc: 0.5802
1536/4849 [========>.....................] - ETA: 3:54 - loss: 0.6754 - acc: 0.5801
1600/4849 [========>.....................] - ETA: 3:48 - loss: 0.6755 - acc: 0.5800
1664/4849 [=========>....................] - ETA: 3:43 - loss: 0.6749 - acc: 0.5799
1728/4849 [=========>....................] - ETA: 3:38 - loss: 0.6749 - acc: 0.5799
1792/4849 [==========>...................] - ETA: 3:33 - loss: 0.6744 - acc: 0.5815
1856/4849 [==========>...................] - ETA: 3:28 - loss: 0.6729 - acc: 0.5846
1920/4849 [==========>...................] - ETA: 3:23 - loss: 0.6733 - acc: 0.5844
1984/4849 [===========>..................] - ETA: 3:18 - loss: 0.6728 - acc: 0.5842
2048/4849 [===========>..................] - ETA: 3:14 - loss: 0.6732 - acc: 0.5820
2112/4849 [============>.................] - ETA: 3:09 - loss: 0.6718 - acc: 0.5833
2176/4849 [============>.................] - ETA: 3:05 - loss: 0.6703 - acc: 0.5846
2240/4849 [============>.................] - ETA: 3:00 - loss: 0.6717 - acc: 0.5813
2304/4849 [=============>................] - ETA: 2:55 - loss: 0.6738 - acc: 0.5760
2368/4849 [=============>................] - ETA: 2:51 - loss: 0.6740 - acc: 0.5777
2432/4849 [==============>...............] - ETA: 2:46 - loss: 0.6753 - acc: 0.5761
2496/4849 [==============>...............] - ETA: 2:41 - loss: 0.6749 - acc: 0.5761
2560/4849 [==============>...............] - ETA: 2:37 - loss: 0.6745 - acc: 0.5770
2624/4849 [===============>..............] - ETA: 2:32 - loss: 0.6751 - acc: 0.5770
2688/4849 [===============>..............] - ETA: 2:28 - loss: 0.6749 - acc: 0.5770
2752/4849 [================>.............] - ETA: 2:23 - loss: 0.6746 - acc: 0.5767
2816/4849 [================>.............] - ETA: 2:19 - loss: 0.6739 - acc: 0.5778
2880/4849 [================>.............] - ETA: 2:14 - loss: 0.6750 - acc: 0.5757
2944/4849 [=================>............] - ETA: 2:10 - loss: 0.6744 - acc: 0.5764
3008/4849 [=================>............] - ETA: 2:05 - loss: 0.6743 - acc: 0.5775
3072/4849 [==================>...........] - ETA: 2:01 - loss: 0.6748 - acc: 0.5749
3136/4849 [==================>...........] - ETA: 1:56 - loss: 0.6752 - acc: 0.5746
3200/4849 [==================>...........] - ETA: 1:51 - loss: 0.6752 - acc: 0.5737
3264/4849 [===================>..........] - ETA: 1:47 - loss: 0.6758 - acc: 0.5723
3328/4849 [===================>..........] - ETA: 1:43 - loss: 0.6762 - acc: 0.5721
3392/4849 [===================>..........] - ETA: 1:38 - loss: 0.6759 - acc: 0.5728
3456/4849 [====================>.........] - ETA: 1:34 - loss: 0.6758 - acc: 0.5729
3520/4849 [====================>.........] - ETA: 1:29 - loss: 0.6753 - acc: 0.5736
3584/4849 [=====================>........] - ETA: 1:25 - loss: 0.6752 - acc: 0.5739
3648/4849 [=====================>........] - ETA: 1:21 - loss: 0.6743 - acc: 0.5768
3712/4849 [=====================>........] - ETA: 1:16 - loss: 0.6747 - acc: 0.5760
3776/4849 [======================>.......] - ETA: 1:12 - loss: 0.6747 - acc: 0.5768
3840/4849 [======================>.......] - ETA: 1:08 - loss: 0.6754 - acc: 0.5758
3904/4849 [=======================>......] - ETA: 1:03 - loss: 0.6757 - acc: 0.5756
3968/4849 [=======================>......] - ETA: 59s - loss: 0.6762 - acc: 0.5743 
4032/4849 [=======================>......] - ETA: 55s - loss: 0.6755 - acc: 0.5766
4096/4849 [========================>.....] - ETA: 50s - loss: 0.6757 - acc: 0.5769
4160/4849 [========================>.....] - ETA: 46s - loss: 0.6759 - acc: 0.5764
4224/4849 [=========================>....] - ETA: 42s - loss: 0.6760 - acc: 0.5767
4288/4849 [=========================>....] - ETA: 37s - loss: 0.6755 - acc: 0.5788
4352/4849 [=========================>....] - ETA: 33s - loss: 0.6759 - acc: 0.5784
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6755 - acc: 0.5795
4480/4849 [==========================>...] - ETA: 24s - loss: 0.6753 - acc: 0.5792
4544/4849 [===========================>..] - ETA: 20s - loss: 0.6751 - acc: 0.5790
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6745 - acc: 0.5803
4672/4849 [===========================>..] - ETA: 11s - loss: 0.6735 - acc: 0.5815
4736/4849 [============================>.] - ETA: 7s - loss: 0.6732 - acc: 0.5823 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6729 - acc: 0.5825
4849/4849 [==============================] - 342s 70ms/step - loss: 0.6730 - acc: 0.5822 - val_loss: 0.7062 - val_acc: 0.5380

Epoch 00007: val_acc did not improve from 0.56772
Epoch 8/10

  64/4849 [..............................] - ETA: 5:10 - loss: 0.6926 - acc: 0.5625
 128/4849 [..............................] - ETA: 5:13 - loss: 0.7137 - acc: 0.4844
 192/4849 [>.............................] - ETA: 5:10 - loss: 0.7088 - acc: 0.5000
 256/4849 [>.............................] - ETA: 5:12 - loss: 0.7010 - acc: 0.5078
 320/4849 [>.............................] - ETA: 5:11 - loss: 0.6825 - acc: 0.5437
 384/4849 [=>............................] - ETA: 5:05 - loss: 0.6815 - acc: 0.5417
 448/4849 [=>............................] - ETA: 5:04 - loss: 0.6830 - acc: 0.5469
 512/4849 [==>...........................] - ETA: 4:55 - loss: 0.6818 - acc: 0.5566
 576/4849 [==>...........................] - ETA: 4:52 - loss: 0.6810 - acc: 0.5660
 640/4849 [==>...........................] - ETA: 4:48 - loss: 0.6803 - acc: 0.5687
 704/4849 [===>..........................] - ETA: 4:45 - loss: 0.6781 - acc: 0.5696
 768/4849 [===>..........................] - ETA: 4:42 - loss: 0.6794 - acc: 0.5729
 832/4849 [====>.........................] - ETA: 4:37 - loss: 0.6779 - acc: 0.5757
 896/4849 [====>.........................] - ETA: 4:33 - loss: 0.6746 - acc: 0.5804
 960/4849 [====>.........................] - ETA: 4:28 - loss: 0.6767 - acc: 0.5740
1024/4849 [=====>........................] - ETA: 4:24 - loss: 0.6772 - acc: 0.5752
1088/4849 [=====>........................] - ETA: 4:20 - loss: 0.6774 - acc: 0.5735
1152/4849 [======>.......................] - ETA: 4:15 - loss: 0.6790 - acc: 0.5720
1216/4849 [======>.......................] - ETA: 4:12 - loss: 0.6810 - acc: 0.5699
1280/4849 [======>.......................] - ETA: 4:06 - loss: 0.6811 - acc: 0.5719
1344/4849 [=======>......................] - ETA: 4:01 - loss: 0.6798 - acc: 0.5737
1408/4849 [=======>......................] - ETA: 3:56 - loss: 0.6791 - acc: 0.5732
1472/4849 [========>.....................] - ETA: 3:53 - loss: 0.6807 - acc: 0.5727
1536/4849 [========>.....................] - ETA: 3:49 - loss: 0.6795 - acc: 0.5762
1600/4849 [========>.....................] - ETA: 3:45 - loss: 0.6810 - acc: 0.5731
1664/4849 [=========>....................] - ETA: 3:40 - loss: 0.6791 - acc: 0.5787
1728/4849 [=========>....................] - ETA: 3:36 - loss: 0.6793 - acc: 0.5799
1792/4849 [==========>...................] - ETA: 3:32 - loss: 0.6788 - acc: 0.5798
1856/4849 [==========>...................] - ETA: 3:27 - loss: 0.6793 - acc: 0.5787
1920/4849 [==========>...................] - ETA: 3:22 - loss: 0.6793 - acc: 0.5771
1984/4849 [===========>..................] - ETA: 3:18 - loss: 0.6789 - acc: 0.5796
2048/4849 [===========>..................] - ETA: 3:13 - loss: 0.6785 - acc: 0.5806
2112/4849 [============>.................] - ETA: 3:09 - loss: 0.6772 - acc: 0.5852
2176/4849 [============>.................] - ETA: 3:04 - loss: 0.6777 - acc: 0.5836
2240/4849 [============>.................] - ETA: 2:59 - loss: 0.6775 - acc: 0.5835
2304/4849 [=============>................] - ETA: 2:55 - loss: 0.6768 - acc: 0.5842
2368/4849 [=============>................] - ETA: 2:51 - loss: 0.6767 - acc: 0.5861
2432/4849 [==============>...............] - ETA: 2:47 - loss: 0.6754 - acc: 0.5884
2496/4849 [==============>...............] - ETA: 2:42 - loss: 0.6743 - acc: 0.5901
2560/4849 [==============>...............] - ETA: 2:38 - loss: 0.6731 - acc: 0.5926
2624/4849 [===============>..............] - ETA: 2:34 - loss: 0.6729 - acc: 0.5941
2688/4849 [===============>..............] - ETA: 2:29 - loss: 0.6724 - acc: 0.5945
2752/4849 [================>.............] - ETA: 2:25 - loss: 0.6727 - acc: 0.5938
2816/4849 [================>.............] - ETA: 2:20 - loss: 0.6725 - acc: 0.5938
2880/4849 [================>.............] - ETA: 2:16 - loss: 0.6710 - acc: 0.5965
2944/4849 [=================>............] - ETA: 2:11 - loss: 0.6695 - acc: 0.5988
3008/4849 [=================>............] - ETA: 2:07 - loss: 0.6685 - acc: 0.5994
3072/4849 [==================>...........] - ETA: 2:03 - loss: 0.6683 - acc: 0.5990
3136/4849 [==================>...........] - ETA: 1:59 - loss: 0.6693 - acc: 0.5973
3200/4849 [==================>...........] - ETA: 1:54 - loss: 0.6697 - acc: 0.5966
3264/4849 [===================>..........] - ETA: 1:50 - loss: 0.6701 - acc: 0.5962
3328/4849 [===================>..........] - ETA: 1:45 - loss: 0.6695 - acc: 0.5968
3392/4849 [===================>..........] - ETA: 1:41 - loss: 0.6701 - acc: 0.5958
3456/4849 [====================>.........] - ETA: 1:36 - loss: 0.6706 - acc: 0.5955
3520/4849 [====================>.........] - ETA: 1:32 - loss: 0.6707 - acc: 0.5955
3584/4849 [=====================>........] - ETA: 1:27 - loss: 0.6707 - acc: 0.5954
3648/4849 [=====================>........] - ETA: 1:23 - loss: 0.6716 - acc: 0.5946
3712/4849 [=====================>........] - ETA: 1:18 - loss: 0.6715 - acc: 0.5948
3776/4849 [======================>.......] - ETA: 1:14 - loss: 0.6721 - acc: 0.5932
3840/4849 [======================>.......] - ETA: 1:09 - loss: 0.6727 - acc: 0.5919
3904/4849 [=======================>......] - ETA: 1:05 - loss: 0.6727 - acc: 0.5914
3968/4849 [=======================>......] - ETA: 1:01 - loss: 0.6725 - acc: 0.5920
4032/4849 [=======================>......] - ETA: 56s - loss: 0.6722 - acc: 0.5923 
4096/4849 [========================>.....] - ETA: 52s - loss: 0.6727 - acc: 0.5911
4160/4849 [========================>.....] - ETA: 47s - loss: 0.6725 - acc: 0.5918
4224/4849 [=========================>....] - ETA: 43s - loss: 0.6723 - acc: 0.5916
4288/4849 [=========================>....] - ETA: 38s - loss: 0.6727 - acc: 0.5910
4352/4849 [=========================>....] - ETA: 34s - loss: 0.6731 - acc: 0.5908
4416/4849 [==========================>...] - ETA: 29s - loss: 0.6739 - acc: 0.5901
4480/4849 [==========================>...] - ETA: 25s - loss: 0.6740 - acc: 0.5902
4544/4849 [===========================>..] - ETA: 21s - loss: 0.6742 - acc: 0.5907
4608/4849 [===========================>..] - ETA: 16s - loss: 0.6741 - acc: 0.5916
4672/4849 [===========================>..] - ETA: 12s - loss: 0.6740 - acc: 0.5916
4736/4849 [============================>.] - ETA: 7s - loss: 0.6748 - acc: 0.5899 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6752 - acc: 0.5887
4849/4849 [==============================] - 349s 72ms/step - loss: 0.6750 - acc: 0.5888 - val_loss: 0.6964 - val_acc: 0.5436

Epoch 00008: val_acc did not improve from 0.56772
Epoch 9/10

  64/4849 [..............................] - ETA: 5:23 - loss: 0.6638 - acc: 0.5938
 128/4849 [..............................] - ETA: 5:22 - loss: 0.6605 - acc: 0.6094
 192/4849 [>.............................] - ETA: 5:09 - loss: 0.6870 - acc: 0.5521
 256/4849 [>.............................] - ETA: 5:09 - loss: 0.6758 - acc: 0.5742
 320/4849 [>.............................] - ETA: 5:04 - loss: 0.6673 - acc: 0.5969
 384/4849 [=>............................] - ETA: 4:57 - loss: 0.6685 - acc: 0.5833
 448/4849 [=>............................] - ETA: 4:54 - loss: 0.6677 - acc: 0.5737
 512/4849 [==>...........................] - ETA: 4:47 - loss: 0.6724 - acc: 0.5684
 576/4849 [==>...........................] - ETA: 4:48 - loss: 0.6702 - acc: 0.5712
 640/4849 [==>...........................] - ETA: 4:49 - loss: 0.6716 - acc: 0.5719
 704/4849 [===>..........................] - ETA: 4:49 - loss: 0.6662 - acc: 0.5852
 768/4849 [===>..........................] - ETA: 4:46 - loss: 0.6680 - acc: 0.5807
 832/4849 [====>.........................] - ETA: 4:42 - loss: 0.6706 - acc: 0.5793
 896/4849 [====>.........................] - ETA: 4:40 - loss: 0.6734 - acc: 0.5759
 960/4849 [====>.........................] - ETA: 4:35 - loss: 0.6704 - acc: 0.5823
1024/4849 [=====>........................] - ETA: 4:33 - loss: 0.6721 - acc: 0.5791
1088/4849 [=====>........................] - ETA: 4:29 - loss: 0.6690 - acc: 0.5873
1152/4849 [======>.......................] - ETA: 4:27 - loss: 0.6703 - acc: 0.5842
1216/4849 [======>.......................] - ETA: 4:24 - loss: 0.6685 - acc: 0.5863
1280/4849 [======>.......................] - ETA: 4:21 - loss: 0.6692 - acc: 0.5852
1344/4849 [=======>......................] - ETA: 4:17 - loss: 0.6696 - acc: 0.5885
1408/4849 [=======>......................] - ETA: 4:12 - loss: 0.6687 - acc: 0.5888
1472/4849 [========>.....................] - ETA: 4:07 - loss: 0.6692 - acc: 0.5876
1536/4849 [========>.....................] - ETA: 4:03 - loss: 0.6695 - acc: 0.5885
1600/4849 [========>.....................] - ETA: 4:00 - loss: 0.6699 - acc: 0.5887
1664/4849 [=========>....................] - ETA: 3:54 - loss: 0.6709 - acc: 0.5883
1728/4849 [=========>....................] - ETA: 3:49 - loss: 0.6709 - acc: 0.5885
1792/4849 [==========>...................] - ETA: 3:43 - loss: 0.6727 - acc: 0.5859
1856/4849 [==========>...................] - ETA: 3:38 - loss: 0.6722 - acc: 0.5846
1920/4849 [==========>...................] - ETA: 3:33 - loss: 0.6723 - acc: 0.5859
1984/4849 [===========>..................] - ETA: 3:27 - loss: 0.6735 - acc: 0.5832
2048/4849 [===========>..................] - ETA: 3:22 - loss: 0.6740 - acc: 0.5815
2112/4849 [============>.................] - ETA: 3:17 - loss: 0.6752 - acc: 0.5814
2176/4849 [============>.................] - ETA: 3:12 - loss: 0.6747 - acc: 0.5827
2240/4849 [============>.................] - ETA: 3:07 - loss: 0.6744 - acc: 0.5813
2304/4849 [=============>................] - ETA: 3:03 - loss: 0.6741 - acc: 0.5820
2368/4849 [=============>................] - ETA: 2:58 - loss: 0.6737 - acc: 0.5832
2432/4849 [==============>...............] - ETA: 2:53 - loss: 0.6737 - acc: 0.5822
2496/4849 [==============>...............] - ETA: 2:49 - loss: 0.6728 - acc: 0.5849
2560/4849 [==============>...............] - ETA: 2:45 - loss: 0.6729 - acc: 0.5859
2624/4849 [===============>..............] - ETA: 2:41 - loss: 0.6720 - acc: 0.5877
2688/4849 [===============>..............] - ETA: 2:36 - loss: 0.6726 - acc: 0.5867
2752/4849 [================>.............] - ETA: 2:32 - loss: 0.6733 - acc: 0.5850
2816/4849 [================>.............] - ETA: 2:27 - loss: 0.6742 - acc: 0.5835
2880/4849 [================>.............] - ETA: 2:23 - loss: 0.6731 - acc: 0.5847
2944/4849 [=================>............] - ETA: 2:18 - loss: 0.6730 - acc: 0.5832
3008/4849 [=================>............] - ETA: 2:14 - loss: 0.6736 - acc: 0.5824
3072/4849 [==================>...........] - ETA: 2:10 - loss: 0.6746 - acc: 0.5804
3136/4849 [==================>...........] - ETA: 2:05 - loss: 0.6743 - acc: 0.5807
3200/4849 [==================>...........] - ETA: 2:01 - loss: 0.6740 - acc: 0.5819
3264/4849 [===================>..........] - ETA: 1:56 - loss: 0.6731 - acc: 0.5846
3328/4849 [===================>..........] - ETA: 1:52 - loss: 0.6733 - acc: 0.5847
3392/4849 [===================>..........] - ETA: 1:47 - loss: 0.6724 - acc: 0.5858
3456/4849 [====================>.........] - ETA: 1:42 - loss: 0.6726 - acc: 0.5854
3520/4849 [====================>.........] - ETA: 1:38 - loss: 0.6722 - acc: 0.5861
3584/4849 [=====================>........] - ETA: 1:33 - loss: 0.6728 - acc: 0.5843
3648/4849 [=====================>........] - ETA: 1:28 - loss: 0.6728 - acc: 0.5850
3712/4849 [=====================>........] - ETA: 1:23 - loss: 0.6729 - acc: 0.5857
3776/4849 [======================>.......] - ETA: 1:19 - loss: 0.6730 - acc: 0.5853
3840/4849 [======================>.......] - ETA: 1:14 - loss: 0.6731 - acc: 0.5844
3904/4849 [=======================>......] - ETA: 1:10 - loss: 0.6728 - acc: 0.5845
3968/4849 [=======================>......] - ETA: 1:05 - loss: 0.6729 - acc: 0.5852
4032/4849 [=======================>......] - ETA: 1:00 - loss: 0.6723 - acc: 0.5856
4096/4849 [========================>.....] - ETA: 56s - loss: 0.6725 - acc: 0.5857 
4160/4849 [========================>.....] - ETA: 51s - loss: 0.6720 - acc: 0.5851
4224/4849 [=========================>....] - ETA: 46s - loss: 0.6724 - acc: 0.5840
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6723 - acc: 0.5844
4352/4849 [=========================>....] - ETA: 36s - loss: 0.6724 - acc: 0.5839
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6722 - acc: 0.5851
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6722 - acc: 0.5857
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6726 - acc: 0.5856
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6729 - acc: 0.5849
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6725 - acc: 0.5854
4736/4849 [============================>.] - ETA: 8s - loss: 0.6722 - acc: 0.5861 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6720 - acc: 0.5863
4849/4849 [==============================] - 376s 78ms/step - loss: 0.6718 - acc: 0.5863 - val_loss: 0.6935 - val_acc: 0.5455

Epoch 00009: val_acc did not improve from 0.56772
Epoch 10/10

  64/4849 [..............................] - ETA: 5:12 - loss: 0.6948 - acc: 0.5312
 128/4849 [..............................] - ETA: 5:17 - loss: 0.7019 - acc: 0.5312
 192/4849 [>.............................] - ETA: 5:02 - loss: 0.6951 - acc: 0.5417
 256/4849 [>.............................] - ETA: 5:03 - loss: 0.6873 - acc: 0.5547
 320/4849 [>.............................] - ETA: 4:58 - loss: 0.6855 - acc: 0.5594
 384/4849 [=>............................] - ETA: 4:57 - loss: 0.6782 - acc: 0.5807
 448/4849 [=>............................] - ETA: 4:55 - loss: 0.6792 - acc: 0.5759
 512/4849 [==>...........................] - ETA: 4:46 - loss: 0.6821 - acc: 0.5625
 576/4849 [==>...........................] - ETA: 4:45 - loss: 0.6871 - acc: 0.5521
 640/4849 [==>...........................] - ETA: 4:42 - loss: 0.6893 - acc: 0.5453
 704/4849 [===>..........................] - ETA: 4:37 - loss: 0.6880 - acc: 0.5540
 768/4849 [===>..........................] - ETA: 4:35 - loss: 0.6826 - acc: 0.5638
 832/4849 [====>.........................] - ETA: 4:31 - loss: 0.6847 - acc: 0.5529
 896/4849 [====>.........................] - ETA: 4:28 - loss: 0.6818 - acc: 0.5592
 960/4849 [====>.........................] - ETA: 4:23 - loss: 0.6807 - acc: 0.5604
1024/4849 [=====>........................] - ETA: 4:19 - loss: 0.6803 - acc: 0.5625
1088/4849 [=====>........................] - ETA: 4:16 - loss: 0.6815 - acc: 0.5625
1152/4849 [======>.......................] - ETA: 4:11 - loss: 0.6797 - acc: 0.5660
1216/4849 [======>.......................] - ETA: 4:07 - loss: 0.6790 - acc: 0.5691
1280/4849 [======>.......................] - ETA: 4:05 - loss: 0.6779 - acc: 0.5719
1344/4849 [=======>......................] - ETA: 4:03 - loss: 0.6764 - acc: 0.5766
1408/4849 [=======>......................] - ETA: 3:58 - loss: 0.6763 - acc: 0.5774
1472/4849 [========>.....................] - ETA: 3:54 - loss: 0.6765 - acc: 0.5788
1536/4849 [========>.....................] - ETA: 3:49 - loss: 0.6759 - acc: 0.5814
1600/4849 [========>.....................] - ETA: 3:46 - loss: 0.6758 - acc: 0.5825
1664/4849 [=========>....................] - ETA: 3:43 - loss: 0.6789 - acc: 0.5793
1728/4849 [=========>....................] - ETA: 3:38 - loss: 0.6806 - acc: 0.5764
1792/4849 [==========>...................] - ETA: 3:36 - loss: 0.6816 - acc: 0.5759
1856/4849 [==========>...................] - ETA: 3:32 - loss: 0.6809 - acc: 0.5770
1920/4849 [==========>...................] - ETA: 3:28 - loss: 0.6822 - acc: 0.5740
1984/4849 [===========>..................] - ETA: 3:23 - loss: 0.6835 - acc: 0.5721
2048/4849 [===========>..................] - ETA: 3:19 - loss: 0.6848 - acc: 0.5693
2112/4849 [============>.................] - ETA: 3:15 - loss: 0.6833 - acc: 0.5705
2176/4849 [============>.................] - ETA: 3:11 - loss: 0.6829 - acc: 0.5717
2240/4849 [============>.................] - ETA: 3:06 - loss: 0.6828 - acc: 0.5710
2304/4849 [=============>................] - ETA: 3:01 - loss: 0.6839 - acc: 0.5694
2368/4849 [=============>................] - ETA: 2:57 - loss: 0.6835 - acc: 0.5705
2432/4849 [==============>...............] - ETA: 2:53 - loss: 0.6827 - acc: 0.5720
2496/4849 [==============>...............] - ETA: 2:49 - loss: 0.6824 - acc: 0.5717
2560/4849 [==============>...............] - ETA: 2:45 - loss: 0.6823 - acc: 0.5734
2624/4849 [===============>..............] - ETA: 2:41 - loss: 0.6820 - acc: 0.5736
2688/4849 [===============>..............] - ETA: 2:36 - loss: 0.6814 - acc: 0.5774
2752/4849 [================>.............] - ETA: 2:32 - loss: 0.6812 - acc: 0.5763
2816/4849 [================>.............] - ETA: 2:28 - loss: 0.6808 - acc: 0.5763
2880/4849 [================>.............] - ETA: 2:23 - loss: 0.6809 - acc: 0.5740
2944/4849 [=================>............] - ETA: 2:19 - loss: 0.6800 - acc: 0.5747
3008/4849 [=================>............] - ETA: 2:14 - loss: 0.6798 - acc: 0.5748
3072/4849 [==================>...........] - ETA: 2:10 - loss: 0.6793 - acc: 0.5758
3136/4849 [==================>...........] - ETA: 2:05 - loss: 0.6789 - acc: 0.5768
3200/4849 [==================>...........] - ETA: 2:01 - loss: 0.6789 - acc: 0.5772
3264/4849 [===================>..........] - ETA: 1:57 - loss: 0.6786 - acc: 0.5778
3328/4849 [===================>..........] - ETA: 1:52 - loss: 0.6781 - acc: 0.5781
3392/4849 [===================>..........] - ETA: 1:47 - loss: 0.6783 - acc: 0.5778
3456/4849 [====================>.........] - ETA: 1:42 - loss: 0.6782 - acc: 0.5790
3520/4849 [====================>.........] - ETA: 1:37 - loss: 0.6781 - acc: 0.5781
3584/4849 [=====================>........] - ETA: 1:33 - loss: 0.6779 - acc: 0.5795
3648/4849 [=====================>........] - ETA: 1:28 - loss: 0.6777 - acc: 0.5803
3712/4849 [=====================>........] - ETA: 1:23 - loss: 0.6770 - acc: 0.5827
3776/4849 [======================>.......] - ETA: 1:19 - loss: 0.6771 - acc: 0.5829
3840/4849 [======================>.......] - ETA: 1:14 - loss: 0.6770 - acc: 0.5839
3904/4849 [=======================>......] - ETA: 1:09 - loss: 0.6766 - acc: 0.5853
3968/4849 [=======================>......] - ETA: 1:05 - loss: 0.6762 - acc: 0.5854
4032/4849 [=======================>......] - ETA: 1:00 - loss: 0.6769 - acc: 0.5841
4096/4849 [========================>.....] - ETA: 55s - loss: 0.6771 - acc: 0.5830 
4160/4849 [========================>.....] - ETA: 51s - loss: 0.6763 - acc: 0.5846
4224/4849 [=========================>....] - ETA: 46s - loss: 0.6758 - acc: 0.5845
4288/4849 [=========================>....] - ETA: 41s - loss: 0.6749 - acc: 0.5858
4352/4849 [=========================>....] - ETA: 37s - loss: 0.6747 - acc: 0.5859
4416/4849 [==========================>...] - ETA: 32s - loss: 0.6742 - acc: 0.5863
4480/4849 [==========================>...] - ETA: 27s - loss: 0.6740 - acc: 0.5871
4544/4849 [===========================>..] - ETA: 22s - loss: 0.6733 - acc: 0.5882
4608/4849 [===========================>..] - ETA: 17s - loss: 0.6729 - acc: 0.5888
4672/4849 [===========================>..] - ETA: 13s - loss: 0.6729 - acc: 0.5886
4736/4849 [============================>.] - ETA: 8s - loss: 0.6725 - acc: 0.5889 
4800/4849 [============================>.] - ETA: 3s - loss: 0.6713 - acc: 0.5908
4849/4849 [==============================] - 372s 77ms/step - loss: 0.6713 - acc: 0.5910 - val_loss: 0.7055 - val_acc: 0.5584

Epoch 00010: val_acc did not improve from 0.56772
样本个数 674
样本个数 1348
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f72a8320c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f72a8320c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f747830c810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f747830c810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72a808a050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72a808a050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70f47a5a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f70f47a5a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72784e6a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72784e6a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72785ad410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72785ad410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70f47a5290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f70f47a5290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7278662050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7278662050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f725873db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f725873db90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7278492cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7278492cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7278553690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7278553690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f725873ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f725873ded0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f727824cd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f727824cd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72781e3890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72781e3890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72780f5610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72780f5610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fd4322bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fd4322bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72781e3810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72781e3810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72781a79d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72781a79d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f725870e6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f725870e6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72585183d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72585183d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f727812d950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f727812d950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72781950d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72781950d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7258411b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7258411b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f727842cad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f727842cad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7258553d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7258553d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7258433250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7258433250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f725855fe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f725855fe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f725829cc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f725829cc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7258277cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7258277cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72386d4ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72386d4ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72387cb0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72387cb0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7258301850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7258301850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb04df150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6fb04df150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72386e8c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72386e8c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7238379150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7238379150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72384ef350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72384ef350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72386e82d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72386e82d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f723869d0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f723869d0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72384707d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72384707d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f723807d890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f723807d890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72380a9910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72380a9910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72383420d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72383420d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72384a0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72384a0fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72381f1b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72381f1b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f723050e950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f723050e950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f723807d350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f723807d350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72306850d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72306850d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7230408e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7230408e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72305c17d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72305c17d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7230576bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7230576bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72303137d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72303137d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f723050e410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f723050e410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f723022ef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f723022ef50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72306536d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72306536d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72147c43d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72147c43d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f723026c110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f723026c110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7230297150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7230297150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72146e6150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72146e6150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f721458fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f721458fc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72144e67d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f72144e67d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7214486610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7214486610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72380f52d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f72380f52d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7214498c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7214498c90>>: AttributeError: module 'gast' has no attribute 'Str'
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1348 [>.............................] - ETA: 3:30
 128/1348 [=>............................] - ETA: 1:52
 192/1348 [===>..........................] - ETA: 1:19
 256/1348 [====>.........................] - ETA: 1:01
 320/1348 [======>.......................] - ETA: 50s 
 384/1348 [=======>......................] - ETA: 44s
 448/1348 [========>.....................] - ETA: 38s
 512/1348 [==========>...................] - ETA: 33s
 576/1348 [===========>..................] - ETA: 29s
 640/1348 [=============>................] - ETA: 26s
 704/1348 [==============>...............] - ETA: 23s
 768/1348 [================>.............] - ETA: 20s
 832/1348 [=================>............] - ETA: 17s
 896/1348 [==================>...........] - ETA: 15s
 960/1348 [====================>.........] - ETA: 12s
1024/1348 [=====================>........] - ETA: 10s
1088/1348 [=======================>......] - ETA: 8s 
1152/1348 [========================>.....] - ETA: 6s
1216/1348 [==========================>...] - ETA: 4s
1280/1348 [===========================>..] - ETA: 2s
1344/1348 [============================>.] - ETA: 0s
1348/1348 [==============================] - 41s 31ms/step
loss: 0.6725101446185692
acc: 0.5786350148367952
