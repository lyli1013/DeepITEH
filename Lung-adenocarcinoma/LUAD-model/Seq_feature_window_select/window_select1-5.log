nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 978
样本个数 1956
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa578329b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa578329b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa5a292df90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa5a292df90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5782da110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5782da110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa578296fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa578296fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa578151e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa578151e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa57819c2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa57819c2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa578296e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa578296e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa578191510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa578191510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa56ff54cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa56ff54cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa56fe6c910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa56fe6c910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56ff19e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56ff19e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa57819c750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa57819c750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56feb6710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56feb6710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa56fe662d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa56fe662d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa56fb0ef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa56fb0ef90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56fc86a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56fc86a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa57817fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa57817fe10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56fb4d290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56fb4d290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa56f977250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa56f977250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa56f87b490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa56f87b490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56fb53f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56fb53f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa57814f790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa57814f790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56f8ec490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56f8ec490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa56f919890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa56f919890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5675e3bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5675e3bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a2c77810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a2c77810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa56f919a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa56f919a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56761b0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa56761b0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa567337510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa567337510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5671e5f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5671e5f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa57828d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa57828d710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa567337f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa567337f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa567220950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa567220950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa566fc39d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa566fc39d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa566e85e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa566e85e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5670639d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5670639d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa566fc3f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa566fc3f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5671df1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5671df1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa566cdc1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa566cdc1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa566b99e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa566b99e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa566fe2950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa566fe2950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa566f50050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa566f50050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa566d264d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa566d264d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa566990110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa566990110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5668b99d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5668b99d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa566bc9650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa566bc9650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa566990210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa566990210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5668f4650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5668f4650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa556649610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa556649610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5668b2810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5668b2810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5667a4350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5667a4350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa556687090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa556687090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa556551590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa556551590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa566beba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa566beba50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5562b4310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5562b4310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5780588d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5780588d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5563653d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5563653d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa55623d250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa55623d250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa55603d810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa55603d810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa556072810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa556072810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa555fbabd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa555fbabd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa55612cd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa55612cd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa54df61c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa54df61c90>>: AttributeError: module 'gast' has no attribute 'Str'
2023-01-10 22:10:47.246817: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2023-01-10 22:10:47.327614: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2023-01-10 22:10:47.397445: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e9058d5290 executing computations on platform Host. Devices:
2023-01-10 22:10:47.397538: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2023-01-10 22:10:48.091358: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window_select1-5.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 3:35 - loss: 0.7433 - acc: 0.5781
 128/1760 [=>............................] - ETA: 2:25 - loss: 0.8139 - acc: 0.5000
 192/1760 [==>...........................] - ETA: 2:06 - loss: 0.7975 - acc: 0.4896
 256/1760 [===>..........................] - ETA: 1:49 - loss: 0.7637 - acc: 0.5312
 320/1760 [====>.........................] - ETA: 1:43 - loss: 0.7400 - acc: 0.5531
 384/1760 [=====>........................] - ETA: 1:34 - loss: 0.7265 - acc: 0.5625
 448/1760 [======>.......................] - ETA: 1:31 - loss: 0.7208 - acc: 0.5714
 512/1760 [=======>......................] - ETA: 1:25 - loss: 0.7402 - acc: 0.5566
 576/1760 [========>.....................] - ETA: 1:19 - loss: 0.7395 - acc: 0.5417
 640/1760 [=========>....................] - ETA: 1:13 - loss: 0.7424 - acc: 0.5359
 704/1760 [===========>..................] - ETA: 1:08 - loss: 0.7409 - acc: 0.5312
 768/1760 [============>.................] - ETA: 1:04 - loss: 0.7387 - acc: 0.5299
 832/1760 [=============>................] - ETA: 59s - loss: 0.7315 - acc: 0.5373 
 896/1760 [==============>...............] - ETA: 55s - loss: 0.7329 - acc: 0.5312
 960/1760 [===============>..............] - ETA: 50s - loss: 0.7287 - acc: 0.5365
1024/1760 [================>.............] - ETA: 45s - loss: 0.7269 - acc: 0.5371
1088/1760 [=================>............] - ETA: 41s - loss: 0.7238 - acc: 0.5414
1152/1760 [==================>...........] - ETA: 37s - loss: 0.7270 - acc: 0.5365
1216/1760 [===================>..........] - ETA: 33s - loss: 0.7254 - acc: 0.5370
1280/1760 [====================>.........] - ETA: 29s - loss: 0.7268 - acc: 0.5352
1344/1760 [=====================>........] - ETA: 25s - loss: 0.7249 - acc: 0.5387
1408/1760 [=======================>......] - ETA: 21s - loss: 0.7237 - acc: 0.5384
1472/1760 [========================>.....] - ETA: 17s - loss: 0.7238 - acc: 0.5387
1536/1760 [=========================>....] - ETA: 13s - loss: 0.7225 - acc: 0.5430
1600/1760 [==========================>...] - ETA: 9s - loss: 0.7205 - acc: 0.5431 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7201 - acc: 0.5415
1728/1760 [============================>.] - ETA: 1s - loss: 0.7226 - acc: 0.5353
1760/1760 [==============================] - 112s 63ms/step - loss: 0.7219 - acc: 0.5358 - val_loss: 0.6761 - val_acc: 0.5969

Epoch 00001: val_acc improved from -inf to 0.59694, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window01/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:38 - loss: 0.6490 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:35 - loss: 0.6790 - acc: 0.5547
 192/1760 [==>...........................] - ETA: 1:27 - loss: 0.6819 - acc: 0.5729
 256/1760 [===>..........................] - ETA: 1:21 - loss: 0.6850 - acc: 0.5547
 320/1760 [====>.........................] - ETA: 1:21 - loss: 0.6857 - acc: 0.5594
 384/1760 [=====>........................] - ETA: 1:15 - loss: 0.6880 - acc: 0.5625
 448/1760 [======>.......................] - ETA: 1:10 - loss: 0.6914 - acc: 0.5603
 512/1760 [=======>......................] - ETA: 1:06 - loss: 0.6861 - acc: 0.5703
 576/1760 [========>.....................] - ETA: 1:03 - loss: 0.6921 - acc: 0.5642
 640/1760 [=========>....................] - ETA: 59s - loss: 0.6888 - acc: 0.5656 
 704/1760 [===========>..................] - ETA: 56s - loss: 0.6867 - acc: 0.5724
 768/1760 [============>.................] - ETA: 52s - loss: 0.6893 - acc: 0.5677
 832/1760 [=============>................] - ETA: 48s - loss: 0.6841 - acc: 0.5733
 896/1760 [==============>...............] - ETA: 45s - loss: 0.6804 - acc: 0.5826
 960/1760 [===============>..............] - ETA: 42s - loss: 0.6806 - acc: 0.5813
1024/1760 [================>.............] - ETA: 38s - loss: 0.6809 - acc: 0.5840
1088/1760 [=================>............] - ETA: 34s - loss: 0.6810 - acc: 0.5846
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6821 - acc: 0.5816
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6862 - acc: 0.5765
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6864 - acc: 0.5742
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6853 - acc: 0.5744
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6852 - acc: 0.5717
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6867 - acc: 0.5720
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6836 - acc: 0.5755
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6832 - acc: 0.5750 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6822 - acc: 0.5781
1728/1760 [============================>.] - ETA: 1s - loss: 0.6800 - acc: 0.5833
1760/1760 [==============================] - 103s 58ms/step - loss: 0.6791 - acc: 0.5847 - val_loss: 0.6775 - val_acc: 0.5408

Epoch 00002: val_acc did not improve from 0.59694
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:42 - loss: 0.6772 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:41 - loss: 0.6531 - acc: 0.6094
 192/1760 [==>...........................] - ETA: 1:39 - loss: 0.6668 - acc: 0.5938
 256/1760 [===>..........................] - ETA: 1:35 - loss: 0.6668 - acc: 0.6055
 320/1760 [====>.........................] - ETA: 1:32 - loss: 0.6731 - acc: 0.5938
 384/1760 [=====>........................] - ETA: 1:30 - loss: 0.6802 - acc: 0.5833
 448/1760 [======>.......................] - ETA: 1:23 - loss: 0.6742 - acc: 0.5848
 512/1760 [=======>......................] - ETA: 1:24 - loss: 0.6803 - acc: 0.5879
 576/1760 [========>.....................] - ETA: 1:21 - loss: 0.6759 - acc: 0.5938
 640/1760 [=========>....................] - ETA: 1:17 - loss: 0.6785 - acc: 0.5875
 704/1760 [===========>..................] - ETA: 1:12 - loss: 0.6777 - acc: 0.5895
 768/1760 [============>.................] - ETA: 1:07 - loss: 0.6761 - acc: 0.5911
 832/1760 [=============>................] - ETA: 1:02 - loss: 0.6756 - acc: 0.5901
 896/1760 [==============>...............] - ETA: 58s - loss: 0.6732 - acc: 0.5938 
 960/1760 [===============>..............] - ETA: 54s - loss: 0.6789 - acc: 0.5885
1024/1760 [================>.............] - ETA: 49s - loss: 0.6772 - acc: 0.5879
1088/1760 [=================>............] - ETA: 44s - loss: 0.6722 - acc: 0.5956
1152/1760 [==================>...........] - ETA: 40s - loss: 0.6706 - acc: 0.5955
1216/1760 [===================>..........] - ETA: 36s - loss: 0.6769 - acc: 0.5872
1280/1760 [====================>.........] - ETA: 32s - loss: 0.6767 - acc: 0.5867
1344/1760 [=====================>........] - ETA: 28s - loss: 0.6756 - acc: 0.5900
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6748 - acc: 0.5902
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6761 - acc: 0.5910
1536/1760 [=========================>....] - ETA: 14s - loss: 0.6778 - acc: 0.5918
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6785 - acc: 0.5919
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6819 - acc: 0.5871 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6826 - acc: 0.5862
1760/1760 [==============================] - 123s 70ms/step - loss: 0.6798 - acc: 0.5903 - val_loss: 0.6670 - val_acc: 0.6276

Epoch 00003: val_acc improved from 0.59694 to 0.62755, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window01/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:39 - loss: 0.6100 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:48 - loss: 0.6538 - acc: 0.5938
 192/1760 [==>...........................] - ETA: 1:38 - loss: 0.6718 - acc: 0.5885
 256/1760 [===>..........................] - ETA: 1:32 - loss: 0.6972 - acc: 0.5664
 320/1760 [====>.........................] - ETA: 1:30 - loss: 0.7088 - acc: 0.5531
 384/1760 [=====>........................] - ETA: 1:28 - loss: 0.7029 - acc: 0.5547
 448/1760 [======>.......................] - ETA: 1:25 - loss: 0.6994 - acc: 0.5625
 512/1760 [=======>......................] - ETA: 1:20 - loss: 0.6925 - acc: 0.5742
 576/1760 [========>.....................] - ETA: 1:18 - loss: 0.6868 - acc: 0.5799
 640/1760 [=========>....................] - ETA: 1:14 - loss: 0.6967 - acc: 0.5703
 704/1760 [===========>..................] - ETA: 1:10 - loss: 0.7045 - acc: 0.5639
 768/1760 [============>.................] - ETA: 1:06 - loss: 0.7108 - acc: 0.5547
 832/1760 [=============>................] - ETA: 1:02 - loss: 0.7073 - acc: 0.5589
 896/1760 [==============>...............] - ETA: 58s - loss: 0.7046 - acc: 0.5592 
 960/1760 [===============>..............] - ETA: 54s - loss: 0.7001 - acc: 0.5646
1024/1760 [================>.............] - ETA: 50s - loss: 0.6996 - acc: 0.5654
1088/1760 [=================>............] - ETA: 45s - loss: 0.6967 - acc: 0.5726
1152/1760 [==================>...........] - ETA: 41s - loss: 0.6973 - acc: 0.5720
1216/1760 [===================>..........] - ETA: 36s - loss: 0.6958 - acc: 0.5715
1280/1760 [====================>.........] - ETA: 32s - loss: 0.6950 - acc: 0.5727
1344/1760 [=====================>........] - ETA: 28s - loss: 0.6962 - acc: 0.5729
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6961 - acc: 0.5710
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6958 - acc: 0.5747
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6972 - acc: 0.5729
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6927 - acc: 0.5775
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6916 - acc: 0.5769 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6895 - acc: 0.5775
1760/1760 [==============================] - 125s 71ms/step - loss: 0.6899 - acc: 0.5756 - val_loss: 0.6581 - val_acc: 0.5714

Epoch 00004: val_acc did not improve from 0.62755
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:44 - loss: 0.6588 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:39 - loss: 0.6812 - acc: 0.5703
 192/1760 [==>...........................] - ETA: 1:35 - loss: 0.6766 - acc: 0.5677
 256/1760 [===>..........................] - ETA: 1:34 - loss: 0.6729 - acc: 0.5742
 320/1760 [====>.........................] - ETA: 1:28 - loss: 0.6732 - acc: 0.5750
 384/1760 [=====>........................] - ETA: 1:24 - loss: 0.6665 - acc: 0.5807
 448/1760 [======>.......................] - ETA: 1:21 - loss: 0.6631 - acc: 0.5893
 512/1760 [=======>......................] - ETA: 1:16 - loss: 0.6577 - acc: 0.5977
 576/1760 [========>.....................] - ETA: 1:13 - loss: 0.6634 - acc: 0.5885
 640/1760 [=========>....................] - ETA: 1:10 - loss: 0.6583 - acc: 0.6000
 704/1760 [===========>..................] - ETA: 1:07 - loss: 0.6603 - acc: 0.5938
 768/1760 [============>.................] - ETA: 1:03 - loss: 0.6601 - acc: 0.5977
 832/1760 [=============>................] - ETA: 1:00 - loss: 0.6479 - acc: 0.6142
 896/1760 [==============>...............] - ETA: 55s - loss: 0.6479 - acc: 0.6116 
 960/1760 [===============>..............] - ETA: 51s - loss: 0.6494 - acc: 0.6104
1024/1760 [================>.............] - ETA: 47s - loss: 0.6523 - acc: 0.6104
1088/1760 [=================>............] - ETA: 43s - loss: 0.6532 - acc: 0.6094
1152/1760 [==================>...........] - ETA: 39s - loss: 0.6578 - acc: 0.6050
1216/1760 [===================>..........] - ETA: 35s - loss: 0.6621 - acc: 0.5995
1280/1760 [====================>.........] - ETA: 31s - loss: 0.6626 - acc: 0.5977
1344/1760 [=====================>........] - ETA: 27s - loss: 0.6624 - acc: 0.5997
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6630 - acc: 0.5980
1472/1760 [========================>.....] - ETA: 18s - loss: 0.6612 - acc: 0.6012
1536/1760 [=========================>....] - ETA: 14s - loss: 0.6611 - acc: 0.6022
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6618 - acc: 0.6025
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6626 - acc: 0.6028 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6637 - acc: 0.6013
1760/1760 [==============================] - 120s 68ms/step - loss: 0.6623 - acc: 0.6034 - val_loss: 0.6510 - val_acc: 0.6378

Epoch 00005: val_acc improved from 0.62755 to 0.63776, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window01/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:46 - loss: 0.7475 - acc: 0.5312
 128/1760 [=>............................] - ETA: 1:44 - loss: 0.7256 - acc: 0.5703
 192/1760 [==>...........................] - ETA: 1:40 - loss: 0.7366 - acc: 0.5469
 256/1760 [===>..........................] - ETA: 1:36 - loss: 0.7197 - acc: 0.5508
 320/1760 [====>.........................] - ETA: 1:30 - loss: 0.7001 - acc: 0.5719
 384/1760 [=====>........................] - ETA: 1:31 - loss: 0.6850 - acc: 0.5911
 448/1760 [======>.......................] - ETA: 1:26 - loss: 0.6732 - acc: 0.6004
 512/1760 [=======>......................] - ETA: 1:22 - loss: 0.6702 - acc: 0.5957
 576/1760 [========>.....................] - ETA: 1:18 - loss: 0.6746 - acc: 0.5799
 640/1760 [=========>....................] - ETA: 1:15 - loss: 0.6788 - acc: 0.5750
 704/1760 [===========>..................] - ETA: 1:10 - loss: 0.6751 - acc: 0.5795
 768/1760 [============>.................] - ETA: 1:06 - loss: 0.6786 - acc: 0.5755
 832/1760 [=============>................] - ETA: 1:02 - loss: 0.6757 - acc: 0.5817
 896/1760 [==============>...............] - ETA: 57s - loss: 0.6738 - acc: 0.5792 
 960/1760 [===============>..............] - ETA: 53s - loss: 0.6755 - acc: 0.5792
1024/1760 [================>.............] - ETA: 49s - loss: 0.6730 - acc: 0.5830
1088/1760 [=================>............] - ETA: 45s - loss: 0.6750 - acc: 0.5818
1152/1760 [==================>...........] - ETA: 40s - loss: 0.6716 - acc: 0.5911
1216/1760 [===================>..........] - ETA: 36s - loss: 0.6732 - acc: 0.5880
1280/1760 [====================>.........] - ETA: 32s - loss: 0.6720 - acc: 0.5898
1344/1760 [=====================>........] - ETA: 28s - loss: 0.6711 - acc: 0.5930
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6727 - acc: 0.5895
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6699 - acc: 0.5958
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6697 - acc: 0.5964
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6716 - acc: 0.5938
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6694 - acc: 0.5980 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6716 - acc: 0.5978
1760/1760 [==============================] - 126s 72ms/step - loss: 0.6734 - acc: 0.5943 - val_loss: 0.6401 - val_acc: 0.6378

Epoch 00006: val_acc did not improve from 0.63776
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:40 - loss: 0.6497 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:33 - loss: 0.6654 - acc: 0.6250
 192/1760 [==>...........................] - ETA: 1:33 - loss: 0.6473 - acc: 0.6406
 256/1760 [===>..........................] - ETA: 1:31 - loss: 0.6267 - acc: 0.6680
 320/1760 [====>.........................] - ETA: 1:29 - loss: 0.6285 - acc: 0.6656
 384/1760 [=====>........................] - ETA: 1:24 - loss: 0.6436 - acc: 0.6458
 448/1760 [======>.......................] - ETA: 1:22 - loss: 0.6403 - acc: 0.6518
 512/1760 [=======>......................] - ETA: 1:18 - loss: 0.6445 - acc: 0.6387
 576/1760 [========>.....................] - ETA: 1:14 - loss: 0.6476 - acc: 0.6354
 640/1760 [=========>....................] - ETA: 1:09 - loss: 0.6410 - acc: 0.6453
 704/1760 [===========>..................] - ETA: 1:05 - loss: 0.6458 - acc: 0.6392
 768/1760 [============>.................] - ETA: 1:02 - loss: 0.6472 - acc: 0.6341
 832/1760 [=============>................] - ETA: 58s - loss: 0.6414 - acc: 0.6370 
 896/1760 [==============>...............] - ETA: 54s - loss: 0.6424 - acc: 0.6350
 960/1760 [===============>..............] - ETA: 50s - loss: 0.6419 - acc: 0.6344
1024/1760 [================>.............] - ETA: 46s - loss: 0.6432 - acc: 0.6279
1088/1760 [=================>............] - ETA: 42s - loss: 0.6430 - acc: 0.6278
1152/1760 [==================>...........] - ETA: 38s - loss: 0.6439 - acc: 0.6267
1216/1760 [===================>..........] - ETA: 35s - loss: 0.6427 - acc: 0.6250
1280/1760 [====================>.........] - ETA: 30s - loss: 0.6447 - acc: 0.6258
1344/1760 [=====================>........] - ETA: 26s - loss: 0.6443 - acc: 0.6257
1408/1760 [=======================>......] - ETA: 22s - loss: 0.6460 - acc: 0.6200
1472/1760 [========================>.....] - ETA: 18s - loss: 0.6497 - acc: 0.6155
1536/1760 [=========================>....] - ETA: 14s - loss: 0.6500 - acc: 0.6159
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6518 - acc: 0.6144
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6482 - acc: 0.6184 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6499 - acc: 0.6152
1760/1760 [==============================] - 119s 67ms/step - loss: 0.6491 - acc: 0.6159 - val_loss: 0.6490 - val_acc: 0.6020

Epoch 00007: val_acc did not improve from 0.63776
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:46 - loss: 0.6887 - acc: 0.5312
 128/1760 [=>............................] - ETA: 1:37 - loss: 0.6605 - acc: 0.5859
 192/1760 [==>...........................] - ETA: 1:34 - loss: 0.6455 - acc: 0.5938
 256/1760 [===>..........................] - ETA: 1:34 - loss: 0.6474 - acc: 0.5977
 320/1760 [====>.........................] - ETA: 1:31 - loss: 0.6434 - acc: 0.6125
 384/1760 [=====>........................] - ETA: 1:25 - loss: 0.6349 - acc: 0.6172
 448/1760 [======>.......................] - ETA: 1:21 - loss: 0.6439 - acc: 0.6138
 512/1760 [=======>......................] - ETA: 1:18 - loss: 0.6498 - acc: 0.6133
 576/1760 [========>.....................] - ETA: 1:14 - loss: 0.6520 - acc: 0.6094
 640/1760 [=========>....................] - ETA: 1:11 - loss: 0.6550 - acc: 0.6047
 704/1760 [===========>..................] - ETA: 1:07 - loss: 0.6526 - acc: 0.6094
 768/1760 [============>.................] - ETA: 1:02 - loss: 0.6493 - acc: 0.6094
 832/1760 [=============>................] - ETA: 59s - loss: 0.6505 - acc: 0.6094 
 896/1760 [==============>...............] - ETA: 56s - loss: 0.6524 - acc: 0.6060
 960/1760 [===============>..............] - ETA: 51s - loss: 0.6518 - acc: 0.6094
1024/1760 [================>.............] - ETA: 47s - loss: 0.6469 - acc: 0.6133
1088/1760 [=================>............] - ETA: 43s - loss: 0.6469 - acc: 0.6112
1152/1760 [==================>...........] - ETA: 39s - loss: 0.6467 - acc: 0.6102
1216/1760 [===================>..........] - ETA: 34s - loss: 0.6467 - acc: 0.6094
1280/1760 [====================>.........] - ETA: 30s - loss: 0.6409 - acc: 0.6148
1344/1760 [=====================>........] - ETA: 26s - loss: 0.6436 - acc: 0.6116
1408/1760 [=======================>......] - ETA: 22s - loss: 0.6436 - acc: 0.6101
1472/1760 [========================>.....] - ETA: 18s - loss: 0.6463 - acc: 0.6094
1536/1760 [=========================>....] - ETA: 14s - loss: 0.6460 - acc: 0.6113
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6446 - acc: 0.6112
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6426 - acc: 0.6130 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6421 - acc: 0.6157
1760/1760 [==============================] - 118s 67ms/step - loss: 0.6411 - acc: 0.6165 - val_loss: 0.6487 - val_acc: 0.6327

Epoch 00008: val_acc did not improve from 0.63776
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:29 - loss: 0.5478 - acc: 0.6875
 128/1760 [=>............................] - ETA: 1:32 - loss: 0.6103 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 1:26 - loss: 0.6089 - acc: 0.6562
 256/1760 [===>..........................] - ETA: 1:27 - loss: 0.6103 - acc: 0.6523
 320/1760 [====>.........................] - ETA: 1:26 - loss: 0.6180 - acc: 0.6500
 384/1760 [=====>........................] - ETA: 1:24 - loss: 0.6266 - acc: 0.6354
 448/1760 [======>.......................] - ETA: 1:20 - loss: 0.6326 - acc: 0.6295
 512/1760 [=======>......................] - ETA: 1:16 - loss: 0.6294 - acc: 0.6250
 576/1760 [========>.....................] - ETA: 1:13 - loss: 0.6357 - acc: 0.6198
 640/1760 [=========>....................] - ETA: 1:08 - loss: 0.6338 - acc: 0.6281
 704/1760 [===========>..................] - ETA: 1:04 - loss: 0.6366 - acc: 0.6264
 768/1760 [============>.................] - ETA: 1:00 - loss: 0.6297 - acc: 0.6380
 832/1760 [=============>................] - ETA: 56s - loss: 0.6301 - acc: 0.6358 
 896/1760 [==============>...............] - ETA: 52s - loss: 0.6286 - acc: 0.6362
 960/1760 [===============>..............] - ETA: 49s - loss: 0.6304 - acc: 0.6302
1024/1760 [================>.............] - ETA: 44s - loss: 0.6292 - acc: 0.6328
1088/1760 [=================>............] - ETA: 41s - loss: 0.6310 - acc: 0.6324
1152/1760 [==================>...........] - ETA: 37s - loss: 0.6293 - acc: 0.6415
1216/1760 [===================>..........] - ETA: 33s - loss: 0.6264 - acc: 0.6431
1280/1760 [====================>.........] - ETA: 29s - loss: 0.6250 - acc: 0.6469
1344/1760 [=====================>........] - ETA: 25s - loss: 0.6252 - acc: 0.6443
1408/1760 [=======================>......] - ETA: 21s - loss: 0.6265 - acc: 0.6428
1472/1760 [========================>.....] - ETA: 17s - loss: 0.6270 - acc: 0.6427
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6284 - acc: 0.6419
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6278 - acc: 0.6419 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6311 - acc: 0.6382
1728/1760 [============================>.] - ETA: 1s - loss: 0.6311 - acc: 0.6395
1760/1760 [==============================] - 108s 61ms/step - loss: 0.6307 - acc: 0.6420 - val_loss: 0.6767 - val_acc: 0.6020

Epoch 00009: val_acc did not improve from 0.63776
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:41 - loss: 0.6266 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:26 - loss: 0.6248 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 1:21 - loss: 0.6310 - acc: 0.6406
 256/1760 [===>..........................] - ETA: 1:18 - loss: 0.6395 - acc: 0.6367
 320/1760 [====>.........................] - ETA: 1:16 - loss: 0.6602 - acc: 0.5969
 384/1760 [=====>........................] - ETA: 1:12 - loss: 0.6546 - acc: 0.6094
 448/1760 [======>.......................] - ETA: 1:08 - loss: 0.6497 - acc: 0.6250
 512/1760 [=======>......................] - ETA: 1:05 - loss: 0.6443 - acc: 0.6309
 576/1760 [========>.....................] - ETA: 1:02 - loss: 0.6424 - acc: 0.6319
 640/1760 [=========>....................] - ETA: 58s - loss: 0.6398 - acc: 0.6375 
 704/1760 [===========>..................] - ETA: 55s - loss: 0.6335 - acc: 0.6463
 768/1760 [============>.................] - ETA: 51s - loss: 0.6288 - acc: 0.6536
 832/1760 [=============>................] - ETA: 48s - loss: 0.6324 - acc: 0.6502
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6352 - acc: 0.6462
 960/1760 [===============>..............] - ETA: 42s - loss: 0.6306 - acc: 0.6510
1024/1760 [================>.............] - ETA: 39s - loss: 0.6325 - acc: 0.6484
1088/1760 [=================>............] - ETA: 36s - loss: 0.6310 - acc: 0.6498
1152/1760 [==================>...........] - ETA: 32s - loss: 0.6288 - acc: 0.6493
1216/1760 [===================>..........] - ETA: 29s - loss: 0.6269 - acc: 0.6488
1280/1760 [====================>.........] - ETA: 26s - loss: 0.6283 - acc: 0.6461
1344/1760 [=====================>........] - ETA: 23s - loss: 0.6301 - acc: 0.6451
1408/1760 [=======================>......] - ETA: 19s - loss: 0.6316 - acc: 0.6463
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6298 - acc: 0.6508
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6292 - acc: 0.6510
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6298 - acc: 0.6500 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6298 - acc: 0.6490
1728/1760 [============================>.] - ETA: 1s - loss: 0.6345 - acc: 0.6424
1760/1760 [==============================] - 109s 62ms/step - loss: 0.6364 - acc: 0.6409 - val_loss: 0.6279 - val_acc: 0.6531

Epoch 00010: val_acc improved from 0.63776 to 0.65306, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window01/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
样本个数 200
样本个数 400
window_select1-5.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 11s
128/400 [========>.....................] - ETA: 7s 
192/400 [=============>................] - ETA: 5s
256/400 [==================>...........] - ETA: 3s
320/400 [=======================>......] - ETA: 1s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 9s 22ms/step
loss: 0.6576709282398224
acc: 0.6
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9f3c1dcad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9f3c1dcad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9f3c193150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9f3c193150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591df6710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591df6710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa591df6a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa591df6a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa591ee7c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa591ee7c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591ea56d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591ea56d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa591df6590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa591df6590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a28b1490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a28b1490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f34764750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f34764750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3475c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3475c710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f34700b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f34700b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f347648d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f347648d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c193a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c193a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f3465e150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f3465e150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f34389090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f34389090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ee40c5ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ee40c5ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f346f1810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f346f1810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ee40e7f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ee40e7f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f34418790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f34418790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9edc665710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9edc665710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc54b5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc54b5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9edc732890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9edc732890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc7d4f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc7d4f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ee403ba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ee403ba90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9edc3b1790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9edc3b1790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc26d1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc26d1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9edc703610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9edc703610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc4f3890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc4f3890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9edc176490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9edc176490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9edc04c890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9edc04c890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc3b7bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc3b7bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9edc176e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9edc176e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc05cb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc05cb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ea862ed90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ea862ed90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ea861b990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ea861b990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc150150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9edc150150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ea86725d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ea86725d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ea83a1d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ea83a1d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ea8387610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ea8387610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ea81cac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ea81cac90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f346f1410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f346f1410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ea8387650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ea8387650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ea80f35d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ea80f35d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e847930d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e847930d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e846b6850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e846b6850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ea81d9b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ea81d9b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e84793e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e84793e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e845fef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e845fef50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e84466510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e84466510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e84307a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e84307a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e84480ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e84480ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e844664d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e844664d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e843e4550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e843e4550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e84340590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e84340590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e84109450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e84109450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e6c6e94d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e6c6e94d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e8420b490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e8420b490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e8406a9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e8406a9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e6c5d37d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e6c5d37d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e6c5582d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e6c5582d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e8405f350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e8405f350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e84153ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e84153ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e6c558a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e6c558a50>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 4:32 - loss: 0.6870 - acc: 0.4688
 128/1760 [=>............................] - ETA: 2:58 - loss: 0.6811 - acc: 0.5156
 192/1760 [==>...........................] - ETA: 2:30 - loss: 0.7048 - acc: 0.5104
 256/1760 [===>..........................] - ETA: 2:14 - loss: 0.7144 - acc: 0.5039
 320/1760 [====>.........................] - ETA: 2:02 - loss: 0.7147 - acc: 0.5125
 384/1760 [=====>........................] - ETA: 1:52 - loss: 0.7262 - acc: 0.5104
 448/1760 [======>.......................] - ETA: 1:43 - loss: 0.7311 - acc: 0.4978
 512/1760 [=======>......................] - ETA: 1:36 - loss: 0.7297 - acc: 0.5059
 576/1760 [========>.....................] - ETA: 1:31 - loss: 0.7234 - acc: 0.5191
 640/1760 [=========>....................] - ETA: 1:24 - loss: 0.7207 - acc: 0.5250
 704/1760 [===========>..................] - ETA: 1:18 - loss: 0.7196 - acc: 0.5298
 768/1760 [============>.................] - ETA: 1:13 - loss: 0.7211 - acc: 0.5299
 832/1760 [=============>................] - ETA: 1:07 - loss: 0.7273 - acc: 0.5240
 896/1760 [==============>...............] - ETA: 1:02 - loss: 0.7282 - acc: 0.5234
 960/1760 [===============>..............] - ETA: 57s - loss: 0.7326 - acc: 0.5198 
1024/1760 [================>.............] - ETA: 52s - loss: 0.7357 - acc: 0.5107
1088/1760 [=================>............] - ETA: 47s - loss: 0.7341 - acc: 0.5119
1152/1760 [==================>...........] - ETA: 42s - loss: 0.7333 - acc: 0.5113
1216/1760 [===================>..........] - ETA: 38s - loss: 0.7329 - acc: 0.5099
1280/1760 [====================>.........] - ETA: 33s - loss: 0.7325 - acc: 0.5086
1344/1760 [=====================>........] - ETA: 28s - loss: 0.7326 - acc: 0.5067
1408/1760 [=======================>......] - ETA: 24s - loss: 0.7312 - acc: 0.5050
1472/1760 [========================>.....] - ETA: 19s - loss: 0.7279 - acc: 0.5095
1536/1760 [=========================>....] - ETA: 15s - loss: 0.7277 - acc: 0.5078
1600/1760 [==========================>...] - ETA: 11s - loss: 0.7245 - acc: 0.5119
1664/1760 [===========================>..] - ETA: 6s - loss: 0.7211 - acc: 0.5180 
1728/1760 [============================>.] - ETA: 2s - loss: 0.7222 - acc: 0.5162
1760/1760 [==============================] - 125s 71ms/step - loss: 0.7219 - acc: 0.5148 - val_loss: 0.6655 - val_acc: 0.6429

Epoch 00001: val_acc improved from -inf to 0.64286, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window02/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:41 - loss: 0.6483 - acc: 0.6562
 128/1760 [=>............................] - ETA: 1:19 - loss: 0.6722 - acc: 0.5781
 192/1760 [==>...........................] - ETA: 1:12 - loss: 0.6712 - acc: 0.5938
 256/1760 [===>..........................] - ETA: 1:09 - loss: 0.6726 - acc: 0.5820
 320/1760 [====>.........................] - ETA: 1:06 - loss: 0.6767 - acc: 0.5750
 384/1760 [=====>........................] - ETA: 1:03 - loss: 0.6788 - acc: 0.5703
 448/1760 [======>.......................] - ETA: 1:01 - loss: 0.6903 - acc: 0.5580
 512/1760 [=======>......................] - ETA: 59s - loss: 0.6919 - acc: 0.5566 
 576/1760 [========>.....................] - ETA: 56s - loss: 0.6930 - acc: 0.5503
 640/1760 [=========>....................] - ETA: 53s - loss: 0.7000 - acc: 0.5406
 704/1760 [===========>..................] - ETA: 50s - loss: 0.6992 - acc: 0.5440
 768/1760 [============>.................] - ETA: 47s - loss: 0.7004 - acc: 0.5417
 832/1760 [=============>................] - ETA: 44s - loss: 0.7024 - acc: 0.5337
 896/1760 [==============>...............] - ETA: 41s - loss: 0.7007 - acc: 0.5391
 960/1760 [===============>..............] - ETA: 38s - loss: 0.7074 - acc: 0.5281
1024/1760 [================>.............] - ETA: 35s - loss: 0.7071 - acc: 0.5283
1088/1760 [=================>............] - ETA: 32s - loss: 0.7085 - acc: 0.5267
1152/1760 [==================>...........] - ETA: 29s - loss: 0.7098 - acc: 0.5269
1216/1760 [===================>..........] - ETA: 26s - loss: 0.7086 - acc: 0.5288
1280/1760 [====================>.........] - ETA: 23s - loss: 0.7084 - acc: 0.5297
1344/1760 [=====================>........] - ETA: 20s - loss: 0.7076 - acc: 0.5312
1408/1760 [=======================>......] - ETA: 17s - loss: 0.7099 - acc: 0.5305
1472/1760 [========================>.....] - ETA: 13s - loss: 0.7088 - acc: 0.5326
1536/1760 [=========================>....] - ETA: 10s - loss: 0.7066 - acc: 0.5365
1600/1760 [==========================>...] - ETA: 7s - loss: 0.7057 - acc: 0.5406 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.7041 - acc: 0.5427
1728/1760 [============================>.] - ETA: 1s - loss: 0.7038 - acc: 0.5417
1760/1760 [==============================] - 90s 51ms/step - loss: 0.7034 - acc: 0.5432 - val_loss: 0.6617 - val_acc: 0.6173

Epoch 00002: val_acc did not improve from 0.64286
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:24 - loss: 0.7213 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:26 - loss: 0.6975 - acc: 0.5781
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.6793 - acc: 0.5938
 256/1760 [===>..........................] - ETA: 1:14 - loss: 0.6947 - acc: 0.5586
 320/1760 [====>.........................] - ETA: 1:10 - loss: 0.6859 - acc: 0.5781
 384/1760 [=====>........................] - ETA: 1:05 - loss: 0.6929 - acc: 0.5729
 448/1760 [======>.......................] - ETA: 1:03 - loss: 0.6932 - acc: 0.5714
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6897 - acc: 0.5723
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6973 - acc: 0.5556 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6938 - acc: 0.5547
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6936 - acc: 0.5511
 768/1760 [============>.................] - ETA: 50s - loss: 0.6929 - acc: 0.5547
 832/1760 [=============>................] - ETA: 47s - loss: 0.6907 - acc: 0.5577
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6916 - acc: 0.5536
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6926 - acc: 0.5552
1024/1760 [================>.............] - ETA: 37s - loss: 0.6897 - acc: 0.5615
1088/1760 [=================>............] - ETA: 34s - loss: 0.6909 - acc: 0.5616
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6898 - acc: 0.5599
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6885 - acc: 0.5617
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6904 - acc: 0.5617
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6891 - acc: 0.5618
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6894 - acc: 0.5618
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6876 - acc: 0.5618
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6871 - acc: 0.5612
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6882 - acc: 0.5619 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6879 - acc: 0.5637
1728/1760 [============================>.] - ETA: 1s - loss: 0.6869 - acc: 0.5660
1760/1760 [==============================] - 95s 54ms/step - loss: 0.6876 - acc: 0.5648 - val_loss: 0.6555 - val_acc: 0.5765

Epoch 00003: val_acc did not improve from 0.64286
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:32 - loss: 0.6926 - acc: 0.5469
 128/1760 [=>............................] - ETA: 1:30 - loss: 0.7010 - acc: 0.5547
 192/1760 [==>...........................] - ETA: 1:25 - loss: 0.6995 - acc: 0.5729
 256/1760 [===>..........................] - ETA: 1:18 - loss: 0.6918 - acc: 0.5742
 320/1760 [====>.........................] - ETA: 1:12 - loss: 0.6958 - acc: 0.5687
 384/1760 [=====>........................] - ETA: 1:09 - loss: 0.6974 - acc: 0.5729
 448/1760 [======>.......................] - ETA: 1:07 - loss: 0.7031 - acc: 0.5692
 512/1760 [=======>......................] - ETA: 1:03 - loss: 0.6984 - acc: 0.5703
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6960 - acc: 0.5660
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6922 - acc: 0.5734 
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6904 - acc: 0.5753
 768/1760 [============>.................] - ETA: 50s - loss: 0.6915 - acc: 0.5755
 832/1760 [=============>................] - ETA: 46s - loss: 0.6910 - acc: 0.5757
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6889 - acc: 0.5826
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6862 - acc: 0.5854
1024/1760 [================>.............] - ETA: 37s - loss: 0.6849 - acc: 0.5889
1088/1760 [=================>............] - ETA: 34s - loss: 0.6822 - acc: 0.5919
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6816 - acc: 0.5894
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6833 - acc: 0.5822
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6831 - acc: 0.5836
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6791 - acc: 0.5915
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6784 - acc: 0.5916
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6788 - acc: 0.5897
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6795 - acc: 0.5892
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6791 - acc: 0.5856 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6797 - acc: 0.5853
1728/1760 [============================>.] - ETA: 1s - loss: 0.6788 - acc: 0.5885
1760/1760 [==============================] - 95s 54ms/step - loss: 0.6776 - acc: 0.5915 - val_loss: 0.6236 - val_acc: 0.6071

Epoch 00004: val_acc did not improve from 0.64286
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:17 - loss: 0.6540 - acc: 0.5781
 128/1760 [=>............................] - ETA: 1:16 - loss: 0.6644 - acc: 0.5781
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.6543 - acc: 0.5885
 256/1760 [===>..........................] - ETA: 1:13 - loss: 0.6487 - acc: 0.6016
 320/1760 [====>.........................] - ETA: 1:10 - loss: 0.6559 - acc: 0.6031
 384/1760 [=====>........................] - ETA: 1:09 - loss: 0.6546 - acc: 0.5938
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6630 - acc: 0.5893
 512/1760 [=======>......................] - ETA: 1:03 - loss: 0.6647 - acc: 0.5820
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6592 - acc: 0.5868 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6602 - acc: 0.5844
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6599 - acc: 0.5866
 768/1760 [============>.................] - ETA: 48s - loss: 0.6622 - acc: 0.5859
 832/1760 [=============>................] - ETA: 45s - loss: 0.6610 - acc: 0.5913
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6627 - acc: 0.5904
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6615 - acc: 0.5917
1024/1760 [================>.............] - ETA: 36s - loss: 0.6621 - acc: 0.5918
1088/1760 [=================>............] - ETA: 33s - loss: 0.6652 - acc: 0.5938
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6629 - acc: 0.5972
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6630 - acc: 0.6003
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6633 - acc: 0.5969
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6634 - acc: 0.5975
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6631 - acc: 0.5966
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6616 - acc: 0.5999
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6601 - acc: 0.6029
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6590 - acc: 0.6044 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6600 - acc: 0.6052
1728/1760 [============================>.] - ETA: 1s - loss: 0.6597 - acc: 0.6071
1760/1760 [==============================] - 92s 52ms/step - loss: 0.6614 - acc: 0.6068 - val_loss: 0.6024 - val_acc: 0.6276

Epoch 00005: val_acc did not improve from 0.64286
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:14 - loss: 0.6060 - acc: 0.7188
 128/1760 [=>............................] - ETA: 1:13 - loss: 0.6210 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 1:11 - loss: 0.6300 - acc: 0.6667
 256/1760 [===>..........................] - ETA: 1:09 - loss: 0.6272 - acc: 0.6758
 320/1760 [====>.........................] - ETA: 1:08 - loss: 0.6466 - acc: 0.6406
 384/1760 [=====>........................] - ETA: 1:07 - loss: 0.6558 - acc: 0.6198
 448/1760 [======>.......................] - ETA: 1:03 - loss: 0.6578 - acc: 0.6138
 512/1760 [=======>......................] - ETA: 59s - loss: 0.6608 - acc: 0.6094 
 576/1760 [========>.....................] - ETA: 56s - loss: 0.6480 - acc: 0.6285
 640/1760 [=========>....................] - ETA: 54s - loss: 0.6533 - acc: 0.6281
 704/1760 [===========>..................] - ETA: 51s - loss: 0.6508 - acc: 0.6278
 768/1760 [============>.................] - ETA: 47s - loss: 0.6514 - acc: 0.6198
 832/1760 [=============>................] - ETA: 45s - loss: 0.6482 - acc: 0.6226
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6471 - acc: 0.6228
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6476 - acc: 0.6198
1024/1760 [================>.............] - ETA: 36s - loss: 0.6469 - acc: 0.6211
1088/1760 [=================>............] - ETA: 33s - loss: 0.6513 - acc: 0.6176
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6493 - acc: 0.6207
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6489 - acc: 0.6192
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6489 - acc: 0.6195
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6510 - acc: 0.6161
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6539 - acc: 0.6115
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6558 - acc: 0.6073
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6541 - acc: 0.6113
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6538 - acc: 0.6119 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6552 - acc: 0.6118
1728/1760 [============================>.] - ETA: 1s - loss: 0.6577 - acc: 0.6059
1760/1760 [==============================] - 93s 53ms/step - loss: 0.6588 - acc: 0.6063 - val_loss: 0.6138 - val_acc: 0.6429

Epoch 00006: val_acc did not improve from 0.64286
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:24 - loss: 0.6399 - acc: 0.6562
 128/1760 [=>............................] - ETA: 1:16 - loss: 0.6344 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 1:13 - loss: 0.6477 - acc: 0.6458
 256/1760 [===>..........................] - ETA: 1:11 - loss: 0.6618 - acc: 0.6250
 320/1760 [====>.........................] - ETA: 1:09 - loss: 0.6535 - acc: 0.6312
 384/1760 [=====>........................] - ETA: 1:07 - loss: 0.6535 - acc: 0.6146
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6483 - acc: 0.6295
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6522 - acc: 0.6172
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6497 - acc: 0.6250 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6434 - acc: 0.6344
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6353 - acc: 0.6406
 768/1760 [============>.................] - ETA: 48s - loss: 0.6293 - acc: 0.6458
 832/1760 [=============>................] - ETA: 45s - loss: 0.6258 - acc: 0.6454
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6294 - acc: 0.6395
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6317 - acc: 0.6406
1024/1760 [================>.............] - ETA: 36s - loss: 0.6351 - acc: 0.6387
1088/1760 [=================>............] - ETA: 32s - loss: 0.6374 - acc: 0.6342
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6400 - acc: 0.6345
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6411 - acc: 0.6357
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6460 - acc: 0.6289
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6469 - acc: 0.6287
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6473 - acc: 0.6300
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6469 - acc: 0.6291
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6481 - acc: 0.6276
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6487 - acc: 0.6262 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6500 - acc: 0.6250
1728/1760 [============================>.] - ETA: 1s - loss: 0.6491 - acc: 0.6273
1760/1760 [==============================] - 89s 51ms/step - loss: 0.6479 - acc: 0.6284 - val_loss: 0.6278 - val_acc: 0.6480

Epoch 00007: val_acc improved from 0.64286 to 0.64796, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window02/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:22 - loss: 0.6115 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:17 - loss: 0.6762 - acc: 0.5859
 192/1760 [==>...........................] - ETA: 1:13 - loss: 0.6759 - acc: 0.6250
 256/1760 [===>..........................] - ETA: 1:14 - loss: 0.6768 - acc: 0.6055
 320/1760 [====>.........................] - ETA: 1:09 - loss: 0.6772 - acc: 0.5906
 384/1760 [=====>........................] - ETA: 1:06 - loss: 0.6711 - acc: 0.6016
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6661 - acc: 0.6161
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6624 - acc: 0.6191
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6605 - acc: 0.6233 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6543 - acc: 0.6234
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6594 - acc: 0.6151
 768/1760 [============>.................] - ETA: 50s - loss: 0.6609 - acc: 0.6172
 832/1760 [=============>................] - ETA: 46s - loss: 0.6570 - acc: 0.6214
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6537 - acc: 0.6272
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6541 - acc: 0.6260
1024/1760 [================>.............] - ETA: 36s - loss: 0.6522 - acc: 0.6279
1088/1760 [=================>............] - ETA: 34s - loss: 0.6557 - acc: 0.6222
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6597 - acc: 0.6189
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6572 - acc: 0.6192
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6538 - acc: 0.6219
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6544 - acc: 0.6198
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6549 - acc: 0.6207
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6568 - acc: 0.6189
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6544 - acc: 0.6250
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6572 - acc: 0.6206 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6581 - acc: 0.6160
1728/1760 [============================>.] - ETA: 1s - loss: 0.6592 - acc: 0.6152
1760/1760 [==============================] - 93s 53ms/step - loss: 0.6586 - acc: 0.6153 - val_loss: 0.6091 - val_acc: 0.6531

Epoch 00008: val_acc improved from 0.64796 to 0.65306, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window02/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:35 - loss: 0.6748 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:32 - loss: 0.6229 - acc: 0.6953
 192/1760 [==>...........................] - ETA: 1:22 - loss: 0.6327 - acc: 0.6927
 256/1760 [===>..........................] - ETA: 1:18 - loss: 0.6253 - acc: 0.6797
 320/1760 [====>.........................] - ETA: 1:14 - loss: 0.6271 - acc: 0.6781
 384/1760 [=====>........................] - ETA: 1:12 - loss: 0.6448 - acc: 0.6536
 448/1760 [======>.......................] - ETA: 1:08 - loss: 0.6330 - acc: 0.6696
 512/1760 [=======>......................] - ETA: 1:04 - loss: 0.6324 - acc: 0.6660
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6327 - acc: 0.6701
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6340 - acc: 0.6594 
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6339 - acc: 0.6562
 768/1760 [============>.................] - ETA: 49s - loss: 0.6328 - acc: 0.6549
 832/1760 [=============>................] - ETA: 47s - loss: 0.6338 - acc: 0.6562
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6313 - acc: 0.6551
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6307 - acc: 0.6542
1024/1760 [================>.............] - ETA: 37s - loss: 0.6349 - acc: 0.6445
1088/1760 [=================>............] - ETA: 34s - loss: 0.6361 - acc: 0.6406
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6398 - acc: 0.6345
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6405 - acc: 0.6340
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6406 - acc: 0.6320
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6408 - acc: 0.6332
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6420 - acc: 0.6286
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6440 - acc: 0.6243
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6437 - acc: 0.6263
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6434 - acc: 0.6275 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6432 - acc: 0.6262
1728/1760 [============================>.] - ETA: 1s - loss: 0.6428 - acc: 0.6267
1760/1760 [==============================] - 95s 54ms/step - loss: 0.6427 - acc: 0.6261 - val_loss: 0.6223 - val_acc: 0.6531

Epoch 00009: val_acc did not improve from 0.65306
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:23 - loss: 0.6594 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:16 - loss: 0.6475 - acc: 0.6016
 192/1760 [==>...........................] - ETA: 1:14 - loss: 0.6627 - acc: 0.5729
 256/1760 [===>..........................] - ETA: 1:14 - loss: 0.6669 - acc: 0.5742
 320/1760 [====>.........................] - ETA: 1:12 - loss: 0.6734 - acc: 0.5656
 384/1760 [=====>........................] - ETA: 1:10 - loss: 0.6696 - acc: 0.5833
 448/1760 [======>.......................] - ETA: 1:06 - loss: 0.6626 - acc: 0.6071
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6580 - acc: 0.6113
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6612 - acc: 0.6059 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6592 - acc: 0.6109
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6564 - acc: 0.6151
 768/1760 [============>.................] - ETA: 50s - loss: 0.6496 - acc: 0.6237
 832/1760 [=============>................] - ETA: 47s - loss: 0.6516 - acc: 0.6178
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6547 - acc: 0.6127
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6598 - acc: 0.6094
1024/1760 [================>.............] - ETA: 37s - loss: 0.6586 - acc: 0.6143
1088/1760 [=================>............] - ETA: 34s - loss: 0.6579 - acc: 0.6149
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6522 - acc: 0.6207
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6543 - acc: 0.6168
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6491 - acc: 0.6227
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6497 - acc: 0.6198
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6462 - acc: 0.6236
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6462 - acc: 0.6216
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6462 - acc: 0.6217
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6509 - acc: 0.6162 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6488 - acc: 0.6178
1728/1760 [============================>.] - ETA: 1s - loss: 0.6478 - acc: 0.6186
1760/1760 [==============================] - 96s 55ms/step - loss: 0.6491 - acc: 0.6176 - val_loss: 0.6330 - val_acc: 0.6276

Epoch 00010: val_acc did not improve from 0.65306
样本个数 200
样本个数 400
window_select1-5.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 14s
128/400 [========>.....................] - ETA: 8s 
192/400 [=============>................] - ETA: 5s
256/400 [==================>...........] - ETA: 3s
320/400 [=======================>......] - ETA: 1s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 9s 22ms/step
loss: 0.6610178899765015
acc: 0.5925
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9dc4683590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9dc4683590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9dc46caf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9dc46caf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a27a7990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a27a7990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5893a8090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5893a8090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9dc4693610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9dc4693610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9dc457fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9dc457fc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5893a8890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5893a8890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591eea190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591eea190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa591e52410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa591e52410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9dc44cffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9dc44cffd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9dc4390750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9dc4390750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9dc45a81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9dc45a81d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e6c3f8c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e6c3f8c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9dc4198b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9dc4198b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9dc40c6b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9dc40c6b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9dc41cae10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9dc41cae10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9dc4198f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9dc4198f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9dc41b0550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9dc41b0550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9dc414c490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9dc414c490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d8437c550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d8437c550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d845b0f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d845b0f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9dc414cdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9dc414cdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d842b59d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d842b59d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d84182690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d84182690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d841fd890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d841fd890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d84178610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d84178610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d84182b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d84182b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d840db910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d840db910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d8417c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d8417c390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d64652990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d64652990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d840c7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d840c7290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e84660ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e84660ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d840cda10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d840cda10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d6435ead0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d6435ead0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d6462cb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d6462cb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d840cd710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d840cd710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d6435efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d6435efd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d64257810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d64257810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d447c4e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d447c4e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4467e590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4467e590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9dc4428b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9dc4428b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d447c4910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d447c4910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d44566d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d44566d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4473e250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4473e250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4445b910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4445b910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d442f36d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d442f36d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4473e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4473e8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d444f0ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d444f0ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4419fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4419fc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d440b0490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d440b0490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d2476e990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d2476e990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d445f81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d445f81d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d444bfd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d444bfd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d44196d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d44196d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d24512d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d24512d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d245ebad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d245ebad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d440b0050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d440b0050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d245eb7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d245eb7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d242fc8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d242fc8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d24273610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d24273610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d245c5410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d245c5410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d242fc490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d242fc490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d2411b250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d2411b250>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 5:51 - loss: 0.7464 - acc: 0.4844
 128/1760 [=>............................] - ETA: 3:44 - loss: 0.7930 - acc: 0.4766
 192/1760 [==>...........................] - ETA: 2:59 - loss: 0.7707 - acc: 0.5000
 256/1760 [===>..........................] - ETA: 2:32 - loss: 0.7575 - acc: 0.5156
 320/1760 [====>.........................] - ETA: 2:17 - loss: 0.7544 - acc: 0.5031
 384/1760 [=====>........................] - ETA: 2:04 - loss: 0.7490 - acc: 0.4974
 448/1760 [======>.......................] - ETA: 1:56 - loss: 0.7398 - acc: 0.5089
 512/1760 [=======>......................] - ETA: 1:46 - loss: 0.7384 - acc: 0.5137
 576/1760 [========>.....................] - ETA: 1:39 - loss: 0.7334 - acc: 0.5208
 640/1760 [=========>....................] - ETA: 1:32 - loss: 0.7293 - acc: 0.5250
 704/1760 [===========>..................] - ETA: 1:25 - loss: 0.7269 - acc: 0.5270
 768/1760 [============>.................] - ETA: 1:18 - loss: 0.7210 - acc: 0.5339
 832/1760 [=============>................] - ETA: 1:11 - loss: 0.7191 - acc: 0.5385
 896/1760 [==============>...............] - ETA: 1:05 - loss: 0.7179 - acc: 0.5402
 960/1760 [===============>..............] - ETA: 59s - loss: 0.7184 - acc: 0.5396 
1024/1760 [================>.............] - ETA: 53s - loss: 0.7246 - acc: 0.5312
1088/1760 [=================>............] - ETA: 48s - loss: 0.7222 - acc: 0.5276
1152/1760 [==================>...........] - ETA: 43s - loss: 0.7241 - acc: 0.5252
1216/1760 [===================>..........] - ETA: 38s - loss: 0.7220 - acc: 0.5296
1280/1760 [====================>.........] - ETA: 34s - loss: 0.7254 - acc: 0.5258
1344/1760 [=====================>........] - ETA: 29s - loss: 0.7251 - acc: 0.5275
1408/1760 [=======================>......] - ETA: 24s - loss: 0.7264 - acc: 0.5263
1472/1760 [========================>.....] - ETA: 20s - loss: 0.7233 - acc: 0.5319
1536/1760 [=========================>....] - ETA: 15s - loss: 0.7243 - acc: 0.5286
1600/1760 [==========================>...] - ETA: 11s - loss: 0.7244 - acc: 0.5288
1664/1760 [===========================>..] - ETA: 6s - loss: 0.7233 - acc: 0.5282 
1728/1760 [============================>.] - ETA: 2s - loss: 0.7244 - acc: 0.5260
1760/1760 [==============================] - 127s 72ms/step - loss: 0.7240 - acc: 0.5256 - val_loss: 0.6718 - val_acc: 0.5765

Epoch 00001: val_acc improved from -inf to 0.57653, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window03/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:34 - loss: 0.6192 - acc: 0.6406
 128/1760 [=>............................] - ETA: 1:31 - loss: 0.6609 - acc: 0.5859
 192/1760 [==>...........................] - ETA: 1:32 - loss: 0.6664 - acc: 0.5729
 256/1760 [===>..........................] - ETA: 1:31 - loss: 0.6826 - acc: 0.5586
 320/1760 [====>.........................] - ETA: 1:26 - loss: 0.6959 - acc: 0.5312
 384/1760 [=====>........................] - ETA: 1:22 - loss: 0.7008 - acc: 0.5260
 448/1760 [======>.......................] - ETA: 1:19 - loss: 0.7031 - acc: 0.5290
 512/1760 [=======>......................] - ETA: 1:17 - loss: 0.7020 - acc: 0.5312
 576/1760 [========>.....................] - ETA: 1:13 - loss: 0.6939 - acc: 0.5347
 640/1760 [=========>....................] - ETA: 1:10 - loss: 0.6920 - acc: 0.5359
 704/1760 [===========>..................] - ETA: 1:06 - loss: 0.6950 - acc: 0.5355
 768/1760 [============>.................] - ETA: 1:02 - loss: 0.6957 - acc: 0.5391
 832/1760 [=============>................] - ETA: 57s - loss: 0.6962 - acc: 0.5433 
 896/1760 [==============>...............] - ETA: 52s - loss: 0.6928 - acc: 0.5458
 960/1760 [===============>..............] - ETA: 47s - loss: 0.6930 - acc: 0.5469
1024/1760 [================>.............] - ETA: 43s - loss: 0.6911 - acc: 0.5508
1088/1760 [=================>............] - ETA: 39s - loss: 0.6936 - acc: 0.5469
1152/1760 [==================>...........] - ETA: 35s - loss: 0.6945 - acc: 0.5503
1216/1760 [===================>..........] - ETA: 31s - loss: 0.6994 - acc: 0.5461
1280/1760 [====================>.........] - ETA: 28s - loss: 0.7001 - acc: 0.5437
1344/1760 [=====================>........] - ETA: 24s - loss: 0.6966 - acc: 0.5506
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6989 - acc: 0.5504
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6978 - acc: 0.5503
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6989 - acc: 0.5495
1600/1760 [==========================>...] - ETA: 9s - loss: 0.7000 - acc: 0.5456 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7000 - acc: 0.5439
1728/1760 [============================>.] - ETA: 1s - loss: 0.6992 - acc: 0.5440
1760/1760 [==============================] - 107s 61ms/step - loss: 0.6977 - acc: 0.5460 - val_loss: 0.6546 - val_acc: 0.5765

Epoch 00002: val_acc did not improve from 0.57653
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:54 - loss: 0.6653 - acc: 0.5312
 128/1760 [=>............................] - ETA: 1:36 - loss: 0.6731 - acc: 0.5859
 192/1760 [==>...........................] - ETA: 1:28 - loss: 0.6733 - acc: 0.5885
 256/1760 [===>..........................] - ETA: 1:23 - loss: 0.6768 - acc: 0.5977
 320/1760 [====>.........................] - ETA: 1:16 - loss: 0.6756 - acc: 0.6031
 384/1760 [=====>........................] - ETA: 1:11 - loss: 0.6756 - acc: 0.5990
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6750 - acc: 0.5982
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6675 - acc: 0.6133
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6717 - acc: 0.5990 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6722 - acc: 0.5953
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6700 - acc: 0.5952
 768/1760 [============>.................] - ETA: 50s - loss: 0.6727 - acc: 0.5924
 832/1760 [=============>................] - ETA: 46s - loss: 0.6708 - acc: 0.5962
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6761 - acc: 0.5893
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6813 - acc: 0.5854
1024/1760 [================>.............] - ETA: 36s - loss: 0.6805 - acc: 0.5879
1088/1760 [=================>............] - ETA: 33s - loss: 0.6817 - acc: 0.5846
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6794 - acc: 0.5868
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6812 - acc: 0.5822
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6809 - acc: 0.5844
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6817 - acc: 0.5848
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6838 - acc: 0.5781
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6856 - acc: 0.5734
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6841 - acc: 0.5755
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6858 - acc: 0.5744 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6861 - acc: 0.5745
1728/1760 [============================>.] - ETA: 1s - loss: 0.6857 - acc: 0.5735
1760/1760 [==============================] - 91s 52ms/step - loss: 0.6862 - acc: 0.5739 - val_loss: 0.6441 - val_acc: 0.6276

Epoch 00003: val_acc improved from 0.57653 to 0.62755, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window03/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:17 - loss: 0.6068 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:22 - loss: 0.6690 - acc: 0.5625
 192/1760 [==>...........................] - ETA: 1:16 - loss: 0.6736 - acc: 0.5573
 256/1760 [===>..........................] - ETA: 1:13 - loss: 0.6775 - acc: 0.5703
 320/1760 [====>.........................] - ETA: 1:11 - loss: 0.6773 - acc: 0.5813
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6748 - acc: 0.5833
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6810 - acc: 0.5692
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6834 - acc: 0.5586
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6790 - acc: 0.5677
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6769 - acc: 0.5750 
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6750 - acc: 0.5838
 768/1760 [============>.................] - ETA: 48s - loss: 0.6751 - acc: 0.5846
 832/1760 [=============>................] - ETA: 45s - loss: 0.6718 - acc: 0.5877
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6709 - acc: 0.5859
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6750 - acc: 0.5844
1024/1760 [================>.............] - ETA: 35s - loss: 0.6773 - acc: 0.5781
1088/1760 [=================>............] - ETA: 32s - loss: 0.6774 - acc: 0.5818
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6767 - acc: 0.5868
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6771 - acc: 0.5831
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6800 - acc: 0.5828
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6800 - acc: 0.5856
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6785 - acc: 0.5881
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6806 - acc: 0.5876
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6808 - acc: 0.5859
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6795 - acc: 0.5875 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6785 - acc: 0.5871
1728/1760 [============================>.] - ETA: 1s - loss: 0.6797 - acc: 0.5851
1760/1760 [==============================] - 91s 52ms/step - loss: 0.6811 - acc: 0.5835 - val_loss: 0.6653 - val_acc: 0.6071

Epoch 00004: val_acc did not improve from 0.62755
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:35 - loss: 0.6476 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:25 - loss: 0.6519 - acc: 0.5938
 192/1760 [==>...........................] - ETA: 1:20 - loss: 0.6610 - acc: 0.5885
 256/1760 [===>..........................] - ETA: 1:13 - loss: 0.6631 - acc: 0.5781
 320/1760 [====>.........................] - ETA: 1:11 - loss: 0.6653 - acc: 0.5813
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6733 - acc: 0.5807
 448/1760 [======>.......................] - ETA: 1:03 - loss: 0.6648 - acc: 0.5915
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6672 - acc: 0.5957
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6666 - acc: 0.5972 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6755 - acc: 0.5813
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6764 - acc: 0.5795
 768/1760 [============>.................] - ETA: 48s - loss: 0.6739 - acc: 0.5859
 832/1760 [=============>................] - ETA: 46s - loss: 0.6728 - acc: 0.5889
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6726 - acc: 0.5938
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6740 - acc: 0.5927
1024/1760 [================>.............] - ETA: 35s - loss: 0.6730 - acc: 0.5938
1088/1760 [=================>............] - ETA: 33s - loss: 0.6714 - acc: 0.5956
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6716 - acc: 0.5955
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6737 - acc: 0.5905
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6725 - acc: 0.5914
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6705 - acc: 0.5930
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6710 - acc: 0.5902
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6707 - acc: 0.5910
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6741 - acc: 0.5866
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6736 - acc: 0.5887 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6732 - acc: 0.5889
1728/1760 [============================>.] - ETA: 1s - loss: 0.6747 - acc: 0.5868
1760/1760 [==============================] - 91s 52ms/step - loss: 0.6742 - acc: 0.5869 - val_loss: 0.6575 - val_acc: 0.6122

Epoch 00005: val_acc did not improve from 0.62755
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:15 - loss: 0.7045 - acc: 0.5000
 128/1760 [=>............................] - ETA: 1:11 - loss: 0.6571 - acc: 0.6172
 192/1760 [==>...........................] - ETA: 1:11 - loss: 0.6536 - acc: 0.5990
 256/1760 [===>..........................] - ETA: 1:08 - loss: 0.6437 - acc: 0.6133
 320/1760 [====>.........................] - ETA: 1:04 - loss: 0.6554 - acc: 0.5969
 384/1760 [=====>........................] - ETA: 1:03 - loss: 0.6661 - acc: 0.5833
 448/1760 [======>.......................] - ETA: 1:02 - loss: 0.6658 - acc: 0.5848
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6602 - acc: 0.5938
 576/1760 [========>.....................] - ETA: 57s - loss: 0.6668 - acc: 0.5851 
 640/1760 [=========>....................] - ETA: 54s - loss: 0.6616 - acc: 0.5891
 704/1760 [===========>..................] - ETA: 50s - loss: 0.6561 - acc: 0.5952
 768/1760 [============>.................] - ETA: 47s - loss: 0.6551 - acc: 0.5964
 832/1760 [=============>................] - ETA: 44s - loss: 0.6597 - acc: 0.5889
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6598 - acc: 0.5904
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6594 - acc: 0.5917
1024/1760 [================>.............] - ETA: 35s - loss: 0.6609 - acc: 0.5889
1088/1760 [=================>............] - ETA: 32s - loss: 0.6638 - acc: 0.5827
1152/1760 [==================>...........] - ETA: 28s - loss: 0.6636 - acc: 0.5816
1216/1760 [===================>..........] - ETA: 25s - loss: 0.6630 - acc: 0.5831
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6626 - acc: 0.5859
1344/1760 [=====================>........] - ETA: 19s - loss: 0.6636 - acc: 0.5878
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6654 - acc: 0.5852
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6667 - acc: 0.5815
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6659 - acc: 0.5827
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6644 - acc: 0.5863 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6670 - acc: 0.5835
1728/1760 [============================>.] - ETA: 1s - loss: 0.6662 - acc: 0.5845
1760/1760 [==============================] - 87s 49ms/step - loss: 0.6667 - acc: 0.5852 - val_loss: 0.6852 - val_acc: 0.6224

Epoch 00006: val_acc did not improve from 0.62755
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:08 - loss: 0.6906 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:12 - loss: 0.6634 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 1:13 - loss: 0.6641 - acc: 0.6406
 256/1760 [===>..........................] - ETA: 1:12 - loss: 0.6708 - acc: 0.6289
 320/1760 [====>.........................] - ETA: 1:08 - loss: 0.6655 - acc: 0.6344
 384/1760 [=====>........................] - ETA: 1:05 - loss: 0.6651 - acc: 0.6224
 448/1760 [======>.......................] - ETA: 1:01 - loss: 0.6586 - acc: 0.6339
 512/1760 [=======>......................] - ETA: 58s - loss: 0.6580 - acc: 0.6328 
 576/1760 [========>.....................] - ETA: 56s - loss: 0.6563 - acc: 0.6319
 640/1760 [=========>....................] - ETA: 52s - loss: 0.6590 - acc: 0.6312
 704/1760 [===========>..................] - ETA: 49s - loss: 0.6570 - acc: 0.6278
 768/1760 [============>.................] - ETA: 46s - loss: 0.6595 - acc: 0.6198
 832/1760 [=============>................] - ETA: 43s - loss: 0.6586 - acc: 0.6202
 896/1760 [==============>...............] - ETA: 40s - loss: 0.6604 - acc: 0.6127
 960/1760 [===============>..............] - ETA: 37s - loss: 0.6610 - acc: 0.6146
1024/1760 [================>.............] - ETA: 34s - loss: 0.6557 - acc: 0.6211
1088/1760 [=================>............] - ETA: 32s - loss: 0.6584 - acc: 0.6149
1152/1760 [==================>...........] - ETA: 28s - loss: 0.6575 - acc: 0.6146
1216/1760 [===================>..........] - ETA: 25s - loss: 0.6566 - acc: 0.6168
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6557 - acc: 0.6172
1344/1760 [=====================>........] - ETA: 19s - loss: 0.6565 - acc: 0.6168
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6546 - acc: 0.6172
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6537 - acc: 0.6175
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6549 - acc: 0.6159
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6562 - acc: 0.6162 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6535 - acc: 0.6202
1728/1760 [============================>.] - ETA: 1s - loss: 0.6500 - acc: 0.6221
1760/1760 [==============================] - 88s 50ms/step - loss: 0.6500 - acc: 0.6233 - val_loss: 0.6208 - val_acc: 0.6429

Epoch 00007: val_acc improved from 0.62755 to 0.64286, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window03/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:19 - loss: 0.5958 - acc: 0.7031
 128/1760 [=>............................] - ETA: 1:24 - loss: 0.5937 - acc: 0.6641
 192/1760 [==>...........................] - ETA: 1:17 - loss: 0.5806 - acc: 0.6823
 256/1760 [===>..........................] - ETA: 1:16 - loss: 0.5942 - acc: 0.6602
 320/1760 [====>.........................] - ETA: 1:11 - loss: 0.6169 - acc: 0.6344
 384/1760 [=====>........................] - ETA: 1:07 - loss: 0.6096 - acc: 0.6458
 448/1760 [======>.......................] - ETA: 1:03 - loss: 0.6131 - acc: 0.6406
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6203 - acc: 0.6387
 576/1760 [========>.....................] - ETA: 56s - loss: 0.6225 - acc: 0.6389 
 640/1760 [=========>....................] - ETA: 53s - loss: 0.6393 - acc: 0.6234
 704/1760 [===========>..................] - ETA: 51s - loss: 0.6344 - acc: 0.6278
 768/1760 [============>.................] - ETA: 48s - loss: 0.6343 - acc: 0.6289
 832/1760 [=============>................] - ETA: 44s - loss: 0.6329 - acc: 0.6310
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6360 - acc: 0.6317
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6384 - acc: 0.6323
1024/1760 [================>.............] - ETA: 36s - loss: 0.6467 - acc: 0.6211
1088/1760 [=================>............] - ETA: 33s - loss: 0.6470 - acc: 0.6232
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6453 - acc: 0.6267
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6497 - acc: 0.6201
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6531 - acc: 0.6180
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6549 - acc: 0.6168
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6515 - acc: 0.6207
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6510 - acc: 0.6202
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6515 - acc: 0.6185
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6521 - acc: 0.6181 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6518 - acc: 0.6184
1728/1760 [============================>.] - ETA: 1s - loss: 0.6507 - acc: 0.6215
1760/1760 [==============================] - 91s 52ms/step - loss: 0.6513 - acc: 0.6205 - val_loss: 0.6192 - val_acc: 0.6633

Epoch 00008: val_acc improved from 0.64286 to 0.66327, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window03/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:24 - loss: 0.6116 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:18 - loss: 0.6440 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 1:14 - loss: 0.6375 - acc: 0.6458
 256/1760 [===>..........................] - ETA: 1:12 - loss: 0.6480 - acc: 0.6328
 320/1760 [====>.........................] - ETA: 1:13 - loss: 0.6508 - acc: 0.6281
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6495 - acc: 0.6276
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6446 - acc: 0.6362
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6391 - acc: 0.6387
 576/1760 [========>.....................] - ETA: 56s - loss: 0.6465 - acc: 0.6285 
 640/1760 [=========>....................] - ETA: 54s - loss: 0.6436 - acc: 0.6281
 704/1760 [===========>..................] - ETA: 51s - loss: 0.6470 - acc: 0.6236
 768/1760 [============>.................] - ETA: 48s - loss: 0.6476 - acc: 0.6198
 832/1760 [=============>................] - ETA: 45s - loss: 0.6451 - acc: 0.6238
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6421 - acc: 0.6261
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6370 - acc: 0.6354
1024/1760 [================>.............] - ETA: 35s - loss: 0.6418 - acc: 0.6279
1088/1760 [=================>............] - ETA: 32s - loss: 0.6415 - acc: 0.6259
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6442 - acc: 0.6233
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6432 - acc: 0.6225
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6433 - acc: 0.6242
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6484 - acc: 0.6176
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6479 - acc: 0.6165
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6491 - acc: 0.6148
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6502 - acc: 0.6146
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6492 - acc: 0.6150 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6464 - acc: 0.6202
1728/1760 [============================>.] - ETA: 1s - loss: 0.6474 - acc: 0.6198
1760/1760 [==============================] - 89s 50ms/step - loss: 0.6470 - acc: 0.6199 - val_loss: 0.6646 - val_acc: 0.6378

Epoch 00009: val_acc did not improve from 0.66327
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:30 - loss: 0.6099 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:18 - loss: 0.6222 - acc: 0.5938
 192/1760 [==>...........................] - ETA: 1:13 - loss: 0.6121 - acc: 0.6094
 256/1760 [===>..........................] - ETA: 1:10 - loss: 0.6139 - acc: 0.6211
 320/1760 [====>.........................] - ETA: 1:07 - loss: 0.6210 - acc: 0.6188
 384/1760 [=====>........................] - ETA: 1:04 - loss: 0.6230 - acc: 0.6172
 448/1760 [======>.......................] - ETA: 1:00 - loss: 0.6271 - acc: 0.6161
 512/1760 [=======>......................] - ETA: 58s - loss: 0.6328 - acc: 0.6113 
 576/1760 [========>.....................] - ETA: 56s - loss: 0.6396 - acc: 0.6128
 640/1760 [=========>....................] - ETA: 52s - loss: 0.6378 - acc: 0.6234
 704/1760 [===========>..................] - ETA: 49s - loss: 0.6477 - acc: 0.6122
 768/1760 [============>.................] - ETA: 47s - loss: 0.6457 - acc: 0.6159
 832/1760 [=============>................] - ETA: 44s - loss: 0.6471 - acc: 0.6166
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6442 - acc: 0.6217
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6468 - acc: 0.6156
1024/1760 [================>.............] - ETA: 35s - loss: 0.6430 - acc: 0.6211
1088/1760 [=================>............] - ETA: 32s - loss: 0.6439 - acc: 0.6241
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6409 - acc: 0.6259
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6407 - acc: 0.6283
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6383 - acc: 0.6344
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6404 - acc: 0.6317
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6436 - acc: 0.6286
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6422 - acc: 0.6325
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6420 - acc: 0.6335
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6444 - acc: 0.6312 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6443 - acc: 0.6292
1728/1760 [============================>.] - ETA: 1s - loss: 0.6462 - acc: 0.6285
1760/1760 [==============================] - 90s 51ms/step - loss: 0.6466 - acc: 0.6284 - val_loss: 0.6094 - val_acc: 0.6633

Epoch 00010: val_acc did not improve from 0.66327
样本个数 200
样本个数 400
window_select1-5.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 21s
128/400 [========>.....................] - ETA: 12s
192/400 [=============>................] - ETA: 7s 
256/400 [==================>...........] - ETA: 5s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 12s 31ms/step
loss: 0.6634665131568909
acc: 0.595
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9c84431710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9c84431710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9c84434510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9c84434510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a27a73d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a27a73d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5894933d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5894933d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5894b9950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5894b9950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa580a1e290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa580a1e290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c843ef6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c843ef6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c84319d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c84319d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c843c8810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c843c8810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c8407a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c8407a1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c843e7d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c843e7d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c840bf910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c840bf910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c843cec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c843cec50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c6463fa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c6463fa10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c6458f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c6458f590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c841ea510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c841ea510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c6459d110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c6459d110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c442e2f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c442e2f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c44200cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c44200cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c44135fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c44135fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c646e0310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c646e0310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c645660d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c645660d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c645a8910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c645a8910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c2471c0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c2471c0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c245d0250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c245d0250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c2471c210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c2471c210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c44135190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c44135190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c645a9b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c645a9b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c44108390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c44108390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c646d3b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c646d3b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c24480dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c24480dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c245e9c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c245e9c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c243be1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c243be1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c241a0590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c241a0590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c04767950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c04767950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c240937d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c240937d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c240d9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c240d9e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c04758810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c04758810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c04577d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c04577d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c04470290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c04470290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9de411bf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9de411bf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c04577f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c04577f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c0451fdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c0451fdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c0423ef10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c0423ef10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c0421ef10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c0421ef10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be477dd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be477dd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c0426fd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9c0426fd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c04197250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c04197250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be471f090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be471f090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c040f9810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9c040f9810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c84434990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c84434990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9be47ee150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9be47ee150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be459df50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be459df50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be439fcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be439fcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9be42f8310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9be42f8310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be4692150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be4692150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9be43c03d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9be43c03d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be430d410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be430d410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be43c0090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9be43c0090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9be4050c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9be4050c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be42f8c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9be42f8c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9be43bad90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9be43bad90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bc47d3f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bc47d3f10>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 7:01 - loss: 0.7474 - acc: 0.5156
 128/1760 [=>............................] - ETA: 4:18 - loss: 0.7379 - acc: 0.5312
 192/1760 [==>...........................] - ETA: 3:26 - loss: 0.7258 - acc: 0.5573
 256/1760 [===>..........................] - ETA: 2:56 - loss: 0.7457 - acc: 0.5273
 320/1760 [====>.........................] - ETA: 2:33 - loss: 0.7468 - acc: 0.5094
 384/1760 [=====>........................] - ETA: 2:16 - loss: 0.7597 - acc: 0.4896
 448/1760 [======>.......................] - ETA: 2:05 - loss: 0.7524 - acc: 0.5045
 512/1760 [=======>......................] - ETA: 1:54 - loss: 0.7452 - acc: 0.5098
 576/1760 [========>.....................] - ETA: 1:47 - loss: 0.7364 - acc: 0.5191
 640/1760 [=========>....................] - ETA: 1:38 - loss: 0.7337 - acc: 0.5234
 704/1760 [===========>..................] - ETA: 1:31 - loss: 0.7282 - acc: 0.5284
 768/1760 [============>.................] - ETA: 1:23 - loss: 0.7304 - acc: 0.5247
 832/1760 [=============>................] - ETA: 1:16 - loss: 0.7240 - acc: 0.5337
 896/1760 [==============>...............] - ETA: 1:10 - loss: 0.7251 - acc: 0.5290
 960/1760 [===============>..............] - ETA: 1:04 - loss: 0.7253 - acc: 0.5312
1024/1760 [================>.............] - ETA: 58s - loss: 0.7236 - acc: 0.5342 
1088/1760 [=================>............] - ETA: 53s - loss: 0.7198 - acc: 0.5404
1152/1760 [==================>...........] - ETA: 48s - loss: 0.7199 - acc: 0.5425
1216/1760 [===================>..........] - ETA: 42s - loss: 0.7215 - acc: 0.5378
1280/1760 [====================>.........] - ETA: 37s - loss: 0.7191 - acc: 0.5391
1344/1760 [=====================>........] - ETA: 32s - loss: 0.7168 - acc: 0.5409
1408/1760 [=======================>......] - ETA: 27s - loss: 0.7205 - acc: 0.5341
1472/1760 [========================>.....] - ETA: 22s - loss: 0.7223 - acc: 0.5326
1536/1760 [=========================>....] - ETA: 17s - loss: 0.7227 - acc: 0.5319
1600/1760 [==========================>...] - ETA: 12s - loss: 0.7222 - acc: 0.5337
1664/1760 [===========================>..] - ETA: 7s - loss: 0.7214 - acc: 0.5337 
1728/1760 [============================>.] - ETA: 2s - loss: 0.7202 - acc: 0.5336
1760/1760 [==============================] - 142s 81ms/step - loss: 0.7201 - acc: 0.5335 - val_loss: 0.6924 - val_acc: 0.5102

Epoch 00001: val_acc improved from -inf to 0.51020, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window04/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:49 - loss: 0.6803 - acc: 0.5781
 128/1760 [=>............................] - ETA: 1:50 - loss: 0.6798 - acc: 0.5625
 192/1760 [==>...........................] - ETA: 1:48 - loss: 0.6760 - acc: 0.5729
 256/1760 [===>..........................] - ETA: 1:44 - loss: 0.6820 - acc: 0.5586
 320/1760 [====>.........................] - ETA: 1:40 - loss: 0.6904 - acc: 0.5281
 384/1760 [=====>........................] - ETA: 1:37 - loss: 0.6927 - acc: 0.5234
 448/1760 [======>.......................] - ETA: 1:31 - loss: 0.6951 - acc: 0.5179
 512/1760 [=======>......................] - ETA: 1:26 - loss: 0.6966 - acc: 0.5312
 576/1760 [========>.....................] - ETA: 1:22 - loss: 0.6918 - acc: 0.5365
 640/1760 [=========>....................] - ETA: 1:18 - loss: 0.6938 - acc: 0.5297
 704/1760 [===========>..................] - ETA: 1:13 - loss: 0.6975 - acc: 0.5241
 768/1760 [============>.................] - ETA: 1:09 - loss: 0.6988 - acc: 0.5286
 832/1760 [=============>................] - ETA: 1:04 - loss: 0.7020 - acc: 0.5252
 896/1760 [==============>...............] - ETA: 1:00 - loss: 0.7031 - acc: 0.5190
 960/1760 [===============>..............] - ETA: 55s - loss: 0.7019 - acc: 0.5177 
1024/1760 [================>.............] - ETA: 51s - loss: 0.7024 - acc: 0.5166
1088/1760 [=================>............] - ETA: 46s - loss: 0.7016 - acc: 0.5202
1152/1760 [==================>...........] - ETA: 42s - loss: 0.7051 - acc: 0.5139
1216/1760 [===================>..........] - ETA: 37s - loss: 0.7035 - acc: 0.5164
1280/1760 [====================>.........] - ETA: 33s - loss: 0.7015 - acc: 0.5203
1344/1760 [=====================>........] - ETA: 28s - loss: 0.7009 - acc: 0.5238
1408/1760 [=======================>......] - ETA: 24s - loss: 0.7025 - acc: 0.5213
1472/1760 [========================>.....] - ETA: 19s - loss: 0.7016 - acc: 0.5217
1536/1760 [=========================>....] - ETA: 15s - loss: 0.7002 - acc: 0.5273
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6993 - acc: 0.5294
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6974 - acc: 0.5337 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6959 - acc: 0.5347
1760/1760 [==============================] - 123s 70ms/step - loss: 0.6968 - acc: 0.5312 - val_loss: 0.6829 - val_acc: 0.5408

Epoch 00002: val_acc improved from 0.51020 to 0.54082, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window04/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:47 - loss: 0.6660 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:45 - loss: 0.6493 - acc: 0.6016
 192/1760 [==>...........................] - ETA: 1:36 - loss: 0.6489 - acc: 0.6354
 256/1760 [===>..........................] - ETA: 1:32 - loss: 0.6461 - acc: 0.6328
 320/1760 [====>.........................] - ETA: 1:28 - loss: 0.6499 - acc: 0.6375
 384/1760 [=====>........................] - ETA: 1:22 - loss: 0.6461 - acc: 0.6380
 448/1760 [======>.......................] - ETA: 1:17 - loss: 0.6514 - acc: 0.6205
 512/1760 [=======>......................] - ETA: 1:15 - loss: 0.6536 - acc: 0.6250
 576/1760 [========>.....................] - ETA: 1:11 - loss: 0.6523 - acc: 0.6233
 640/1760 [=========>....................] - ETA: 1:07 - loss: 0.6491 - acc: 0.6266
 704/1760 [===========>..................] - ETA: 1:04 - loss: 0.6523 - acc: 0.6165
 768/1760 [============>.................] - ETA: 1:00 - loss: 0.6533 - acc: 0.6107
 832/1760 [=============>................] - ETA: 56s - loss: 0.6542 - acc: 0.6106 
 896/1760 [==============>...............] - ETA: 52s - loss: 0.6550 - acc: 0.6105
 960/1760 [===============>..............] - ETA: 48s - loss: 0.6546 - acc: 0.6115
1024/1760 [================>.............] - ETA: 44s - loss: 0.6559 - acc: 0.6104
1088/1760 [=================>............] - ETA: 40s - loss: 0.6585 - acc: 0.6066
1152/1760 [==================>...........] - ETA: 36s - loss: 0.6602 - acc: 0.6042
1216/1760 [===================>..........] - ETA: 32s - loss: 0.6614 - acc: 0.6020
1280/1760 [====================>.........] - ETA: 28s - loss: 0.6586 - acc: 0.6055
1344/1760 [=====================>........] - ETA: 24s - loss: 0.6623 - acc: 0.6027
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6653 - acc: 0.5987
1472/1760 [========================>.....] - ETA: 17s - loss: 0.6688 - acc: 0.5944
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6687 - acc: 0.5918
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6664 - acc: 0.5956 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6664 - acc: 0.5992
1728/1760 [============================>.] - ETA: 1s - loss: 0.6686 - acc: 0.5961
1760/1760 [==============================] - 107s 61ms/step - loss: 0.6687 - acc: 0.5943 - val_loss: 0.6947 - val_acc: 0.5816

Epoch 00003: val_acc improved from 0.54082 to 0.58163, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window04/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:27 - loss: 0.6151 - acc: 0.6875
 128/1760 [=>............................] - ETA: 1:23 - loss: 0.6382 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 1:23 - loss: 0.6642 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 1:21 - loss: 0.6770 - acc: 0.5820
 320/1760 [====>.........................] - ETA: 1:18 - loss: 0.6825 - acc: 0.5687
 384/1760 [=====>........................] - ETA: 1:13 - loss: 0.6897 - acc: 0.5625
 448/1760 [======>.......................] - ETA: 1:11 - loss: 0.6908 - acc: 0.5603
 512/1760 [=======>......................] - ETA: 1:08 - loss: 0.6877 - acc: 0.5645
 576/1760 [========>.....................] - ETA: 1:05 - loss: 0.6933 - acc: 0.5538
 640/1760 [=========>....................] - ETA: 1:02 - loss: 0.6885 - acc: 0.5609
 704/1760 [===========>..................] - ETA: 58s - loss: 0.6850 - acc: 0.5696 
 768/1760 [============>.................] - ETA: 55s - loss: 0.6813 - acc: 0.5729
 832/1760 [=============>................] - ETA: 52s - loss: 0.6787 - acc: 0.5757
 896/1760 [==============>...............] - ETA: 49s - loss: 0.6815 - acc: 0.5737
 960/1760 [===============>..............] - ETA: 45s - loss: 0.6825 - acc: 0.5719
1024/1760 [================>.............] - ETA: 41s - loss: 0.6791 - acc: 0.5762
1088/1760 [=================>............] - ETA: 37s - loss: 0.6816 - acc: 0.5708
1152/1760 [==================>...........] - ETA: 34s - loss: 0.6766 - acc: 0.5799
1216/1760 [===================>..........] - ETA: 30s - loss: 0.6740 - acc: 0.5872
1280/1760 [====================>.........] - ETA: 26s - loss: 0.6746 - acc: 0.5891
1344/1760 [=====================>........] - ETA: 23s - loss: 0.6726 - acc: 0.5908
1408/1760 [=======================>......] - ETA: 19s - loss: 0.6716 - acc: 0.5923
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6724 - acc: 0.5924
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6739 - acc: 0.5911
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6792 - acc: 0.5856 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6804 - acc: 0.5853
1728/1760 [============================>.] - ETA: 1s - loss: 0.6788 - acc: 0.5885
1760/1760 [==============================] - 98s 56ms/step - loss: 0.6789 - acc: 0.5892 - val_loss: 0.6702 - val_acc: 0.5612

Epoch 00004: val_acc did not improve from 0.58163
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:08 - loss: 0.7017 - acc: 0.5000
 128/1760 [=>............................] - ETA: 1:08 - loss: 0.6866 - acc: 0.5312
 192/1760 [==>...........................] - ETA: 1:11 - loss: 0.6774 - acc: 0.5573
 256/1760 [===>..........................] - ETA: 1:09 - loss: 0.6765 - acc: 0.5703
 320/1760 [====>.........................] - ETA: 1:07 - loss: 0.6751 - acc: 0.5813
 384/1760 [=====>........................] - ETA: 1:05 - loss: 0.6658 - acc: 0.5938
 448/1760 [======>.......................] - ETA: 1:01 - loss: 0.6590 - acc: 0.6071
 512/1760 [=======>......................] - ETA: 59s - loss: 0.6493 - acc: 0.6230 
 576/1760 [========>.....................] - ETA: 55s - loss: 0.6508 - acc: 0.6181
 640/1760 [=========>....................] - ETA: 53s - loss: 0.6501 - acc: 0.6203
 704/1760 [===========>..................] - ETA: 49s - loss: 0.6534 - acc: 0.6193
 768/1760 [============>.................] - ETA: 46s - loss: 0.6546 - acc: 0.6172
 832/1760 [=============>................] - ETA: 43s - loss: 0.6563 - acc: 0.6154
 896/1760 [==============>...............] - ETA: 40s - loss: 0.6600 - acc: 0.6083
 960/1760 [===============>..............] - ETA: 37s - loss: 0.6604 - acc: 0.6073
1024/1760 [================>.............] - ETA: 34s - loss: 0.6613 - acc: 0.6084
1088/1760 [=================>............] - ETA: 31s - loss: 0.6605 - acc: 0.6066
1152/1760 [==================>...........] - ETA: 28s - loss: 0.6588 - acc: 0.6094
1216/1760 [===================>..........] - ETA: 25s - loss: 0.6606 - acc: 0.6094
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6574 - acc: 0.6125
1344/1760 [=====================>........] - ETA: 19s - loss: 0.6577 - acc: 0.6153
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6594 - acc: 0.6122
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6588 - acc: 0.6135
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6612 - acc: 0.6107
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6624 - acc: 0.6075 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6604 - acc: 0.6112
1728/1760 [============================>.] - ETA: 1s - loss: 0.6590 - acc: 0.6111
1760/1760 [==============================] - 88s 50ms/step - loss: 0.6605 - acc: 0.6108 - val_loss: 0.6887 - val_acc: 0.5765

Epoch 00005: val_acc did not improve from 0.58163
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:14 - loss: 0.7037 - acc: 0.5156
 128/1760 [=>............................] - ETA: 1:08 - loss: 0.7051 - acc: 0.5391
 192/1760 [==>...........................] - ETA: 1:06 - loss: 0.7095 - acc: 0.5469
 256/1760 [===>..........................] - ETA: 1:02 - loss: 0.7023 - acc: 0.5430
 320/1760 [====>.........................] - ETA: 1:01 - loss: 0.6948 - acc: 0.5687
 384/1760 [=====>........................] - ETA: 58s - loss: 0.6823 - acc: 0.5807 
 448/1760 [======>.......................] - ETA: 55s - loss: 0.6872 - acc: 0.5848
 512/1760 [=======>......................] - ETA: 52s - loss: 0.6846 - acc: 0.5801
 576/1760 [========>.....................] - ETA: 51s - loss: 0.6857 - acc: 0.5903
 640/1760 [=========>....................] - ETA: 49s - loss: 0.6875 - acc: 0.5859
 704/1760 [===========>..................] - ETA: 47s - loss: 0.6814 - acc: 0.5923
 768/1760 [============>.................] - ETA: 44s - loss: 0.6850 - acc: 0.5898
 832/1760 [=============>................] - ETA: 41s - loss: 0.6809 - acc: 0.5938
 896/1760 [==============>...............] - ETA: 39s - loss: 0.6794 - acc: 0.5949
 960/1760 [===============>..............] - ETA: 36s - loss: 0.6806 - acc: 0.5896
1024/1760 [================>.............] - ETA: 34s - loss: 0.6795 - acc: 0.5879
1088/1760 [=================>............] - ETA: 31s - loss: 0.6799 - acc: 0.5864
1152/1760 [==================>...........] - ETA: 28s - loss: 0.6798 - acc: 0.5825
1216/1760 [===================>..........] - ETA: 25s - loss: 0.6808 - acc: 0.5789
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6778 - acc: 0.5813
1344/1760 [=====================>........] - ETA: 18s - loss: 0.6758 - acc: 0.5811
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6761 - acc: 0.5824
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6733 - acc: 0.5849
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6723 - acc: 0.5859
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6704 - acc: 0.5894 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6698 - acc: 0.5901
1728/1760 [============================>.] - ETA: 1s - loss: 0.6702 - acc: 0.5903
1760/1760 [==============================] - 87s 49ms/step - loss: 0.6689 - acc: 0.5926 - val_loss: 0.6542 - val_acc: 0.6327

Epoch 00006: val_acc improved from 0.58163 to 0.63265, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window04/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:10 - loss: 0.6539 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:13 - loss: 0.6525 - acc: 0.6406
 192/1760 [==>...........................] - ETA: 1:15 - loss: 0.6544 - acc: 0.6250
 256/1760 [===>..........................] - ETA: 1:11 - loss: 0.6463 - acc: 0.6445
 320/1760 [====>.........................] - ETA: 1:10 - loss: 0.6386 - acc: 0.6594
 384/1760 [=====>........................] - ETA: 1:06 - loss: 0.6411 - acc: 0.6484
 448/1760 [======>.......................] - ETA: 1:03 - loss: 0.6415 - acc: 0.6384
 512/1760 [=======>......................] - ETA: 59s - loss: 0.6429 - acc: 0.6348 
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6497 - acc: 0.6319
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6477 - acc: 0.6344
 704/1760 [===========>..................] - ETA: 51s - loss: 0.6524 - acc: 0.6278
 768/1760 [============>.................] - ETA: 49s - loss: 0.6535 - acc: 0.6211
 832/1760 [=============>................] - ETA: 46s - loss: 0.6513 - acc: 0.6262
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6507 - acc: 0.6283
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6493 - acc: 0.6271
1024/1760 [================>.............] - ETA: 37s - loss: 0.6438 - acc: 0.6328
1088/1760 [=================>............] - ETA: 33s - loss: 0.6463 - acc: 0.6296
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6491 - acc: 0.6250
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6509 - acc: 0.6217
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6541 - acc: 0.6188
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6531 - acc: 0.6176
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6532 - acc: 0.6193
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6495 - acc: 0.6209
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6525 - acc: 0.6191
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6529 - acc: 0.6181 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6506 - acc: 0.6208
1728/1760 [============================>.] - ETA: 1s - loss: 0.6508 - acc: 0.6209
1760/1760 [==============================] - 93s 53ms/step - loss: 0.6518 - acc: 0.6210 - val_loss: 0.6444 - val_acc: 0.6429

Epoch 00007: val_acc improved from 0.63265 to 0.64286, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window04/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:31 - loss: 0.6157 - acc: 0.6562
 128/1760 [=>............................] - ETA: 1:20 - loss: 0.6202 - acc: 0.6562
 192/1760 [==>...........................] - ETA: 1:16 - loss: 0.6263 - acc: 0.6562
 256/1760 [===>..........................] - ETA: 1:12 - loss: 0.6375 - acc: 0.6367
 320/1760 [====>.........................] - ETA: 1:06 - loss: 0.6338 - acc: 0.6469
 384/1760 [=====>........................] - ETA: 1:03 - loss: 0.6400 - acc: 0.6406
 448/1760 [======>.......................] - ETA: 1:00 - loss: 0.6471 - acc: 0.6339
 512/1760 [=======>......................] - ETA: 57s - loss: 0.6483 - acc: 0.6270 
 576/1760 [========>.....................] - ETA: 55s - loss: 0.6546 - acc: 0.6215
 640/1760 [=========>....................] - ETA: 52s - loss: 0.6522 - acc: 0.6250
 704/1760 [===========>..................] - ETA: 49s - loss: 0.6517 - acc: 0.6264
 768/1760 [============>.................] - ETA: 45s - loss: 0.6493 - acc: 0.6302
 832/1760 [=============>................] - ETA: 42s - loss: 0.6478 - acc: 0.6298
 896/1760 [==============>...............] - ETA: 39s - loss: 0.6475 - acc: 0.6317
 960/1760 [===============>..............] - ETA: 36s - loss: 0.6487 - acc: 0.6250
1024/1760 [================>.............] - ETA: 33s - loss: 0.6486 - acc: 0.6279
1088/1760 [=================>............] - ETA: 30s - loss: 0.6470 - acc: 0.6296
1152/1760 [==================>...........] - ETA: 28s - loss: 0.6402 - acc: 0.6380
1216/1760 [===================>..........] - ETA: 25s - loss: 0.6433 - acc: 0.6332
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6427 - acc: 0.6344
1344/1760 [=====================>........] - ETA: 19s - loss: 0.6460 - acc: 0.6324
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6444 - acc: 0.6349
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6426 - acc: 0.6379
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6434 - acc: 0.6387
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6458 - acc: 0.6331 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6452 - acc: 0.6322
1728/1760 [============================>.] - ETA: 1s - loss: 0.6462 - acc: 0.6314
1760/1760 [==============================] - 88s 50ms/step - loss: 0.6462 - acc: 0.6312 - val_loss: 0.6333 - val_acc: 0.6531

Epoch 00008: val_acc improved from 0.64286 to 0.65306, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window04/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:13 - loss: 0.7198 - acc: 0.5781
 128/1760 [=>............................] - ETA: 1:15 - loss: 0.6456 - acc: 0.6250
 192/1760 [==>...........................] - ETA: 1:12 - loss: 0.6299 - acc: 0.6510
 256/1760 [===>..........................] - ETA: 1:11 - loss: 0.6194 - acc: 0.6758
 320/1760 [====>.........................] - ETA: 1:08 - loss: 0.6191 - acc: 0.6625
 384/1760 [=====>........................] - ETA: 1:05 - loss: 0.6303 - acc: 0.6536
 448/1760 [======>.......................] - ETA: 1:02 - loss: 0.6240 - acc: 0.6607
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6266 - acc: 0.6543
 576/1760 [========>.....................] - ETA: 57s - loss: 0.6264 - acc: 0.6649 
 640/1760 [=========>....................] - ETA: 53s - loss: 0.6267 - acc: 0.6641
 704/1760 [===========>..................] - ETA: 49s - loss: 0.6285 - acc: 0.6591
 768/1760 [============>.................] - ETA: 47s - loss: 0.6262 - acc: 0.6602
 832/1760 [=============>................] - ETA: 43s - loss: 0.6260 - acc: 0.6599
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6236 - acc: 0.6641
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6312 - acc: 0.6562
1024/1760 [================>.............] - ETA: 35s - loss: 0.6375 - acc: 0.6504
1088/1760 [=================>............] - ETA: 32s - loss: 0.6382 - acc: 0.6517
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6374 - acc: 0.6519
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6356 - acc: 0.6546
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6369 - acc: 0.6531
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6361 - acc: 0.6518
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6394 - acc: 0.6499
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6403 - acc: 0.6481
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6407 - acc: 0.6478
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6429 - acc: 0.6475 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6430 - acc: 0.6466
1728/1760 [============================>.] - ETA: 1s - loss: 0.6458 - acc: 0.6424
1760/1760 [==============================] - 91s 52ms/step - loss: 0.6456 - acc: 0.6415 - val_loss: 0.6879 - val_acc: 0.5867

Epoch 00009: val_acc did not improve from 0.65306
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:22 - loss: 0.6572 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:17 - loss: 0.6477 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 1:12 - loss: 0.6370 - acc: 0.6615
 256/1760 [===>..........................] - ETA: 1:09 - loss: 0.6484 - acc: 0.6523
 320/1760 [====>.........................] - ETA: 1:07 - loss: 0.6374 - acc: 0.6562
 384/1760 [=====>........................] - ETA: 1:06 - loss: 0.6335 - acc: 0.6693
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6402 - acc: 0.6585
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6364 - acc: 0.6641
 576/1760 [========>.....................] - ETA: 56s - loss: 0.6449 - acc: 0.6493 
 640/1760 [=========>....................] - ETA: 54s - loss: 0.6395 - acc: 0.6594
 704/1760 [===========>..................] - ETA: 51s - loss: 0.6404 - acc: 0.6520
 768/1760 [============>.................] - ETA: 48s - loss: 0.6485 - acc: 0.6432
 832/1760 [=============>................] - ETA: 44s - loss: 0.6491 - acc: 0.6418
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6503 - acc: 0.6395
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6487 - acc: 0.6375
1024/1760 [================>.............] - ETA: 36s - loss: 0.6485 - acc: 0.6328
1088/1760 [=================>............] - ETA: 32s - loss: 0.6472 - acc: 0.6324
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6439 - acc: 0.6354
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6422 - acc: 0.6357
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6389 - acc: 0.6398
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6434 - acc: 0.6324
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6421 - acc: 0.6328
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6405 - acc: 0.6359
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6387 - acc: 0.6387
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6386 - acc: 0.6362 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6411 - acc: 0.6352
1728/1760 [============================>.] - ETA: 1s - loss: 0.6441 - acc: 0.6319
1760/1760 [==============================] - 91s 51ms/step - loss: 0.6421 - acc: 0.6347 - val_loss: 0.6432 - val_acc: 0.6122

Epoch 00010: val_acc did not improve from 0.65306
样本个数 200
样本个数 400
window_select1-5.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 25s
128/400 [========>.....................] - ETA: 13s
192/400 [=============>................] - ETA: 8s 
256/400 [==================>...........] - ETA: 5s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 12s 29ms/step
loss: 0.6728614282608032
acc: 0.5975
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9ba41cf890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9ba41cf890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9b481137d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9b481137d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f34061210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f34061210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b24216810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b24216810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa580a33d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa580a33d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3405de50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3405de50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa580a89cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa580a89cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b48120750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b48120750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b2467b190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b2467b190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b2459c310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b2459c310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b24712c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b24712c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b2467b090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b2467b090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b246f4b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b246f4b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b2437e510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b2437e510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b243a0d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b243a0d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b24597750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b24597750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b2437e650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b2437e650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b2427cd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b2427cd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b242afd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b242afd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ae4620fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ae4620fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b4810c490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b4810c490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b242af550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b242af550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ae4505750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ae4505750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ae441ba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ae441ba50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ae4389190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ae4389190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ae4658cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ae4658cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ae441b890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ae441b890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ae420bc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ae420bc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ae4103f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ae4103f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ac47a3cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ac47a3cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ae4159990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ae4159990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ae4103a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ae4103a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ae415ee50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ae415ee50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ac47e12d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ac47e12d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ac45049d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ac45049d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ac43b1b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ac43b1b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ac4592c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ac4592c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ac4440890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ac4440890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ac447bed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ac447bed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ac4275810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ac4275810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ac4504090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ac4504090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ac4488750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ac4488750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ac4197610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ac4197610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ab4762c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ab4762c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ab4686950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ab4686950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ab4721c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ab4721c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ab4762f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ab4762f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ab462c350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ab462c350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ab452f250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ab452f250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ab42d7f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ab42d7f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ab422fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ab422fc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ab4777250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ab4777250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ab430f6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ab430f6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ab432c810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ab432c810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ab4098fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ab4098fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ab4321850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ab4321850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ab432cb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ab432cb10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9a887aa2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9a887aa2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9a88543bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9a88543bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9a8849bc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9a8849bc90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9a883a0c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9a883a0c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9a886a3850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9a886a3850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9a8844a850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9a8844a850>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 7:44 - loss: 0.8146 - acc: 0.4688
 128/1760 [=>............................] - ETA: 4:40 - loss: 0.8889 - acc: 0.4922
 192/1760 [==>...........................] - ETA: 3:36 - loss: 0.8331 - acc: 0.5000
 256/1760 [===>..........................] - ETA: 3:03 - loss: 0.8212 - acc: 0.4844
 320/1760 [====>.........................] - ETA: 2:38 - loss: 0.7987 - acc: 0.5031
 384/1760 [=====>........................] - ETA: 2:22 - loss: 0.7829 - acc: 0.5052
 448/1760 [======>.......................] - ETA: 2:08 - loss: 0.7802 - acc: 0.4978
 512/1760 [=======>......................] - ETA: 1:58 - loss: 0.7637 - acc: 0.5059
 576/1760 [========>.....................] - ETA: 1:48 - loss: 0.7611 - acc: 0.5087
 640/1760 [=========>....................] - ETA: 1:39 - loss: 0.7572 - acc: 0.5078
 704/1760 [===========>..................] - ETA: 1:32 - loss: 0.7562 - acc: 0.5000
 768/1760 [============>.................] - ETA: 1:23 - loss: 0.7531 - acc: 0.4961
 832/1760 [=============>................] - ETA: 1:18 - loss: 0.7504 - acc: 0.5000
 896/1760 [==============>...............] - ETA: 1:11 - loss: 0.7452 - acc: 0.5056
 960/1760 [===============>..............] - ETA: 1:05 - loss: 0.7414 - acc: 0.5115
1024/1760 [================>.............] - ETA: 59s - loss: 0.7372 - acc: 0.5127 
1088/1760 [=================>............] - ETA: 54s - loss: 0.7371 - acc: 0.5074
1152/1760 [==================>...........] - ETA: 48s - loss: 0.7348 - acc: 0.5052
1216/1760 [===================>..........] - ETA: 43s - loss: 0.7364 - acc: 0.5049
1280/1760 [====================>.........] - ETA: 38s - loss: 0.7357 - acc: 0.5047
1344/1760 [=====================>........] - ETA: 32s - loss: 0.7346 - acc: 0.5030
1408/1760 [=======================>......] - ETA: 27s - loss: 0.7350 - acc: 0.5000
1472/1760 [========================>.....] - ETA: 22s - loss: 0.7341 - acc: 0.5007
1536/1760 [=========================>....] - ETA: 17s - loss: 0.7337 - acc: 0.5007
1600/1760 [==========================>...] - ETA: 12s - loss: 0.7329 - acc: 0.5006
1664/1760 [===========================>..] - ETA: 7s - loss: 0.7322 - acc: 0.5000 
1728/1760 [============================>.] - ETA: 2s - loss: 0.7329 - acc: 0.4988
1760/1760 [==============================] - 143s 81ms/step - loss: 0.7333 - acc: 0.4983 - val_loss: 0.6851 - val_acc: 0.5459

Epoch 00001: val_acc improved from -inf to 0.54592, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window05/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:55 - loss: 0.7649 - acc: 0.5469
 128/1760 [=>............................] - ETA: 1:55 - loss: 0.7338 - acc: 0.5156
 192/1760 [==>...........................] - ETA: 1:52 - loss: 0.7242 - acc: 0.5208
 256/1760 [===>..........................] - ETA: 1:44 - loss: 0.7252 - acc: 0.5195
 320/1760 [====>.........................] - ETA: 1:41 - loss: 0.7205 - acc: 0.5250
 384/1760 [=====>........................] - ETA: 1:35 - loss: 0.7141 - acc: 0.5365
 448/1760 [======>.......................] - ETA: 1:30 - loss: 0.7071 - acc: 0.5424
 512/1760 [=======>......................] - ETA: 1:26 - loss: 0.7075 - acc: 0.5449
 576/1760 [========>.....................] - ETA: 1:21 - loss: 0.7085 - acc: 0.5503
 640/1760 [=========>....................] - ETA: 1:16 - loss: 0.7112 - acc: 0.5391
 704/1760 [===========>..................] - ETA: 1:12 - loss: 0.7132 - acc: 0.5355
 768/1760 [============>.................] - ETA: 1:08 - loss: 0.7090 - acc: 0.5404
 832/1760 [=============>................] - ETA: 1:04 - loss: 0.7128 - acc: 0.5312
 896/1760 [==============>...............] - ETA: 1:00 - loss: 0.7117 - acc: 0.5335
 960/1760 [===============>..............] - ETA: 54s - loss: 0.7101 - acc: 0.5323 
1024/1760 [================>.............] - ETA: 49s - loss: 0.7076 - acc: 0.5332
1088/1760 [=================>............] - ETA: 45s - loss: 0.7064 - acc: 0.5349
1152/1760 [==================>...........] - ETA: 40s - loss: 0.7042 - acc: 0.5391
1216/1760 [===================>..........] - ETA: 36s - loss: 0.7025 - acc: 0.5403
1280/1760 [====================>.........] - ETA: 31s - loss: 0.7029 - acc: 0.5367
1344/1760 [=====================>........] - ETA: 27s - loss: 0.7013 - acc: 0.5409
1408/1760 [=======================>......] - ETA: 23s - loss: 0.7014 - acc: 0.5398
1472/1760 [========================>.....] - ETA: 18s - loss: 0.7020 - acc: 0.5374
1536/1760 [=========================>....] - ETA: 14s - loss: 0.6995 - acc: 0.5410
1600/1760 [==========================>...] - ETA: 10s - loss: 0.7015 - acc: 0.5387
1664/1760 [===========================>..] - ETA: 6s - loss: 0.7026 - acc: 0.5355 
1728/1760 [============================>.] - ETA: 2s - loss: 0.7037 - acc: 0.5318
1760/1760 [==============================] - 119s 68ms/step - loss: 0.7028 - acc: 0.5324 - val_loss: 0.6582 - val_acc: 0.6531

Epoch 00002: val_acc improved from 0.54592 to 0.65306, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window05/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:42 - loss: 0.6623 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:34 - loss: 0.6547 - acc: 0.6250
 192/1760 [==>...........................] - ETA: 1:35 - loss: 0.6842 - acc: 0.5573
 256/1760 [===>..........................] - ETA: 1:31 - loss: 0.6735 - acc: 0.6055
 320/1760 [====>.........................] - ETA: 1:27 - loss: 0.6721 - acc: 0.6062
 384/1760 [=====>........................] - ETA: 1:25 - loss: 0.6805 - acc: 0.5859
 448/1760 [======>.......................] - ETA: 1:20 - loss: 0.6756 - acc: 0.5982
 512/1760 [=======>......................] - ETA: 1:15 - loss: 0.6738 - acc: 0.5918
 576/1760 [========>.....................] - ETA: 1:12 - loss: 0.6770 - acc: 0.5885
 640/1760 [=========>....................] - ETA: 1:08 - loss: 0.6810 - acc: 0.5859
 704/1760 [===========>..................] - ETA: 1:04 - loss: 0.6842 - acc: 0.5824
 768/1760 [============>.................] - ETA: 1:00 - loss: 0.6813 - acc: 0.5833
 832/1760 [=============>................] - ETA: 55s - loss: 0.6818 - acc: 0.5793 
 896/1760 [==============>...............] - ETA: 52s - loss: 0.6828 - acc: 0.5781
 960/1760 [===============>..............] - ETA: 48s - loss: 0.6829 - acc: 0.5771
1024/1760 [================>.............] - ETA: 44s - loss: 0.6814 - acc: 0.5781
1088/1760 [=================>............] - ETA: 41s - loss: 0.6812 - acc: 0.5800
1152/1760 [==================>...........] - ETA: 37s - loss: 0.6832 - acc: 0.5720
1216/1760 [===================>..........] - ETA: 33s - loss: 0.6831 - acc: 0.5757
1280/1760 [====================>.........] - ETA: 29s - loss: 0.6848 - acc: 0.5695
1344/1760 [=====================>........] - ETA: 25s - loss: 0.6848 - acc: 0.5722
1408/1760 [=======================>......] - ETA: 21s - loss: 0.6829 - acc: 0.5767
1472/1760 [========================>.....] - ETA: 17s - loss: 0.6822 - acc: 0.5822
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6858 - acc: 0.5775
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6875 - acc: 0.5756 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6867 - acc: 0.5751
1728/1760 [============================>.] - ETA: 1s - loss: 0.6869 - acc: 0.5752
1760/1760 [==============================] - 110s 62ms/step - loss: 0.6877 - acc: 0.5744 - val_loss: 0.6791 - val_acc: 0.5867

Epoch 00003: val_acc did not improve from 0.65306
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:29 - loss: 0.6352 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:24 - loss: 0.6484 - acc: 0.5859
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.6600 - acc: 0.5625
 256/1760 [===>..........................] - ETA: 1:15 - loss: 0.6727 - acc: 0.5430
 320/1760 [====>.........................] - ETA: 1:17 - loss: 0.6771 - acc: 0.5469
 384/1760 [=====>........................] - ETA: 1:13 - loss: 0.6734 - acc: 0.5703
 448/1760 [======>.......................] - ETA: 1:10 - loss: 0.6775 - acc: 0.5625
 512/1760 [=======>......................] - ETA: 1:06 - loss: 0.6761 - acc: 0.5723
 576/1760 [========>.....................] - ETA: 1:02 - loss: 0.6725 - acc: 0.5816
 640/1760 [=========>....................] - ETA: 59s - loss: 0.6741 - acc: 0.5766 
 704/1760 [===========>..................] - ETA: 55s - loss: 0.6752 - acc: 0.5696
 768/1760 [============>.................] - ETA: 52s - loss: 0.6762 - acc: 0.5677
 832/1760 [=============>................] - ETA: 48s - loss: 0.6736 - acc: 0.5697
 896/1760 [==============>...............] - ETA: 45s - loss: 0.6741 - acc: 0.5703
 960/1760 [===============>..............] - ETA: 42s - loss: 0.6751 - acc: 0.5698
1024/1760 [================>.............] - ETA: 38s - loss: 0.6734 - acc: 0.5703
1088/1760 [=================>............] - ETA: 35s - loss: 0.6715 - acc: 0.5754
1152/1760 [==================>...........] - ETA: 32s - loss: 0.6724 - acc: 0.5738
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6727 - acc: 0.5724
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6734 - acc: 0.5727
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6746 - acc: 0.5714
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6747 - acc: 0.5717
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6745 - acc: 0.5734
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6727 - acc: 0.5768
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6751 - acc: 0.5731 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6766 - acc: 0.5715
1728/1760 [============================>.] - ETA: 1s - loss: 0.6750 - acc: 0.5747
1760/1760 [==============================] - 94s 53ms/step - loss: 0.6748 - acc: 0.5773 - val_loss: 0.6900 - val_acc: 0.5357

Epoch 00004: val_acc did not improve from 0.65306
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:18 - loss: 0.6365 - acc: 0.7188
 128/1760 [=>............................] - ETA: 1:13 - loss: 0.6361 - acc: 0.6797
 192/1760 [==>...........................] - ETA: 1:11 - loss: 0.6249 - acc: 0.6979
 256/1760 [===>..........................] - ETA: 1:08 - loss: 0.6241 - acc: 0.6797
 320/1760 [====>.........................] - ETA: 1:07 - loss: 0.6326 - acc: 0.6750
 384/1760 [=====>........................] - ETA: 1:03 - loss: 0.6317 - acc: 0.6719
 448/1760 [======>.......................] - ETA: 1:01 - loss: 0.6405 - acc: 0.6562
 512/1760 [=======>......................] - ETA: 59s - loss: 0.6516 - acc: 0.6426 
 576/1760 [========>.....................] - ETA: 56s - loss: 0.6514 - acc: 0.6302
 640/1760 [=========>....................] - ETA: 53s - loss: 0.6518 - acc: 0.6281
 704/1760 [===========>..................] - ETA: 50s - loss: 0.6562 - acc: 0.6207
 768/1760 [============>.................] - ETA: 47s - loss: 0.6548 - acc: 0.6198
 832/1760 [=============>................] - ETA: 44s - loss: 0.6536 - acc: 0.6202
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6572 - acc: 0.6194
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6564 - acc: 0.6229
1024/1760 [================>.............] - ETA: 35s - loss: 0.6575 - acc: 0.6172
1088/1760 [=================>............] - ETA: 32s - loss: 0.6575 - acc: 0.6176
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6605 - acc: 0.6094
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6578 - acc: 0.6168
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6569 - acc: 0.6172
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6580 - acc: 0.6131
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6611 - acc: 0.6072
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6616 - acc: 0.6067
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6623 - acc: 0.6035
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6639 - acc: 0.6012 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6642 - acc: 0.6016
1728/1760 [============================>.] - ETA: 1s - loss: 0.6651 - acc: 0.6024
1760/1760 [==============================] - 90s 51ms/step - loss: 0.6661 - acc: 0.6028 - val_loss: 0.7195 - val_acc: 0.5306

Epoch 00005: val_acc did not improve from 0.65306
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:10 - loss: 0.7296 - acc: 0.5000
 128/1760 [=>............................] - ETA: 1:08 - loss: 0.7024 - acc: 0.5156
 192/1760 [==>...........................] - ETA: 1:05 - loss: 0.7002 - acc: 0.5365
 256/1760 [===>..........................] - ETA: 1:01 - loss: 0.7111 - acc: 0.5312
 320/1760 [====>.........................] - ETA: 59s - loss: 0.6893 - acc: 0.5625 
 384/1760 [=====>........................] - ETA: 58s - loss: 0.6954 - acc: 0.5521
 448/1760 [======>.......................] - ETA: 55s - loss: 0.6884 - acc: 0.5625
 512/1760 [=======>......................] - ETA: 52s - loss: 0.6839 - acc: 0.5703
 576/1760 [========>.....................] - ETA: 51s - loss: 0.6896 - acc: 0.5642
 640/1760 [=========>....................] - ETA: 48s - loss: 0.6847 - acc: 0.5703
 704/1760 [===========>..................] - ETA: 46s - loss: 0.6807 - acc: 0.5682
 768/1760 [============>.................] - ETA: 45s - loss: 0.6808 - acc: 0.5716
 832/1760 [=============>................] - ETA: 42s - loss: 0.6858 - acc: 0.5661
 896/1760 [==============>...............] - ETA: 39s - loss: 0.6838 - acc: 0.5658
 960/1760 [===============>..............] - ETA: 36s - loss: 0.6846 - acc: 0.5604
1024/1760 [================>.............] - ETA: 34s - loss: 0.6875 - acc: 0.5596
1088/1760 [=================>............] - ETA: 31s - loss: 0.6872 - acc: 0.5616
1152/1760 [==================>...........] - ETA: 27s - loss: 0.6888 - acc: 0.5573
1216/1760 [===================>..........] - ETA: 25s - loss: 0.6898 - acc: 0.5567
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6886 - acc: 0.5594
1344/1760 [=====================>........] - ETA: 19s - loss: 0.6895 - acc: 0.5580
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6929 - acc: 0.5504
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6912 - acc: 0.5510
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6877 - acc: 0.5592
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6879 - acc: 0.5587 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6846 - acc: 0.5625
1728/1760 [============================>.] - ETA: 1s - loss: 0.6853 - acc: 0.5631
1760/1760 [==============================] - 86s 49ms/step - loss: 0.6867 - acc: 0.5625 - val_loss: 0.6517 - val_acc: 0.6480

Epoch 00006: val_acc did not improve from 0.65306
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:17 - loss: 0.6831 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:24 - loss: 0.6748 - acc: 0.5859
 192/1760 [==>...........................] - ETA: 1:16 - loss: 0.7006 - acc: 0.5521
 256/1760 [===>..........................] - ETA: 1:10 - loss: 0.6957 - acc: 0.5508
 320/1760 [====>.........................] - ETA: 1:05 - loss: 0.6794 - acc: 0.5844
 384/1760 [=====>........................] - ETA: 1:03 - loss: 0.6786 - acc: 0.5807
 448/1760 [======>.......................] - ETA: 59s - loss: 0.6709 - acc: 0.5982 
 512/1760 [=======>......................] - ETA: 57s - loss: 0.6696 - acc: 0.6094
 576/1760 [========>.....................] - ETA: 54s - loss: 0.6663 - acc: 0.6076
 640/1760 [=========>....................] - ETA: 51s - loss: 0.6604 - acc: 0.6141
 704/1760 [===========>..................] - ETA: 48s - loss: 0.6623 - acc: 0.6065
 768/1760 [============>.................] - ETA: 44s - loss: 0.6605 - acc: 0.6042
 832/1760 [=============>................] - ETA: 41s - loss: 0.6581 - acc: 0.6082
 896/1760 [==============>...............] - ETA: 39s - loss: 0.6570 - acc: 0.6060
 960/1760 [===============>..............] - ETA: 36s - loss: 0.6555 - acc: 0.6042
1024/1760 [================>.............] - ETA: 33s - loss: 0.6527 - acc: 0.6094
1088/1760 [=================>............] - ETA: 30s - loss: 0.6523 - acc: 0.6094
1152/1760 [==================>...........] - ETA: 27s - loss: 0.6539 - acc: 0.6068
1216/1760 [===================>..........] - ETA: 24s - loss: 0.6524 - acc: 0.6069
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6552 - acc: 0.6023
1344/1760 [=====================>........] - ETA: 19s - loss: 0.6590 - acc: 0.6019
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6587 - acc: 0.6044
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6595 - acc: 0.6012
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6596 - acc: 0.6016
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6602 - acc: 0.6019 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6629 - acc: 0.6010
1728/1760 [============================>.] - ETA: 1s - loss: 0.6627 - acc: 0.5995
1760/1760 [==============================] - 84s 48ms/step - loss: 0.6605 - acc: 0.6023 - val_loss: 0.6666 - val_acc: 0.5663

Epoch 00007: val_acc did not improve from 0.65306
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:13 - loss: 0.6648 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:20 - loss: 0.6462 - acc: 0.6719
 192/1760 [==>...........................] - ETA: 1:13 - loss: 0.6615 - acc: 0.6302
 256/1760 [===>..........................] - ETA: 1:09 - loss: 0.6440 - acc: 0.6445
 320/1760 [====>.........................] - ETA: 1:05 - loss: 0.6508 - acc: 0.6406
 384/1760 [=====>........................] - ETA: 1:01 - loss: 0.6531 - acc: 0.6354
 448/1760 [======>.......................] - ETA: 59s - loss: 0.6587 - acc: 0.6272 
 512/1760 [=======>......................] - ETA: 56s - loss: 0.6601 - acc: 0.6172
 576/1760 [========>.....................] - ETA: 53s - loss: 0.6568 - acc: 0.6267
 640/1760 [=========>....................] - ETA: 50s - loss: 0.6553 - acc: 0.6250
 704/1760 [===========>..................] - ETA: 47s - loss: 0.6496 - acc: 0.6307
 768/1760 [============>.................] - ETA: 44s - loss: 0.6507 - acc: 0.6276
 832/1760 [=============>................] - ETA: 42s - loss: 0.6556 - acc: 0.6226
 896/1760 [==============>...............] - ETA: 40s - loss: 0.6556 - acc: 0.6194
 960/1760 [===============>..............] - ETA: 37s - loss: 0.6584 - acc: 0.6135
1024/1760 [================>.............] - ETA: 34s - loss: 0.6571 - acc: 0.6152
1088/1760 [=================>............] - ETA: 31s - loss: 0.6552 - acc: 0.6195
1152/1760 [==================>...........] - ETA: 28s - loss: 0.6525 - acc: 0.6207
1216/1760 [===================>..........] - ETA: 25s - loss: 0.6534 - acc: 0.6184
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6527 - acc: 0.6180
1344/1760 [=====================>........] - ETA: 19s - loss: 0.6550 - acc: 0.6183
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6557 - acc: 0.6172
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6533 - acc: 0.6202
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6545 - acc: 0.6191
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6537 - acc: 0.6212 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6533 - acc: 0.6232
1728/1760 [============================>.] - ETA: 1s - loss: 0.6559 - acc: 0.6215
1760/1760 [==============================] - 86s 49ms/step - loss: 0.6555 - acc: 0.6216 - val_loss: 0.6845 - val_acc: 0.5816

Epoch 00008: val_acc did not improve from 0.65306
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:27 - loss: 0.6257 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:18 - loss: 0.6246 - acc: 0.6719
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.6253 - acc: 0.6667
 256/1760 [===>..........................] - ETA: 1:14 - loss: 0.6438 - acc: 0.6445
 320/1760 [====>.........................] - ETA: 1:10 - loss: 0.6379 - acc: 0.6438
 384/1760 [=====>........................] - ETA: 1:05 - loss: 0.6354 - acc: 0.6432
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6280 - acc: 0.6473
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6292 - acc: 0.6504
 576/1760 [========>.....................] - ETA: 56s - loss: 0.6321 - acc: 0.6476 
 640/1760 [=========>....................] - ETA: 53s - loss: 0.6362 - acc: 0.6484
 704/1760 [===========>..................] - ETA: 49s - loss: 0.6381 - acc: 0.6491
 768/1760 [============>.................] - ETA: 47s - loss: 0.6395 - acc: 0.6445
 832/1760 [=============>................] - ETA: 43s - loss: 0.6509 - acc: 0.6274
 896/1760 [==============>...............] - ETA: 40s - loss: 0.6535 - acc: 0.6217
 960/1760 [===============>..............] - ETA: 37s - loss: 0.6526 - acc: 0.6240
1024/1760 [================>.............] - ETA: 34s - loss: 0.6535 - acc: 0.6240
1088/1760 [=================>............] - ETA: 31s - loss: 0.6522 - acc: 0.6232
1152/1760 [==================>...........] - ETA: 28s - loss: 0.6515 - acc: 0.6233
1216/1760 [===================>..........] - ETA: 25s - loss: 0.6514 - acc: 0.6242
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6509 - acc: 0.6227
1344/1760 [=====================>........] - ETA: 19s - loss: 0.6531 - acc: 0.6198
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6501 - acc: 0.6250
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6508 - acc: 0.6257
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6514 - acc: 0.6237
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6526 - acc: 0.6206 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6514 - acc: 0.6214
1728/1760 [============================>.] - ETA: 1s - loss: 0.6493 - acc: 0.6244
1760/1760 [==============================] - 85s 48ms/step - loss: 0.6485 - acc: 0.6256 - val_loss: 0.6325 - val_acc: 0.6224

Epoch 00009: val_acc did not improve from 0.65306
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:29 - loss: 0.6649 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:26 - loss: 0.6321 - acc: 0.6875
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.6587 - acc: 0.6354
 256/1760 [===>..........................] - ETA: 1:14 - loss: 0.6758 - acc: 0.6211
 320/1760 [====>.........................] - ETA: 1:10 - loss: 0.6654 - acc: 0.6250
 384/1760 [=====>........................] - ETA: 1:07 - loss: 0.6579 - acc: 0.6198
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6508 - acc: 0.6250
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6501 - acc: 0.6270
 576/1760 [========>.....................] - ETA: 57s - loss: 0.6563 - acc: 0.6181 
 640/1760 [=========>....................] - ETA: 54s - loss: 0.6599 - acc: 0.6125
 704/1760 [===========>..................] - ETA: 50s - loss: 0.6587 - acc: 0.6207
 768/1760 [============>.................] - ETA: 47s - loss: 0.6634 - acc: 0.6146
 832/1760 [=============>................] - ETA: 44s - loss: 0.6599 - acc: 0.6214
 896/1760 [==============>...............] - ETA: 40s - loss: 0.6605 - acc: 0.6172
 960/1760 [===============>..............] - ETA: 37s - loss: 0.6584 - acc: 0.6188
1024/1760 [================>.............] - ETA: 34s - loss: 0.6545 - acc: 0.6211
1088/1760 [=================>............] - ETA: 31s - loss: 0.6532 - acc: 0.6232
1152/1760 [==================>...........] - ETA: 28s - loss: 0.6511 - acc: 0.6241
1216/1760 [===================>..........] - ETA: 25s - loss: 0.6514 - acc: 0.6234
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6469 - acc: 0.6305
1344/1760 [=====================>........] - ETA: 19s - loss: 0.6454 - acc: 0.6302
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6450 - acc: 0.6286
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6438 - acc: 0.6291
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6428 - acc: 0.6283
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6454 - acc: 0.6231 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6457 - acc: 0.6226
1728/1760 [============================>.] - ETA: 1s - loss: 0.6455 - acc: 0.6256
1760/1760 [==============================] - 87s 49ms/step - loss: 0.6442 - acc: 0.6278 - val_loss: 0.6362 - val_acc: 0.6327

Epoch 00010: val_acc did not improve from 0.65306
样本个数 200
样本个数 400
window_select1-5.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 33s
128/400 [========>.....................] - ETA: 16s
192/400 [=============>................] - ETA: 10s
256/400 [==================>...........] - ETA: 6s 
320/400 [=======================>......] - ETA: 3s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 15s 37ms/step
loss: 0.6768011379241944
acc: 0.6025
