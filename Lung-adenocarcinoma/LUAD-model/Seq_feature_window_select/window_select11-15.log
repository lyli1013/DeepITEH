nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 978
样本个数 1956
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f8da2a37d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f8da2a37d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f8dcd051f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f8dcd051f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da29e6cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da29e6cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da29a7210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da29a7210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da2919cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da2919cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dccece510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dccece510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da2859390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da2859390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da2764210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da2764210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da29d9e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da29d9e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da25d4050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da25d4050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da274f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da274f610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da27a8950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da27a8950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da29234d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da29234d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da2398650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da2398650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da2572c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da2572c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da225e4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da225e4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da27c53d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da27c53d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da2168e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da2168e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da200fa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da200fa50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da1f975d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da1f975d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da1fe1f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da1fe1f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da2423f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da2423f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da1f98c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da1f98c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da1d3d650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da1d3d650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da1f973d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da1f973d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da29381d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da29381d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da1d3dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da1d3dc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da1c18c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da1c18c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da1a6fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da1a6fc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da1bcfad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da1bcfad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da1bcd7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da1bcd7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da1a6f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da1a6f590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da1bcdf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da1bcdf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da16fd1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8da16fd1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da16f9190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8da16f9190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da16b2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da16b2a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da19290d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da19290d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d9141f690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d9141f690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d91428710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d91428710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8d912f3410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8d912f3410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d91669dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d91669dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8d914281d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8d914281d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d913db890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d913db890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d91190550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d91190550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8d9107bd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8d9107bd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d9109bf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d9109bf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8d912fcf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8d912fcf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d910262d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d910262d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d91001490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d91001490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8d88e46e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8d88e46e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d90ff2c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d90ff2c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8d9108e910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8d9108e910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d88ad1f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d88ad1f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d88ae68d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d88ae68d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8d889d3390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8d889d3390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da2251310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da2251310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8d88db9090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8d88db9090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da2575f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da2575f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d88de9f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d88de9f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8d7866ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8d7866ac50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d889e4c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d889e4c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da1ca4510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8da1ca4510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d8879fd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8d8879fd50>>: AttributeError: module 'gast' has no attribute 'Str'
2023-01-11 12:05:13.323487: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2023-01-11 12:05:13.359554: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2023-01-11 12:05:13.396767: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555ae4261e60 executing computations on platform Host. Devices:
2023-01-11 12:05:13.396831: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2023-01-11 12:05:13.818276: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window_select11-15.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 3:17 - loss: 0.7616 - acc: 0.5312
 128/1760 [=>............................] - ETA: 2:11 - loss: 0.7530 - acc: 0.5391
 192/1760 [==>...........................] - ETA: 1:50 - loss: 0.7523 - acc: 0.5417
 256/1760 [===>..........................] - ETA: 1:43 - loss: 0.7443 - acc: 0.5430
 320/1760 [====>.........................] - ETA: 1:31 - loss: 0.7362 - acc: 0.5500
 384/1760 [=====>........................] - ETA: 1:22 - loss: 0.7364 - acc: 0.5365
 448/1760 [======>.......................] - ETA: 1:14 - loss: 0.7220 - acc: 0.5536
 512/1760 [=======>......................] - ETA: 1:12 - loss: 0.7278 - acc: 0.5508
 576/1760 [========>.....................] - ETA: 1:06 - loss: 0.7304 - acc: 0.5399
 640/1760 [=========>....................] - ETA: 1:00 - loss: 0.7213 - acc: 0.5484
 704/1760 [===========>..................] - ETA: 55s - loss: 0.7197 - acc: 0.5469 
 768/1760 [============>.................] - ETA: 52s - loss: 0.7208 - acc: 0.5443
 832/1760 [=============>................] - ETA: 48s - loss: 0.7158 - acc: 0.5493
 896/1760 [==============>...............] - ETA: 44s - loss: 0.7117 - acc: 0.5547
 960/1760 [===============>..............] - ETA: 40s - loss: 0.7107 - acc: 0.5573
1024/1760 [================>.............] - ETA: 37s - loss: 0.7100 - acc: 0.5576
1088/1760 [=================>............] - ETA: 33s - loss: 0.7090 - acc: 0.5570
1152/1760 [==================>...........] - ETA: 30s - loss: 0.7058 - acc: 0.5625
1216/1760 [===================>..........] - ETA: 27s - loss: 0.7047 - acc: 0.5609
1280/1760 [====================>.........] - ETA: 23s - loss: 0.7010 - acc: 0.5656
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6953 - acc: 0.5722
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6963 - acc: 0.5668
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6950 - acc: 0.5693
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6946 - acc: 0.5677
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6927 - acc: 0.5706 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6916 - acc: 0.5709
1728/1760 [============================>.] - ETA: 1s - loss: 0.6917 - acc: 0.5712
1760/1760 [==============================] - 90s 51ms/step - loss: 0.6918 - acc: 0.5705 - val_loss: 0.7512 - val_acc: 0.5102

Epoch 00001: val_acc improved from -inf to 0.51020, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window11/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:27 - loss: 0.7513 - acc: 0.5469
 128/1760 [=>............................] - ETA: 1:31 - loss: 0.7105 - acc: 0.5781
 192/1760 [==>...........................] - ETA: 1:21 - loss: 0.7131 - acc: 0.5521
 256/1760 [===>..........................] - ETA: 1:13 - loss: 0.7082 - acc: 0.5547
 320/1760 [====>.........................] - ETA: 1:08 - loss: 0.6928 - acc: 0.5687
 384/1760 [=====>........................] - ETA: 1:03 - loss: 0.6914 - acc: 0.5625
 448/1760 [======>.......................] - ETA: 59s - loss: 0.6993 - acc: 0.5513 
 512/1760 [=======>......................] - ETA: 56s - loss: 0.7019 - acc: 0.5527
 576/1760 [========>.....................] - ETA: 52s - loss: 0.7039 - acc: 0.5417
 640/1760 [=========>....................] - ETA: 49s - loss: 0.7012 - acc: 0.5469
 704/1760 [===========>..................] - ETA: 46s - loss: 0.7056 - acc: 0.5440
 768/1760 [============>.................] - ETA: 44s - loss: 0.7020 - acc: 0.5495
 832/1760 [=============>................] - ETA: 41s - loss: 0.7012 - acc: 0.5565
 896/1760 [==============>...............] - ETA: 37s - loss: 0.6995 - acc: 0.5580
 960/1760 [===============>..............] - ETA: 35s - loss: 0.6960 - acc: 0.5604
1024/1760 [================>.............] - ETA: 32s - loss: 0.6979 - acc: 0.5596
1088/1760 [=================>............] - ETA: 29s - loss: 0.6958 - acc: 0.5671
1152/1760 [==================>...........] - ETA: 26s - loss: 0.6934 - acc: 0.5686
1216/1760 [===================>..........] - ETA: 23s - loss: 0.6917 - acc: 0.5699
1280/1760 [====================>.........] - ETA: 20s - loss: 0.6901 - acc: 0.5742
1344/1760 [=====================>........] - ETA: 17s - loss: 0.6909 - acc: 0.5729
1408/1760 [=======================>......] - ETA: 15s - loss: 0.6892 - acc: 0.5760
1472/1760 [========================>.....] - ETA: 12s - loss: 0.6881 - acc: 0.5747
1536/1760 [=========================>....] - ETA: 9s - loss: 0.6855 - acc: 0.5768 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.6841 - acc: 0.5806
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6858 - acc: 0.5775
1728/1760 [============================>.] - ETA: 1s - loss: 0.6855 - acc: 0.5775
1760/1760 [==============================] - 80s 45ms/step - loss: 0.6851 - acc: 0.5790 - val_loss: 0.6221 - val_acc: 0.6633

Epoch 00002: val_acc improved from 0.51020 to 0.66327, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window11/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:29 - loss: 0.5571 - acc: 0.7188
 128/1760 [=>............................] - ETA: 1:18 - loss: 0.6716 - acc: 0.6250
 192/1760 [==>...........................] - ETA: 1:14 - loss: 0.6917 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 1:09 - loss: 0.6705 - acc: 0.6250
 320/1760 [====>.........................] - ETA: 1:04 - loss: 0.6622 - acc: 0.6250
 384/1760 [=====>........................] - ETA: 1:01 - loss: 0.6520 - acc: 0.6406
 448/1760 [======>.......................] - ETA: 57s - loss: 0.6393 - acc: 0.6540 
 512/1760 [=======>......................] - ETA: 55s - loss: 0.6376 - acc: 0.6504
 576/1760 [========>.....................] - ETA: 51s - loss: 0.6383 - acc: 0.6424
 640/1760 [=========>....................] - ETA: 48s - loss: 0.6230 - acc: 0.6641
 704/1760 [===========>..................] - ETA: 46s - loss: 0.6184 - acc: 0.6662
 768/1760 [============>.................] - ETA: 43s - loss: 0.6147 - acc: 0.6719
 832/1760 [=============>................] - ETA: 40s - loss: 0.6162 - acc: 0.6695
 896/1760 [==============>...............] - ETA: 37s - loss: 0.6170 - acc: 0.6663
 960/1760 [===============>..............] - ETA: 35s - loss: 0.6251 - acc: 0.6562
1024/1760 [================>.............] - ETA: 32s - loss: 0.6226 - acc: 0.6592
1088/1760 [=================>............] - ETA: 29s - loss: 0.6230 - acc: 0.6572
1152/1760 [==================>...........] - ETA: 26s - loss: 0.6236 - acc: 0.6554
1216/1760 [===================>..........] - ETA: 24s - loss: 0.6237 - acc: 0.6546
1280/1760 [====================>.........] - ETA: 21s - loss: 0.6184 - acc: 0.6578
1344/1760 [=====================>........] - ETA: 18s - loss: 0.6185 - acc: 0.6592
1408/1760 [=======================>......] - ETA: 15s - loss: 0.6197 - acc: 0.6598
1472/1760 [========================>.....] - ETA: 12s - loss: 0.6197 - acc: 0.6596
1536/1760 [=========================>....] - ETA: 9s - loss: 0.6166 - acc: 0.6634 
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6145 - acc: 0.6650
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6189 - acc: 0.6629
1728/1760 [============================>.] - ETA: 1s - loss: 0.6219 - acc: 0.6603
1760/1760 [==============================] - 81s 46ms/step - loss: 0.6221 - acc: 0.6608 - val_loss: 0.6013 - val_acc: 0.6582

Epoch 00003: val_acc did not improve from 0.66327
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:26 - loss: 0.5667 - acc: 0.7344
 128/1760 [=>............................] - ETA: 1:19 - loss: 0.6183 - acc: 0.6562
 192/1760 [==>...........................] - ETA: 1:13 - loss: 0.6291 - acc: 0.6198
 256/1760 [===>..........................] - ETA: 1:08 - loss: 0.6291 - acc: 0.6328
 320/1760 [====>.........................] - ETA: 1:05 - loss: 0.6155 - acc: 0.6531
 384/1760 [=====>........................] - ETA: 1:01 - loss: 0.6183 - acc: 0.6536
 448/1760 [======>.......................] - ETA: 57s - loss: 0.6269 - acc: 0.6451 
 512/1760 [=======>......................] - ETA: 53s - loss: 0.6057 - acc: 0.6660
 576/1760 [========>.....................] - ETA: 52s - loss: 0.6085 - acc: 0.6667
 640/1760 [=========>....................] - ETA: 49s - loss: 0.6114 - acc: 0.6656
 704/1760 [===========>..................] - ETA: 46s - loss: 0.6052 - acc: 0.6690
 768/1760 [============>.................] - ETA: 43s - loss: 0.6015 - acc: 0.6758
 832/1760 [=============>................] - ETA: 40s - loss: 0.6050 - acc: 0.6767
 896/1760 [==============>...............] - ETA: 37s - loss: 0.6039 - acc: 0.6786
 960/1760 [===============>..............] - ETA: 34s - loss: 0.6025 - acc: 0.6823
1024/1760 [================>.............] - ETA: 31s - loss: 0.6067 - acc: 0.6787
1088/1760 [=================>............] - ETA: 28s - loss: 0.6048 - acc: 0.6801
1152/1760 [==================>...........] - ETA: 26s - loss: 0.6091 - acc: 0.6762
1216/1760 [===================>..........] - ETA: 23s - loss: 0.6096 - acc: 0.6735
1280/1760 [====================>.........] - ETA: 20s - loss: 0.6068 - acc: 0.6727
1344/1760 [=====================>........] - ETA: 17s - loss: 0.6085 - acc: 0.6726
1408/1760 [=======================>......] - ETA: 14s - loss: 0.6091 - acc: 0.6733
1472/1760 [========================>.....] - ETA: 12s - loss: 0.6110 - acc: 0.6685
1536/1760 [=========================>....] - ETA: 9s - loss: 0.6115 - acc: 0.6680 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.6108 - acc: 0.6687
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6109 - acc: 0.6701
1728/1760 [============================>.] - ETA: 1s - loss: 0.6131 - acc: 0.6661
1760/1760 [==============================] - 79s 45ms/step - loss: 0.6139 - acc: 0.6642 - val_loss: 0.6132 - val_acc: 0.6582

Epoch 00004: val_acc did not improve from 0.66327
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:25 - loss: 0.6222 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:22 - loss: 0.5926 - acc: 0.6875
 192/1760 [==>...........................] - ETA: 1:17 - loss: 0.5808 - acc: 0.7031
 256/1760 [===>..........................] - ETA: 1:16 - loss: 0.5900 - acc: 0.6992
 320/1760 [====>.........................] - ETA: 1:13 - loss: 0.5890 - acc: 0.6937
 384/1760 [=====>........................] - ETA: 1:09 - loss: 0.5986 - acc: 0.6797
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6095 - acc: 0.6696
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6105 - acc: 0.6602
 576/1760 [========>.....................] - ETA: 57s - loss: 0.5973 - acc: 0.6753 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.5921 - acc: 0.6813
 704/1760 [===========>..................] - ETA: 51s - loss: 0.5907 - acc: 0.6832
 768/1760 [============>.................] - ETA: 48s - loss: 0.5965 - acc: 0.6745
 832/1760 [=============>................] - ETA: 45s - loss: 0.5981 - acc: 0.6743
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6034 - acc: 0.6708
 960/1760 [===============>..............] - ETA: 39s - loss: 0.5990 - acc: 0.6740
1024/1760 [================>.............] - ETA: 36s - loss: 0.5990 - acc: 0.6738
1088/1760 [=================>............] - ETA: 33s - loss: 0.5972 - acc: 0.6792
1152/1760 [==================>...........] - ETA: 29s - loss: 0.5983 - acc: 0.6771
1216/1760 [===================>..........] - ETA: 27s - loss: 0.5950 - acc: 0.6801
1280/1760 [====================>.........] - ETA: 24s - loss: 0.5975 - acc: 0.6789
1344/1760 [=====================>........] - ETA: 21s - loss: 0.5928 - acc: 0.6830
1408/1760 [=======================>......] - ETA: 17s - loss: 0.5936 - acc: 0.6847
1472/1760 [========================>.....] - ETA: 14s - loss: 0.5928 - acc: 0.6834
1536/1760 [=========================>....] - ETA: 11s - loss: 0.5940 - acc: 0.6803
1600/1760 [==========================>...] - ETA: 8s - loss: 0.5959 - acc: 0.6769 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.5987 - acc: 0.6749
1728/1760 [============================>.] - ETA: 1s - loss: 0.6000 - acc: 0.6748
1760/1760 [==============================] - 93s 53ms/step - loss: 0.5990 - acc: 0.6756 - val_loss: 0.7190 - val_acc: 0.5918

Epoch 00005: val_acc did not improve from 0.66327
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:24 - loss: 0.5445 - acc: 0.7188
 128/1760 [=>............................] - ETA: 1:28 - loss: 0.4984 - acc: 0.7578
 192/1760 [==>...........................] - ETA: 1:20 - loss: 0.5381 - acc: 0.7188
 256/1760 [===>..........................] - ETA: 1:17 - loss: 0.5351 - acc: 0.7070
 320/1760 [====>.........................] - ETA: 1:14 - loss: 0.5427 - acc: 0.7094
 384/1760 [=====>........................] - ETA: 1:09 - loss: 0.5720 - acc: 0.6797
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.5704 - acc: 0.6808
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.5695 - acc: 0.6777
 576/1760 [========>.....................] - ETA: 57s - loss: 0.5685 - acc: 0.6788 
 640/1760 [=========>....................] - ETA: 54s - loss: 0.5763 - acc: 0.6750
 704/1760 [===========>..................] - ETA: 50s - loss: 0.5789 - acc: 0.6818
 768/1760 [============>.................] - ETA: 48s - loss: 0.5754 - acc: 0.6849
 832/1760 [=============>................] - ETA: 45s - loss: 0.5762 - acc: 0.6875
 896/1760 [==============>...............] - ETA: 42s - loss: 0.5778 - acc: 0.6875
 960/1760 [===============>..............] - ETA: 39s - loss: 0.5836 - acc: 0.6844
1024/1760 [================>.............] - ETA: 37s - loss: 0.5859 - acc: 0.6807
1088/1760 [=================>............] - ETA: 33s - loss: 0.5840 - acc: 0.6829
1152/1760 [==================>...........] - ETA: 30s - loss: 0.5893 - acc: 0.6780
1216/1760 [===================>..........] - ETA: 27s - loss: 0.5947 - acc: 0.6719
1280/1760 [====================>.........] - ETA: 23s - loss: 0.5899 - acc: 0.6766
1344/1760 [=====================>........] - ETA: 20s - loss: 0.5937 - acc: 0.6741
1408/1760 [=======================>......] - ETA: 17s - loss: 0.5934 - acc: 0.6733
1472/1760 [========================>.....] - ETA: 14s - loss: 0.5955 - acc: 0.6719
1536/1760 [=========================>....] - ETA: 10s - loss: 0.5932 - acc: 0.6758
1600/1760 [==========================>...] - ETA: 7s - loss: 0.5957 - acc: 0.6763 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.5950 - acc: 0.6773
1728/1760 [============================>.] - ETA: 1s - loss: 0.5922 - acc: 0.6811
1760/1760 [==============================] - 88s 50ms/step - loss: 0.5896 - acc: 0.6824 - val_loss: 0.5890 - val_acc: 0.6735

Epoch 00006: val_acc improved from 0.66327 to 0.67347, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window11/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:13 - loss: 0.6927 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:08 - loss: 0.6028 - acc: 0.6797
 192/1760 [==>...........................] - ETA: 1:02 - loss: 0.5990 - acc: 0.6719
 256/1760 [===>..........................] - ETA: 59s - loss: 0.5897 - acc: 0.6836 
 320/1760 [====>.........................] - ETA: 57s - loss: 0.5882 - acc: 0.6875
 384/1760 [=====>........................] - ETA: 54s - loss: 0.5744 - acc: 0.7005
 448/1760 [======>.......................] - ETA: 51s - loss: 0.5898 - acc: 0.6920
 512/1760 [=======>......................] - ETA: 48s - loss: 0.5833 - acc: 0.6934
 576/1760 [========>.....................] - ETA: 46s - loss: 0.5821 - acc: 0.6979
 640/1760 [=========>....................] - ETA: 43s - loss: 0.5815 - acc: 0.7016
 704/1760 [===========>..................] - ETA: 41s - loss: 0.5869 - acc: 0.6918
 768/1760 [============>.................] - ETA: 38s - loss: 0.5958 - acc: 0.6888
 832/1760 [=============>................] - ETA: 36s - loss: 0.6018 - acc: 0.6839
 896/1760 [==============>...............] - ETA: 34s - loss: 0.6012 - acc: 0.6808
 960/1760 [===============>..............] - ETA: 31s - loss: 0.5984 - acc: 0.6833
1024/1760 [================>.............] - ETA: 29s - loss: 0.5966 - acc: 0.6875
1088/1760 [=================>............] - ETA: 26s - loss: 0.5994 - acc: 0.6811
1152/1760 [==================>...........] - ETA: 24s - loss: 0.6067 - acc: 0.6753
1216/1760 [===================>..........] - ETA: 21s - loss: 0.6072 - acc: 0.6727
1280/1760 [====================>.........] - ETA: 19s - loss: 0.6061 - acc: 0.6750
1344/1760 [=====================>........] - ETA: 16s - loss: 0.6033 - acc: 0.6763
1408/1760 [=======================>......] - ETA: 13s - loss: 0.6018 - acc: 0.6761
1472/1760 [========================>.....] - ETA: 11s - loss: 0.6010 - acc: 0.6773
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6004 - acc: 0.6790 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.5966 - acc: 0.6831
1664/1760 [===========================>..] - ETA: 3s - loss: 0.5964 - acc: 0.6809
1728/1760 [============================>.] - ETA: 1s - loss: 0.5995 - acc: 0.6782
1760/1760 [==============================] - 73s 41ms/step - loss: 0.5999 - acc: 0.6784 - val_loss: 0.7364 - val_acc: 0.6071

Epoch 00007: val_acc did not improve from 0.67347
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:08 - loss: 0.6577 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:02 - loss: 0.6225 - acc: 0.6641
 192/1760 [==>...........................] - ETA: 58s - loss: 0.6176 - acc: 0.6667 
 256/1760 [===>..........................] - ETA: 55s - loss: 0.5993 - acc: 0.6914
 320/1760 [====>.........................] - ETA: 53s - loss: 0.5959 - acc: 0.6813
 384/1760 [=====>........................] - ETA: 50s - loss: 0.5884 - acc: 0.6953
 448/1760 [======>.......................] - ETA: 49s - loss: 0.5830 - acc: 0.6964
 512/1760 [=======>......................] - ETA: 46s - loss: 0.5889 - acc: 0.6914
 576/1760 [========>.....................] - ETA: 43s - loss: 0.5908 - acc: 0.6927
 640/1760 [=========>....................] - ETA: 41s - loss: 0.5897 - acc: 0.6891
 704/1760 [===========>..................] - ETA: 39s - loss: 0.5817 - acc: 0.6960
 768/1760 [============>.................] - ETA: 36s - loss: 0.5801 - acc: 0.6966
 832/1760 [=============>................] - ETA: 34s - loss: 0.5904 - acc: 0.6887
 896/1760 [==============>...............] - ETA: 32s - loss: 0.5912 - acc: 0.6908
 960/1760 [===============>..............] - ETA: 29s - loss: 0.5862 - acc: 0.6958
1024/1760 [================>.............] - ETA: 27s - loss: 0.5915 - acc: 0.6904
1088/1760 [=================>............] - ETA: 25s - loss: 0.5910 - acc: 0.6903
1152/1760 [==================>...........] - ETA: 22s - loss: 0.5936 - acc: 0.6884
1216/1760 [===================>..........] - ETA: 20s - loss: 0.5929 - acc: 0.6883
1280/1760 [====================>.........] - ETA: 17s - loss: 0.5923 - acc: 0.6875
1344/1760 [=====================>........] - ETA: 15s - loss: 0.5942 - acc: 0.6830
1408/1760 [=======================>......] - ETA: 13s - loss: 0.5932 - acc: 0.6818
1472/1760 [========================>.....] - ETA: 10s - loss: 0.5959 - acc: 0.6814
1536/1760 [=========================>....] - ETA: 8s - loss: 0.5958 - acc: 0.6810 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.5918 - acc: 0.6844
1664/1760 [===========================>..] - ETA: 3s - loss: 0.5909 - acc: 0.6851
1728/1760 [============================>.] - ETA: 1s - loss: 0.5898 - acc: 0.6846
1760/1760 [==============================] - 70s 40ms/step - loss: 0.5923 - acc: 0.6830 - val_loss: 0.5910 - val_acc: 0.6531

Epoch 00008: val_acc did not improve from 0.67347
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:00 - loss: 0.5621 - acc: 0.7500
 128/1760 [=>............................] - ETA: 58s - loss: 0.5681 - acc: 0.7344 
 192/1760 [==>...........................] - ETA: 56s - loss: 0.5696 - acc: 0.7292
 256/1760 [===>..........................] - ETA: 55s - loss: 0.5798 - acc: 0.7148
 320/1760 [====>.........................] - ETA: 52s - loss: 0.5849 - acc: 0.7125
 384/1760 [=====>........................] - ETA: 51s - loss: 0.5926 - acc: 0.7005
 448/1760 [======>.......................] - ETA: 48s - loss: 0.5890 - acc: 0.7009
 512/1760 [=======>......................] - ETA: 45s - loss: 0.5904 - acc: 0.6953
 576/1760 [========>.....................] - ETA: 44s - loss: 0.5839 - acc: 0.7014
 640/1760 [=========>....................] - ETA: 41s - loss: 0.5891 - acc: 0.6937
 704/1760 [===========>..................] - ETA: 38s - loss: 0.5833 - acc: 0.6974
 768/1760 [============>.................] - ETA: 36s - loss: 0.5799 - acc: 0.7005
 832/1760 [=============>................] - ETA: 35s - loss: 0.5790 - acc: 0.7043
 896/1760 [==============>...............] - ETA: 33s - loss: 0.5888 - acc: 0.6964
 960/1760 [===============>..............] - ETA: 31s - loss: 0.5822 - acc: 0.7021
1024/1760 [================>.............] - ETA: 29s - loss: 0.5780 - acc: 0.7090
1088/1760 [=================>............] - ETA: 26s - loss: 0.5777 - acc: 0.7105
1152/1760 [==================>...........] - ETA: 24s - loss: 0.5786 - acc: 0.7109
1216/1760 [===================>..........] - ETA: 22s - loss: 0.5810 - acc: 0.7089
1280/1760 [====================>.........] - ETA: 19s - loss: 0.5814 - acc: 0.7086
1344/1760 [=====================>........] - ETA: 17s - loss: 0.5783 - acc: 0.7121
1408/1760 [=======================>......] - ETA: 14s - loss: 0.5793 - acc: 0.7109
1472/1760 [========================>.....] - ETA: 11s - loss: 0.5786 - acc: 0.7079
1536/1760 [=========================>....] - ETA: 9s - loss: 0.5788 - acc: 0.7051 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.5792 - acc: 0.7025
1664/1760 [===========================>..] - ETA: 4s - loss: 0.5768 - acc: 0.7055
1728/1760 [============================>.] - ETA: 1s - loss: 0.5793 - acc: 0.7020
1760/1760 [==============================] - 79s 45ms/step - loss: 0.5787 - acc: 0.7034 - val_loss: 0.5612 - val_acc: 0.7194

Epoch 00009: val_acc improved from 0.67347 to 0.71939, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window11/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:25 - loss: 0.6131 - acc: 0.6875
 128/1760 [=>............................] - ETA: 1:15 - loss: 0.5829 - acc: 0.7266
 192/1760 [==>...........................] - ETA: 1:10 - loss: 0.6010 - acc: 0.7031
 256/1760 [===>..........................] - ETA: 1:06 - loss: 0.5985 - acc: 0.6992
 320/1760 [====>.........................] - ETA: 1:04 - loss: 0.5768 - acc: 0.7188
 384/1760 [=====>........................] - ETA: 1:03 - loss: 0.5740 - acc: 0.7109
 448/1760 [======>.......................] - ETA: 1:02 - loss: 0.5830 - acc: 0.7031
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.5839 - acc: 0.6992
 576/1760 [========>.....................] - ETA: 57s - loss: 0.5744 - acc: 0.7049 
 640/1760 [=========>....................] - ETA: 53s - loss: 0.5845 - acc: 0.7000
 704/1760 [===========>..................] - ETA: 50s - loss: 0.5897 - acc: 0.6918
 768/1760 [============>.................] - ETA: 46s - loss: 0.5988 - acc: 0.6862
 832/1760 [=============>................] - ETA: 44s - loss: 0.6006 - acc: 0.6851
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6039 - acc: 0.6819
 960/1760 [===============>..............] - ETA: 38s - loss: 0.5994 - acc: 0.6865
1024/1760 [================>.............] - ETA: 34s - loss: 0.6005 - acc: 0.6846
1088/1760 [=================>............] - ETA: 31s - loss: 0.5975 - acc: 0.6857
1152/1760 [==================>...........] - ETA: 28s - loss: 0.5980 - acc: 0.6849
1216/1760 [===================>..........] - ETA: 25s - loss: 0.5978 - acc: 0.6850
1280/1760 [====================>.........] - ETA: 22s - loss: 0.5965 - acc: 0.6844
1344/1760 [=====================>........] - ETA: 19s - loss: 0.5936 - acc: 0.6882
1408/1760 [=======================>......] - ETA: 16s - loss: 0.5935 - acc: 0.6889
1472/1760 [========================>.....] - ETA: 13s - loss: 0.5956 - acc: 0.6861
1536/1760 [=========================>....] - ETA: 10s - loss: 0.5948 - acc: 0.6875
1600/1760 [==========================>...] - ETA: 7s - loss: 0.5976 - acc: 0.6850 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.5931 - acc: 0.6905
1728/1760 [============================>.] - ETA: 1s - loss: 0.5912 - acc: 0.6921
1760/1760 [==============================] - 88s 50ms/step - loss: 0.5908 - acc: 0.6932 - val_loss: 0.5834 - val_acc: 0.7092

Epoch 00010: val_acc did not improve from 0.71939
样本个数 200
样本个数 400
window_select11-15.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 10s
128/400 [========>.....................] - ETA: 6s 
192/400 [=============>................] - ETA: 4s
256/400 [==================>...........] - ETA: 2s
320/400 [=======================>......] - ETA: 1s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 8s 20ms/step
loss: 0.6259313762187958
acc: 0.68
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f875c11aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f875c11aed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f873c1ee310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f873c1ee310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da29e6090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da29e6090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8dcced61d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8dcced61d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8dbc5b58d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8dbc5b58d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dccffa550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dccffa550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8dccfd4ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8dccfd4ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dcd037810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dcd037810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f873c78ccd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f873c78ccd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f875c0c6790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f875c0c6790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dccff7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dccff7290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8dccf5b7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8dccf5b7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f875c0da890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f875c0da890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d786227d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8d786227d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f873c3aa350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f873c3aa350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f873c33dc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f873c33dc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f873c630950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f873c630950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc7c6c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc7c6c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86fc7d6510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86fc7d6510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86fc6add90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86fc6add90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc6fd6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc6fd6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86fc7d63d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86fc7d63d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc58bd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc58bd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86fc434310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86fc434310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86fc3deb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86fc3deb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc1b10d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc1b10d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86fc434550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86fc434550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc3d9ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc3d9ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86fc0a9590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86fc0a9590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86fc09c210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86fc09c210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc58d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc58d710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86fc0ae110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86fc0ae110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc09c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86fc09c390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86dc741dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86dc741dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86dc4623d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86dc4623d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86dc56de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86dc56de90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86dc71ec10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86dc71ec10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86dc5b5990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86dc5b5990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86dc462d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86dc462d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86dc0c7a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86dc0c7a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f873c61d310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f873c61d310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86dc276c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86dc276c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86dc113f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86dc113f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86bc690c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86bc690c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86bc5f84d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86bc5f84d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86bc65b9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86bc65b9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86bc690310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86bc690310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86bc469d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86bc469d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86bc3c3250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86bc3c3250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86bc427a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86bc427a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86bc49b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86bc49b1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86bc5f8650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86bc5f8650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86bc5a4310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86bc5a4310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86bc092d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86bc092d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86bc095310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86bc095310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f869c7cebd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f869c7cebd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86bc2883d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86bc2883d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86bc0ae7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86bc0ae7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f869c6fa1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f869c6fa1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f869c413e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f869c413e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f869c53e350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f869c53e350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f869c6fa250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f869c6fa250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f869c498310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f869c498310>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 3:15 - loss: 0.7199 - acc: 0.5312
 128/1760 [=>............................] - ETA: 2:13 - loss: 0.7374 - acc: 0.5156
 192/1760 [==>...........................] - ETA: 1:47 - loss: 0.7194 - acc: 0.5521
 256/1760 [===>..........................] - ETA: 1:36 - loss: 0.7656 - acc: 0.5234
 320/1760 [====>.........................] - ETA: 1:26 - loss: 0.7606 - acc: 0.5312
 384/1760 [=====>........................] - ETA: 1:17 - loss: 0.7628 - acc: 0.5234
 448/1760 [======>.......................] - ETA: 1:11 - loss: 0.7631 - acc: 0.5201
 512/1760 [=======>......................] - ETA: 1:08 - loss: 0.7508 - acc: 0.5176
 576/1760 [========>.....................] - ETA: 1:03 - loss: 0.7511 - acc: 0.5139
 640/1760 [=========>....................] - ETA: 59s - loss: 0.7446 - acc: 0.5172 
 704/1760 [===========>..................] - ETA: 54s - loss: 0.7435 - acc: 0.5213
 768/1760 [============>.................] - ETA: 50s - loss: 0.7456 - acc: 0.5208
 832/1760 [=============>................] - ETA: 47s - loss: 0.7451 - acc: 0.5228
 896/1760 [==============>...............] - ETA: 43s - loss: 0.7405 - acc: 0.5223
 960/1760 [===============>..............] - ETA: 40s - loss: 0.7419 - acc: 0.5219
1024/1760 [================>.............] - ETA: 36s - loss: 0.7361 - acc: 0.5264
1088/1760 [=================>............] - ETA: 33s - loss: 0.7338 - acc: 0.5248
1152/1760 [==================>...........] - ETA: 30s - loss: 0.7311 - acc: 0.5278
1216/1760 [===================>..........] - ETA: 27s - loss: 0.7316 - acc: 0.5271
1280/1760 [====================>.........] - ETA: 23s - loss: 0.7318 - acc: 0.5258
1344/1760 [=====================>........] - ETA: 20s - loss: 0.7315 - acc: 0.5253
1408/1760 [=======================>......] - ETA: 17s - loss: 0.7296 - acc: 0.5270
1472/1760 [========================>.....] - ETA: 14s - loss: 0.7279 - acc: 0.5299
1536/1760 [=========================>....] - ETA: 10s - loss: 0.7290 - acc: 0.5280
1600/1760 [==========================>...] - ETA: 7s - loss: 0.7286 - acc: 0.5269 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.7288 - acc: 0.5258
1728/1760 [============================>.] - ETA: 1s - loss: 0.7300 - acc: 0.5226
1760/1760 [==============================] - 92s 52ms/step - loss: 0.7305 - acc: 0.5227 - val_loss: 0.6626 - val_acc: 0.5612

Epoch 00001: val_acc improved from -inf to 0.56122, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window12/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:15 - loss: 0.6976 - acc: 0.4844
 128/1760 [=>............................] - ETA: 1:20 - loss: 0.6540 - acc: 0.6094
 192/1760 [==>...........................] - ETA: 1:15 - loss: 0.6593 - acc: 0.6094
 256/1760 [===>..........................] - ETA: 1:15 - loss: 0.6629 - acc: 0.6055
 320/1760 [====>.........................] - ETA: 1:12 - loss: 0.6762 - acc: 0.5813
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6793 - acc: 0.5755
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6746 - acc: 0.5871
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6682 - acc: 0.5977
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6774 - acc: 0.5851 
 640/1760 [=========>....................] - ETA: 54s - loss: 0.6745 - acc: 0.5797
 704/1760 [===========>..................] - ETA: 51s - loss: 0.6803 - acc: 0.5724
 768/1760 [============>.................] - ETA: 48s - loss: 0.6767 - acc: 0.5768
 832/1760 [=============>................] - ETA: 45s - loss: 0.6783 - acc: 0.5781
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6764 - acc: 0.5826
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6768 - acc: 0.5833
1024/1760 [================>.............] - ETA: 36s - loss: 0.6748 - acc: 0.5859
1088/1760 [=================>............] - ETA: 33s - loss: 0.6770 - acc: 0.5800
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6743 - acc: 0.5825
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6744 - acc: 0.5839
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6737 - acc: 0.5875
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6753 - acc: 0.5848
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6754 - acc: 0.5838
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6723 - acc: 0.5863
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6729 - acc: 0.5866
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6769 - acc: 0.5831 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6816 - acc: 0.5757
1728/1760 [============================>.] - ETA: 1s - loss: 0.6795 - acc: 0.5799
1760/1760 [==============================] - 91s 52ms/step - loss: 0.6802 - acc: 0.5784 - val_loss: 0.6393 - val_acc: 0.6276

Epoch 00002: val_acc improved from 0.56122 to 0.62755, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window12/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:02 - loss: 0.6658 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:08 - loss: 0.6640 - acc: 0.5703
 192/1760 [==>...........................] - ETA: 1:06 - loss: 0.6665 - acc: 0.5625
 256/1760 [===>..........................] - ETA: 1:01 - loss: 0.6763 - acc: 0.5508
 320/1760 [====>.........................] - ETA: 58s - loss: 0.6749 - acc: 0.5625 
 384/1760 [=====>........................] - ETA: 55s - loss: 0.6786 - acc: 0.5651
 448/1760 [======>.......................] - ETA: 51s - loss: 0.6759 - acc: 0.5647
 512/1760 [=======>......................] - ETA: 51s - loss: 0.6753 - acc: 0.5645
 576/1760 [========>.....................] - ETA: 48s - loss: 0.6763 - acc: 0.5590
 640/1760 [=========>....................] - ETA: 45s - loss: 0.6743 - acc: 0.5641
 704/1760 [===========>..................] - ETA: 42s - loss: 0.6730 - acc: 0.5668
 768/1760 [============>.................] - ETA: 39s - loss: 0.6723 - acc: 0.5729
 832/1760 [=============>................] - ETA: 37s - loss: 0.6711 - acc: 0.5757
 896/1760 [==============>...............] - ETA: 34s - loss: 0.6756 - acc: 0.5737
 960/1760 [===============>..............] - ETA: 31s - loss: 0.6727 - acc: 0.5813
1024/1760 [================>.............] - ETA: 29s - loss: 0.6708 - acc: 0.5801
1088/1760 [=================>............] - ETA: 27s - loss: 0.6716 - acc: 0.5818
1152/1760 [==================>...........] - ETA: 24s - loss: 0.6743 - acc: 0.5816
1216/1760 [===================>..........] - ETA: 22s - loss: 0.6724 - acc: 0.5839
1280/1760 [====================>.........] - ETA: 19s - loss: 0.6693 - acc: 0.5883
1344/1760 [=====================>........] - ETA: 16s - loss: 0.6684 - acc: 0.5938
1408/1760 [=======================>......] - ETA: 14s - loss: 0.6692 - acc: 0.5945
1472/1760 [========================>.....] - ETA: 11s - loss: 0.6707 - acc: 0.5944
1536/1760 [=========================>....] - ETA: 9s - loss: 0.6730 - acc: 0.5938 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.6712 - acc: 0.5950
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6711 - acc: 0.5974
1728/1760 [============================>.] - ETA: 1s - loss: 0.6675 - acc: 0.6007
1760/1760 [==============================] - 74s 42ms/step - loss: 0.6697 - acc: 0.5983 - val_loss: 0.6236 - val_acc: 0.6378

Epoch 00003: val_acc improved from 0.62755 to 0.63776, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window12/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:02 - loss: 0.6602 - acc: 0.6406
 128/1760 [=>............................] - ETA: 58s - loss: 0.6770 - acc: 0.6172 
 192/1760 [==>...........................] - ETA: 58s - loss: 0.6653 - acc: 0.6302
 256/1760 [===>..........................] - ETA: 58s - loss: 0.6699 - acc: 0.6094
 320/1760 [====>.........................] - ETA: 55s - loss: 0.6655 - acc: 0.6062
 384/1760 [=====>........................] - ETA: 52s - loss: 0.6583 - acc: 0.6224
 448/1760 [======>.......................] - ETA: 50s - loss: 0.6641 - acc: 0.6071
 512/1760 [=======>......................] - ETA: 47s - loss: 0.6692 - acc: 0.6094
 576/1760 [========>.....................] - ETA: 45s - loss: 0.6633 - acc: 0.6128
 640/1760 [=========>....................] - ETA: 42s - loss: 0.6573 - acc: 0.6172
 704/1760 [===========>..................] - ETA: 39s - loss: 0.6561 - acc: 0.6136
 768/1760 [============>.................] - ETA: 37s - loss: 0.6580 - acc: 0.6146
 832/1760 [=============>................] - ETA: 34s - loss: 0.6561 - acc: 0.6166
 896/1760 [==============>...............] - ETA: 32s - loss: 0.6517 - acc: 0.6194
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6513 - acc: 0.6198
1024/1760 [================>.............] - ETA: 27s - loss: 0.6531 - acc: 0.6191
1088/1760 [=================>............] - ETA: 25s - loss: 0.6505 - acc: 0.6232
1152/1760 [==================>...........] - ETA: 23s - loss: 0.6463 - acc: 0.6319
1216/1760 [===================>..........] - ETA: 20s - loss: 0.6454 - acc: 0.6349
1280/1760 [====================>.........] - ETA: 18s - loss: 0.6495 - acc: 0.6305
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6499 - acc: 0.6295
1408/1760 [=======================>......] - ETA: 13s - loss: 0.6508 - acc: 0.6271
1472/1760 [========================>.....] - ETA: 11s - loss: 0.6466 - acc: 0.6318
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6486 - acc: 0.6289 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.6509 - acc: 0.6262
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6488 - acc: 0.6268
1728/1760 [============================>.] - ETA: 1s - loss: 0.6497 - acc: 0.6267
1760/1760 [==============================] - 71s 40ms/step - loss: 0.6494 - acc: 0.6284 - val_loss: 0.6335 - val_acc: 0.6020

Epoch 00004: val_acc did not improve from 0.63776
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:07 - loss: 0.6892 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:02 - loss: 0.6609 - acc: 0.6562
 192/1760 [==>...........................] - ETA: 1:01 - loss: 0.6650 - acc: 0.6406
 256/1760 [===>..........................] - ETA: 57s - loss: 0.6699 - acc: 0.6328 
 320/1760 [====>.........................] - ETA: 54s - loss: 0.6700 - acc: 0.6281
 384/1760 [=====>........................] - ETA: 52s - loss: 0.6672 - acc: 0.6198
 448/1760 [======>.......................] - ETA: 49s - loss: 0.6612 - acc: 0.6228
 512/1760 [=======>......................] - ETA: 46s - loss: 0.6616 - acc: 0.6191
 576/1760 [========>.....................] - ETA: 44s - loss: 0.6586 - acc: 0.6285
 640/1760 [=========>....................] - ETA: 41s - loss: 0.6567 - acc: 0.6297
 704/1760 [===========>..................] - ETA: 39s - loss: 0.6516 - acc: 0.6321
 768/1760 [============>.................] - ETA: 36s - loss: 0.6558 - acc: 0.6250
 832/1760 [=============>................] - ETA: 34s - loss: 0.6518 - acc: 0.6262
 896/1760 [==============>...............] - ETA: 32s - loss: 0.6481 - acc: 0.6283
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6438 - acc: 0.6312
1024/1760 [================>.............] - ETA: 27s - loss: 0.6419 - acc: 0.6279
1088/1760 [=================>............] - ETA: 25s - loss: 0.6436 - acc: 0.6259
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6442 - acc: 0.6250
1216/1760 [===================>..........] - ETA: 20s - loss: 0.6440 - acc: 0.6217
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6386 - acc: 0.6242
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6397 - acc: 0.6235
1408/1760 [=======================>......] - ETA: 13s - loss: 0.6412 - acc: 0.6229
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6393 - acc: 0.6243
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6371 - acc: 0.6270 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6395 - acc: 0.6256
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6379 - acc: 0.6304
1728/1760 [============================>.] - ETA: 1s - loss: 0.6393 - acc: 0.6273
1760/1760 [==============================] - 69s 39ms/step - loss: 0.6404 - acc: 0.6256 - val_loss: 0.6157 - val_acc: 0.6480

Epoch 00005: val_acc improved from 0.63776 to 0.64796, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window12/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:03 - loss: 0.6252 - acc: 0.6562
 128/1760 [=>............................] - ETA: 59s - loss: 0.6659 - acc: 0.6016 
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6635 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 54s - loss: 0.6540 - acc: 0.6016
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6461 - acc: 0.6188
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6416 - acc: 0.6276
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6409 - acc: 0.6250
 512/1760 [=======>......................] - ETA: 45s - loss: 0.6494 - acc: 0.6152
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6483 - acc: 0.6215
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6459 - acc: 0.6266
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6438 - acc: 0.6349
 768/1760 [============>.................] - ETA: 36s - loss: 0.6473 - acc: 0.6328
 832/1760 [=============>................] - ETA: 33s - loss: 0.6485 - acc: 0.6262
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6521 - acc: 0.6161
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6500 - acc: 0.6188
1024/1760 [================>.............] - ETA: 26s - loss: 0.6458 - acc: 0.6260
1088/1760 [=================>............] - ETA: 24s - loss: 0.6446 - acc: 0.6259
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6454 - acc: 0.6285
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6412 - acc: 0.6357
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6399 - acc: 0.6367
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6407 - acc: 0.6332
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6427 - acc: 0.6307
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6435 - acc: 0.6298
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6442 - acc: 0.6289 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6418 - acc: 0.6338
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6408 - acc: 0.6340
1728/1760 [============================>.] - ETA: 1s - loss: 0.6388 - acc: 0.6366
1760/1760 [==============================] - 68s 39ms/step - loss: 0.6386 - acc: 0.6375 - val_loss: 0.6971 - val_acc: 0.5765

Epoch 00006: val_acc did not improve from 0.64796
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:05 - loss: 0.6499 - acc: 0.5781
 128/1760 [=>............................] - ETA: 1:00 - loss: 0.6817 - acc: 0.5781
 192/1760 [==>...........................] - ETA: 58s - loss: 0.6927 - acc: 0.5573 
 256/1760 [===>..........................] - ETA: 56s - loss: 0.6760 - acc: 0.5781
 320/1760 [====>.........................] - ETA: 53s - loss: 0.6813 - acc: 0.5750
 384/1760 [=====>........................] - ETA: 50s - loss: 0.6768 - acc: 0.5781
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6717 - acc: 0.5804
 512/1760 [=======>......................] - ETA: 45s - loss: 0.6704 - acc: 0.5918
 576/1760 [========>.....................] - ETA: 43s - loss: 0.6633 - acc: 0.5990
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6583 - acc: 0.6031
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6588 - acc: 0.6080
 768/1760 [============>.................] - ETA: 36s - loss: 0.6538 - acc: 0.6120
 832/1760 [=============>................] - ETA: 34s - loss: 0.6542 - acc: 0.6106
 896/1760 [==============>...............] - ETA: 32s - loss: 0.6536 - acc: 0.6116
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6506 - acc: 0.6146
1024/1760 [================>.............] - ETA: 27s - loss: 0.6521 - acc: 0.6113
1088/1760 [=================>............] - ETA: 24s - loss: 0.6516 - acc: 0.6121
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6532 - acc: 0.6094
1216/1760 [===================>..........] - ETA: 20s - loss: 0.6506 - acc: 0.6127
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6454 - acc: 0.6195
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6416 - acc: 0.6250
1408/1760 [=======================>......] - ETA: 13s - loss: 0.6399 - acc: 0.6264
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6415 - acc: 0.6284
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6381 - acc: 0.6309 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6362 - acc: 0.6338
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6367 - acc: 0.6346
1728/1760 [============================>.] - ETA: 1s - loss: 0.6361 - acc: 0.6372
1760/1760 [==============================] - 68s 39ms/step - loss: 0.6346 - acc: 0.6386 - val_loss: 0.6098 - val_acc: 0.6582

Epoch 00007: val_acc improved from 0.64796 to 0.65816, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window12/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 8/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.6558 - acc: 0.6406
 128/1760 [=>............................] - ETA: 57s - loss: 0.6147 - acc: 0.6406
 192/1760 [==>...........................] - ETA: 55s - loss: 0.5894 - acc: 0.6719
 256/1760 [===>..........................] - ETA: 53s - loss: 0.5840 - acc: 0.6875
 320/1760 [====>.........................] - ETA: 51s - loss: 0.5819 - acc: 0.6875
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6003 - acc: 0.6745
 448/1760 [======>.......................] - ETA: 46s - loss: 0.5966 - acc: 0.6786
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6102 - acc: 0.6641
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6056 - acc: 0.6736
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6143 - acc: 0.6625
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6078 - acc: 0.6619
 768/1760 [============>.................] - ETA: 35s - loss: 0.6095 - acc: 0.6615
 832/1760 [=============>................] - ETA: 33s - loss: 0.6113 - acc: 0.6550
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6095 - acc: 0.6574
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6109 - acc: 0.6583
1024/1760 [================>.............] - ETA: 26s - loss: 0.6104 - acc: 0.6572
1088/1760 [=================>............] - ETA: 24s - loss: 0.6142 - acc: 0.6535
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6176 - acc: 0.6484
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6199 - acc: 0.6439
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6170 - acc: 0.6477
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6209 - acc: 0.6429
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6240 - acc: 0.6385
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6293 - acc: 0.6325
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6283 - acc: 0.6361 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6285 - acc: 0.6394
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6304 - acc: 0.6388
1728/1760 [============================>.] - ETA: 1s - loss: 0.6301 - acc: 0.6395
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6298 - acc: 0.6392 - val_loss: 0.6556 - val_acc: 0.6786

Epoch 00008: val_acc improved from 0.65816 to 0.67857, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window12/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 9/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.6355 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:00 - loss: 0.6579 - acc: 0.6016
 192/1760 [==>...........................] - ETA: 57s - loss: 0.6318 - acc: 0.6250 
 256/1760 [===>..........................] - ETA: 55s - loss: 0.6257 - acc: 0.6289
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6424 - acc: 0.6188
 384/1760 [=====>........................] - ETA: 50s - loss: 0.6308 - acc: 0.6354
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6240 - acc: 0.6406
 512/1760 [=======>......................] - ETA: 45s - loss: 0.6251 - acc: 0.6387
 576/1760 [========>.....................] - ETA: 44s - loss: 0.6400 - acc: 0.6267
 640/1760 [=========>....................] - ETA: 41s - loss: 0.6408 - acc: 0.6219
 704/1760 [===========>..................] - ETA: 39s - loss: 0.6369 - acc: 0.6264
 768/1760 [============>.................] - ETA: 37s - loss: 0.6364 - acc: 0.6315
 832/1760 [=============>................] - ETA: 34s - loss: 0.6341 - acc: 0.6346
 896/1760 [==============>...............] - ETA: 32s - loss: 0.6353 - acc: 0.6328
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6359 - acc: 0.6323
1024/1760 [================>.............] - ETA: 27s - loss: 0.6353 - acc: 0.6328
1088/1760 [=================>............] - ETA: 24s - loss: 0.6339 - acc: 0.6360
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6346 - acc: 0.6345
1216/1760 [===================>..........] - ETA: 20s - loss: 0.6357 - acc: 0.6332
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6314 - acc: 0.6398
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6301 - acc: 0.6414
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6311 - acc: 0.6420
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6311 - acc: 0.6413
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6303 - acc: 0.6413 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6310 - acc: 0.6412
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6283 - acc: 0.6424
1728/1760 [============================>.] - ETA: 1s - loss: 0.6290 - acc: 0.6418
1760/1760 [==============================] - 68s 39ms/step - loss: 0.6291 - acc: 0.6409 - val_loss: 0.6388 - val_acc: 0.6480

Epoch 00009: val_acc did not improve from 0.67857
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:01 - loss: 0.6259 - acc: 0.6250
 128/1760 [=>............................] - ETA: 58s - loss: 0.6282 - acc: 0.6406 
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6410 - acc: 0.6354
 256/1760 [===>..........................] - ETA: 54s - loss: 0.6366 - acc: 0.6484
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6194 - acc: 0.6500
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6171 - acc: 0.6484
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6328 - acc: 0.6406
 512/1760 [=======>......................] - ETA: 45s - loss: 0.6375 - acc: 0.6387
 576/1760 [========>.....................] - ETA: 43s - loss: 0.6322 - acc: 0.6476
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6276 - acc: 0.6547
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6264 - acc: 0.6577
 768/1760 [============>.................] - ETA: 36s - loss: 0.6234 - acc: 0.6628
 832/1760 [=============>................] - ETA: 33s - loss: 0.6314 - acc: 0.6562
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6285 - acc: 0.6596
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6205 - acc: 0.6677
1024/1760 [================>.............] - ETA: 26s - loss: 0.6192 - acc: 0.6670
1088/1760 [=================>............] - ETA: 24s - loss: 0.6223 - acc: 0.6664
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6259 - acc: 0.6597
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6234 - acc: 0.6637
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6236 - acc: 0.6625
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6214 - acc: 0.6637
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6201 - acc: 0.6662
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6203 - acc: 0.6651
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6202 - acc: 0.6634 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6187 - acc: 0.6644
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6190 - acc: 0.6653
1728/1760 [============================>.] - ETA: 1s - loss: 0.6178 - acc: 0.6678
1760/1760 [==============================] - 68s 38ms/step - loss: 0.6178 - acc: 0.6670 - val_loss: 0.6078 - val_acc: 0.6480

Epoch 00010: val_acc did not improve from 0.67857
样本个数 200
样本个数 400
window_select11-15.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 12s
128/400 [========>.....................] - ETA: 6s 
192/400 [=============>................] - ETA: 4s
256/400 [==================>...........] - ETA: 2s
320/400 [=======================>......] - ETA: 1s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 7s 17ms/step
loss: 0.6712591886520386
acc: 0.635
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f86005cd910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f86005cd910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f85c05d7c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f85c05d7c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dbc5b5c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dbc5b5c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8db3afb250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8db3afb250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8db3afd350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8db3afd350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db3b06890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db3b06890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8db3afb510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8db3afb510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db3afa150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db3afa150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f869c498610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f869c498610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8dbc5a57d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8dbc5a57d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db3b060d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db3b060d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8600598b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8600598b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86001f2710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86001f2710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86003700d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86003700d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f860007c3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f860007c3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86000e1d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86000e1d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8600410710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8600410710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f85dc7acdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f85dc7acdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8600677290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8600677290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86006653d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86006653d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86005d2050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86005d2050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f85c0526050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f85c0526050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f85c02222d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f85c02222d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f898418af50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f898418af50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f85c03036d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f85c03036d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f859c6906d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f859c6906d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f85c0450850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f85c0450850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f85c009c450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f85c009c450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f859c582d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f859c582d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f859c518f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f859c518f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f859c56c490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f859c56c490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f85c00a8c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f85c00a8c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f859c488550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f859c488550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f859c7ca090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f859c7ca090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f859c4b09d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f859c4b09d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dbc523b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dbc523b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f859c2b5690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f859c2b5690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f859c1a0990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f859c1a0990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f859c298fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f859c298fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f859c1a4b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f859c1a4b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dbc523450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dbc523450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f859c19da90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f859c19da90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f858052ba10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f858052ba10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f858044f790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f858044f790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8580602a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8580602a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f858051e150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f858051e150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f859c18fc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f859c18fc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8580321a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8580321a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f85803a29d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f85803a29d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f858030fa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f858030fa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c69d810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c69d810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f85803a2c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f85803a2c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c69d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c69d8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f855c5adf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f855c5adf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f855c453810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f855c453810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c53fd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c53fd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f85800c7fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f85800c7fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c2d0790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c2d0790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f855c24d5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f855c24d5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f855c487e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f855c487e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c52cc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c52cc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f855c24db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f855c24db90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c493150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f855c493150>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 5:19 - loss: 0.7616 - acc: 0.4688
 128/1760 [=>............................] - ETA: 3:03 - loss: 0.7467 - acc: 0.5078
 192/1760 [==>...........................] - ETA: 2:16 - loss: 0.7397 - acc: 0.5052
 256/1760 [===>..........................] - ETA: 1:51 - loss: 0.7527 - acc: 0.4844
 320/1760 [====>.........................] - ETA: 1:34 - loss: 0.7533 - acc: 0.4844
 384/1760 [=====>........................] - ETA: 1:23 - loss: 0.7430 - acc: 0.4896
 448/1760 [======>.......................] - ETA: 1:15 - loss: 0.7455 - acc: 0.5000
 512/1760 [=======>......................] - ETA: 1:07 - loss: 0.7459 - acc: 0.4980
 576/1760 [========>.....................] - ETA: 1:01 - loss: 0.7530 - acc: 0.4878
 640/1760 [=========>....................] - ETA: 56s - loss: 0.7508 - acc: 0.4859 
 704/1760 [===========>..................] - ETA: 51s - loss: 0.7527 - acc: 0.4872
 768/1760 [============>.................] - ETA: 47s - loss: 0.7444 - acc: 0.4883
 832/1760 [=============>................] - ETA: 43s - loss: 0.7432 - acc: 0.4916
 896/1760 [==============>...............] - ETA: 39s - loss: 0.7451 - acc: 0.4955
 960/1760 [===============>..............] - ETA: 36s - loss: 0.7438 - acc: 0.5021
1024/1760 [================>.............] - ETA: 33s - loss: 0.7437 - acc: 0.4941
1088/1760 [=================>............] - ETA: 30s - loss: 0.7414 - acc: 0.5000
1152/1760 [==================>...........] - ETA: 26s - loss: 0.7392 - acc: 0.5035
1216/1760 [===================>..........] - ETA: 23s - loss: 0.7359 - acc: 0.5082
1280/1760 [====================>.........] - ETA: 20s - loss: 0.7351 - acc: 0.5094
1344/1760 [=====================>........] - ETA: 17s - loss: 0.7322 - acc: 0.5126
1408/1760 [=======================>......] - ETA: 15s - loss: 0.7277 - acc: 0.5163
1472/1760 [========================>.....] - ETA: 12s - loss: 0.7285 - acc: 0.5156
1536/1760 [=========================>....] - ETA: 9s - loss: 0.7258 - acc: 0.5195 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.7236 - acc: 0.5212
1664/1760 [===========================>..] - ETA: 4s - loss: 0.7261 - acc: 0.5186
1728/1760 [============================>.] - ETA: 1s - loss: 0.7267 - acc: 0.5197
1760/1760 [==============================] - 78s 44ms/step - loss: 0.7277 - acc: 0.5182 - val_loss: 0.6558 - val_acc: 0.6122

Epoch 00001: val_acc improved from -inf to 0.61224, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window13/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:03 - loss: 0.7058 - acc: 0.5156
 128/1760 [=>............................] - ETA: 1:00 - loss: 0.6907 - acc: 0.5547
 192/1760 [==>...........................] - ETA: 57s - loss: 0.6831 - acc: 0.5677 
 256/1760 [===>..........................] - ETA: 54s - loss: 0.6816 - acc: 0.5820
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6915 - acc: 0.5750
 384/1760 [=====>........................] - ETA: 50s - loss: 0.6862 - acc: 0.5651
 448/1760 [======>.......................] - ETA: 49s - loss: 0.6921 - acc: 0.5580
 512/1760 [=======>......................] - ETA: 47s - loss: 0.6938 - acc: 0.5586
 576/1760 [========>.....................] - ETA: 44s - loss: 0.6925 - acc: 0.5590
 640/1760 [=========>....................] - ETA: 41s - loss: 0.6989 - acc: 0.5531
 704/1760 [===========>..................] - ETA: 39s - loss: 0.6981 - acc: 0.5469
 768/1760 [============>.................] - ETA: 37s - loss: 0.7004 - acc: 0.5404
 832/1760 [=============>................] - ETA: 34s - loss: 0.7012 - acc: 0.5409
 896/1760 [==============>...............] - ETA: 31s - loss: 0.7011 - acc: 0.5391
 960/1760 [===============>..............] - ETA: 29s - loss: 0.7004 - acc: 0.5417
1024/1760 [================>.............] - ETA: 26s - loss: 0.7023 - acc: 0.5400
1088/1760 [=================>............] - ETA: 24s - loss: 0.7012 - acc: 0.5395
1152/1760 [==================>...........] - ETA: 22s - loss: 0.7054 - acc: 0.5356
1216/1760 [===================>..........] - ETA: 19s - loss: 0.7059 - acc: 0.5378
1280/1760 [====================>.........] - ETA: 17s - loss: 0.7036 - acc: 0.5406
1344/1760 [=====================>........] - ETA: 15s - loss: 0.7042 - acc: 0.5387
1408/1760 [=======================>......] - ETA: 12s - loss: 0.7017 - acc: 0.5419
1472/1760 [========================>.....] - ETA: 10s - loss: 0.7011 - acc: 0.5435
1536/1760 [=========================>....] - ETA: 8s - loss: 0.7001 - acc: 0.5430 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6993 - acc: 0.5456
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6965 - acc: 0.5463
1728/1760 [============================>.] - ETA: 1s - loss: 0.6960 - acc: 0.5446
1760/1760 [==============================] - 68s 39ms/step - loss: 0.6956 - acc: 0.5460 - val_loss: 0.6417 - val_acc: 0.6531

Epoch 00002: val_acc improved from 0.61224 to 0.65306, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window13/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:02 - loss: 0.7792 - acc: 0.4375
 128/1760 [=>............................] - ETA: 59s - loss: 0.7252 - acc: 0.5234 
 192/1760 [==>...........................] - ETA: 56s - loss: 0.7179 - acc: 0.5208
 256/1760 [===>..........................] - ETA: 53s - loss: 0.7026 - acc: 0.5391
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6866 - acc: 0.5625
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6920 - acc: 0.5495
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6875 - acc: 0.5536
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6878 - acc: 0.5508
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6864 - acc: 0.5625
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6840 - acc: 0.5703
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6801 - acc: 0.5753
 768/1760 [============>.................] - ETA: 35s - loss: 0.6829 - acc: 0.5703
 832/1760 [=============>................] - ETA: 33s - loss: 0.6814 - acc: 0.5697
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6786 - acc: 0.5737
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6802 - acc: 0.5740
1024/1760 [================>.............] - ETA: 26s - loss: 0.6796 - acc: 0.5762
1088/1760 [=================>............] - ETA: 24s - loss: 0.6785 - acc: 0.5763
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6776 - acc: 0.5755
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6831 - acc: 0.5691
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6843 - acc: 0.5703
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6830 - acc: 0.5707
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6826 - acc: 0.5732
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6806 - acc: 0.5754
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6791 - acc: 0.5768 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6804 - acc: 0.5781
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6825 - acc: 0.5769
1728/1760 [============================>.] - ETA: 1s - loss: 0.6801 - acc: 0.5799
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6796 - acc: 0.5807 - val_loss: 0.7065 - val_acc: 0.5663

Epoch 00003: val_acc did not improve from 0.65306
Epoch 4/10

  64/1760 [>.............................] - ETA: 58s - loss: 0.6319 - acc: 0.6094
 128/1760 [=>............................] - ETA: 58s - loss: 0.6504 - acc: 0.5703
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6543 - acc: 0.5781
 256/1760 [===>..........................] - ETA: 53s - loss: 0.6576 - acc: 0.5859
 320/1760 [====>.........................] - ETA: 50s - loss: 0.6533 - acc: 0.5969
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6600 - acc: 0.5833
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6621 - acc: 0.5804
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6577 - acc: 0.5898
 576/1760 [========>.....................] - ETA: 41s - loss: 0.6562 - acc: 0.5920
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6580 - acc: 0.5891
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6525 - acc: 0.6009
 768/1760 [============>.................] - ETA: 35s - loss: 0.6527 - acc: 0.5990
 832/1760 [=============>................] - ETA: 32s - loss: 0.6607 - acc: 0.5889
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6592 - acc: 0.5949
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6591 - acc: 0.5958
1024/1760 [================>.............] - ETA: 26s - loss: 0.6571 - acc: 0.5977
1088/1760 [=================>............] - ETA: 24s - loss: 0.6592 - acc: 0.5956
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6574 - acc: 0.6024
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6582 - acc: 0.6003
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6576 - acc: 0.6023
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6569 - acc: 0.6064
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6526 - acc: 0.6094
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6544 - acc: 0.6094
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6547 - acc: 0.6087 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6581 - acc: 0.6081
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6591 - acc: 0.6064
1728/1760 [============================>.] - ETA: 1s - loss: 0.6597 - acc: 0.6065
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6605 - acc: 0.6057 - val_loss: 0.6267 - val_acc: 0.6735

Epoch 00004: val_acc improved from 0.65306 to 0.67347, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window13/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 5/10

  64/1760 [>.............................] - ETA: 58s - loss: 0.6437 - acc: 0.6406
 128/1760 [=>............................] - ETA: 56s - loss: 0.6602 - acc: 0.6016
 192/1760 [==>...........................] - ETA: 55s - loss: 0.6618 - acc: 0.6094
 256/1760 [===>..........................] - ETA: 52s - loss: 0.6477 - acc: 0.6289
 320/1760 [====>.........................] - ETA: 50s - loss: 0.6381 - acc: 0.6438
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6471 - acc: 0.6302
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6542 - acc: 0.6295
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6522 - acc: 0.6270
 576/1760 [========>.....................] - ETA: 41s - loss: 0.6481 - acc: 0.6337
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6533 - acc: 0.6297
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6544 - acc: 0.6293
 768/1760 [============>.................] - ETA: 35s - loss: 0.6561 - acc: 0.6263
 832/1760 [=============>................] - ETA: 32s - loss: 0.6564 - acc: 0.6298
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6557 - acc: 0.6339
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6566 - acc: 0.6312
1024/1760 [================>.............] - ETA: 26s - loss: 0.6531 - acc: 0.6338
1088/1760 [=================>............] - ETA: 23s - loss: 0.6569 - acc: 0.6305
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6567 - acc: 0.6276
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6573 - acc: 0.6308
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6562 - acc: 0.6273
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6558 - acc: 0.6265
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6555 - acc: 0.6257
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6560 - acc: 0.6277
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6561 - acc: 0.6263 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6569 - acc: 0.6256
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6612 - acc: 0.6202
1728/1760 [============================>.] - ETA: 1s - loss: 0.6591 - acc: 0.6227
1760/1760 [==============================] - 66s 38ms/step - loss: 0.6599 - acc: 0.6210 - val_loss: 0.6220 - val_acc: 0.6480

Epoch 00005: val_acc did not improve from 0.67347
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:00 - loss: 0.7129 - acc: 0.5625
 128/1760 [=>............................] - ETA: 57s - loss: 0.7124 - acc: 0.5312 
 192/1760 [==>...........................] - ETA: 55s - loss: 0.7086 - acc: 0.5417
 256/1760 [===>..........................] - ETA: 53s - loss: 0.6936 - acc: 0.5547
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6837 - acc: 0.5844
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6756 - acc: 0.5885
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6621 - acc: 0.6027
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6596 - acc: 0.6074
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6621 - acc: 0.6007
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6559 - acc: 0.6062
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6561 - acc: 0.6122
 768/1760 [============>.................] - ETA: 35s - loss: 0.6594 - acc: 0.6146
 832/1760 [=============>................] - ETA: 33s - loss: 0.6526 - acc: 0.6214
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6480 - acc: 0.6272
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6432 - acc: 0.6312
1024/1760 [================>.............] - ETA: 26s - loss: 0.6421 - acc: 0.6348
1088/1760 [=================>............] - ETA: 24s - loss: 0.6488 - acc: 0.6268
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6447 - acc: 0.6293
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6452 - acc: 0.6258
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6470 - acc: 0.6234
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6473 - acc: 0.6190
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6476 - acc: 0.6207
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6468 - acc: 0.6209
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6497 - acc: 0.6165 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6501 - acc: 0.6156
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6505 - acc: 0.6166
1728/1760 [============================>.] - ETA: 1s - loss: 0.6493 - acc: 0.6169
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6493 - acc: 0.6182 - val_loss: 0.6570 - val_acc: 0.5969

Epoch 00006: val_acc did not improve from 0.67347
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:01 - loss: 0.7028 - acc: 0.6094
 128/1760 [=>............................] - ETA: 58s - loss: 0.6940 - acc: 0.5938 
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6900 - acc: 0.5990
 256/1760 [===>..........................] - ETA: 55s - loss: 0.6747 - acc: 0.6055
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6571 - acc: 0.6281
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6661 - acc: 0.6120
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6686 - acc: 0.6094
 512/1760 [=======>......................] - ETA: 45s - loss: 0.6683 - acc: 0.6074
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6652 - acc: 0.6163
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6663 - acc: 0.6125
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6599 - acc: 0.6193
 768/1760 [============>.................] - ETA: 35s - loss: 0.6528 - acc: 0.6250
 832/1760 [=============>................] - ETA: 33s - loss: 0.6491 - acc: 0.6286
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6491 - acc: 0.6272
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6516 - acc: 0.6271
1024/1760 [================>.............] - ETA: 26s - loss: 0.6462 - acc: 0.6338
1088/1760 [=================>............] - ETA: 24s - loss: 0.6472 - acc: 0.6342
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6461 - acc: 0.6363
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6462 - acc: 0.6365
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6450 - acc: 0.6375
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6450 - acc: 0.6347
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6472 - acc: 0.6321
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6479 - acc: 0.6325
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6466 - acc: 0.6328 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6457 - acc: 0.6325
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6462 - acc: 0.6310
1728/1760 [============================>.] - ETA: 1s - loss: 0.6437 - acc: 0.6331
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6430 - acc: 0.6341 - val_loss: 0.7379 - val_acc: 0.5969

Epoch 00007: val_acc did not improve from 0.67347
Epoch 8/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.6081 - acc: 0.6250
 128/1760 [=>............................] - ETA: 57s - loss: 0.6391 - acc: 0.6172
 192/1760 [==>...........................] - ETA: 55s - loss: 0.6369 - acc: 0.6250
 256/1760 [===>..........................] - ETA: 53s - loss: 0.6432 - acc: 0.6406
 320/1760 [====>.........................] - ETA: 50s - loss: 0.6398 - acc: 0.6469
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6456 - acc: 0.6406
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6363 - acc: 0.6429
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6409 - acc: 0.6406
 576/1760 [========>.....................] - ETA: 41s - loss: 0.6497 - acc: 0.6319
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6479 - acc: 0.6312
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6445 - acc: 0.6364
 768/1760 [============>.................] - ETA: 35s - loss: 0.6518 - acc: 0.6289
 832/1760 [=============>................] - ETA: 32s - loss: 0.6504 - acc: 0.6262
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6547 - acc: 0.6250
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6512 - acc: 0.6281
1024/1760 [================>.............] - ETA: 26s - loss: 0.6468 - acc: 0.6309
1088/1760 [=================>............] - ETA: 24s - loss: 0.6402 - acc: 0.6379
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6399 - acc: 0.6363
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6394 - acc: 0.6357
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6360 - acc: 0.6406
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6355 - acc: 0.6436
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6366 - acc: 0.6413
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6388 - acc: 0.6399
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6407 - acc: 0.6341 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6394 - acc: 0.6350
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6393 - acc: 0.6340
1728/1760 [============================>.] - ETA: 1s - loss: 0.6401 - acc: 0.6348
1760/1760 [==============================] - 66s 38ms/step - loss: 0.6421 - acc: 0.6335 - val_loss: 0.6566 - val_acc: 0.6020

Epoch 00008: val_acc did not improve from 0.67347
Epoch 9/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.6679 - acc: 0.5938
 128/1760 [=>............................] - ETA: 57s - loss: 0.6696 - acc: 0.5859
 192/1760 [==>...........................] - ETA: 55s - loss: 0.6562 - acc: 0.6094
 256/1760 [===>..........................] - ETA: 53s - loss: 0.6438 - acc: 0.6289
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6412 - acc: 0.6375
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6398 - acc: 0.6354
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6310 - acc: 0.6451
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6317 - acc: 0.6406
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6345 - acc: 0.6424
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6291 - acc: 0.6484
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6284 - acc: 0.6463
 768/1760 [============>.................] - ETA: 35s - loss: 0.6339 - acc: 0.6406
 832/1760 [=============>................] - ETA: 33s - loss: 0.6370 - acc: 0.6382
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6370 - acc: 0.6395
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6393 - acc: 0.6354
1024/1760 [================>.............] - ETA: 26s - loss: 0.6399 - acc: 0.6338
1088/1760 [=================>............] - ETA: 24s - loss: 0.6433 - acc: 0.6314
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6446 - acc: 0.6285
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6435 - acc: 0.6308
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6423 - acc: 0.6320
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6423 - acc: 0.6317
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6422 - acc: 0.6300
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6419 - acc: 0.6318
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6411 - acc: 0.6289 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6400 - acc: 0.6319
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6400 - acc: 0.6292
1728/1760 [============================>.] - ETA: 1s - loss: 0.6438 - acc: 0.6273
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6430 - acc: 0.6273 - val_loss: 0.6263 - val_acc: 0.6429

Epoch 00009: val_acc did not improve from 0.67347
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:02 - loss: 0.6271 - acc: 0.6875
 128/1760 [=>............................] - ETA: 57s - loss: 0.6197 - acc: 0.6562 
 192/1760 [==>...........................] - ETA: 55s - loss: 0.6232 - acc: 0.6615
 256/1760 [===>..........................] - ETA: 54s - loss: 0.6230 - acc: 0.6758
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6222 - acc: 0.6687
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6220 - acc: 0.6641
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6389 - acc: 0.6429
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6529 - acc: 0.6328
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6456 - acc: 0.6441
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6464 - acc: 0.6453
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6466 - acc: 0.6420
 768/1760 [============>.................] - ETA: 35s - loss: 0.6507 - acc: 0.6393
 832/1760 [=============>................] - ETA: 33s - loss: 0.6495 - acc: 0.6370
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6480 - acc: 0.6339
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6412 - acc: 0.6438
1024/1760 [================>.............] - ETA: 26s - loss: 0.6381 - acc: 0.6484
1088/1760 [=================>............] - ETA: 24s - loss: 0.6355 - acc: 0.6517
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6382 - acc: 0.6502
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6399 - acc: 0.6497
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6371 - acc: 0.6523
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6365 - acc: 0.6533
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6355 - acc: 0.6548
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6334 - acc: 0.6562
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6333 - acc: 0.6549 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6328 - acc: 0.6538
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6345 - acc: 0.6514
1728/1760 [============================>.] - ETA: 1s - loss: 0.6342 - acc: 0.6505
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6345 - acc: 0.6511 - val_loss: 0.6096 - val_acc: 0.6888

Epoch 00010: val_acc improved from 0.67347 to 0.68878, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window13/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
样本个数 200
样本个数 400
window_select11-15.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 13s
128/400 [========>.....................] - ETA: 7s 
192/400 [=============>................] - ETA: 4s
256/400 [==================>...........] - ETA: 2s
320/400 [=======================>......] - ETA: 1s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 7s 18ms/step
loss: 0.6643767786026001
acc: 0.61
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f84bc371150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f84bc371150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f875c0dd610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f875c0dd610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da29e6e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8da29e6e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8dab0dd5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8dab0dd5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f847c319f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f847c319f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db3bc75d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db3bc75d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8dab0dd510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8dab0dd510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db3c3e050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8db3c3e050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f847c319450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f847c319450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f847c3177d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f847c3177d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f849c67a6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f849c67a6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8db3b138d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8db3b138d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f849c7c9890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f849c7c9890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f849c7ccf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f849c7ccf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f849c49cd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f849c49cd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84bc0983d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84bc0983d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f849c6e4510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f849c6e4510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f849c4d26d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f849c4d26d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f849c4e6310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f849c4e6310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f847c1d5e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f847c1d5e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f847c224e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f847c224e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f847c254750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f847c254750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f847463a410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f847463a410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8474607410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8474607410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f84745c1710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f84745c1710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f847463a690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f847463a690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8474607650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8474607650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84744d5b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84744d5b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f84745c9310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f84745c9310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f847450f190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f847450f190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84744d58d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84744d58d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f84742d1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f84742d1c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84741f0e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84741f0e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f843c7a0810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f843c7a0810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f843c737750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f843c737750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84741b9090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84741b9090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f843c7afad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f843c7afad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f843c7c3e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f843c7c3e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f843c7b8dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f843c7b8dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f843c3abc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f843c3abc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f843c6b1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f843c6b1c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8474230f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8474230f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f843c459dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f843c459dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f843c372110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f843c372110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f84347c13d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f84347c13d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f843c1581d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f843c1581d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f843c3724d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f843c3724d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84346e22d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84346e22d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8434633b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8434633b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f84345b44d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f84345b44d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f843c03b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f843c03b1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8434633bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8434633bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84343dd250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84343dd250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f84345c1190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f84345c1190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f843417a410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f843417a410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84340d6e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84340d6e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f84344e2310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f84344e2310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84342a7890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f84342a7890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8434260e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8434260e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f83fc6b07d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f83fc6b07d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f83fc717210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f83fc717210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f84340d6c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f84340d6c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f83fc55fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f83fc55fc50>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 4:04 - loss: 0.7581 - acc: 0.4844
 128/1760 [=>............................] - ETA: 2:25 - loss: 0.7746 - acc: 0.5000
 192/1760 [==>...........................] - ETA: 1:51 - loss: 0.7534 - acc: 0.5260
 256/1760 [===>..........................] - ETA: 1:33 - loss: 0.7345 - acc: 0.5352
 320/1760 [====>.........................] - ETA: 1:21 - loss: 0.7379 - acc: 0.5375
 384/1760 [=====>........................] - ETA: 1:13 - loss: 0.7498 - acc: 0.5234
 448/1760 [======>.......................] - ETA: 1:06 - loss: 0.7542 - acc: 0.5022
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.7442 - acc: 0.5059
 576/1760 [========>.....................] - ETA: 56s - loss: 0.7397 - acc: 0.5035 
 640/1760 [=========>....................] - ETA: 51s - loss: 0.7449 - acc: 0.5016
 704/1760 [===========>..................] - ETA: 48s - loss: 0.7459 - acc: 0.4957
 768/1760 [============>.................] - ETA: 44s - loss: 0.7415 - acc: 0.5013
 832/1760 [=============>................] - ETA: 40s - loss: 0.7377 - acc: 0.5024
 896/1760 [==============>...............] - ETA: 37s - loss: 0.7359 - acc: 0.5045
 960/1760 [===============>..............] - ETA: 34s - loss: 0.7354 - acc: 0.5031
1024/1760 [================>.............] - ETA: 31s - loss: 0.7319 - acc: 0.5049
1088/1760 [=================>............] - ETA: 28s - loss: 0.7314 - acc: 0.5037
1152/1760 [==================>...........] - ETA: 25s - loss: 0.7298 - acc: 0.5095
1216/1760 [===================>..........] - ETA: 22s - loss: 0.7275 - acc: 0.5115
1280/1760 [====================>.........] - ETA: 19s - loss: 0.7277 - acc: 0.5117
1344/1760 [=====================>........] - ETA: 17s - loss: 0.7257 - acc: 0.5156
1408/1760 [=======================>......] - ETA: 14s - loss: 0.7257 - acc: 0.5149
1472/1760 [========================>.....] - ETA: 11s - loss: 0.7258 - acc: 0.5156
1536/1760 [=========================>....] - ETA: 9s - loss: 0.7239 - acc: 0.5176 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.7221 - acc: 0.5194
1664/1760 [===========================>..] - ETA: 3s - loss: 0.7210 - acc: 0.5222
1728/1760 [============================>.] - ETA: 1s - loss: 0.7186 - acc: 0.5266
1760/1760 [==============================] - 75s 43ms/step - loss: 0.7206 - acc: 0.5256 - val_loss: 0.6822 - val_acc: 0.5306

Epoch 00001: val_acc improved from -inf to 0.53061, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window14/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:00 - loss: 0.7395 - acc: 0.4375
 128/1760 [=>............................] - ETA: 58s - loss: 0.7037 - acc: 0.5312 
 192/1760 [==>...........................] - ETA: 56s - loss: 0.7059 - acc: 0.5469
 256/1760 [===>..........................] - ETA: 53s - loss: 0.7147 - acc: 0.5469
 320/1760 [====>.........................] - ETA: 51s - loss: 0.7125 - acc: 0.5406
 384/1760 [=====>........................] - ETA: 49s - loss: 0.7088 - acc: 0.5312
 448/1760 [======>.......................] - ETA: 47s - loss: 0.7164 - acc: 0.5112
 512/1760 [=======>......................] - ETA: 45s - loss: 0.7138 - acc: 0.5195
 576/1760 [========>.....................] - ETA: 43s - loss: 0.7159 - acc: 0.5104
 640/1760 [=========>....................] - ETA: 41s - loss: 0.7142 - acc: 0.5141
 704/1760 [===========>..................] - ETA: 38s - loss: 0.7132 - acc: 0.5185
 768/1760 [============>.................] - ETA: 36s - loss: 0.7125 - acc: 0.5182
 832/1760 [=============>................] - ETA: 33s - loss: 0.7117 - acc: 0.5168
 896/1760 [==============>...............] - ETA: 31s - loss: 0.7141 - acc: 0.5123
 960/1760 [===============>..............] - ETA: 29s - loss: 0.7118 - acc: 0.5177
1024/1760 [================>.............] - ETA: 26s - loss: 0.7097 - acc: 0.5146
1088/1760 [=================>............] - ETA: 24s - loss: 0.7088 - acc: 0.5156
1152/1760 [==================>...........] - ETA: 22s - loss: 0.7058 - acc: 0.5191
1216/1760 [===================>..........] - ETA: 19s - loss: 0.7085 - acc: 0.5123
1280/1760 [====================>.........] - ETA: 17s - loss: 0.7083 - acc: 0.5141
1344/1760 [=====================>........] - ETA: 15s - loss: 0.7069 - acc: 0.5149
1408/1760 [=======================>......] - ETA: 12s - loss: 0.7084 - acc: 0.5142
1472/1760 [========================>.....] - ETA: 10s - loss: 0.7055 - acc: 0.5170
1536/1760 [=========================>....] - ETA: 8s - loss: 0.7054 - acc: 0.5156 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.7034 - acc: 0.5206
1664/1760 [===========================>..] - ETA: 3s - loss: 0.7019 - acc: 0.5228
1728/1760 [============================>.] - ETA: 1s - loss: 0.7033 - acc: 0.5203
1760/1760 [==============================] - 67s 38ms/step - loss: 0.7037 - acc: 0.5199 - val_loss: 0.6666 - val_acc: 0.6224

Epoch 00002: val_acc improved from 0.53061 to 0.62245, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window14/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.6183 - acc: 0.7188
 128/1760 [=>............................] - ETA: 59s - loss: 0.6622 - acc: 0.5781
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6815 - acc: 0.5573
 256/1760 [===>..........................] - ETA: 54s - loss: 0.6892 - acc: 0.5391
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6846 - acc: 0.5594
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6817 - acc: 0.5573
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6832 - acc: 0.5580
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6802 - acc: 0.5566
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6874 - acc: 0.5451
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6879 - acc: 0.5437
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6851 - acc: 0.5511
 768/1760 [============>.................] - ETA: 36s - loss: 0.6822 - acc: 0.5560
 832/1760 [=============>................] - ETA: 33s - loss: 0.6818 - acc: 0.5565
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6835 - acc: 0.5569
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6824 - acc: 0.5573
1024/1760 [================>.............] - ETA: 26s - loss: 0.6816 - acc: 0.5605
1088/1760 [=================>............] - ETA: 24s - loss: 0.6806 - acc: 0.5625
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6831 - acc: 0.5582
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6831 - acc: 0.5625
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6823 - acc: 0.5625
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6806 - acc: 0.5677
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6808 - acc: 0.5682
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6796 - acc: 0.5707
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6793 - acc: 0.5716 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6796 - acc: 0.5713
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6794 - acc: 0.5733
1728/1760 [============================>.] - ETA: 1s - loss: 0.6807 - acc: 0.5718
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6801 - acc: 0.5722 - val_loss: 0.6477 - val_acc: 0.6071

Epoch 00003: val_acc did not improve from 0.62245
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:00 - loss: 0.5718 - acc: 0.8125
 128/1760 [=>............................] - ETA: 57s - loss: 0.6456 - acc: 0.6562 
 192/1760 [==>...........................] - ETA: 55s - loss: 0.6423 - acc: 0.6510
 256/1760 [===>..........................] - ETA: 53s - loss: 0.6536 - acc: 0.6250
 320/1760 [====>.........................] - ETA: 50s - loss: 0.6568 - acc: 0.6312
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6674 - acc: 0.6250
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6603 - acc: 0.6317
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6654 - acc: 0.6270
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6571 - acc: 0.6424
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6612 - acc: 0.6328
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6597 - acc: 0.6321
 768/1760 [============>.................] - ETA: 35s - loss: 0.6619 - acc: 0.6276
 832/1760 [=============>................] - ETA: 33s - loss: 0.6623 - acc: 0.6262
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6637 - acc: 0.6228
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6665 - acc: 0.6135
1024/1760 [================>.............] - ETA: 26s - loss: 0.6682 - acc: 0.6094
1088/1760 [=================>............] - ETA: 24s - loss: 0.6684 - acc: 0.6085
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6695 - acc: 0.6068
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6702 - acc: 0.6053
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6689 - acc: 0.6055
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6669 - acc: 0.6094
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6701 - acc: 0.6037
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6686 - acc: 0.6060
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6676 - acc: 0.6055 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6685 - acc: 0.6031
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6690 - acc: 0.6004
1728/1760 [============================>.] - ETA: 1s - loss: 0.6679 - acc: 0.6019
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6666 - acc: 0.6023 - val_loss: 0.6525 - val_acc: 0.5765

Epoch 00004: val_acc did not improve from 0.62245
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:01 - loss: 0.6302 - acc: 0.5469
 128/1760 [=>............................] - ETA: 57s - loss: 0.6514 - acc: 0.5469 
 192/1760 [==>...........................] - ETA: 55s - loss: 0.6495 - acc: 0.5729
 256/1760 [===>..........................] - ETA: 52s - loss: 0.6480 - acc: 0.5820
 320/1760 [====>.........................] - ETA: 50s - loss: 0.6826 - acc: 0.5531
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6772 - acc: 0.5729
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6720 - acc: 0.5826
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6723 - acc: 0.5820
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6749 - acc: 0.5781
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6692 - acc: 0.5813
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6786 - acc: 0.5710
 768/1760 [============>.................] - ETA: 35s - loss: 0.6734 - acc: 0.5781
 832/1760 [=============>................] - ETA: 32s - loss: 0.6699 - acc: 0.5805
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6694 - acc: 0.5804
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6708 - acc: 0.5813
1024/1760 [================>.............] - ETA: 26s - loss: 0.6719 - acc: 0.5771
1088/1760 [=================>............] - ETA: 23s - loss: 0.6724 - acc: 0.5772
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6738 - acc: 0.5781
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6703 - acc: 0.5847
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6713 - acc: 0.5828
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6707 - acc: 0.5818
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6714 - acc: 0.5817
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6689 - acc: 0.5876
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6670 - acc: 0.5911 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6662 - acc: 0.5913
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6669 - acc: 0.5895
1728/1760 [============================>.] - ETA: 1s - loss: 0.6650 - acc: 0.5938
1760/1760 [==============================] - 66s 38ms/step - loss: 0.6660 - acc: 0.5932 - val_loss: 0.6622 - val_acc: 0.6020

Epoch 00005: val_acc did not improve from 0.62245
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:00 - loss: 0.6842 - acc: 0.6250
 128/1760 [=>............................] - ETA: 59s - loss: 0.6802 - acc: 0.5859 
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6763 - acc: 0.5885
 256/1760 [===>..........................] - ETA: 53s - loss: 0.6680 - acc: 0.6016
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6638 - acc: 0.5969
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6549 - acc: 0.6042
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6524 - acc: 0.6071
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6529 - acc: 0.6016
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6556 - acc: 0.6042
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6540 - acc: 0.6031
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6601 - acc: 0.5994
 768/1760 [============>.................] - ETA: 35s - loss: 0.6575 - acc: 0.6016
 832/1760 [=============>................] - ETA: 33s - loss: 0.6556 - acc: 0.6058
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6565 - acc: 0.6049
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6574 - acc: 0.6042
1024/1760 [================>.............] - ETA: 26s - loss: 0.6571 - acc: 0.6055
1088/1760 [=================>............] - ETA: 24s - loss: 0.6549 - acc: 0.6103
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6568 - acc: 0.6094
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6564 - acc: 0.6102
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6633 - acc: 0.6000
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6643 - acc: 0.5975
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6642 - acc: 0.5973
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6632 - acc: 0.5978
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6606 - acc: 0.6009 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6633 - acc: 0.5962
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6654 - acc: 0.5938
1728/1760 [============================>.] - ETA: 1s - loss: 0.6653 - acc: 0.5961
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6659 - acc: 0.5955 - val_loss: 0.6826 - val_acc: 0.5714

Epoch 00006: val_acc did not improve from 0.62245
Epoch 7/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.6876 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:00 - loss: 0.6588 - acc: 0.6094
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6453 - acc: 0.6250 
 256/1760 [===>..........................] - ETA: 54s - loss: 0.6446 - acc: 0.6289
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6475 - acc: 0.6188
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6507 - acc: 0.6094
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6444 - acc: 0.6228
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6502 - acc: 0.6133
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6525 - acc: 0.6042
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6530 - acc: 0.6094
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6499 - acc: 0.6094
 768/1760 [============>.................] - ETA: 35s - loss: 0.6442 - acc: 0.6146
 832/1760 [=============>................] - ETA: 33s - loss: 0.6423 - acc: 0.6190
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6386 - acc: 0.6239
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6374 - acc: 0.6302
1024/1760 [================>.............] - ETA: 26s - loss: 0.6448 - acc: 0.6230
1088/1760 [=================>............] - ETA: 24s - loss: 0.6505 - acc: 0.6149
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6528 - acc: 0.6146
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6543 - acc: 0.6143
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6514 - acc: 0.6211
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6473 - acc: 0.6272
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6496 - acc: 0.6250
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6476 - acc: 0.6270
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6474 - acc: 0.6250 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6469 - acc: 0.6281
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6433 - acc: 0.6328
1728/1760 [============================>.] - ETA: 1s - loss: 0.6451 - acc: 0.6291
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6451 - acc: 0.6307 - val_loss: 0.6840 - val_acc: 0.5765

Epoch 00007: val_acc did not improve from 0.62245
Epoch 8/10

  64/1760 [>.............................] - ETA: 58s - loss: 0.6242 - acc: 0.5938
 128/1760 [=>............................] - ETA: 57s - loss: 0.6754 - acc: 0.5547
 192/1760 [==>...........................] - ETA: 54s - loss: 0.6822 - acc: 0.5781
 256/1760 [===>..........................] - ETA: 53s - loss: 0.6532 - acc: 0.6055
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6553 - acc: 0.6062
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6692 - acc: 0.5964
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6605 - acc: 0.6116
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6499 - acc: 0.6191
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6518 - acc: 0.6163
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6571 - acc: 0.6094
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6613 - acc: 0.6037
 768/1760 [============>.................] - ETA: 35s - loss: 0.6558 - acc: 0.6146
 832/1760 [=============>................] - ETA: 33s - loss: 0.6550 - acc: 0.6130
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6513 - acc: 0.6172
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6539 - acc: 0.6146
1024/1760 [================>.............] - ETA: 26s - loss: 0.6501 - acc: 0.6182
1088/1760 [=================>............] - ETA: 24s - loss: 0.6443 - acc: 0.6232
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6442 - acc: 0.6207
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6426 - acc: 0.6225
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6451 - acc: 0.6180
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6459 - acc: 0.6168
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6470 - acc: 0.6165
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6499 - acc: 0.6135
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6482 - acc: 0.6178 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6461 - acc: 0.6194
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6420 - acc: 0.6238
1728/1760 [============================>.] - ETA: 1s - loss: 0.6414 - acc: 0.6256
1760/1760 [==============================] - 66s 38ms/step - loss: 0.6414 - acc: 0.6244 - val_loss: 0.6540 - val_acc: 0.6020

Epoch 00008: val_acc did not improve from 0.62245
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:01 - loss: 0.5952 - acc: 0.6875
 128/1760 [=>............................] - ETA: 58s - loss: 0.6233 - acc: 0.6641 
 192/1760 [==>...........................] - ETA: 55s - loss: 0.6546 - acc: 0.6406
 256/1760 [===>..........................] - ETA: 53s - loss: 0.6452 - acc: 0.6406
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6446 - acc: 0.6469
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6380 - acc: 0.6510
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6291 - acc: 0.6562
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6278 - acc: 0.6621
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6300 - acc: 0.6562
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6336 - acc: 0.6484
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6375 - acc: 0.6449
 768/1760 [============>.................] - ETA: 35s - loss: 0.6411 - acc: 0.6458
 832/1760 [=============>................] - ETA: 33s - loss: 0.6392 - acc: 0.6442
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6414 - acc: 0.6406
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6416 - acc: 0.6406
1024/1760 [================>.............] - ETA: 26s - loss: 0.6427 - acc: 0.6426
1088/1760 [=================>............] - ETA: 23s - loss: 0.6463 - acc: 0.6369
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6472 - acc: 0.6337
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6445 - acc: 0.6332
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6497 - acc: 0.6281
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6457 - acc: 0.6324
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6441 - acc: 0.6328
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6450 - acc: 0.6311
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6401 - acc: 0.6361 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6409 - acc: 0.6356
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6431 - acc: 0.6316
1728/1760 [============================>.] - ETA: 1s - loss: 0.6425 - acc: 0.6314
1760/1760 [==============================] - 66s 38ms/step - loss: 0.6414 - acc: 0.6335 - val_loss: 0.6833 - val_acc: 0.5510

Epoch 00009: val_acc did not improve from 0.62245
Epoch 10/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.6626 - acc: 0.6094
 128/1760 [=>............................] - ETA: 57s - loss: 0.6585 - acc: 0.6172
 192/1760 [==>...........................] - ETA: 55s - loss: 0.6484 - acc: 0.6094
 256/1760 [===>..........................] - ETA: 53s - loss: 0.6417 - acc: 0.6211
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6509 - acc: 0.6094
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6458 - acc: 0.6146
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6384 - acc: 0.6228
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6340 - acc: 0.6309
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6396 - acc: 0.6285
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6461 - acc: 0.6234
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6389 - acc: 0.6335
 768/1760 [============>.................] - ETA: 35s - loss: 0.6407 - acc: 0.6354
 832/1760 [=============>................] - ETA: 33s - loss: 0.6444 - acc: 0.6310
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6450 - acc: 0.6283
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6412 - acc: 0.6344
1024/1760 [================>.............] - ETA: 26s - loss: 0.6421 - acc: 0.6338
1088/1760 [=================>............] - ETA: 23s - loss: 0.6398 - acc: 0.6360
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6380 - acc: 0.6337
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6385 - acc: 0.6324
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6392 - acc: 0.6297
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6396 - acc: 0.6324
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6414 - acc: 0.6293
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6424 - acc: 0.6250
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6420 - acc: 0.6250 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6393 - acc: 0.6256
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6421 - acc: 0.6250
1728/1760 [============================>.] - ETA: 1s - loss: 0.6391 - acc: 0.6273
1760/1760 [==============================] - 66s 37ms/step - loss: 0.6370 - acc: 0.6290 - val_loss: 0.6426 - val_acc: 0.6429

Epoch 00010: val_acc improved from 0.62245 to 0.64286, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window14/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
样本个数 200
样本个数 400
window_select11-15.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 16s
128/400 [========>.....................] - ETA: 8s 
192/400 [=============>................] - ETA: 5s
256/400 [==================>...........] - ETA: 3s
320/400 [=======================>......] - ETA: 1s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 8s 19ms/step
loss: 0.6316694951057434
acc: 0.6425
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f837c0ab950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f837c0ab950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f837c048c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f837c048c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dab19c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8dab19c710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f873c05b550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f873c05b550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f835c148ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f835c148ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f837c069090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f837c069090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f873c05b0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f873c05b0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f835c6d8d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f835c6d8d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f837c058c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f837c058c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f835c4cee50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f835c4cee50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f835c438290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f835c438290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f835c4caa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f835c4caa90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f835c5b22d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f835c5b22d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f835c507450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f835c507450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f835c42e6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f835c42e6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f835c4c8850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f835c4c8850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f835c3cc890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f835c3cc890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f835c2dd250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f835c2dd250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f853c6aaf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f853c6aaf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f831c5a4410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f831c5a4410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f831c76cdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f831c76cdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f851c3d8490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f851c3d8490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f831c4aae10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f831c4aae10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f831c355910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f831c355910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f831c1fa750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f831c1fa750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f831c33fd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f831c33fd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f831c417e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f831c417e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f831c240a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f831c240a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f831c06c250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f831c06c250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82fc7bcf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82fc7bcf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f831c272ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f831c272ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f831c06c090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f831c06c090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82fc6d0510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82fc6d0510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82fc70c590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82fc70c590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82fc4e7590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82fc4e7590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82fc799310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82fc799310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82fc5a0850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82fc5a0850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82fc70f450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82fc70f450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82fc1d4290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82fc1d4290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82fc156050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82fc156050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82fc42cbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82fc42cbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82fc0cbe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82fc0cbe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82fc163b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82fc163b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82f4658710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82f4658710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82f4540610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82f4540610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82f4648710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82f4648710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82f4658c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82f4658c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82f440d110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82f440d110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82f43689d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82f43689d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82f425b310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82f425b310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82f4393490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82f4393490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82f45b0e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82f45b0e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82f40fd150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82f40fd150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82f43775d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82f43775d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82bc6f3ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82bc6f3ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82f420ccd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82f420ccd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82f42c3950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82f42c3950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82bc6be750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82bc6be750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82bc4eefd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f82bc4eefd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82bc74de50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f82bc74de50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82bc4d05d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82bc4d05d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82bc4ee610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f82bc4ee610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82bc28afd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f82bc28afd0>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 4:42 - loss: 0.7171 - acc: 0.5156
 128/1760 [=>............................] - ETA: 2:47 - loss: 0.7612 - acc: 0.4844
 192/1760 [==>...........................] - ETA: 2:05 - loss: 0.7566 - acc: 0.5000
 256/1760 [===>..........................] - ETA: 1:43 - loss: 0.7502 - acc: 0.4883
 320/1760 [====>.........................] - ETA: 1:29 - loss: 0.7454 - acc: 0.4906
 384/1760 [=====>........................] - ETA: 1:19 - loss: 0.7431 - acc: 0.4922
 448/1760 [======>.......................] - ETA: 1:11 - loss: 0.7466 - acc: 0.4888
 512/1760 [=======>......................] - ETA: 1:04 - loss: 0.7411 - acc: 0.4961
 576/1760 [========>.....................] - ETA: 59s - loss: 0.7352 - acc: 0.5017 
 640/1760 [=========>....................] - ETA: 54s - loss: 0.7287 - acc: 0.5109
 704/1760 [===========>..................] - ETA: 50s - loss: 0.7231 - acc: 0.5185
 768/1760 [============>.................] - ETA: 45s - loss: 0.7265 - acc: 0.5091
 832/1760 [=============>................] - ETA: 42s - loss: 0.7235 - acc: 0.5132
 896/1760 [==============>...............] - ETA: 38s - loss: 0.7190 - acc: 0.5179
 960/1760 [===============>..............] - ETA: 35s - loss: 0.7210 - acc: 0.5198
1024/1760 [================>.............] - ETA: 31s - loss: 0.7254 - acc: 0.5137
1088/1760 [=================>............] - ETA: 28s - loss: 0.7221 - acc: 0.5165
1152/1760 [==================>...........] - ETA: 25s - loss: 0.7238 - acc: 0.5165
1216/1760 [===================>..........] - ETA: 22s - loss: 0.7265 - acc: 0.5132
1280/1760 [====================>.........] - ETA: 20s - loss: 0.7263 - acc: 0.5141
1344/1760 [=====================>........] - ETA: 17s - loss: 0.7246 - acc: 0.5141
1408/1760 [=======================>......] - ETA: 14s - loss: 0.7271 - acc: 0.5107
1472/1760 [========================>.....] - ETA: 11s - loss: 0.7262 - acc: 0.5109
1536/1760 [=========================>....] - ETA: 9s - loss: 0.7244 - acc: 0.5124 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.7232 - acc: 0.5138
1664/1760 [===========================>..] - ETA: 3s - loss: 0.7233 - acc: 0.5114
1728/1760 [============================>.] - ETA: 1s - loss: 0.7230 - acc: 0.5116
1760/1760 [==============================] - 75s 43ms/step - loss: 0.7223 - acc: 0.5119 - val_loss: 0.6610 - val_acc: 0.5867

Epoch 00001: val_acc improved from -inf to 0.58673, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window15/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.6975 - acc: 0.6094
 128/1760 [=>............................] - ETA: 57s - loss: 0.6936 - acc: 0.5938
 192/1760 [==>...........................] - ETA: 55s - loss: 0.7149 - acc: 0.5573
 256/1760 [===>..........................] - ETA: 54s - loss: 0.7041 - acc: 0.5586
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6887 - acc: 0.5875
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6903 - acc: 0.5859
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6969 - acc: 0.5647
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6932 - acc: 0.5723
 576/1760 [========>.....................] - ETA: 41s - loss: 0.6932 - acc: 0.5764
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6985 - acc: 0.5625
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6990 - acc: 0.5639
 768/1760 [============>.................] - ETA: 35s - loss: 0.7035 - acc: 0.5547
 832/1760 [=============>................] - ETA: 32s - loss: 0.7066 - acc: 0.5529
 896/1760 [==============>...............] - ETA: 30s - loss: 0.7056 - acc: 0.5502
 960/1760 [===============>..............] - ETA: 28s - loss: 0.7062 - acc: 0.5531
1024/1760 [================>.............] - ETA: 25s - loss: 0.7076 - acc: 0.5488
1088/1760 [=================>............] - ETA: 23s - loss: 0.7094 - acc: 0.5506
1152/1760 [==================>...........] - ETA: 21s - loss: 0.7055 - acc: 0.5538
1216/1760 [===================>..........] - ETA: 19s - loss: 0.7047 - acc: 0.5559
1280/1760 [====================>.........] - ETA: 16s - loss: 0.7040 - acc: 0.5531
1344/1760 [=====================>........] - ETA: 14s - loss: 0.7056 - acc: 0.5491
1408/1760 [=======================>......] - ETA: 12s - loss: 0.7042 - acc: 0.5504
1472/1760 [========================>.....] - ETA: 10s - loss: 0.7020 - acc: 0.5523
1536/1760 [=========================>....] - ETA: 7s - loss: 0.7032 - acc: 0.5521 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.7025 - acc: 0.5556
1664/1760 [===========================>..] - ETA: 3s - loss: 0.7038 - acc: 0.5523
1728/1760 [============================>.] - ETA: 1s - loss: 0.7046 - acc: 0.5515
1760/1760 [==============================] - 65s 37ms/step - loss: 0.7034 - acc: 0.5540 - val_loss: 0.6639 - val_acc: 0.6276

Epoch 00002: val_acc improved from 0.58673 to 0.62755, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window15/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 58s - loss: 0.6348 - acc: 0.7031
 128/1760 [=>............................] - ETA: 57s - loss: 0.6813 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 54s - loss: 0.6965 - acc: 0.5833
 256/1760 [===>..........................] - ETA: 52s - loss: 0.6922 - acc: 0.5859
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6984 - acc: 0.5719
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6962 - acc: 0.5703
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6887 - acc: 0.5804
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6839 - acc: 0.5879
 576/1760 [========>.....................] - ETA: 41s - loss: 0.6786 - acc: 0.5955
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6794 - acc: 0.5938
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6822 - acc: 0.5838
 768/1760 [============>.................] - ETA: 35s - loss: 0.6811 - acc: 0.5807
 832/1760 [=============>................] - ETA: 33s - loss: 0.6835 - acc: 0.5745
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6859 - acc: 0.5714
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6863 - acc: 0.5719
1024/1760 [================>.............] - ETA: 26s - loss: 0.6862 - acc: 0.5713
1088/1760 [=================>............] - ETA: 23s - loss: 0.6886 - acc: 0.5653
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6925 - acc: 0.5582
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6935 - acc: 0.5576
1280/1760 [====================>.........] - ETA: 16s - loss: 0.6955 - acc: 0.5508
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6940 - acc: 0.5551
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6941 - acc: 0.5547
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6933 - acc: 0.5564
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6944 - acc: 0.5560 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6934 - acc: 0.5569
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6918 - acc: 0.5601
1728/1760 [============================>.] - ETA: 1s - loss: 0.6936 - acc: 0.5573
1760/1760 [==============================] - 66s 37ms/step - loss: 0.6932 - acc: 0.5574 - val_loss: 0.6707 - val_acc: 0.5765

Epoch 00003: val_acc did not improve from 0.62755
Epoch 4/10

  64/1760 [>.............................] - ETA: 57s - loss: 0.6214 - acc: 0.6875
 128/1760 [=>............................] - ETA: 56s - loss: 0.6481 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 53s - loss: 0.6616 - acc: 0.6146
 256/1760 [===>..........................] - ETA: 51s - loss: 0.6576 - acc: 0.6172
 320/1760 [====>.........................] - ETA: 49s - loss: 0.6557 - acc: 0.6219
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6553 - acc: 0.6276
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6590 - acc: 0.6250
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6564 - acc: 0.6270
 576/1760 [========>.....................] - ETA: 41s - loss: 0.6611 - acc: 0.6233
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6628 - acc: 0.6094
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6671 - acc: 0.6051
 768/1760 [============>.................] - ETA: 35s - loss: 0.6666 - acc: 0.6068
 832/1760 [=============>................] - ETA: 32s - loss: 0.6708 - acc: 0.6034
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6702 - acc: 0.6004
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6737 - acc: 0.5948
1024/1760 [================>.............] - ETA: 26s - loss: 0.6743 - acc: 0.5928
1088/1760 [=================>............] - ETA: 23s - loss: 0.6775 - acc: 0.5846
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6770 - acc: 0.5859
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6781 - acc: 0.5831
1280/1760 [====================>.........] - ETA: 16s - loss: 0.6772 - acc: 0.5875
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6763 - acc: 0.5885
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6762 - acc: 0.5874
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6766 - acc: 0.5849
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6744 - acc: 0.5866 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6775 - acc: 0.5825
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6777 - acc: 0.5823
1728/1760 [============================>.] - ETA: 1s - loss: 0.6789 - acc: 0.5810
1760/1760 [==============================] - 65s 37ms/step - loss: 0.6802 - acc: 0.5790 - val_loss: 0.6626 - val_acc: 0.6020

Epoch 00004: val_acc did not improve from 0.62755
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:00 - loss: 0.6939 - acc: 0.6094
 128/1760 [=>............................] - ETA: 56s - loss: 0.6880 - acc: 0.5859 
 192/1760 [==>...........................] - ETA: 54s - loss: 0.6863 - acc: 0.5938
 256/1760 [===>..........................] - ETA: 52s - loss: 0.6921 - acc: 0.5664
 320/1760 [====>.........................] - ETA: 49s - loss: 0.6933 - acc: 0.5437
 384/1760 [=====>........................] - ETA: 47s - loss: 0.6891 - acc: 0.5573
 448/1760 [======>.......................] - ETA: 45s - loss: 0.6932 - acc: 0.5558
 512/1760 [=======>......................] - ETA: 43s - loss: 0.6943 - acc: 0.5547
 576/1760 [========>.....................] - ETA: 41s - loss: 0.6888 - acc: 0.5642
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6861 - acc: 0.5734
 704/1760 [===========>..................] - ETA: 36s - loss: 0.6868 - acc: 0.5682
 768/1760 [============>.................] - ETA: 34s - loss: 0.6903 - acc: 0.5599
 832/1760 [=============>................] - ETA: 32s - loss: 0.6868 - acc: 0.5661
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6880 - acc: 0.5681
 960/1760 [===============>..............] - ETA: 27s - loss: 0.6908 - acc: 0.5656
1024/1760 [================>.............] - ETA: 25s - loss: 0.6897 - acc: 0.5664
1088/1760 [=================>............] - ETA: 23s - loss: 0.6845 - acc: 0.5790
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6851 - acc: 0.5738
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6843 - acc: 0.5748
1280/1760 [====================>.........] - ETA: 16s - loss: 0.6829 - acc: 0.5750
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6832 - acc: 0.5699
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6806 - acc: 0.5739
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6774 - acc: 0.5788
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6773 - acc: 0.5814 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6749 - acc: 0.5869
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6733 - acc: 0.5889
1728/1760 [============================>.] - ETA: 1s - loss: 0.6729 - acc: 0.5909
1760/1760 [==============================] - 65s 37ms/step - loss: 0.6718 - acc: 0.5920 - val_loss: 0.6398 - val_acc: 0.6633

Epoch 00005: val_acc improved from 0.62755 to 0.66327, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window15/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 6/10

  64/1760 [>.............................] - ETA: 58s - loss: 0.6076 - acc: 0.7031
 128/1760 [=>............................] - ETA: 56s - loss: 0.6313 - acc: 0.6875
 192/1760 [==>...........................] - ETA: 54s - loss: 0.6418 - acc: 0.6510
 256/1760 [===>..........................] - ETA: 52s - loss: 0.6488 - acc: 0.6328
 320/1760 [====>.........................] - ETA: 50s - loss: 0.6466 - acc: 0.6375
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6504 - acc: 0.6276
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6552 - acc: 0.6295
 512/1760 [=======>......................] - ETA: 43s - loss: 0.6506 - acc: 0.6328
 576/1760 [========>.....................] - ETA: 41s - loss: 0.6557 - acc: 0.6285
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6583 - acc: 0.6250
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6538 - acc: 0.6321
 768/1760 [============>.................] - ETA: 34s - loss: 0.6532 - acc: 0.6315
 832/1760 [=============>................] - ETA: 32s - loss: 0.6589 - acc: 0.6250
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6550 - acc: 0.6261
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6576 - acc: 0.6219
1024/1760 [================>.............] - ETA: 25s - loss: 0.6585 - acc: 0.6172
1088/1760 [=================>............] - ETA: 23s - loss: 0.6630 - acc: 0.6094
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6655 - acc: 0.6068
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6644 - acc: 0.6094
1280/1760 [====================>.........] - ETA: 16s - loss: 0.6643 - acc: 0.6125
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6665 - acc: 0.6101
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6698 - acc: 0.6065
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6754 - acc: 0.5999
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6745 - acc: 0.6022 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6744 - acc: 0.6006
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6719 - acc: 0.6040
1728/1760 [============================>.] - ETA: 1s - loss: 0.6705 - acc: 0.6059
1760/1760 [==============================] - 66s 37ms/step - loss: 0.6723 - acc: 0.6017 - val_loss: 0.6469 - val_acc: 0.5918

Epoch 00006: val_acc did not improve from 0.66327
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:01 - loss: 0.6853 - acc: 0.6562
 128/1760 [=>............................] - ETA: 58s - loss: 0.6878 - acc: 0.6328 
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6968 - acc: 0.5990
 256/1760 [===>..........................] - ETA: 54s - loss: 0.7036 - acc: 0.5938
 320/1760 [====>.........................] - ETA: 52s - loss: 0.7073 - acc: 0.5813
 384/1760 [=====>........................] - ETA: 49s - loss: 0.7037 - acc: 0.5755
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6965 - acc: 0.5781
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6900 - acc: 0.5742
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6867 - acc: 0.5799
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6890 - acc: 0.5781
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6900 - acc: 0.5753
 768/1760 [============>.................] - ETA: 35s - loss: 0.6853 - acc: 0.5846
 832/1760 [=============>................] - ETA: 33s - loss: 0.6829 - acc: 0.5853
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6810 - acc: 0.5859
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6813 - acc: 0.5906
1024/1760 [================>.............] - ETA: 26s - loss: 0.6828 - acc: 0.5879
1088/1760 [=================>............] - ETA: 23s - loss: 0.6813 - acc: 0.5882
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6797 - acc: 0.5903
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6824 - acc: 0.5863
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6808 - acc: 0.5898
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6808 - acc: 0.5900
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6798 - acc: 0.5881
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6825 - acc: 0.5836
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6801 - acc: 0.5879 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6794 - acc: 0.5881
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6800 - acc: 0.5853
1728/1760 [============================>.] - ETA: 1s - loss: 0.6795 - acc: 0.5856
1760/1760 [==============================] - 66s 38ms/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.6443 - val_acc: 0.5969

Epoch 00007: val_acc did not improve from 0.66327
Epoch 8/10

  64/1760 [>.............................] - ETA: 58s - loss: 0.6686 - acc: 0.5781
 128/1760 [=>............................] - ETA: 58s - loss: 0.6789 - acc: 0.5781
 192/1760 [==>...........................] - ETA: 58s - loss: 0.6657 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 55s - loss: 0.6596 - acc: 0.6133
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6504 - acc: 0.6312
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6473 - acc: 0.6302
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6494 - acc: 0.6205
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6511 - acc: 0.6191
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6488 - acc: 0.6198
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6509 - acc: 0.6188
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6541 - acc: 0.6151
 768/1760 [============>.................] - ETA: 35s - loss: 0.6519 - acc: 0.6211
 832/1760 [=============>................] - ETA: 33s - loss: 0.6489 - acc: 0.6250
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6493 - acc: 0.6261
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6475 - acc: 0.6271
1024/1760 [================>.............] - ETA: 26s - loss: 0.6498 - acc: 0.6221
1088/1760 [=================>............] - ETA: 24s - loss: 0.6493 - acc: 0.6186
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6475 - acc: 0.6189
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6467 - acc: 0.6201
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6432 - acc: 0.6250
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6450 - acc: 0.6220
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6484 - acc: 0.6200
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6505 - acc: 0.6202
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6492 - acc: 0.6224 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6525 - acc: 0.6212
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6545 - acc: 0.6202
1728/1760 [============================>.] - ETA: 1s - loss: 0.6538 - acc: 0.6204
1760/1760 [==============================] - 66s 38ms/step - loss: 0.6534 - acc: 0.6205 - val_loss: 0.6489 - val_acc: 0.6122

Epoch 00008: val_acc did not improve from 0.66327
Epoch 9/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.5912 - acc: 0.7344
 128/1760 [=>............................] - ETA: 56s - loss: 0.5729 - acc: 0.7734
 192/1760 [==>...........................] - ETA: 54s - loss: 0.5928 - acc: 0.7292
 256/1760 [===>..........................] - ETA: 52s - loss: 0.6134 - acc: 0.6875
 320/1760 [====>.........................] - ETA: 50s - loss: 0.6117 - acc: 0.6906
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6253 - acc: 0.6667
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6263 - acc: 0.6607
 512/1760 [=======>......................] - ETA: 43s - loss: 0.6345 - acc: 0.6523
 576/1760 [========>.....................] - ETA: 41s - loss: 0.6373 - acc: 0.6406
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6424 - acc: 0.6281
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6432 - acc: 0.6335
 768/1760 [============>.................] - ETA: 35s - loss: 0.6451 - acc: 0.6315
 832/1760 [=============>................] - ETA: 32s - loss: 0.6437 - acc: 0.6298
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6440 - acc: 0.6306
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6468 - acc: 0.6229
1024/1760 [================>.............] - ETA: 25s - loss: 0.6431 - acc: 0.6309
1088/1760 [=================>............] - ETA: 23s - loss: 0.6425 - acc: 0.6314
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6453 - acc: 0.6267
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6429 - acc: 0.6283
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6450 - acc: 0.6250
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6433 - acc: 0.6272
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6454 - acc: 0.6264
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6456 - acc: 0.6257
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6443 - acc: 0.6283 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6424 - acc: 0.6319
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6420 - acc: 0.6310
1728/1760 [============================>.] - ETA: 1s - loss: 0.6463 - acc: 0.6267
1760/1760 [==============================] - 66s 38ms/step - loss: 0.6466 - acc: 0.6256 - val_loss: 0.6565 - val_acc: 0.6276

Epoch 00009: val_acc did not improve from 0.66327
Epoch 10/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.6524 - acc: 0.6562
 128/1760 [=>............................] - ETA: 58s - loss: 0.6639 - acc: 0.6172
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6386 - acc: 0.6406
 256/1760 [===>..........................] - ETA: 53s - loss: 0.6286 - acc: 0.6523
 320/1760 [====>.........................] - ETA: 50s - loss: 0.6221 - acc: 0.6656
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6291 - acc: 0.6458
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6358 - acc: 0.6429
 512/1760 [=======>......................] - ETA: 43s - loss: 0.6381 - acc: 0.6406
 576/1760 [========>.....................] - ETA: 41s - loss: 0.6332 - acc: 0.6441
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6376 - acc: 0.6422
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6397 - acc: 0.6406
 768/1760 [============>.................] - ETA: 34s - loss: 0.6433 - acc: 0.6432
 832/1760 [=============>................] - ETA: 32s - loss: 0.6404 - acc: 0.6430
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6448 - acc: 0.6339
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6424 - acc: 0.6354
1024/1760 [================>.............] - ETA: 25s - loss: 0.6451 - acc: 0.6318
1088/1760 [=================>............] - ETA: 23s - loss: 0.6429 - acc: 0.6324
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6447 - acc: 0.6319
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6440 - acc: 0.6332
1280/1760 [====================>.........] - ETA: 16s - loss: 0.6445 - acc: 0.6320
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6430 - acc: 0.6362
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6430 - acc: 0.6385
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6402 - acc: 0.6427
1536/1760 [=========================>....] - ETA: 7s - loss: 0.6409 - acc: 0.6400 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6405 - acc: 0.6400
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6413 - acc: 0.6406
1728/1760 [============================>.] - ETA: 1s - loss: 0.6434 - acc: 0.6372
1760/1760 [==============================] - 65s 37ms/step - loss: 0.6443 - acc: 0.6364 - val_loss: 0.6240 - val_acc: 0.6480

Epoch 00010: val_acc did not improve from 0.66327
样本个数 200
样本个数 400
window_select11-15.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 28s
128/400 [========>.....................] - ETA: 14s
192/400 [=============>................] - ETA: 8s 
256/400 [==================>...........] - ETA: 5s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 12s 30ms/step
loss: 0.6619176006317139
acc: 0.62
