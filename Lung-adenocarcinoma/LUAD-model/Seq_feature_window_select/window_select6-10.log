nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 978
样本个数 1956
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f86824c2950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f86824c2950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f86ac9f2d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f86ac9f2d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86824307d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86824307d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8682471a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8682471a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86822e3cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86822e3cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f868246d4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f868246d4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8682436210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8682436210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8682452110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8682452110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f868214ca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f868214ca90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f867a04b890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f867a04b890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86821eadd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86821eadd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f868214cc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f868214cc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679eea150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679eea150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8679edfe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8679edfe10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8679e508d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8679e508d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86823215d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86823215d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f867a04bb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f867a04bb10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679d7f990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679d7f990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8679b20ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8679b20ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8679b42610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8679b42610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679b38e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679b38e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8679c9b490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8679c9b490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679a90d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679a90d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8679a74250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8679a74250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8671792390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8671792390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86823bfe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86823bfe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86717b2950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86717b2950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86716d3810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86716d3810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8671658490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8671658490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f867170ec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f867170ec90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8671453e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8671453e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86717a80d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86717a80d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86712190d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86712190d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f867144fd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f867144fd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86711f8e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f86711f8e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8671176c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8671176c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8671292610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8671292610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f866905d050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f866905d050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8668e90310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8668e90310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8668e0add0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8668e0add0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8668c28750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8668c28750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8668e90cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8668e90cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8668d6e950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8668d6e950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8668b59f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8668b59f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8668a8b990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8668a8b990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8668b58e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8668b58e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8668b59290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8668b59290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8668b29d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8668b29d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8660876950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8660876950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8660873650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8660873650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679e1d110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679e1d110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f866083af10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f866083af10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f866087af90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f866087af90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86604f7610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f86604f7610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8660411f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8660411f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679a90a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8679a90a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86604f7c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f86604f7c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f866073d650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f866073d650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8660406750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8660406750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f866017c790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f866017c790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8660750850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8660750850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f866022e210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f866022e210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8657ff2a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8657ff2a50>>: AttributeError: module 'gast' has no attribute 'Str'
2023-01-10 22:10:34.303160: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2023-01-10 22:10:34.355020: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2023-01-10 22:10:34.415219: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562e7842eee0 executing computations on platform Host. Devices:
2023-01-10 22:10:34.415355: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2023-01-10 22:10:34.957530: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window_select6-10.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 3:23 - loss: 0.8154 - acc: 0.4375
 128/1760 [=>............................] - ETA: 2:28 - loss: 0.8046 - acc: 0.4219
 192/1760 [==>...........................] - ETA: 2:07 - loss: 0.7843 - acc: 0.4531
 256/1760 [===>..........................] - ETA: 1:51 - loss: 0.7622 - acc: 0.4922
 320/1760 [====>.........................] - ETA: 1:42 - loss: 0.7618 - acc: 0.5000
 384/1760 [=====>........................] - ETA: 1:36 - loss: 0.7638 - acc: 0.4896
 448/1760 [======>.......................] - ETA: 1:29 - loss: 0.7678 - acc: 0.4799
 512/1760 [=======>......................] - ETA: 1:23 - loss: 0.7689 - acc: 0.4785
 576/1760 [========>.....................] - ETA: 1:17 - loss: 0.7649 - acc: 0.4792
 640/1760 [=========>....................] - ETA: 1:13 - loss: 0.7620 - acc: 0.4828
 704/1760 [===========>..................] - ETA: 1:07 - loss: 0.7567 - acc: 0.4901
 768/1760 [============>.................] - ETA: 1:03 - loss: 0.7528 - acc: 0.4961
 832/1760 [=============>................] - ETA: 1:01 - loss: 0.7485 - acc: 0.5012
 896/1760 [==============>...............] - ETA: 56s - loss: 0.7466 - acc: 0.5089 
 960/1760 [===============>..............] - ETA: 51s - loss: 0.7421 - acc: 0.5115
1024/1760 [================>.............] - ETA: 47s - loss: 0.7410 - acc: 0.5098
1088/1760 [=================>............] - ETA: 43s - loss: 0.7389 - acc: 0.5110
1152/1760 [==================>...........] - ETA: 38s - loss: 0.7402 - acc: 0.5078
1216/1760 [===================>..........] - ETA: 34s - loss: 0.7392 - acc: 0.5058
1280/1760 [====================>.........] - ETA: 30s - loss: 0.7354 - acc: 0.5117
1344/1760 [=====================>........] - ETA: 25s - loss: 0.7321 - acc: 0.5164
1408/1760 [=======================>......] - ETA: 21s - loss: 0.7326 - acc: 0.5163
1472/1760 [========================>.....] - ETA: 17s - loss: 0.7312 - acc: 0.5190
1536/1760 [=========================>....] - ETA: 13s - loss: 0.7309 - acc: 0.5208
1600/1760 [==========================>...] - ETA: 9s - loss: 0.7287 - acc: 0.5225 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7265 - acc: 0.5252
1728/1760 [============================>.] - ETA: 1s - loss: 0.7270 - acc: 0.5249
1760/1760 [==============================] - 113s 64ms/step - loss: 0.7273 - acc: 0.5261 - val_loss: 0.6597 - val_acc: 0.6378

Epoch 00001: val_acc improved from -inf to 0.63776, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window06/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:39 - loss: 0.7465 - acc: 0.3594
 128/1760 [=>............................] - ETA: 1:30 - loss: 0.7351 - acc: 0.4062
 192/1760 [==>...........................] - ETA: 1:29 - loss: 0.7211 - acc: 0.4479
 256/1760 [===>..........................] - ETA: 1:22 - loss: 0.7132 - acc: 0.4922
 320/1760 [====>.........................] - ETA: 1:16 - loss: 0.7190 - acc: 0.4875
 384/1760 [=====>........................] - ETA: 1:12 - loss: 0.7137 - acc: 0.5026
 448/1760 [======>.......................] - ETA: 1:08 - loss: 0.7254 - acc: 0.5000
 512/1760 [=======>......................] - ETA: 1:05 - loss: 0.7178 - acc: 0.5137
 576/1760 [========>.....................] - ETA: 1:02 - loss: 0.7135 - acc: 0.5278
 640/1760 [=========>....................] - ETA: 59s - loss: 0.7130 - acc: 0.5297 
 704/1760 [===========>..................] - ETA: 56s - loss: 0.7134 - acc: 0.5312
 768/1760 [============>.................] - ETA: 52s - loss: 0.7125 - acc: 0.5286
 832/1760 [=============>................] - ETA: 49s - loss: 0.7092 - acc: 0.5337
 896/1760 [==============>...............] - ETA: 45s - loss: 0.7104 - acc: 0.5268
 960/1760 [===============>..............] - ETA: 41s - loss: 0.7115 - acc: 0.5219
1024/1760 [================>.............] - ETA: 38s - loss: 0.7097 - acc: 0.5244
1088/1760 [=================>............] - ETA: 35s - loss: 0.7097 - acc: 0.5211
1152/1760 [==================>...........] - ETA: 31s - loss: 0.7058 - acc: 0.5304
1216/1760 [===================>..........] - ETA: 28s - loss: 0.7037 - acc: 0.5312
1280/1760 [====================>.........] - ETA: 25s - loss: 0.7043 - acc: 0.5336
1344/1760 [=====================>........] - ETA: 22s - loss: 0.7043 - acc: 0.5335
1408/1760 [=======================>......] - ETA: 18s - loss: 0.7060 - acc: 0.5320
1472/1760 [========================>.....] - ETA: 15s - loss: 0.7062 - acc: 0.5333
1536/1760 [=========================>....] - ETA: 12s - loss: 0.7063 - acc: 0.5339
1600/1760 [==========================>...] - ETA: 8s - loss: 0.7058 - acc: 0.5375 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7060 - acc: 0.5373
1728/1760 [============================>.] - ETA: 1s - loss: 0.7072 - acc: 0.5359
1760/1760 [==============================] - 105s 60ms/step - loss: 0.7067 - acc: 0.5375 - val_loss: 0.6491 - val_acc: 0.6327

Epoch 00002: val_acc did not improve from 0.63776
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:57 - loss: 0.6915 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:49 - loss: 0.6816 - acc: 0.5859
 192/1760 [==>...........................] - ETA: 1:41 - loss: 0.6812 - acc: 0.5625
 256/1760 [===>..........................] - ETA: 1:39 - loss: 0.6959 - acc: 0.5430
 320/1760 [====>.........................] - ETA: 1:34 - loss: 0.6897 - acc: 0.5469
 384/1760 [=====>........................] - ETA: 1:31 - loss: 0.6904 - acc: 0.5573
 448/1760 [======>.......................] - ETA: 1:29 - loss: 0.6838 - acc: 0.5580
 512/1760 [=======>......................] - ETA: 1:25 - loss: 0.6799 - acc: 0.5664
 576/1760 [========>.....................] - ETA: 1:20 - loss: 0.6766 - acc: 0.5747
 640/1760 [=========>....................] - ETA: 1:15 - loss: 0.6792 - acc: 0.5703
 704/1760 [===========>..................] - ETA: 1:11 - loss: 0.6779 - acc: 0.5682
 768/1760 [============>.................] - ETA: 1:07 - loss: 0.6801 - acc: 0.5638
 832/1760 [=============>................] - ETA: 1:03 - loss: 0.6802 - acc: 0.5625
 896/1760 [==============>...............] - ETA: 58s - loss: 0.6835 - acc: 0.5558 
 960/1760 [===============>..............] - ETA: 54s - loss: 0.6845 - acc: 0.5542
1024/1760 [================>.............] - ETA: 50s - loss: 0.6856 - acc: 0.5596
1088/1760 [=================>............] - ETA: 46s - loss: 0.6825 - acc: 0.5643
1152/1760 [==================>...........] - ETA: 41s - loss: 0.6821 - acc: 0.5642
1216/1760 [===================>..........] - ETA: 37s - loss: 0.6811 - acc: 0.5674
1280/1760 [====================>.........] - ETA: 32s - loss: 0.6783 - acc: 0.5734
1344/1760 [=====================>........] - ETA: 28s - loss: 0.6784 - acc: 0.5714
1408/1760 [=======================>......] - ETA: 24s - loss: 0.6805 - acc: 0.5710
1472/1760 [========================>.....] - ETA: 20s - loss: 0.6784 - acc: 0.5727
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6762 - acc: 0.5762
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6766 - acc: 0.5750
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6740 - acc: 0.5775 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6766 - acc: 0.5747
1760/1760 [==============================] - 129s 73ms/step - loss: 0.6762 - acc: 0.5750 - val_loss: 0.6777 - val_acc: 0.5867

Epoch 00003: val_acc did not improve from 0.63776
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:52 - loss: 0.7046 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:53 - loss: 0.7335 - acc: 0.5391
 192/1760 [==>...........................] - ETA: 1:53 - loss: 0.7142 - acc: 0.5260
 256/1760 [===>..........................] - ETA: 1:54 - loss: 0.7034 - acc: 0.5586
 320/1760 [====>.........................] - ETA: 1:46 - loss: 0.7154 - acc: 0.5281
 384/1760 [=====>........................] - ETA: 1:42 - loss: 0.7064 - acc: 0.5391
 448/1760 [======>.......................] - ETA: 1:36 - loss: 0.7019 - acc: 0.5379
 512/1760 [=======>......................] - ETA: 1:31 - loss: 0.6965 - acc: 0.5430
 576/1760 [========>.....................] - ETA: 1:26 - loss: 0.6933 - acc: 0.5521
 640/1760 [=========>....................] - ETA: 1:21 - loss: 0.6978 - acc: 0.5531
 704/1760 [===========>..................] - ETA: 1:17 - loss: 0.6968 - acc: 0.5582
 768/1760 [============>.................] - ETA: 1:12 - loss: 0.6986 - acc: 0.5612
 832/1760 [=============>................] - ETA: 1:07 - loss: 0.6956 - acc: 0.5661
 896/1760 [==============>...............] - ETA: 1:02 - loss: 0.6944 - acc: 0.5625
 960/1760 [===============>..............] - ETA: 57s - loss: 0.6958 - acc: 0.5604 
1024/1760 [================>.............] - ETA: 53s - loss: 0.6934 - acc: 0.5674
1088/1760 [=================>............] - ETA: 49s - loss: 0.6951 - acc: 0.5634
1152/1760 [==================>...........] - ETA: 44s - loss: 0.6982 - acc: 0.5590
1216/1760 [===================>..........] - ETA: 39s - loss: 0.6982 - acc: 0.5584
1280/1760 [====================>.........] - ETA: 35s - loss: 0.6998 - acc: 0.5547
1344/1760 [=====================>........] - ETA: 30s - loss: 0.6988 - acc: 0.5543
1408/1760 [=======================>......] - ETA: 25s - loss: 0.6978 - acc: 0.5554
1472/1760 [========================>.....] - ETA: 20s - loss: 0.6963 - acc: 0.5557
1536/1760 [=========================>....] - ETA: 16s - loss: 0.6947 - acc: 0.5599
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6964 - acc: 0.5575
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6954 - acc: 0.5577 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6942 - acc: 0.5573
1760/1760 [==============================] - 132s 75ms/step - loss: 0.6930 - acc: 0.5585 - val_loss: 0.6520 - val_acc: 0.6020

Epoch 00004: val_acc did not improve from 0.63776
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:54 - loss: 0.7247 - acc: 0.5000
 128/1760 [=>............................] - ETA: 1:48 - loss: 0.6969 - acc: 0.5000
 192/1760 [==>...........................] - ETA: 1:45 - loss: 0.6974 - acc: 0.5052
 256/1760 [===>..........................] - ETA: 1:46 - loss: 0.6901 - acc: 0.5430
 320/1760 [====>.........................] - ETA: 1:41 - loss: 0.6771 - acc: 0.5594
 384/1760 [=====>........................] - ETA: 1:36 - loss: 0.6682 - acc: 0.5729
 448/1760 [======>.......................] - ETA: 1:31 - loss: 0.6643 - acc: 0.5871
 512/1760 [=======>......................] - ETA: 1:26 - loss: 0.6699 - acc: 0.5781
 576/1760 [========>.....................] - ETA: 1:22 - loss: 0.6653 - acc: 0.5885
 640/1760 [=========>....................] - ETA: 1:17 - loss: 0.6609 - acc: 0.5938
 704/1760 [===========>..................] - ETA: 1:13 - loss: 0.6641 - acc: 0.5881
 768/1760 [============>.................] - ETA: 1:08 - loss: 0.6652 - acc: 0.5911
 832/1760 [=============>................] - ETA: 1:03 - loss: 0.6663 - acc: 0.5925
 896/1760 [==============>...............] - ETA: 59s - loss: 0.6699 - acc: 0.5859 
 960/1760 [===============>..............] - ETA: 56s - loss: 0.6687 - acc: 0.5854
1024/1760 [================>.............] - ETA: 51s - loss: 0.6702 - acc: 0.5840
1088/1760 [=================>............] - ETA: 47s - loss: 0.6692 - acc: 0.5827
1152/1760 [==================>...........] - ETA: 42s - loss: 0.6689 - acc: 0.5833
1216/1760 [===================>..........] - ETA: 37s - loss: 0.6664 - acc: 0.5888
1280/1760 [====================>.........] - ETA: 33s - loss: 0.6717 - acc: 0.5844
1344/1760 [=====================>........] - ETA: 28s - loss: 0.6724 - acc: 0.5856
1408/1760 [=======================>......] - ETA: 24s - loss: 0.6724 - acc: 0.5874
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6711 - acc: 0.5904
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6730 - acc: 0.5892
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6730 - acc: 0.5906
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6738 - acc: 0.5883 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6732 - acc: 0.5874
1760/1760 [==============================] - 128s 73ms/step - loss: 0.6749 - acc: 0.5847 - val_loss: 0.6446 - val_acc: 0.5918

Epoch 00005: val_acc did not improve from 0.63776
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:57 - loss: 0.7115 - acc: 0.5469
 128/1760 [=>............................] - ETA: 1:58 - loss: 0.7103 - acc: 0.5156
 192/1760 [==>...........................] - ETA: 1:47 - loss: 0.7133 - acc: 0.5208
 256/1760 [===>..........................] - ETA: 1:44 - loss: 0.6997 - acc: 0.5508
 320/1760 [====>.........................] - ETA: 1:39 - loss: 0.7032 - acc: 0.5437
 384/1760 [=====>........................] - ETA: 1:34 - loss: 0.6839 - acc: 0.5599
 448/1760 [======>.......................] - ETA: 1:31 - loss: 0.6750 - acc: 0.5737
 512/1760 [=======>......................] - ETA: 1:25 - loss: 0.6759 - acc: 0.5781
 576/1760 [========>.....................] - ETA: 1:22 - loss: 0.6694 - acc: 0.5990
 640/1760 [=========>....................] - ETA: 1:17 - loss: 0.6692 - acc: 0.6000
 704/1760 [===========>..................] - ETA: 1:13 - loss: 0.6663 - acc: 0.6037
 768/1760 [============>.................] - ETA: 1:08 - loss: 0.6640 - acc: 0.6094
 832/1760 [=============>................] - ETA: 1:03 - loss: 0.6568 - acc: 0.6202
 896/1760 [==============>...............] - ETA: 59s - loss: 0.6598 - acc: 0.6127 
 960/1760 [===============>..............] - ETA: 54s - loss: 0.6619 - acc: 0.6094
1024/1760 [================>.............] - ETA: 50s - loss: 0.6629 - acc: 0.6104
1088/1760 [=================>............] - ETA: 46s - loss: 0.6617 - acc: 0.6140
1152/1760 [==================>...........] - ETA: 42s - loss: 0.6610 - acc: 0.6120
1216/1760 [===================>..........] - ETA: 37s - loss: 0.6635 - acc: 0.6053
1280/1760 [====================>.........] - ETA: 33s - loss: 0.6599 - acc: 0.6125
1344/1760 [=====================>........] - ETA: 29s - loss: 0.6582 - acc: 0.6153
1408/1760 [=======================>......] - ETA: 24s - loss: 0.6603 - acc: 0.6122
1472/1760 [========================>.....] - ETA: 20s - loss: 0.6603 - acc: 0.6155
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6623 - acc: 0.6152
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6648 - acc: 0.6125
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6616 - acc: 0.6160 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6639 - acc: 0.6128
1760/1760 [==============================] - 128s 73ms/step - loss: 0.6623 - acc: 0.6142 - val_loss: 0.6506 - val_acc: 0.5969

Epoch 00006: val_acc did not improve from 0.63776
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:52 - loss: 0.6132 - acc: 0.6875
 128/1760 [=>............................] - ETA: 1:49 - loss: 0.6271 - acc: 0.6562
 192/1760 [==>...........................] - ETA: 1:46 - loss: 0.6387 - acc: 0.6458
 256/1760 [===>..........................] - ETA: 1:44 - loss: 0.6452 - acc: 0.6172
 320/1760 [====>.........................] - ETA: 1:38 - loss: 0.6492 - acc: 0.6156
 384/1760 [=====>........................] - ETA: 1:33 - loss: 0.6511 - acc: 0.6120
 448/1760 [======>.......................] - ETA: 1:28 - loss: 0.6592 - acc: 0.5982
 512/1760 [=======>......................] - ETA: 1:24 - loss: 0.6631 - acc: 0.5957
 576/1760 [========>.....................] - ETA: 1:20 - loss: 0.6641 - acc: 0.5920
 640/1760 [=========>....................] - ETA: 1:17 - loss: 0.6707 - acc: 0.5813
 704/1760 [===========>..................] - ETA: 1:13 - loss: 0.6634 - acc: 0.5909
 768/1760 [============>.................] - ETA: 1:08 - loss: 0.6666 - acc: 0.5859
 832/1760 [=============>................] - ETA: 1:04 - loss: 0.6620 - acc: 0.5913
 896/1760 [==============>...............] - ETA: 59s - loss: 0.6613 - acc: 0.5938 
 960/1760 [===============>..............] - ETA: 55s - loss: 0.6598 - acc: 0.5958
1024/1760 [================>.............] - ETA: 50s - loss: 0.6590 - acc: 0.5986
1088/1760 [=================>............] - ETA: 46s - loss: 0.6636 - acc: 0.5947
1152/1760 [==================>...........] - ETA: 41s - loss: 0.6643 - acc: 0.5946
1216/1760 [===================>..........] - ETA: 37s - loss: 0.6641 - acc: 0.5938
1280/1760 [====================>.........] - ETA: 33s - loss: 0.6633 - acc: 0.5969
1344/1760 [=====================>........] - ETA: 28s - loss: 0.6621 - acc: 0.5967
1408/1760 [=======================>......] - ETA: 24s - loss: 0.6624 - acc: 0.5973
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6628 - acc: 0.5978
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6618 - acc: 0.6022
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6614 - acc: 0.6025
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6615 - acc: 0.6028 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6598 - acc: 0.6036
1760/1760 [==============================] - 125s 71ms/step - loss: 0.6599 - acc: 0.6040 - val_loss: 0.6492 - val_acc: 0.6224

Epoch 00007: val_acc did not improve from 0.63776
Epoch 8/10

  64/1760 [>.............................] - ETA: 2:01 - loss: 0.5853 - acc: 0.7031
 128/1760 [=>............................] - ETA: 1:44 - loss: 0.6147 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 1:38 - loss: 0.6082 - acc: 0.6510
 256/1760 [===>..........................] - ETA: 1:38 - loss: 0.6061 - acc: 0.6680
 320/1760 [====>.........................] - ETA: 1:34 - loss: 0.5991 - acc: 0.6813
 384/1760 [=====>........................] - ETA: 1:31 - loss: 0.6171 - acc: 0.6667
 448/1760 [======>.......................] - ETA: 1:25 - loss: 0.6430 - acc: 0.6451
 512/1760 [=======>......................] - ETA: 1:22 - loss: 0.6375 - acc: 0.6465
 576/1760 [========>.....................] - ETA: 1:17 - loss: 0.6319 - acc: 0.6545
 640/1760 [=========>....................] - ETA: 1:13 - loss: 0.6324 - acc: 0.6562
 704/1760 [===========>..................] - ETA: 1:09 - loss: 0.6338 - acc: 0.6477
 768/1760 [============>.................] - ETA: 1:04 - loss: 0.6344 - acc: 0.6523
 832/1760 [=============>................] - ETA: 1:00 - loss: 0.6408 - acc: 0.6454
 896/1760 [==============>...............] - ETA: 56s - loss: 0.6404 - acc: 0.6473 
 960/1760 [===============>..............] - ETA: 52s - loss: 0.6462 - acc: 0.6406
1024/1760 [================>.............] - ETA: 48s - loss: 0.6463 - acc: 0.6396
1088/1760 [=================>............] - ETA: 44s - loss: 0.6476 - acc: 0.6342
1152/1760 [==================>...........] - ETA: 40s - loss: 0.6463 - acc: 0.6363
1216/1760 [===================>..........] - ETA: 35s - loss: 0.6433 - acc: 0.6406
1280/1760 [====================>.........] - ETA: 31s - loss: 0.6394 - acc: 0.6461
1344/1760 [=====================>........] - ETA: 27s - loss: 0.6405 - acc: 0.6458
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6392 - acc: 0.6477
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6394 - acc: 0.6447
1536/1760 [=========================>....] - ETA: 14s - loss: 0.6404 - acc: 0.6439
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6407 - acc: 0.6444
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6418 - acc: 0.6442 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6409 - acc: 0.6458
1760/1760 [==============================] - 122s 69ms/step - loss: 0.6398 - acc: 0.6472 - val_loss: 0.6472 - val_acc: 0.6224

Epoch 00008: val_acc did not improve from 0.63776
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:54 - loss: 0.6405 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:49 - loss: 0.6494 - acc: 0.6406
 192/1760 [==>...........................] - ETA: 1:45 - loss: 0.6402 - acc: 0.6562
 256/1760 [===>..........................] - ETA: 1:41 - loss: 0.6218 - acc: 0.6680
 320/1760 [====>.........................] - ETA: 1:35 - loss: 0.6198 - acc: 0.6594
 384/1760 [=====>........................] - ETA: 1:31 - loss: 0.6268 - acc: 0.6484
 448/1760 [======>.......................] - ETA: 1:27 - loss: 0.6364 - acc: 0.6406
 512/1760 [=======>......................] - ETA: 1:22 - loss: 0.6410 - acc: 0.6328
 576/1760 [========>.....................] - ETA: 1:18 - loss: 0.6393 - acc: 0.6337
 640/1760 [=========>....................] - ETA: 1:14 - loss: 0.6438 - acc: 0.6203
 704/1760 [===========>..................] - ETA: 1:09 - loss: 0.6396 - acc: 0.6236
 768/1760 [============>.................] - ETA: 1:03 - loss: 0.6388 - acc: 0.6237
 832/1760 [=============>................] - ETA: 58s - loss: 0.6340 - acc: 0.6274 
 896/1760 [==============>...............] - ETA: 53s - loss: 0.6304 - acc: 0.6328
 960/1760 [===============>..............] - ETA: 48s - loss: 0.6356 - acc: 0.6302
1024/1760 [================>.............] - ETA: 44s - loss: 0.6394 - acc: 0.6270
1088/1760 [=================>............] - ETA: 40s - loss: 0.6423 - acc: 0.6241
1152/1760 [==================>...........] - ETA: 36s - loss: 0.6392 - acc: 0.6259
1216/1760 [===================>..........] - ETA: 32s - loss: 0.6455 - acc: 0.6225
1280/1760 [====================>.........] - ETA: 28s - loss: 0.6467 - acc: 0.6203
1344/1760 [=====================>........] - ETA: 24s - loss: 0.6480 - acc: 0.6205
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6495 - acc: 0.6172
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6527 - acc: 0.6121
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6537 - acc: 0.6133
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6520 - acc: 0.6150 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6488 - acc: 0.6208
1728/1760 [============================>.] - ETA: 1s - loss: 0.6518 - acc: 0.6181
1760/1760 [==============================] - 106s 60ms/step - loss: 0.6514 - acc: 0.6182 - val_loss: 0.7660 - val_acc: 0.5612

Epoch 00009: val_acc did not improve from 0.63776
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:31 - loss: 0.6243 - acc: 0.6406
 128/1760 [=>............................] - ETA: 1:33 - loss: 0.6872 - acc: 0.5859
 192/1760 [==>...........................] - ETA: 1:33 - loss: 0.6925 - acc: 0.5677
 256/1760 [===>..........................] - ETA: 1:27 - loss: 0.6957 - acc: 0.5586
 320/1760 [====>.........................] - ETA: 1:23 - loss: 0.6867 - acc: 0.5656
 384/1760 [=====>........................] - ETA: 1:21 - loss: 0.6847 - acc: 0.5625
 448/1760 [======>.......................] - ETA: 1:23 - loss: 0.6739 - acc: 0.5804
 512/1760 [=======>......................] - ETA: 1:20 - loss: 0.6717 - acc: 0.5859
 576/1760 [========>.....................] - ETA: 1:16 - loss: 0.6731 - acc: 0.5885
 640/1760 [=========>....................] - ETA: 1:12 - loss: 0.6741 - acc: 0.5922
 704/1760 [===========>..................] - ETA: 1:09 - loss: 0.6727 - acc: 0.5938
 768/1760 [============>.................] - ETA: 1:05 - loss: 0.6761 - acc: 0.5885
 832/1760 [=============>................] - ETA: 1:01 - loss: 0.6736 - acc: 0.5889
 896/1760 [==============>...............] - ETA: 57s - loss: 0.6675 - acc: 0.5971 
 960/1760 [===============>..............] - ETA: 53s - loss: 0.6613 - acc: 0.6052
1024/1760 [================>.............] - ETA: 49s - loss: 0.6596 - acc: 0.6055
1088/1760 [=================>............] - ETA: 46s - loss: 0.6601 - acc: 0.6048
1152/1760 [==================>...........] - ETA: 42s - loss: 0.6614 - acc: 0.6033
1216/1760 [===================>..........] - ETA: 38s - loss: 0.6577 - acc: 0.6077
1280/1760 [====================>.........] - ETA: 34s - loss: 0.6581 - acc: 0.6062
1344/1760 [=====================>........] - ETA: 29s - loss: 0.6557 - acc: 0.6094
1408/1760 [=======================>......] - ETA: 25s - loss: 0.6527 - acc: 0.6122
1472/1760 [========================>.....] - ETA: 20s - loss: 0.6559 - acc: 0.6087
1536/1760 [=========================>....] - ETA: 16s - loss: 0.6559 - acc: 0.6087
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6555 - acc: 0.6075
1664/1760 [===========================>..] - ETA: 7s - loss: 0.6565 - acc: 0.6064 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6557 - acc: 0.6100
1760/1760 [==============================] - 135s 77ms/step - loss: 0.6546 - acc: 0.6114 - val_loss: 0.6445 - val_acc: 0.6327

Epoch 00010: val_acc did not improve from 0.63776
样本个数 200
样本个数 400
window_select6-10.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 9s
128/400 [========>.....................] - ETA: 5s
192/400 [=============>................] - ETA: 4s
256/400 [==================>...........] - ETA: 3s
320/400 [=======================>......] - ETA: 1s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 8s 20ms/step
loss: 0.685409803390503
acc: 0.59
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f804036f6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f804036f6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f8038423310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f8038423310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8682471c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8682471c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8693f4eed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8693f4eed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8693f4a210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8693f4a210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86ac9e3190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86ac9e3190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8693f56710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8693f56710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f804021f810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f804021f810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80403a3090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80403a3090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80400b0cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80400b0cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f804020dbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f804020dbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80403ab710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80403ab710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8040128a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8040128a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80387bf350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80387bf350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f803857f450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f803857f450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80386f8f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80386f8f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f804009e510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f804009e510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8038484c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8038484c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8000171990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8000171990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80000e5c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f80000e5c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80001740d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80001740d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f800018d790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f800018d790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ff8688e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ff8688e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80000d7490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80000d7490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ff861d2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ff861d2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80384ebe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80384ebe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8000157a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f8000157a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ff84babd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ff84babd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ff854c050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ff854c050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ff8262990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ff8262990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ff826db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ff826db90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ff85d9b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ff85d9b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ff808f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ff808f1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ff8372710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ff8372710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fc0602f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fc0602f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc075e690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc075e690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ff80f8e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ff80f8e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc055ffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc055ffd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7fc04ec650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7fc04ec650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fc044a6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fc044a6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc0208750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc0208750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7fc06ba490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7fc06ba490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc0358350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc0358350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7fc03092d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7fc03092d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fc01b1190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fc01b1190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc0355310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc0355310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7fc0165e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7fc0165e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc01081d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc01081d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7fa0727e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7fa0727e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fa0608250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fa0608250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fa04cd550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fa04cd550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7fa0738fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7fa0738fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fa03f2dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fa03f2dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7fa0282410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7fa0282410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fa057a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fa057a550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fa032db50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fa032db50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7fa0486c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7fa0486c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fa01edcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fa01edcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7f8078e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7f8078e550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fa01de110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7fa01de110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fa021bd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fa021bd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7f8078de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7f8078de10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7f804c5a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7f804c5a90>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 3:44 - loss: 0.7184 - acc: 0.5312
 128/1760 [=>............................] - ETA: 2:27 - loss: 0.7479 - acc: 0.4844
 192/1760 [==>...........................] - ETA: 1:58 - loss: 0.7454 - acc: 0.5156
 256/1760 [===>..........................] - ETA: 1:43 - loss: 0.7344 - acc: 0.5312
 320/1760 [====>.........................] - ETA: 1:36 - loss: 0.7370 - acc: 0.5250
 384/1760 [=====>........................] - ETA: 1:28 - loss: 0.7378 - acc: 0.5286
 448/1760 [======>.......................] - ETA: 1:21 - loss: 0.7222 - acc: 0.5424
 512/1760 [=======>......................] - ETA: 1:16 - loss: 0.7157 - acc: 0.5527
 576/1760 [========>.....................] - ETA: 1:10 - loss: 0.7251 - acc: 0.5469
 640/1760 [=========>....................] - ETA: 1:06 - loss: 0.7248 - acc: 0.5484
 704/1760 [===========>..................] - ETA: 1:02 - loss: 0.7296 - acc: 0.5483
 768/1760 [============>.................] - ETA: 57s - loss: 0.7245 - acc: 0.5573 
 832/1760 [=============>................] - ETA: 53s - loss: 0.7274 - acc: 0.5589
 896/1760 [==============>...............] - ETA: 48s - loss: 0.7298 - acc: 0.5491
 960/1760 [===============>..............] - ETA: 44s - loss: 0.7257 - acc: 0.5573
1024/1760 [================>.............] - ETA: 41s - loss: 0.7258 - acc: 0.5586
1088/1760 [=================>............] - ETA: 37s - loss: 0.7243 - acc: 0.5616
1152/1760 [==================>...........] - ETA: 33s - loss: 0.7251 - acc: 0.5564
1216/1760 [===================>..........] - ETA: 30s - loss: 0.7249 - acc: 0.5576
1280/1760 [====================>.........] - ETA: 26s - loss: 0.7270 - acc: 0.5523
1344/1760 [=====================>........] - ETA: 22s - loss: 0.7231 - acc: 0.5573
1408/1760 [=======================>......] - ETA: 19s - loss: 0.7233 - acc: 0.5589
1472/1760 [========================>.....] - ETA: 15s - loss: 0.7223 - acc: 0.5584
1536/1760 [=========================>....] - ETA: 12s - loss: 0.7208 - acc: 0.5592
1600/1760 [==========================>...] - ETA: 8s - loss: 0.7160 - acc: 0.5656 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7199 - acc: 0.5601
1728/1760 [============================>.] - ETA: 1s - loss: 0.7223 - acc: 0.5584
1760/1760 [==============================] - 101s 57ms/step - loss: 0.7216 - acc: 0.5580 - val_loss: 0.7025 - val_acc: 0.5102

Epoch 00001: val_acc improved from -inf to 0.51020, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window07/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:44 - loss: 0.6928 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:36 - loss: 0.6742 - acc: 0.5703
 192/1760 [==>...........................] - ETA: 1:32 - loss: 0.6708 - acc: 0.5625
 256/1760 [===>..........................] - ETA: 1:26 - loss: 0.6770 - acc: 0.5664
 320/1760 [====>.........................] - ETA: 1:21 - loss: 0.7057 - acc: 0.5406
 384/1760 [=====>........................] - ETA: 1:15 - loss: 0.6947 - acc: 0.5443
 448/1760 [======>.......................] - ETA: 1:13 - loss: 0.6944 - acc: 0.5491
 512/1760 [=======>......................] - ETA: 1:08 - loss: 0.6970 - acc: 0.5469
 576/1760 [========>.....................] - ETA: 1:03 - loss: 0.6976 - acc: 0.5417
 640/1760 [=========>....................] - ETA: 1:01 - loss: 0.6957 - acc: 0.5500
 704/1760 [===========>..................] - ETA: 57s - loss: 0.6929 - acc: 0.5582 
 768/1760 [============>.................] - ETA: 53s - loss: 0.6922 - acc: 0.5586
 832/1760 [=============>................] - ETA: 49s - loss: 0.6900 - acc: 0.5625
 896/1760 [==============>...............] - ETA: 46s - loss: 0.6872 - acc: 0.5670
 960/1760 [===============>..............] - ETA: 42s - loss: 0.6810 - acc: 0.5750
1024/1760 [================>.............] - ETA: 39s - loss: 0.6766 - acc: 0.5762
1088/1760 [=================>............] - ETA: 35s - loss: 0.6782 - acc: 0.5754
1152/1760 [==================>...........] - ETA: 32s - loss: 0.6800 - acc: 0.5703
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6821 - acc: 0.5691
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6830 - acc: 0.5641
1344/1760 [=====================>........] - ETA: 22s - loss: 0.6820 - acc: 0.5670
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6820 - acc: 0.5675
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6784 - acc: 0.5713
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6799 - acc: 0.5723
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6815 - acc: 0.5744 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6820 - acc: 0.5745
1728/1760 [============================>.] - ETA: 1s - loss: 0.6842 - acc: 0.5752
1760/1760 [==============================] - 98s 56ms/step - loss: 0.6816 - acc: 0.5784 - val_loss: 0.6686 - val_acc: 0.5816

Epoch 00002: val_acc improved from 0.51020 to 0.58163, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window07/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:38 - loss: 0.6843 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:32 - loss: 0.6773 - acc: 0.6016
 192/1760 [==>...........................] - ETA: 1:27 - loss: 0.6688 - acc: 0.6094
 256/1760 [===>..........................] - ETA: 1:20 - loss: 0.6754 - acc: 0.6016
 320/1760 [====>.........................] - ETA: 1:15 - loss: 0.6676 - acc: 0.6000
 384/1760 [=====>........................] - ETA: 1:13 - loss: 0.6596 - acc: 0.6146
 448/1760 [======>.......................] - ETA: 1:08 - loss: 0.6511 - acc: 0.6339
 512/1760 [=======>......................] - ETA: 1:09 - loss: 0.6510 - acc: 0.6328
 576/1760 [========>.....................] - ETA: 1:05 - loss: 0.6576 - acc: 0.6233
 640/1760 [=========>....................] - ETA: 1:01 - loss: 0.6621 - acc: 0.6156
 704/1760 [===========>..................] - ETA: 58s - loss: 0.6609 - acc: 0.6222 
 768/1760 [============>.................] - ETA: 54s - loss: 0.6578 - acc: 0.6185
 832/1760 [=============>................] - ETA: 50s - loss: 0.6591 - acc: 0.6142
 896/1760 [==============>...............] - ETA: 47s - loss: 0.6579 - acc: 0.6161
 960/1760 [===============>..............] - ETA: 44s - loss: 0.6599 - acc: 0.6083
1024/1760 [================>.............] - ETA: 40s - loss: 0.6593 - acc: 0.6133
1088/1760 [=================>............] - ETA: 36s - loss: 0.6592 - acc: 0.6140
1152/1760 [==================>...........] - ETA: 33s - loss: 0.6636 - acc: 0.6076
1216/1760 [===================>..........] - ETA: 29s - loss: 0.6671 - acc: 0.6044
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6625 - acc: 0.6102
1344/1760 [=====================>........] - ETA: 22s - loss: 0.6654 - acc: 0.6071
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6660 - acc: 0.6058
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6658 - acc: 0.6053
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6634 - acc: 0.6074
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6636 - acc: 0.6062 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6645 - acc: 0.6088
1728/1760 [============================>.] - ETA: 1s - loss: 0.6668 - acc: 0.6059
1760/1760 [==============================] - 99s 56ms/step - loss: 0.6679 - acc: 0.6057 - val_loss: 0.6818 - val_acc: 0.5357

Epoch 00003: val_acc did not improve from 0.58163
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:27 - loss: 0.7489 - acc: 0.5312
 128/1760 [=>............................] - ETA: 1:23 - loss: 0.7260 - acc: 0.5391
 192/1760 [==>...........................] - ETA: 1:20 - loss: 0.7103 - acc: 0.5365
 256/1760 [===>..........................] - ETA: 1:15 - loss: 0.6996 - acc: 0.5508
 320/1760 [====>.........................] - ETA: 1:12 - loss: 0.6855 - acc: 0.5750
 384/1760 [=====>........................] - ETA: 1:11 - loss: 0.6830 - acc: 0.5729
 448/1760 [======>.......................] - ETA: 1:07 - loss: 0.6790 - acc: 0.5781
 512/1760 [=======>......................] - ETA: 1:04 - loss: 0.6787 - acc: 0.5820
 576/1760 [========>.....................] - ETA: 1:01 - loss: 0.6792 - acc: 0.5816
 640/1760 [=========>....................] - ETA: 58s - loss: 0.6777 - acc: 0.5781 
 704/1760 [===========>..................] - ETA: 54s - loss: 0.6763 - acc: 0.5739
 768/1760 [============>.................] - ETA: 51s - loss: 0.6702 - acc: 0.5859
 832/1760 [=============>................] - ETA: 47s - loss: 0.6720 - acc: 0.5829
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6738 - acc: 0.5848
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6744 - acc: 0.5833
1024/1760 [================>.............] - ETA: 38s - loss: 0.6761 - acc: 0.5781
1088/1760 [=================>............] - ETA: 34s - loss: 0.6746 - acc: 0.5735
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6753 - acc: 0.5703
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6751 - acc: 0.5740
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6715 - acc: 0.5758
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6706 - acc: 0.5751
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6680 - acc: 0.5810
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6672 - acc: 0.5842
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6673 - acc: 0.5840
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6683 - acc: 0.5825 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6712 - acc: 0.5799
1728/1760 [============================>.] - ETA: 1s - loss: 0.6718 - acc: 0.5799
1760/1760 [==============================] - 95s 54ms/step - loss: 0.6710 - acc: 0.5824 - val_loss: 0.6749 - val_acc: 0.5816

Epoch 00004: val_acc did not improve from 0.58163
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:36 - loss: 0.6579 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:26 - loss: 0.6956 - acc: 0.5469
 192/1760 [==>...........................] - ETA: 1:20 - loss: 0.6696 - acc: 0.5677
 256/1760 [===>..........................] - ETA: 1:21 - loss: 0.6690 - acc: 0.5820
 320/1760 [====>.........................] - ETA: 1:16 - loss: 0.6713 - acc: 0.5687
 384/1760 [=====>........................] - ETA: 1:11 - loss: 0.6675 - acc: 0.5781
 448/1760 [======>.......................] - ETA: 1:07 - loss: 0.6604 - acc: 0.5871
 512/1760 [=======>......................] - ETA: 1:05 - loss: 0.6710 - acc: 0.5781
 576/1760 [========>.....................] - ETA: 1:01 - loss: 0.6667 - acc: 0.5920
 640/1760 [=========>....................] - ETA: 57s - loss: 0.6644 - acc: 0.5969 
 704/1760 [===========>..................] - ETA: 54s - loss: 0.6620 - acc: 0.5966
 768/1760 [============>.................] - ETA: 51s - loss: 0.6599 - acc: 0.6029
 832/1760 [=============>................] - ETA: 47s - loss: 0.6616 - acc: 0.6022
 896/1760 [==============>...............] - ETA: 45s - loss: 0.6666 - acc: 0.6004
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6653 - acc: 0.6000
1024/1760 [================>.............] - ETA: 38s - loss: 0.6650 - acc: 0.5996
1088/1760 [=================>............] - ETA: 34s - loss: 0.6638 - acc: 0.6011
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6602 - acc: 0.6085
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6581 - acc: 0.6110
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6551 - acc: 0.6141
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6524 - acc: 0.6168
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6524 - acc: 0.6186
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6564 - acc: 0.6128
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6589 - acc: 0.6100
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6576 - acc: 0.6119 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6562 - acc: 0.6130
1728/1760 [============================>.] - ETA: 1s - loss: 0.6535 - acc: 0.6146
1760/1760 [==============================] - 93s 53ms/step - loss: 0.6542 - acc: 0.6136 - val_loss: 0.6450 - val_acc: 0.6327

Epoch 00005: val_acc improved from 0.58163 to 0.63265, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window07/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:32 - loss: 0.6983 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:24 - loss: 0.6352 - acc: 0.6250
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.6477 - acc: 0.6198
 256/1760 [===>..........................] - ETA: 1:14 - loss: 0.6486 - acc: 0.6211
 320/1760 [====>.........................] - ETA: 1:11 - loss: 0.6519 - acc: 0.6250
 384/1760 [=====>........................] - ETA: 1:09 - loss: 0.6459 - acc: 0.6354
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6514 - acc: 0.6295
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6476 - acc: 0.6250
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6469 - acc: 0.6302 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6528 - acc: 0.6297
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6516 - acc: 0.6307
 768/1760 [============>.................] - ETA: 49s - loss: 0.6467 - acc: 0.6393
 832/1760 [=============>................] - ETA: 45s - loss: 0.6518 - acc: 0.6370
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6532 - acc: 0.6384
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6584 - acc: 0.6323
1024/1760 [================>.............] - ETA: 36s - loss: 0.6538 - acc: 0.6348
1088/1760 [=================>............] - ETA: 33s - loss: 0.6545 - acc: 0.6296
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6530 - acc: 0.6285
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6504 - acc: 0.6324
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6519 - acc: 0.6281
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6497 - acc: 0.6302
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6527 - acc: 0.6250
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6464 - acc: 0.6311
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6445 - acc: 0.6335
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6449 - acc: 0.6319 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6457 - acc: 0.6322
1728/1760 [============================>.] - ETA: 1s - loss: 0.6470 - acc: 0.6319
1760/1760 [==============================] - 92s 52ms/step - loss: 0.6481 - acc: 0.6307 - val_loss: 0.6544 - val_acc: 0.6633

Epoch 00006: val_acc improved from 0.63265 to 0.66327, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window07/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:23 - loss: 0.6592 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:16 - loss: 0.6249 - acc: 0.6562
 192/1760 [==>...........................] - ETA: 1:13 - loss: 0.6135 - acc: 0.6615
 256/1760 [===>..........................] - ETA: 1:12 - loss: 0.6217 - acc: 0.6445
 320/1760 [====>.........................] - ETA: 1:11 - loss: 0.6160 - acc: 0.6469
 384/1760 [=====>........................] - ETA: 1:07 - loss: 0.6164 - acc: 0.6536
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6130 - acc: 0.6607
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6292 - acc: 0.6445
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6344 - acc: 0.6354 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6297 - acc: 0.6422
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6326 - acc: 0.6435
 768/1760 [============>.................] - ETA: 49s - loss: 0.6380 - acc: 0.6380
 832/1760 [=============>................] - ETA: 46s - loss: 0.6413 - acc: 0.6346
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6518 - acc: 0.6272
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6559 - acc: 0.6240
1024/1760 [================>.............] - ETA: 36s - loss: 0.6535 - acc: 0.6260
1088/1760 [=================>............] - ETA: 33s - loss: 0.6489 - acc: 0.6296
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6478 - acc: 0.6337
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6493 - acc: 0.6308
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6528 - acc: 0.6281
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6527 - acc: 0.6295
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6541 - acc: 0.6293
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6541 - acc: 0.6298
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6551 - acc: 0.6283
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6562 - acc: 0.6269 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6531 - acc: 0.6292
1728/1760 [============================>.] - ETA: 1s - loss: 0.6510 - acc: 0.6314
1760/1760 [==============================] - 96s 54ms/step - loss: 0.6508 - acc: 0.6318 - val_loss: 0.6436 - val_acc: 0.6327

Epoch 00007: val_acc did not improve from 0.66327
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:16 - loss: 0.6293 - acc: 0.7031
 128/1760 [=>............................] - ETA: 1:20 - loss: 0.6232 - acc: 0.6875
 192/1760 [==>...........................] - ETA: 1:19 - loss: 0.6134 - acc: 0.6875
 256/1760 [===>..........................] - ETA: 1:20 - loss: 0.6147 - acc: 0.6758
 320/1760 [====>.........................] - ETA: 1:15 - loss: 0.6222 - acc: 0.6781
 384/1760 [=====>........................] - ETA: 1:11 - loss: 0.6341 - acc: 0.6693
 448/1760 [======>.......................] - ETA: 1:08 - loss: 0.6243 - acc: 0.6808
 512/1760 [=======>......................] - ETA: 1:04 - loss: 0.6219 - acc: 0.6816
 576/1760 [========>.....................] - ETA: 1:01 - loss: 0.6259 - acc: 0.6736
 640/1760 [=========>....................] - ETA: 59s - loss: 0.6346 - acc: 0.6641 
 704/1760 [===========>..................] - ETA: 56s - loss: 0.6318 - acc: 0.6634
 768/1760 [============>.................] - ETA: 53s - loss: 0.6341 - acc: 0.6589
 832/1760 [=============>................] - ETA: 49s - loss: 0.6318 - acc: 0.6647
 896/1760 [==============>...............] - ETA: 46s - loss: 0.6327 - acc: 0.6674
 960/1760 [===============>..............] - ETA: 42s - loss: 0.6393 - acc: 0.6625
1024/1760 [================>.............] - ETA: 38s - loss: 0.6418 - acc: 0.6562
1088/1760 [=================>............] - ETA: 35s - loss: 0.6437 - acc: 0.6535
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6425 - acc: 0.6580
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6391 - acc: 0.6604
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6420 - acc: 0.6555
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6447 - acc: 0.6533
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6435 - acc: 0.6527
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6451 - acc: 0.6515
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6439 - acc: 0.6517
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6395 - acc: 0.6556 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6383 - acc: 0.6556
1728/1760 [============================>.] - ETA: 1s - loss: 0.6401 - acc: 0.6539
1760/1760 [==============================] - 96s 55ms/step - loss: 0.6388 - acc: 0.6557 - val_loss: 0.6653 - val_acc: 0.6071

Epoch 00008: val_acc did not improve from 0.66327
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:14 - loss: 0.5930 - acc: 0.7031
 128/1760 [=>............................] - ETA: 1:23 - loss: 0.5905 - acc: 0.7031
 192/1760 [==>...........................] - ETA: 1:20 - loss: 0.5991 - acc: 0.6927
 256/1760 [===>..........................] - ETA: 1:17 - loss: 0.6047 - acc: 0.6875
 320/1760 [====>.........................] - ETA: 1:16 - loss: 0.6046 - acc: 0.6813
 384/1760 [=====>........................] - ETA: 1:14 - loss: 0.6353 - acc: 0.6589
 448/1760 [======>.......................] - ETA: 1:10 - loss: 0.6298 - acc: 0.6607
 512/1760 [=======>......................] - ETA: 1:05 - loss: 0.6422 - acc: 0.6523
 576/1760 [========>.....................] - ETA: 1:02 - loss: 0.6446 - acc: 0.6458
 640/1760 [=========>....................] - ETA: 58s - loss: 0.6502 - acc: 0.6344 
 704/1760 [===========>..................] - ETA: 54s - loss: 0.6486 - acc: 0.6364
 768/1760 [============>.................] - ETA: 51s - loss: 0.6509 - acc: 0.6354
 832/1760 [=============>................] - ETA: 47s - loss: 0.6465 - acc: 0.6370
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6415 - acc: 0.6395
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6429 - acc: 0.6406
1024/1760 [================>.............] - ETA: 38s - loss: 0.6441 - acc: 0.6377
1088/1760 [=================>............] - ETA: 35s - loss: 0.6435 - acc: 0.6406
1152/1760 [==================>...........] - ETA: 32s - loss: 0.6449 - acc: 0.6354
1216/1760 [===================>..........] - ETA: 29s - loss: 0.6460 - acc: 0.6357
1280/1760 [====================>.........] - ETA: 26s - loss: 0.6463 - acc: 0.6344
1344/1760 [=====================>........] - ETA: 22s - loss: 0.6441 - acc: 0.6354
1408/1760 [=======================>......] - ETA: 19s - loss: 0.6437 - acc: 0.6378
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6410 - acc: 0.6413
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6417 - acc: 0.6400
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6406 - acc: 0.6412 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6410 - acc: 0.6436
1728/1760 [============================>.] - ETA: 1s - loss: 0.6388 - acc: 0.6458
1760/1760 [==============================] - 112s 63ms/step - loss: 0.6374 - acc: 0.6483 - val_loss: 0.6429 - val_acc: 0.6378

Epoch 00009: val_acc did not improve from 0.66327
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:56 - loss: 0.6325 - acc: 0.6406
 128/1760 [=>............................] - ETA: 1:51 - loss: 0.6361 - acc: 0.6406
 192/1760 [==>...........................] - ETA: 1:52 - loss: 0.6369 - acc: 0.6250
 256/1760 [===>..........................] - ETA: 1:49 - loss: 0.6523 - acc: 0.6133
 320/1760 [====>.........................] - ETA: 1:45 - loss: 0.6541 - acc: 0.6219
 384/1760 [=====>........................] - ETA: 1:39 - loss: 0.6481 - acc: 0.6302
 448/1760 [======>.......................] - ETA: 1:36 - loss: 0.6644 - acc: 0.6071
 512/1760 [=======>......................] - ETA: 1:30 - loss: 0.6555 - acc: 0.6074
 576/1760 [========>.....................] - ETA: 1:26 - loss: 0.6522 - acc: 0.6181
 640/1760 [=========>....................] - ETA: 1:22 - loss: 0.6506 - acc: 0.6141
 704/1760 [===========>..................] - ETA: 1:17 - loss: 0.6470 - acc: 0.6207
 768/1760 [============>.................] - ETA: 1:13 - loss: 0.6453 - acc: 0.6263
 832/1760 [=============>................] - ETA: 1:08 - loss: 0.6467 - acc: 0.6226
 896/1760 [==============>...............] - ETA: 1:04 - loss: 0.6434 - acc: 0.6295
 960/1760 [===============>..............] - ETA: 1:00 - loss: 0.6423 - acc: 0.6281
1024/1760 [================>.............] - ETA: 55s - loss: 0.6450 - acc: 0.6260 
1088/1760 [=================>............] - ETA: 50s - loss: 0.6437 - acc: 0.6268
1152/1760 [==================>...........] - ETA: 46s - loss: 0.6391 - acc: 0.6354
1216/1760 [===================>..........] - ETA: 41s - loss: 0.6404 - acc: 0.6373
1280/1760 [====================>.........] - ETA: 36s - loss: 0.6382 - acc: 0.6414
1344/1760 [=====================>........] - ETA: 31s - loss: 0.6364 - acc: 0.6451
1408/1760 [=======================>......] - ETA: 26s - loss: 0.6369 - acc: 0.6435
1472/1760 [========================>.....] - ETA: 21s - loss: 0.6390 - acc: 0.6427
1536/1760 [=========================>....] - ETA: 17s - loss: 0.6376 - acc: 0.6426
1600/1760 [==========================>...] - ETA: 12s - loss: 0.6378 - acc: 0.6425
1664/1760 [===========================>..] - ETA: 7s - loss: 0.6372 - acc: 0.6424 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6347 - acc: 0.6447
1760/1760 [==============================] - 139s 79ms/step - loss: 0.6373 - acc: 0.6420 - val_loss: 0.6590 - val_acc: 0.6684

Epoch 00010: val_acc improved from 0.66327 to 0.66837, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window07/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
样本个数 200
样本个数 400
window_select6-10.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 15s
128/400 [========>.....................] - ETA: 8s 
192/400 [=============>................] - ETA: 5s
256/400 [==================>...........] - ETA: 3s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 10s 26ms/step
loss: 0.6923621940612793
acc: 0.6
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7f00098810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7f00098810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7ee0165250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7ee0165250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8693552850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8693552850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ec0063ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ec0063ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ec0063650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ec0063650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ee0707990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ee0707990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f869404ef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f869404ef50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8693622990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8693622990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ec0056690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ec0056690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ee06474d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ee06474d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ee0592390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ee0592390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f869353f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f869353f590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ee0523b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ee0523b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ee0623050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ee0623050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ee04ce390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ee04ce390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ee0364e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ee0364e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ee0623310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ee0623310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc01d3f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7fc01d3f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8693638610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8693638610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ea0698310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ea0698310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ea0772590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ea0772590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ee024f410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ee024f410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ee035c110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ee035c110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ea0372d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ea0372d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ea0349810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ea0349810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ea05b2790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ea05b2790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ea0372050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ea0372050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ea0269e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ea0269e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e807e8f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e807e8f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e806c8e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e806c8e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ea0259ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ea0259ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ea008e9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7ea008e9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e805d9ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e805d9ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e804df150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e804df150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e80510310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e80510310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e8056d890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e8056d890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e80519f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e80519f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e80506350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e80506350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e803d1e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e803d1e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e800b5110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e800b5110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e802803d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e802803d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e803d1dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e803d1dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e8007a750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e8007a750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e6069cc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e6069cc90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e605656d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e605656d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e80080590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e80080590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e800b5410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e800b5410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e6050f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e6050f5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e6068ae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e6068ae50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e6027a090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e6027a090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e603114d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e603114d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e60572bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e60572bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e6027aad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e6027aad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e40782150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e40782150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e4076c110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e4076c110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e405ac610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e405ac610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e40782850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e40782850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e40776190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e40776190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e404f3c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7e404f3c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e4033dd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7e4033dd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e406d4450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e406d4450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e404f3dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e404f3dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e402f0c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e402f0c10>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 4:35 - loss: 0.7214 - acc: 0.5156
 128/1760 [=>............................] - ETA: 2:47 - loss: 0.8240 - acc: 0.4609
 192/1760 [==>...........................] - ETA: 2:15 - loss: 0.7883 - acc: 0.4792
 256/1760 [===>..........................] - ETA: 1:54 - loss: 0.7743 - acc: 0.4844
 320/1760 [====>.........................] - ETA: 1:42 - loss: 0.7668 - acc: 0.4875
 384/1760 [=====>........................] - ETA: 1:35 - loss: 0.7669 - acc: 0.4818
 448/1760 [======>.......................] - ETA: 1:27 - loss: 0.7644 - acc: 0.4888
 512/1760 [=======>......................] - ETA: 1:21 - loss: 0.7649 - acc: 0.4824
 576/1760 [========>.....................] - ETA: 1:15 - loss: 0.7562 - acc: 0.4861
 640/1760 [=========>....................] - ETA: 1:08 - loss: 0.7475 - acc: 0.4984
 704/1760 [===========>..................] - ETA: 1:03 - loss: 0.7426 - acc: 0.5028
 768/1760 [============>.................] - ETA: 58s - loss: 0.7419 - acc: 0.5013 
 832/1760 [=============>................] - ETA: 54s - loss: 0.7387 - acc: 0.4988
 896/1760 [==============>...............] - ETA: 49s - loss: 0.7380 - acc: 0.5000
 960/1760 [===============>..............] - ETA: 46s - loss: 0.7357 - acc: 0.5042
1024/1760 [================>.............] - ETA: 42s - loss: 0.7326 - acc: 0.5117
1088/1760 [=================>............] - ETA: 38s - loss: 0.7296 - acc: 0.5156
1152/1760 [==================>...........] - ETA: 34s - loss: 0.7282 - acc: 0.5130
1216/1760 [===================>..........] - ETA: 30s - loss: 0.7291 - acc: 0.5066
1280/1760 [====================>.........] - ETA: 27s - loss: 0.7281 - acc: 0.5070
1344/1760 [=====================>........] - ETA: 23s - loss: 0.7294 - acc: 0.5030
1408/1760 [=======================>......] - ETA: 19s - loss: 0.7287 - acc: 0.5043
1472/1760 [========================>.....] - ETA: 16s - loss: 0.7249 - acc: 0.5109
1536/1760 [=========================>....] - ETA: 12s - loss: 0.7228 - acc: 0.5111
1600/1760 [==========================>...] - ETA: 8s - loss: 0.7236 - acc: 0.5119 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7229 - acc: 0.5114
1728/1760 [============================>.] - ETA: 1s - loss: 0.7210 - acc: 0.5139
1760/1760 [==============================] - 104s 59ms/step - loss: 0.7190 - acc: 0.5170 - val_loss: 0.6543 - val_acc: 0.6122

Epoch 00001: val_acc improved from -inf to 0.61224, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window08/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:36 - loss: 0.6670 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:30 - loss: 0.6576 - acc: 0.5781
 192/1760 [==>...........................] - ETA: 1:20 - loss: 0.6847 - acc: 0.5417
 256/1760 [===>..........................] - ETA: 1:15 - loss: 0.6904 - acc: 0.5352
 320/1760 [====>.........................] - ETA: 1:14 - loss: 0.6791 - acc: 0.5563
 384/1760 [=====>........................] - ETA: 1:10 - loss: 0.6818 - acc: 0.5495
 448/1760 [======>.......................] - ETA: 1:07 - loss: 0.6834 - acc: 0.5580
 512/1760 [=======>......................] - ETA: 1:03 - loss: 0.6817 - acc: 0.5527
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6853 - acc: 0.5590
 640/1760 [=========>....................] - ETA: 57s - loss: 0.6837 - acc: 0.5641 
 704/1760 [===========>..................] - ETA: 55s - loss: 0.6783 - acc: 0.5753
 768/1760 [============>.................] - ETA: 51s - loss: 0.6790 - acc: 0.5755
 832/1760 [=============>................] - ETA: 48s - loss: 0.6816 - acc: 0.5697
 896/1760 [==============>...............] - ETA: 45s - loss: 0.6813 - acc: 0.5725
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6802 - acc: 0.5740
1024/1760 [================>.............] - ETA: 38s - loss: 0.6796 - acc: 0.5713
1088/1760 [=================>............] - ETA: 35s - loss: 0.6786 - acc: 0.5680
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6807 - acc: 0.5720
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6824 - acc: 0.5707
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6825 - acc: 0.5711
1344/1760 [=====================>........] - ETA: 22s - loss: 0.6821 - acc: 0.5714
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6837 - acc: 0.5703
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6877 - acc: 0.5686
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6886 - acc: 0.5697
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6882 - acc: 0.5681 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6888 - acc: 0.5679
1728/1760 [============================>.] - ETA: 1s - loss: 0.6902 - acc: 0.5677
1760/1760 [==============================] - 98s 56ms/step - loss: 0.6894 - acc: 0.5687 - val_loss: 0.6706 - val_acc: 0.6378

Epoch 00002: val_acc improved from 0.61224 to 0.63776, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window08/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:21 - loss: 0.7275 - acc: 0.5000
 128/1760 [=>............................] - ETA: 1:22 - loss: 0.7270 - acc: 0.5078
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.7150 - acc: 0.4948
 256/1760 [===>..........................] - ETA: 1:20 - loss: 0.7105 - acc: 0.5078
 320/1760 [====>.........................] - ETA: 1:18 - loss: 0.7181 - acc: 0.5062
 384/1760 [=====>........................] - ETA: 1:13 - loss: 0.7071 - acc: 0.5182
 448/1760 [======>.......................] - ETA: 1:08 - loss: 0.6999 - acc: 0.5312
 512/1760 [=======>......................] - ETA: 1:05 - loss: 0.6976 - acc: 0.5312
 576/1760 [========>.....................] - ETA: 1:01 - loss: 0.6958 - acc: 0.5417
 640/1760 [=========>....................] - ETA: 57s - loss: 0.6952 - acc: 0.5453 
 704/1760 [===========>..................] - ETA: 54s - loss: 0.6942 - acc: 0.5440
 768/1760 [============>.................] - ETA: 51s - loss: 0.6966 - acc: 0.5430
 832/1760 [=============>................] - ETA: 47s - loss: 0.6985 - acc: 0.5433
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6987 - acc: 0.5424
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6956 - acc: 0.5458
1024/1760 [================>.............] - ETA: 38s - loss: 0.6918 - acc: 0.5518
1088/1760 [=================>............] - ETA: 34s - loss: 0.6897 - acc: 0.5533
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6891 - acc: 0.5556
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6886 - acc: 0.5600
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6878 - acc: 0.5602
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6849 - acc: 0.5655
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6859 - acc: 0.5675
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6876 - acc: 0.5673
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6860 - acc: 0.5723
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6842 - acc: 0.5756 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6806 - acc: 0.5823
1728/1760 [============================>.] - ETA: 1s - loss: 0.6799 - acc: 0.5851
1760/1760 [==============================] - 94s 54ms/step - loss: 0.6816 - acc: 0.5830 - val_loss: 0.6910 - val_acc: 0.5408

Epoch 00003: val_acc did not improve from 0.63776
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:24 - loss: 0.6200 - acc: 0.7188
 128/1760 [=>............................] - ETA: 1:23 - loss: 0.6417 - acc: 0.6719
 192/1760 [==>...........................] - ETA: 1:17 - loss: 0.6406 - acc: 0.6458
 256/1760 [===>..........................] - ETA: 1:17 - loss: 0.6276 - acc: 0.6680
 320/1760 [====>.........................] - ETA: 1:12 - loss: 0.6317 - acc: 0.6656
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6449 - acc: 0.6589
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6558 - acc: 0.6585
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6446 - acc: 0.6660
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6446 - acc: 0.6632 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6434 - acc: 0.6641
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6344 - acc: 0.6733
 768/1760 [============>.................] - ETA: 50s - loss: 0.6473 - acc: 0.6615
 832/1760 [=============>................] - ETA: 47s - loss: 0.6490 - acc: 0.6562
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6532 - acc: 0.6518
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6544 - acc: 0.6490
1024/1760 [================>.............] - ETA: 37s - loss: 0.6547 - acc: 0.6445
1088/1760 [=================>............] - ETA: 34s - loss: 0.6535 - acc: 0.6461
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6528 - acc: 0.6484
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6539 - acc: 0.6447
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6567 - acc: 0.6391
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6573 - acc: 0.6414
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6590 - acc: 0.6378
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6566 - acc: 0.6386
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6556 - acc: 0.6400
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6587 - acc: 0.6356 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6576 - acc: 0.6340
1728/1760 [============================>.] - ETA: 1s - loss: 0.6596 - acc: 0.6302
1760/1760 [==============================] - 96s 55ms/step - loss: 0.6613 - acc: 0.6295 - val_loss: 0.7103 - val_acc: 0.5306

Epoch 00004: val_acc did not improve from 0.63776
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:23 - loss: 0.7367 - acc: 0.5156
 128/1760 [=>............................] - ETA: 1:25 - loss: 0.7000 - acc: 0.5547
 192/1760 [==>...........................] - ETA: 1:21 - loss: 0.6840 - acc: 0.5677
 256/1760 [===>..........................] - ETA: 1:17 - loss: 0.6873 - acc: 0.5664
 320/1760 [====>.........................] - ETA: 1:15 - loss: 0.6837 - acc: 0.5719
 384/1760 [=====>........................] - ETA: 1:12 - loss: 0.6848 - acc: 0.5729
 448/1760 [======>.......................] - ETA: 1:09 - loss: 0.6779 - acc: 0.5781
 512/1760 [=======>......................] - ETA: 1:05 - loss: 0.6860 - acc: 0.5703
 576/1760 [========>.....................] - ETA: 1:01 - loss: 0.6906 - acc: 0.5660
 640/1760 [=========>....................] - ETA: 58s - loss: 0.6981 - acc: 0.5609 
 704/1760 [===========>..................] - ETA: 54s - loss: 0.6973 - acc: 0.5597
 768/1760 [============>.................] - ETA: 51s - loss: 0.6952 - acc: 0.5599
 832/1760 [=============>................] - ETA: 48s - loss: 0.6903 - acc: 0.5613
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6873 - acc: 0.5703
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6889 - acc: 0.5719
1024/1760 [================>.............] - ETA: 37s - loss: 0.6825 - acc: 0.5781
1088/1760 [=================>............] - ETA: 34s - loss: 0.6834 - acc: 0.5790
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6828 - acc: 0.5738
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6834 - acc: 0.5740
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6808 - acc: 0.5766
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6771 - acc: 0.5796
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6777 - acc: 0.5795
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6790 - acc: 0.5768
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6769 - acc: 0.5801
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6752 - acc: 0.5800 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6762 - acc: 0.5793
1728/1760 [============================>.] - ETA: 1s - loss: 0.6740 - acc: 0.5845
1760/1760 [==============================] - 95s 54ms/step - loss: 0.6745 - acc: 0.5835 - val_loss: 0.6883 - val_acc: 0.6276

Epoch 00005: val_acc did not improve from 0.63776
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:51 - loss: 0.6551 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:44 - loss: 0.6688 - acc: 0.6094
 192/1760 [==>...........................] - ETA: 1:31 - loss: 0.6887 - acc: 0.5781
 256/1760 [===>..........................] - ETA: 1:27 - loss: 0.6779 - acc: 0.5938
 320/1760 [====>.........................] - ETA: 1:25 - loss: 0.6711 - acc: 0.5906
 384/1760 [=====>........................] - ETA: 1:20 - loss: 0.6613 - acc: 0.6094
 448/1760 [======>.......................] - ETA: 1:16 - loss: 0.6566 - acc: 0.6250
 512/1760 [=======>......................] - ETA: 1:10 - loss: 0.6530 - acc: 0.6309
 576/1760 [========>.....................] - ETA: 1:06 - loss: 0.6486 - acc: 0.6389
 640/1760 [=========>....................] - ETA: 1:01 - loss: 0.6585 - acc: 0.6266
 704/1760 [===========>..................] - ETA: 58s - loss: 0.6695 - acc: 0.6165 
 768/1760 [============>.................] - ETA: 55s - loss: 0.6725 - acc: 0.6146
 832/1760 [=============>................] - ETA: 51s - loss: 0.6749 - acc: 0.6154
 896/1760 [==============>...............] - ETA: 47s - loss: 0.6732 - acc: 0.6127
 960/1760 [===============>..............] - ETA: 43s - loss: 0.6708 - acc: 0.6104
1024/1760 [================>.............] - ETA: 40s - loss: 0.6686 - acc: 0.6104
1088/1760 [=================>............] - ETA: 37s - loss: 0.6654 - acc: 0.6149
1152/1760 [==================>...........] - ETA: 33s - loss: 0.6622 - acc: 0.6155
1216/1760 [===================>..........] - ETA: 29s - loss: 0.6629 - acc: 0.6151
1280/1760 [====================>.........] - ETA: 26s - loss: 0.6639 - acc: 0.6109
1344/1760 [=====================>........] - ETA: 22s - loss: 0.6617 - acc: 0.6146
1408/1760 [=======================>......] - ETA: 19s - loss: 0.6627 - acc: 0.6136
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6634 - acc: 0.6114
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6645 - acc: 0.6100
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6635 - acc: 0.6125 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6633 - acc: 0.6118
1728/1760 [============================>.] - ETA: 1s - loss: 0.6635 - acc: 0.6094
1760/1760 [==============================] - 99s 56ms/step - loss: 0.6635 - acc: 0.6091 - val_loss: 0.6653 - val_acc: 0.6276

Epoch 00006: val_acc did not improve from 0.63776
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:25 - loss: 0.6639 - acc: 0.6406
 128/1760 [=>............................] - ETA: 1:23 - loss: 0.6556 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.6429 - acc: 0.6406
 256/1760 [===>..........................] - ETA: 1:13 - loss: 0.6442 - acc: 0.6367
 320/1760 [====>.........................] - ETA: 1:09 - loss: 0.6453 - acc: 0.6250
 384/1760 [=====>........................] - ETA: 1:07 - loss: 0.6438 - acc: 0.6250
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6416 - acc: 0.6295
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6409 - acc: 0.6309
 576/1760 [========>.....................] - ETA: 57s - loss: 0.6425 - acc: 0.6250 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6408 - acc: 0.6281
 704/1760 [===========>..................] - ETA: 51s - loss: 0.6405 - acc: 0.6250
 768/1760 [============>.................] - ETA: 48s - loss: 0.6366 - acc: 0.6276
 832/1760 [=============>................] - ETA: 45s - loss: 0.6410 - acc: 0.6250
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6398 - acc: 0.6250
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6364 - acc: 0.6302
1024/1760 [================>.............] - ETA: 36s - loss: 0.6367 - acc: 0.6309
1088/1760 [=================>............] - ETA: 33s - loss: 0.6350 - acc: 0.6333
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6332 - acc: 0.6354
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6334 - acc: 0.6357
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6326 - acc: 0.6375
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6317 - acc: 0.6399
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6285 - acc: 0.6449
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6280 - acc: 0.6488
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6289 - acc: 0.6478
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6304 - acc: 0.6469 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6330 - acc: 0.6460
1728/1760 [============================>.] - ETA: 1s - loss: 0.6335 - acc: 0.6435
1760/1760 [==============================] - 94s 54ms/step - loss: 0.6342 - acc: 0.6426 - val_loss: 0.6696 - val_acc: 0.6071

Epoch 00007: val_acc did not improve from 0.63776
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:21 - loss: 0.6577 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:19 - loss: 0.6582 - acc: 0.6250
 192/1760 [==>...........................] - ETA: 1:14 - loss: 0.6595 - acc: 0.6146
 256/1760 [===>..........................] - ETA: 1:11 - loss: 0.6692 - acc: 0.5977
 320/1760 [====>.........................] - ETA: 1:09 - loss: 0.6829 - acc: 0.5719
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6801 - acc: 0.5651
 448/1760 [======>.......................] - ETA: 1:06 - loss: 0.6702 - acc: 0.5781
 512/1760 [=======>......................] - ETA: 1:04 - loss: 0.6661 - acc: 0.5820
 576/1760 [========>.....................] - ETA: 1:02 - loss: 0.6660 - acc: 0.5799
 640/1760 [=========>....................] - ETA: 1:00 - loss: 0.6632 - acc: 0.5797
 704/1760 [===========>..................] - ETA: 57s - loss: 0.6621 - acc: 0.5795 
 768/1760 [============>.................] - ETA: 53s - loss: 0.6595 - acc: 0.5859
 832/1760 [=============>................] - ETA: 50s - loss: 0.6566 - acc: 0.5962
 896/1760 [==============>...............] - ETA: 47s - loss: 0.6563 - acc: 0.5938
 960/1760 [===============>..............] - ETA: 43s - loss: 0.6517 - acc: 0.6000
1024/1760 [================>.............] - ETA: 41s - loss: 0.6515 - acc: 0.6006
1088/1760 [=================>............] - ETA: 37s - loss: 0.6511 - acc: 0.6002
1152/1760 [==================>...........] - ETA: 34s - loss: 0.6509 - acc: 0.6016
1216/1760 [===================>..........] - ETA: 30s - loss: 0.6491 - acc: 0.6077
1280/1760 [====================>.........] - ETA: 27s - loss: 0.6482 - acc: 0.6109
1344/1760 [=====================>........] - ETA: 23s - loss: 0.6461 - acc: 0.6153
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6470 - acc: 0.6158
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6473 - acc: 0.6135
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6459 - acc: 0.6165
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6462 - acc: 0.6181 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6455 - acc: 0.6172
1728/1760 [============================>.] - ETA: 1s - loss: 0.6444 - acc: 0.6169
1760/1760 [==============================] - 109s 62ms/step - loss: 0.6443 - acc: 0.6153 - val_loss: 0.6833 - val_acc: 0.6071

Epoch 00008: val_acc did not improve from 0.63776
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:54 - loss: 0.6001 - acc: 0.6562
 128/1760 [=>............................] - ETA: 1:44 - loss: 0.6149 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 1:39 - loss: 0.6079 - acc: 0.6510
 256/1760 [===>..........................] - ETA: 1:37 - loss: 0.6129 - acc: 0.6562
 320/1760 [====>.........................] - ETA: 1:29 - loss: 0.5993 - acc: 0.6750
 384/1760 [=====>........................] - ETA: 1:25 - loss: 0.5950 - acc: 0.6693
 448/1760 [======>.......................] - ETA: 1:19 - loss: 0.6037 - acc: 0.6607
 512/1760 [=======>......................] - ETA: 1:15 - loss: 0.6059 - acc: 0.6562
 576/1760 [========>.....................] - ETA: 1:10 - loss: 0.6076 - acc: 0.6632
 640/1760 [=========>....................] - ETA: 1:06 - loss: 0.6092 - acc: 0.6609
 704/1760 [===========>..................] - ETA: 1:03 - loss: 0.6090 - acc: 0.6648
 768/1760 [============>.................] - ETA: 1:00 - loss: 0.6170 - acc: 0.6576
 832/1760 [=============>................] - ETA: 56s - loss: 0.6170 - acc: 0.6635 
 896/1760 [==============>...............] - ETA: 53s - loss: 0.6162 - acc: 0.6607
 960/1760 [===============>..............] - ETA: 49s - loss: 0.6200 - acc: 0.6583
1024/1760 [================>.............] - ETA: 45s - loss: 0.6203 - acc: 0.6582
1088/1760 [=================>............] - ETA: 42s - loss: 0.6218 - acc: 0.6526
1152/1760 [==================>...........] - ETA: 38s - loss: 0.6196 - acc: 0.6571
1216/1760 [===================>..........] - ETA: 34s - loss: 0.6147 - acc: 0.6620
1280/1760 [====================>.........] - ETA: 30s - loss: 0.6166 - acc: 0.6617
1344/1760 [=====================>........] - ETA: 26s - loss: 0.6209 - acc: 0.6577
1408/1760 [=======================>......] - ETA: 22s - loss: 0.6213 - acc: 0.6577
1472/1760 [========================>.....] - ETA: 18s - loss: 0.6227 - acc: 0.6562
1536/1760 [=========================>....] - ETA: 14s - loss: 0.6213 - acc: 0.6576
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6243 - acc: 0.6550
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6255 - acc: 0.6538 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6247 - acc: 0.6551
1760/1760 [==============================] - 122s 69ms/step - loss: 0.6237 - acc: 0.6562 - val_loss: 0.6522 - val_acc: 0.6224

Epoch 00009: val_acc did not improve from 0.63776
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:51 - loss: 0.6341 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:53 - loss: 0.6697 - acc: 0.6172
 192/1760 [==>...........................] - ETA: 1:43 - loss: 0.6897 - acc: 0.5469
 256/1760 [===>..........................] - ETA: 1:40 - loss: 0.6830 - acc: 0.5469
 320/1760 [====>.........................] - ETA: 1:38 - loss: 0.6736 - acc: 0.5625
 384/1760 [=====>........................] - ETA: 1:34 - loss: 0.6608 - acc: 0.5859
 448/1760 [======>.......................] - ETA: 1:31 - loss: 0.6617 - acc: 0.5893
 512/1760 [=======>......................] - ETA: 1:27 - loss: 0.6654 - acc: 0.5859
 576/1760 [========>.....................] - ETA: 1:24 - loss: 0.6584 - acc: 0.5955
 640/1760 [=========>....................] - ETA: 1:19 - loss: 0.6521 - acc: 0.6078
 704/1760 [===========>..................] - ETA: 1:15 - loss: 0.6473 - acc: 0.6108
 768/1760 [============>.................] - ETA: 1:11 - loss: 0.6471 - acc: 0.6107
 832/1760 [=============>................] - ETA: 1:06 - loss: 0.6397 - acc: 0.6178
 896/1760 [==============>...............] - ETA: 1:02 - loss: 0.6422 - acc: 0.6194
 960/1760 [===============>..............] - ETA: 58s - loss: 0.6400 - acc: 0.6229 
1024/1760 [================>.............] - ETA: 54s - loss: 0.6389 - acc: 0.6250
1088/1760 [=================>............] - ETA: 49s - loss: 0.6362 - acc: 0.6278
1152/1760 [==================>...........] - ETA: 45s - loss: 0.6344 - acc: 0.6293
1216/1760 [===================>..........] - ETA: 40s - loss: 0.6331 - acc: 0.6340
1280/1760 [====================>.........] - ETA: 35s - loss: 0.6325 - acc: 0.6359
1344/1760 [=====================>........] - ETA: 30s - loss: 0.6337 - acc: 0.6324
1408/1760 [=======================>......] - ETA: 26s - loss: 0.6338 - acc: 0.6321
1472/1760 [========================>.....] - ETA: 21s - loss: 0.6341 - acc: 0.6338
1536/1760 [=========================>....] - ETA: 16s - loss: 0.6331 - acc: 0.6374
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6325 - acc: 0.6400
1664/1760 [===========================>..] - ETA: 7s - loss: 0.6354 - acc: 0.6346 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6388 - acc: 0.6331
1760/1760 [==============================] - 137s 78ms/step - loss: 0.6383 - acc: 0.6335 - val_loss: 0.7469 - val_acc: 0.5663

Epoch 00010: val_acc did not improve from 0.63776
样本个数 200
样本个数 400
window_select6-10.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 19s
128/400 [========>.....................] - ETA: 9s 
192/400 [=============>................] - ETA: 6s
256/400 [==================>...........] - ETA: 3s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 10s 24ms/step
loss: 0.6646333575248718
acc: 0.6075
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7da45b36d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7da45b36d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7da45b5ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7da45b5ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8682471490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8682471490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8693674190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8693674190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7da4518350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7da4518350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7da440f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7da440f5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e401b2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e401b2390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7da4540fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7da4540fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7da45727d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7da45727d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7da4324810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7da4324810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e80473c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7e80473c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7da4572f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7da4572f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7da4231490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7da4231490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d8077f3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d8077f3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d80788690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d80788690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7da42e9a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7da42e9a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d8077f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d8077f610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d80773f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d80773f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d80776c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d80776c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d806ec4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d806ec4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d6039c7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d6039c7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d807768d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d807768d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d602f5cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d602f5cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d806f8ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d806f8ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d40758110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d40758110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d60041e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d60041e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d600d78d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d600d78d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d600b1f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d600b1f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d405b81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d405b81d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d4055f2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d4055f2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d40476750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d40476750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d405b8950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d405b8950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d40507b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d40507b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d40485b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d40485b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d400d9f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d400d9f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d4024f650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d4024f650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d4042e210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d4042e210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d40255550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d40255550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d2070f890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d2070f890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d2068f350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d2068f350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d206a1d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d206a1d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d4021fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d4021fe10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d2067ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d2067ec50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d203ab8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d203ab8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d203c18d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d203c18d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d20325dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d20325dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d2067ed90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d2067ed90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d205e3b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d205e3b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d200c3ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d200c3ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d006bae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d006bae50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d201b07d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d201b07d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d00774fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d00774fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d202432d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d202432d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d0075add0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d0075add0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d004a4bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d004a4bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d2047f810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d2047f810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d00415450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d00415450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d004912d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d004912d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d001ce7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7d001ce7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d001c3fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7d001c3fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d001a8650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d001a8650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d001ce690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7d001ce690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d0022a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d0022a550>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 5:43 - loss: 0.7134 - acc: 0.5625
 128/1760 [=>............................] - ETA: 3:25 - loss: 0.7565 - acc: 0.5469
 192/1760 [==>...........................] - ETA: 2:37 - loss: 0.8364 - acc: 0.4740
 256/1760 [===>..........................] - ETA: 2:08 - loss: 0.7995 - acc: 0.5000
 320/1760 [====>.........................] - ETA: 1:50 - loss: 0.8068 - acc: 0.4813
 384/1760 [=====>........................] - ETA: 1:40 - loss: 0.8052 - acc: 0.4740
 448/1760 [======>.......................] - ETA: 1:30 - loss: 0.7874 - acc: 0.4821
 512/1760 [=======>......................] - ETA: 1:24 - loss: 0.7759 - acc: 0.4941
 576/1760 [========>.....................] - ETA: 1:17 - loss: 0.7755 - acc: 0.4844
 640/1760 [=========>....................] - ETA: 1:10 - loss: 0.7676 - acc: 0.4953
 704/1760 [===========>..................] - ETA: 1:05 - loss: 0.7656 - acc: 0.4915
 768/1760 [============>.................] - ETA: 1:00 - loss: 0.7580 - acc: 0.5052
 832/1760 [=============>................] - ETA: 56s - loss: 0.7567 - acc: 0.4976 
 896/1760 [==============>...............] - ETA: 51s - loss: 0.7563 - acc: 0.4989
 960/1760 [===============>..............] - ETA: 47s - loss: 0.7540 - acc: 0.5052
1024/1760 [================>.............] - ETA: 42s - loss: 0.7516 - acc: 0.5059
1088/1760 [=================>............] - ETA: 38s - loss: 0.7482 - acc: 0.5092
1152/1760 [==================>...........] - ETA: 35s - loss: 0.7449 - acc: 0.5087
1216/1760 [===================>..........] - ETA: 31s - loss: 0.7454 - acc: 0.5090
1280/1760 [====================>.........] - ETA: 27s - loss: 0.7431 - acc: 0.5117
1344/1760 [=====================>........] - ETA: 23s - loss: 0.7462 - acc: 0.5045
1408/1760 [=======================>......] - ETA: 19s - loss: 0.7441 - acc: 0.5057
1472/1760 [========================>.....] - ETA: 16s - loss: 0.7406 - acc: 0.5082
1536/1760 [=========================>....] - ETA: 12s - loss: 0.7397 - acc: 0.5065
1600/1760 [==========================>...] - ETA: 9s - loss: 0.7389 - acc: 0.5081 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7374 - acc: 0.5066
1728/1760 [============================>.] - ETA: 1s - loss: 0.7370 - acc: 0.5069
1760/1760 [==============================] - 106s 60ms/step - loss: 0.7361 - acc: 0.5068 - val_loss: 0.6678 - val_acc: 0.6071

Epoch 00001: val_acc improved from -inf to 0.60714, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window09/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:20 - loss: 0.7022 - acc: 0.5312
 128/1760 [=>............................] - ETA: 1:15 - loss: 0.7250 - acc: 0.5234
 192/1760 [==>...........................] - ETA: 1:13 - loss: 0.7176 - acc: 0.5156
 256/1760 [===>..........................] - ETA: 1:10 - loss: 0.7047 - acc: 0.5352
 320/1760 [====>.........................] - ETA: 1:07 - loss: 0.7056 - acc: 0.5375
 384/1760 [=====>........................] - ETA: 1:05 - loss: 0.7029 - acc: 0.5365
 448/1760 [======>.......................] - ETA: 1:02 - loss: 0.7013 - acc: 0.5379
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6971 - acc: 0.5410
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6969 - acc: 0.5469 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6962 - acc: 0.5500
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6914 - acc: 0.5540
 768/1760 [============>.................] - ETA: 49s - loss: 0.6942 - acc: 0.5495
 832/1760 [=============>................] - ETA: 46s - loss: 0.6934 - acc: 0.5481
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6940 - acc: 0.5446
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6953 - acc: 0.5437
1024/1760 [================>.............] - ETA: 36s - loss: 0.6924 - acc: 0.5498
1088/1760 [=================>............] - ETA: 33s - loss: 0.6974 - acc: 0.5450
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6965 - acc: 0.5486
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6991 - acc: 0.5493
1280/1760 [====================>.........] - ETA: 24s - loss: 0.7004 - acc: 0.5477
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6980 - acc: 0.5521
1408/1760 [=======================>......] - ETA: 17s - loss: 0.7009 - acc: 0.5462
1472/1760 [========================>.....] - ETA: 14s - loss: 0.7007 - acc: 0.5462
1536/1760 [=========================>....] - ETA: 11s - loss: 0.7009 - acc: 0.5456
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6991 - acc: 0.5469 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.7000 - acc: 0.5457
1728/1760 [============================>.] - ETA: 1s - loss: 0.6982 - acc: 0.5469
1760/1760 [==============================] - 96s 54ms/step - loss: 0.6962 - acc: 0.5506 - val_loss: 0.6675 - val_acc: 0.5765

Epoch 00002: val_acc did not improve from 0.60714
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:17 - loss: 0.6967 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:21 - loss: 0.6861 - acc: 0.5469
 192/1760 [==>...........................] - ETA: 1:16 - loss: 0.6941 - acc: 0.5312
 256/1760 [===>..........................] - ETA: 1:13 - loss: 0.6854 - acc: 0.5312
 320/1760 [====>.........................] - ETA: 1:14 - loss: 0.6881 - acc: 0.5437
 384/1760 [=====>........................] - ETA: 1:10 - loss: 0.6863 - acc: 0.5443
 448/1760 [======>.......................] - ETA: 1:10 - loss: 0.6822 - acc: 0.5446
 512/1760 [=======>......................] - ETA: 1:06 - loss: 0.6864 - acc: 0.5391
 576/1760 [========>.....................] - ETA: 1:02 - loss: 0.6862 - acc: 0.5382
 640/1760 [=========>....................] - ETA: 1:00 - loss: 0.6799 - acc: 0.5484
 704/1760 [===========>..................] - ETA: 56s - loss: 0.6815 - acc: 0.5412 
 768/1760 [============>.................] - ETA: 52s - loss: 0.6820 - acc: 0.5430
 832/1760 [=============>................] - ETA: 49s - loss: 0.6817 - acc: 0.5445
 896/1760 [==============>...............] - ETA: 46s - loss: 0.6801 - acc: 0.5435
 960/1760 [===============>..............] - ETA: 43s - loss: 0.6857 - acc: 0.5417
1024/1760 [================>.............] - ETA: 39s - loss: 0.6871 - acc: 0.5400
1088/1760 [=================>............] - ETA: 35s - loss: 0.6846 - acc: 0.5478
1152/1760 [==================>...........] - ETA: 32s - loss: 0.6833 - acc: 0.5503
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6817 - acc: 0.5543
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6829 - acc: 0.5508
1344/1760 [=====================>........] - ETA: 22s - loss: 0.6821 - acc: 0.5543
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6793 - acc: 0.5575
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6762 - acc: 0.5659
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6751 - acc: 0.5671
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6735 - acc: 0.5694 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6711 - acc: 0.5751
1728/1760 [============================>.] - ETA: 1s - loss: 0.6693 - acc: 0.5799
1760/1760 [==============================] - 97s 55ms/step - loss: 0.6691 - acc: 0.5801 - val_loss: 0.6526 - val_acc: 0.6020

Epoch 00003: val_acc did not improve from 0.60714
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:36 - loss: 0.7393 - acc: 0.4844
 128/1760 [=>............................] - ETA: 1:26 - loss: 0.7086 - acc: 0.5312
 192/1760 [==>...........................] - ETA: 1:22 - loss: 0.6861 - acc: 0.5521
 256/1760 [===>..........................] - ETA: 1:18 - loss: 0.6837 - acc: 0.5586
 320/1760 [====>.........................] - ETA: 1:13 - loss: 0.6731 - acc: 0.5750
 384/1760 [=====>........................] - ETA: 1:09 - loss: 0.6651 - acc: 0.5885
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6685 - acc: 0.5871
 512/1760 [=======>......................] - ETA: 1:03 - loss: 0.6669 - acc: 0.5957
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6729 - acc: 0.5885
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6760 - acc: 0.5891 
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6712 - acc: 0.5938
 768/1760 [============>.................] - ETA: 50s - loss: 0.6710 - acc: 0.5911
 832/1760 [=============>................] - ETA: 47s - loss: 0.6728 - acc: 0.5853
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6704 - acc: 0.5893
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6736 - acc: 0.5875
1024/1760 [================>.............] - ETA: 37s - loss: 0.6737 - acc: 0.5879
1088/1760 [=================>............] - ETA: 34s - loss: 0.6745 - acc: 0.5836
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6717 - acc: 0.5877
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6703 - acc: 0.5921
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6701 - acc: 0.5898
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6680 - acc: 0.5960
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6648 - acc: 0.6001
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6655 - acc: 0.5985
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6663 - acc: 0.5990
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6666 - acc: 0.5994 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6646 - acc: 0.6010
1728/1760 [============================>.] - ETA: 1s - loss: 0.6665 - acc: 0.6007
1760/1760 [==============================] - 94s 53ms/step - loss: 0.6671 - acc: 0.6006 - val_loss: 0.6310 - val_acc: 0.6480

Epoch 00004: val_acc improved from 0.60714 to 0.64796, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window09/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:18 - loss: 0.7152 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:14 - loss: 0.6651 - acc: 0.6172
 192/1760 [==>...........................] - ETA: 1:12 - loss: 0.6535 - acc: 0.6250
 256/1760 [===>..........................] - ETA: 1:10 - loss: 0.6631 - acc: 0.6172
 320/1760 [====>.........................] - ETA: 1:07 - loss: 0.6786 - acc: 0.6094
 384/1760 [=====>........................] - ETA: 1:06 - loss: 0.6698 - acc: 0.6172
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6695 - acc: 0.6205
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6685 - acc: 0.6133
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6630 - acc: 0.6215 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6554 - acc: 0.6297
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6560 - acc: 0.6293
 768/1760 [============>.................] - ETA: 49s - loss: 0.6575 - acc: 0.6250
 832/1760 [=============>................] - ETA: 46s - loss: 0.6581 - acc: 0.6250
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6615 - acc: 0.6172
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6596 - acc: 0.6198
1024/1760 [================>.............] - ETA: 37s - loss: 0.6613 - acc: 0.6172
1088/1760 [=================>............] - ETA: 34s - loss: 0.6644 - acc: 0.6149
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6656 - acc: 0.6128
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6677 - acc: 0.6069
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6689 - acc: 0.6055
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6704 - acc: 0.6034
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6700 - acc: 0.6037
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6672 - acc: 0.6073
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6674 - acc: 0.6055
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6689 - acc: 0.6044 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6687 - acc: 0.6052
1728/1760 [============================>.] - ETA: 1s - loss: 0.6686 - acc: 0.6053
1760/1760 [==============================] - 94s 53ms/step - loss: 0.6703 - acc: 0.6023 - val_loss: 0.6538 - val_acc: 0.6276

Epoch 00005: val_acc did not improve from 0.64796
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:34 - loss: 0.6770 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:26 - loss: 0.7040 - acc: 0.5625
 192/1760 [==>...........................] - ETA: 1:26 - loss: 0.6811 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 1:20 - loss: 0.6699 - acc: 0.6172
 320/1760 [====>.........................] - ETA: 1:19 - loss: 0.6733 - acc: 0.5938
 384/1760 [=====>........................] - ETA: 1:15 - loss: 0.6673 - acc: 0.6042
 448/1760 [======>.......................] - ETA: 1:10 - loss: 0.6625 - acc: 0.6049
 512/1760 [=======>......................] - ETA: 1:05 - loss: 0.6526 - acc: 0.6191
 576/1760 [========>.....................] - ETA: 1:02 - loss: 0.6540 - acc: 0.6198
 640/1760 [=========>....................] - ETA: 58s - loss: 0.6588 - acc: 0.6188 
 704/1760 [===========>..................] - ETA: 55s - loss: 0.6639 - acc: 0.6136
 768/1760 [============>.................] - ETA: 51s - loss: 0.6632 - acc: 0.6198
 832/1760 [=============>................] - ETA: 48s - loss: 0.6666 - acc: 0.6166
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6633 - acc: 0.6205
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6659 - acc: 0.6198
1024/1760 [================>.............] - ETA: 37s - loss: 0.6655 - acc: 0.6230
1088/1760 [=================>............] - ETA: 34s - loss: 0.6657 - acc: 0.6232
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6659 - acc: 0.6224
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6666 - acc: 0.6209
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6646 - acc: 0.6234
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6659 - acc: 0.6205
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6641 - acc: 0.6214
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6622 - acc: 0.6236
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6615 - acc: 0.6211
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6591 - acc: 0.6238 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6585 - acc: 0.6250
1728/1760 [============================>.] - ETA: 1s - loss: 0.6599 - acc: 0.6238
1760/1760 [==============================] - 100s 57ms/step - loss: 0.6594 - acc: 0.6250 - val_loss: 0.6623 - val_acc: 0.5561

Epoch 00006: val_acc did not improve from 0.64796
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:33 - loss: 0.6546 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:25 - loss: 0.6476 - acc: 0.5938
 192/1760 [==>...........................] - ETA: 1:23 - loss: 0.6651 - acc: 0.5677
 256/1760 [===>..........................] - ETA: 1:21 - loss: 0.6841 - acc: 0.5625
 320/1760 [====>.........................] - ETA: 1:19 - loss: 0.6749 - acc: 0.5781
 384/1760 [=====>........................] - ETA: 1:16 - loss: 0.6609 - acc: 0.6094
 448/1760 [======>.......................] - ETA: 1:15 - loss: 0.6645 - acc: 0.6049
 512/1760 [=======>......................] - ETA: 1:12 - loss: 0.6682 - acc: 0.6094
 576/1760 [========>.....................] - ETA: 1:09 - loss: 0.6668 - acc: 0.6042
 640/1760 [=========>....................] - ETA: 1:06 - loss: 0.6645 - acc: 0.6078
 704/1760 [===========>..................] - ETA: 1:02 - loss: 0.6610 - acc: 0.6151
 768/1760 [============>.................] - ETA: 58s - loss: 0.6614 - acc: 0.6081 
 832/1760 [=============>................] - ETA: 54s - loss: 0.6650 - acc: 0.6058
 896/1760 [==============>...............] - ETA: 51s - loss: 0.6618 - acc: 0.6071
 960/1760 [===============>..............] - ETA: 47s - loss: 0.6603 - acc: 0.6115
1024/1760 [================>.............] - ETA: 43s - loss: 0.6572 - acc: 0.6133
1088/1760 [=================>............] - ETA: 40s - loss: 0.6570 - acc: 0.6131
1152/1760 [==================>...........] - ETA: 36s - loss: 0.6557 - acc: 0.6155
1216/1760 [===================>..........] - ETA: 32s - loss: 0.6511 - acc: 0.6217
1280/1760 [====================>.........] - ETA: 28s - loss: 0.6502 - acc: 0.6195
1344/1760 [=====================>........] - ETA: 24s - loss: 0.6502 - acc: 0.6213
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6478 - acc: 0.6243
1472/1760 [========================>.....] - ETA: 17s - loss: 0.6465 - acc: 0.6264
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6469 - acc: 0.6250
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6459 - acc: 0.6250 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6473 - acc: 0.6226
1728/1760 [============================>.] - ETA: 1s - loss: 0.6485 - acc: 0.6209
1760/1760 [==============================] - 111s 63ms/step - loss: 0.6474 - acc: 0.6216 - val_loss: 0.6386 - val_acc: 0.6480

Epoch 00007: val_acc did not improve from 0.64796
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:29 - loss: 0.6710 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:31 - loss: 0.6374 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 1:31 - loss: 0.6421 - acc: 0.6458
 256/1760 [===>..........................] - ETA: 1:26 - loss: 0.6549 - acc: 0.6211
 320/1760 [====>.........................] - ETA: 1:28 - loss: 0.6581 - acc: 0.6312
 384/1760 [=====>........................] - ETA: 1:24 - loss: 0.6541 - acc: 0.6302
 448/1760 [======>.......................] - ETA: 1:19 - loss: 0.6389 - acc: 0.6429
 512/1760 [=======>......................] - ETA: 1:17 - loss: 0.6421 - acc: 0.6328
 576/1760 [========>.....................] - ETA: 1:12 - loss: 0.6436 - acc: 0.6267
 640/1760 [=========>....................] - ETA: 1:08 - loss: 0.6468 - acc: 0.6297
 704/1760 [===========>..................] - ETA: 1:04 - loss: 0.6521 - acc: 0.6264
 768/1760 [============>.................] - ETA: 1:00 - loss: 0.6487 - acc: 0.6276
 832/1760 [=============>................] - ETA: 56s - loss: 0.6478 - acc: 0.6262 
 896/1760 [==============>...............] - ETA: 52s - loss: 0.6472 - acc: 0.6272
 960/1760 [===============>..............] - ETA: 48s - loss: 0.6432 - acc: 0.6333
1024/1760 [================>.............] - ETA: 44s - loss: 0.6460 - acc: 0.6299
1088/1760 [=================>............] - ETA: 40s - loss: 0.6424 - acc: 0.6369
1152/1760 [==================>...........] - ETA: 36s - loss: 0.6424 - acc: 0.6345
1216/1760 [===================>..........] - ETA: 32s - loss: 0.6398 - acc: 0.6390
1280/1760 [====================>.........] - ETA: 28s - loss: 0.6443 - acc: 0.6352
1344/1760 [=====================>........] - ETA: 24s - loss: 0.6473 - acc: 0.6317
1408/1760 [=======================>......] - ETA: 21s - loss: 0.6476 - acc: 0.6300
1472/1760 [========================>.....] - ETA: 17s - loss: 0.6450 - acc: 0.6332
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6462 - acc: 0.6315
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6471 - acc: 0.6312 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6464 - acc: 0.6310
1728/1760 [============================>.] - ETA: 1s - loss: 0.6459 - acc: 0.6325
1760/1760 [==============================] - 113s 64ms/step - loss: 0.6469 - acc: 0.6324 - val_loss: 0.6456 - val_acc: 0.5765

Epoch 00008: val_acc did not improve from 0.64796
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:38 - loss: 0.6666 - acc: 0.5781
 128/1760 [=>............................] - ETA: 1:47 - loss: 0.6403 - acc: 0.6094
 192/1760 [==>...........................] - ETA: 1:40 - loss: 0.6476 - acc: 0.6094
 256/1760 [===>..........................] - ETA: 1:39 - loss: 0.6384 - acc: 0.6328
 320/1760 [====>.........................] - ETA: 1:35 - loss: 0.6373 - acc: 0.6344
 384/1760 [=====>........................] - ETA: 1:31 - loss: 0.6472 - acc: 0.6302
 448/1760 [======>.......................] - ETA: 1:28 - loss: 0.6388 - acc: 0.6384
 512/1760 [=======>......................] - ETA: 1:24 - loss: 0.6389 - acc: 0.6309
 576/1760 [========>.....................] - ETA: 1:21 - loss: 0.6336 - acc: 0.6389
 640/1760 [=========>....................] - ETA: 1:18 - loss: 0.6312 - acc: 0.6391
 704/1760 [===========>..................] - ETA: 1:15 - loss: 0.6281 - acc: 0.6420
 768/1760 [============>.................] - ETA: 1:11 - loss: 0.6299 - acc: 0.6393
 832/1760 [=============>................] - ETA: 1:06 - loss: 0.6362 - acc: 0.6346
 896/1760 [==============>...............] - ETA: 1:02 - loss: 0.6359 - acc: 0.6373
 960/1760 [===============>..............] - ETA: 56s - loss: 0.6378 - acc: 0.6375 
1024/1760 [================>.............] - ETA: 52s - loss: 0.6384 - acc: 0.6357
1088/1760 [=================>............] - ETA: 48s - loss: 0.6391 - acc: 0.6342
1152/1760 [==================>...........] - ETA: 43s - loss: 0.6367 - acc: 0.6389
1216/1760 [===================>..........] - ETA: 38s - loss: 0.6364 - acc: 0.6390
1280/1760 [====================>.........] - ETA: 34s - loss: 0.6338 - acc: 0.6438
1344/1760 [=====================>........] - ETA: 29s - loss: 0.6353 - acc: 0.6414
1408/1760 [=======================>......] - ETA: 24s - loss: 0.6380 - acc: 0.6364
1472/1760 [========================>.....] - ETA: 20s - loss: 0.6367 - acc: 0.6372
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6394 - acc: 0.6328
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6379 - acc: 0.6344
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6374 - acc: 0.6352 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6387 - acc: 0.6348
1760/1760 [==============================] - 132s 75ms/step - loss: 0.6393 - acc: 0.6330 - val_loss: 0.6633 - val_acc: 0.6327

Epoch 00009: val_acc did not improve from 0.64796
Epoch 10/10

  64/1760 [>.............................] - ETA: 2:22 - loss: 0.6955 - acc: 0.5469
 128/1760 [=>............................] - ETA: 2:15 - loss: 0.6760 - acc: 0.5547
 192/1760 [==>...........................] - ETA: 2:04 - loss: 0.6675 - acc: 0.5729
 256/1760 [===>..........................] - ETA: 1:57 - loss: 0.6456 - acc: 0.6172
 320/1760 [====>.........................] - ETA: 1:50 - loss: 0.6421 - acc: 0.6188
 384/1760 [=====>........................] - ETA: 1:45 - loss: 0.6436 - acc: 0.6120
 448/1760 [======>.......................] - ETA: 1:40 - loss: 0.6431 - acc: 0.6250
 512/1760 [=======>......................] - ETA: 1:34 - loss: 0.6474 - acc: 0.6211
 576/1760 [========>.....................] - ETA: 1:28 - loss: 0.6519 - acc: 0.6181
 640/1760 [=========>....................] - ETA: 1:24 - loss: 0.6612 - acc: 0.6031
 704/1760 [===========>..................] - ETA: 1:19 - loss: 0.6597 - acc: 0.6094
 768/1760 [============>.................] - ETA: 1:15 - loss: 0.6551 - acc: 0.6211
 832/1760 [=============>................] - ETA: 1:09 - loss: 0.6563 - acc: 0.6178
 896/1760 [==============>...............] - ETA: 1:04 - loss: 0.6560 - acc: 0.6228
 960/1760 [===============>..............] - ETA: 1:00 - loss: 0.6526 - acc: 0.6250
1024/1760 [================>.............] - ETA: 54s - loss: 0.6504 - acc: 0.6230 
1088/1760 [=================>............] - ETA: 50s - loss: 0.6478 - acc: 0.6232
1152/1760 [==================>...........] - ETA: 45s - loss: 0.6464 - acc: 0.6241
1216/1760 [===================>..........] - ETA: 40s - loss: 0.6451 - acc: 0.6283
1280/1760 [====================>.........] - ETA: 35s - loss: 0.6456 - acc: 0.6305
1344/1760 [=====================>........] - ETA: 30s - loss: 0.6432 - acc: 0.6347
1408/1760 [=======================>......] - ETA: 25s - loss: 0.6466 - acc: 0.6307
1472/1760 [========================>.....] - ETA: 20s - loss: 0.6455 - acc: 0.6311
1536/1760 [=========================>....] - ETA: 16s - loss: 0.6441 - acc: 0.6302
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6449 - acc: 0.6288
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6461 - acc: 0.6298 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6467 - acc: 0.6302
1760/1760 [==============================] - 132s 75ms/step - loss: 0.6454 - acc: 0.6307 - val_loss: 0.6250 - val_acc: 0.6684

Epoch 00010: val_acc improved from 0.64796 to 0.66837, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window09/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
样本个数 200
样本个数 400
window_select6-10.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 23s
128/400 [========>.....................] - ETA: 12s
192/400 [=============>................] - ETA: 7s 
256/400 [==================>...........] - ETA: 4s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 11s 26ms/step
loss: 0.6634290742874146
acc: 0.605
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7c6028b6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f7c6028b6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7c4039c050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f7c4039c050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86824713d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f86824713d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80381da850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f80381da850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c602a07d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c602a07d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d40492790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d40492790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80381da650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f80381da650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c602a0a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c602a0a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c6008f490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c6008f490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c407275d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c407275d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c6003f210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c6003f210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c407ec510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c407ec510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d401acf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7d401acf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c6008f9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c6008f9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c40474510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c40474510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c600d1950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c600d1950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c407bc210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c407bc210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80383ab7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f80383ab7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f803839ca10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f803839ca10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c00796750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c00796750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7f0004e490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7f0004e490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e4011fd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7e4011fd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c007d8690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c007d8690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c60316890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c60316890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c00443d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c00443d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c00598610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c00598610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c00613dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c00613dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c007e3750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c007e3750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c002b00d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7c002b00d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c001a3a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c001a3a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c002a4d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c002a4d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c003af810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c003af810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c002b9b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c002b9b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7be0717150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7be0717150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7be05cf950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7be05cf950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c00048ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7c00048ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c002a4690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7c002a4690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7be0527e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7be0527e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7be0515350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7be0515350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7be030c150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7be030c150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7be0379550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7be0379550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7be0515ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7be0515ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7be01d6790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7be01d6790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7be044ca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7be044ca90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c202ab550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7c202ab550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7be0176750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7be0176750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7be01d0d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7be01d0d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7be00cd890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7be00cd890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bd858da90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bd858da90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bd85c05d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bd85c05d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bd849c150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bd849c150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7be0453810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7be0453810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bd851bc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bd851bc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bd829e050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7bd829e050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bd82b7990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7bd82b7990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bd857ea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bd857ea10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bd8142610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bd8142610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bd829e390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7bd829e390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ba06c0e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f7ba06c0e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ba071a150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f7ba071a150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ba0612550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ba0612550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bd81b1f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f7bd81b1f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ba04ba190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f7ba04ba190>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 5:59 - loss: 0.7417 - acc: 0.4844
 128/1760 [=>............................] - ETA: 3:32 - loss: 0.7751 - acc: 0.4922
 192/1760 [==>...........................] - ETA: 2:41 - loss: 0.7574 - acc: 0.4896
 256/1760 [===>..........................] - ETA: 2:15 - loss: 0.7547 - acc: 0.4961
 320/1760 [====>.........................] - ETA: 1:57 - loss: 0.7672 - acc: 0.4656
 384/1760 [=====>........................] - ETA: 1:43 - loss: 0.7527 - acc: 0.4766
 448/1760 [======>.......................] - ETA: 1:34 - loss: 0.7475 - acc: 0.4866
 512/1760 [=======>......................] - ETA: 1:26 - loss: 0.7480 - acc: 0.4805
 576/1760 [========>.....................] - ETA: 1:19 - loss: 0.7394 - acc: 0.4948
 640/1760 [=========>....................] - ETA: 1:14 - loss: 0.7442 - acc: 0.4891
 704/1760 [===========>..................] - ETA: 1:09 - loss: 0.7351 - acc: 0.4957
 768/1760 [============>.................] - ETA: 1:03 - loss: 0.7398 - acc: 0.4870
 832/1760 [=============>................] - ETA: 57s - loss: 0.7358 - acc: 0.5000 
 896/1760 [==============>...............] - ETA: 54s - loss: 0.7339 - acc: 0.5033
 960/1760 [===============>..............] - ETA: 49s - loss: 0.7362 - acc: 0.5000
1024/1760 [================>.............] - ETA: 45s - loss: 0.7354 - acc: 0.5010
1088/1760 [=================>............] - ETA: 41s - loss: 0.7335 - acc: 0.5055
1152/1760 [==================>...........] - ETA: 36s - loss: 0.7285 - acc: 0.5104
1216/1760 [===================>..........] - ETA: 32s - loss: 0.7285 - acc: 0.5099
1280/1760 [====================>.........] - ETA: 28s - loss: 0.7276 - acc: 0.5086
1344/1760 [=====================>........] - ETA: 24s - loss: 0.7255 - acc: 0.5119
1408/1760 [=======================>......] - ETA: 20s - loss: 0.7241 - acc: 0.5142
1472/1760 [========================>.....] - ETA: 16s - loss: 0.7268 - acc: 0.5122
1536/1760 [=========================>....] - ETA: 12s - loss: 0.7254 - acc: 0.5098
1600/1760 [==========================>...] - ETA: 9s - loss: 0.7232 - acc: 0.5125 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7196 - acc: 0.5192
1728/1760 [============================>.] - ETA: 1s - loss: 0.7192 - acc: 0.5197
1760/1760 [==============================] - 105s 60ms/step - loss: 0.7194 - acc: 0.5193 - val_loss: 0.6849 - val_acc: 0.5408

Epoch 00001: val_acc improved from -inf to 0.54082, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window10/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:15 - loss: 0.6930 - acc: 0.5312
 128/1760 [=>............................] - ETA: 1:19 - loss: 0.7092 - acc: 0.5391
 192/1760 [==>...........................] - ETA: 1:16 - loss: 0.7015 - acc: 0.5677
 256/1760 [===>..........................] - ETA: 1:12 - loss: 0.6880 - acc: 0.5859
 320/1760 [====>.........................] - ETA: 1:14 - loss: 0.6792 - acc: 0.6000
 384/1760 [=====>........................] - ETA: 1:10 - loss: 0.6649 - acc: 0.6250
 448/1760 [======>.......................] - ETA: 1:06 - loss: 0.6651 - acc: 0.6205
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6688 - acc: 0.6152
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6711 - acc: 0.6059 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6744 - acc: 0.6031
 704/1760 [===========>..................] - ETA: 51s - loss: 0.6757 - acc: 0.5923
 768/1760 [============>.................] - ETA: 48s - loss: 0.6767 - acc: 0.5885
 832/1760 [=============>................] - ETA: 44s - loss: 0.6722 - acc: 0.5974
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6708 - acc: 0.5926
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6708 - acc: 0.5938
1024/1760 [================>.............] - ETA: 35s - loss: 0.6717 - acc: 0.5898
1088/1760 [=================>............] - ETA: 32s - loss: 0.6737 - acc: 0.5836
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6733 - acc: 0.5859
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6777 - acc: 0.5798
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6745 - acc: 0.5852
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6731 - acc: 0.5900
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6747 - acc: 0.5852
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6728 - acc: 0.5870
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6732 - acc: 0.5846
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6741 - acc: 0.5856 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6751 - acc: 0.5817
1728/1760 [============================>.] - ETA: 1s - loss: 0.6764 - acc: 0.5781
1760/1760 [==============================] - 90s 51ms/step - loss: 0.6753 - acc: 0.5784 - val_loss: 0.6427 - val_acc: 0.6071

Epoch 00002: val_acc improved from 0.54082 to 0.60714, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window10/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:26 - loss: 0.6393 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:19 - loss: 0.6786 - acc: 0.5547
 192/1760 [==>...........................] - ETA: 1:15 - loss: 0.6905 - acc: 0.5260
 256/1760 [===>..........................] - ETA: 1:10 - loss: 0.7092 - acc: 0.5078
 320/1760 [====>.........................] - ETA: 1:07 - loss: 0.7061 - acc: 0.5000
 384/1760 [=====>........................] - ETA: 1:03 - loss: 0.6941 - acc: 0.5182
 448/1760 [======>.......................] - ETA: 1:00 - loss: 0.6954 - acc: 0.5223
 512/1760 [=======>......................] - ETA: 59s - loss: 0.6967 - acc: 0.5156 
 576/1760 [========>.....................] - ETA: 56s - loss: 0.7000 - acc: 0.5122
 640/1760 [=========>....................] - ETA: 53s - loss: 0.7019 - acc: 0.5172
 704/1760 [===========>..................] - ETA: 50s - loss: 0.6947 - acc: 0.5341
 768/1760 [============>.................] - ETA: 48s - loss: 0.6876 - acc: 0.5495
 832/1760 [=============>................] - ETA: 45s - loss: 0.6881 - acc: 0.5529
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6918 - acc: 0.5502
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6897 - acc: 0.5531
1024/1760 [================>.............] - ETA: 35s - loss: 0.6885 - acc: 0.5566
1088/1760 [=================>............] - ETA: 32s - loss: 0.6839 - acc: 0.5616
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6803 - acc: 0.5686
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6791 - acc: 0.5691
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6785 - acc: 0.5703
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6784 - acc: 0.5692
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6768 - acc: 0.5696
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6761 - acc: 0.5679
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6774 - acc: 0.5671
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6774 - acc: 0.5675 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6745 - acc: 0.5703
1728/1760 [============================>.] - ETA: 1s - loss: 0.6723 - acc: 0.5723
1760/1760 [==============================] - 90s 51ms/step - loss: 0.6726 - acc: 0.5733 - val_loss: 0.6415 - val_acc: 0.6224

Epoch 00003: val_acc improved from 0.60714 to 0.62245, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window10/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:32 - loss: 0.6140 - acc: 0.6406
 128/1760 [=>............................] - ETA: 1:21 - loss: 0.6528 - acc: 0.6250
 192/1760 [==>...........................] - ETA: 1:26 - loss: 0.6586 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 1:22 - loss: 0.6411 - acc: 0.6172
 320/1760 [====>.........................] - ETA: 1:18 - loss: 0.6243 - acc: 0.6281
 384/1760 [=====>........................] - ETA: 1:13 - loss: 0.6327 - acc: 0.6172
 448/1760 [======>.......................] - ETA: 1:11 - loss: 0.6322 - acc: 0.6250
 512/1760 [=======>......................] - ETA: 1:06 - loss: 0.6370 - acc: 0.6270
 576/1760 [========>.....................] - ETA: 1:02 - loss: 0.6404 - acc: 0.6233
 640/1760 [=========>....................] - ETA: 58s - loss: 0.6443 - acc: 0.6219 
 704/1760 [===========>..................] - ETA: 55s - loss: 0.6383 - acc: 0.6293
 768/1760 [============>.................] - ETA: 51s - loss: 0.6494 - acc: 0.6198
 832/1760 [=============>................] - ETA: 48s - loss: 0.6546 - acc: 0.6154
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6544 - acc: 0.6138
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6511 - acc: 0.6167
1024/1760 [================>.............] - ETA: 37s - loss: 0.6514 - acc: 0.6133
1088/1760 [=================>............] - ETA: 34s - loss: 0.6532 - acc: 0.6085
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6527 - acc: 0.6102
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6535 - acc: 0.6086
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6547 - acc: 0.6070
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6568 - acc: 0.6057
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6569 - acc: 0.6037
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6553 - acc: 0.6080
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6536 - acc: 0.6087
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6516 - acc: 0.6119 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6545 - acc: 0.6100
1728/1760 [============================>.] - ETA: 1s - loss: 0.6542 - acc: 0.6100
1760/1760 [==============================] - 93s 53ms/step - loss: 0.6561 - acc: 0.6068 - val_loss: 0.6296 - val_acc: 0.6276

Epoch 00004: val_acc improved from 0.62245 to 0.62755, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window10/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:35 - loss: 0.6291 - acc: 0.6562
 128/1760 [=>............................] - ETA: 1:23 - loss: 0.6385 - acc: 0.6719
 192/1760 [==>...........................] - ETA: 1:19 - loss: 0.6445 - acc: 0.6562
 256/1760 [===>..........................] - ETA: 1:15 - loss: 0.6461 - acc: 0.6602
 320/1760 [====>.........................] - ETA: 1:13 - loss: 0.6364 - acc: 0.6594
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6345 - acc: 0.6510
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6286 - acc: 0.6473
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6314 - acc: 0.6465
 576/1760 [========>.....................] - ETA: 57s - loss: 0.6352 - acc: 0.6458 
 640/1760 [=========>....................] - ETA: 53s - loss: 0.6372 - acc: 0.6422
 704/1760 [===========>..................] - ETA: 50s - loss: 0.6423 - acc: 0.6420
 768/1760 [============>.................] - ETA: 47s - loss: 0.6378 - acc: 0.6445
 832/1760 [=============>................] - ETA: 44s - loss: 0.6370 - acc: 0.6454
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6416 - acc: 0.6417
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6459 - acc: 0.6375
1024/1760 [================>.............] - ETA: 35s - loss: 0.6462 - acc: 0.6338
1088/1760 [=================>............] - ETA: 32s - loss: 0.6477 - acc: 0.6342
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6486 - acc: 0.6337
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6466 - acc: 0.6340
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6491 - acc: 0.6305
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6509 - acc: 0.6295
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6497 - acc: 0.6300
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6496 - acc: 0.6298
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6486 - acc: 0.6328
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6475 - acc: 0.6344 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6502 - acc: 0.6322
1728/1760 [============================>.] - ETA: 1s - loss: 0.6496 - acc: 0.6319
1760/1760 [==============================] - 90s 51ms/step - loss: 0.6487 - acc: 0.6324 - val_loss: 0.6554 - val_acc: 0.6327

Epoch 00005: val_acc improved from 0.62755 to 0.63265, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window10/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:21 - loss: 0.6225 - acc: 0.6562
 128/1760 [=>............................] - ETA: 1:19 - loss: 0.6500 - acc: 0.6250
 192/1760 [==>...........................] - ETA: 1:16 - loss: 0.6567 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 1:15 - loss: 0.6502 - acc: 0.6250
 320/1760 [====>.........................] - ETA: 1:11 - loss: 0.6502 - acc: 0.6281
 384/1760 [=====>........................] - ETA: 1:07 - loss: 0.6556 - acc: 0.6250
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6470 - acc: 0.6317
 512/1760 [=======>......................] - ETA: 1:03 - loss: 0.6464 - acc: 0.6309
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6458 - acc: 0.6372
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6481 - acc: 0.6312 
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6457 - acc: 0.6349
 768/1760 [============>.................] - ETA: 50s - loss: 0.6402 - acc: 0.6406
 832/1760 [=============>................] - ETA: 46s - loss: 0.6440 - acc: 0.6370
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6412 - acc: 0.6395
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6383 - acc: 0.6406
1024/1760 [================>.............] - ETA: 37s - loss: 0.6435 - acc: 0.6338
1088/1760 [=================>............] - ETA: 34s - loss: 0.6471 - acc: 0.6268
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6455 - acc: 0.6293
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6436 - acc: 0.6283
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6451 - acc: 0.6250
1344/1760 [=====================>........] - ETA: 22s - loss: 0.6429 - acc: 0.6250
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6428 - acc: 0.6271
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6431 - acc: 0.6291
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6391 - acc: 0.6341
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6360 - acc: 0.6350 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6348 - acc: 0.6364
1728/1760 [============================>.] - ETA: 1s - loss: 0.6335 - acc: 0.6360
1760/1760 [==============================] - 104s 59ms/step - loss: 0.6337 - acc: 0.6364 - val_loss: 0.6507 - val_acc: 0.6378

Epoch 00006: val_acc improved from 0.63265 to 0.63776, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window10/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:31 - loss: 0.5820 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:30 - loss: 0.5953 - acc: 0.7031
 192/1760 [==>...........................] - ETA: 1:30 - loss: 0.6184 - acc: 0.6719
 256/1760 [===>..........................] - ETA: 1:29 - loss: 0.6175 - acc: 0.6797
 320/1760 [====>.........................] - ETA: 1:24 - loss: 0.6252 - acc: 0.6719
 384/1760 [=====>........................] - ETA: 1:23 - loss: 0.6242 - acc: 0.6641
 448/1760 [======>.......................] - ETA: 1:18 - loss: 0.6172 - acc: 0.6607
 512/1760 [=======>......................] - ETA: 1:14 - loss: 0.6333 - acc: 0.6543
 576/1760 [========>.....................] - ETA: 1:12 - loss: 0.6317 - acc: 0.6545
 640/1760 [=========>....................] - ETA: 1:08 - loss: 0.6305 - acc: 0.6578
 704/1760 [===========>..................] - ETA: 1:03 - loss: 0.6320 - acc: 0.6548
 768/1760 [============>.................] - ETA: 1:00 - loss: 0.6396 - acc: 0.6471
 832/1760 [=============>................] - ETA: 56s - loss: 0.6480 - acc: 0.6394 
 896/1760 [==============>...............] - ETA: 52s - loss: 0.6565 - acc: 0.6317
 960/1760 [===============>..............] - ETA: 48s - loss: 0.6568 - acc: 0.6302
1024/1760 [================>.............] - ETA: 44s - loss: 0.6537 - acc: 0.6328
1088/1760 [=================>............] - ETA: 40s - loss: 0.6481 - acc: 0.6379
1152/1760 [==================>...........] - ETA: 36s - loss: 0.6488 - acc: 0.6337
1216/1760 [===================>..........] - ETA: 32s - loss: 0.6461 - acc: 0.6382
1280/1760 [====================>.........] - ETA: 28s - loss: 0.6455 - acc: 0.6383
1344/1760 [=====================>........] - ETA: 24s - loss: 0.6459 - acc: 0.6376
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6457 - acc: 0.6371
1472/1760 [========================>.....] - ETA: 17s - loss: 0.6448 - acc: 0.6393
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6423 - acc: 0.6406
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6437 - acc: 0.6406 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6416 - acc: 0.6418
1728/1760 [============================>.] - ETA: 1s - loss: 0.6439 - acc: 0.6383
1760/1760 [==============================] - 111s 63ms/step - loss: 0.6456 - acc: 0.6369 - val_loss: 0.6284 - val_acc: 0.6480

Epoch 00007: val_acc improved from 0.63776 to 0.64796, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window10/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:50 - loss: 0.6424 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:40 - loss: 0.6175 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 1:36 - loss: 0.6132 - acc: 0.6458
 256/1760 [===>..........................] - ETA: 1:33 - loss: 0.6000 - acc: 0.6797
 320/1760 [====>.........................] - ETA: 1:27 - loss: 0.5970 - acc: 0.6906
 384/1760 [=====>........................] - ETA: 1:21 - loss: 0.5973 - acc: 0.6875
 448/1760 [======>.......................] - ETA: 1:18 - loss: 0.5941 - acc: 0.6942
 512/1760 [=======>......................] - ETA: 1:15 - loss: 0.6059 - acc: 0.6875
 576/1760 [========>.....................] - ETA: 1:11 - loss: 0.6095 - acc: 0.6840
 640/1760 [=========>....................] - ETA: 1:09 - loss: 0.6056 - acc: 0.6891
 704/1760 [===========>..................] - ETA: 1:04 - loss: 0.6065 - acc: 0.6818
 768/1760 [============>.................] - ETA: 1:00 - loss: 0.6135 - acc: 0.6810
 832/1760 [=============>................] - ETA: 56s - loss: 0.6219 - acc: 0.6683 
 896/1760 [==============>...............] - ETA: 52s - loss: 0.6227 - acc: 0.6685
 960/1760 [===============>..............] - ETA: 48s - loss: 0.6228 - acc: 0.6677
1024/1760 [================>.............] - ETA: 44s - loss: 0.6223 - acc: 0.6641
1088/1760 [=================>............] - ETA: 40s - loss: 0.6207 - acc: 0.6673
1152/1760 [==================>...........] - ETA: 36s - loss: 0.6212 - acc: 0.6641
1216/1760 [===================>..........] - ETA: 33s - loss: 0.6241 - acc: 0.6604
1280/1760 [====================>.........] - ETA: 29s - loss: 0.6269 - acc: 0.6594
1344/1760 [=====================>........] - ETA: 25s - loss: 0.6262 - acc: 0.6600
1408/1760 [=======================>......] - ETA: 21s - loss: 0.6250 - acc: 0.6605
1472/1760 [========================>.....] - ETA: 17s - loss: 0.6248 - acc: 0.6590
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6269 - acc: 0.6576
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6270 - acc: 0.6556 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6265 - acc: 0.6562
1728/1760 [============================>.] - ETA: 1s - loss: 0.6251 - acc: 0.6568
1760/1760 [==============================] - 114s 65ms/step - loss: 0.6250 - acc: 0.6580 - val_loss: 0.7073 - val_acc: 0.5612

Epoch 00008: val_acc did not improve from 0.64796
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:44 - loss: 0.7005 - acc: 0.5469
 128/1760 [=>............................] - ETA: 1:56 - loss: 0.6477 - acc: 0.6016
 192/1760 [==>...........................] - ETA: 1:46 - loss: 0.6575 - acc: 0.6146
 256/1760 [===>..........................] - ETA: 1:41 - loss: 0.6511 - acc: 0.6211
 320/1760 [====>.........................] - ETA: 1:40 - loss: 0.6456 - acc: 0.6344
 384/1760 [=====>........................] - ETA: 1:35 - loss: 0.6400 - acc: 0.6380
 448/1760 [======>.......................] - ETA: 1:29 - loss: 0.6534 - acc: 0.6339
 512/1760 [=======>......................] - ETA: 1:27 - loss: 0.6569 - acc: 0.6367
 576/1760 [========>.....................] - ETA: 1:21 - loss: 0.6386 - acc: 0.6476
 640/1760 [=========>....................] - ETA: 1:17 - loss: 0.6342 - acc: 0.6453
 704/1760 [===========>..................] - ETA: 1:13 - loss: 0.6384 - acc: 0.6406
 768/1760 [============>.................] - ETA: 1:08 - loss: 0.6405 - acc: 0.6367
 832/1760 [=============>................] - ETA: 1:03 - loss: 0.6447 - acc: 0.6262
 896/1760 [==============>...............] - ETA: 59s - loss: 0.6439 - acc: 0.6295 
 960/1760 [===============>..............] - ETA: 54s - loss: 0.6414 - acc: 0.6354
1024/1760 [================>.............] - ETA: 50s - loss: 0.6422 - acc: 0.6348
1088/1760 [=================>............] - ETA: 45s - loss: 0.6411 - acc: 0.6314
1152/1760 [==================>...........] - ETA: 41s - loss: 0.6390 - acc: 0.6345
1216/1760 [===================>..........] - ETA: 37s - loss: 0.6429 - acc: 0.6266
1280/1760 [====================>.........] - ETA: 32s - loss: 0.6389 - acc: 0.6312
1344/1760 [=====================>........] - ETA: 28s - loss: 0.6371 - acc: 0.6332
1408/1760 [=======================>......] - ETA: 24s - loss: 0.6358 - acc: 0.6335
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6342 - acc: 0.6359
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6311 - acc: 0.6413
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6315 - acc: 0.6406
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6304 - acc: 0.6418 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6302 - acc: 0.6429
1760/1760 [==============================] - 127s 72ms/step - loss: 0.6302 - acc: 0.6432 - val_loss: 0.6295 - val_acc: 0.6429

Epoch 00009: val_acc did not improve from 0.64796
Epoch 10/10

  64/1760 [>.............................] - ETA: 2:27 - loss: 0.5506 - acc: 0.6875
 128/1760 [=>............................] - ETA: 2:15 - loss: 0.5279 - acc: 0.7266
 192/1760 [==>...........................] - ETA: 2:05 - loss: 0.5643 - acc: 0.6927
 256/1760 [===>..........................] - ETA: 1:57 - loss: 0.5838 - acc: 0.6836
 320/1760 [====>.........................] - ETA: 1:51 - loss: 0.5760 - acc: 0.6906
 384/1760 [=====>........................] - ETA: 1:46 - loss: 0.6011 - acc: 0.6667
 448/1760 [======>.......................] - ETA: 1:42 - loss: 0.6082 - acc: 0.6629
 512/1760 [=======>......................] - ETA: 1:36 - loss: 0.6163 - acc: 0.6504
 576/1760 [========>.....................] - ETA: 1:30 - loss: 0.6268 - acc: 0.6441
 640/1760 [=========>....................] - ETA: 1:25 - loss: 0.6293 - acc: 0.6406
 704/1760 [===========>..................] - ETA: 1:20 - loss: 0.6248 - acc: 0.6406
 768/1760 [============>.................] - ETA: 1:15 - loss: 0.6331 - acc: 0.6341
 832/1760 [=============>................] - ETA: 1:09 - loss: 0.6401 - acc: 0.6274
 896/1760 [==============>...............] - ETA: 1:04 - loss: 0.6366 - acc: 0.6306
 960/1760 [===============>..............] - ETA: 1:00 - loss: 0.6411 - acc: 0.6302
1024/1760 [================>.............] - ETA: 55s - loss: 0.6377 - acc: 0.6328 
1088/1760 [=================>............] - ETA: 50s - loss: 0.6383 - acc: 0.6333
1152/1760 [==================>...........] - ETA: 46s - loss: 0.6449 - acc: 0.6259
1216/1760 [===================>..........] - ETA: 41s - loss: 0.6503 - acc: 0.6201
1280/1760 [====================>.........] - ETA: 36s - loss: 0.6488 - acc: 0.6234
1344/1760 [=====================>........] - ETA: 31s - loss: 0.6463 - acc: 0.6250
1408/1760 [=======================>......] - ETA: 26s - loss: 0.6465 - acc: 0.6207
1472/1760 [========================>.....] - ETA: 21s - loss: 0.6455 - acc: 0.6182
1536/1760 [=========================>....] - ETA: 16s - loss: 0.6453 - acc: 0.6178
1600/1760 [==========================>...] - ETA: 12s - loss: 0.6455 - acc: 0.6188
1664/1760 [===========================>..] - ETA: 7s - loss: 0.6442 - acc: 0.6214 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6439 - acc: 0.6233
1760/1760 [==============================] - 139s 79ms/step - loss: 0.6434 - acc: 0.6233 - val_loss: 0.6322 - val_acc: 0.6327

Epoch 00010: val_acc did not improve from 0.64796
样本个数 200
样本个数 400
window_select6-10.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 19s
128/400 [========>.....................] - ETA: 9s 
192/400 [=============>................] - ETA: 6s
256/400 [==================>...........] - ETA: 3s
320/400 [=======================>......] - ETA: 1s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 9s 22ms/step
loss: 0.644047029018402
acc: 0.6225
