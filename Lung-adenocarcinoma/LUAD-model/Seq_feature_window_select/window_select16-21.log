nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 978
样本个数 1956
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f540d8429d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f540d8429d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f540d844410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f540d844410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d7af710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d7af710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f540d720b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f540d720b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f540d732790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f540d732790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d6b4590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d6b4590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f540d7201d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f540d7201d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d5c4bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d5c4bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f540d4be0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f540d4be0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f540d69e610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f540d69e610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d6a4510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d6a4510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f540d4be910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f540d4be910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d76bd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d76bd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f540d6b7850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f540d6b7850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f5405101790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f5405101790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f54053bbbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f54053bbbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f54053e2150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f54053e2150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5405101790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5405101790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5404e332d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5404e332d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f5404eae050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f5404eae050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5404f77c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5404f77c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f5404e33450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f5404e33450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f54051a4310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f54051a4310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53fcb25ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53fcb25ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f540d6b78d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f540d6b78d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5404eae890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5404eae890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53fcb25b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53fcb25b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5404d763d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5404d763d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53fcb46610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53fcb46610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53fc7cdc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53fc7cdc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53fc881750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53fc881750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53fcab4d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53fcab4d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53fc74b510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53fc74b510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53fc5d9790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53fc5d9790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53f4450650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53f4450650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53f42faed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53f42faed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53fc5d9b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53fc5d9b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d724350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540d724350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53f41f8810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53f41f8810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53f40e51d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53f40e51d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53f4161550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53f4161550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53f41f8990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53f41f8990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53f4161e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53f4161e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53f4207410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53f4207410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53f3e2d890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53f3e2d890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53f419ce90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53f419ce90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53f40eef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53f40eef90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53f3da2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53f3da2a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53f3cba750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53f3cba750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53f3dcef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53f3dcef90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53ebbb9710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53ebbb9710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53f3cba410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53f3cba410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53ebbe8e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53ebbe8e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53ebaad290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53ebaad290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53eb78c450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53eb78c450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540537d610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f540537d610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53eb96e290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53eb96e290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53eb65b050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53eb65b050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53eb7e0210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f53eb7e0210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53eb50add0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f53eb50add0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53e346c950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53e346c950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53eb55a910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f53eb55a910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53eb7eb250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f53eb7eb250>>: AttributeError: module 'gast' has no attribute 'Str'
2023-01-10 22:10:17.105322: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2023-01-10 22:10:17.173336: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2023-01-10 22:10:17.220034: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5614d8c22dc0 executing computations on platform Host. Devices:
2023-01-10 22:10:17.220115: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2023-01-10 22:10:17.839903: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
window_select16-21.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 4:58 - loss: 0.8007 - acc: 0.3750
 128/1760 [=>............................] - ETA: 3:54 - loss: 0.7998 - acc: 0.4531
 192/1760 [==>...........................] - ETA: 3:13 - loss: 0.7853 - acc: 0.4792
 256/1760 [===>..........................] - ETA: 2:52 - loss: 0.7820 - acc: 0.4727
 320/1760 [====>.........................] - ETA: 2:36 - loss: 0.7694 - acc: 0.4844
 384/1760 [=====>........................] - ETA: 2:23 - loss: 0.7495 - acc: 0.5026
 448/1760 [======>.......................] - ETA: 2:11 - loss: 0.7389 - acc: 0.5179
 512/1760 [=======>......................] - ETA: 2:00 - loss: 0.7328 - acc: 0.5254
 576/1760 [========>.....................] - ETA: 1:51 - loss: 0.7341 - acc: 0.5139
 640/1760 [=========>....................] - ETA: 1:43 - loss: 0.7336 - acc: 0.5109
 704/1760 [===========>..................] - ETA: 1:34 - loss: 0.7294 - acc: 0.5199
 768/1760 [============>.................] - ETA: 1:26 - loss: 0.7320 - acc: 0.5195
 832/1760 [=============>................] - ETA: 1:19 - loss: 0.7327 - acc: 0.5192
 896/1760 [==============>...............] - ETA: 1:13 - loss: 0.7301 - acc: 0.5257
 960/1760 [===============>..............] - ETA: 1:07 - loss: 0.7280 - acc: 0.5292
1024/1760 [================>.............] - ETA: 1:00 - loss: 0.7265 - acc: 0.5332
1088/1760 [=================>............] - ETA: 54s - loss: 0.7273 - acc: 0.5349 
1152/1760 [==================>...........] - ETA: 48s - loss: 0.7248 - acc: 0.5373
1216/1760 [===================>..........] - ETA: 41s - loss: 0.7238 - acc: 0.5354
1280/1760 [====================>.........] - ETA: 36s - loss: 0.7233 - acc: 0.5367
1344/1760 [=====================>........] - ETA: 30s - loss: 0.7218 - acc: 0.5402
1408/1760 [=======================>......] - ETA: 25s - loss: 0.7212 - acc: 0.5405
1472/1760 [========================>.....] - ETA: 20s - loss: 0.7220 - acc: 0.5374
1536/1760 [=========================>....] - ETA: 15s - loss: 0.7220 - acc: 0.5365
1600/1760 [==========================>...] - ETA: 11s - loss: 0.7232 - acc: 0.5337
1664/1760 [===========================>..] - ETA: 6s - loss: 0.7227 - acc: 0.5343 
1728/1760 [============================>.] - ETA: 2s - loss: 0.7220 - acc: 0.5359
1760/1760 [==============================] - 127s 72ms/step - loss: 0.7213 - acc: 0.5381 - val_loss: 0.6743 - val_acc: 0.5408

Epoch 00001: val_acc improved from -inf to 0.54082, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window16/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:39 - loss: 0.6555 - acc: 0.5781
 128/1760 [=>............................] - ETA: 1:33 - loss: 0.6666 - acc: 0.6094
 192/1760 [==>...........................] - ETA: 1:28 - loss: 0.6949 - acc: 0.5573
 256/1760 [===>..........................] - ETA: 1:28 - loss: 0.7032 - acc: 0.5312
 320/1760 [====>.........................] - ETA: 1:24 - loss: 0.7009 - acc: 0.5344
 384/1760 [=====>........................] - ETA: 1:21 - loss: 0.7042 - acc: 0.5365
 448/1760 [======>.......................] - ETA: 1:16 - loss: 0.7000 - acc: 0.5379
 512/1760 [=======>......................] - ETA: 1:11 - loss: 0.6975 - acc: 0.5566
 576/1760 [========>.....................] - ETA: 1:06 - loss: 0.6983 - acc: 0.5538
 640/1760 [=========>....................] - ETA: 1:02 - loss: 0.6991 - acc: 0.5453
 704/1760 [===========>..................] - ETA: 58s - loss: 0.6996 - acc: 0.5426 
 768/1760 [============>.................] - ETA: 56s - loss: 0.6969 - acc: 0.5469
 832/1760 [=============>................] - ETA: 52s - loss: 0.6982 - acc: 0.5445
 896/1760 [==============>...............] - ETA: 49s - loss: 0.6971 - acc: 0.5458
 960/1760 [===============>..............] - ETA: 45s - loss: 0.6951 - acc: 0.5479
1024/1760 [================>.............] - ETA: 41s - loss: 0.6957 - acc: 0.5469
1088/1760 [=================>............] - ETA: 37s - loss: 0.6968 - acc: 0.5460
1152/1760 [==================>...........] - ETA: 34s - loss: 0.6947 - acc: 0.5530
1216/1760 [===================>..........] - ETA: 30s - loss: 0.6946 - acc: 0.5543
1280/1760 [====================>.........] - ETA: 26s - loss: 0.6928 - acc: 0.5570
1344/1760 [=====================>........] - ETA: 23s - loss: 0.6962 - acc: 0.5528
1408/1760 [=======================>......] - ETA: 19s - loss: 0.6971 - acc: 0.5511
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6978 - acc: 0.5503
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6983 - acc: 0.5508
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6999 - acc: 0.5481 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6996 - acc: 0.5493
1728/1760 [============================>.] - ETA: 1s - loss: 0.6975 - acc: 0.5544
1760/1760 [==============================] - 102s 58ms/step - loss: 0.6978 - acc: 0.5540 - val_loss: 0.6597 - val_acc: 0.6071

Epoch 00002: val_acc improved from 0.54082 to 0.60714, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window16/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:24 - loss: 0.7004 - acc: 0.5312
 128/1760 [=>............................] - ETA: 1:20 - loss: 0.6955 - acc: 0.5547
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.6811 - acc: 0.5677
 256/1760 [===>..........................] - ETA: 1:18 - loss: 0.6801 - acc: 0.5703
 320/1760 [====>.........................] - ETA: 1:16 - loss: 0.6828 - acc: 0.5687
 384/1760 [=====>........................] - ETA: 1:14 - loss: 0.6809 - acc: 0.5781
 448/1760 [======>.......................] - ETA: 1:09 - loss: 0.6775 - acc: 0.5848
 512/1760 [=======>......................] - ETA: 1:06 - loss: 0.6823 - acc: 0.5820
 576/1760 [========>.....................] - ETA: 1:02 - loss: 0.6838 - acc: 0.5729
 640/1760 [=========>....................] - ETA: 58s - loss: 0.6780 - acc: 0.5781 
 704/1760 [===========>..................] - ETA: 55s - loss: 0.6783 - acc: 0.5767
 768/1760 [============>.................] - ETA: 51s - loss: 0.6752 - acc: 0.5846
 832/1760 [=============>................] - ETA: 49s - loss: 0.6728 - acc: 0.5853
 896/1760 [==============>...............] - ETA: 46s - loss: 0.6702 - acc: 0.5915
 960/1760 [===============>..............] - ETA: 43s - loss: 0.6702 - acc: 0.5927
1024/1760 [================>.............] - ETA: 40s - loss: 0.6704 - acc: 0.5938
1088/1760 [=================>............] - ETA: 37s - loss: 0.6701 - acc: 0.5919
1152/1760 [==================>...........] - ETA: 34s - loss: 0.6740 - acc: 0.5929
1216/1760 [===================>..........] - ETA: 31s - loss: 0.6746 - acc: 0.5913
1280/1760 [====================>.........] - ETA: 27s - loss: 0.6746 - acc: 0.5898
1344/1760 [=====================>........] - ETA: 24s - loss: 0.6734 - acc: 0.5915
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6725 - acc: 0.5923
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6682 - acc: 0.5992
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6723 - acc: 0.5951
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6710 - acc: 0.5981 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6698 - acc: 0.5968
1728/1760 [============================>.] - ETA: 1s - loss: 0.6687 - acc: 0.6001
1760/1760 [==============================] - 111s 63ms/step - loss: 0.6679 - acc: 0.6000 - val_loss: 0.6535 - val_acc: 0.6071

Epoch 00003: val_acc did not improve from 0.60714
Epoch 4/10

  64/1760 [>.............................] - ETA: 2:13 - loss: 0.6059 - acc: 0.6406
 128/1760 [=>............................] - ETA: 2:03 - loss: 0.6226 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 1:55 - loss: 0.6447 - acc: 0.6146
 256/1760 [===>..........................] - ETA: 1:46 - loss: 0.6472 - acc: 0.6094
 320/1760 [====>.........................] - ETA: 1:41 - loss: 0.6511 - acc: 0.6094
 384/1760 [=====>........................] - ETA: 1:35 - loss: 0.6558 - acc: 0.6016
 448/1760 [======>.......................] - ETA: 1:31 - loss: 0.6541 - acc: 0.6049
 512/1760 [=======>......................] - ETA: 1:26 - loss: 0.6578 - acc: 0.6035
 576/1760 [========>.....................] - ETA: 1:20 - loss: 0.6642 - acc: 0.5990
 640/1760 [=========>....................] - ETA: 1:16 - loss: 0.6599 - acc: 0.6078
 704/1760 [===========>..................] - ETA: 1:11 - loss: 0.6581 - acc: 0.6094
 768/1760 [============>.................] - ETA: 1:07 - loss: 0.6543 - acc: 0.6146
 832/1760 [=============>................] - ETA: 1:02 - loss: 0.6562 - acc: 0.6106
 896/1760 [==============>...............] - ETA: 58s - loss: 0.6555 - acc: 0.6094 
 960/1760 [===============>..............] - ETA: 54s - loss: 0.6534 - acc: 0.6104
1024/1760 [================>.............] - ETA: 49s - loss: 0.6599 - acc: 0.6055
1088/1760 [=================>............] - ETA: 45s - loss: 0.6637 - acc: 0.6039
1152/1760 [==================>...........] - ETA: 41s - loss: 0.6679 - acc: 0.5972
1216/1760 [===================>..........] - ETA: 37s - loss: 0.6674 - acc: 0.5970
1280/1760 [====================>.........] - ETA: 32s - loss: 0.6665 - acc: 0.5992
1344/1760 [=====================>........] - ETA: 28s - loss: 0.6663 - acc: 0.5952
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6675 - acc: 0.5930
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6690 - acc: 0.5904
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6711 - acc: 0.5866
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6728 - acc: 0.5837
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6735 - acc: 0.5835 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6754 - acc: 0.5793
1760/1760 [==============================] - 126s 71ms/step - loss: 0.6758 - acc: 0.5790 - val_loss: 0.6908 - val_acc: 0.5612

Epoch 00004: val_acc did not improve from 0.60714
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:51 - loss: 0.6377 - acc: 0.6562
 128/1760 [=>............................] - ETA: 1:54 - loss: 0.6602 - acc: 0.6094
 192/1760 [==>...........................] - ETA: 1:48 - loss: 0.6669 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 1:45 - loss: 0.6612 - acc: 0.6133
 320/1760 [====>.........................] - ETA: 1:41 - loss: 0.6522 - acc: 0.6156
 384/1760 [=====>........................] - ETA: 1:35 - loss: 0.6611 - acc: 0.6016
 448/1760 [======>.......................] - ETA: 1:32 - loss: 0.6699 - acc: 0.5848
 512/1760 [=======>......................] - ETA: 1:27 - loss: 0.6677 - acc: 0.5957
 576/1760 [========>.....................] - ETA: 1:22 - loss: 0.6660 - acc: 0.5955
 640/1760 [=========>....................] - ETA: 1:17 - loss: 0.6622 - acc: 0.6031
 704/1760 [===========>..................] - ETA: 1:11 - loss: 0.6582 - acc: 0.6108
 768/1760 [============>.................] - ETA: 1:08 - loss: 0.6556 - acc: 0.6120
 832/1760 [=============>................] - ETA: 1:05 - loss: 0.6569 - acc: 0.6106
 896/1760 [==============>...............] - ETA: 1:00 - loss: 0.6598 - acc: 0.6105
 960/1760 [===============>..............] - ETA: 56s - loss: 0.6643 - acc: 0.6104 
1024/1760 [================>.............] - ETA: 51s - loss: 0.6588 - acc: 0.6191
1088/1760 [=================>............] - ETA: 47s - loss: 0.6582 - acc: 0.6176
1152/1760 [==================>...........] - ETA: 43s - loss: 0.6576 - acc: 0.6172
1216/1760 [===================>..........] - ETA: 39s - loss: 0.6618 - acc: 0.6135
1280/1760 [====================>.........] - ETA: 34s - loss: 0.6624 - acc: 0.6133
1344/1760 [=====================>........] - ETA: 29s - loss: 0.6615 - acc: 0.6131
1408/1760 [=======================>......] - ETA: 25s - loss: 0.6623 - acc: 0.6143
1472/1760 [========================>.....] - ETA: 20s - loss: 0.6634 - acc: 0.6141
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6607 - acc: 0.6152
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6615 - acc: 0.6150
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6635 - acc: 0.6136 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6649 - acc: 0.6128
1760/1760 [==============================] - 130s 74ms/step - loss: 0.6645 - acc: 0.6119 - val_loss: 0.6449 - val_acc: 0.6327

Epoch 00005: val_acc improved from 0.60714 to 0.63265, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window16/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 6/10

  64/1760 [>.............................] - ETA: 2:10 - loss: 0.6572 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:59 - loss: 0.6609 - acc: 0.5938
 192/1760 [==>...........................] - ETA: 1:54 - loss: 0.6598 - acc: 0.5990
 256/1760 [===>..........................] - ETA: 1:46 - loss: 0.6596 - acc: 0.6016
 320/1760 [====>.........................] - ETA: 1:42 - loss: 0.6594 - acc: 0.5969
 384/1760 [=====>........................] - ETA: 1:38 - loss: 0.6567 - acc: 0.6068
 448/1760 [======>.......................] - ETA: 1:32 - loss: 0.6438 - acc: 0.6228
 512/1760 [=======>......................] - ETA: 1:27 - loss: 0.6531 - acc: 0.6055
 576/1760 [========>.....................] - ETA: 1:22 - loss: 0.6568 - acc: 0.5990
 640/1760 [=========>....................] - ETA: 1:18 - loss: 0.6597 - acc: 0.6047
 704/1760 [===========>..................] - ETA: 1:14 - loss: 0.6636 - acc: 0.5980
 768/1760 [============>.................] - ETA: 1:10 - loss: 0.6613 - acc: 0.6055
 832/1760 [=============>................] - ETA: 1:05 - loss: 0.6625 - acc: 0.6010
 896/1760 [==============>...............] - ETA: 1:00 - loss: 0.6650 - acc: 0.6004
 960/1760 [===============>..............] - ETA: 56s - loss: 0.6635 - acc: 0.6062 
1024/1760 [================>.............] - ETA: 51s - loss: 0.6636 - acc: 0.6025
1088/1760 [=================>............] - ETA: 47s - loss: 0.6639 - acc: 0.6029
1152/1760 [==================>...........] - ETA: 42s - loss: 0.6622 - acc: 0.6033
1216/1760 [===================>..........] - ETA: 38s - loss: 0.6658 - acc: 0.6012
1280/1760 [====================>.........] - ETA: 33s - loss: 0.6651 - acc: 0.6039
1344/1760 [=====================>........] - ETA: 29s - loss: 0.6630 - acc: 0.6101
1408/1760 [=======================>......] - ETA: 24s - loss: 0.6643 - acc: 0.6094
1472/1760 [========================>.....] - ETA: 20s - loss: 0.6625 - acc: 0.6087
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6610 - acc: 0.6100
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6593 - acc: 0.6106
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6590 - acc: 0.6112 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6592 - acc: 0.6094
1760/1760 [==============================] - 129s 73ms/step - loss: 0.6590 - acc: 0.6091 - val_loss: 0.6380 - val_acc: 0.6224

Epoch 00006: val_acc did not improve from 0.63265
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:58 - loss: 0.6721 - acc: 0.6406
 128/1760 [=>............................] - ETA: 1:50 - loss: 0.6298 - acc: 0.6797
 192/1760 [==>...........................] - ETA: 1:47 - loss: 0.6523 - acc: 0.6406
 256/1760 [===>..........................] - ETA: 1:42 - loss: 0.6372 - acc: 0.6328
 320/1760 [====>.........................] - ETA: 1:39 - loss: 0.6338 - acc: 0.6469
 384/1760 [=====>........................] - ETA: 1:35 - loss: 0.6381 - acc: 0.6432
 448/1760 [======>.......................] - ETA: 1:30 - loss: 0.6381 - acc: 0.6406
 512/1760 [=======>......................] - ETA: 1:28 - loss: 0.6462 - acc: 0.6270
 576/1760 [========>.....................] - ETA: 1:22 - loss: 0.6425 - acc: 0.6285
 640/1760 [=========>....................] - ETA: 1:19 - loss: 0.6406 - acc: 0.6266
 704/1760 [===========>..................] - ETA: 1:14 - loss: 0.6447 - acc: 0.6222
 768/1760 [============>.................] - ETA: 1:10 - loss: 0.6438 - acc: 0.6263
 832/1760 [=============>................] - ETA: 1:06 - loss: 0.6412 - acc: 0.6286
 896/1760 [==============>...............] - ETA: 1:01 - loss: 0.6434 - acc: 0.6272
 960/1760 [===============>..............] - ETA: 57s - loss: 0.6452 - acc: 0.6260 
1024/1760 [================>.............] - ETA: 53s - loss: 0.6477 - acc: 0.6221
1088/1760 [=================>............] - ETA: 48s - loss: 0.6467 - acc: 0.6232
1152/1760 [==================>...........] - ETA: 43s - loss: 0.6468 - acc: 0.6215
1216/1760 [===================>..........] - ETA: 38s - loss: 0.6461 - acc: 0.6250
1280/1760 [====================>.........] - ETA: 34s - loss: 0.6465 - acc: 0.6234
1344/1760 [=====================>........] - ETA: 29s - loss: 0.6507 - acc: 0.6183
1408/1760 [=======================>......] - ETA: 24s - loss: 0.6483 - acc: 0.6207
1472/1760 [========================>.....] - ETA: 20s - loss: 0.6463 - acc: 0.6230
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6519 - acc: 0.6159
1600/1760 [==========================>...] - ETA: 11s - loss: 0.6510 - acc: 0.6144
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6501 - acc: 0.6172 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6519 - acc: 0.6128
1760/1760 [==============================] - 130s 74ms/step - loss: 0.6500 - acc: 0.6148 - val_loss: 0.6522 - val_acc: 0.6429

Epoch 00007: val_acc improved from 0.63265 to 0.64286, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window16/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:48 - loss: 0.8070 - acc: 0.4844
 128/1760 [=>............................] - ETA: 1:43 - loss: 0.7150 - acc: 0.5781
 192/1760 [==>...........................] - ETA: 1:45 - loss: 0.6723 - acc: 0.6198
 256/1760 [===>..........................] - ETA: 1:41 - loss: 0.6617 - acc: 0.6172
 320/1760 [====>.........................] - ETA: 1:39 - loss: 0.6444 - acc: 0.6344
 384/1760 [=====>........................] - ETA: 1:36 - loss: 0.6452 - acc: 0.6302
 448/1760 [======>.......................] - ETA: 1:31 - loss: 0.6462 - acc: 0.6339
 512/1760 [=======>......................] - ETA: 1:26 - loss: 0.6404 - acc: 0.6406
 576/1760 [========>.....................] - ETA: 1:23 - loss: 0.6429 - acc: 0.6372
 640/1760 [=========>....................] - ETA: 1:17 - loss: 0.6422 - acc: 0.6391
 704/1760 [===========>..................] - ETA: 1:12 - loss: 0.6469 - acc: 0.6264
 768/1760 [============>.................] - ETA: 1:07 - loss: 0.6486 - acc: 0.6250
 832/1760 [=============>................] - ETA: 1:03 - loss: 0.6470 - acc: 0.6226
 896/1760 [==============>...............] - ETA: 58s - loss: 0.6447 - acc: 0.6272 
 960/1760 [===============>..............] - ETA: 54s - loss: 0.6444 - acc: 0.6281
1024/1760 [================>.............] - ETA: 50s - loss: 0.6413 - acc: 0.6318
1088/1760 [=================>............] - ETA: 45s - loss: 0.6458 - acc: 0.6259
1152/1760 [==================>...........] - ETA: 41s - loss: 0.6449 - acc: 0.6302
1216/1760 [===================>..........] - ETA: 36s - loss: 0.6464 - acc: 0.6275
1280/1760 [====================>.........] - ETA: 32s - loss: 0.6434 - acc: 0.6281
1344/1760 [=====================>........] - ETA: 28s - loss: 0.6454 - acc: 0.6265
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6443 - acc: 0.6293
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6453 - acc: 0.6311
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6446 - acc: 0.6341
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6447 - acc: 0.6350
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6431 - acc: 0.6352 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6452 - acc: 0.6331
1760/1760 [==============================] - 124s 71ms/step - loss: 0.6463 - acc: 0.6324 - val_loss: 0.6765 - val_acc: 0.5612

Epoch 00008: val_acc did not improve from 0.64286
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:49 - loss: 0.6648 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:50 - loss: 0.6347 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 1:45 - loss: 0.6302 - acc: 0.6302
 256/1760 [===>..........................] - ETA: 1:38 - loss: 0.6216 - acc: 0.6328
 320/1760 [====>.........................] - ETA: 1:33 - loss: 0.6180 - acc: 0.6531
 384/1760 [=====>........................] - ETA: 1:28 - loss: 0.6196 - acc: 0.6458
 448/1760 [======>.......................] - ETA: 1:25 - loss: 0.6347 - acc: 0.6362
 512/1760 [=======>......................] - ETA: 1:20 - loss: 0.6352 - acc: 0.6406
 576/1760 [========>.....................] - ETA: 1:16 - loss: 0.6396 - acc: 0.6424
 640/1760 [=========>....................] - ETA: 1:11 - loss: 0.6318 - acc: 0.6516
 704/1760 [===========>..................] - ETA: 1:08 - loss: 0.6317 - acc: 0.6420
 768/1760 [============>.................] - ETA: 1:04 - loss: 0.6310 - acc: 0.6432
 832/1760 [=============>................] - ETA: 1:00 - loss: 0.6304 - acc: 0.6430
 896/1760 [==============>...............] - ETA: 56s - loss: 0.6299 - acc: 0.6451 
 960/1760 [===============>..............] - ETA: 51s - loss: 0.6279 - acc: 0.6510
1024/1760 [================>.............] - ETA: 47s - loss: 0.6274 - acc: 0.6523
1088/1760 [=================>............] - ETA: 43s - loss: 0.6316 - acc: 0.6480
1152/1760 [==================>...........] - ETA: 39s - loss: 0.6348 - acc: 0.6450
1216/1760 [===================>..........] - ETA: 35s - loss: 0.6368 - acc: 0.6439
1280/1760 [====================>.........] - ETA: 31s - loss: 0.6370 - acc: 0.6445
1344/1760 [=====================>........] - ETA: 27s - loss: 0.6402 - acc: 0.6414
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6417 - acc: 0.6392
1472/1760 [========================>.....] - ETA: 18s - loss: 0.6451 - acc: 0.6372
1536/1760 [=========================>....] - ETA: 14s - loss: 0.6429 - acc: 0.6393
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6469 - acc: 0.6344
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6482 - acc: 0.6346 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6479 - acc: 0.6343
1760/1760 [==============================] - 121s 69ms/step - loss: 0.6477 - acc: 0.6335 - val_loss: 0.7047 - val_acc: 0.5510

Epoch 00009: val_acc did not improve from 0.64286
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:48 - loss: 0.6842 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:50 - loss: 0.6544 - acc: 0.6094
 192/1760 [==>...........................] - ETA: 1:48 - loss: 0.6430 - acc: 0.6302
 256/1760 [===>..........................] - ETA: 1:42 - loss: 0.6345 - acc: 0.6406
 320/1760 [====>.........................] - ETA: 1:37 - loss: 0.6260 - acc: 0.6469
 384/1760 [=====>........................] - ETA: 1:30 - loss: 0.6242 - acc: 0.6536
 448/1760 [======>.......................] - ETA: 1:25 - loss: 0.6260 - acc: 0.6429
 512/1760 [=======>......................] - ETA: 1:18 - loss: 0.6368 - acc: 0.6289
 576/1760 [========>.....................] - ETA: 1:12 - loss: 0.6472 - acc: 0.6233
 640/1760 [=========>....................] - ETA: 1:07 - loss: 0.6489 - acc: 0.6234
 704/1760 [===========>..................] - ETA: 1:02 - loss: 0.6490 - acc: 0.6236
 768/1760 [============>.................] - ETA: 58s - loss: 0.6417 - acc: 0.6328 
 832/1760 [=============>................] - ETA: 54s - loss: 0.6402 - acc: 0.6406
 896/1760 [==============>...............] - ETA: 50s - loss: 0.6391 - acc: 0.6384
 960/1760 [===============>..............] - ETA: 46s - loss: 0.6358 - acc: 0.6406
1024/1760 [================>.............] - ETA: 42s - loss: 0.6366 - acc: 0.6396
1088/1760 [=================>............] - ETA: 38s - loss: 0.6384 - acc: 0.6388
1152/1760 [==================>...........] - ETA: 34s - loss: 0.6407 - acc: 0.6354
1216/1760 [===================>..........] - ETA: 31s - loss: 0.6412 - acc: 0.6340
1280/1760 [====================>.........] - ETA: 27s - loss: 0.6402 - acc: 0.6359
1344/1760 [=====================>........] - ETA: 23s - loss: 0.6358 - acc: 0.6391
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6337 - acc: 0.6420
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6368 - acc: 0.6427
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6338 - acc: 0.6465
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6341 - acc: 0.6462 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6358 - acc: 0.6430
1728/1760 [============================>.] - ETA: 1s - loss: 0.6352 - acc: 0.6429
1760/1760 [==============================] - 105s 60ms/step - loss: 0.6371 - acc: 0.6426 - val_loss: 0.6275 - val_acc: 0.6480

Epoch 00010: val_acc improved from 0.64286 to 0.64796, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window16/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
样本个数 200
样本个数 400
window_select16-21.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 13s
128/400 [========>.....................] - ETA: 8s 
192/400 [=============>................] - ETA: 6s
256/400 [==================>...........] - ETA: 4s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 11s 29ms/step
loss: 0.6663924336433411
acc: 0.6225
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f4dd871d590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f4dd871d590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f4dd8715a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f4dd8715a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f544fd38190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f544fd38190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f542f262c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f542f262c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f544fd98f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f544fd98f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f542f3bdf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f542f3bdf50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f542f262bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f542f262bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f544fe2da10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f544fe2da10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4dd8530610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4dd8530610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4dd8487890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4dd8487890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4dd86369d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4dd86369d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4df41afbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4df41afbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4dd8279e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4dd8279e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4dd81decd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4dd81decd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4dd8141310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4dd8141310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4dd82cbb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4dd82cbb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4dd81dec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4dd81dec90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d9462d850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d9462d850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4dd8140e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4dd8140e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4dd8236e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4dd8236e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4dd8128650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4dd8128650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d9458d150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d9458d150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d9430d850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d9430d850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d942d2750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d942d2750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d942b1990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d942b1990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d94596790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d94596790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d942d2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d942d2a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d7479cc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d7479cc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d9426dcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d9426dcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d74643ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d74643ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d9406dd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d9406dd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d9412a290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d9412a290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d745c30d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d745c30d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d745f7890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d745f7890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d74396a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d74396a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d74669e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d74669e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d745f7bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d745f7bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d741ad090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d741ad090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d740b4f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d740b4f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d740f8550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d740f8550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d740d0a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d740d0a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d547d48d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d547d48d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d7416af50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d7416af50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d54505810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d54505810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d5447a7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d5447a7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d54468790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d54468790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d5475b790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d5475b790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d5447cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d5447cf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d542286d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d542286d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d541a3810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d541a3810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d54273d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d54273d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d54228d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d54228d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d347858d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d347858d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d540c5950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d540c5950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d345a6f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d345a6f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d346d29d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d346d29d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d345f4390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d345f4390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d344a59d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d344a59d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d3464d650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4d3464d650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d34325490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4d34325490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d34199e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d34199e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d344a5f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4d344a5f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d54111610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4d54111610>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 5:31 - loss: 0.7464 - acc: 0.4688
 128/1760 [=>............................] - ETA: 3:41 - loss: 0.7613 - acc: 0.4766
 192/1760 [==>...........................] - ETA: 3:03 - loss: 0.7418 - acc: 0.4792
 256/1760 [===>..........................] - ETA: 2:38 - loss: 0.7302 - acc: 0.5156
 320/1760 [====>.........................] - ETA: 2:20 - loss: 0.7542 - acc: 0.4938
 384/1760 [=====>........................] - ETA: 2:05 - loss: 0.7541 - acc: 0.4974
 448/1760 [======>.......................] - ETA: 1:53 - loss: 0.7529 - acc: 0.4933
 512/1760 [=======>......................] - ETA: 1:43 - loss: 0.7448 - acc: 0.4961
 576/1760 [========>.....................] - ETA: 1:35 - loss: 0.7463 - acc: 0.4948
 640/1760 [=========>....................] - ETA: 1:27 - loss: 0.7397 - acc: 0.5094
 704/1760 [===========>..................] - ETA: 1:21 - loss: 0.7329 - acc: 0.5185
 768/1760 [============>.................] - ETA: 1:15 - loss: 0.7334 - acc: 0.5143
 832/1760 [=============>................] - ETA: 1:10 - loss: 0.7287 - acc: 0.5216
 896/1760 [==============>...............] - ETA: 1:04 - loss: 0.7278 - acc: 0.5212
 960/1760 [===============>..............] - ETA: 59s - loss: 0.7265 - acc: 0.5219 
1024/1760 [================>.............] - ETA: 54s - loss: 0.7242 - acc: 0.5215
1088/1760 [=================>............] - ETA: 49s - loss: 0.7251 - acc: 0.5119
1152/1760 [==================>...........] - ETA: 44s - loss: 0.7250 - acc: 0.5113
1216/1760 [===================>..........] - ETA: 39s - loss: 0.7251 - acc: 0.5074
1280/1760 [====================>.........] - ETA: 34s - loss: 0.7234 - acc: 0.5109
1344/1760 [=====================>........] - ETA: 29s - loss: 0.7252 - acc: 0.5060
1408/1760 [=======================>......] - ETA: 25s - loss: 0.7250 - acc: 0.5057
1472/1760 [========================>.....] - ETA: 20s - loss: 0.7238 - acc: 0.5068
1536/1760 [=========================>....] - ETA: 16s - loss: 0.7242 - acc: 0.5072
1600/1760 [==========================>...] - ETA: 11s - loss: 0.7236 - acc: 0.5050
1664/1760 [===========================>..] - ETA: 6s - loss: 0.7232 - acc: 0.5024 
1728/1760 [============================>.] - ETA: 2s - loss: 0.7230 - acc: 0.5023
1760/1760 [==============================] - 131s 75ms/step - loss: 0.7226 - acc: 0.5040 - val_loss: 0.6710 - val_acc: 0.5918

Epoch 00001: val_acc improved from -inf to 0.59184, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window17/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:49 - loss: 0.6482 - acc: 0.6406
 128/1760 [=>............................] - ETA: 1:45 - loss: 0.6644 - acc: 0.6094
 192/1760 [==>...........................] - ETA: 1:43 - loss: 0.6782 - acc: 0.5938
 256/1760 [===>..........................] - ETA: 1:41 - loss: 0.6733 - acc: 0.5898
 320/1760 [====>.........................] - ETA: 1:36 - loss: 0.6762 - acc: 0.5938
 384/1760 [=====>........................] - ETA: 1:30 - loss: 0.6870 - acc: 0.5547
 448/1760 [======>.......................] - ETA: 1:25 - loss: 0.6894 - acc: 0.5536
 512/1760 [=======>......................] - ETA: 1:19 - loss: 0.6900 - acc: 0.5469
 576/1760 [========>.....................] - ETA: 1:13 - loss: 0.6976 - acc: 0.5312
 640/1760 [=========>....................] - ETA: 1:08 - loss: 0.6982 - acc: 0.5328
 704/1760 [===========>..................] - ETA: 1:03 - loss: 0.6976 - acc: 0.5312
 768/1760 [============>.................] - ETA: 58s - loss: 0.6991 - acc: 0.5299 
 832/1760 [=============>................] - ETA: 53s - loss: 0.7007 - acc: 0.5276
 896/1760 [==============>...............] - ETA: 48s - loss: 0.6979 - acc: 0.5290
 960/1760 [===============>..............] - ETA: 44s - loss: 0.6958 - acc: 0.5354
1024/1760 [================>.............] - ETA: 40s - loss: 0.6956 - acc: 0.5391
1088/1760 [=================>............] - ETA: 37s - loss: 0.6927 - acc: 0.5404
1152/1760 [==================>...........] - ETA: 33s - loss: 0.6937 - acc: 0.5347
1216/1760 [===================>..........] - ETA: 29s - loss: 0.6908 - acc: 0.5411
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6906 - acc: 0.5391
1344/1760 [=====================>........] - ETA: 22s - loss: 0.6894 - acc: 0.5417
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6908 - acc: 0.5398
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6928 - acc: 0.5326
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6947 - acc: 0.5299
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6935 - acc: 0.5331 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6938 - acc: 0.5349
1728/1760 [============================>.] - ETA: 1s - loss: 0.6937 - acc: 0.5347
1760/1760 [==============================] - 98s 56ms/step - loss: 0.6936 - acc: 0.5352 - val_loss: 0.6645 - val_acc: 0.6173

Epoch 00002: val_acc improved from 0.59184 to 0.61735, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window17/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:32 - loss: 0.7072 - acc: 0.5781
 128/1760 [=>............................] - ETA: 1:25 - loss: 0.6847 - acc: 0.5938
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.6801 - acc: 0.5938
 256/1760 [===>..........................] - ETA: 1:15 - loss: 0.6786 - acc: 0.5859
 320/1760 [====>.........................] - ETA: 1:14 - loss: 0.6758 - acc: 0.5844
 384/1760 [=====>........................] - ETA: 1:10 - loss: 0.6776 - acc: 0.5833
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6741 - acc: 0.5915
 512/1760 [=======>......................] - ETA: 1:03 - loss: 0.6779 - acc: 0.5820
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6727 - acc: 0.5799
 640/1760 [=========>....................] - ETA: 57s - loss: 0.6748 - acc: 0.5813 
 704/1760 [===========>..................] - ETA: 54s - loss: 0.6768 - acc: 0.5838
 768/1760 [============>.................] - ETA: 50s - loss: 0.6762 - acc: 0.5833
 832/1760 [=============>................] - ETA: 47s - loss: 0.6728 - acc: 0.5889
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6725 - acc: 0.5926
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6722 - acc: 0.5917
1024/1760 [================>.............] - ETA: 37s - loss: 0.6743 - acc: 0.5889
1088/1760 [=================>............] - ETA: 34s - loss: 0.6749 - acc: 0.5882
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6755 - acc: 0.5885
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6763 - acc: 0.5896
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6771 - acc: 0.5852
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6781 - acc: 0.5841
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6791 - acc: 0.5838
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6780 - acc: 0.5836
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6796 - acc: 0.5801
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6788 - acc: 0.5787 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6795 - acc: 0.5775
1728/1760 [============================>.] - ETA: 1s - loss: 0.6814 - acc: 0.5752
1760/1760 [==============================] - 96s 55ms/step - loss: 0.6832 - acc: 0.5733 - val_loss: 0.6544 - val_acc: 0.6378

Epoch 00003: val_acc improved from 0.61735 to 0.63776, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window17/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:30 - loss: 0.7055 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:28 - loss: 0.6797 - acc: 0.6016
 192/1760 [==>...........................] - ETA: 1:22 - loss: 0.6737 - acc: 0.6094
 256/1760 [===>..........................] - ETA: 1:19 - loss: 0.6775 - acc: 0.6016
 320/1760 [====>.........................] - ETA: 1:15 - loss: 0.6650 - acc: 0.6188
 384/1760 [=====>........................] - ETA: 1:12 - loss: 0.6675 - acc: 0.6068
 448/1760 [======>.......................] - ETA: 1:08 - loss: 0.6664 - acc: 0.6004
 512/1760 [=======>......................] - ETA: 1:04 - loss: 0.6666 - acc: 0.5996
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6704 - acc: 0.5938
 640/1760 [=========>....................] - ETA: 58s - loss: 0.6723 - acc: 0.5906 
 704/1760 [===========>..................] - ETA: 55s - loss: 0.6720 - acc: 0.5923
 768/1760 [============>.................] - ETA: 51s - loss: 0.6697 - acc: 0.5938
 832/1760 [=============>................] - ETA: 48s - loss: 0.6728 - acc: 0.5901
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6763 - acc: 0.5859
 960/1760 [===============>..............] - ETA: 41s - loss: 0.6707 - acc: 0.5927
1024/1760 [================>.............] - ETA: 38s - loss: 0.6724 - acc: 0.5889
1088/1760 [=================>............] - ETA: 35s - loss: 0.6709 - acc: 0.5910
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6739 - acc: 0.5868
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6717 - acc: 0.5888
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6728 - acc: 0.5891
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6733 - acc: 0.5893
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6750 - acc: 0.5859
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6749 - acc: 0.5876
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6751 - acc: 0.5853
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6755 - acc: 0.5837 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6750 - acc: 0.5847
1728/1760 [============================>.] - ETA: 1s - loss: 0.6740 - acc: 0.5845
1760/1760 [==============================] - 97s 55ms/step - loss: 0.6742 - acc: 0.5830 - val_loss: 0.6719 - val_acc: 0.5561

Epoch 00004: val_acc did not improve from 0.63776
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:30 - loss: 0.7228 - acc: 0.5156
 128/1760 [=>............................] - ETA: 1:22 - loss: 0.6770 - acc: 0.5859
 192/1760 [==>...........................] - ETA: 1:20 - loss: 0.6902 - acc: 0.5833
 256/1760 [===>..........................] - ETA: 1:17 - loss: 0.6751 - acc: 0.6016
 320/1760 [====>.........................] - ETA: 1:12 - loss: 0.6898 - acc: 0.5656
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6896 - acc: 0.5599
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6891 - acc: 0.5625
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6849 - acc: 0.5703
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6803 - acc: 0.5729 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6802 - acc: 0.5687
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6758 - acc: 0.5710
 768/1760 [============>.................] - ETA: 50s - loss: 0.6748 - acc: 0.5716
 832/1760 [=============>................] - ETA: 47s - loss: 0.6718 - acc: 0.5733
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6684 - acc: 0.5770
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6690 - acc: 0.5792
1024/1760 [================>.............] - ETA: 37s - loss: 0.6734 - acc: 0.5752
1088/1760 [=================>............] - ETA: 34s - loss: 0.6722 - acc: 0.5772
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6723 - acc: 0.5807
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6690 - acc: 0.5847
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6694 - acc: 0.5813
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6684 - acc: 0.5818
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6678 - acc: 0.5859
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6643 - acc: 0.5890
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6646 - acc: 0.5885
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6653 - acc: 0.5875 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6649 - acc: 0.5871
1728/1760 [============================>.] - ETA: 1s - loss: 0.6632 - acc: 0.5897
1760/1760 [==============================] - 95s 54ms/step - loss: 0.6627 - acc: 0.5903 - val_loss: 0.6362 - val_acc: 0.6429

Epoch 00005: val_acc improved from 0.63776 to 0.64286, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window17/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:29 - loss: 0.6858 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:25 - loss: 0.6650 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 1:21 - loss: 0.6604 - acc: 0.6302
 256/1760 [===>..........................] - ETA: 1:16 - loss: 0.6515 - acc: 0.6523
 320/1760 [====>.........................] - ETA: 1:15 - loss: 0.6300 - acc: 0.6719
 384/1760 [=====>........................] - ETA: 1:12 - loss: 0.6254 - acc: 0.6719
 448/1760 [======>.......................] - ETA: 1:07 - loss: 0.6338 - acc: 0.6607
 512/1760 [=======>......................] - ETA: 1:03 - loss: 0.6434 - acc: 0.6504
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6458 - acc: 0.6441
 640/1760 [=========>....................] - ETA: 57s - loss: 0.6364 - acc: 0.6516 
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6357 - acc: 0.6491
 768/1760 [============>.................] - ETA: 50s - loss: 0.6375 - acc: 0.6458
 832/1760 [=============>................] - ETA: 47s - loss: 0.6325 - acc: 0.6514
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6322 - acc: 0.6540
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6332 - acc: 0.6521
1024/1760 [================>.............] - ETA: 37s - loss: 0.6340 - acc: 0.6494
1088/1760 [=================>............] - ETA: 34s - loss: 0.6335 - acc: 0.6498
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6331 - acc: 0.6476
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6369 - acc: 0.6390
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6400 - acc: 0.6336
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6436 - acc: 0.6310
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6431 - acc: 0.6328
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6435 - acc: 0.6332
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6435 - acc: 0.6322
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6419 - acc: 0.6344 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6410 - acc: 0.6358
1728/1760 [============================>.] - ETA: 1s - loss: 0.6431 - acc: 0.6308
1760/1760 [==============================] - 94s 53ms/step - loss: 0.6438 - acc: 0.6290 - val_loss: 0.6300 - val_acc: 0.6020

Epoch 00006: val_acc did not improve from 0.64286
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:39 - loss: 0.6510 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:28 - loss: 0.6640 - acc: 0.5938
 192/1760 [==>...........................] - ETA: 1:24 - loss: 0.6567 - acc: 0.5990
 256/1760 [===>..........................] - ETA: 1:19 - loss: 0.6662 - acc: 0.5938
 320/1760 [====>.........................] - ETA: 1:15 - loss: 0.6562 - acc: 0.6000
 384/1760 [=====>........................] - ETA: 1:11 - loss: 0.6567 - acc: 0.5911
 448/1760 [======>.......................] - ETA: 1:07 - loss: 0.6521 - acc: 0.5915
 512/1760 [=======>......................] - ETA: 1:04 - loss: 0.6551 - acc: 0.5898
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6592 - acc: 0.5851
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6654 - acc: 0.5844 
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6633 - acc: 0.5852
 768/1760 [============>.................] - ETA: 50s - loss: 0.6649 - acc: 0.5833
 832/1760 [=============>................] - ETA: 47s - loss: 0.6672 - acc: 0.5829
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6658 - acc: 0.5837
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6675 - acc: 0.5833
1024/1760 [================>.............] - ETA: 37s - loss: 0.6699 - acc: 0.5771
1088/1760 [=================>............] - ETA: 34s - loss: 0.6699 - acc: 0.5809
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6696 - acc: 0.5816
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6680 - acc: 0.5831
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6650 - acc: 0.5891
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6663 - acc: 0.5893
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6653 - acc: 0.5945
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6652 - acc: 0.5965
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6662 - acc: 0.5944
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6652 - acc: 0.5956 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6641 - acc: 0.5980
1728/1760 [============================>.] - ETA: 1s - loss: 0.6631 - acc: 0.5978
1760/1760 [==============================] - 93s 53ms/step - loss: 0.6637 - acc: 0.5983 - val_loss: 0.6515 - val_acc: 0.5612

Epoch 00007: val_acc did not improve from 0.64286
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:21 - loss: 0.5810 - acc: 0.7031
 128/1760 [=>............................] - ETA: 1:17 - loss: 0.6258 - acc: 0.6875
 192/1760 [==>...........................] - ETA: 1:14 - loss: 0.6115 - acc: 0.7031
 256/1760 [===>..........................] - ETA: 1:11 - loss: 0.6148 - acc: 0.6875
 320/1760 [====>.........................] - ETA: 1:07 - loss: 0.6242 - acc: 0.6687
 384/1760 [=====>........................] - ETA: 1:04 - loss: 0.6403 - acc: 0.6510
 448/1760 [======>.......................] - ETA: 1:01 - loss: 0.6561 - acc: 0.6339
 512/1760 [=======>......................] - ETA: 58s - loss: 0.6554 - acc: 0.6367 
 576/1760 [========>.....................] - ETA: 56s - loss: 0.6563 - acc: 0.6354
 640/1760 [=========>....................] - ETA: 53s - loss: 0.6565 - acc: 0.6297
 704/1760 [===========>..................] - ETA: 50s - loss: 0.6506 - acc: 0.6349
 768/1760 [============>.................] - ETA: 47s - loss: 0.6576 - acc: 0.6237
 832/1760 [=============>................] - ETA: 44s - loss: 0.6596 - acc: 0.6238
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6603 - acc: 0.6217
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6562 - acc: 0.6271
1024/1760 [================>.............] - ETA: 36s - loss: 0.6523 - acc: 0.6309
1088/1760 [=================>............] - ETA: 33s - loss: 0.6476 - acc: 0.6333
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6465 - acc: 0.6372
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6479 - acc: 0.6349
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6492 - acc: 0.6320
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6464 - acc: 0.6339
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6457 - acc: 0.6321
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6459 - acc: 0.6298
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6454 - acc: 0.6289
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6453 - acc: 0.6300 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6436 - acc: 0.6310
1728/1760 [============================>.] - ETA: 1s - loss: 0.6436 - acc: 0.6302
1760/1760 [==============================] - 93s 53ms/step - loss: 0.6417 - acc: 0.6318 - val_loss: 0.6300 - val_acc: 0.6224

Epoch 00008: val_acc did not improve from 0.64286
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:19 - loss: 0.6029 - acc: 0.7188
 128/1760 [=>............................] - ETA: 1:23 - loss: 0.5918 - acc: 0.7188
 192/1760 [==>...........................] - ETA: 1:24 - loss: 0.6151 - acc: 0.6979
 256/1760 [===>..........................] - ETA: 1:21 - loss: 0.6105 - acc: 0.6797
 320/1760 [====>.........................] - ETA: 1:15 - loss: 0.6232 - acc: 0.6656
 384/1760 [=====>........................] - ETA: 1:12 - loss: 0.6200 - acc: 0.6745
 448/1760 [======>.......................] - ETA: 1:09 - loss: 0.6164 - acc: 0.6719
 512/1760 [=======>......................] - ETA: 1:06 - loss: 0.6174 - acc: 0.6641
 576/1760 [========>.....................] - ETA: 1:01 - loss: 0.6101 - acc: 0.6753
 640/1760 [=========>....................] - ETA: 58s - loss: 0.6156 - acc: 0.6641 
 704/1760 [===========>..................] - ETA: 55s - loss: 0.6128 - acc: 0.6662
 768/1760 [============>.................] - ETA: 51s - loss: 0.6092 - acc: 0.6693
 832/1760 [=============>................] - ETA: 48s - loss: 0.6076 - acc: 0.6719
 896/1760 [==============>...............] - ETA: 45s - loss: 0.6157 - acc: 0.6652
 960/1760 [===============>..............] - ETA: 42s - loss: 0.6196 - acc: 0.6625
1024/1760 [================>.............] - ETA: 38s - loss: 0.6230 - acc: 0.6641
1088/1760 [=================>............] - ETA: 35s - loss: 0.6239 - acc: 0.6627
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6300 - acc: 0.6545
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6284 - acc: 0.6579
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6282 - acc: 0.6602
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6303 - acc: 0.6570
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6310 - acc: 0.6555
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6326 - acc: 0.6522
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6362 - acc: 0.6484
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6392 - acc: 0.6444 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6414 - acc: 0.6406
1728/1760 [============================>.] - ETA: 1s - loss: 0.6440 - acc: 0.6372
1760/1760 [==============================] - 98s 56ms/step - loss: 0.6423 - acc: 0.6392 - val_loss: 0.6357 - val_acc: 0.6378

Epoch 00009: val_acc did not improve from 0.64286
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:22 - loss: 0.5922 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:21 - loss: 0.6130 - acc: 0.6094
 192/1760 [==>...........................] - ETA: 1:17 - loss: 0.6232 - acc: 0.6146
 256/1760 [===>..........................] - ETA: 1:14 - loss: 0.6356 - acc: 0.6016
 320/1760 [====>.........................] - ETA: 1:10 - loss: 0.6460 - acc: 0.6000
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6454 - acc: 0.6172
 448/1760 [======>.......................] - ETA: 1:04 - loss: 0.6410 - acc: 0.6295
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6476 - acc: 0.6191
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6431 - acc: 0.6233 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6414 - acc: 0.6250
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6381 - acc: 0.6236
 768/1760 [============>.................] - ETA: 49s - loss: 0.6402 - acc: 0.6159
 832/1760 [=============>................] - ETA: 46s - loss: 0.6412 - acc: 0.6166
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6433 - acc: 0.6172
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6464 - acc: 0.6156
1024/1760 [================>.............] - ETA: 36s - loss: 0.6477 - acc: 0.6104
1088/1760 [=================>............] - ETA: 33s - loss: 0.6491 - acc: 0.6075
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6462 - acc: 0.6102
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6414 - acc: 0.6184
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6372 - acc: 0.6242
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6353 - acc: 0.6265
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6365 - acc: 0.6257
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6356 - acc: 0.6257
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6336 - acc: 0.6302
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6333 - acc: 0.6306 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6318 - acc: 0.6292
1728/1760 [============================>.] - ETA: 1s - loss: 0.6323 - acc: 0.6308
1760/1760 [==============================] - 94s 53ms/step - loss: 0.6352 - acc: 0.6290 - val_loss: 0.6508 - val_acc: 0.5867

Epoch 00010: val_acc did not improve from 0.64286
样本个数 200
样本个数 400
window_select16-21.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 17s
128/400 [========>.....................] - ETA: 10s
192/400 [=============>................] - ETA: 6s 
256/400 [==================>...........] - ETA: 4s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 11s 26ms/step
loss: 0.6706832480430603
acc: 0.575
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f4c944a1050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f4c944a1050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f4c9444bd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f4c9444bd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f542f29cb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f542f29cb10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c5445dbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c5445dbd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f542f3af9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f542f3af9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f542f35e650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f542f35e650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f542f367e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f542f367e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c544581d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c544581d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f542f396710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f542f396710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c94284390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c94284390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c9428a9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c9428a9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c94338890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c94338890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c7476e410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c7476e410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c746d8190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c746d8190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c74730e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c74730e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c74719c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c74719c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c74773a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c74773a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c54396390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c54396390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c7468b590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c7468b590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c54186290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c54186290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c9443db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c9443db90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c7468b0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c7468b0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c7476e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c7476e8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c34798610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c34798610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c34745f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c34745f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c3456ff50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c3456ff50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c34798350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c34798350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c3451a750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c3451a750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c34471c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c34471c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c342cf490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c342cf490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c346170d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c346170d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c34471710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c34471710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c3415ee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c3415ee10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c34389690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c34389690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c34075610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c34075610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c34157790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c34157790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c346694d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c346694d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c147dfb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c147dfb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c147912d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c147912d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c340b1c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c340b1c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c147ce510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c147ce510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c14791150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c14791150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c14638e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c14638e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c145029d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c145029d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c141a4910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c141a4910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c142a8290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c142a8290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c1429cc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c1429cc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c147ce1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c147ce1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c142b2610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c142b2610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4bf4681490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4bf4681490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4bf4703e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4bf4703e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c141a6310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c141a6310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4bf471db50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4bf471db50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4bf44a2fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4bf44a2fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4bf43a7590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4bf43a7590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4bf442bb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4bf442bb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4bf44a2d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4bf44a2d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c147ced50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c147ced50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4bf40c5250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4bf40c5250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4bf4050d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4bf4050d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4bf416d510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4bf416d510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4bf40c5c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4bf40c5c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c1418c850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c1418c850>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 5:06 - loss: 0.7807 - acc: 0.4375
 128/1760 [=>............................] - ETA: 3:16 - loss: 0.7666 - acc: 0.4766
 192/1760 [==>...........................] - ETA: 2:37 - loss: 0.7384 - acc: 0.5052
 256/1760 [===>..........................] - ETA: 2:13 - loss: 0.7474 - acc: 0.4883
 320/1760 [====>.........................] - ETA: 1:58 - loss: 0.7534 - acc: 0.4844
 384/1760 [=====>........................] - ETA: 1:47 - loss: 0.7546 - acc: 0.4792
 448/1760 [======>.......................] - ETA: 1:38 - loss: 0.7527 - acc: 0.4866
 512/1760 [=======>......................] - ETA: 1:31 - loss: 0.7475 - acc: 0.4902
 576/1760 [========>.....................] - ETA: 1:25 - loss: 0.7460 - acc: 0.4948
 640/1760 [=========>....................] - ETA: 1:19 - loss: 0.7442 - acc: 0.5016
 704/1760 [===========>..................] - ETA: 1:13 - loss: 0.7415 - acc: 0.4943
 768/1760 [============>.................] - ETA: 1:09 - loss: 0.7372 - acc: 0.4974
 832/1760 [=============>................] - ETA: 1:03 - loss: 0.7270 - acc: 0.5156
 896/1760 [==============>...............] - ETA: 58s - loss: 0.7241 - acc: 0.5201 
 960/1760 [===============>..............] - ETA: 52s - loss: 0.7277 - acc: 0.5135
1024/1760 [================>.............] - ETA: 47s - loss: 0.7241 - acc: 0.5176
1088/1760 [=================>............] - ETA: 42s - loss: 0.7257 - acc: 0.5175
1152/1760 [==================>...........] - ETA: 37s - loss: 0.7235 - acc: 0.5191
1216/1760 [===================>..........] - ETA: 33s - loss: 0.7260 - acc: 0.5140
1280/1760 [====================>.........] - ETA: 29s - loss: 0.7274 - acc: 0.5078
1344/1760 [=====================>........] - ETA: 25s - loss: 0.7256 - acc: 0.5097
1408/1760 [=======================>......] - ETA: 20s - loss: 0.7247 - acc: 0.5092
1472/1760 [========================>.....] - ETA: 17s - loss: 0.7227 - acc: 0.5115
1536/1760 [=========================>....] - ETA: 13s - loss: 0.7197 - acc: 0.5156
1600/1760 [==========================>...] - ETA: 9s - loss: 0.7183 - acc: 0.5150 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7194 - acc: 0.5126
1728/1760 [============================>.] - ETA: 1s - loss: 0.7189 - acc: 0.5122
1760/1760 [==============================] - 108s 61ms/step - loss: 0.7182 - acc: 0.5142 - val_loss: 0.6769 - val_acc: 0.5408

Epoch 00001: val_acc improved from -inf to 0.54082, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window18/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:18 - loss: 0.6976 - acc: 0.5312
 128/1760 [=>............................] - ETA: 1:13 - loss: 0.7185 - acc: 0.4844
 192/1760 [==>...........................] - ETA: 1:16 - loss: 0.7040 - acc: 0.4948
 256/1760 [===>..........................] - ETA: 1:15 - loss: 0.7056 - acc: 0.4883
 320/1760 [====>.........................] - ETA: 1:12 - loss: 0.7038 - acc: 0.5031
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6999 - acc: 0.5078
 448/1760 [======>.......................] - ETA: 1:07 - loss: 0.7022 - acc: 0.5000
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6990 - acc: 0.5078
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6997 - acc: 0.5104 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6966 - acc: 0.5188
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6942 - acc: 0.5241
 768/1760 [============>.................] - ETA: 50s - loss: 0.6919 - acc: 0.5312
 832/1760 [=============>................] - ETA: 47s - loss: 0.6899 - acc: 0.5373
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6966 - acc: 0.5335
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6988 - acc: 0.5333
1024/1760 [================>.............] - ETA: 37s - loss: 0.6976 - acc: 0.5361
1088/1760 [=================>............] - ETA: 34s - loss: 0.6999 - acc: 0.5331
1152/1760 [==================>...........] - ETA: 31s - loss: 0.7039 - acc: 0.5304
1216/1760 [===================>..........] - ETA: 27s - loss: 0.7027 - acc: 0.5329
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6997 - acc: 0.5398
1344/1760 [=====================>........] - ETA: 21s - loss: 0.7015 - acc: 0.5372
1408/1760 [=======================>......] - ETA: 17s - loss: 0.7009 - acc: 0.5348
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6999 - acc: 0.5360
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6975 - acc: 0.5423
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6991 - acc: 0.5387 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6996 - acc: 0.5361
1728/1760 [============================>.] - ETA: 1s - loss: 0.6999 - acc: 0.5341
1760/1760 [==============================] - 94s 53ms/step - loss: 0.7010 - acc: 0.5330 - val_loss: 0.6783 - val_acc: 0.5765

Epoch 00002: val_acc improved from 0.54082 to 0.57653, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window18/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:19 - loss: 0.7283 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:25 - loss: 0.7025 - acc: 0.5625
 192/1760 [==>...........................] - ETA: 1:22 - loss: 0.6967 - acc: 0.5625
 256/1760 [===>..........................] - ETA: 1:17 - loss: 0.6970 - acc: 0.5586
 320/1760 [====>.........................] - ETA: 1:14 - loss: 0.6915 - acc: 0.5687
 384/1760 [=====>........................] - ETA: 1:10 - loss: 0.6884 - acc: 0.5755
 448/1760 [======>.......................] - ETA: 1:07 - loss: 0.6879 - acc: 0.5737
 512/1760 [=======>......................] - ETA: 1:03 - loss: 0.6887 - acc: 0.5723
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6940 - acc: 0.5625
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6917 - acc: 0.5578 
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6913 - acc: 0.5582
 768/1760 [============>.................] - ETA: 50s - loss: 0.6935 - acc: 0.5534
 832/1760 [=============>................] - ETA: 47s - loss: 0.6905 - acc: 0.5577
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6905 - acc: 0.5580
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6918 - acc: 0.5542
1024/1760 [================>.............] - ETA: 37s - loss: 0.6913 - acc: 0.5527
1088/1760 [=================>............] - ETA: 34s - loss: 0.6899 - acc: 0.5551
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6920 - acc: 0.5486
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6873 - acc: 0.5576
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6841 - acc: 0.5656
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6827 - acc: 0.5662
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6851 - acc: 0.5618
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6845 - acc: 0.5632
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6853 - acc: 0.5599
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6836 - acc: 0.5637 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6825 - acc: 0.5649
1728/1760 [============================>.] - ETA: 1s - loss: 0.6806 - acc: 0.5671
1760/1760 [==============================] - 95s 54ms/step - loss: 0.6805 - acc: 0.5665 - val_loss: 0.6845 - val_acc: 0.5408

Epoch 00003: val_acc did not improve from 0.57653
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:20 - loss: 0.6703 - acc: 0.5469
 128/1760 [=>............................] - ETA: 1:20 - loss: 0.6606 - acc: 0.5625
 192/1760 [==>...........................] - ETA: 1:16 - loss: 0.6622 - acc: 0.5729
 256/1760 [===>..........................] - ETA: 1:13 - loss: 0.6749 - acc: 0.5547
 320/1760 [====>.........................] - ETA: 1:09 - loss: 0.6901 - acc: 0.5469
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6886 - acc: 0.5599
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6865 - acc: 0.5580
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6899 - acc: 0.5488
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6901 - acc: 0.5486 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6885 - acc: 0.5547
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6812 - acc: 0.5668
 768/1760 [============>.................] - ETA: 49s - loss: 0.6820 - acc: 0.5651
 832/1760 [=============>................] - ETA: 46s - loss: 0.6890 - acc: 0.5565
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6943 - acc: 0.5491
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6909 - acc: 0.5531
1024/1760 [================>.............] - ETA: 37s - loss: 0.6901 - acc: 0.5576
1088/1760 [=================>............] - ETA: 33s - loss: 0.6883 - acc: 0.5588
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6871 - acc: 0.5608
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6855 - acc: 0.5666
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6876 - acc: 0.5609
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6888 - acc: 0.5618
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6930 - acc: 0.5568
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6933 - acc: 0.5550
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6936 - acc: 0.5540
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6961 - acc: 0.5537 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6954 - acc: 0.5547
1728/1760 [============================>.] - ETA: 1s - loss: 0.6951 - acc: 0.5567
1760/1760 [==============================] - 92s 52ms/step - loss: 0.6947 - acc: 0.5580 - val_loss: 0.6640 - val_acc: 0.6173

Epoch 00004: val_acc improved from 0.57653 to 0.61735, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window18/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:14 - loss: 0.6926 - acc: 0.5000
 128/1760 [=>............................] - ETA: 1:17 - loss: 0.6949 - acc: 0.5312
 192/1760 [==>...........................] - ETA: 1:17 - loss: 0.6911 - acc: 0.5365
 256/1760 [===>..........................] - ETA: 1:16 - loss: 0.6920 - acc: 0.5352
 320/1760 [====>.........................] - ETA: 1:11 - loss: 0.6874 - acc: 0.5469
 384/1760 [=====>........................] - ETA: 1:09 - loss: 0.6838 - acc: 0.5521
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6782 - acc: 0.5558
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6757 - acc: 0.5605
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6825 - acc: 0.5538 
 640/1760 [=========>....................] - ETA: 54s - loss: 0.6827 - acc: 0.5531
 704/1760 [===========>..................] - ETA: 51s - loss: 0.6780 - acc: 0.5639
 768/1760 [============>.................] - ETA: 48s - loss: 0.6743 - acc: 0.5651
 832/1760 [=============>................] - ETA: 45s - loss: 0.6697 - acc: 0.5733
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6710 - acc: 0.5759
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6685 - acc: 0.5792
1024/1760 [================>.............] - ETA: 36s - loss: 0.6674 - acc: 0.5791
1088/1760 [=================>............] - ETA: 33s - loss: 0.6684 - acc: 0.5772
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6679 - acc: 0.5807
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6657 - acc: 0.5847
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6649 - acc: 0.5859
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6657 - acc: 0.5841
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6666 - acc: 0.5817
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6643 - acc: 0.5856
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6643 - acc: 0.5879
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6633 - acc: 0.5894 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6619 - acc: 0.5931
1728/1760 [============================>.] - ETA: 1s - loss: 0.6617 - acc: 0.5932
1760/1760 [==============================] - 92s 53ms/step - loss: 0.6606 - acc: 0.5938 - val_loss: 0.6543 - val_acc: 0.5969

Epoch 00005: val_acc did not improve from 0.61735
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:33 - loss: 0.6298 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:23 - loss: 0.5883 - acc: 0.7031
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.5774 - acc: 0.6979
 256/1760 [===>..........................] - ETA: 1:15 - loss: 0.6093 - acc: 0.6719
 320/1760 [====>.........................] - ETA: 1:12 - loss: 0.6202 - acc: 0.6562
 384/1760 [=====>........................] - ETA: 1:09 - loss: 0.6350 - acc: 0.6458
 448/1760 [======>.......................] - ETA: 1:06 - loss: 0.6357 - acc: 0.6496
 512/1760 [=======>......................] - ETA: 1:03 - loss: 0.6330 - acc: 0.6562
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6366 - acc: 0.6510 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6369 - acc: 0.6516
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6455 - acc: 0.6364
 768/1760 [============>.................] - ETA: 49s - loss: 0.6440 - acc: 0.6367
 832/1760 [=============>................] - ETA: 46s - loss: 0.6420 - acc: 0.6394
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6418 - acc: 0.6406
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6435 - acc: 0.6385
1024/1760 [================>.............] - ETA: 36s - loss: 0.6434 - acc: 0.6367
1088/1760 [=================>............] - ETA: 33s - loss: 0.6430 - acc: 0.6369
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6446 - acc: 0.6363
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6462 - acc: 0.6324
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6467 - acc: 0.6328
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6467 - acc: 0.6324
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6443 - acc: 0.6349
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6452 - acc: 0.6338
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6482 - acc: 0.6322
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6488 - acc: 0.6306 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6471 - acc: 0.6340
1728/1760 [============================>.] - ETA: 1s - loss: 0.6483 - acc: 0.6331
1760/1760 [==============================] - 96s 54ms/step - loss: 0.6462 - acc: 0.6364 - val_loss: 0.6447 - val_acc: 0.6735

Epoch 00006: val_acc improved from 0.61735 to 0.67347, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window18/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:20 - loss: 0.5969 - acc: 0.6719
 128/1760 [=>............................] - ETA: 1:24 - loss: 0.6521 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 1:21 - loss: 0.6561 - acc: 0.6094
 256/1760 [===>..........................] - ETA: 1:16 - loss: 0.6693 - acc: 0.5898
 320/1760 [====>.........................] - ETA: 1:13 - loss: 0.6683 - acc: 0.5844
 384/1760 [=====>........................] - ETA: 1:10 - loss: 0.6759 - acc: 0.5807
 448/1760 [======>.......................] - ETA: 1:10 - loss: 0.6670 - acc: 0.5982
 512/1760 [=======>......................] - ETA: 1:05 - loss: 0.6680 - acc: 0.6016
 576/1760 [========>.....................] - ETA: 1:01 - loss: 0.6678 - acc: 0.5990
 640/1760 [=========>....................] - ETA: 57s - loss: 0.6649 - acc: 0.6000 
 704/1760 [===========>..................] - ETA: 54s - loss: 0.6601 - acc: 0.6051
 768/1760 [============>.................] - ETA: 51s - loss: 0.6563 - acc: 0.6107
 832/1760 [=============>................] - ETA: 47s - loss: 0.6613 - acc: 0.6094
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6570 - acc: 0.6150
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6560 - acc: 0.6167
1024/1760 [================>.............] - ETA: 37s - loss: 0.6561 - acc: 0.6133
1088/1760 [=================>............] - ETA: 33s - loss: 0.6577 - acc: 0.6094
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6570 - acc: 0.6111
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6602 - acc: 0.6061
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6594 - acc: 0.6062
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6594 - acc: 0.6086
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6583 - acc: 0.6094
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6571 - acc: 0.6067
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6576 - acc: 0.6035
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6560 - acc: 0.6050 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6570 - acc: 0.6046
1728/1760 [============================>.] - ETA: 1s - loss: 0.6554 - acc: 0.6059
1760/1760 [==============================] - 92s 52ms/step - loss: 0.6554 - acc: 0.6057 - val_loss: 0.6696 - val_acc: 0.6122

Epoch 00007: val_acc did not improve from 0.67347
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:22 - loss: 0.5844 - acc: 0.7500
 128/1760 [=>............................] - ETA: 1:26 - loss: 0.6095 - acc: 0.6719
 192/1760 [==>...........................] - ETA: 1:19 - loss: 0.6132 - acc: 0.6771
 256/1760 [===>..........................] - ETA: 1:18 - loss: 0.6171 - acc: 0.6719
 320/1760 [====>.........................] - ETA: 1:12 - loss: 0.5996 - acc: 0.6969
 384/1760 [=====>........................] - ETA: 1:09 - loss: 0.6045 - acc: 0.6953
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6220 - acc: 0.6763
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6250 - acc: 0.6758
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6441 - acc: 0.6545 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6454 - acc: 0.6516
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6465 - acc: 0.6435
 768/1760 [============>.................] - ETA: 50s - loss: 0.6463 - acc: 0.6458
 832/1760 [=============>................] - ETA: 46s - loss: 0.6426 - acc: 0.6490
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6396 - acc: 0.6507
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6377 - acc: 0.6531
1024/1760 [================>.............] - ETA: 37s - loss: 0.6423 - acc: 0.6465
1088/1760 [=================>............] - ETA: 34s - loss: 0.6435 - acc: 0.6452
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6442 - acc: 0.6432
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6433 - acc: 0.6431
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6461 - acc: 0.6359
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6446 - acc: 0.6369
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6446 - acc: 0.6378
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6451 - acc: 0.6365
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6457 - acc: 0.6367
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6454 - acc: 0.6362 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6458 - acc: 0.6388
1728/1760 [============================>.] - ETA: 1s - loss: 0.6445 - acc: 0.6395
1760/1760 [==============================] - 93s 53ms/step - loss: 0.6450 - acc: 0.6392 - val_loss: 0.6825 - val_acc: 0.5714

Epoch 00008: val_acc did not improve from 0.67347
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:26 - loss: 0.6749 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:26 - loss: 0.6486 - acc: 0.6406
 192/1760 [==>...........................] - ETA: 1:23 - loss: 0.6411 - acc: 0.6406
 256/1760 [===>..........................] - ETA: 1:21 - loss: 0.6473 - acc: 0.6445
 320/1760 [====>.........................] - ETA: 1:18 - loss: 0.6424 - acc: 0.6406
 384/1760 [=====>........................] - ETA: 1:15 - loss: 0.6419 - acc: 0.6484
 448/1760 [======>.......................] - ETA: 1:13 - loss: 0.6484 - acc: 0.6339
 512/1760 [=======>......................] - ETA: 1:09 - loss: 0.6473 - acc: 0.6230
 576/1760 [========>.....................] - ETA: 1:06 - loss: 0.6511 - acc: 0.6198
 640/1760 [=========>....................] - ETA: 1:03 - loss: 0.6498 - acc: 0.6172
 704/1760 [===========>..................] - ETA: 1:00 - loss: 0.6492 - acc: 0.6136
 768/1760 [============>.................] - ETA: 57s - loss: 0.6493 - acc: 0.6172 
 832/1760 [=============>................] - ETA: 53s - loss: 0.6464 - acc: 0.6226
 896/1760 [==============>...............] - ETA: 49s - loss: 0.6500 - acc: 0.6194
 960/1760 [===============>..............] - ETA: 46s - loss: 0.6516 - acc: 0.6188
1024/1760 [================>.............] - ETA: 42s - loss: 0.6536 - acc: 0.6123
1088/1760 [=================>............] - ETA: 38s - loss: 0.6496 - acc: 0.6167
1152/1760 [==================>...........] - ETA: 35s - loss: 0.6485 - acc: 0.6181
1216/1760 [===================>..........] - ETA: 31s - loss: 0.6484 - acc: 0.6160
1280/1760 [====================>.........] - ETA: 28s - loss: 0.6438 - acc: 0.6219
1344/1760 [=====================>........] - ETA: 24s - loss: 0.6410 - acc: 0.6243
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6425 - acc: 0.6214
1472/1760 [========================>.....] - ETA: 17s - loss: 0.6416 - acc: 0.6236
1536/1760 [=========================>....] - ETA: 13s - loss: 0.6409 - acc: 0.6250
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6406 - acc: 0.6231 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6388 - acc: 0.6256
1728/1760 [============================>.] - ETA: 1s - loss: 0.6383 - acc: 0.6273
1760/1760 [==============================] - 110s 63ms/step - loss: 0.6387 - acc: 0.6267 - val_loss: 0.6533 - val_acc: 0.6224

Epoch 00009: val_acc did not improve from 0.67347
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:49 - loss: 0.6168 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:35 - loss: 0.6234 - acc: 0.6172
 192/1760 [==>...........................] - ETA: 1:33 - loss: 0.6366 - acc: 0.6146
 256/1760 [===>..........................] - ETA: 1:26 - loss: 0.6265 - acc: 0.6211
 320/1760 [====>.........................] - ETA: 1:24 - loss: 0.6348 - acc: 0.6062
 384/1760 [=====>........................] - ETA: 1:20 - loss: 0.6374 - acc: 0.6068
 448/1760 [======>.......................] - ETA: 1:18 - loss: 0.6409 - acc: 0.6094
 512/1760 [=======>......................] - ETA: 1:16 - loss: 0.6469 - acc: 0.6074
 576/1760 [========>.....................] - ETA: 1:13 - loss: 0.6462 - acc: 0.6059
 640/1760 [=========>....................] - ETA: 1:10 - loss: 0.6474 - acc: 0.6047
 704/1760 [===========>..................] - ETA: 1:06 - loss: 0.6470 - acc: 0.6094
 768/1760 [============>.................] - ETA: 1:02 - loss: 0.6387 - acc: 0.6185
 832/1760 [=============>................] - ETA: 58s - loss: 0.6393 - acc: 0.6202 
 896/1760 [==============>...............] - ETA: 55s - loss: 0.6393 - acc: 0.6172
 960/1760 [===============>..............] - ETA: 52s - loss: 0.6414 - acc: 0.6188
1024/1760 [================>.............] - ETA: 47s - loss: 0.6416 - acc: 0.6182
1088/1760 [=================>............] - ETA: 44s - loss: 0.6410 - acc: 0.6158
1152/1760 [==================>...........] - ETA: 39s - loss: 0.6438 - acc: 0.6155
1216/1760 [===================>..........] - ETA: 35s - loss: 0.6451 - acc: 0.6184
1280/1760 [====================>.........] - ETA: 31s - loss: 0.6498 - acc: 0.6156
1344/1760 [=====================>........] - ETA: 27s - loss: 0.6469 - acc: 0.6228
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6441 - acc: 0.6257
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6413 - acc: 0.6284
1536/1760 [=========================>....] - ETA: 14s - loss: 0.6411 - acc: 0.6315
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6434 - acc: 0.6312
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6415 - acc: 0.6322 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6402 - acc: 0.6337
1760/1760 [==============================] - 124s 70ms/step - loss: 0.6393 - acc: 0.6335 - val_loss: 0.7341 - val_acc: 0.5459

Epoch 00010: val_acc did not improve from 0.67347
样本个数 200
样本个数 400
window_select16-21.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 21s
128/400 [========>.....................] - ETA: 11s
192/400 [=============>................] - ETA: 7s 
256/400 [==================>...........] - ETA: 4s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 11s 28ms/step
loss: 0.6510449647903442
acc: 0.5925
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f4b541c56d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f4b541c56d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f4c5444e5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f4c5444e5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b1417afd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b1417afd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5415e6c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5415e6c1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4b541eda10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4b541eda10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b541ab910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b541ab910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f5415e6c590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f5415e6c590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f541e9bdcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f541e9bdcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4b3473f4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4b3473f4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4b346c7dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4b346c7dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b347ab990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b347ab990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4b3473f690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4b3473f690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b1419b450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b1419b450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4b34472b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4b34472b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4b3464c590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4b3464c590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b34318290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b34318290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4b34472c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4b34472c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b140d8310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b140d8310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4b343df650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4b343df650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4af463ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4af463ac50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b3436c210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b3436c210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4b3431a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4b3431a390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4af46b3d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4af46b3d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4af47e5550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4af47e5550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4af43f3090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4af43f3090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4af44de350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4af44de350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4af47023d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4af47023d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4af45984d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4af45984d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4af44a1110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4af44a1110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4af40ca350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4af40ca350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4af4434e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4af4434e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4af44a1410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4af44a1410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4af41a1b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4af41a1b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4af4039050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4af4039050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4ad4632450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4ad4632450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ad45ff190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ad45ff190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4af4039550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4af4039550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ad4523590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ad4523590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4ad42e08d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4ad42e08d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4ad4338290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4ad4338290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ad4518250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ad4518250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4ad45b34d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4ad45b34d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b541dbe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4b541dbe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4ad422b890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4ad422b890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4ab47c2c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4ab47c2c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ad422c810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ad422c810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4b543d23d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4b543d23d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ab479efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ab479efd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4ab47bca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4ab47bca90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4ab44af690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4ab44af690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ab424a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ab424a390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4ab44e0ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4ab44e0ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ab43af850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ab43af850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4ab41a89d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4ab41a89d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4ab40ba810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4ab40ba810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4a9472a750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4a9472a750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4ab4419110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4ab4419110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4a947bd1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4a947bd1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4a946bded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4a946bded0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4a94593550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4a94593550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4a943c8250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4a943c8250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4a946c1b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4a946c1b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4a943dcb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4a943dcb10>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 6:08 - loss: 0.7061 - acc: 0.5312
 128/1760 [=>............................] - ETA: 3:40 - loss: 0.7729 - acc: 0.5156
 192/1760 [==>...........................] - ETA: 2:50 - loss: 0.7393 - acc: 0.5365
 256/1760 [===>..........................] - ETA: 2:21 - loss: 0.7330 - acc: 0.5469
 320/1760 [====>.........................] - ETA: 2:04 - loss: 0.7448 - acc: 0.5281
 384/1760 [=====>........................] - ETA: 1:52 - loss: 0.7343 - acc: 0.5391
 448/1760 [======>.......................] - ETA: 1:42 - loss: 0.7337 - acc: 0.5379
 512/1760 [=======>......................] - ETA: 1:34 - loss: 0.7371 - acc: 0.5254
 576/1760 [========>.....................] - ETA: 1:27 - loss: 0.7321 - acc: 0.5295
 640/1760 [=========>....................] - ETA: 1:20 - loss: 0.7295 - acc: 0.5344
 704/1760 [===========>..................] - ETA: 1:16 - loss: 0.7316 - acc: 0.5256
 768/1760 [============>.................] - ETA: 1:09 - loss: 0.7348 - acc: 0.5208
 832/1760 [=============>................] - ETA: 1:04 - loss: 0.7331 - acc: 0.5192
 896/1760 [==============>...............] - ETA: 59s - loss: 0.7385 - acc: 0.5156 
 960/1760 [===============>..............] - ETA: 54s - loss: 0.7377 - acc: 0.5146
1024/1760 [================>.............] - ETA: 49s - loss: 0.7386 - acc: 0.5107
1088/1760 [=================>............] - ETA: 45s - loss: 0.7393 - acc: 0.5129
1152/1760 [==================>...........] - ETA: 40s - loss: 0.7378 - acc: 0.5130
1216/1760 [===================>..........] - ETA: 36s - loss: 0.7357 - acc: 0.5156
1280/1760 [====================>.........] - ETA: 31s - loss: 0.7340 - acc: 0.5141
1344/1760 [=====================>........] - ETA: 27s - loss: 0.7314 - acc: 0.5179
1408/1760 [=======================>......] - ETA: 22s - loss: 0.7272 - acc: 0.5256
1472/1760 [========================>.....] - ETA: 18s - loss: 0.7263 - acc: 0.5265
1536/1760 [=========================>....] - ETA: 14s - loss: 0.7246 - acc: 0.5299
1600/1760 [==========================>...] - ETA: 9s - loss: 0.7253 - acc: 0.5256 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7246 - acc: 0.5246
1728/1760 [============================>.] - ETA: 1s - loss: 0.7240 - acc: 0.5249
1760/1760 [==============================] - 113s 64ms/step - loss: 0.7249 - acc: 0.5216 - val_loss: 0.6773 - val_acc: 0.5459

Epoch 00001: val_acc improved from -inf to 0.54592, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window19/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:18 - loss: 0.6915 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:19 - loss: 0.7112 - acc: 0.5312
 192/1760 [==>...........................] - ETA: 1:18 - loss: 0.6989 - acc: 0.5312
 256/1760 [===>..........................] - ETA: 1:14 - loss: 0.6912 - acc: 0.5352
 320/1760 [====>.........................] - ETA: 1:11 - loss: 0.6941 - acc: 0.5375
 384/1760 [=====>........................] - ETA: 1:07 - loss: 0.6913 - acc: 0.5312
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6938 - acc: 0.5357
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6965 - acc: 0.5293
 576/1760 [========>.....................] - ETA: 58s - loss: 0.7009 - acc: 0.5208 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.7059 - acc: 0.5172
 704/1760 [===========>..................] - ETA: 52s - loss: 0.7039 - acc: 0.5227
 768/1760 [============>.................] - ETA: 49s - loss: 0.7076 - acc: 0.5208
 832/1760 [=============>................] - ETA: 45s - loss: 0.7051 - acc: 0.5216
 896/1760 [==============>...............] - ETA: 42s - loss: 0.7079 - acc: 0.5201
 960/1760 [===============>..............] - ETA: 39s - loss: 0.7104 - acc: 0.5135
1024/1760 [================>.............] - ETA: 36s - loss: 0.7133 - acc: 0.5068
1088/1760 [=================>............] - ETA: 33s - loss: 0.7092 - acc: 0.5101
1152/1760 [==================>...........] - ETA: 29s - loss: 0.7106 - acc: 0.5122
1216/1760 [===================>..........] - ETA: 27s - loss: 0.7100 - acc: 0.5107
1280/1760 [====================>.........] - ETA: 23s - loss: 0.7108 - acc: 0.5094
1344/1760 [=====================>........] - ETA: 20s - loss: 0.7084 - acc: 0.5126
1408/1760 [=======================>......] - ETA: 17s - loss: 0.7040 - acc: 0.5199
1472/1760 [========================>.....] - ETA: 14s - loss: 0.7052 - acc: 0.5149
1536/1760 [=========================>....] - ETA: 11s - loss: 0.7064 - acc: 0.5104
1600/1760 [==========================>...] - ETA: 8s - loss: 0.7067 - acc: 0.5075 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.7071 - acc: 0.5072
1728/1760 [============================>.] - ETA: 1s - loss: 0.7071 - acc: 0.5069
1760/1760 [==============================] - 92s 52ms/step - loss: 0.7055 - acc: 0.5102 - val_loss: 0.6792 - val_acc: 0.5306

Epoch 00002: val_acc did not improve from 0.54592
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:16 - loss: 0.7024 - acc: 0.5469
 128/1760 [=>............................] - ETA: 1:11 - loss: 0.7142 - acc: 0.5312
 192/1760 [==>...........................] - ETA: 1:09 - loss: 0.7096 - acc: 0.5417
 256/1760 [===>..........................] - ETA: 1:08 - loss: 0.7151 - acc: 0.5156
 320/1760 [====>.........................] - ETA: 1:04 - loss: 0.7171 - acc: 0.5094
 384/1760 [=====>........................] - ETA: 1:03 - loss: 0.7033 - acc: 0.5260
 448/1760 [======>.......................] - ETA: 1:00 - loss: 0.6951 - acc: 0.5312
 512/1760 [=======>......................] - ETA: 58s - loss: 0.6915 - acc: 0.5410 
 576/1760 [========>.....................] - ETA: 55s - loss: 0.6950 - acc: 0.5399
 640/1760 [=========>....................] - ETA: 52s - loss: 0.6939 - acc: 0.5391
 704/1760 [===========>..................] - ETA: 49s - loss: 0.6936 - acc: 0.5398
 768/1760 [============>.................] - ETA: 47s - loss: 0.6935 - acc: 0.5378
 832/1760 [=============>................] - ETA: 44s - loss: 0.6981 - acc: 0.5288
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6960 - acc: 0.5368
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6939 - acc: 0.5417
1024/1760 [================>.............] - ETA: 35s - loss: 0.6926 - acc: 0.5420
1088/1760 [=================>............] - ETA: 32s - loss: 0.6923 - acc: 0.5423
1152/1760 [==================>...........] - ETA: 28s - loss: 0.6886 - acc: 0.5503
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6892 - acc: 0.5493
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6886 - acc: 0.5508
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6902 - acc: 0.5469
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6904 - acc: 0.5462
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6899 - acc: 0.5469
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6912 - acc: 0.5449
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6899 - acc: 0.5469 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6903 - acc: 0.5475
1728/1760 [============================>.] - ETA: 1s - loss: 0.6903 - acc: 0.5463
1760/1760 [==============================] - 92s 52ms/step - loss: 0.6906 - acc: 0.5455 - val_loss: 0.6591 - val_acc: 0.6122

Epoch 00003: val_acc improved from 0.54592 to 0.61224, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window19/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:15 - loss: 0.6225 - acc: 0.6562
 128/1760 [=>............................] - ETA: 1:15 - loss: 0.6482 - acc: 0.6172
 192/1760 [==>...........................] - ETA: 1:11 - loss: 0.6542 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 1:08 - loss: 0.6556 - acc: 0.5977
 320/1760 [====>.........................] - ETA: 1:07 - loss: 0.6491 - acc: 0.6031
 384/1760 [=====>........................] - ETA: 1:05 - loss: 0.6548 - acc: 0.5990
 448/1760 [======>.......................] - ETA: 1:03 - loss: 0.6620 - acc: 0.5960
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6637 - acc: 0.5957
 576/1760 [========>.....................] - ETA: 59s - loss: 0.6652 - acc: 0.5972 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6773 - acc: 0.5844
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6806 - acc: 0.5824
 768/1760 [============>.................] - ETA: 50s - loss: 0.6770 - acc: 0.5846
 832/1760 [=============>................] - ETA: 47s - loss: 0.6787 - acc: 0.5817
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6790 - acc: 0.5781
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6778 - acc: 0.5813
1024/1760 [================>.............] - ETA: 37s - loss: 0.6825 - acc: 0.5703
1088/1760 [=================>............] - ETA: 34s - loss: 0.6828 - acc: 0.5680
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6848 - acc: 0.5642
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6855 - acc: 0.5658
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6848 - acc: 0.5664
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6859 - acc: 0.5640
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6859 - acc: 0.5625
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6838 - acc: 0.5666
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6859 - acc: 0.5651
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6846 - acc: 0.5681 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6842 - acc: 0.5703
1728/1760 [============================>.] - ETA: 1s - loss: 0.6831 - acc: 0.5712
1760/1760 [==============================] - 94s 53ms/step - loss: 0.6823 - acc: 0.5722 - val_loss: 0.6785 - val_acc: 0.5867

Epoch 00004: val_acc did not improve from 0.61224
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:12 - loss: 0.6425 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:15 - loss: 0.6751 - acc: 0.5703
 192/1760 [==>...........................] - ETA: 1:13 - loss: 0.6764 - acc: 0.5729
 256/1760 [===>..........................] - ETA: 1:12 - loss: 0.6646 - acc: 0.5977
 320/1760 [====>.........................] - ETA: 1:10 - loss: 0.6619 - acc: 0.5938
 384/1760 [=====>........................] - ETA: 1:07 - loss: 0.6681 - acc: 0.5703
 448/1760 [======>.......................] - ETA: 1:03 - loss: 0.6682 - acc: 0.5781
 512/1760 [=======>......................] - ETA: 59s - loss: 0.6722 - acc: 0.5742 
 576/1760 [========>.....................] - ETA: 57s - loss: 0.6748 - acc: 0.5799
 640/1760 [=========>....................] - ETA: 53s - loss: 0.6757 - acc: 0.5781
 704/1760 [===========>..................] - ETA: 50s - loss: 0.6723 - acc: 0.5852
 768/1760 [============>.................] - ETA: 48s - loss: 0.6726 - acc: 0.5859
 832/1760 [=============>................] - ETA: 44s - loss: 0.6708 - acc: 0.5913
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6681 - acc: 0.5926
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6664 - acc: 0.5969
1024/1760 [================>.............] - ETA: 36s - loss: 0.6631 - acc: 0.6035
1088/1760 [=================>............] - ETA: 33s - loss: 0.6624 - acc: 0.6057
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6632 - acc: 0.6076
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6639 - acc: 0.6094
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6624 - acc: 0.6102
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6627 - acc: 0.6101
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6630 - acc: 0.6058
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6668 - acc: 0.5992
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6669 - acc: 0.6003
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6672 - acc: 0.5994 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6676 - acc: 0.5980
1728/1760 [============================>.] - ETA: 1s - loss: 0.6685 - acc: 0.5972
1760/1760 [==============================] - 90s 51ms/step - loss: 0.6698 - acc: 0.5972 - val_loss: 0.6492 - val_acc: 0.5867

Epoch 00005: val_acc did not improve from 0.61224
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:11 - loss: 0.7318 - acc: 0.5000
 128/1760 [=>............................] - ETA: 1:13 - loss: 0.7206 - acc: 0.5000
 192/1760 [==>...........................] - ETA: 1:09 - loss: 0.6990 - acc: 0.5260
 256/1760 [===>..........................] - ETA: 1:11 - loss: 0.6967 - acc: 0.5469
 320/1760 [====>.........................] - ETA: 1:07 - loss: 0.6876 - acc: 0.5656
 384/1760 [=====>........................] - ETA: 1:04 - loss: 0.6812 - acc: 0.5729
 448/1760 [======>.......................] - ETA: 1:00 - loss: 0.6835 - acc: 0.5580
 512/1760 [=======>......................] - ETA: 59s - loss: 0.6866 - acc: 0.5508 
 576/1760 [========>.....................] - ETA: 55s - loss: 0.6867 - acc: 0.5521
 640/1760 [=========>....................] - ETA: 53s - loss: 0.6787 - acc: 0.5672
 704/1760 [===========>..................] - ETA: 50s - loss: 0.6809 - acc: 0.5653
 768/1760 [============>.................] - ETA: 47s - loss: 0.6827 - acc: 0.5638
 832/1760 [=============>................] - ETA: 44s - loss: 0.6777 - acc: 0.5733
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6763 - acc: 0.5759
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6756 - acc: 0.5781
1024/1760 [================>.............] - ETA: 35s - loss: 0.6740 - acc: 0.5840
1088/1760 [=================>............] - ETA: 32s - loss: 0.6769 - acc: 0.5781
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6771 - acc: 0.5764
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6752 - acc: 0.5798
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6728 - acc: 0.5844
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6747 - acc: 0.5826
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6778 - acc: 0.5795
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6800 - acc: 0.5747
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6810 - acc: 0.5716
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6805 - acc: 0.5744 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6785 - acc: 0.5787
1728/1760 [============================>.] - ETA: 1s - loss: 0.6775 - acc: 0.5816
1760/1760 [==============================] - 91s 52ms/step - loss: 0.6778 - acc: 0.5801 - val_loss: 0.6480 - val_acc: 0.6276

Epoch 00006: val_acc improved from 0.61224 to 0.62755, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window19/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:15 - loss: 0.6743 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:20 - loss: 0.6754 - acc: 0.6016
 192/1760 [==>...........................] - ETA: 1:17 - loss: 0.6460 - acc: 0.6458
 256/1760 [===>..........................] - ETA: 1:14 - loss: 0.6360 - acc: 0.6562
 320/1760 [====>.........................] - ETA: 1:11 - loss: 0.6419 - acc: 0.6438
 384/1760 [=====>........................] - ETA: 1:07 - loss: 0.6496 - acc: 0.6276
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6472 - acc: 0.6250
 512/1760 [=======>......................] - ETA: 1:02 - loss: 0.6525 - acc: 0.6211
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6576 - acc: 0.6146 
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6594 - acc: 0.6062
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6627 - acc: 0.5980
 768/1760 [============>.................] - ETA: 49s - loss: 0.6638 - acc: 0.5938
 832/1760 [=============>................] - ETA: 45s - loss: 0.6606 - acc: 0.5962
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6597 - acc: 0.5960
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6605 - acc: 0.5979
1024/1760 [================>.............] - ETA: 36s - loss: 0.6581 - acc: 0.5986
1088/1760 [=================>............] - ETA: 33s - loss: 0.6590 - acc: 0.6020
1152/1760 [==================>...........] - ETA: 30s - loss: 0.6607 - acc: 0.5998
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6617 - acc: 0.5954
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6581 - acc: 0.6000
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6590 - acc: 0.5990
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6594 - acc: 0.5973
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6595 - acc: 0.5985
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6582 - acc: 0.5990
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6552 - acc: 0.6012 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6525 - acc: 0.6052
1728/1760 [============================>.] - ETA: 1s - loss: 0.6531 - acc: 0.6071
1760/1760 [==============================] - 94s 53ms/step - loss: 0.6553 - acc: 0.6034 - val_loss: 0.6757 - val_acc: 0.5714

Epoch 00007: val_acc did not improve from 0.62755
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:38 - loss: 0.5811 - acc: 0.7031
 128/1760 [=>............................] - ETA: 1:33 - loss: 0.6044 - acc: 0.6797
 192/1760 [==>...........................] - ETA: 1:28 - loss: 0.6114 - acc: 0.6823
 256/1760 [===>..........................] - ETA: 1:27 - loss: 0.6268 - acc: 0.6602
 320/1760 [====>.........................] - ETA: 1:22 - loss: 0.6256 - acc: 0.6562
 384/1760 [=====>........................] - ETA: 1:18 - loss: 0.6243 - acc: 0.6484
 448/1760 [======>.......................] - ETA: 1:13 - loss: 0.6264 - acc: 0.6496
 512/1760 [=======>......................] - ETA: 1:09 - loss: 0.6312 - acc: 0.6465
 576/1760 [========>.....................] - ETA: 1:05 - loss: 0.6290 - acc: 0.6510
 640/1760 [=========>....................] - ETA: 1:01 - loss: 0.6435 - acc: 0.6344
 704/1760 [===========>..................] - ETA: 59s - loss: 0.6471 - acc: 0.6264 
 768/1760 [============>.................] - ETA: 55s - loss: 0.6507 - acc: 0.6250
 832/1760 [=============>................] - ETA: 52s - loss: 0.6510 - acc: 0.6238
 896/1760 [==============>...............] - ETA: 49s - loss: 0.6518 - acc: 0.6228
 960/1760 [===============>..............] - ETA: 45s - loss: 0.6504 - acc: 0.6188
1024/1760 [================>.............] - ETA: 42s - loss: 0.6546 - acc: 0.6113
1088/1760 [=================>............] - ETA: 38s - loss: 0.6531 - acc: 0.6131
1152/1760 [==================>...........] - ETA: 34s - loss: 0.6560 - acc: 0.6068
1216/1760 [===================>..........] - ETA: 30s - loss: 0.6558 - acc: 0.6061
1280/1760 [====================>.........] - ETA: 27s - loss: 0.6552 - acc: 0.6078
1344/1760 [=====================>........] - ETA: 23s - loss: 0.6565 - acc: 0.6042
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6559 - acc: 0.6023
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6562 - acc: 0.6012
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6565 - acc: 0.5996
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6581 - acc: 0.5981 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6585 - acc: 0.5974
1728/1760 [============================>.] - ETA: 1s - loss: 0.6576 - acc: 0.6013
1760/1760 [==============================] - 107s 61ms/step - loss: 0.6583 - acc: 0.6006 - val_loss: 0.6558 - val_acc: 0.6122

Epoch 00008: val_acc did not improve from 0.62755
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:45 - loss: 0.6587 - acc: 0.6094
 128/1760 [=>............................] - ETA: 1:34 - loss: 0.6383 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 1:30 - loss: 0.6259 - acc: 0.6510
 256/1760 [===>..........................] - ETA: 1:26 - loss: 0.6252 - acc: 0.6602
 320/1760 [====>.........................] - ETA: 1:22 - loss: 0.6369 - acc: 0.6438
 384/1760 [=====>........................] - ETA: 1:18 - loss: 0.6399 - acc: 0.6328
 448/1760 [======>.......................] - ETA: 1:13 - loss: 0.6457 - acc: 0.6205
 512/1760 [=======>......................] - ETA: 1:11 - loss: 0.6432 - acc: 0.6211
 576/1760 [========>.....................] - ETA: 1:08 - loss: 0.6438 - acc: 0.6181
 640/1760 [=========>....................] - ETA: 1:04 - loss: 0.6460 - acc: 0.6188
 704/1760 [===========>..................] - ETA: 1:00 - loss: 0.6512 - acc: 0.6193
 768/1760 [============>.................] - ETA: 57s - loss: 0.6529 - acc: 0.6159 
 832/1760 [=============>................] - ETA: 53s - loss: 0.6546 - acc: 0.6154
 896/1760 [==============>...............] - ETA: 49s - loss: 0.6515 - acc: 0.6217
 960/1760 [===============>..............] - ETA: 46s - loss: 0.6486 - acc: 0.6208
1024/1760 [================>.............] - ETA: 42s - loss: 0.6474 - acc: 0.6240
1088/1760 [=================>............] - ETA: 38s - loss: 0.6469 - acc: 0.6250
1152/1760 [==================>...........] - ETA: 35s - loss: 0.6466 - acc: 0.6285
1216/1760 [===================>..........] - ETA: 31s - loss: 0.6482 - acc: 0.6250
1280/1760 [====================>.........] - ETA: 27s - loss: 0.6503 - acc: 0.6266
1344/1760 [=====================>........] - ETA: 24s - loss: 0.6475 - acc: 0.6317
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6474 - acc: 0.6328
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6481 - acc: 0.6318
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6480 - acc: 0.6322
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6485 - acc: 0.6325 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6468 - acc: 0.6346
1728/1760 [============================>.] - ETA: 1s - loss: 0.6464 - acc: 0.6354
1760/1760 [==============================] - 109s 62ms/step - loss: 0.6463 - acc: 0.6358 - val_loss: 0.6176 - val_acc: 0.6684

Epoch 00009: val_acc improved from 0.62755 to 0.66837, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window19/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:35 - loss: 0.6075 - acc: 0.7188
 128/1760 [=>............................] - ETA: 1:31 - loss: 0.6537 - acc: 0.6406
 192/1760 [==>...........................] - ETA: 1:35 - loss: 0.6684 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 1:28 - loss: 0.6654 - acc: 0.6016
 320/1760 [====>.........................] - ETA: 1:23 - loss: 0.6587 - acc: 0.6125
 384/1760 [=====>........................] - ETA: 1:18 - loss: 0.6584 - acc: 0.6146
 448/1760 [======>.......................] - ETA: 1:15 - loss: 0.6557 - acc: 0.6228
 512/1760 [=======>......................] - ETA: 1:12 - loss: 0.6571 - acc: 0.6270
 576/1760 [========>.....................] - ETA: 1:08 - loss: 0.6595 - acc: 0.6215
 640/1760 [=========>....................] - ETA: 1:05 - loss: 0.6547 - acc: 0.6250
 704/1760 [===========>..................] - ETA: 1:03 - loss: 0.6535 - acc: 0.6151
 768/1760 [============>.................] - ETA: 59s - loss: 0.6609 - acc: 0.6094 
 832/1760 [=============>................] - ETA: 56s - loss: 0.6549 - acc: 0.6190
 896/1760 [==============>...............] - ETA: 53s - loss: 0.6559 - acc: 0.6150
 960/1760 [===============>..............] - ETA: 49s - loss: 0.6579 - acc: 0.6146
1024/1760 [================>.............] - ETA: 46s - loss: 0.6581 - acc: 0.6143
1088/1760 [=================>............] - ETA: 43s - loss: 0.6570 - acc: 0.6140
1152/1760 [==================>...........] - ETA: 39s - loss: 0.6553 - acc: 0.6181
1216/1760 [===================>..........] - ETA: 35s - loss: 0.6515 - acc: 0.6250
1280/1760 [====================>.........] - ETA: 31s - loss: 0.6543 - acc: 0.6203
1344/1760 [=====================>........] - ETA: 27s - loss: 0.6528 - acc: 0.6235
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6535 - acc: 0.6229
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6574 - acc: 0.6182
1536/1760 [=========================>....] - ETA: 14s - loss: 0.6526 - acc: 0.6237
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6543 - acc: 0.6194
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6543 - acc: 0.6184 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6551 - acc: 0.6163
1760/1760 [==============================] - 123s 70ms/step - loss: 0.6547 - acc: 0.6176 - val_loss: 0.6399 - val_acc: 0.6173

Epoch 00010: val_acc did not improve from 0.66837
样本个数 200
样本个数 400
window_select16-21.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 24s
128/400 [========>.....................] - ETA: 12s
192/400 [=============>................] - ETA: 7s 
256/400 [==================>...........] - ETA: 4s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 12s 30ms/step
loss: 0.6517287349700928
acc: 0.6125
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f49f46e29d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f49f46e29d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f49f4684110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f49f4684110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4db46ab310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4db46ab310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5415ea1e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5415ea1e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49b46a3b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49b46a3b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5415fbc8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5415fbc8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f5415ea1990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f5415ea1990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49f456ea50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49f456ea50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5415fcb210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5415fcb210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49f4423d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49f4423d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5415fcc190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5415fcc190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49f439d690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49f439d690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49f43a1fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49f43a1fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49f414fbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49f414fbd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49f413c690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49f413c690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49f423b390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49f423b390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49f41f10d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49f41f10d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49f41ebd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49f41ebd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49f40aa650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49f40aa650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49b4530fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49b4530fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ad4509850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4ad4509850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49f46cb150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49f46cb150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49b440df50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49b440df50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49b42d2490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49b42d2490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49b4153710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49b4153710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49b4224050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49b4224050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49b42d2f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49b42d2f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4994725c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4994725c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4994642790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4994642790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f499469b310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f499469b310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49b412cb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49b412cb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4994642c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4994642c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49944b68d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49944b68d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49946d4890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49946d4890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49942c7b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49942c7b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f499431cdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f499431cdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4994562f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4994562f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49943478d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49943478d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49747c1ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49747c1ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49747a5690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49747a5690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4994368c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4994368c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49747c1e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49747c1e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4974717210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4974717210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4994072850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4994072850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f497444e2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f497444e2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49743c1810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49743c1810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49745cf990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49745cf990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49742db250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49742db250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49742d0550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49742d0550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49743ea290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49743ea290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49742169d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49742169d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4974213310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4974213310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f495073f110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f495073f110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4974166990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4974166990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49506ed0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49506ed0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f495071af50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f495071af50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4974166710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4974166710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49504b5d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49504b5d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4950353b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4950353b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49502cef10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f49502cef10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f497408f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f497408f590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49503534d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f49503534d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49505a8e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f49505a8e90>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 7:19 - loss: 0.7664 - acc: 0.5000
 128/1760 [=>............................] - ETA: 4:24 - loss: 0.7692 - acc: 0.5469
 192/1760 [==>...........................] - ETA: 3:19 - loss: 0.7541 - acc: 0.5469
 256/1760 [===>..........................] - ETA: 2:44 - loss: 0.7478 - acc: 0.5547
 320/1760 [====>.........................] - ETA: 2:23 - loss: 0.7417 - acc: 0.5625
 384/1760 [=====>........................] - ETA: 2:05 - loss: 0.7649 - acc: 0.5417
 448/1760 [======>.......................] - ETA: 1:53 - loss: 0.7590 - acc: 0.5402
 512/1760 [=======>......................] - ETA: 1:42 - loss: 0.7512 - acc: 0.5469
 576/1760 [========>.....................] - ETA: 1:34 - loss: 0.7476 - acc: 0.5382
 640/1760 [=========>....................] - ETA: 1:26 - loss: 0.7483 - acc: 0.5344
 704/1760 [===========>..................] - ETA: 1:19 - loss: 0.7460 - acc: 0.5355
 768/1760 [============>.................] - ETA: 1:13 - loss: 0.7450 - acc: 0.5365
 832/1760 [=============>................] - ETA: 1:08 - loss: 0.7453 - acc: 0.5337
 896/1760 [==============>...............] - ETA: 1:02 - loss: 0.7406 - acc: 0.5357
 960/1760 [===============>..............] - ETA: 56s - loss: 0.7389 - acc: 0.5344 
1024/1760 [================>.............] - ETA: 51s - loss: 0.7355 - acc: 0.5381
1088/1760 [=================>............] - ETA: 46s - loss: 0.7394 - acc: 0.5312
1152/1760 [==================>...........] - ETA: 41s - loss: 0.7370 - acc: 0.5347
1216/1760 [===================>..........] - ETA: 36s - loss: 0.7372 - acc: 0.5321
1280/1760 [====================>.........] - ETA: 31s - loss: 0.7351 - acc: 0.5336
1344/1760 [=====================>........] - ETA: 27s - loss: 0.7355 - acc: 0.5312
1408/1760 [=======================>......] - ETA: 22s - loss: 0.7337 - acc: 0.5334
1472/1760 [========================>.....] - ETA: 18s - loss: 0.7317 - acc: 0.5360
1536/1760 [=========================>....] - ETA: 13s - loss: 0.7325 - acc: 0.5319
1600/1760 [==========================>...] - ETA: 9s - loss: 0.7317 - acc: 0.5319 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.7320 - acc: 0.5282
1728/1760 [============================>.] - ETA: 1s - loss: 0.7317 - acc: 0.5237
1760/1760 [==============================] - 113s 64ms/step - loss: 0.7311 - acc: 0.5233 - val_loss: 0.6654 - val_acc: 0.6276

Epoch 00001: val_acc improved from -inf to 0.62755, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window20/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:20 - loss: 0.7103 - acc: 0.5156
 128/1760 [=>............................] - ETA: 1:21 - loss: 0.7042 - acc: 0.5391
 192/1760 [==>...........................] - ETA: 1:19 - loss: 0.7191 - acc: 0.5156
 256/1760 [===>..........................] - ETA: 1:16 - loss: 0.7013 - acc: 0.5312
 320/1760 [====>.........................] - ETA: 1:11 - loss: 0.6931 - acc: 0.5563
 384/1760 [=====>........................] - ETA: 1:10 - loss: 0.7048 - acc: 0.5365
 448/1760 [======>.......................] - ETA: 1:08 - loss: 0.7065 - acc: 0.5290
 512/1760 [=======>......................] - ETA: 1:04 - loss: 0.6951 - acc: 0.5527
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6956 - acc: 0.5556
 640/1760 [=========>....................] - ETA: 57s - loss: 0.6938 - acc: 0.5594 
 704/1760 [===========>..................] - ETA: 54s - loss: 0.6903 - acc: 0.5668
 768/1760 [============>.................] - ETA: 50s - loss: 0.6889 - acc: 0.5716
 832/1760 [=============>................] - ETA: 47s - loss: 0.6912 - acc: 0.5637
 896/1760 [==============>...............] - ETA: 44s - loss: 0.6921 - acc: 0.5636
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6938 - acc: 0.5615
1024/1760 [================>.............] - ETA: 37s - loss: 0.6944 - acc: 0.5625
1088/1760 [=================>............] - ETA: 34s - loss: 0.6959 - acc: 0.5542
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6956 - acc: 0.5573
1216/1760 [===================>..........] - ETA: 27s - loss: 0.6963 - acc: 0.5551
1280/1760 [====================>.........] - ETA: 24s - loss: 0.6948 - acc: 0.5586
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6954 - acc: 0.5565
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6959 - acc: 0.5568
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6965 - acc: 0.5564
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6966 - acc: 0.5566
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6947 - acc: 0.5600 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6933 - acc: 0.5607
1728/1760 [============================>.] - ETA: 1s - loss: 0.6930 - acc: 0.5590
1760/1760 [==============================] - 92s 52ms/step - loss: 0.6921 - acc: 0.5597 - val_loss: 0.6567 - val_acc: 0.5867

Epoch 00002: val_acc did not improve from 0.62755
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:21 - loss: 0.6868 - acc: 0.5938
 128/1760 [=>............................] - ETA: 1:26 - loss: 0.6729 - acc: 0.5625
 192/1760 [==>...........................] - ETA: 1:24 - loss: 0.6961 - acc: 0.5729
 256/1760 [===>..........................] - ETA: 1:17 - loss: 0.6883 - acc: 0.5781
 320/1760 [====>.........................] - ETA: 1:13 - loss: 0.6875 - acc: 0.5719
 384/1760 [=====>........................] - ETA: 1:08 - loss: 0.6786 - acc: 0.5833
 448/1760 [======>.......................] - ETA: 1:05 - loss: 0.6798 - acc: 0.5804
 512/1760 [=======>......................] - ETA: 1:01 - loss: 0.6809 - acc: 0.5703
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6860 - acc: 0.5677 
 640/1760 [=========>....................] - ETA: 54s - loss: 0.6872 - acc: 0.5687
 704/1760 [===========>..................] - ETA: 51s - loss: 0.6851 - acc: 0.5710
 768/1760 [============>.................] - ETA: 49s - loss: 0.6834 - acc: 0.5716
 832/1760 [=============>................] - ETA: 45s - loss: 0.6799 - acc: 0.5757
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6858 - acc: 0.5681
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6867 - acc: 0.5698
1024/1760 [================>.............] - ETA: 36s - loss: 0.6867 - acc: 0.5713
1088/1760 [=================>............] - ETA: 33s - loss: 0.6862 - acc: 0.5744
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6817 - acc: 0.5799
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6803 - acc: 0.5814
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6833 - acc: 0.5797
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6811 - acc: 0.5841
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6805 - acc: 0.5838
1472/1760 [========================>.....] - ETA: 14s - loss: 0.6821 - acc: 0.5836
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6839 - acc: 0.5801
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6846 - acc: 0.5787 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6855 - acc: 0.5769
1728/1760 [============================>.] - ETA: 1s - loss: 0.6828 - acc: 0.5787
1760/1760 [==============================] - 91s 51ms/step - loss: 0.6835 - acc: 0.5778 - val_loss: 0.6465 - val_acc: 0.6071

Epoch 00003: val_acc did not improve from 0.62755
Epoch 4/10

  64/1760 [>.............................] - ETA: 1:23 - loss: 0.6941 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:15 - loss: 0.6575 - acc: 0.6250
 192/1760 [==>...........................] - ETA: 1:12 - loss: 0.6550 - acc: 0.6458
 256/1760 [===>..........................] - ETA: 1:10 - loss: 0.6584 - acc: 0.6289
 320/1760 [====>.........................] - ETA: 1:10 - loss: 0.6586 - acc: 0.6281
 384/1760 [=====>........................] - ETA: 1:06 - loss: 0.6555 - acc: 0.6250
 448/1760 [======>.......................] - ETA: 1:03 - loss: 0.6545 - acc: 0.6228
 512/1760 [=======>......................] - ETA: 1:00 - loss: 0.6615 - acc: 0.6133
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6674 - acc: 0.6024 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6646 - acc: 0.6047
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6673 - acc: 0.5994
 768/1760 [============>.................] - ETA: 48s - loss: 0.6687 - acc: 0.6003
 832/1760 [=============>................] - ETA: 45s - loss: 0.6668 - acc: 0.6046
 896/1760 [==============>...............] - ETA: 41s - loss: 0.6656 - acc: 0.6049
 960/1760 [===============>..............] - ETA: 38s - loss: 0.6614 - acc: 0.6146
1024/1760 [================>.............] - ETA: 35s - loss: 0.6625 - acc: 0.6162
1088/1760 [=================>............] - ETA: 32s - loss: 0.6596 - acc: 0.6186
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6598 - acc: 0.6181
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6624 - acc: 0.6127
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6654 - acc: 0.6062
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6614 - acc: 0.6124
1408/1760 [=======================>......] - ETA: 17s - loss: 0.6591 - acc: 0.6143
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6590 - acc: 0.6168
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6598 - acc: 0.6126
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6605 - acc: 0.6112 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6599 - acc: 0.6142
1728/1760 [============================>.] - ETA: 1s - loss: 0.6599 - acc: 0.6157
1760/1760 [==============================] - 90s 51ms/step - loss: 0.6592 - acc: 0.6170 - val_loss: 0.6898 - val_acc: 0.5408

Epoch 00004: val_acc did not improve from 0.62755
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:22 - loss: 0.7086 - acc: 0.5156
 128/1760 [=>............................] - ETA: 1:26 - loss: 0.6495 - acc: 0.6250
 192/1760 [==>...........................] - ETA: 1:19 - loss: 0.6525 - acc: 0.6250
 256/1760 [===>..........................] - ETA: 1:18 - loss: 0.6440 - acc: 0.6289
 320/1760 [====>.........................] - ETA: 1:13 - loss: 0.6453 - acc: 0.6156
 384/1760 [=====>........................] - ETA: 1:10 - loss: 0.6503 - acc: 0.6094
 448/1760 [======>.......................] - ETA: 1:06 - loss: 0.6520 - acc: 0.6116
 512/1760 [=======>......................] - ETA: 1:03 - loss: 0.6547 - acc: 0.6133
 576/1760 [========>.....................] - ETA: 58s - loss: 0.6500 - acc: 0.6198 
 640/1760 [=========>....................] - ETA: 55s - loss: 0.6439 - acc: 0.6250
 704/1760 [===========>..................] - ETA: 52s - loss: 0.6533 - acc: 0.6165
 768/1760 [============>.................] - ETA: 48s - loss: 0.6575 - acc: 0.6146
 832/1760 [=============>................] - ETA: 45s - loss: 0.6561 - acc: 0.6238
 896/1760 [==============>...............] - ETA: 42s - loss: 0.6524 - acc: 0.6250
 960/1760 [===============>..............] - ETA: 39s - loss: 0.6489 - acc: 0.6292
1024/1760 [================>.............] - ETA: 36s - loss: 0.6555 - acc: 0.6230
1088/1760 [=================>............] - ETA: 32s - loss: 0.6511 - acc: 0.6268
1152/1760 [==================>...........] - ETA: 29s - loss: 0.6534 - acc: 0.6224
1216/1760 [===================>..........] - ETA: 26s - loss: 0.6542 - acc: 0.6168
1280/1760 [====================>.........] - ETA: 23s - loss: 0.6512 - acc: 0.6211
1344/1760 [=====================>........] - ETA: 20s - loss: 0.6510 - acc: 0.6220
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6523 - acc: 0.6172
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6518 - acc: 0.6182
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6529 - acc: 0.6185
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6518 - acc: 0.6194 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6523 - acc: 0.6190
1728/1760 [============================>.] - ETA: 1s - loss: 0.6509 - acc: 0.6204
1760/1760 [==============================] - 90s 51ms/step - loss: 0.6512 - acc: 0.6170 - val_loss: 0.6829 - val_acc: 0.5714

Epoch 00005: val_acc did not improve from 0.62755
Epoch 6/10

  64/1760 [>.............................] - ETA: 1:14 - loss: 0.5793 - acc: 0.6875
 128/1760 [=>............................] - ETA: 1:16 - loss: 0.5782 - acc: 0.7031
 192/1760 [==>...........................] - ETA: 1:14 - loss: 0.5965 - acc: 0.6719
 256/1760 [===>..........................] - ETA: 1:10 - loss: 0.6217 - acc: 0.6289
 320/1760 [====>.........................] - ETA: 1:06 - loss: 0.6231 - acc: 0.6250
 384/1760 [=====>........................] - ETA: 1:03 - loss: 0.6321 - acc: 0.6120
 448/1760 [======>.......................] - ETA: 1:00 - loss: 0.6302 - acc: 0.6272
 512/1760 [=======>......................] - ETA: 58s - loss: 0.6274 - acc: 0.6367 
 576/1760 [========>.....................] - ETA: 54s - loss: 0.6380 - acc: 0.6267
 640/1760 [=========>....................] - ETA: 52s - loss: 0.6402 - acc: 0.6203
 704/1760 [===========>..................] - ETA: 48s - loss: 0.6368 - acc: 0.6264
 768/1760 [============>.................] - ETA: 46s - loss: 0.6339 - acc: 0.6263
 832/1760 [=============>................] - ETA: 43s - loss: 0.6433 - acc: 0.6178
 896/1760 [==============>...............] - ETA: 40s - loss: 0.6424 - acc: 0.6183
 960/1760 [===============>..............] - ETA: 37s - loss: 0.6416 - acc: 0.6188
1024/1760 [================>.............] - ETA: 34s - loss: 0.6377 - acc: 0.6221
1088/1760 [=================>............] - ETA: 31s - loss: 0.6374 - acc: 0.6213
1152/1760 [==================>...........] - ETA: 28s - loss: 0.6360 - acc: 0.6250
1216/1760 [===================>..........] - ETA: 25s - loss: 0.6417 - acc: 0.6242
1280/1760 [====================>.........] - ETA: 22s - loss: 0.6468 - acc: 0.6180
1344/1760 [=====================>........] - ETA: 19s - loss: 0.6444 - acc: 0.6205
1408/1760 [=======================>......] - ETA: 16s - loss: 0.6464 - acc: 0.6207
1472/1760 [========================>.....] - ETA: 13s - loss: 0.6460 - acc: 0.6223
1536/1760 [=========================>....] - ETA: 10s - loss: 0.6470 - acc: 0.6211
1600/1760 [==========================>...] - ETA: 7s - loss: 0.6484 - acc: 0.6219 
1664/1760 [===========================>..] - ETA: 4s - loss: 0.6497 - acc: 0.6208
1728/1760 [============================>.] - ETA: 1s - loss: 0.6461 - acc: 0.6256
1760/1760 [==============================] - 87s 50ms/step - loss: 0.6467 - acc: 0.6244 - val_loss: 0.6545 - val_acc: 0.5816

Epoch 00006: val_acc did not improve from 0.62755
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:32 - loss: 0.5827 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:24 - loss: 0.5945 - acc: 0.6562
 192/1760 [==>...........................] - ETA: 1:20 - loss: 0.5895 - acc: 0.6667
 256/1760 [===>..........................] - ETA: 1:17 - loss: 0.5894 - acc: 0.6758
 320/1760 [====>.........................] - ETA: 1:14 - loss: 0.5937 - acc: 0.6844
 384/1760 [=====>........................] - ETA: 1:09 - loss: 0.5935 - acc: 0.6875
 448/1760 [======>.......................] - ETA: 1:07 - loss: 0.6039 - acc: 0.6763
 512/1760 [=======>......................] - ETA: 1:04 - loss: 0.6143 - acc: 0.6660
 576/1760 [========>.....................] - ETA: 1:00 - loss: 0.6179 - acc: 0.6632
 640/1760 [=========>....................] - ETA: 56s - loss: 0.6195 - acc: 0.6594 
 704/1760 [===========>..................] - ETA: 53s - loss: 0.6204 - acc: 0.6548
 768/1760 [============>.................] - ETA: 50s - loss: 0.6301 - acc: 0.6419
 832/1760 [=============>................] - ETA: 46s - loss: 0.6282 - acc: 0.6466
 896/1760 [==============>...............] - ETA: 43s - loss: 0.6362 - acc: 0.6395
 960/1760 [===============>..............] - ETA: 40s - loss: 0.6355 - acc: 0.6417
1024/1760 [================>.............] - ETA: 37s - loss: 0.6396 - acc: 0.6357
1088/1760 [=================>............] - ETA: 34s - loss: 0.6405 - acc: 0.6342
1152/1760 [==================>...........] - ETA: 31s - loss: 0.6395 - acc: 0.6380
1216/1760 [===================>..........] - ETA: 28s - loss: 0.6384 - acc: 0.6382
1280/1760 [====================>.........] - ETA: 25s - loss: 0.6389 - acc: 0.6367
1344/1760 [=====================>........] - ETA: 21s - loss: 0.6416 - acc: 0.6332
1408/1760 [=======================>......] - ETA: 18s - loss: 0.6415 - acc: 0.6342
1472/1760 [========================>.....] - ETA: 15s - loss: 0.6412 - acc: 0.6338
1536/1760 [=========================>....] - ETA: 11s - loss: 0.6420 - acc: 0.6328
1600/1760 [==========================>...] - ETA: 8s - loss: 0.6434 - acc: 0.6306 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6459 - acc: 0.6274
1728/1760 [============================>.] - ETA: 1s - loss: 0.6449 - acc: 0.6291
1760/1760 [==============================] - 100s 57ms/step - loss: 0.6448 - acc: 0.6278 - val_loss: 0.7170 - val_acc: 0.5510

Epoch 00007: val_acc did not improve from 0.62755
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:34 - loss: 0.5558 - acc: 0.7812
 128/1760 [=>............................] - ETA: 1:28 - loss: 0.6040 - acc: 0.7031
 192/1760 [==>...........................] - ETA: 1:26 - loss: 0.6253 - acc: 0.6771
 256/1760 [===>..........................] - ETA: 1:24 - loss: 0.6292 - acc: 0.6602
 320/1760 [====>.........................] - ETA: 1:20 - loss: 0.6438 - acc: 0.6344
 384/1760 [=====>........................] - ETA: 1:18 - loss: 0.6522 - acc: 0.6172
 448/1760 [======>.......................] - ETA: 1:14 - loss: 0.6523 - acc: 0.6161
 512/1760 [=======>......................] - ETA: 1:11 - loss: 0.6432 - acc: 0.6289
 576/1760 [========>.....................] - ETA: 1:06 - loss: 0.6453 - acc: 0.6319
 640/1760 [=========>....................] - ETA: 1:04 - loss: 0.6506 - acc: 0.6219
 704/1760 [===========>..................] - ETA: 59s - loss: 0.6460 - acc: 0.6278 
 768/1760 [============>.................] - ETA: 56s - loss: 0.6422 - acc: 0.6328
 832/1760 [=============>................] - ETA: 52s - loss: 0.6369 - acc: 0.6394
 896/1760 [==============>...............] - ETA: 49s - loss: 0.6410 - acc: 0.6373
 960/1760 [===============>..............] - ETA: 45s - loss: 0.6402 - acc: 0.6396
1024/1760 [================>.............] - ETA: 42s - loss: 0.6388 - acc: 0.6406
1088/1760 [=================>............] - ETA: 38s - loss: 0.6381 - acc: 0.6379
1152/1760 [==================>...........] - ETA: 34s - loss: 0.6368 - acc: 0.6406
1216/1760 [===================>..........] - ETA: 31s - loss: 0.6383 - acc: 0.6390
1280/1760 [====================>.........] - ETA: 27s - loss: 0.6378 - acc: 0.6383
1344/1760 [=====================>........] - ETA: 23s - loss: 0.6384 - acc: 0.6406
1408/1760 [=======================>......] - ETA: 20s - loss: 0.6370 - acc: 0.6406
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6352 - acc: 0.6420
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6358 - acc: 0.6393
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6352 - acc: 0.6419 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6337 - acc: 0.6400
1728/1760 [============================>.] - ETA: 1s - loss: 0.6396 - acc: 0.6360
1760/1760 [==============================] - 105s 60ms/step - loss: 0.6405 - acc: 0.6358 - val_loss: 0.6822 - val_acc: 0.5459

Epoch 00008: val_acc did not improve from 0.62755
Epoch 9/10

  64/1760 [>.............................] - ETA: 1:36 - loss: 0.6662 - acc: 0.5469
 128/1760 [=>............................] - ETA: 1:25 - loss: 0.6556 - acc: 0.5938
 192/1760 [==>...........................] - ETA: 1:26 - loss: 0.6500 - acc: 0.6094
 256/1760 [===>..........................] - ETA: 1:24 - loss: 0.6429 - acc: 0.6289
 320/1760 [====>.........................] - ETA: 1:22 - loss: 0.6489 - acc: 0.6344
 384/1760 [=====>........................] - ETA: 1:19 - loss: 0.6345 - acc: 0.6510
 448/1760 [======>.......................] - ETA: 1:14 - loss: 0.6373 - acc: 0.6384
 512/1760 [=======>......................] - ETA: 1:10 - loss: 0.6372 - acc: 0.6465
 576/1760 [========>.....................] - ETA: 1:07 - loss: 0.6337 - acc: 0.6528
 640/1760 [=========>....................] - ETA: 1:03 - loss: 0.6338 - acc: 0.6547
 704/1760 [===========>..................] - ETA: 1:00 - loss: 0.6370 - acc: 0.6449
 768/1760 [============>.................] - ETA: 57s - loss: 0.6390 - acc: 0.6367 
 832/1760 [=============>................] - ETA: 53s - loss: 0.6396 - acc: 0.6346
 896/1760 [==============>...............] - ETA: 49s - loss: 0.6423 - acc: 0.6317
 960/1760 [===============>..............] - ETA: 45s - loss: 0.6361 - acc: 0.6406
1024/1760 [================>.............] - ETA: 41s - loss: 0.6345 - acc: 0.6436
1088/1760 [=================>............] - ETA: 38s - loss: 0.6332 - acc: 0.6443
1152/1760 [==================>...........] - ETA: 34s - loss: 0.6361 - acc: 0.6398
1216/1760 [===================>..........] - ETA: 30s - loss: 0.6339 - acc: 0.6414
1280/1760 [====================>.........] - ETA: 27s - loss: 0.6382 - acc: 0.6359
1344/1760 [=====================>........] - ETA: 23s - loss: 0.6390 - acc: 0.6339
1408/1760 [=======================>......] - ETA: 19s - loss: 0.6366 - acc: 0.6364
1472/1760 [========================>.....] - ETA: 16s - loss: 0.6368 - acc: 0.6352
1536/1760 [=========================>....] - ETA: 12s - loss: 0.6349 - acc: 0.6354
1600/1760 [==========================>...] - ETA: 9s - loss: 0.6364 - acc: 0.6344 
1664/1760 [===========================>..] - ETA: 5s - loss: 0.6363 - acc: 0.6340
1728/1760 [============================>.] - ETA: 1s - loss: 0.6364 - acc: 0.6314
1760/1760 [==============================] - 106s 60ms/step - loss: 0.6364 - acc: 0.6324 - val_loss: 0.6243 - val_acc: 0.6939

Epoch 00009: val_acc improved from 0.62755 to 0.69388, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window20/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:50 - loss: 0.5767 - acc: 0.7031
 128/1760 [=>............................] - ETA: 1:46 - loss: 0.6126 - acc: 0.6719
 192/1760 [==>...........................] - ETA: 1:42 - loss: 0.6165 - acc: 0.6615
 256/1760 [===>..........................] - ETA: 1:36 - loss: 0.6318 - acc: 0.6523
 320/1760 [====>.........................] - ETA: 1:34 - loss: 0.6328 - acc: 0.6406
 384/1760 [=====>........................] - ETA: 1:30 - loss: 0.6362 - acc: 0.6380
 448/1760 [======>.......................] - ETA: 1:26 - loss: 0.6287 - acc: 0.6406
 512/1760 [=======>......................] - ETA: 1:23 - loss: 0.6224 - acc: 0.6445
 576/1760 [========>.....................] - ETA: 1:19 - loss: 0.6199 - acc: 0.6510
 640/1760 [=========>....................] - ETA: 1:14 - loss: 0.6156 - acc: 0.6578
 704/1760 [===========>..................] - ETA: 1:11 - loss: 0.6116 - acc: 0.6690
 768/1760 [============>.................] - ETA: 1:07 - loss: 0.6139 - acc: 0.6680
 832/1760 [=============>................] - ETA: 1:02 - loss: 0.6190 - acc: 0.6635
 896/1760 [==============>...............] - ETA: 57s - loss: 0.6171 - acc: 0.6674 
 960/1760 [===============>..............] - ETA: 53s - loss: 0.6177 - acc: 0.6677
1024/1760 [================>.............] - ETA: 49s - loss: 0.6188 - acc: 0.6660
1088/1760 [=================>............] - ETA: 45s - loss: 0.6219 - acc: 0.6636
1152/1760 [==================>...........] - ETA: 41s - loss: 0.6172 - acc: 0.6693
1216/1760 [===================>..........] - ETA: 36s - loss: 0.6246 - acc: 0.6612
1280/1760 [====================>.........] - ETA: 32s - loss: 0.6289 - acc: 0.6531
1344/1760 [=====================>........] - ETA: 27s - loss: 0.6283 - acc: 0.6503
1408/1760 [=======================>......] - ETA: 23s - loss: 0.6303 - acc: 0.6470
1472/1760 [========================>.....] - ETA: 19s - loss: 0.6291 - acc: 0.6467
1536/1760 [=========================>....] - ETA: 15s - loss: 0.6317 - acc: 0.6439
1600/1760 [==========================>...] - ETA: 10s - loss: 0.6293 - acc: 0.6444
1664/1760 [===========================>..] - ETA: 6s - loss: 0.6305 - acc: 0.6454 
1728/1760 [============================>.] - ETA: 2s - loss: 0.6309 - acc: 0.6429
1760/1760 [==============================] - 124s 70ms/step - loss: 0.6330 - acc: 0.6420 - val_loss: 0.6544 - val_acc: 0.6173

Epoch 00010: val_acc did not improve from 0.69388
样本个数 200
样本个数 400
window_select16-21.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 28s
128/400 [========>.....................] - ETA: 13s
192/400 [=============>................] - ETA: 8s 
256/400 [==================>...........] - ETA: 4s
320/400 [=======================>......] - ETA: 2s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 12s 29ms/step
loss: 0.6650570774078369
acc: 0.595
样本个数 978
样本个数 1956
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f48b44369d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f48b44369d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f48b4435e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f48b4435e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c74308190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4c74308190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c743085d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4c743085d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c74310050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4c74310050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48984c8390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48984c8390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c74308810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4c74308810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4db46e2310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4db46e2310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f48743dc890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f48743dc890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f48b425d3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f48b425d3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48b415eb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48b415eb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f48b4332ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f48b4332ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4a5874f750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4a5874f750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49f468bc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f49f468bc90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4898602190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f4898602190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48986da390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48986da390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4ad40be310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4ad40be310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4874314e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4874314e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f48986bce50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f48986bce50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f48741e9ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f48741e9ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f487428b3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f487428b3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f48985fe810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f48985fe810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4874247590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4874247590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f486c727d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f486c727d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f486c6c1050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f486c6c1050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48743d6b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48743d6b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f486c727350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f486c727350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f486c6d9190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f486c6d9190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f486c402090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f486c402090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f486c2d55d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f486c2d55d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f486c5b9190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f486c5b9190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f486c4021d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f486c4021d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f486c5b99d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f486c5b99d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f486c105650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f486c105650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f486c076fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f486c076fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f486c0cfed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f486c0cfed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f486c1f0810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f486c1f0810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f486c0e9890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f486c0e9890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4834642d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f4834642d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f48344e7e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f48344e7e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f483455b950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f483455b950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f486c076f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f486c076f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48347be810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48347be810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f483443b850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f483443b850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f48342a28d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f48342a28d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4db4691c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4db4691c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f48342cb350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f48342cb350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48147e33d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48147e33d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f48347b4250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f48347b4250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f481464ca10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f481464ca10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48146e61d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48146e61d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f48146dd250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f48146dd250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4814447950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f4814447950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f48145c9910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f48145c9910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f48143755d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f48143755d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f481465b810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f481465b810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f481447f650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f481447f650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48142e9ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48142e9ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f48142d1b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f48142d1b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f47f4750990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f47f4750990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48142d5710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f48142d5710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4814420b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f4814420b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f47f45bc050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f47f45bc050>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 1760 samples, validate on 196 samples
Epoch 1/10

  64/1760 [>.............................] - ETA: 5:39 - loss: 0.8032 - acc: 0.5000
 128/1760 [=>............................] - ETA: 3:11 - loss: 0.8316 - acc: 0.4609
 192/1760 [==>...........................] - ETA: 2:20 - loss: 0.8061 - acc: 0.4688
 256/1760 [===>..........................] - ETA: 1:53 - loss: 0.7870 - acc: 0.4727
 320/1760 [====>.........................] - ETA: 1:37 - loss: 0.7733 - acc: 0.4844
 384/1760 [=====>........................] - ETA: 1:24 - loss: 0.7617 - acc: 0.4948
 448/1760 [======>.......................] - ETA: 1:16 - loss: 0.7554 - acc: 0.5067
 512/1760 [=======>......................] - ETA: 1:08 - loss: 0.7610 - acc: 0.5078
 576/1760 [========>.....................] - ETA: 1:03 - loss: 0.7567 - acc: 0.5156
 640/1760 [=========>....................] - ETA: 58s - loss: 0.7523 - acc: 0.5141 
 704/1760 [===========>..................] - ETA: 53s - loss: 0.7432 - acc: 0.5227
 768/1760 [============>.................] - ETA: 49s - loss: 0.7434 - acc: 0.5234
 832/1760 [=============>................] - ETA: 45s - loss: 0.7528 - acc: 0.5168
 896/1760 [==============>...............] - ETA: 41s - loss: 0.7613 - acc: 0.5100
 960/1760 [===============>..............] - ETA: 37s - loss: 0.7581 - acc: 0.5198
1024/1760 [================>.............] - ETA: 34s - loss: 0.7530 - acc: 0.5254
1088/1760 [=================>............] - ETA: 30s - loss: 0.7505 - acc: 0.5239
1152/1760 [==================>...........] - ETA: 27s - loss: 0.7484 - acc: 0.5295
1216/1760 [===================>..........] - ETA: 24s - loss: 0.7429 - acc: 0.5337
1280/1760 [====================>.........] - ETA: 21s - loss: 0.7392 - acc: 0.5375
1344/1760 [=====================>........] - ETA: 18s - loss: 0.7391 - acc: 0.5365
1408/1760 [=======================>......] - ETA: 15s - loss: 0.7366 - acc: 0.5376
1472/1760 [========================>.....] - ETA: 12s - loss: 0.7339 - acc: 0.5380
1536/1760 [=========================>....] - ETA: 9s - loss: 0.7377 - acc: 0.5312 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.7388 - acc: 0.5288
1664/1760 [===========================>..] - ETA: 4s - loss: 0.7356 - acc: 0.5331
1728/1760 [============================>.] - ETA: 1s - loss: 0.7331 - acc: 0.5353
1760/1760 [==============================] - 80s 45ms/step - loss: 0.7331 - acc: 0.5347 - val_loss: 0.6910 - val_acc: 0.5357

Epoch 00001: val_acc improved from -inf to 0.53571, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window21/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 2/10

  64/1760 [>.............................] - ETA: 1:09 - loss: 0.6881 - acc: 0.5625
 128/1760 [=>............................] - ETA: 1:02 - loss: 0.7116 - acc: 0.5156
 192/1760 [==>...........................] - ETA: 59s - loss: 0.7003 - acc: 0.5417 
 256/1760 [===>..........................] - ETA: 56s - loss: 0.6994 - acc: 0.5391
 320/1760 [====>.........................] - ETA: 53s - loss: 0.6978 - acc: 0.5406
 384/1760 [=====>........................] - ETA: 51s - loss: 0.6990 - acc: 0.5312
 448/1760 [======>.......................] - ETA: 49s - loss: 0.6931 - acc: 0.5357
 512/1760 [=======>......................] - ETA: 47s - loss: 0.6926 - acc: 0.5371
 576/1760 [========>.....................] - ETA: 44s - loss: 0.6924 - acc: 0.5312
 640/1760 [=========>....................] - ETA: 42s - loss: 0.6949 - acc: 0.5281
 704/1760 [===========>..................] - ETA: 40s - loss: 0.6946 - acc: 0.5256
 768/1760 [============>.................] - ETA: 37s - loss: 0.6940 - acc: 0.5273
 832/1760 [=============>................] - ETA: 35s - loss: 0.6935 - acc: 0.5312
 896/1760 [==============>...............] - ETA: 32s - loss: 0.6917 - acc: 0.5368
 960/1760 [===============>..............] - ETA: 30s - loss: 0.6905 - acc: 0.5406
1024/1760 [================>.............] - ETA: 27s - loss: 0.6920 - acc: 0.5371
1088/1760 [=================>............] - ETA: 25s - loss: 0.6931 - acc: 0.5377
1152/1760 [==================>...........] - ETA: 23s - loss: 0.6945 - acc: 0.5356
1216/1760 [===================>..........] - ETA: 20s - loss: 0.6936 - acc: 0.5378
1280/1760 [====================>.........] - ETA: 18s - loss: 0.6924 - acc: 0.5391
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6956 - acc: 0.5350
1408/1760 [=======================>......] - ETA: 13s - loss: 0.6957 - acc: 0.5334
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6974 - acc: 0.5312
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6981 - acc: 0.5299 
1600/1760 [==========================>...] - ETA: 6s - loss: 0.6965 - acc: 0.5331
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6949 - acc: 0.5361
1728/1760 [============================>.] - ETA: 1s - loss: 0.6956 - acc: 0.5336
1760/1760 [==============================] - 70s 40ms/step - loss: 0.6960 - acc: 0.5324 - val_loss: 0.6700 - val_acc: 0.5918

Epoch 00002: val_acc improved from 0.53571 to 0.59184, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window21/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 3/10

  64/1760 [>.............................] - ETA: 1:01 - loss: 0.7397 - acc: 0.4531
 128/1760 [=>............................] - ETA: 58s - loss: 0.7061 - acc: 0.5625 
 192/1760 [==>...........................] - ETA: 56s - loss: 0.7168 - acc: 0.5312
 256/1760 [===>..........................] - ETA: 55s - loss: 0.7061 - acc: 0.5430
 320/1760 [====>.........................] - ETA: 53s - loss: 0.7071 - acc: 0.5375
 384/1760 [=====>........................] - ETA: 50s - loss: 0.7096 - acc: 0.5391
 448/1760 [======>.......................] - ETA: 48s - loss: 0.7111 - acc: 0.5402
 512/1760 [=======>......................] - ETA: 45s - loss: 0.7106 - acc: 0.5391
 576/1760 [========>.....................] - ETA: 43s - loss: 0.7194 - acc: 0.5295
 640/1760 [=========>....................] - ETA: 41s - loss: 0.7161 - acc: 0.5359
 704/1760 [===========>..................] - ETA: 39s - loss: 0.7091 - acc: 0.5483
 768/1760 [============>.................] - ETA: 36s - loss: 0.7119 - acc: 0.5417
 832/1760 [=============>................] - ETA: 34s - loss: 0.7119 - acc: 0.5373
 896/1760 [==============>...............] - ETA: 31s - loss: 0.7109 - acc: 0.5413
 960/1760 [===============>..............] - ETA: 29s - loss: 0.7105 - acc: 0.5417
1024/1760 [================>.............] - ETA: 27s - loss: 0.7083 - acc: 0.5410
1088/1760 [=================>............] - ETA: 24s - loss: 0.7052 - acc: 0.5423
1152/1760 [==================>...........] - ETA: 22s - loss: 0.7007 - acc: 0.5495
1216/1760 [===================>..........] - ETA: 20s - loss: 0.7001 - acc: 0.5493
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6978 - acc: 0.5508
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6977 - acc: 0.5499
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6983 - acc: 0.5476
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6977 - acc: 0.5489
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6984 - acc: 0.5475 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6986 - acc: 0.5481
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6973 - acc: 0.5487
1728/1760 [============================>.] - ETA: 1s - loss: 0.6978 - acc: 0.5503
1760/1760 [==============================] - 68s 39ms/step - loss: 0.6964 - acc: 0.5534 - val_loss: 0.6640 - val_acc: 0.6327

Epoch 00003: val_acc improved from 0.59184 to 0.63265, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window21/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 4/10

  64/1760 [>.............................] - ETA: 57s - loss: 0.6758 - acc: 0.6719
 128/1760 [=>............................] - ETA: 56s - loss: 0.6656 - acc: 0.6562
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6682 - acc: 0.6250
 256/1760 [===>..........................] - ETA: 54s - loss: 0.6704 - acc: 0.5938
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6629 - acc: 0.6031
 384/1760 [=====>........................] - ETA: 50s - loss: 0.6721 - acc: 0.5964
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6683 - acc: 0.5960
 512/1760 [=======>......................] - ETA: 46s - loss: 0.6706 - acc: 0.5918
 576/1760 [========>.....................] - ETA: 43s - loss: 0.6784 - acc: 0.5781
 640/1760 [=========>....................] - ETA: 41s - loss: 0.6801 - acc: 0.5719
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6777 - acc: 0.5696
 768/1760 [============>.................] - ETA: 36s - loss: 0.6803 - acc: 0.5703
 832/1760 [=============>................] - ETA: 33s - loss: 0.6813 - acc: 0.5649
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6818 - acc: 0.5658
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6821 - acc: 0.5656
1024/1760 [================>.............] - ETA: 26s - loss: 0.6857 - acc: 0.5586
1088/1760 [=================>............] - ETA: 24s - loss: 0.6865 - acc: 0.5588
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6818 - acc: 0.5668
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6778 - acc: 0.5732
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6760 - acc: 0.5773
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6748 - acc: 0.5811
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6726 - acc: 0.5817
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6704 - acc: 0.5856
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6688 - acc: 0.5905 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6683 - acc: 0.5913
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6691 - acc: 0.5913
1728/1760 [============================>.] - ETA: 1s - loss: 0.6701 - acc: 0.5897
1760/1760 [==============================] - 69s 39ms/step - loss: 0.6697 - acc: 0.5909 - val_loss: 0.6773 - val_acc: 0.6224

Epoch 00004: val_acc did not improve from 0.63265
Epoch 5/10

  64/1760 [>.............................] - ETA: 1:02 - loss: 0.6316 - acc: 0.6250
 128/1760 [=>............................] - ETA: 1:01 - loss: 0.6539 - acc: 0.6172
 192/1760 [==>...........................] - ETA: 59s - loss: 0.6531 - acc: 0.6302 
 256/1760 [===>..........................] - ETA: 56s - loss: 0.6532 - acc: 0.6328
 320/1760 [====>.........................] - ETA: 53s - loss: 0.6664 - acc: 0.6219
 384/1760 [=====>........................] - ETA: 51s - loss: 0.6795 - acc: 0.6016
 448/1760 [======>.......................] - ETA: 49s - loss: 0.6799 - acc: 0.6027
 512/1760 [=======>......................] - ETA: 46s - loss: 0.6723 - acc: 0.6035
 576/1760 [========>.....................] - ETA: 44s - loss: 0.6658 - acc: 0.6007
 640/1760 [=========>....................] - ETA: 42s - loss: 0.6626 - acc: 0.6047
 704/1760 [===========>..................] - ETA: 39s - loss: 0.6567 - acc: 0.6108
 768/1760 [============>.................] - ETA: 36s - loss: 0.6591 - acc: 0.6094
 832/1760 [=============>................] - ETA: 34s - loss: 0.6596 - acc: 0.6106
 896/1760 [==============>...............] - ETA: 32s - loss: 0.6571 - acc: 0.6127
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6599 - acc: 0.6062
1024/1760 [================>.............] - ETA: 27s - loss: 0.6651 - acc: 0.6016
1088/1760 [=================>............] - ETA: 25s - loss: 0.6659 - acc: 0.6039
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6726 - acc: 0.5972
1216/1760 [===================>..........] - ETA: 20s - loss: 0.6729 - acc: 0.5970
1280/1760 [====================>.........] - ETA: 18s - loss: 0.6740 - acc: 0.5953
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6733 - acc: 0.5960
1408/1760 [=======================>......] - ETA: 13s - loss: 0.6729 - acc: 0.5952
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6713 - acc: 0.5978
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6736 - acc: 0.5957 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6748 - acc: 0.5956
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6721 - acc: 0.5956
1728/1760 [============================>.] - ETA: 1s - loss: 0.6736 - acc: 0.5926
1760/1760 [==============================] - 69s 39ms/step - loss: 0.6742 - acc: 0.5926 - val_loss: 0.6623 - val_acc: 0.5612

Epoch 00005: val_acc did not improve from 0.63265
Epoch 6/10

  64/1760 [>.............................] - ETA: 59s - loss: 0.6216 - acc: 0.6719
 128/1760 [=>............................] - ETA: 57s - loss: 0.6419 - acc: 0.6328
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6451 - acc: 0.6406
 256/1760 [===>..........................] - ETA: 54s - loss: 0.6600 - acc: 0.6172
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6504 - acc: 0.6344
 384/1760 [=====>........................] - ETA: 50s - loss: 0.6614 - acc: 0.6172
 448/1760 [======>.......................] - ETA: 47s - loss: 0.6603 - acc: 0.6228
 512/1760 [=======>......................] - ETA: 45s - loss: 0.6544 - acc: 0.6387
 576/1760 [========>.....................] - ETA: 43s - loss: 0.6536 - acc: 0.6441
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6555 - acc: 0.6391
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6684 - acc: 0.6250
 768/1760 [============>.................] - ETA: 35s - loss: 0.6650 - acc: 0.6289
 832/1760 [=============>................] - ETA: 33s - loss: 0.6616 - acc: 0.6310
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6612 - acc: 0.6317
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6620 - acc: 0.6281
1024/1760 [================>.............] - ETA: 26s - loss: 0.6636 - acc: 0.6230
1088/1760 [=================>............] - ETA: 24s - loss: 0.6680 - acc: 0.6176
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6669 - acc: 0.6181
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6669 - acc: 0.6160
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6673 - acc: 0.6133
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6652 - acc: 0.6131
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6675 - acc: 0.6094
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6658 - acc: 0.6101
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6682 - acc: 0.6061 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6678 - acc: 0.6062
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6692 - acc: 0.6052
1728/1760 [============================>.] - ETA: 1s - loss: 0.6704 - acc: 0.6042
1760/1760 [==============================] - 68s 38ms/step - loss: 0.6694 - acc: 0.6051 - val_loss: 0.6564 - val_acc: 0.5816

Epoch 00006: val_acc did not improve from 0.63265
Epoch 7/10

  64/1760 [>.............................] - ETA: 1:00 - loss: 0.7044 - acc: 0.5625
 128/1760 [=>............................] - ETA: 59s - loss: 0.6871 - acc: 0.5938 
 192/1760 [==>...........................] - ETA: 57s - loss: 0.6924 - acc: 0.5938
 256/1760 [===>..........................] - ETA: 55s - loss: 0.6763 - acc: 0.6055
 320/1760 [====>.........................] - ETA: 52s - loss: 0.6738 - acc: 0.6156
 384/1760 [=====>........................] - ETA: 50s - loss: 0.6665 - acc: 0.6094
 448/1760 [======>.......................] - ETA: 48s - loss: 0.6647 - acc: 0.6161
 512/1760 [=======>......................] - ETA: 45s - loss: 0.6564 - acc: 0.6230
 576/1760 [========>.....................] - ETA: 43s - loss: 0.6547 - acc: 0.6250
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6516 - acc: 0.6266
 704/1760 [===========>..................] - ETA: 38s - loss: 0.6461 - acc: 0.6293
 768/1760 [============>.................] - ETA: 36s - loss: 0.6405 - acc: 0.6354
 832/1760 [=============>................] - ETA: 33s - loss: 0.6418 - acc: 0.6370
 896/1760 [==============>...............] - ETA: 31s - loss: 0.6433 - acc: 0.6417
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6440 - acc: 0.6427
1024/1760 [================>.............] - ETA: 27s - loss: 0.6421 - acc: 0.6436
1088/1760 [=================>............] - ETA: 24s - loss: 0.6414 - acc: 0.6461
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6456 - acc: 0.6406
1216/1760 [===================>..........] - ETA: 20s - loss: 0.6433 - acc: 0.6414
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6416 - acc: 0.6445
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6414 - acc: 0.6443
1408/1760 [=======================>......] - ETA: 13s - loss: 0.6443 - acc: 0.6413
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6466 - acc: 0.6386
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6447 - acc: 0.6432 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6457 - acc: 0.6425
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6438 - acc: 0.6430
1728/1760 [============================>.] - ETA: 1s - loss: 0.6429 - acc: 0.6435
1760/1760 [==============================] - 69s 39ms/step - loss: 0.6432 - acc: 0.6443 - val_loss: 0.7017 - val_acc: 0.5816

Epoch 00007: val_acc did not improve from 0.63265
Epoch 8/10

  64/1760 [>.............................] - ETA: 1:00 - loss: 0.7290 - acc: 0.5625
 128/1760 [=>............................] - ETA: 58s - loss: 0.7203 - acc: 0.5469 
 192/1760 [==>...........................] - ETA: 58s - loss: 0.6763 - acc: 0.6042
 256/1760 [===>..........................] - ETA: 56s - loss: 0.6668 - acc: 0.6289
 320/1760 [====>.........................] - ETA: 53s - loss: 0.6489 - acc: 0.6469
 384/1760 [=====>........................] - ETA: 51s - loss: 0.6531 - acc: 0.6406
 448/1760 [======>.......................] - ETA: 49s - loss: 0.6490 - acc: 0.6362
 512/1760 [=======>......................] - ETA: 46s - loss: 0.6472 - acc: 0.6387
 576/1760 [========>.....................] - ETA: 44s - loss: 0.6475 - acc: 0.6406
 640/1760 [=========>....................] - ETA: 41s - loss: 0.6626 - acc: 0.6297
 704/1760 [===========>..................] - ETA: 39s - loss: 0.6673 - acc: 0.6222
 768/1760 [============>.................] - ETA: 36s - loss: 0.6670 - acc: 0.6198
 832/1760 [=============>................] - ETA: 34s - loss: 0.6628 - acc: 0.6250
 896/1760 [==============>...............] - ETA: 32s - loss: 0.6686 - acc: 0.6183
 960/1760 [===============>..............] - ETA: 29s - loss: 0.6663 - acc: 0.6198
1024/1760 [================>.............] - ETA: 27s - loss: 0.6658 - acc: 0.6230
1088/1760 [=================>............] - ETA: 24s - loss: 0.6618 - acc: 0.6268
1152/1760 [==================>...........] - ETA: 22s - loss: 0.6640 - acc: 0.6224
1216/1760 [===================>..........] - ETA: 20s - loss: 0.6635 - acc: 0.6209
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6614 - acc: 0.6203
1344/1760 [=====================>........] - ETA: 15s - loss: 0.6632 - acc: 0.6176
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6639 - acc: 0.6179
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6665 - acc: 0.6148
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6651 - acc: 0.6146 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6650 - acc: 0.6138
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6661 - acc: 0.6106
1728/1760 [============================>.] - ETA: 1s - loss: 0.6660 - acc: 0.6105
1760/1760 [==============================] - 68s 39ms/step - loss: 0.6641 - acc: 0.6119 - val_loss: 0.6404 - val_acc: 0.6480

Epoch 00008: val_acc improved from 0.63265 to 0.64796, saving model to /data/lyli/Lung-adenocarcinoma/LUAD-model/Seq_feature_exact/window21/checkpoints/final_seq_model/LUAD_seq_model_2_mer.hdf5
Epoch 9/10

  64/1760 [>.............................] - ETA: 57s - loss: 0.6763 - acc: 0.5938
 128/1760 [=>............................] - ETA: 56s - loss: 0.6389 - acc: 0.6484
 192/1760 [==>...........................] - ETA: 55s - loss: 0.6262 - acc: 0.6667
 256/1760 [===>..........................] - ETA: 52s - loss: 0.6490 - acc: 0.6328
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6519 - acc: 0.6219
 384/1760 [=====>........................] - ETA: 48s - loss: 0.6422 - acc: 0.6328
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6520 - acc: 0.6183
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6476 - acc: 0.6172
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6447 - acc: 0.6163
 640/1760 [=========>....................] - ETA: 40s - loss: 0.6493 - acc: 0.6125
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6418 - acc: 0.6264
 768/1760 [============>.................] - ETA: 35s - loss: 0.6425 - acc: 0.6250
 832/1760 [=============>................] - ETA: 33s - loss: 0.6462 - acc: 0.6226
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6432 - acc: 0.6283
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6454 - acc: 0.6271
1024/1760 [================>.............] - ETA: 26s - loss: 0.6508 - acc: 0.6201
1088/1760 [=================>............] - ETA: 24s - loss: 0.6535 - acc: 0.6195
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6520 - acc: 0.6207
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6504 - acc: 0.6209
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6516 - acc: 0.6203
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6511 - acc: 0.6198
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6565 - acc: 0.6129
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6574 - acc: 0.6121
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6562 - acc: 0.6107 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6533 - acc: 0.6162
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6542 - acc: 0.6178
1728/1760 [============================>.] - ETA: 1s - loss: 0.6551 - acc: 0.6163
1760/1760 [==============================] - 67s 38ms/step - loss: 0.6574 - acc: 0.6148 - val_loss: 0.6619 - val_acc: 0.5867

Epoch 00009: val_acc did not improve from 0.64796
Epoch 10/10

  64/1760 [>.............................] - ETA: 1:02 - loss: 0.5679 - acc: 0.6719
 128/1760 [=>............................] - ETA: 59s - loss: 0.5904 - acc: 0.6875 
 192/1760 [==>...........................] - ETA: 56s - loss: 0.6318 - acc: 0.6354
 256/1760 [===>..........................] - ETA: 54s - loss: 0.6287 - acc: 0.6445
 320/1760 [====>.........................] - ETA: 51s - loss: 0.6363 - acc: 0.6344
 384/1760 [=====>........................] - ETA: 49s - loss: 0.6403 - acc: 0.6328
 448/1760 [======>.......................] - ETA: 46s - loss: 0.6421 - acc: 0.6272
 512/1760 [=======>......................] - ETA: 44s - loss: 0.6460 - acc: 0.6230
 576/1760 [========>.....................] - ETA: 42s - loss: 0.6493 - acc: 0.6250
 640/1760 [=========>....................] - ETA: 39s - loss: 0.6494 - acc: 0.6219
 704/1760 [===========>..................] - ETA: 37s - loss: 0.6395 - acc: 0.6378
 768/1760 [============>.................] - ETA: 35s - loss: 0.6411 - acc: 0.6328
 832/1760 [=============>................] - ETA: 32s - loss: 0.6390 - acc: 0.6382
 896/1760 [==============>...............] - ETA: 30s - loss: 0.6396 - acc: 0.6395
 960/1760 [===============>..............] - ETA: 28s - loss: 0.6433 - acc: 0.6302
1024/1760 [================>.............] - ETA: 26s - loss: 0.6443 - acc: 0.6289
1088/1760 [=================>............] - ETA: 23s - loss: 0.6457 - acc: 0.6268
1152/1760 [==================>...........] - ETA: 21s - loss: 0.6448 - acc: 0.6293
1216/1760 [===================>..........] - ETA: 19s - loss: 0.6477 - acc: 0.6258
1280/1760 [====================>.........] - ETA: 17s - loss: 0.6488 - acc: 0.6234
1344/1760 [=====================>........] - ETA: 14s - loss: 0.6477 - acc: 0.6235
1408/1760 [=======================>......] - ETA: 12s - loss: 0.6490 - acc: 0.6243
1472/1760 [========================>.....] - ETA: 10s - loss: 0.6505 - acc: 0.6223
1536/1760 [=========================>....] - ETA: 8s - loss: 0.6501 - acc: 0.6237 
1600/1760 [==========================>...] - ETA: 5s - loss: 0.6488 - acc: 0.6250
1664/1760 [===========================>..] - ETA: 3s - loss: 0.6486 - acc: 0.6226
1728/1760 [============================>.] - ETA: 1s - loss: 0.6496 - acc: 0.6209
1760/1760 [==============================] - 66s 38ms/step - loss: 0.6505 - acc: 0.6205 - val_loss: 0.7064 - val_acc: 0.5867

Epoch 00010: val_acc did not improve from 0.64796
样本个数 200
样本个数 400
window_select16-21.py:146: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

 64/400 [===>..........................] - ETA: 44s
128/400 [========>.....................] - ETA: 20s
192/400 [=============>................] - ETA: 11s
256/400 [==================>...........] - ETA: 6s 
320/400 [=======================>......] - ETA: 3s
384/400 [===========================>..] - ETA: 0s
400/400 [==============================] - 14s 35ms/step
loss: 0.6690715742111206
acc: 0.5925
