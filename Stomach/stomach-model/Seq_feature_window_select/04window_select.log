nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 3154
样本个数 6308
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe74398d310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe74398d310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe75aecbf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe75aecbf90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe76b42ed50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe76b42ed50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75af6b290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75af6b290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74391d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74391d710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75af6b4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75af6b4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75af6b2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75af6b2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75af71450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75af71450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe743706790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe743706790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74369bcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74369bcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743739050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743739050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe743706ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe743706ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75afe4610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75afe4610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74391b510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74391b510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe743422950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe743422950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743706e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743706e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe74372ff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe74372ff10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74332f0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74332f0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7430cd890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7430cd890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe743125050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe743125050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74309abd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74309abd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7432d6350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7432d6350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743053250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743053250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7430d51d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7430d51d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe742d63310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe742d63310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe742cd9090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe742cd9090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe742e0fe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe742e0fe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe742b6bc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe742b6bc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe742a90f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe742a90f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe742aa0d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe742aa0d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe742a55850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe742a55850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe742a90050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe742a90050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe742a04710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe742a04710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7427393d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7427393d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7426a2210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7426a2210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7427265d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7427265d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe742739750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe742739750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe76b581bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe76b581bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74245b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74245b8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7423ba3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7423ba3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74243a090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74243a090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75af71b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75af71b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7424d3710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7424d3710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe742470cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe742470cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7420c3890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7420c3890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7423bfb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7423bfb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe742470b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe742470b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7422593d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7422593d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe742339210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe742339210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe741d86f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe741d86f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe741c00f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe741c00f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe741e2d790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe741e2d790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74391fe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74391fe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe741cf7590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe741cf7590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7419dec10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7419dec10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7434d6210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7434d6210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe741cf7410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe741cf7410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe741a0a290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe741a0a290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7419c4f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7419c4f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7417bb9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7417bb9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe741b18610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe741b18610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7419c4310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7419c4310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74173cfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74173cfd0>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-28 22:30:44.086286: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-28 22:30:44.172955: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-28 22:30:44.258934: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561c68586e00 executing computations on platform Host. Devices:
2022-11-28 22:30:44.259077: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-28 22:30:45.346035: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 21:16 - loss: 0.7177 - acc: 0.5938
 128/5677 [..............................] - ETA: 16:05 - loss: 0.7828 - acc: 0.4922
 192/5677 [>.............................] - ETA: 13:42 - loss: 0.8047 - acc: 0.4531
 256/5677 [>.............................] - ETA: 12:19 - loss: 0.7939 - acc: 0.4727
 320/5677 [>.............................] - ETA: 11:13 - loss: 0.7746 - acc: 0.4938
 384/5677 [=>............................] - ETA: 10:31 - loss: 0.7683 - acc: 0.4870
 448/5677 [=>............................] - ETA: 9:55 - loss: 0.7601 - acc: 0.4933 
 512/5677 [=>............................] - ETA: 9:26 - loss: 0.7602 - acc: 0.4863
 576/5677 [==>...........................] - ETA: 9:03 - loss: 0.7524 - acc: 0.5017
 640/5677 [==>...........................] - ETA: 8:42 - loss: 0.7512 - acc: 0.5016
 704/5677 [==>...........................] - ETA: 8:21 - loss: 0.7412 - acc: 0.5114
 768/5677 [===>..........................] - ETA: 8:07 - loss: 0.7399 - acc: 0.5052
 832/5677 [===>..........................] - ETA: 7:53 - loss: 0.7408 - acc: 0.5036
 896/5677 [===>..........................] - ETA: 7:40 - loss: 0.7398 - acc: 0.5011
 960/5677 [====>.........................] - ETA: 7:30 - loss: 0.7359 - acc: 0.5000
1024/5677 [====>.........................] - ETA: 7:21 - loss: 0.7378 - acc: 0.4990
1088/5677 [====>.........................] - ETA: 7:11 - loss: 0.7411 - acc: 0.4954
1152/5677 [=====>........................] - ETA: 7:06 - loss: 0.7355 - acc: 0.5069
1216/5677 [=====>........................] - ETA: 6:59 - loss: 0.7326 - acc: 0.5090
1280/5677 [=====>........................] - ETA: 6:53 - loss: 0.7311 - acc: 0.5109
1344/5677 [======>.......................] - ETA: 6:45 - loss: 0.7293 - acc: 0.5119
1408/5677 [======>.......................] - ETA: 6:37 - loss: 0.7293 - acc: 0.5135
1472/5677 [======>.......................] - ETA: 6:29 - loss: 0.7286 - acc: 0.5156
1536/5677 [=======>......................] - ETA: 6:21 - loss: 0.7291 - acc: 0.5150
1600/5677 [=======>......................] - ETA: 6:11 - loss: 0.7270 - acc: 0.5181
1664/5677 [=======>......................] - ETA: 6:04 - loss: 0.7274 - acc: 0.5174
1728/5677 [========>.....................] - ETA: 5:59 - loss: 0.7269 - acc: 0.5174
1792/5677 [========>.....................] - ETA: 5:54 - loss: 0.7274 - acc: 0.5184
1856/5677 [========>.....................] - ETA: 5:47 - loss: 0.7283 - acc: 0.5162
1920/5677 [=========>....................] - ETA: 5:41 - loss: 0.7308 - acc: 0.5141
1984/5677 [=========>....................] - ETA: 5:34 - loss: 0.7299 - acc: 0.5126
2048/5677 [=========>....................] - ETA: 5:27 - loss: 0.7295 - acc: 0.5103
2112/5677 [==========>...................] - ETA: 5:22 - loss: 0.7284 - acc: 0.5118
2176/5677 [==========>...................] - ETA: 5:15 - loss: 0.7291 - acc: 0.5106
2240/5677 [==========>...................] - ETA: 5:11 - loss: 0.7288 - acc: 0.5080
2304/5677 [===========>..................] - ETA: 5:03 - loss: 0.7282 - acc: 0.5087
2368/5677 [===========>..................] - ETA: 4:56 - loss: 0.7276 - acc: 0.5097
2432/5677 [===========>..................] - ETA: 4:51 - loss: 0.7278 - acc: 0.5082
2496/5677 [============>.................] - ETA: 4:47 - loss: 0.7272 - acc: 0.5100
2560/5677 [============>.................] - ETA: 4:40 - loss: 0.7258 - acc: 0.5113
2624/5677 [============>.................] - ETA: 4:33 - loss: 0.7268 - acc: 0.5088
2688/5677 [=============>................] - ETA: 4:28 - loss: 0.7270 - acc: 0.5071
2752/5677 [=============>................] - ETA: 4:23 - loss: 0.7267 - acc: 0.5076
2816/5677 [=============>................] - ETA: 4:16 - loss: 0.7264 - acc: 0.5075
2880/5677 [==============>...............] - ETA: 4:09 - loss: 0.7251 - acc: 0.5094
2944/5677 [==============>...............] - ETA: 4:02 - loss: 0.7245 - acc: 0.5085
3008/5677 [==============>...............] - ETA: 3:57 - loss: 0.7239 - acc: 0.5090
3072/5677 [===============>..............] - ETA: 3:52 - loss: 0.7238 - acc: 0.5088
3136/5677 [===============>..............] - ETA: 3:47 - loss: 0.7227 - acc: 0.5105
3200/5677 [===============>..............] - ETA: 3:40 - loss: 0.7224 - acc: 0.5094
3264/5677 [================>.............] - ETA: 3:34 - loss: 0.7221 - acc: 0.5086
3328/5677 [================>.............] - ETA: 3:27 - loss: 0.7220 - acc: 0.5090
3392/5677 [================>.............] - ETA: 3:21 - loss: 0.7213 - acc: 0.5091
3456/5677 [=================>............] - ETA: 3:17 - loss: 0.7216 - acc: 0.5087
3520/5677 [=================>............] - ETA: 3:12 - loss: 0.7229 - acc: 0.5074
3584/5677 [=================>............] - ETA: 3:06 - loss: 0.7235 - acc: 0.5067
3648/5677 [==================>...........] - ETA: 3:00 - loss: 0.7232 - acc: 0.5063
3712/5677 [==================>...........] - ETA: 2:54 - loss: 0.7227 - acc: 0.5070
3776/5677 [==================>...........] - ETA: 2:48 - loss: 0.7227 - acc: 0.5058
3840/5677 [===================>..........] - ETA: 2:41 - loss: 0.7226 - acc: 0.5057
3904/5677 [===================>..........] - ETA: 2:35 - loss: 0.7226 - acc: 0.5038
3968/5677 [===================>..........] - ETA: 2:29 - loss: 0.7222 - acc: 0.5040
4032/5677 [====================>.........] - ETA: 2:24 - loss: 0.7221 - acc: 0.5040
4096/5677 [====================>.........] - ETA: 2:19 - loss: 0.7223 - acc: 0.5044
4160/5677 [====================>.........] - ETA: 2:13 - loss: 0.7213 - acc: 0.5062
4224/5677 [=====================>........] - ETA: 2:08 - loss: 0.7213 - acc: 0.5059
4288/5677 [=====================>........] - ETA: 2:02 - loss: 0.7216 - acc: 0.5051
4352/5677 [=====================>........] - ETA: 1:56 - loss: 0.7213 - acc: 0.5057
4416/5677 [======================>.......] - ETA: 1:50 - loss: 0.7206 - acc: 0.5068
4480/5677 [======================>.......] - ETA: 1:44 - loss: 0.7203 - acc: 0.5074
4544/5677 [=======================>......] - ETA: 1:39 - loss: 0.7196 - acc: 0.5086
4608/5677 [=======================>......] - ETA: 1:33 - loss: 0.7194 - acc: 0.5082
4672/5677 [=======================>......] - ETA: 1:28 - loss: 0.7196 - acc: 0.5075
4736/5677 [========================>.....] - ETA: 1:23 - loss: 0.7196 - acc: 0.5072
4800/5677 [========================>.....] - ETA: 1:17 - loss: 0.7189 - acc: 0.5077
4864/5677 [========================>.....] - ETA: 1:12 - loss: 0.7187 - acc: 0.5078
4928/5677 [=========================>....] - ETA: 1:06 - loss: 0.7183 - acc: 0.5081
4992/5677 [=========================>....] - ETA: 1:00 - loss: 0.7180 - acc: 0.5076
5056/5677 [=========================>....] - ETA: 54s - loss: 0.7177 - acc: 0.5085 
5120/5677 [==========================>...] - ETA: 49s - loss: 0.7177 - acc: 0.5088
5184/5677 [==========================>...] - ETA: 43s - loss: 0.7172 - acc: 0.5096
5248/5677 [==========================>...] - ETA: 37s - loss: 0.7173 - acc: 0.5088
5312/5677 [===========================>..] - ETA: 32s - loss: 0.7169 - acc: 0.5092
5376/5677 [===========================>..] - ETA: 26s - loss: 0.7168 - acc: 0.5093
5440/5677 [===========================>..] - ETA: 21s - loss: 0.7164 - acc: 0.5097
5504/5677 [============================>.] - ETA: 15s - loss: 0.7163 - acc: 0.5096
5568/5677 [============================>.] - ETA: 9s - loss: 0.7162 - acc: 0.5092 
5632/5677 [============================>.] - ETA: 3s - loss: 0.7157 - acc: 0.5101
5677/5677 [==============================] - 517s 91ms/step - loss: 0.7152 - acc: 0.5112 - val_loss: 0.6889 - val_acc: 0.5277

Epoch 00001: val_acc improved from -inf to 0.52773, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window16/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 10:44 - loss: 0.7129 - acc: 0.5000
 128/5677 [..............................] - ETA: 9:49 - loss: 0.7114 - acc: 0.5000 
 192/5677 [>.............................] - ETA: 9:17 - loss: 0.7134 - acc: 0.4688
 256/5677 [>.............................] - ETA: 9:02 - loss: 0.7050 - acc: 0.4961
 320/5677 [>.............................] - ETA: 8:52 - loss: 0.7004 - acc: 0.5062
 384/5677 [=>............................] - ETA: 8:49 - loss: 0.7012 - acc: 0.4974
 448/5677 [=>............................] - ETA: 8:45 - loss: 0.6979 - acc: 0.5067
 512/5677 [=>............................] - ETA: 8:50 - loss: 0.6981 - acc: 0.5137
 576/5677 [==>...........................] - ETA: 8:43 - loss: 0.7016 - acc: 0.5052
 640/5677 [==>...........................] - ETA: 8:34 - loss: 0.6944 - acc: 0.5234
 704/5677 [==>...........................] - ETA: 8:26 - loss: 0.6951 - acc: 0.5270
 768/5677 [===>..........................] - ETA: 8:17 - loss: 0.6947 - acc: 0.5286
 832/5677 [===>..........................] - ETA: 8:09 - loss: 0.6962 - acc: 0.5264
 896/5677 [===>..........................] - ETA: 8:01 - loss: 0.6946 - acc: 0.5290
 960/5677 [====>.........................] - ETA: 7:53 - loss: 0.6939 - acc: 0.5302
1024/5677 [====>.........................] - ETA: 7:45 - loss: 0.6940 - acc: 0.5283
1088/5677 [====>.........................] - ETA: 7:37 - loss: 0.6937 - acc: 0.5248
1152/5677 [=====>........................] - ETA: 7:29 - loss: 0.6928 - acc: 0.5269
1216/5677 [=====>........................] - ETA: 7:21 - loss: 0.6943 - acc: 0.5238
1280/5677 [=====>........................] - ETA: 7:15 - loss: 0.6943 - acc: 0.5227
1344/5677 [======>.......................] - ETA: 7:07 - loss: 0.6950 - acc: 0.5208
1408/5677 [======>.......................] - ETA: 7:01 - loss: 0.6958 - acc: 0.5170
1472/5677 [======>.......................] - ETA: 6:53 - loss: 0.6961 - acc: 0.5156
1536/5677 [=======>......................] - ETA: 6:42 - loss: 0.6956 - acc: 0.5169
1600/5677 [=======>......................] - ETA: 6:32 - loss: 0.6946 - acc: 0.5212
1664/5677 [=======>......................] - ETA: 6:23 - loss: 0.6951 - acc: 0.5192
1728/5677 [========>.....................] - ETA: 6:13 - loss: 0.6964 - acc: 0.5174
1792/5677 [========>.....................] - ETA: 6:04 - loss: 0.6983 - acc: 0.5128
1856/5677 [========>.....................] - ETA: 5:55 - loss: 0.6987 - acc: 0.5124
1920/5677 [=========>....................] - ETA: 5:49 - loss: 0.6982 - acc: 0.5125
1984/5677 [=========>....................] - ETA: 5:42 - loss: 0.6973 - acc: 0.5141
2048/5677 [=========>....................] - ETA: 5:34 - loss: 0.6977 - acc: 0.5117
2112/5677 [==========>...................] - ETA: 5:27 - loss: 0.6965 - acc: 0.5170
2176/5677 [==========>...................] - ETA: 5:17 - loss: 0.6966 - acc: 0.5161
2240/5677 [==========>...................] - ETA: 5:08 - loss: 0.6975 - acc: 0.5129
2304/5677 [===========>..................] - ETA: 5:01 - loss: 0.6976 - acc: 0.5139
2368/5677 [===========>..................] - ETA: 4:53 - loss: 0.6978 - acc: 0.5148
2432/5677 [===========>..................] - ETA: 4:45 - loss: 0.6977 - acc: 0.5160
2496/5677 [============>.................] - ETA: 4:38 - loss: 0.6975 - acc: 0.5160
2560/5677 [============>.................] - ETA: 4:33 - loss: 0.6976 - acc: 0.5141
2624/5677 [============>.................] - ETA: 4:28 - loss: 0.6976 - acc: 0.5133
2688/5677 [=============>................] - ETA: 4:23 - loss: 0.6968 - acc: 0.5153
2752/5677 [=============>................] - ETA: 4:17 - loss: 0.6968 - acc: 0.5156
2816/5677 [=============>................] - ETA: 4:11 - loss: 0.6969 - acc: 0.5142
2880/5677 [==============>...............] - ETA: 4:04 - loss: 0.6966 - acc: 0.5142
2944/5677 [==============>...............] - ETA: 3:57 - loss: 0.6973 - acc: 0.5139
3008/5677 [==============>...............] - ETA: 3:51 - loss: 0.6962 - acc: 0.5186
3072/5677 [===============>..............] - ETA: 3:44 - loss: 0.6968 - acc: 0.5176
3136/5677 [===============>..............] - ETA: 3:38 - loss: 0.6971 - acc: 0.5166
3200/5677 [===============>..............] - ETA: 3:32 - loss: 0.6966 - acc: 0.5172
3264/5677 [================>.............] - ETA: 3:27 - loss: 0.6965 - acc: 0.5178
3328/5677 [================>.............] - ETA: 3:22 - loss: 0.6962 - acc: 0.5180
3392/5677 [================>.............] - ETA: 3:16 - loss: 0.6960 - acc: 0.5180
3456/5677 [=================>............] - ETA: 3:10 - loss: 0.6957 - acc: 0.5191
3520/5677 [=================>............] - ETA: 3:04 - loss: 0.6963 - acc: 0.5185
3584/5677 [=================>............] - ETA: 2:58 - loss: 0.6968 - acc: 0.5179
3648/5677 [==================>...........] - ETA: 2:52 - loss: 0.6966 - acc: 0.5184
3712/5677 [==================>...........] - ETA: 2:46 - loss: 0.6964 - acc: 0.5186
3776/5677 [==================>...........] - ETA: 2:42 - loss: 0.6960 - acc: 0.5204
3840/5677 [===================>..........] - ETA: 2:36 - loss: 0.6960 - acc: 0.5201
3904/5677 [===================>..........] - ETA: 2:31 - loss: 0.6960 - acc: 0.5205
3968/5677 [===================>..........] - ETA: 2:25 - loss: 0.6961 - acc: 0.5199
4032/5677 [====================>.........] - ETA: 2:20 - loss: 0.6963 - acc: 0.5196
4096/5677 [====================>.........] - ETA: 2:14 - loss: 0.6961 - acc: 0.5195
4160/5677 [====================>.........] - ETA: 2:09 - loss: 0.6954 - acc: 0.5224
4224/5677 [=====================>........] - ETA: 2:03 - loss: 0.6960 - acc: 0.5223
4288/5677 [=====================>........] - ETA: 1:58 - loss: 0.6955 - acc: 0.5245
4352/5677 [=====================>........] - ETA: 1:52 - loss: 0.6958 - acc: 0.5237
4416/5677 [======================>.......] - ETA: 1:47 - loss: 0.6954 - acc: 0.5249
4480/5677 [======================>.......] - ETA: 1:41 - loss: 0.6954 - acc: 0.5257
4544/5677 [=======================>......] - ETA: 1:36 - loss: 0.6950 - acc: 0.5268
4608/5677 [=======================>......] - ETA: 1:30 - loss: 0.6946 - acc: 0.5276
4672/5677 [=======================>......] - ETA: 1:25 - loss: 0.6940 - acc: 0.5293
4736/5677 [========================>.....] - ETA: 1:19 - loss: 0.6943 - acc: 0.5279
4800/5677 [========================>.....] - ETA: 1:14 - loss: 0.6949 - acc: 0.5271
4864/5677 [========================>.....] - ETA: 1:08 - loss: 0.6939 - acc: 0.5292
4928/5677 [=========================>....] - ETA: 1:03 - loss: 0.6946 - acc: 0.5274
4992/5677 [=========================>....] - ETA: 58s - loss: 0.6956 - acc: 0.5250 
5056/5677 [=========================>....] - ETA: 52s - loss: 0.6956 - acc: 0.5245
5120/5677 [==========================>...] - ETA: 47s - loss: 0.6954 - acc: 0.5260
5184/5677 [==========================>...] - ETA: 41s - loss: 0.6950 - acc: 0.5272
5248/5677 [==========================>...] - ETA: 36s - loss: 0.6952 - acc: 0.5267
5312/5677 [===========================>..] - ETA: 30s - loss: 0.6953 - acc: 0.5267
5376/5677 [===========================>..] - ETA: 25s - loss: 0.6958 - acc: 0.5253
5440/5677 [===========================>..] - ETA: 19s - loss: 0.6959 - acc: 0.5246
5504/5677 [============================>.] - ETA: 14s - loss: 0.6959 - acc: 0.5249
5568/5677 [============================>.] - ETA: 9s - loss: 0.6957 - acc: 0.5257 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6961 - acc: 0.5243
5677/5677 [==============================] - 497s 88ms/step - loss: 0.6966 - acc: 0.5232 - val_loss: 0.6786 - val_acc: 0.5674

Epoch 00002: val_acc improved from 0.52773 to 0.56735, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window16/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 7:07 - loss: 0.6961 - acc: 0.5625
 128/5677 [..............................] - ETA: 6:35 - loss: 0.6926 - acc: 0.5312
 192/5677 [>.............................] - ETA: 6:24 - loss: 0.6935 - acc: 0.5208
 256/5677 [>.............................] - ETA: 6:17 - loss: 0.6946 - acc: 0.5039
 320/5677 [>.............................] - ETA: 6:16 - loss: 0.6925 - acc: 0.5094
 384/5677 [=>............................] - ETA: 6:19 - loss: 0.6913 - acc: 0.5156
 448/5677 [=>............................] - ETA: 6:13 - loss: 0.6985 - acc: 0.5022
 512/5677 [=>............................] - ETA: 6:11 - loss: 0.6972 - acc: 0.5059
 576/5677 [==>...........................] - ETA: 6:12 - loss: 0.7011 - acc: 0.5000
 640/5677 [==>...........................] - ETA: 6:11 - loss: 0.6975 - acc: 0.5047
 704/5677 [==>...........................] - ETA: 6:21 - loss: 0.6967 - acc: 0.5071
 768/5677 [===>..........................] - ETA: 6:18 - loss: 0.6972 - acc: 0.5104
 832/5677 [===>..........................] - ETA: 6:18 - loss: 0.6954 - acc: 0.5168
 896/5677 [===>..........................] - ETA: 6:13 - loss: 0.6948 - acc: 0.5190
 960/5677 [====>.........................] - ETA: 6:10 - loss: 0.6965 - acc: 0.5167
1024/5677 [====>.........................] - ETA: 6:03 - loss: 0.6970 - acc: 0.5156
1088/5677 [====>.........................] - ETA: 5:57 - loss: 0.6962 - acc: 0.5193
1152/5677 [=====>........................] - ETA: 5:52 - loss: 0.6957 - acc: 0.5234
1216/5677 [=====>........................] - ETA: 5:46 - loss: 0.6955 - acc: 0.5238
1280/5677 [=====>........................] - ETA: 5:40 - loss: 0.6970 - acc: 0.5227
1344/5677 [======>.......................] - ETA: 5:35 - loss: 0.6973 - acc: 0.5208
1408/5677 [======>.......................] - ETA: 5:31 - loss: 0.6979 - acc: 0.5213
1472/5677 [======>.......................] - ETA: 5:26 - loss: 0.6977 - acc: 0.5204
1536/5677 [=======>......................] - ETA: 5:24 - loss: 0.6987 - acc: 0.5208
1600/5677 [=======>......................] - ETA: 5:18 - loss: 0.6986 - acc: 0.5219
1664/5677 [=======>......................] - ETA: 5:13 - loss: 0.6985 - acc: 0.5222
1728/5677 [========>.....................] - ETA: 5:08 - loss: 0.6966 - acc: 0.5249
1792/5677 [========>.....................] - ETA: 5:03 - loss: 0.6958 - acc: 0.5268
1856/5677 [========>.....................] - ETA: 4:58 - loss: 0.6944 - acc: 0.5329
1920/5677 [=========>....................] - ETA: 4:52 - loss: 0.6946 - acc: 0.5323
1984/5677 [=========>....................] - ETA: 4:47 - loss: 0.6961 - acc: 0.5282
2048/5677 [=========>....................] - ETA: 4:41 - loss: 0.6957 - acc: 0.5283
2112/5677 [==========>...................] - ETA: 4:36 - loss: 0.6967 - acc: 0.5251
2176/5677 [==========>...................] - ETA: 4:31 - loss: 0.6972 - acc: 0.5244
2240/5677 [==========>...................] - ETA: 4:25 - loss: 0.6967 - acc: 0.5254
2304/5677 [===========>..................] - ETA: 4:20 - loss: 0.6962 - acc: 0.5278
2368/5677 [===========>..................] - ETA: 4:15 - loss: 0.6960 - acc: 0.5283
2432/5677 [===========>..................] - ETA: 4:09 - loss: 0.6961 - acc: 0.5275
2496/5677 [============>.................] - ETA: 4:04 - loss: 0.6956 - acc: 0.5292
2560/5677 [============>.................] - ETA: 3:59 - loss: 0.6957 - acc: 0.5277
2624/5677 [============>.................] - ETA: 3:54 - loss: 0.6955 - acc: 0.5271
2688/5677 [=============>................] - ETA: 3:48 - loss: 0.6961 - acc: 0.5257
2752/5677 [=============>................] - ETA: 3:44 - loss: 0.6970 - acc: 0.5218
2816/5677 [=============>................] - ETA: 3:38 - loss: 0.6973 - acc: 0.5210
2880/5677 [==============>...............] - ETA: 3:33 - loss: 0.6969 - acc: 0.5229
2944/5677 [==============>...............] - ETA: 3:28 - loss: 0.6969 - acc: 0.5231
3008/5677 [==============>...............] - ETA: 3:23 - loss: 0.6972 - acc: 0.5229
3072/5677 [===============>..............] - ETA: 3:18 - loss: 0.6973 - acc: 0.5228
3136/5677 [===============>..............] - ETA: 3:12 - loss: 0.6975 - acc: 0.5207
3200/5677 [===============>..............] - ETA: 3:07 - loss: 0.6969 - acc: 0.5234
3264/5677 [================>.............] - ETA: 3:02 - loss: 0.6969 - acc: 0.5230
3328/5677 [================>.............] - ETA: 2:57 - loss: 0.6968 - acc: 0.5219
3392/5677 [================>.............] - ETA: 2:52 - loss: 0.6965 - acc: 0.5227
3456/5677 [=================>............] - ETA: 2:47 - loss: 0.6962 - acc: 0.5231
3520/5677 [=================>............] - ETA: 2:42 - loss: 0.6953 - acc: 0.5247
3584/5677 [=================>............] - ETA: 2:36 - loss: 0.6948 - acc: 0.5259
3648/5677 [==================>...........] - ETA: 2:31 - loss: 0.6952 - acc: 0.5252
3712/5677 [==================>...........] - ETA: 2:27 - loss: 0.6941 - acc: 0.5275
3776/5677 [==================>...........] - ETA: 2:22 - loss: 0.6947 - acc: 0.5270
3840/5677 [===================>..........] - ETA: 2:17 - loss: 0.6958 - acc: 0.5247
3904/5677 [===================>..........] - ETA: 2:12 - loss: 0.6964 - acc: 0.5246
3968/5677 [===================>..........] - ETA: 2:07 - loss: 0.6963 - acc: 0.5242
4032/5677 [====================>.........] - ETA: 2:02 - loss: 0.6960 - acc: 0.5243
4096/5677 [====================>.........] - ETA: 1:57 - loss: 0.6962 - acc: 0.5232
4160/5677 [====================>.........] - ETA: 1:52 - loss: 0.6969 - acc: 0.5216
4224/5677 [=====================>........] - ETA: 1:47 - loss: 0.6969 - acc: 0.5220
4288/5677 [=====================>........] - ETA: 1:42 - loss: 0.6965 - acc: 0.5224
4352/5677 [=====================>........] - ETA: 1:37 - loss: 0.6962 - acc: 0.5244
4416/5677 [======================>.......] - ETA: 1:32 - loss: 0.6958 - acc: 0.5245
4480/5677 [======================>.......] - ETA: 1:28 - loss: 0.6953 - acc: 0.5254
4544/5677 [=======================>......] - ETA: 1:23 - loss: 0.6950 - acc: 0.5264
4608/5677 [=======================>......] - ETA: 1:18 - loss: 0.6951 - acc: 0.5265
4672/5677 [=======================>......] - ETA: 1:13 - loss: 0.6950 - acc: 0.5270
4736/5677 [========================>.....] - ETA: 1:08 - loss: 0.6954 - acc: 0.5260
4800/5677 [========================>.....] - ETA: 1:04 - loss: 0.6948 - acc: 0.5273
4864/5677 [========================>.....] - ETA: 59s - loss: 0.6949 - acc: 0.5269 
4928/5677 [=========================>....] - ETA: 54s - loss: 0.6950 - acc: 0.5264
4992/5677 [=========================>....] - ETA: 49s - loss: 0.6950 - acc: 0.5260
5056/5677 [=========================>....] - ETA: 45s - loss: 0.6952 - acc: 0.5253
5120/5677 [==========================>...] - ETA: 40s - loss: 0.6952 - acc: 0.5246
5184/5677 [==========================>...] - ETA: 35s - loss: 0.6957 - acc: 0.5230
5248/5677 [==========================>...] - ETA: 31s - loss: 0.6959 - acc: 0.5225
5312/5677 [===========================>..] - ETA: 26s - loss: 0.6956 - acc: 0.5247
5376/5677 [===========================>..] - ETA: 21s - loss: 0.6950 - acc: 0.5273
5440/5677 [===========================>..] - ETA: 17s - loss: 0.6951 - acc: 0.5270
5504/5677 [============================>.] - ETA: 12s - loss: 0.6949 - acc: 0.5285
5568/5677 [============================>.] - ETA: 7s - loss: 0.6951 - acc: 0.5284 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6953 - acc: 0.5277
5677/5677 [==============================] - 425s 75ms/step - loss: 0.6952 - acc: 0.5276 - val_loss: 0.6820 - val_acc: 0.5610

Epoch 00003: val_acc did not improve from 0.56735
Epoch 4/10

  64/5677 [..............................] - ETA: 6:16 - loss: 0.6831 - acc: 0.6406
 128/5677 [..............................] - ETA: 6:04 - loss: 0.6852 - acc: 0.5938
 192/5677 [>.............................] - ETA: 5:56 - loss: 0.6928 - acc: 0.5521
 256/5677 [>.............................] - ETA: 5:54 - loss: 0.6913 - acc: 0.5508
 320/5677 [>.............................] - ETA: 5:52 - loss: 0.6941 - acc: 0.5406
 384/5677 [=>............................] - ETA: 5:45 - loss: 0.6942 - acc: 0.5417
 448/5677 [=>............................] - ETA: 5:42 - loss: 0.6958 - acc: 0.5402
 512/5677 [=>............................] - ETA: 5:37 - loss: 0.6979 - acc: 0.5332
 576/5677 [==>...........................] - ETA: 5:30 - loss: 0.6969 - acc: 0.5365
 640/5677 [==>...........................] - ETA: 5:28 - loss: 0.6988 - acc: 0.5297
 704/5677 [==>...........................] - ETA: 5:23 - loss: 0.6942 - acc: 0.5440
 768/5677 [===>..........................] - ETA: 5:19 - loss: 0.6918 - acc: 0.5456
 832/5677 [===>..........................] - ETA: 5:15 - loss: 0.6912 - acc: 0.5493
 896/5677 [===>..........................] - ETA: 5:11 - loss: 0.6911 - acc: 0.5458
 960/5677 [====>.........................] - ETA: 5:07 - loss: 0.6900 - acc: 0.5510
1024/5677 [====>.........................] - ETA: 5:02 - loss: 0.6894 - acc: 0.5488
1088/5677 [====>.........................] - ETA: 4:57 - loss: 0.6898 - acc: 0.5450
1152/5677 [=====>........................] - ETA: 4:55 - loss: 0.6912 - acc: 0.5417
1216/5677 [=====>........................] - ETA: 4:51 - loss: 0.6903 - acc: 0.5485
1280/5677 [=====>........................] - ETA: 4:47 - loss: 0.6910 - acc: 0.5477
1344/5677 [======>.......................] - ETA: 4:43 - loss: 0.6916 - acc: 0.5454
1408/5677 [======>.......................] - ETA: 4:39 - loss: 0.6930 - acc: 0.5419
1472/5677 [======>.......................] - ETA: 4:34 - loss: 0.6928 - acc: 0.5408
1536/5677 [=======>......................] - ETA: 4:30 - loss: 0.6918 - acc: 0.5430
1600/5677 [=======>......................] - ETA: 4:25 - loss: 0.6911 - acc: 0.5450
1664/5677 [=======>......................] - ETA: 4:21 - loss: 0.6894 - acc: 0.5517
1728/5677 [========>.....................] - ETA: 4:17 - loss: 0.6901 - acc: 0.5469
1792/5677 [========>.....................] - ETA: 4:15 - loss: 0.6899 - acc: 0.5469
1856/5677 [========>.....................] - ETA: 4:12 - loss: 0.6908 - acc: 0.5442
1920/5677 [=========>....................] - ETA: 4:10 - loss: 0.6904 - acc: 0.5448
1984/5677 [=========>....................] - ETA: 4:07 - loss: 0.6904 - acc: 0.5449
2048/5677 [=========>....................] - ETA: 4:04 - loss: 0.6907 - acc: 0.5439
2112/5677 [==========>...................] - ETA: 4:01 - loss: 0.6913 - acc: 0.5440
2176/5677 [==========>...................] - ETA: 3:58 - loss: 0.6914 - acc: 0.5441
2240/5677 [==========>...................] - ETA: 3:55 - loss: 0.6908 - acc: 0.5460
2304/5677 [===========>..................] - ETA: 3:51 - loss: 0.6912 - acc: 0.5456
2368/5677 [===========>..................] - ETA: 3:48 - loss: 0.6917 - acc: 0.5435
2432/5677 [===========>..................] - ETA: 3:45 - loss: 0.6920 - acc: 0.5424
2496/5677 [============>.................] - ETA: 3:42 - loss: 0.6923 - acc: 0.5413
2560/5677 [============>.................] - ETA: 3:39 - loss: 0.6923 - acc: 0.5418
2624/5677 [============>.................] - ETA: 3:35 - loss: 0.6921 - acc: 0.5427
2688/5677 [=============>................] - ETA: 3:31 - loss: 0.6917 - acc: 0.5417
2752/5677 [=============>................] - ETA: 3:29 - loss: 0.6916 - acc: 0.5429
2816/5677 [=============>................] - ETA: 3:26 - loss: 0.6923 - acc: 0.5401
2880/5677 [==============>...............] - ETA: 3:22 - loss: 0.6924 - acc: 0.5389
2944/5677 [==============>...............] - ETA: 3:18 - loss: 0.6918 - acc: 0.5401
3008/5677 [==============>...............] - ETA: 3:15 - loss: 0.6918 - acc: 0.5399
3072/5677 [===============>..............] - ETA: 3:11 - loss: 0.6918 - acc: 0.5397
3136/5677 [===============>..............] - ETA: 3:09 - loss: 0.6918 - acc: 0.5408
3200/5677 [===============>..............] - ETA: 3:04 - loss: 0.6914 - acc: 0.5425
3264/5677 [================>.............] - ETA: 3:02 - loss: 0.6909 - acc: 0.5429
3328/5677 [================>.............] - ETA: 2:57 - loss: 0.6906 - acc: 0.5427
3392/5677 [================>.............] - ETA: 2:53 - loss: 0.6914 - acc: 0.5395
3456/5677 [=================>............] - ETA: 2:49 - loss: 0.6911 - acc: 0.5408
3520/5677 [=================>............] - ETA: 2:45 - loss: 0.6915 - acc: 0.5409
3584/5677 [=================>............] - ETA: 2:40 - loss: 0.6909 - acc: 0.5416
3648/5677 [==================>...........] - ETA: 2:36 - loss: 0.6912 - acc: 0.5414
3712/5677 [==================>...........] - ETA: 2:32 - loss: 0.6910 - acc: 0.5418
3776/5677 [==================>...........] - ETA: 2:27 - loss: 0.6911 - acc: 0.5395
3840/5677 [===================>..........] - ETA: 2:22 - loss: 0.6913 - acc: 0.5388
3904/5677 [===================>..........] - ETA: 2:18 - loss: 0.6914 - acc: 0.5394
3968/5677 [===================>..........] - ETA: 2:14 - loss: 0.6914 - acc: 0.5401
4032/5677 [====================>.........] - ETA: 2:09 - loss: 0.6915 - acc: 0.5394
4096/5677 [====================>.........] - ETA: 2:04 - loss: 0.6914 - acc: 0.5396
4160/5677 [====================>.........] - ETA: 1:59 - loss: 0.6914 - acc: 0.5394
4224/5677 [=====================>........] - ETA: 1:54 - loss: 0.6911 - acc: 0.5400
4288/5677 [=====================>........] - ETA: 1:49 - loss: 0.6909 - acc: 0.5406
4352/5677 [=====================>........] - ETA: 1:45 - loss: 0.6909 - acc: 0.5407
4416/5677 [======================>.......] - ETA: 1:40 - loss: 0.6905 - acc: 0.5414
4480/5677 [======================>.......] - ETA: 1:35 - loss: 0.6905 - acc: 0.5417
4544/5677 [=======================>......] - ETA: 1:30 - loss: 0.6907 - acc: 0.5416
4608/5677 [=======================>......] - ETA: 1:25 - loss: 0.6909 - acc: 0.5406
4672/5677 [=======================>......] - ETA: 1:20 - loss: 0.6910 - acc: 0.5394
4736/5677 [========================>.....] - ETA: 1:15 - loss: 0.6909 - acc: 0.5403
4800/5677 [========================>.....] - ETA: 1:10 - loss: 0.6912 - acc: 0.5392
4864/5677 [========================>.....] - ETA: 1:05 - loss: 0.6909 - acc: 0.5401
4928/5677 [=========================>....] - ETA: 1:00 - loss: 0.6912 - acc: 0.5396
4992/5677 [=========================>....] - ETA: 55s - loss: 0.6906 - acc: 0.5407 
5056/5677 [=========================>....] - ETA: 50s - loss: 0.6904 - acc: 0.5407
5120/5677 [==========================>...] - ETA: 45s - loss: 0.6901 - acc: 0.5416
5184/5677 [==========================>...] - ETA: 40s - loss: 0.6901 - acc: 0.5417
5248/5677 [==========================>...] - ETA: 35s - loss: 0.6899 - acc: 0.5419
5312/5677 [===========================>..] - ETA: 29s - loss: 0.6894 - acc: 0.5431
5376/5677 [===========================>..] - ETA: 24s - loss: 0.6894 - acc: 0.5435
5440/5677 [===========================>..] - ETA: 19s - loss: 0.6894 - acc: 0.5426
5504/5677 [============================>.] - ETA: 14s - loss: 0.6896 - acc: 0.5427
5568/5677 [============================>.] - ETA: 8s - loss: 0.6895 - acc: 0.5427 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6892 - acc: 0.5435
5677/5677 [==============================] - 490s 86ms/step - loss: 0.6889 - acc: 0.5443 - val_loss: 0.6751 - val_acc: 0.5737

Epoch 00004: val_acc improved from 0.56735 to 0.57369, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window16/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 5/10

  64/5677 [..............................] - ETA: 8:36 - loss: 0.6955 - acc: 0.5625
 128/5677 [..............................] - ETA: 8:27 - loss: 0.6836 - acc: 0.5703
 192/5677 [>.............................] - ETA: 7:48 - loss: 0.6797 - acc: 0.5469
 256/5677 [>.............................] - ETA: 7:43 - loss: 0.6825 - acc: 0.5703
 320/5677 [>.............................] - ETA: 7:26 - loss: 0.6790 - acc: 0.5844
 384/5677 [=>............................] - ETA: 7:23 - loss: 0.6838 - acc: 0.5859
 448/5677 [=>............................] - ETA: 7:13 - loss: 0.6828 - acc: 0.5893
 512/5677 [=>............................] - ETA: 7:28 - loss: 0.6761 - acc: 0.6016
 576/5677 [==>...........................] - ETA: 7:42 - loss: 0.6775 - acc: 0.5990
 640/5677 [==>...........................] - ETA: 7:40 - loss: 0.6804 - acc: 0.5844
 704/5677 [==>...........................] - ETA: 7:38 - loss: 0.6835 - acc: 0.5838
 768/5677 [===>..........................] - ETA: 7:33 - loss: 0.6828 - acc: 0.5807
 832/5677 [===>..........................] - ETA: 7:25 - loss: 0.6841 - acc: 0.5781
 896/5677 [===>..........................] - ETA: 7:14 - loss: 0.6856 - acc: 0.5748
 960/5677 [====>.........................] - ETA: 7:05 - loss: 0.6873 - acc: 0.5698
1024/5677 [====>.........................] - ETA: 6:56 - loss: 0.6879 - acc: 0.5674
1088/5677 [====>.........................] - ETA: 6:50 - loss: 0.6903 - acc: 0.5625
1152/5677 [=====>........................] - ETA: 6:45 - loss: 0.6896 - acc: 0.5668
1216/5677 [=====>........................] - ETA: 6:49 - loss: 0.6896 - acc: 0.5666
1280/5677 [=====>........................] - ETA: 6:47 - loss: 0.6892 - acc: 0.5664
1344/5677 [======>.......................] - ETA: 6:42 - loss: 0.6908 - acc: 0.5632
1408/5677 [======>.......................] - ETA: 6:37 - loss: 0.6896 - acc: 0.5668
1472/5677 [======>.......................] - ETA: 6:33 - loss: 0.6892 - acc: 0.5652
1536/5677 [=======>......................] - ETA: 6:30 - loss: 0.6890 - acc: 0.5638
1600/5677 [=======>......................] - ETA: 6:27 - loss: 0.6887 - acc: 0.5637
1664/5677 [=======>......................] - ETA: 6:21 - loss: 0.6884 - acc: 0.5637
1728/5677 [========>.....................] - ETA: 6:16 - loss: 0.6876 - acc: 0.5654
1792/5677 [========>.....................] - ETA: 6:10 - loss: 0.6871 - acc: 0.5686
1856/5677 [========>.....................] - ETA: 6:05 - loss: 0.6866 - acc: 0.5700
1920/5677 [=========>....................] - ETA: 6:00 - loss: 0.6868 - acc: 0.5687
1984/5677 [=========>....................] - ETA: 5:54 - loss: 0.6868 - acc: 0.5696
2048/5677 [=========>....................] - ETA: 5:48 - loss: 0.6869 - acc: 0.5679
2112/5677 [==========>...................] - ETA: 5:43 - loss: 0.6871 - acc: 0.5658
2176/5677 [==========>...................] - ETA: 5:38 - loss: 0.6877 - acc: 0.5648
2240/5677 [==========>...................] - ETA: 5:32 - loss: 0.6873 - acc: 0.5647
2304/5677 [===========>..................] - ETA: 5:27 - loss: 0.6881 - acc: 0.5616
2368/5677 [===========>..................] - ETA: 5:21 - loss: 0.6886 - acc: 0.5604
2432/5677 [===========>..................] - ETA: 5:15 - loss: 0.6884 - acc: 0.5613
2496/5677 [============>.................] - ETA: 5:09 - loss: 0.6893 - acc: 0.5585
2560/5677 [============>.................] - ETA: 5:04 - loss: 0.6893 - acc: 0.5582
2624/5677 [============>.................] - ETA: 4:57 - loss: 0.6895 - acc: 0.5575
2688/5677 [=============>................] - ETA: 4:50 - loss: 0.6899 - acc: 0.5562
2752/5677 [=============>................] - ETA: 4:43 - loss: 0.6895 - acc: 0.5578
2816/5677 [=============>................] - ETA: 4:36 - loss: 0.6896 - acc: 0.5568
2880/5677 [==============>...............] - ETA: 4:29 - loss: 0.6902 - acc: 0.5559
2944/5677 [==============>...............] - ETA: 4:22 - loss: 0.6897 - acc: 0.5574
3008/5677 [==============>...............] - ETA: 4:15 - loss: 0.6898 - acc: 0.5555
3072/5677 [===============>..............] - ETA: 4:09 - loss: 0.6896 - acc: 0.5557
3136/5677 [===============>..............] - ETA: 4:03 - loss: 0.6892 - acc: 0.5571
3200/5677 [===============>..............] - ETA: 3:57 - loss: 0.6895 - acc: 0.5572
3264/5677 [================>.............] - ETA: 3:51 - loss: 0.6898 - acc: 0.5570
3328/5677 [================>.............] - ETA: 3:44 - loss: 0.6895 - acc: 0.5571
3392/5677 [================>.............] - ETA: 3:37 - loss: 0.6893 - acc: 0.5566
3456/5677 [=================>............] - ETA: 3:31 - loss: 0.6894 - acc: 0.5550
3520/5677 [=================>............] - ETA: 3:24 - loss: 0.6896 - acc: 0.5545
3584/5677 [=================>............] - ETA: 3:18 - loss: 0.6891 - acc: 0.5547
3648/5677 [==================>...........] - ETA: 3:13 - loss: 0.6889 - acc: 0.5559
3712/5677 [==================>...........] - ETA: 3:07 - loss: 0.6887 - acc: 0.5552
3776/5677 [==================>...........] - ETA: 3:01 - loss: 0.6884 - acc: 0.5559
3840/5677 [===================>..........] - ETA: 2:55 - loss: 0.6887 - acc: 0.5539
3904/5677 [===================>..........] - ETA: 2:48 - loss: 0.6883 - acc: 0.5551
3968/5677 [===================>..........] - ETA: 2:42 - loss: 0.6885 - acc: 0.5547
4032/5677 [====================>.........] - ETA: 2:35 - loss: 0.6881 - acc: 0.5553
4096/5677 [====================>.........] - ETA: 2:29 - loss: 0.6878 - acc: 0.5562
4160/5677 [====================>.........] - ETA: 2:23 - loss: 0.6875 - acc: 0.5565
4224/5677 [=====================>........] - ETA: 2:17 - loss: 0.6867 - acc: 0.5589
4288/5677 [=====================>........] - ETA: 2:11 - loss: 0.6874 - acc: 0.5564
4352/5677 [=====================>........] - ETA: 2:05 - loss: 0.6871 - acc: 0.5577
4416/5677 [======================>.......] - ETA: 1:59 - loss: 0.6871 - acc: 0.5575
4480/5677 [======================>.......] - ETA: 1:53 - loss: 0.6871 - acc: 0.5576
4544/5677 [=======================>......] - ETA: 1:47 - loss: 0.6868 - acc: 0.5581
4608/5677 [=======================>......] - ETA: 1:41 - loss: 0.6870 - acc: 0.5584
4672/5677 [=======================>......] - ETA: 1:35 - loss: 0.6865 - acc: 0.5597
4736/5677 [========================>.....] - ETA: 1:29 - loss: 0.6864 - acc: 0.5602
4800/5677 [========================>.....] - ETA: 1:23 - loss: 0.6860 - acc: 0.5610
4864/5677 [========================>.....] - ETA: 1:17 - loss: 0.6862 - acc: 0.5611
4928/5677 [=========================>....] - ETA: 1:11 - loss: 0.6859 - acc: 0.5615
4992/5677 [=========================>....] - ETA: 1:05 - loss: 0.6857 - acc: 0.5613
5056/5677 [=========================>....] - ETA: 59s - loss: 0.6855 - acc: 0.5621 
5120/5677 [==========================>...] - ETA: 53s - loss: 0.6856 - acc: 0.5621
5184/5677 [==========================>...] - ETA: 47s - loss: 0.6852 - acc: 0.5631
5248/5677 [==========================>...] - ETA: 40s - loss: 0.6854 - acc: 0.5627
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6852 - acc: 0.5627
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6854 - acc: 0.5621
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6849 - acc: 0.5634
5504/5677 [============================>.] - ETA: 16s - loss: 0.6849 - acc: 0.5632
5568/5677 [============================>.] - ETA: 10s - loss: 0.6849 - acc: 0.5638
5632/5677 [============================>.] - ETA: 4s - loss: 0.6847 - acc: 0.5639 
5677/5677 [==============================] - 567s 100ms/step - loss: 0.6848 - acc: 0.5642 - val_loss: 0.6651 - val_acc: 0.6070

Epoch 00005: val_acc improved from 0.57369 to 0.60697, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window16/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 9:11 - loss: 0.6862 - acc: 0.5312
 128/5677 [..............................] - ETA: 8:57 - loss: 0.6967 - acc: 0.5703
 192/5677 [>.............................] - ETA: 9:01 - loss: 0.6926 - acc: 0.5729
 256/5677 [>.............................] - ETA: 8:48 - loss: 0.6990 - acc: 0.5547
 320/5677 [>.............................] - ETA: 8:49 - loss: 0.6928 - acc: 0.5625
 384/5677 [=>............................] - ETA: 8:39 - loss: 0.6897 - acc: 0.5651
 448/5677 [=>............................] - ETA: 8:36 - loss: 0.6876 - acc: 0.5737
 512/5677 [=>............................] - ETA: 8:28 - loss: 0.6892 - acc: 0.5742
 576/5677 [==>...........................] - ETA: 8:21 - loss: 0.6878 - acc: 0.5764
 640/5677 [==>...........................] - ETA: 8:15 - loss: 0.6860 - acc: 0.5734
 704/5677 [==>...........................] - ETA: 8:08 - loss: 0.6858 - acc: 0.5739
 768/5677 [===>..........................] - ETA: 8:04 - loss: 0.6883 - acc: 0.5716
 832/5677 [===>..........................] - ETA: 7:55 - loss: 0.6894 - acc: 0.5709
 896/5677 [===>..........................] - ETA: 7:47 - loss: 0.6913 - acc: 0.5647
 960/5677 [====>.........................] - ETA: 7:38 - loss: 0.6899 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 7:27 - loss: 0.6912 - acc: 0.5664
1088/5677 [====>.........................] - ETA: 7:16 - loss: 0.6894 - acc: 0.5699
1152/5677 [=====>........................] - ETA: 7:09 - loss: 0.6883 - acc: 0.5720
1216/5677 [=====>........................] - ETA: 7:00 - loss: 0.6869 - acc: 0.5757
1280/5677 [=====>........................] - ETA: 6:53 - loss: 0.6874 - acc: 0.5750
1344/5677 [======>.......................] - ETA: 6:48 - loss: 0.6867 - acc: 0.5744
1408/5677 [======>.......................] - ETA: 6:45 - loss: 0.6866 - acc: 0.5739
1472/5677 [======>.......................] - ETA: 6:42 - loss: 0.6872 - acc: 0.5734
1536/5677 [=======>......................] - ETA: 6:38 - loss: 0.6867 - acc: 0.5716
1600/5677 [=======>......................] - ETA: 6:34 - loss: 0.6860 - acc: 0.5737
1664/5677 [=======>......................] - ETA: 6:28 - loss: 0.6873 - acc: 0.5715
1728/5677 [========>.....................] - ETA: 6:26 - loss: 0.6874 - acc: 0.5712
1792/5677 [========>.....................] - ETA: 6:19 - loss: 0.6865 - acc: 0.5720
1856/5677 [========>.....................] - ETA: 6:14 - loss: 0.6869 - acc: 0.5727
1920/5677 [=========>....................] - ETA: 6:07 - loss: 0.6857 - acc: 0.5734
1984/5677 [=========>....................] - ETA: 6:01 - loss: 0.6861 - acc: 0.5706
2048/5677 [=========>....................] - ETA: 5:56 - loss: 0.6856 - acc: 0.5713
2112/5677 [==========>...................] - ETA: 5:48 - loss: 0.6857 - acc: 0.5710
2176/5677 [==========>...................] - ETA: 5:42 - loss: 0.6859 - acc: 0.5699
2240/5677 [==========>...................] - ETA: 5:36 - loss: 0.6860 - acc: 0.5696
2304/5677 [===========>..................] - ETA: 5:30 - loss: 0.6864 - acc: 0.5694
2368/5677 [===========>..................] - ETA: 5:24 - loss: 0.6860 - acc: 0.5705
2432/5677 [===========>..................] - ETA: 5:18 - loss: 0.6857 - acc: 0.5715
2496/5677 [============>.................] - ETA: 5:12 - loss: 0.6847 - acc: 0.5729
2560/5677 [============>.................] - ETA: 5:06 - loss: 0.6842 - acc: 0.5746
2624/5677 [============>.................] - ETA: 4:59 - loss: 0.6851 - acc: 0.5705
2688/5677 [=============>................] - ETA: 4:53 - loss: 0.6852 - acc: 0.5696
2752/5677 [=============>................] - ETA: 4:47 - loss: 0.6859 - acc: 0.5690
2816/5677 [=============>................] - ETA: 4:41 - loss: 0.6861 - acc: 0.5675
2880/5677 [==============>...............] - ETA: 4:34 - loss: 0.6865 - acc: 0.5649
2944/5677 [==============>...............] - ETA: 4:28 - loss: 0.6871 - acc: 0.5635
3008/5677 [==============>...............] - ETA: 4:22 - loss: 0.6867 - acc: 0.5635
3072/5677 [===============>..............] - ETA: 4:16 - loss: 0.6876 - acc: 0.5602
3136/5677 [===============>..............] - ETA: 4:09 - loss: 0.6879 - acc: 0.5606
3200/5677 [===============>..............] - ETA: 4:05 - loss: 0.6872 - acc: 0.5619
3264/5677 [================>.............] - ETA: 3:59 - loss: 0.6871 - acc: 0.5616
3328/5677 [================>.............] - ETA: 3:52 - loss: 0.6870 - acc: 0.5607
3392/5677 [================>.............] - ETA: 3:45 - loss: 0.6862 - acc: 0.5628
3456/5677 [=================>............] - ETA: 3:38 - loss: 0.6865 - acc: 0.5634
3520/5677 [=================>............] - ETA: 3:31 - loss: 0.6867 - acc: 0.5628
3584/5677 [=================>............] - ETA: 3:24 - loss: 0.6866 - acc: 0.5622
3648/5677 [==================>...........] - ETA: 3:17 - loss: 0.6869 - acc: 0.5617
3712/5677 [==================>...........] - ETA: 3:11 - loss: 0.6862 - acc: 0.5625
3776/5677 [==================>...........] - ETA: 3:04 - loss: 0.6860 - acc: 0.5633
3840/5677 [===================>..........] - ETA: 2:57 - loss: 0.6855 - acc: 0.5646
3904/5677 [===================>..........] - ETA: 2:51 - loss: 0.6855 - acc: 0.5630
3968/5677 [===================>..........] - ETA: 2:44 - loss: 0.6853 - acc: 0.5630
4032/5677 [====================>.........] - ETA: 2:38 - loss: 0.6857 - acc: 0.5627
4096/5677 [====================>.........] - ETA: 2:31 - loss: 0.6856 - acc: 0.5625
4160/5677 [====================>.........] - ETA: 2:25 - loss: 0.6853 - acc: 0.5627
4224/5677 [=====================>........] - ETA: 2:18 - loss: 0.6852 - acc: 0.5630
4288/5677 [=====================>........] - ETA: 2:12 - loss: 0.6848 - acc: 0.5648
4352/5677 [=====================>........] - ETA: 2:05 - loss: 0.6843 - acc: 0.5655
4416/5677 [======================>.......] - ETA: 1:59 - loss: 0.6840 - acc: 0.5661
4480/5677 [======================>.......] - ETA: 1:53 - loss: 0.6838 - acc: 0.5665
4544/5677 [=======================>......] - ETA: 1:47 - loss: 0.6837 - acc: 0.5665
4608/5677 [=======================>......] - ETA: 1:40 - loss: 0.6835 - acc: 0.5664
4672/5677 [=======================>......] - ETA: 1:34 - loss: 0.6841 - acc: 0.5653
4736/5677 [========================>.....] - ETA: 1:28 - loss: 0.6841 - acc: 0.5648
4800/5677 [========================>.....] - ETA: 1:22 - loss: 0.6839 - acc: 0.5654
4864/5677 [========================>.....] - ETA: 1:16 - loss: 0.6835 - acc: 0.5666
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.6830 - acc: 0.5676
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6825 - acc: 0.5689
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6828 - acc: 0.5680 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6824 - acc: 0.5691
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6826 - acc: 0.5685
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6822 - acc: 0.5690
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6818 - acc: 0.5691
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6818 - acc: 0.5698
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6819 - acc: 0.5691
5504/5677 [============================>.] - ETA: 15s - loss: 0.6817 - acc: 0.5690
5568/5677 [============================>.] - ETA: 10s - loss: 0.6816 - acc: 0.5688
5632/5677 [============================>.] - ETA: 4s - loss: 0.6814 - acc: 0.5687 
5677/5677 [==============================] - 538s 95ms/step - loss: 0.6811 - acc: 0.5693 - val_loss: 0.6662 - val_acc: 0.6054

Epoch 00006: val_acc did not improve from 0.60697
Epoch 7/10

  64/5677 [..............................] - ETA: 7:39 - loss: 0.6229 - acc: 0.6875
 128/5677 [..............................] - ETA: 7:38 - loss: 0.6448 - acc: 0.6250
 192/5677 [>.............................] - ETA: 7:42 - loss: 0.6482 - acc: 0.6146
 256/5677 [>.............................] - ETA: 7:26 - loss: 0.6450 - acc: 0.6133
 320/5677 [>.............................] - ETA: 7:19 - loss: 0.6521 - acc: 0.6031
 384/5677 [=>............................] - ETA: 7:13 - loss: 0.6524 - acc: 0.6068
 448/5677 [=>............................] - ETA: 7:07 - loss: 0.6542 - acc: 0.6027
 512/5677 [=>............................] - ETA: 6:59 - loss: 0.6554 - acc: 0.6016
 576/5677 [==>...........................] - ETA: 6:57 - loss: 0.6579 - acc: 0.5990
 640/5677 [==>...........................] - ETA: 6:49 - loss: 0.6612 - acc: 0.5813
 704/5677 [==>...........................] - ETA: 6:43 - loss: 0.6636 - acc: 0.5795
 768/5677 [===>..........................] - ETA: 6:37 - loss: 0.6646 - acc: 0.5807
 832/5677 [===>..........................] - ETA: 6:32 - loss: 0.6659 - acc: 0.5805
 896/5677 [===>..........................] - ETA: 6:25 - loss: 0.6688 - acc: 0.5781
 960/5677 [====>.........................] - ETA: 6:20 - loss: 0.6697 - acc: 0.5792
1024/5677 [====>.........................] - ETA: 6:14 - loss: 0.6709 - acc: 0.5742
1088/5677 [====>.........................] - ETA: 6:09 - loss: 0.6693 - acc: 0.5809
1152/5677 [=====>........................] - ETA: 6:04 - loss: 0.6675 - acc: 0.5842
1216/5677 [=====>........................] - ETA: 5:59 - loss: 0.6696 - acc: 0.5806
1280/5677 [=====>........................] - ETA: 5:55 - loss: 0.6690 - acc: 0.5813
1344/5677 [======>.......................] - ETA: 5:50 - loss: 0.6715 - acc: 0.5766
1408/5677 [======>.......................] - ETA: 5:45 - loss: 0.6711 - acc: 0.5803
1472/5677 [======>.......................] - ETA: 5:41 - loss: 0.6730 - acc: 0.5761
1536/5677 [=======>......................] - ETA: 5:36 - loss: 0.6750 - acc: 0.5755
1600/5677 [=======>......................] - ETA: 5:31 - loss: 0.6756 - acc: 0.5750
1664/5677 [=======>......................] - ETA: 5:29 - loss: 0.6766 - acc: 0.5709
1728/5677 [========>.....................] - ETA: 5:23 - loss: 0.6756 - acc: 0.5735
1792/5677 [========>.....................] - ETA: 5:19 - loss: 0.6750 - acc: 0.5753
1856/5677 [========>.....................] - ETA: 5:15 - loss: 0.6766 - acc: 0.5700
1920/5677 [=========>....................] - ETA: 5:10 - loss: 0.6782 - acc: 0.5651
1984/5677 [=========>....................] - ETA: 5:06 - loss: 0.6789 - acc: 0.5645
2048/5677 [=========>....................] - ETA: 5:02 - loss: 0.6796 - acc: 0.5645
2112/5677 [==========>...................] - ETA: 4:56 - loss: 0.6798 - acc: 0.5649
2176/5677 [==========>...................] - ETA: 4:52 - loss: 0.6794 - acc: 0.5671
2240/5677 [==========>...................] - ETA: 4:48 - loss: 0.6806 - acc: 0.5634
2304/5677 [===========>..................] - ETA: 4:43 - loss: 0.6804 - acc: 0.5625
2368/5677 [===========>..................] - ETA: 4:37 - loss: 0.6801 - acc: 0.5629
2432/5677 [===========>..................] - ETA: 4:34 - loss: 0.6797 - acc: 0.5666
2496/5677 [============>.................] - ETA: 4:30 - loss: 0.6802 - acc: 0.5645
2560/5677 [============>.................] - ETA: 4:25 - loss: 0.6802 - acc: 0.5648
2624/5677 [============>.................] - ETA: 4:19 - loss: 0.6801 - acc: 0.5659
2688/5677 [=============>................] - ETA: 4:15 - loss: 0.6801 - acc: 0.5658
2752/5677 [=============>................] - ETA: 4:11 - loss: 0.6796 - acc: 0.5672
2816/5677 [=============>................] - ETA: 4:06 - loss: 0.6803 - acc: 0.5650
2880/5677 [==============>...............] - ETA: 4:00 - loss: 0.6801 - acc: 0.5656
2944/5677 [==============>...............] - ETA: 3:54 - loss: 0.6796 - acc: 0.5669
3008/5677 [==============>...............] - ETA: 3:48 - loss: 0.6800 - acc: 0.5665
3072/5677 [===============>..............] - ETA: 3:44 - loss: 0.6796 - acc: 0.5680
3136/5677 [===============>..............] - ETA: 3:39 - loss: 0.6794 - acc: 0.5676
3200/5677 [===============>..............] - ETA: 3:34 - loss: 0.6790 - acc: 0.5697
3264/5677 [================>.............] - ETA: 3:29 - loss: 0.6787 - acc: 0.5708
3328/5677 [================>.............] - ETA: 3:23 - loss: 0.6786 - acc: 0.5718
3392/5677 [================>.............] - ETA: 3:18 - loss: 0.6794 - acc: 0.5699
3456/5677 [=================>............] - ETA: 3:12 - loss: 0.6789 - acc: 0.5709
3520/5677 [=================>............] - ETA: 3:06 - loss: 0.6790 - acc: 0.5705
3584/5677 [=================>............] - ETA: 3:02 - loss: 0.6790 - acc: 0.5703
3648/5677 [==================>...........] - ETA: 2:57 - loss: 0.6790 - acc: 0.5696
3712/5677 [==================>...........] - ETA: 2:52 - loss: 0.6793 - acc: 0.5692
3776/5677 [==================>...........] - ETA: 2:47 - loss: 0.6792 - acc: 0.5699
3840/5677 [===================>..........] - ETA: 2:42 - loss: 0.6788 - acc: 0.5698
3904/5677 [===================>..........] - ETA: 2:36 - loss: 0.6786 - acc: 0.5694
3968/5677 [===================>..........] - ETA: 2:30 - loss: 0.6793 - acc: 0.5683
4032/5677 [====================>.........] - ETA: 2:24 - loss: 0.6800 - acc: 0.5665
4096/5677 [====================>.........] - ETA: 2:18 - loss: 0.6802 - acc: 0.5662
4160/5677 [====================>.........] - ETA: 2:13 - loss: 0.6805 - acc: 0.5649
4224/5677 [=====================>........] - ETA: 2:08 - loss: 0.6803 - acc: 0.5656
4288/5677 [=====================>........] - ETA: 2:03 - loss: 0.6802 - acc: 0.5658
4352/5677 [=====================>........] - ETA: 1:57 - loss: 0.6805 - acc: 0.5657
4416/5677 [======================>.......] - ETA: 1:52 - loss: 0.6805 - acc: 0.5650
4480/5677 [======================>.......] - ETA: 1:46 - loss: 0.6808 - acc: 0.5647
4544/5677 [=======================>......] - ETA: 1:41 - loss: 0.6806 - acc: 0.5645
4608/5677 [=======================>......] - ETA: 1:35 - loss: 0.6810 - acc: 0.5640
4672/5677 [=======================>......] - ETA: 1:30 - loss: 0.6810 - acc: 0.5642
4736/5677 [========================>.....] - ETA: 1:24 - loss: 0.6807 - acc: 0.5657
4800/5677 [========================>.....] - ETA: 1:18 - loss: 0.6808 - acc: 0.5656
4864/5677 [========================>.....] - ETA: 1:13 - loss: 0.6806 - acc: 0.5662
4928/5677 [=========================>....] - ETA: 1:07 - loss: 0.6803 - acc: 0.5666
4992/5677 [=========================>....] - ETA: 1:01 - loss: 0.6804 - acc: 0.5671
5056/5677 [=========================>....] - ETA: 56s - loss: 0.6801 - acc: 0.5682 
5120/5677 [==========================>...] - ETA: 50s - loss: 0.6806 - acc: 0.5674
5184/5677 [==========================>...] - ETA: 44s - loss: 0.6809 - acc: 0.5671
5248/5677 [==========================>...] - ETA: 38s - loss: 0.6806 - acc: 0.5671
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6807 - acc: 0.5668
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6804 - acc: 0.5681
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6806 - acc: 0.5673
5504/5677 [============================>.] - ETA: 15s - loss: 0.6803 - acc: 0.5678
5568/5677 [============================>.] - ETA: 9s - loss: 0.6804 - acc: 0.5681 
5632/5677 [============================>.] - ETA: 4s - loss: 0.6806 - acc: 0.5676
5677/5677 [==============================] - 540s 95ms/step - loss: 0.6806 - acc: 0.5676 - val_loss: 0.6665 - val_acc: 0.5990

Epoch 00007: val_acc did not improve from 0.60697
Epoch 8/10

  64/5677 [..............................] - ETA: 9:41 - loss: 0.7009 - acc: 0.5312
 128/5677 [..............................] - ETA: 9:32 - loss: 0.6847 - acc: 0.5781
 192/5677 [>.............................] - ETA: 9:24 - loss: 0.6807 - acc: 0.5833
 256/5677 [>.............................] - ETA: 9:12 - loss: 0.6911 - acc: 0.5352
 320/5677 [>.............................] - ETA: 9:01 - loss: 0.6935 - acc: 0.5406
 384/5677 [=>............................] - ETA: 8:51 - loss: 0.6936 - acc: 0.5443
 448/5677 [=>............................] - ETA: 8:54 - loss: 0.6943 - acc: 0.5446
 512/5677 [=>............................] - ETA: 8:41 - loss: 0.6904 - acc: 0.5566
 576/5677 [==>...........................] - ETA: 8:28 - loss: 0.6844 - acc: 0.5694
 640/5677 [==>...........................] - ETA: 8:14 - loss: 0.6841 - acc: 0.5609
 704/5677 [==>...........................] - ETA: 8:00 - loss: 0.6795 - acc: 0.5682
 768/5677 [===>..........................] - ETA: 7:48 - loss: 0.6766 - acc: 0.5755
 832/5677 [===>..........................] - ETA: 7:36 - loss: 0.6818 - acc: 0.5673
 896/5677 [===>..........................] - ETA: 7:26 - loss: 0.6839 - acc: 0.5625
 960/5677 [====>.........................] - ETA: 7:15 - loss: 0.6861 - acc: 0.5594
1024/5677 [====>.........................] - ETA: 7:08 - loss: 0.6850 - acc: 0.5576
1088/5677 [====>.........................] - ETA: 6:59 - loss: 0.6834 - acc: 0.5634
1152/5677 [=====>........................] - ETA: 6:52 - loss: 0.6797 - acc: 0.5677
1216/5677 [=====>........................] - ETA: 6:43 - loss: 0.6813 - acc: 0.5650
1280/5677 [=====>........................] - ETA: 6:37 - loss: 0.6830 - acc: 0.5594
1344/5677 [======>.......................] - ETA: 6:28 - loss: 0.6828 - acc: 0.5632
1408/5677 [======>.......................] - ETA: 6:21 - loss: 0.6819 - acc: 0.5661
1472/5677 [======>.......................] - ETA: 6:13 - loss: 0.6811 - acc: 0.5693
1536/5677 [=======>......................] - ETA: 6:07 - loss: 0.6807 - acc: 0.5710
1600/5677 [=======>......................] - ETA: 5:59 - loss: 0.6814 - acc: 0.5694
1664/5677 [=======>......................] - ETA: 5:52 - loss: 0.6806 - acc: 0.5697
1728/5677 [========>.....................] - ETA: 5:46 - loss: 0.6803 - acc: 0.5706
1792/5677 [========>.....................] - ETA: 5:39 - loss: 0.6808 - acc: 0.5681
1856/5677 [========>.....................] - ETA: 5:33 - loss: 0.6809 - acc: 0.5663
1920/5677 [=========>....................] - ETA: 5:27 - loss: 0.6795 - acc: 0.5698
1984/5677 [=========>....................] - ETA: 5:21 - loss: 0.6798 - acc: 0.5696
2048/5677 [=========>....................] - ETA: 5:15 - loss: 0.6797 - acc: 0.5713
2112/5677 [==========>...................] - ETA: 5:09 - loss: 0.6797 - acc: 0.5687
2176/5677 [==========>...................] - ETA: 5:03 - loss: 0.6808 - acc: 0.5666
2240/5677 [==========>...................] - ETA: 4:57 - loss: 0.6804 - acc: 0.5670
2304/5677 [===========>..................] - ETA: 4:50 - loss: 0.6802 - acc: 0.5664
2368/5677 [===========>..................] - ETA: 4:45 - loss: 0.6804 - acc: 0.5655
2432/5677 [===========>..................] - ETA: 4:39 - loss: 0.6809 - acc: 0.5650
2496/5677 [============>.................] - ETA: 4:33 - loss: 0.6801 - acc: 0.5681
2560/5677 [============>.................] - ETA: 4:28 - loss: 0.6807 - acc: 0.5660
2624/5677 [============>.................] - ETA: 4:23 - loss: 0.6803 - acc: 0.5655
2688/5677 [=============>................] - ETA: 4:17 - loss: 0.6809 - acc: 0.5647
2752/5677 [=============>................] - ETA: 4:12 - loss: 0.6807 - acc: 0.5665
2816/5677 [=============>................] - ETA: 4:07 - loss: 0.6804 - acc: 0.5664
2880/5677 [==============>...............] - ETA: 4:02 - loss: 0.6806 - acc: 0.5649
2944/5677 [==============>...............] - ETA: 3:58 - loss: 0.6806 - acc: 0.5652
3008/5677 [==============>...............] - ETA: 3:52 - loss: 0.6800 - acc: 0.5675
3072/5677 [===============>..............] - ETA: 3:47 - loss: 0.6810 - acc: 0.5654
3136/5677 [===============>..............] - ETA: 3:42 - loss: 0.6809 - acc: 0.5660
3200/5677 [===============>..............] - ETA: 3:37 - loss: 0.6806 - acc: 0.5672
3264/5677 [================>.............] - ETA: 3:32 - loss: 0.6809 - acc: 0.5674
3328/5677 [================>.............] - ETA: 3:26 - loss: 0.6808 - acc: 0.5676
3392/5677 [================>.............] - ETA: 3:20 - loss: 0.6800 - acc: 0.5693
3456/5677 [=================>............] - ETA: 3:15 - loss: 0.6804 - acc: 0.5694
3520/5677 [=================>............] - ETA: 3:10 - loss: 0.6800 - acc: 0.5699
3584/5677 [=================>............] - ETA: 3:04 - loss: 0.6804 - acc: 0.5684
3648/5677 [==================>...........] - ETA: 2:58 - loss: 0.6804 - acc: 0.5680
3712/5677 [==================>...........] - ETA: 2:53 - loss: 0.6805 - acc: 0.5682
3776/5677 [==================>...........] - ETA: 2:47 - loss: 0.6810 - acc: 0.5673
3840/5677 [===================>..........] - ETA: 2:41 - loss: 0.6809 - acc: 0.5664
3904/5677 [===================>..........] - ETA: 2:35 - loss: 0.6804 - acc: 0.5681
3968/5677 [===================>..........] - ETA: 2:30 - loss: 0.6803 - acc: 0.5688
4032/5677 [====================>.........] - ETA: 2:24 - loss: 0.6799 - acc: 0.5707
4096/5677 [====================>.........] - ETA: 2:18 - loss: 0.6797 - acc: 0.5708
4160/5677 [====================>.........] - ETA: 2:12 - loss: 0.6801 - acc: 0.5714
4224/5677 [=====================>........] - ETA: 2:06 - loss: 0.6796 - acc: 0.5720
4288/5677 [=====================>........] - ETA: 2:01 - loss: 0.6798 - acc: 0.5721
4352/5677 [=====================>........] - ETA: 1:56 - loss: 0.6797 - acc: 0.5726
4416/5677 [======================>.......] - ETA: 1:50 - loss: 0.6805 - acc: 0.5711
4480/5677 [======================>.......] - ETA: 1:44 - loss: 0.6796 - acc: 0.5734
4544/5677 [=======================>......] - ETA: 1:39 - loss: 0.6788 - acc: 0.5748
4608/5677 [=======================>......] - ETA: 1:33 - loss: 0.6785 - acc: 0.5744
4672/5677 [=======================>......] - ETA: 1:27 - loss: 0.6783 - acc: 0.5751
4736/5677 [========================>.....] - ETA: 1:22 - loss: 0.6778 - acc: 0.5760
4800/5677 [========================>.....] - ETA: 1:16 - loss: 0.6777 - acc: 0.5758
4864/5677 [========================>.....] - ETA: 1:10 - loss: 0.6787 - acc: 0.5744
4928/5677 [=========================>....] - ETA: 1:05 - loss: 0.6792 - acc: 0.5737
4992/5677 [=========================>....] - ETA: 59s - loss: 0.6796 - acc: 0.5733 
5056/5677 [=========================>....] - ETA: 54s - loss: 0.6802 - acc: 0.5720
5120/5677 [==========================>...] - ETA: 48s - loss: 0.6799 - acc: 0.5729
5184/5677 [==========================>...] - ETA: 42s - loss: 0.6800 - acc: 0.5721
5248/5677 [==========================>...] - ETA: 37s - loss: 0.6801 - acc: 0.5715
5312/5677 [===========================>..] - ETA: 31s - loss: 0.6798 - acc: 0.5719
5376/5677 [===========================>..] - ETA: 26s - loss: 0.6802 - acc: 0.5705
5440/5677 [===========================>..] - ETA: 20s - loss: 0.6803 - acc: 0.5702
5504/5677 [============================>.] - ETA: 14s - loss: 0.6802 - acc: 0.5709
5568/5677 [============================>.] - ETA: 9s - loss: 0.6800 - acc: 0.5713 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6801 - acc: 0.5708
5677/5677 [==============================] - 512s 90ms/step - loss: 0.6799 - acc: 0.5716 - val_loss: 0.6760 - val_acc: 0.5594

Epoch 00008: val_acc did not improve from 0.60697
Epoch 9/10

  64/5677 [..............................] - ETA: 8:26 - loss: 0.6761 - acc: 0.6406
 128/5677 [..............................] - ETA: 9:04 - loss: 0.6836 - acc: 0.5781
 192/5677 [>.............................] - ETA: 8:50 - loss: 0.6843 - acc: 0.5521
 256/5677 [>.............................] - ETA: 8:47 - loss: 0.6845 - acc: 0.5664
 320/5677 [>.............................] - ETA: 8:30 - loss: 0.6852 - acc: 0.5594
 384/5677 [=>............................] - ETA: 8:07 - loss: 0.6830 - acc: 0.5651
 448/5677 [=>............................] - ETA: 8:05 - loss: 0.6862 - acc: 0.5580
 512/5677 [=>............................] - ETA: 8:01 - loss: 0.6841 - acc: 0.5605
 576/5677 [==>...........................] - ETA: 7:56 - loss: 0.6845 - acc: 0.5590
 640/5677 [==>...........................] - ETA: 7:40 - loss: 0.6884 - acc: 0.5484
 704/5677 [==>...........................] - ETA: 7:38 - loss: 0.6877 - acc: 0.5469
 768/5677 [===>..........................] - ETA: 7:39 - loss: 0.6857 - acc: 0.5495
 832/5677 [===>..........................] - ETA: 7:31 - loss: 0.6856 - acc: 0.5505
 896/5677 [===>..........................] - ETA: 7:21 - loss: 0.6836 - acc: 0.5580
 960/5677 [====>.........................] - ETA: 7:13 - loss: 0.6841 - acc: 0.5615
1024/5677 [====>.........................] - ETA: 7:04 - loss: 0.6847 - acc: 0.5576
1088/5677 [====>.........................] - ETA: 7:07 - loss: 0.6825 - acc: 0.5671
1152/5677 [=====>........................] - ETA: 7:02 - loss: 0.6840 - acc: 0.5651
1216/5677 [=====>........................] - ETA: 6:54 - loss: 0.6835 - acc: 0.5658
1280/5677 [=====>........................] - ETA: 6:45 - loss: 0.6827 - acc: 0.5672
1344/5677 [======>.......................] - ETA: 6:36 - loss: 0.6828 - acc: 0.5670
1408/5677 [======>.......................] - ETA: 6:28 - loss: 0.6835 - acc: 0.5653
1472/5677 [======>.......................] - ETA: 6:23 - loss: 0.6834 - acc: 0.5673
1536/5677 [=======>......................] - ETA: 6:22 - loss: 0.6839 - acc: 0.5618
1600/5677 [=======>......................] - ETA: 6:17 - loss: 0.6835 - acc: 0.5625
1664/5677 [=======>......................] - ETA: 6:11 - loss: 0.6829 - acc: 0.5643
1728/5677 [========>.....................] - ETA: 6:02 - loss: 0.6830 - acc: 0.5619
1792/5677 [========>.....................] - ETA: 5:54 - loss: 0.6823 - acc: 0.5631
1856/5677 [========>.....................] - ETA: 5:46 - loss: 0.6822 - acc: 0.5647
1920/5677 [=========>....................] - ETA: 5:39 - loss: 0.6806 - acc: 0.5667
1984/5677 [=========>....................] - ETA: 5:35 - loss: 0.6803 - acc: 0.5675
2048/5677 [=========>....................] - ETA: 5:32 - loss: 0.6793 - acc: 0.5713
2112/5677 [==========>...................] - ETA: 5:27 - loss: 0.6784 - acc: 0.5729
2176/5677 [==========>...................] - ETA: 5:23 - loss: 0.6772 - acc: 0.5758
2240/5677 [==========>...................] - ETA: 5:17 - loss: 0.6762 - acc: 0.5759
2304/5677 [===========>..................] - ETA: 5:10 - loss: 0.6754 - acc: 0.5768
2368/5677 [===========>..................] - ETA: 5:04 - loss: 0.6755 - acc: 0.5760
2432/5677 [===========>..................] - ETA: 4:57 - loss: 0.6757 - acc: 0.5769
2496/5677 [============>.................] - ETA: 4:50 - loss: 0.6758 - acc: 0.5765
2560/5677 [============>.................] - ETA: 4:45 - loss: 0.6762 - acc: 0.5773
2624/5677 [============>.................] - ETA: 4:41 - loss: 0.6755 - acc: 0.5796
2688/5677 [=============>................] - ETA: 4:36 - loss: 0.6765 - acc: 0.5785
2752/5677 [=============>................] - ETA: 4:30 - loss: 0.6761 - acc: 0.5792
2816/5677 [=============>................] - ETA: 4:24 - loss: 0.6757 - acc: 0.5795
2880/5677 [==============>...............] - ETA: 4:17 - loss: 0.6750 - acc: 0.5809
2944/5677 [==============>...............] - ETA: 4:11 - loss: 0.6749 - acc: 0.5815
3008/5677 [==============>...............] - ETA: 4:04 - loss: 0.6751 - acc: 0.5818
3072/5677 [===============>..............] - ETA: 3:58 - loss: 0.6754 - acc: 0.5814
3136/5677 [===============>..............] - ETA: 3:54 - loss: 0.6758 - acc: 0.5810
3200/5677 [===============>..............] - ETA: 3:49 - loss: 0.6765 - acc: 0.5784
3264/5677 [================>.............] - ETA: 3:43 - loss: 0.6768 - acc: 0.5781
3328/5677 [================>.............] - ETA: 3:38 - loss: 0.6775 - acc: 0.5769
3392/5677 [================>.............] - ETA: 3:32 - loss: 0.6767 - acc: 0.5787
3456/5677 [=================>............] - ETA: 3:26 - loss: 0.6764 - acc: 0.5796
3520/5677 [=================>............] - ETA: 3:21 - loss: 0.6764 - acc: 0.5793
3584/5677 [=================>............] - ETA: 3:15 - loss: 0.6769 - acc: 0.5770
3648/5677 [==================>...........] - ETA: 3:09 - loss: 0.6768 - acc: 0.5773
3712/5677 [==================>...........] - ETA: 3:04 - loss: 0.6773 - acc: 0.5765
3776/5677 [==================>...........] - ETA: 2:58 - loss: 0.6777 - acc: 0.5747
3840/5677 [===================>..........] - ETA: 2:52 - loss: 0.6778 - acc: 0.5737
3904/5677 [===================>..........] - ETA: 2:47 - loss: 0.6779 - acc: 0.5733
3968/5677 [===================>..........] - ETA: 2:41 - loss: 0.6779 - acc: 0.5733
4032/5677 [====================>.........] - ETA: 2:35 - loss: 0.6778 - acc: 0.5732
4096/5677 [====================>.........] - ETA: 2:29 - loss: 0.6771 - acc: 0.5747
4160/5677 [====================>.........] - ETA: 2:23 - loss: 0.6770 - acc: 0.5755
4224/5677 [=====================>........] - ETA: 2:17 - loss: 0.6768 - acc: 0.5758
4288/5677 [=====================>........] - ETA: 2:11 - loss: 0.6778 - acc: 0.5737
4352/5677 [=====================>........] - ETA: 2:05 - loss: 0.6783 - acc: 0.5733
4416/5677 [======================>.......] - ETA: 1:59 - loss: 0.6783 - acc: 0.5745
4480/5677 [======================>.......] - ETA: 1:53 - loss: 0.6779 - acc: 0.5752
4544/5677 [=======================>......] - ETA: 1:47 - loss: 0.6778 - acc: 0.5755
4608/5677 [=======================>......] - ETA: 1:41 - loss: 0.6780 - acc: 0.5755
4672/5677 [=======================>......] - ETA: 1:35 - loss: 0.6779 - acc: 0.5751
4736/5677 [========================>.....] - ETA: 1:29 - loss: 0.6778 - acc: 0.5754
4800/5677 [========================>.....] - ETA: 1:23 - loss: 0.6780 - acc: 0.5750
4864/5677 [========================>.....] - ETA: 1:17 - loss: 0.6782 - acc: 0.5750
4928/5677 [=========================>....] - ETA: 1:11 - loss: 0.6786 - acc: 0.5747
4992/5677 [=========================>....] - ETA: 1:05 - loss: 0.6785 - acc: 0.5755
5056/5677 [=========================>....] - ETA: 59s - loss: 0.6786 - acc: 0.5748 
5120/5677 [==========================>...] - ETA: 53s - loss: 0.6790 - acc: 0.5742
5184/5677 [==========================>...] - ETA: 47s - loss: 0.6788 - acc: 0.5747
5248/5677 [==========================>...] - ETA: 40s - loss: 0.6786 - acc: 0.5753
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6787 - acc: 0.5751
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6785 - acc: 0.5755
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6783 - acc: 0.5757
5504/5677 [============================>.] - ETA: 16s - loss: 0.6781 - acc: 0.5767
5568/5677 [============================>.] - ETA: 10s - loss: 0.6776 - acc: 0.5776
5632/5677 [============================>.] - ETA: 4s - loss: 0.6778 - acc: 0.5771 
5677/5677 [==============================] - 562s 99ms/step - loss: 0.6780 - acc: 0.5769 - val_loss: 0.6990 - val_acc: 0.5119

Epoch 00009: val_acc did not improve from 0.60697
Epoch 10/10

  64/5677 [..............................] - ETA: 9:10 - loss: 0.6610 - acc: 0.5781
 128/5677 [..............................] - ETA: 9:24 - loss: 0.6677 - acc: 0.5703
 192/5677 [>.............................] - ETA: 9:21 - loss: 0.6735 - acc: 0.5469
 256/5677 [>.............................] - ETA: 9:11 - loss: 0.6723 - acc: 0.5586
 320/5677 [>.............................] - ETA: 8:56 - loss: 0.6669 - acc: 0.5781
 384/5677 [=>............................] - ETA: 8:45 - loss: 0.6655 - acc: 0.5781
 448/5677 [=>............................] - ETA: 8:34 - loss: 0.6622 - acc: 0.5871
 512/5677 [=>............................] - ETA: 8:26 - loss: 0.6652 - acc: 0.5918
 576/5677 [==>...........................] - ETA: 8:19 - loss: 0.6622 - acc: 0.5955
 640/5677 [==>...........................] - ETA: 8:13 - loss: 0.6648 - acc: 0.5906
 704/5677 [==>...........................] - ETA: 8:04 - loss: 0.6633 - acc: 0.5938
 768/5677 [===>..........................] - ETA: 7:58 - loss: 0.6616 - acc: 0.5924
 832/5677 [===>..........................] - ETA: 7:49 - loss: 0.6590 - acc: 0.5962
 896/5677 [===>..........................] - ETA: 7:48 - loss: 0.6585 - acc: 0.6016
 960/5677 [====>.........................] - ETA: 7:36 - loss: 0.6593 - acc: 0.6031
1024/5677 [====>.........................] - ETA: 7:28 - loss: 0.6609 - acc: 0.6016
1088/5677 [====>.........................] - ETA: 7:17 - loss: 0.6627 - acc: 0.5983
1152/5677 [=====>........................] - ETA: 7:06 - loss: 0.6629 - acc: 0.5990
1216/5677 [=====>........................] - ETA: 6:56 - loss: 0.6629 - acc: 0.5979
1280/5677 [=====>........................] - ETA: 6:47 - loss: 0.6630 - acc: 0.5992
1344/5677 [======>.......................] - ETA: 6:38 - loss: 0.6633 - acc: 0.5975
1408/5677 [======>.......................] - ETA: 6:30 - loss: 0.6666 - acc: 0.5923
1472/5677 [======>.......................] - ETA: 6:22 - loss: 0.6674 - acc: 0.5938
1536/5677 [=======>......................] - ETA: 6:14 - loss: 0.6697 - acc: 0.5911
1600/5677 [=======>......................] - ETA: 6:07 - loss: 0.6680 - acc: 0.5950
1664/5677 [=======>......................] - ETA: 6:00 - loss: 0.6680 - acc: 0.5931
1728/5677 [========>.....................] - ETA: 5:53 - loss: 0.6684 - acc: 0.5943
1792/5677 [========>.....................] - ETA: 5:45 - loss: 0.6696 - acc: 0.5915
1856/5677 [========>.....................] - ETA: 5:39 - loss: 0.6687 - acc: 0.5927
1920/5677 [=========>....................] - ETA: 5:32 - loss: 0.6685 - acc: 0.5927
1984/5677 [=========>....................] - ETA: 5:26 - loss: 0.6680 - acc: 0.5932
2048/5677 [=========>....................] - ETA: 5:19 - loss: 0.6685 - acc: 0.5913
2112/5677 [==========>...................] - ETA: 5:14 - loss: 0.6690 - acc: 0.5904
2176/5677 [==========>...................] - ETA: 5:07 - loss: 0.6704 - acc: 0.5869
2240/5677 [==========>...................] - ETA: 5:02 - loss: 0.6713 - acc: 0.5844
2304/5677 [===========>..................] - ETA: 4:55 - loss: 0.6719 - acc: 0.5833
2368/5677 [===========>..................] - ETA: 4:48 - loss: 0.6719 - acc: 0.5840
2432/5677 [===========>..................] - ETA: 4:41 - loss: 0.6720 - acc: 0.5851
2496/5677 [============>.................] - ETA: 4:35 - loss: 0.6715 - acc: 0.5861
2560/5677 [============>.................] - ETA: 4:29 - loss: 0.6717 - acc: 0.5852
2624/5677 [============>.................] - ETA: 4:22 - loss: 0.6717 - acc: 0.5846
2688/5677 [=============>................] - ETA: 4:16 - loss: 0.6715 - acc: 0.5863
2752/5677 [=============>................] - ETA: 4:10 - loss: 0.6722 - acc: 0.5850
2816/5677 [=============>................] - ETA: 4:04 - loss: 0.6724 - acc: 0.5859
2880/5677 [==============>...............] - ETA: 3:58 - loss: 0.6723 - acc: 0.5872
2944/5677 [==============>...............] - ETA: 3:52 - loss: 0.6731 - acc: 0.5853
3008/5677 [==============>...............] - ETA: 3:46 - loss: 0.6735 - acc: 0.5851
3072/5677 [===============>..............] - ETA: 3:40 - loss: 0.6736 - acc: 0.5850
3136/5677 [===============>..............] - ETA: 3:34 - loss: 0.6733 - acc: 0.5845
3200/5677 [===============>..............] - ETA: 3:28 - loss: 0.6736 - acc: 0.5825
3264/5677 [================>.............] - ETA: 3:23 - loss: 0.6739 - acc: 0.5806
3328/5677 [================>.............] - ETA: 3:17 - loss: 0.6736 - acc: 0.5814
3392/5677 [================>.............] - ETA: 3:11 - loss: 0.6738 - acc: 0.5799
3456/5677 [=================>............] - ETA: 3:06 - loss: 0.6739 - acc: 0.5802
3520/5677 [=================>............] - ETA: 3:00 - loss: 0.6738 - acc: 0.5804
3584/5677 [=================>............] - ETA: 2:55 - loss: 0.6732 - acc: 0.5812
3648/5677 [==================>...........] - ETA: 2:49 - loss: 0.6733 - acc: 0.5820
3712/5677 [==================>...........] - ETA: 2:44 - loss: 0.6739 - acc: 0.5805
3776/5677 [==================>...........] - ETA: 2:38 - loss: 0.6740 - acc: 0.5800
3840/5677 [===================>..........] - ETA: 2:33 - loss: 0.6745 - acc: 0.5789
3904/5677 [===================>..........] - ETA: 2:27 - loss: 0.6745 - acc: 0.5786
3968/5677 [===================>..........] - ETA: 2:21 - loss: 0.6747 - acc: 0.5784
4032/5677 [====================>.........] - ETA: 2:16 - loss: 0.6743 - acc: 0.5789
4096/5677 [====================>.........] - ETA: 2:10 - loss: 0.6745 - acc: 0.5784
4160/5677 [====================>.........] - ETA: 2:05 - loss: 0.6749 - acc: 0.5781
4224/5677 [=====================>........] - ETA: 1:59 - loss: 0.6754 - acc: 0.5774
4288/5677 [=====================>........] - ETA: 1:54 - loss: 0.6756 - acc: 0.5767
4352/5677 [=====================>........] - ETA: 1:49 - loss: 0.6760 - acc: 0.5756
4416/5677 [======================>.......] - ETA: 1:44 - loss: 0.6756 - acc: 0.5761
4480/5677 [======================>.......] - ETA: 1:38 - loss: 0.6751 - acc: 0.5772
4544/5677 [=======================>......] - ETA: 1:33 - loss: 0.6747 - acc: 0.5777
4608/5677 [=======================>......] - ETA: 1:28 - loss: 0.6751 - acc: 0.5768
4672/5677 [=======================>......] - ETA: 1:23 - loss: 0.6753 - acc: 0.5766
4736/5677 [========================>.....] - ETA: 1:17 - loss: 0.6755 - acc: 0.5762
4800/5677 [========================>.....] - ETA: 1:12 - loss: 0.6760 - acc: 0.5752
4864/5677 [========================>.....] - ETA: 1:07 - loss: 0.6760 - acc: 0.5757
4928/5677 [=========================>....] - ETA: 1:02 - loss: 0.6757 - acc: 0.5767
4992/5677 [=========================>....] - ETA: 56s - loss: 0.6758 - acc: 0.5759 
5056/5677 [=========================>....] - ETA: 51s - loss: 0.6763 - acc: 0.5752
5120/5677 [==========================>...] - ETA: 46s - loss: 0.6763 - acc: 0.5754
5184/5677 [==========================>...] - ETA: 41s - loss: 0.6766 - acc: 0.5748
5248/5677 [==========================>...] - ETA: 35s - loss: 0.6770 - acc: 0.5743
5312/5677 [===========================>..] - ETA: 30s - loss: 0.6769 - acc: 0.5749
5376/5677 [===========================>..] - ETA: 25s - loss: 0.6768 - acc: 0.5755
5440/5677 [===========================>..] - ETA: 19s - loss: 0.6773 - acc: 0.5746
5504/5677 [============================>.] - ETA: 14s - loss: 0.6774 - acc: 0.5747
5568/5677 [============================>.] - ETA: 9s - loss: 0.6774 - acc: 0.5754 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6770 - acc: 0.5762
5677/5677 [==============================] - 494s 87ms/step - loss: 0.6772 - acc: 0.5762 - val_loss: 0.6654 - val_acc: 0.5911

Epoch 00010: val_acc did not improve from 0.60697
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe76b4bfd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe76b4bfd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe76b3e06d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe76b3e06d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75aed1550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75aed1550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75ae7be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75ae7be10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe75ade8750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe75ade8750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75ac767d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75ac767d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75ad77590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75ad77590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75add2b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75add2b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe743b41f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe743b41f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe75adbce10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe75adbce10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe752a92d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe752a92d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe743b41a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe743b41a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75adb1a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75adb1a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75294cb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75294cb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7529d9750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7529d9750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75ab38890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75ab38890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe752a2aa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe752a2aa10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe11413c290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe11413c290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75294a4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75294a4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7524f3f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7524f3f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7523f5d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7523f5d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7529155d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7529155d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe752570290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe752570290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75235c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75235c710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe752342a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe752342a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe752675750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe752675750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75235cc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75235cc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe752307410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe752307410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe752014e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe752014e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe751f7bed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe751f7bed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe752000690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe752000690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7520140d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7520140d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75224f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75224f5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe751ccb410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe751ccb410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe751d636d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe751d636d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751d52310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751d52310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe751ccb9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe751ccb9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751be24d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751be24d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7519f8e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7519f8e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7519a3c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7519a3c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751a04910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751a04910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7519c8bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7519c8bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751d63290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751d63290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe751679390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe751679390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7515f7f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7515f7f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751679690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751679690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe74173c190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe74173c190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751ec08d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751ec08d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7516ede90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7516ede90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe751307510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe751307510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751146b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe751146b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe751480b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe751480b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7516ed4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7516ed4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7511b1210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7511b1210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe750fc5d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe750fc5d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe114144250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe114144250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7511a1f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe7511a1f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe750fb0190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe750fb0190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe750e6a7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe750e6a7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe750ceb690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe750ceb690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe750c09610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe750c09610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75106de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75106de10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe750afc9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe750afc9d0>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 3:20
 128/1578 [=>............................] - ETA: 2:05
 192/1578 [==>...........................] - ETA: 1:39
 256/1578 [===>..........................] - ETA: 1:23
 320/1578 [=====>........................] - ETA: 1:16
 384/1578 [======>.......................] - ETA: 1:09
 448/1578 [=======>......................] - ETA: 1:02
 512/1578 [========>.....................] - ETA: 56s 
 576/1578 [=========>....................] - ETA: 52s
 640/1578 [===========>..................] - ETA: 48s
 704/1578 [============>.................] - ETA: 44s
 768/1578 [=============>................] - ETA: 40s
 832/1578 [==============>...............] - ETA: 36s
 896/1578 [================>.............] - ETA: 33s
 960/1578 [=================>............] - ETA: 30s
1024/1578 [==================>...........] - ETA: 26s
1088/1578 [===================>..........] - ETA: 23s
1152/1578 [====================>.........] - ETA: 20s
1216/1578 [======================>.......] - ETA: 17s
1280/1578 [=======================>......] - ETA: 14s
1344/1578 [========================>.....] - ETA: 10s
1408/1578 [=========================>....] - ETA: 7s 
1472/1578 [==========================>...] - ETA: 4s
1536/1578 [============================>.] - ETA: 1s
1578/1578 [==============================] - 73s 46ms/step
loss: 0.6790941028238401
acc: 0.5671736374402983
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe11438aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe11438aed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe76b42efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe76b42efd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe745a73590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe745a73590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74ec01290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74ec01290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7509f67d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7509f67d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe76b47f090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe76b47f090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75b0b4950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75b0b4950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75b0d3ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75b0d3ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75b1dcc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe75b1dcc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe75b179290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe75b179290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75b1dc490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75b1dc490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75b1dc450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe75b1dc450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe741d8d3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe741d8d3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe742210690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe742210690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0f45be950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0f45be950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74ecc6bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74ecc6bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe741d65d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe741d65d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f44e0690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f44e0690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0f4463510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0f4463510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0f43408d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0f43408d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f43c0450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f43c0450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0f460d4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0f460d4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f45337d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0f45337d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0f40f9dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0f40f9dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0f40b7210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0f40b7210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d463dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d463dfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0f4441c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0f4441c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d46b8f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d46b8f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0f4421b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0f4421b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0d4694dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0d4694dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d459c550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d459c550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0f40b7c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0f40b7c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d47ab150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d47ab150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0d44af0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0d44af0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0d4106bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0d4106bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d454f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d454f590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0d44afc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0d44afc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d46b3d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d46b3d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0bc7616d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0bc7616d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0bc625610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0bc625610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d41b6a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0d41b6a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0d41b31d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0d41b31d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0bc6a5f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0bc6a5f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0bc5d9150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0bc5d9150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0bc2f6610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0bc2f6610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0bc3ceb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0bc3ceb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0bc5d90d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0bc5d90d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0bc2c6310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0bc2c6310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0bc402fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0bc402fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0bc084b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0bc084b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0bc1e3d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0bc1e3d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0bc339b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0bc339b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0bc10f550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0bc10f550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf384a3810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf384a3810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf382fa6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf382fa6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf38464d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf38464d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe09478af90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe09478af90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf3842d550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf3842d550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf38162cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf38162cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf38142550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf38142550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf38256390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf38256390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf38162410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf38162410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf38052910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf38052910>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 36:57 - loss: 0.7471 - acc: 0.3594
 128/5677 [..............................] - ETA: 23:25 - loss: 0.8176 - acc: 0.4531
 192/5677 [>.............................] - ETA: 18:51 - loss: 0.7862 - acc: 0.4740
 256/5677 [>.............................] - ETA: 16:40 - loss: 0.7669 - acc: 0.4727
 320/5677 [>.............................] - ETA: 15:11 - loss: 0.7545 - acc: 0.4875
 384/5677 [=>............................] - ETA: 14:11 - loss: 0.7495 - acc: 0.4974
 448/5677 [=>............................] - ETA: 13:31 - loss: 0.7497 - acc: 0.4888
 512/5677 [=>............................] - ETA: 12:52 - loss: 0.7459 - acc: 0.4863
 576/5677 [==>...........................] - ETA: 12:21 - loss: 0.7424 - acc: 0.4913
 640/5677 [==>...........................] - ETA: 12:00 - loss: 0.7369 - acc: 0.5016
 704/5677 [==>...........................] - ETA: 11:41 - loss: 0.7347 - acc: 0.4986
 768/5677 [===>..........................] - ETA: 11:37 - loss: 0.7339 - acc: 0.4961
 832/5677 [===>..........................] - ETA: 11:21 - loss: 0.7322 - acc: 0.4964
 896/5677 [===>..........................] - ETA: 11:04 - loss: 0.7313 - acc: 0.4955
 960/5677 [====>.........................] - ETA: 10:51 - loss: 0.7323 - acc: 0.4958
1024/5677 [====>.........................] - ETA: 10:33 - loss: 0.7352 - acc: 0.4922
1088/5677 [====>.........................] - ETA: 10:22 - loss: 0.7340 - acc: 0.4908
1152/5677 [=====>........................] - ETA: 10:11 - loss: 0.7354 - acc: 0.4870
1216/5677 [=====>........................] - ETA: 10:01 - loss: 0.7360 - acc: 0.4901
1280/5677 [=====>........................] - ETA: 9:51 - loss: 0.7341 - acc: 0.4945 
1344/5677 [======>.......................] - ETA: 9:38 - loss: 0.7364 - acc: 0.4918
1408/5677 [======>.......................] - ETA: 9:28 - loss: 0.7359 - acc: 0.4915
1472/5677 [======>.......................] - ETA: 9:18 - loss: 0.7334 - acc: 0.4946
1536/5677 [=======>......................] - ETA: 9:07 - loss: 0.7335 - acc: 0.4954
1600/5677 [=======>......................] - ETA: 8:56 - loss: 0.7354 - acc: 0.4925
1664/5677 [=======>......................] - ETA: 8:49 - loss: 0.7332 - acc: 0.4958
1728/5677 [========>.....................] - ETA: 8:39 - loss: 0.7330 - acc: 0.4948
1792/5677 [========>.....................] - ETA: 8:28 - loss: 0.7311 - acc: 0.5006
1856/5677 [========>.....................] - ETA: 8:18 - loss: 0.7297 - acc: 0.5027
1920/5677 [=========>....................] - ETA: 8:11 - loss: 0.7296 - acc: 0.5010
1984/5677 [=========>....................] - ETA: 8:02 - loss: 0.7315 - acc: 0.4970
2048/5677 [=========>....................] - ETA: 7:51 - loss: 0.7319 - acc: 0.4946
2112/5677 [==========>...................] - ETA: 7:42 - loss: 0.7318 - acc: 0.4957
2176/5677 [==========>...................] - ETA: 7:32 - loss: 0.7314 - acc: 0.4959
2240/5677 [==========>...................] - ETA: 7:23 - loss: 0.7320 - acc: 0.4946
2304/5677 [===========>..................] - ETA: 7:14 - loss: 0.7312 - acc: 0.4957
2368/5677 [===========>..................] - ETA: 7:06 - loss: 0.7311 - acc: 0.4954
2432/5677 [===========>..................] - ETA: 6:57 - loss: 0.7312 - acc: 0.4955
2496/5677 [============>.................] - ETA: 6:48 - loss: 0.7311 - acc: 0.4924
2560/5677 [============>.................] - ETA: 6:38 - loss: 0.7325 - acc: 0.4902
2624/5677 [============>.................] - ETA: 6:30 - loss: 0.7333 - acc: 0.4889
2688/5677 [=============>................] - ETA: 6:21 - loss: 0.7313 - acc: 0.4914
2752/5677 [=============>................] - ETA: 6:13 - loss: 0.7306 - acc: 0.4909
2816/5677 [=============>................] - ETA: 6:03 - loss: 0.7299 - acc: 0.4936
2880/5677 [==============>...............] - ETA: 5:54 - loss: 0.7286 - acc: 0.4962
2944/5677 [==============>...............] - ETA: 5:46 - loss: 0.7273 - acc: 0.4969
3008/5677 [==============>...............] - ETA: 5:37 - loss: 0.7265 - acc: 0.4983
3072/5677 [===============>..............] - ETA: 5:28 - loss: 0.7254 - acc: 0.5010
3136/5677 [===============>..............] - ETA: 5:19 - loss: 0.7251 - acc: 0.5000
3200/5677 [===============>..............] - ETA: 5:11 - loss: 0.7248 - acc: 0.5000
3264/5677 [================>.............] - ETA: 5:03 - loss: 0.7247 - acc: 0.5006
3328/5677 [================>.............] - ETA: 4:56 - loss: 0.7242 - acc: 0.5006
3392/5677 [================>.............] - ETA: 4:47 - loss: 0.7237 - acc: 0.5024
3456/5677 [=================>............] - ETA: 4:38 - loss: 0.7226 - acc: 0.5052
3520/5677 [=================>............] - ETA: 4:30 - loss: 0.7223 - acc: 0.5051
3584/5677 [=================>............] - ETA: 4:20 - loss: 0.7224 - acc: 0.5050
3648/5677 [==================>...........] - ETA: 4:11 - loss: 0.7219 - acc: 0.5063
3712/5677 [==================>...........] - ETA: 4:01 - loss: 0.7218 - acc: 0.5065
3776/5677 [==================>...........] - ETA: 3:52 - loss: 0.7218 - acc: 0.5050
3840/5677 [===================>..........] - ETA: 3:44 - loss: 0.7211 - acc: 0.5073
3904/5677 [===================>..........] - ETA: 3:36 - loss: 0.7202 - acc: 0.5077
3968/5677 [===================>..........] - ETA: 3:28 - loss: 0.7199 - acc: 0.5081
4032/5677 [====================>.........] - ETA: 3:19 - loss: 0.7201 - acc: 0.5074
4096/5677 [====================>.........] - ETA: 3:11 - loss: 0.7195 - acc: 0.5081
4160/5677 [====================>.........] - ETA: 3:03 - loss: 0.7200 - acc: 0.5070
4224/5677 [=====================>........] - ETA: 2:55 - loss: 0.7202 - acc: 0.5059
4288/5677 [=====================>........] - ETA: 2:46 - loss: 0.7201 - acc: 0.5058
4352/5677 [=====================>........] - ETA: 2:38 - loss: 0.7196 - acc: 0.5069
4416/5677 [======================>.......] - ETA: 2:30 - loss: 0.7200 - acc: 0.5063
4480/5677 [======================>.......] - ETA: 2:22 - loss: 0.7201 - acc: 0.5058
4544/5677 [=======================>......] - ETA: 2:14 - loss: 0.7195 - acc: 0.5068
4608/5677 [=======================>......] - ETA: 2:06 - loss: 0.7194 - acc: 0.5065
4672/5677 [=======================>......] - ETA: 1:58 - loss: 0.7194 - acc: 0.5071
4736/5677 [========================>.....] - ETA: 1:51 - loss: 0.7188 - acc: 0.5080
4800/5677 [========================>.....] - ETA: 1:43 - loss: 0.7187 - acc: 0.5079
4864/5677 [========================>.....] - ETA: 1:35 - loss: 0.7179 - acc: 0.5093
4928/5677 [=========================>....] - ETA: 1:28 - loss: 0.7182 - acc: 0.5083
4992/5677 [=========================>....] - ETA: 1:20 - loss: 0.7176 - acc: 0.5092
5056/5677 [=========================>....] - ETA: 1:13 - loss: 0.7176 - acc: 0.5095
5120/5677 [==========================>...] - ETA: 1:05 - loss: 0.7180 - acc: 0.5084
5184/5677 [==========================>...] - ETA: 57s - loss: 0.7178 - acc: 0.5081 
5248/5677 [==========================>...] - ETA: 50s - loss: 0.7178 - acc: 0.5080
5312/5677 [===========================>..] - ETA: 42s - loss: 0.7173 - acc: 0.5087
5376/5677 [===========================>..] - ETA: 35s - loss: 0.7168 - acc: 0.5091
5440/5677 [===========================>..] - ETA: 27s - loss: 0.7168 - acc: 0.5088
5504/5677 [============================>.] - ETA: 20s - loss: 0.7164 - acc: 0.5100
5568/5677 [============================>.] - ETA: 12s - loss: 0.7167 - acc: 0.5090
5632/5677 [============================>.] - ETA: 5s - loss: 0.7165 - acc: 0.5091 
5677/5677 [==============================] - 696s 123ms/step - loss: 0.7163 - acc: 0.5094 - val_loss: 0.6989 - val_acc: 0.5182

Epoch 00001: val_acc improved from -inf to 0.51823, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window17/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 8:52 - loss: 0.6913 - acc: 0.5781
 128/5677 [..............................] - ETA: 8:44 - loss: 0.6900 - acc: 0.6016
 192/5677 [>.............................] - ETA: 8:56 - loss: 0.6898 - acc: 0.5781
 256/5677 [>.............................] - ETA: 9:09 - loss: 0.6870 - acc: 0.5742
 320/5677 [>.............................] - ETA: 9:18 - loss: 0.6937 - acc: 0.5531
 384/5677 [=>............................] - ETA: 9:23 - loss: 0.6917 - acc: 0.5599
 448/5677 [=>............................] - ETA: 9:19 - loss: 0.6909 - acc: 0.5580
 512/5677 [=>............................] - ETA: 9:19 - loss: 0.6945 - acc: 0.5391
 576/5677 [==>...........................] - ETA: 9:20 - loss: 0.6948 - acc: 0.5312
 640/5677 [==>...........................] - ETA: 9:14 - loss: 0.6918 - acc: 0.5391
 704/5677 [==>...........................] - ETA: 9:16 - loss: 0.6946 - acc: 0.5284
 768/5677 [===>..........................] - ETA: 9:08 - loss: 0.6938 - acc: 0.5312
 832/5677 [===>..........................] - ETA: 9:06 - loss: 0.6928 - acc: 0.5337
 896/5677 [===>..........................] - ETA: 8:58 - loss: 0.6932 - acc: 0.5357
 960/5677 [====>.........................] - ETA: 8:54 - loss: 0.6939 - acc: 0.5354
1024/5677 [====>.........................] - ETA: 8:48 - loss: 0.6940 - acc: 0.5371
1088/5677 [====>.........................] - ETA: 8:39 - loss: 0.6943 - acc: 0.5404
1152/5677 [=====>........................] - ETA: 8:32 - loss: 0.6958 - acc: 0.5312
1216/5677 [=====>........................] - ETA: 8:23 - loss: 0.6952 - acc: 0.5321
1280/5677 [=====>........................] - ETA: 8:14 - loss: 0.6962 - acc: 0.5297
1344/5677 [======>.......................] - ETA: 8:06 - loss: 0.6953 - acc: 0.5305
1408/5677 [======>.......................] - ETA: 7:57 - loss: 0.6976 - acc: 0.5227
1472/5677 [======>.......................] - ETA: 7:49 - loss: 0.6973 - acc: 0.5224
1536/5677 [=======>......................] - ETA: 7:40 - loss: 0.6990 - acc: 0.5195
1600/5677 [=======>......................] - ETA: 7:34 - loss: 0.6983 - acc: 0.5219
1664/5677 [=======>......................] - ETA: 7:27 - loss: 0.6978 - acc: 0.5246
1728/5677 [========>.....................] - ETA: 7:19 - loss: 0.6987 - acc: 0.5226
1792/5677 [========>.....................] - ETA: 7:13 - loss: 0.6989 - acc: 0.5251
1856/5677 [========>.....................] - ETA: 7:06 - loss: 0.6990 - acc: 0.5253
1920/5677 [=========>....................] - ETA: 7:00 - loss: 0.6991 - acc: 0.5255
1984/5677 [=========>....................] - ETA: 6:53 - loss: 0.6987 - acc: 0.5282
2048/5677 [=========>....................] - ETA: 6:45 - loss: 0.6987 - acc: 0.5269
2112/5677 [==========>...................] - ETA: 6:37 - loss: 0.6979 - acc: 0.5284
2176/5677 [==========>...................] - ETA: 6:30 - loss: 0.6982 - acc: 0.5285
2240/5677 [==========>...................] - ETA: 6:22 - loss: 0.6992 - acc: 0.5263
2304/5677 [===========>..................] - ETA: 6:16 - loss: 0.6990 - acc: 0.5278
2368/5677 [===========>..................] - ETA: 6:09 - loss: 0.6988 - acc: 0.5296
2432/5677 [===========>..................] - ETA: 6:00 - loss: 0.6997 - acc: 0.5271
2496/5677 [============>.................] - ETA: 5:53 - loss: 0.6997 - acc: 0.5260
2560/5677 [============>.................] - ETA: 5:46 - loss: 0.6989 - acc: 0.5285
2624/5677 [============>.................] - ETA: 5:38 - loss: 0.7007 - acc: 0.5232
2688/5677 [=============>................] - ETA: 5:30 - loss: 0.7000 - acc: 0.5242
2752/5677 [=============>................] - ETA: 5:22 - loss: 0.7010 - acc: 0.5214
2816/5677 [=============>................] - ETA: 5:14 - loss: 0.7002 - acc: 0.5227
2880/5677 [==============>...............] - ETA: 5:07 - loss: 0.6999 - acc: 0.5229
2944/5677 [==============>...............] - ETA: 5:01 - loss: 0.6996 - acc: 0.5234
3008/5677 [==============>...............] - ETA: 4:53 - loss: 0.6998 - acc: 0.5223
3072/5677 [===============>..............] - ETA: 4:45 - loss: 0.7009 - acc: 0.5189
3136/5677 [===============>..............] - ETA: 4:38 - loss: 0.7012 - acc: 0.5188
3200/5677 [===============>..............] - ETA: 4:29 - loss: 0.7011 - acc: 0.5194
3264/5677 [================>.............] - ETA: 4:22 - loss: 0.7019 - acc: 0.5190
3328/5677 [================>.............] - ETA: 4:15 - loss: 0.7017 - acc: 0.5183
3392/5677 [================>.............] - ETA: 4:07 - loss: 0.7011 - acc: 0.5195
3456/5677 [=================>............] - ETA: 4:00 - loss: 0.7010 - acc: 0.5194
3520/5677 [=================>............] - ETA: 3:52 - loss: 0.7006 - acc: 0.5202
3584/5677 [=================>............] - ETA: 3:45 - loss: 0.7008 - acc: 0.5201
3648/5677 [==================>...........] - ETA: 3:38 - loss: 0.7011 - acc: 0.5197
3712/5677 [==================>...........] - ETA: 3:31 - loss: 0.7007 - acc: 0.5213
3776/5677 [==================>...........] - ETA: 3:23 - loss: 0.6996 - acc: 0.5236
3840/5677 [===================>..........] - ETA: 3:16 - loss: 0.6992 - acc: 0.5250
3904/5677 [===================>..........] - ETA: 3:09 - loss: 0.6988 - acc: 0.5254
3968/5677 [===================>..........] - ETA: 3:02 - loss: 0.6980 - acc: 0.5267
4032/5677 [====================>.........] - ETA: 2:55 - loss: 0.6975 - acc: 0.5278
4096/5677 [====================>.........] - ETA: 2:47 - loss: 0.6973 - acc: 0.5286
4160/5677 [====================>.........] - ETA: 2:40 - loss: 0.6973 - acc: 0.5286
4224/5677 [=====================>........] - ETA: 2:33 - loss: 0.6974 - acc: 0.5291
4288/5677 [=====================>........] - ETA: 2:26 - loss: 0.6976 - acc: 0.5292
4352/5677 [=====================>........] - ETA: 2:19 - loss: 0.6978 - acc: 0.5285
4416/5677 [======================>.......] - ETA: 2:12 - loss: 0.6974 - acc: 0.5297
4480/5677 [======================>.......] - ETA: 2:05 - loss: 0.6977 - acc: 0.5290
4544/5677 [=======================>......] - ETA: 1:59 - loss: 0.6978 - acc: 0.5284
4608/5677 [=======================>......] - ETA: 1:52 - loss: 0.6975 - acc: 0.5293
4672/5677 [=======================>......] - ETA: 1:45 - loss: 0.6975 - acc: 0.5280
4736/5677 [========================>.....] - ETA: 1:38 - loss: 0.6975 - acc: 0.5268
4800/5677 [========================>.....] - ETA: 1:31 - loss: 0.6978 - acc: 0.5256
4864/5677 [========================>.....] - ETA: 1:25 - loss: 0.6978 - acc: 0.5257
4928/5677 [=========================>....] - ETA: 1:18 - loss: 0.6977 - acc: 0.5256
4992/5677 [=========================>....] - ETA: 1:11 - loss: 0.6976 - acc: 0.5258
5056/5677 [=========================>....] - ETA: 1:04 - loss: 0.6971 - acc: 0.5273
5120/5677 [==========================>...] - ETA: 58s - loss: 0.6971 - acc: 0.5273 
5184/5677 [==========================>...] - ETA: 51s - loss: 0.6970 - acc: 0.5276
5248/5677 [==========================>...] - ETA: 44s - loss: 0.6966 - acc: 0.5288
5312/5677 [===========================>..] - ETA: 37s - loss: 0.6965 - acc: 0.5294
5376/5677 [===========================>..] - ETA: 31s - loss: 0.6960 - acc: 0.5303
5440/5677 [===========================>..] - ETA: 24s - loss: 0.6958 - acc: 0.5312
5504/5677 [============================>.] - ETA: 17s - loss: 0.6960 - acc: 0.5318
5568/5677 [============================>.] - ETA: 11s - loss: 0.6960 - acc: 0.5318
5632/5677 [============================>.] - ETA: 4s - loss: 0.6957 - acc: 0.5330 
5677/5677 [==============================] - 612s 108ms/step - loss: 0.6961 - acc: 0.5321 - val_loss: 0.6944 - val_acc: 0.5483

Epoch 00002: val_acc improved from 0.51823 to 0.54834, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window17/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 9:05 - loss: 0.7178 - acc: 0.5156
 128/5677 [..............................] - ETA: 9:05 - loss: 0.6968 - acc: 0.5547
 192/5677 [>.............................] - ETA: 9:24 - loss: 0.7052 - acc: 0.5156
 256/5677 [>.............................] - ETA: 9:14 - loss: 0.7045 - acc: 0.5117
 320/5677 [>.............................] - ETA: 9:25 - loss: 0.7019 - acc: 0.5125
 384/5677 [=>............................] - ETA: 9:13 - loss: 0.7070 - acc: 0.5078
 448/5677 [=>............................] - ETA: 9:05 - loss: 0.7063 - acc: 0.5112
 512/5677 [=>............................] - ETA: 9:05 - loss: 0.7051 - acc: 0.5078
 576/5677 [==>...........................] - ETA: 9:01 - loss: 0.7054 - acc: 0.5087
 640/5677 [==>...........................] - ETA: 8:49 - loss: 0.7015 - acc: 0.5203
 704/5677 [==>...........................] - ETA: 8:45 - loss: 0.6970 - acc: 0.5298
 768/5677 [===>..........................] - ETA: 8:40 - loss: 0.6946 - acc: 0.5312
 832/5677 [===>..........................] - ETA: 8:30 - loss: 0.6972 - acc: 0.5216
 896/5677 [===>..........................] - ETA: 8:21 - loss: 0.6959 - acc: 0.5290
 960/5677 [====>.........................] - ETA: 8:09 - loss: 0.6961 - acc: 0.5323
1024/5677 [====>.........................] - ETA: 8:07 - loss: 0.6960 - acc: 0.5342
1088/5677 [====>.........................] - ETA: 8:01 - loss: 0.6968 - acc: 0.5340
1152/5677 [=====>........................] - ETA: 7:55 - loss: 0.6974 - acc: 0.5365
1216/5677 [=====>........................] - ETA: 7:47 - loss: 0.6984 - acc: 0.5354
1280/5677 [=====>........................] - ETA: 7:39 - loss: 0.6967 - acc: 0.5383
1344/5677 [======>.......................] - ETA: 7:32 - loss: 0.6960 - acc: 0.5387
1408/5677 [======>.......................] - ETA: 7:26 - loss: 0.6938 - acc: 0.5455
1472/5677 [======>.......................] - ETA: 7:21 - loss: 0.6938 - acc: 0.5428
1536/5677 [=======>......................] - ETA: 7:17 - loss: 0.6966 - acc: 0.5397
1600/5677 [=======>......................] - ETA: 7:10 - loss: 0.6965 - acc: 0.5363
1664/5677 [=======>......................] - ETA: 7:02 - loss: 0.6973 - acc: 0.5355
1728/5677 [========>.....................] - ETA: 6:53 - loss: 0.6952 - acc: 0.5417
1792/5677 [========>.....................] - ETA: 6:44 - loss: 0.6960 - acc: 0.5407
1856/5677 [========>.....................] - ETA: 6:36 - loss: 0.6954 - acc: 0.5409
1920/5677 [=========>....................] - ETA: 6:29 - loss: 0.6963 - acc: 0.5396
1984/5677 [=========>....................] - ETA: 6:23 - loss: 0.6968 - acc: 0.5373
2048/5677 [=========>....................] - ETA: 6:17 - loss: 0.6963 - acc: 0.5376
2112/5677 [==========>...................] - ETA: 6:10 - loss: 0.6958 - acc: 0.5355
2176/5677 [==========>...................] - ETA: 6:03 - loss: 0.6945 - acc: 0.5381
2240/5677 [==========>...................] - ETA: 5:54 - loss: 0.6939 - acc: 0.5366
2304/5677 [===========>..................] - ETA: 5:47 - loss: 0.6949 - acc: 0.5347
2368/5677 [===========>..................] - ETA: 5:39 - loss: 0.6935 - acc: 0.5380
2432/5677 [===========>..................] - ETA: 5:33 - loss: 0.6923 - acc: 0.5403
2496/5677 [============>.................] - ETA: 5:27 - loss: 0.6921 - acc: 0.5417
2560/5677 [============>.................] - ETA: 5:20 - loss: 0.6913 - acc: 0.5434
2624/5677 [============>.................] - ETA: 5:14 - loss: 0.6911 - acc: 0.5431
2688/5677 [=============>................] - ETA: 5:06 - loss: 0.6916 - acc: 0.5409
2752/5677 [=============>................] - ETA: 4:59 - loss: 0.6920 - acc: 0.5411
2816/5677 [=============>................] - ETA: 4:53 - loss: 0.6915 - acc: 0.5415
2880/5677 [==============>...............] - ETA: 4:46 - loss: 0.6908 - acc: 0.5431
2944/5677 [==============>...............] - ETA: 4:40 - loss: 0.6902 - acc: 0.5448
3008/5677 [==============>...............] - ETA: 4:33 - loss: 0.6899 - acc: 0.5449
3072/5677 [===============>..............] - ETA: 4:27 - loss: 0.6902 - acc: 0.5452
3136/5677 [===============>..............] - ETA: 4:21 - loss: 0.6903 - acc: 0.5434
3200/5677 [===============>..............] - ETA: 4:14 - loss: 0.6896 - acc: 0.5441
3264/5677 [================>.............] - ETA: 4:08 - loss: 0.6908 - acc: 0.5423
3328/5677 [================>.............] - ETA: 4:01 - loss: 0.6906 - acc: 0.5427
3392/5677 [================>.............] - ETA: 3:55 - loss: 0.6897 - acc: 0.5439
3456/5677 [=================>............] - ETA: 3:48 - loss: 0.6894 - acc: 0.5448
3520/5677 [=================>............] - ETA: 3:41 - loss: 0.6896 - acc: 0.5446
3584/5677 [=================>............] - ETA: 3:35 - loss: 0.6894 - acc: 0.5441
3648/5677 [==================>...........] - ETA: 3:28 - loss: 0.6898 - acc: 0.5430
3712/5677 [==================>...........] - ETA: 3:21 - loss: 0.6896 - acc: 0.5445
3776/5677 [==================>...........] - ETA: 3:15 - loss: 0.6893 - acc: 0.5440
3840/5677 [===================>..........] - ETA: 3:08 - loss: 0.6893 - acc: 0.5451
3904/5677 [===================>..........] - ETA: 3:02 - loss: 0.6893 - acc: 0.5459
3968/5677 [===================>..........] - ETA: 2:55 - loss: 0.6894 - acc: 0.5441
4032/5677 [====================>.........] - ETA: 2:49 - loss: 0.6894 - acc: 0.5429
4096/5677 [====================>.........] - ETA: 2:42 - loss: 0.6895 - acc: 0.5432
4160/5677 [====================>.........] - ETA: 2:36 - loss: 0.6897 - acc: 0.5430
4224/5677 [=====================>........] - ETA: 2:29 - loss: 0.6894 - acc: 0.5445
4288/5677 [=====================>........] - ETA: 2:23 - loss: 0.6893 - acc: 0.5443
4352/5677 [=====================>........] - ETA: 2:16 - loss: 0.6896 - acc: 0.5443
4416/5677 [======================>.......] - ETA: 2:09 - loss: 0.6894 - acc: 0.5444
4480/5677 [======================>.......] - ETA: 2:03 - loss: 0.6898 - acc: 0.5435
4544/5677 [=======================>......] - ETA: 1:56 - loss: 0.6901 - acc: 0.5429
4608/5677 [=======================>......] - ETA: 1:50 - loss: 0.6902 - acc: 0.5432
4672/5677 [=======================>......] - ETA: 1:43 - loss: 0.6904 - acc: 0.5426
4736/5677 [========================>.....] - ETA: 1:37 - loss: 0.6902 - acc: 0.5435
4800/5677 [========================>.....] - ETA: 1:30 - loss: 0.6893 - acc: 0.5458
4864/5677 [========================>.....] - ETA: 1:23 - loss: 0.6896 - acc: 0.5450
4928/5677 [=========================>....] - ETA: 1:17 - loss: 0.6900 - acc: 0.5450
4992/5677 [=========================>....] - ETA: 1:10 - loss: 0.6898 - acc: 0.5457
5056/5677 [=========================>....] - ETA: 1:03 - loss: 0.6902 - acc: 0.5451
5120/5677 [==========================>...] - ETA: 57s - loss: 0.6905 - acc: 0.5443 
5184/5677 [==========================>...] - ETA: 50s - loss: 0.6908 - acc: 0.5430
5248/5677 [==========================>...] - ETA: 43s - loss: 0.6910 - acc: 0.5421
5312/5677 [===========================>..] - ETA: 37s - loss: 0.6916 - acc: 0.5409
5376/5677 [===========================>..] - ETA: 30s - loss: 0.6915 - acc: 0.5415
5440/5677 [===========================>..] - ETA: 24s - loss: 0.6918 - acc: 0.5412
5504/5677 [============================>.] - ETA: 17s - loss: 0.6914 - acc: 0.5423
5568/5677 [============================>.] - ETA: 11s - loss: 0.6911 - acc: 0.5431
5632/5677 [============================>.] - ETA: 4s - loss: 0.6910 - acc: 0.5439 
5677/5677 [==============================] - 599s 106ms/step - loss: 0.6911 - acc: 0.5432 - val_loss: 0.6987 - val_acc: 0.5087

Epoch 00003: val_acc did not improve from 0.54834
Epoch 4/10

  64/5677 [..............................] - ETA: 9:03 - loss: 0.6976 - acc: 0.5469
 128/5677 [..............................] - ETA: 8:50 - loss: 0.7066 - acc: 0.5234
 192/5677 [>.............................] - ETA: 8:54 - loss: 0.7073 - acc: 0.5052
 256/5677 [>.............................] - ETA: 8:40 - loss: 0.7004 - acc: 0.5156
 320/5677 [>.............................] - ETA: 8:27 - loss: 0.7002 - acc: 0.5125
 384/5677 [=>............................] - ETA: 8:32 - loss: 0.7012 - acc: 0.5078
 448/5677 [=>............................] - ETA: 8:28 - loss: 0.7018 - acc: 0.5112
 512/5677 [=>............................] - ETA: 8:19 - loss: 0.7009 - acc: 0.5020
 576/5677 [==>...........................] - ETA: 8:11 - loss: 0.7019 - acc: 0.4896
 640/5677 [==>...........................] - ETA: 8:03 - loss: 0.6982 - acc: 0.5031
 704/5677 [==>...........................] - ETA: 7:53 - loss: 0.6957 - acc: 0.5085
 768/5677 [===>..........................] - ETA: 7:43 - loss: 0.6949 - acc: 0.5143
 832/5677 [===>..........................] - ETA: 7:34 - loss: 0.6924 - acc: 0.5168
 896/5677 [===>..........................] - ETA: 7:27 - loss: 0.6921 - acc: 0.5212
 960/5677 [====>.........................] - ETA: 7:15 - loss: 0.6887 - acc: 0.5365
1024/5677 [====>.........................] - ETA: 7:05 - loss: 0.6900 - acc: 0.5352
1088/5677 [====>.........................] - ETA: 6:55 - loss: 0.6901 - acc: 0.5331
1152/5677 [=====>........................] - ETA: 6:46 - loss: 0.6881 - acc: 0.5399
1216/5677 [=====>........................] - ETA: 6:40 - loss: 0.6880 - acc: 0.5387
1280/5677 [=====>........................] - ETA: 6:41 - loss: 0.6891 - acc: 0.5344
1344/5677 [======>.......................] - ETA: 6:33 - loss: 0.6885 - acc: 0.5394
1408/5677 [======>.......................] - ETA: 6:27 - loss: 0.6883 - acc: 0.5433
1472/5677 [======>.......................] - ETA: 6:19 - loss: 0.6891 - acc: 0.5401
1536/5677 [=======>......................] - ETA: 6:12 - loss: 0.6891 - acc: 0.5397
1600/5677 [=======>......................] - ETA: 6:06 - loss: 0.6901 - acc: 0.5387
1664/5677 [=======>......................] - ETA: 5:59 - loss: 0.6895 - acc: 0.5403
1728/5677 [========>.....................] - ETA: 5:52 - loss: 0.6898 - acc: 0.5411
1792/5677 [========>.....................] - ETA: 5:46 - loss: 0.6889 - acc: 0.5419
1856/5677 [========>.....................] - ETA: 5:39 - loss: 0.6881 - acc: 0.5436
1920/5677 [=========>....................] - ETA: 5:34 - loss: 0.6874 - acc: 0.5453
1984/5677 [=========>....................] - ETA: 5:29 - loss: 0.6872 - acc: 0.5464
2048/5677 [=========>....................] - ETA: 5:23 - loss: 0.6883 - acc: 0.5425
2112/5677 [==========>...................] - ETA: 5:16 - loss: 0.6880 - acc: 0.5436
2176/5677 [==========>...................] - ETA: 5:11 - loss: 0.6875 - acc: 0.5441
2240/5677 [==========>...................] - ETA: 5:04 - loss: 0.6874 - acc: 0.5446
2304/5677 [===========>..................] - ETA: 4:58 - loss: 0.6872 - acc: 0.5447
2368/5677 [===========>..................] - ETA: 4:52 - loss: 0.6867 - acc: 0.5452
2432/5677 [===========>..................] - ETA: 4:46 - loss: 0.6857 - acc: 0.5502
2496/5677 [============>.................] - ETA: 4:41 - loss: 0.6856 - acc: 0.5497
2560/5677 [============>.................] - ETA: 4:35 - loss: 0.6846 - acc: 0.5535
2624/5677 [============>.................] - ETA: 4:29 - loss: 0.6846 - acc: 0.5530
2688/5677 [=============>................] - ETA: 4:23 - loss: 0.6849 - acc: 0.5551
2752/5677 [=============>................] - ETA: 4:18 - loss: 0.6842 - acc: 0.5563
2816/5677 [=============>................] - ETA: 4:12 - loss: 0.6837 - acc: 0.5575
2880/5677 [==============>...............] - ETA: 4:06 - loss: 0.6839 - acc: 0.5569
2944/5677 [==============>...............] - ETA: 3:59 - loss: 0.6843 - acc: 0.5557
3008/5677 [==============>...............] - ETA: 3:54 - loss: 0.6848 - acc: 0.5562
3072/5677 [===============>..............] - ETA: 3:48 - loss: 0.6852 - acc: 0.5553
3136/5677 [===============>..............] - ETA: 3:43 - loss: 0.6866 - acc: 0.5504
3200/5677 [===============>..............] - ETA: 3:37 - loss: 0.6869 - acc: 0.5500
3264/5677 [================>.............] - ETA: 3:32 - loss: 0.6864 - acc: 0.5515
3328/5677 [================>.............] - ETA: 3:26 - loss: 0.6860 - acc: 0.5526
3392/5677 [================>.............] - ETA: 3:20 - loss: 0.6854 - acc: 0.5545
3456/5677 [=================>............] - ETA: 3:14 - loss: 0.6854 - acc: 0.5550
3520/5677 [=================>............] - ETA: 3:08 - loss: 0.6851 - acc: 0.5563
3584/5677 [=================>............] - ETA: 3:03 - loss: 0.6850 - acc: 0.5550
3648/5677 [==================>...........] - ETA: 2:57 - loss: 0.6843 - acc: 0.5559
3712/5677 [==================>...........] - ETA: 2:51 - loss: 0.6847 - acc: 0.5555
3776/5677 [==================>...........] - ETA: 2:45 - loss: 0.6845 - acc: 0.5564
3840/5677 [===================>..........] - ETA: 2:40 - loss: 0.6843 - acc: 0.5568
3904/5677 [===================>..........] - ETA: 2:34 - loss: 0.6847 - acc: 0.5564
3968/5677 [===================>..........] - ETA: 2:28 - loss: 0.6848 - acc: 0.5570
4032/5677 [====================>.........] - ETA: 2:23 - loss: 0.6861 - acc: 0.5553
4096/5677 [====================>.........] - ETA: 2:17 - loss: 0.6861 - acc: 0.5557
4160/5677 [====================>.........] - ETA: 2:11 - loss: 0.6855 - acc: 0.5577
4224/5677 [=====================>........] - ETA: 2:06 - loss: 0.6855 - acc: 0.5571
4288/5677 [=====================>........] - ETA: 2:00 - loss: 0.6860 - acc: 0.5567
4352/5677 [=====================>........] - ETA: 1:54 - loss: 0.6863 - acc: 0.5556
4416/5677 [======================>.......] - ETA: 1:49 - loss: 0.6866 - acc: 0.5548
4480/5677 [======================>.......] - ETA: 1:43 - loss: 0.6865 - acc: 0.5540
4544/5677 [=======================>......] - ETA: 1:38 - loss: 0.6861 - acc: 0.5550
4608/5677 [=======================>......] - ETA: 1:32 - loss: 0.6863 - acc: 0.5543
4672/5677 [=======================>......] - ETA: 1:27 - loss: 0.6860 - acc: 0.5546
4736/5677 [========================>.....] - ETA: 1:21 - loss: 0.6861 - acc: 0.5541
4800/5677 [========================>.....] - ETA: 1:15 - loss: 0.6862 - acc: 0.5535
4864/5677 [========================>.....] - ETA: 1:10 - loss: 0.6862 - acc: 0.5532
4928/5677 [=========================>....] - ETA: 1:04 - loss: 0.6865 - acc: 0.5526
4992/5677 [=========================>....] - ETA: 59s - loss: 0.6869 - acc: 0.5513 
5056/5677 [=========================>....] - ETA: 53s - loss: 0.6872 - acc: 0.5504
5120/5677 [==========================>...] - ETA: 48s - loss: 0.6875 - acc: 0.5500
5184/5677 [==========================>...] - ETA: 42s - loss: 0.6876 - acc: 0.5490
5248/5677 [==========================>...] - ETA: 37s - loss: 0.6876 - acc: 0.5495
5312/5677 [===========================>..] - ETA: 31s - loss: 0.6873 - acc: 0.5503
5376/5677 [===========================>..] - ETA: 25s - loss: 0.6872 - acc: 0.5508
5440/5677 [===========================>..] - ETA: 20s - loss: 0.6874 - acc: 0.5502
5504/5677 [============================>.] - ETA: 14s - loss: 0.6872 - acc: 0.5511
5568/5677 [============================>.] - ETA: 9s - loss: 0.6876 - acc: 0.5499 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6875 - acc: 0.5502
5677/5677 [==============================] - 510s 90ms/step - loss: 0.6877 - acc: 0.5501 - val_loss: 0.6950 - val_acc: 0.5040

Epoch 00004: val_acc did not improve from 0.54834
Epoch 5/10

  64/5677 [..............................] - ETA: 8:12 - loss: 0.6722 - acc: 0.6250
 128/5677 [..............................] - ETA: 8:14 - loss: 0.6633 - acc: 0.6250
 192/5677 [>.............................] - ETA: 8:09 - loss: 0.6767 - acc: 0.5729
 256/5677 [>.............................] - ETA: 8:07 - loss: 0.6743 - acc: 0.5742
 320/5677 [>.............................] - ETA: 8:10 - loss: 0.6676 - acc: 0.5875
 384/5677 [=>............................] - ETA: 8:12 - loss: 0.6699 - acc: 0.5755
 448/5677 [=>............................] - ETA: 8:09 - loss: 0.6732 - acc: 0.5647
 512/5677 [=>............................] - ETA: 8:01 - loss: 0.6781 - acc: 0.5488
 576/5677 [==>...........................] - ETA: 7:56 - loss: 0.6775 - acc: 0.5556
 640/5677 [==>...........................] - ETA: 7:45 - loss: 0.6806 - acc: 0.5547
 704/5677 [==>...........................] - ETA: 7:43 - loss: 0.6745 - acc: 0.5639
 768/5677 [===>..........................] - ETA: 7:35 - loss: 0.6742 - acc: 0.5664
 832/5677 [===>..........................] - ETA: 7:31 - loss: 0.6750 - acc: 0.5649
 896/5677 [===>..........................] - ETA: 7:25 - loss: 0.6751 - acc: 0.5670
 960/5677 [====>.........................] - ETA: 7:16 - loss: 0.6758 - acc: 0.5677
1024/5677 [====>.........................] - ETA: 7:08 - loss: 0.6760 - acc: 0.5674
1088/5677 [====>.........................] - ETA: 7:06 - loss: 0.6748 - acc: 0.5708
1152/5677 [=====>........................] - ETA: 7:01 - loss: 0.6753 - acc: 0.5712
1216/5677 [=====>........................] - ETA: 6:55 - loss: 0.6770 - acc: 0.5650
1280/5677 [=====>........................] - ETA: 6:47 - loss: 0.6759 - acc: 0.5727
1344/5677 [======>.......................] - ETA: 6:44 - loss: 0.6764 - acc: 0.5714
1408/5677 [======>.......................] - ETA: 6:39 - loss: 0.6774 - acc: 0.5682
1472/5677 [======>.......................] - ETA: 6:32 - loss: 0.6775 - acc: 0.5679
1536/5677 [=======>......................] - ETA: 6:25 - loss: 0.6777 - acc: 0.5664
1600/5677 [=======>......................] - ETA: 6:21 - loss: 0.6772 - acc: 0.5675
1664/5677 [=======>......................] - ETA: 6:17 - loss: 0.6768 - acc: 0.5679
1728/5677 [========>.....................] - ETA: 6:11 - loss: 0.6779 - acc: 0.5654
1792/5677 [========>.....................] - ETA: 6:05 - loss: 0.6795 - acc: 0.5642
1856/5677 [========>.....................] - ETA: 5:57 - loss: 0.6796 - acc: 0.5647
1920/5677 [=========>....................] - ETA: 5:50 - loss: 0.6788 - acc: 0.5687
1984/5677 [=========>....................] - ETA: 5:44 - loss: 0.6790 - acc: 0.5685
2048/5677 [=========>....................] - ETA: 5:41 - loss: 0.6806 - acc: 0.5654
2112/5677 [==========>...................] - ETA: 5:36 - loss: 0.6804 - acc: 0.5663
2176/5677 [==========>...................] - ETA: 5:31 - loss: 0.6800 - acc: 0.5671
2240/5677 [==========>...................] - ETA: 5:25 - loss: 0.6788 - acc: 0.5710
2304/5677 [===========>..................] - ETA: 5:17 - loss: 0.6787 - acc: 0.5712
2368/5677 [===========>..................] - ETA: 5:10 - loss: 0.6799 - acc: 0.5684
2432/5677 [===========>..................] - ETA: 5:03 - loss: 0.6812 - acc: 0.5650
2496/5677 [============>.................] - ETA: 4:57 - loss: 0.6809 - acc: 0.5661
2560/5677 [============>.................] - ETA: 4:52 - loss: 0.6809 - acc: 0.5656
2624/5677 [============>.................] - ETA: 4:46 - loss: 0.6807 - acc: 0.5655
2688/5677 [=============>................] - ETA: 4:41 - loss: 0.6810 - acc: 0.5658
2752/5677 [=============>................] - ETA: 4:35 - loss: 0.6807 - acc: 0.5676
2816/5677 [=============>................] - ETA: 4:29 - loss: 0.6801 - acc: 0.5682
2880/5677 [==============>...............] - ETA: 4:22 - loss: 0.6806 - acc: 0.5667
2944/5677 [==============>...............] - ETA: 4:16 - loss: 0.6811 - acc: 0.5645
3008/5677 [==============>...............] - ETA: 4:10 - loss: 0.6815 - acc: 0.5632
3072/5677 [===============>..............] - ETA: 4:04 - loss: 0.6814 - acc: 0.5635
3136/5677 [===============>..............] - ETA: 3:59 - loss: 0.6809 - acc: 0.5644
3200/5677 [===============>..............] - ETA: 3:53 - loss: 0.6801 - acc: 0.5675
3264/5677 [================>.............] - ETA: 3:48 - loss: 0.6796 - acc: 0.5686
3328/5677 [================>.............] - ETA: 3:41 - loss: 0.6794 - acc: 0.5697
3392/5677 [================>.............] - ETA: 3:35 - loss: 0.6798 - acc: 0.5687
3456/5677 [=================>............] - ETA: 3:28 - loss: 0.6804 - acc: 0.5671
3520/5677 [=================>............] - ETA: 3:22 - loss: 0.6808 - acc: 0.5662
3584/5677 [=================>............] - ETA: 3:15 - loss: 0.6805 - acc: 0.5670
3648/5677 [==================>...........] - ETA: 3:09 - loss: 0.6806 - acc: 0.5674
3712/5677 [==================>...........] - ETA: 3:03 - loss: 0.6804 - acc: 0.5679
3776/5677 [==================>...........] - ETA: 2:58 - loss: 0.6805 - acc: 0.5675
3840/5677 [===================>..........] - ETA: 2:52 - loss: 0.6806 - acc: 0.5669
3904/5677 [===================>..........] - ETA: 2:46 - loss: 0.6811 - acc: 0.5661
3968/5677 [===================>..........] - ETA: 2:41 - loss: 0.6813 - acc: 0.5660
4032/5677 [====================>.........] - ETA: 2:35 - loss: 0.6812 - acc: 0.5657
4096/5677 [====================>.........] - ETA: 2:29 - loss: 0.6810 - acc: 0.5657
4160/5677 [====================>.........] - ETA: 2:22 - loss: 0.6810 - acc: 0.5651
4224/5677 [=====================>........] - ETA: 2:16 - loss: 0.6808 - acc: 0.5656
4288/5677 [=====================>........] - ETA: 2:10 - loss: 0.6810 - acc: 0.5651
4352/5677 [=====================>........] - ETA: 2:04 - loss: 0.6813 - acc: 0.5643
4416/5677 [======================>.......] - ETA: 1:57 - loss: 0.6814 - acc: 0.5641
4480/5677 [======================>.......] - ETA: 1:51 - loss: 0.6813 - acc: 0.5641
4544/5677 [=======================>......] - ETA: 1:46 - loss: 0.6811 - acc: 0.5638
4608/5677 [=======================>......] - ETA: 1:40 - loss: 0.6811 - acc: 0.5632
4672/5677 [=======================>......] - ETA: 1:34 - loss: 0.6808 - acc: 0.5642
4736/5677 [========================>.....] - ETA: 1:28 - loss: 0.6810 - acc: 0.5640
4800/5677 [========================>.....] - ETA: 1:22 - loss: 0.6812 - acc: 0.5627
4864/5677 [========================>.....] - ETA: 1:16 - loss: 0.6816 - acc: 0.5621
4928/5677 [=========================>....] - ETA: 1:10 - loss: 0.6825 - acc: 0.5607
4992/5677 [=========================>....] - ETA: 1:04 - loss: 0.6824 - acc: 0.5605
5056/5677 [=========================>....] - ETA: 58s - loss: 0.6823 - acc: 0.5601 
5120/5677 [==========================>...] - ETA: 52s - loss: 0.6828 - acc: 0.5584
5184/5677 [==========================>...] - ETA: 46s - loss: 0.6824 - acc: 0.5586
5248/5677 [==========================>...] - ETA: 40s - loss: 0.6822 - acc: 0.5595
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6820 - acc: 0.5595
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6825 - acc: 0.5590
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6825 - acc: 0.5586
5504/5677 [============================>.] - ETA: 16s - loss: 0.6821 - acc: 0.5596
5568/5677 [============================>.] - ETA: 10s - loss: 0.6818 - acc: 0.5596
5632/5677 [============================>.] - ETA: 4s - loss: 0.6813 - acc: 0.5611 
5677/5677 [==============================] - 560s 99ms/step - loss: 0.6815 - acc: 0.5612 - val_loss: 0.6797 - val_acc: 0.5753

Epoch 00005: val_acc improved from 0.54834 to 0.57528, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window17/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 11:01 - loss: 0.6355 - acc: 0.6875
 128/5677 [..............................] - ETA: 10:15 - loss: 0.6454 - acc: 0.6484
 192/5677 [>.............................] - ETA: 10:11 - loss: 0.6598 - acc: 0.6042
 256/5677 [>.............................] - ETA: 9:55 - loss: 0.6816 - acc: 0.5664 
 320/5677 [>.............................] - ETA: 9:52 - loss: 0.6876 - acc: 0.5563
 384/5677 [=>............................] - ETA: 9:34 - loss: 0.6889 - acc: 0.5521
 448/5677 [=>............................] - ETA: 9:25 - loss: 0.6871 - acc: 0.5603
 512/5677 [=>............................] - ETA: 9:14 - loss: 0.6934 - acc: 0.5527
 576/5677 [==>...........................] - ETA: 9:08 - loss: 0.6899 - acc: 0.5556
 640/5677 [==>...........................] - ETA: 9:04 - loss: 0.6900 - acc: 0.5609
 704/5677 [==>...........................] - ETA: 8:53 - loss: 0.6868 - acc: 0.5668
 768/5677 [===>..........................] - ETA: 8:47 - loss: 0.6898 - acc: 0.5638
 832/5677 [===>..........................] - ETA: 8:36 - loss: 0.6904 - acc: 0.5661
 896/5677 [===>..........................] - ETA: 8:27 - loss: 0.6886 - acc: 0.5737
 960/5677 [====>.........................] - ETA: 8:14 - loss: 0.6875 - acc: 0.5792
1024/5677 [====>.........................] - ETA: 8:01 - loss: 0.6878 - acc: 0.5791
1088/5677 [====>.........................] - ETA: 7:49 - loss: 0.6856 - acc: 0.5836
1152/5677 [=====>........................] - ETA: 7:39 - loss: 0.6838 - acc: 0.5851
1216/5677 [=====>........................] - ETA: 7:28 - loss: 0.6810 - acc: 0.5880
1280/5677 [=====>........................] - ETA: 7:19 - loss: 0.6823 - acc: 0.5852
1344/5677 [======>.......................] - ETA: 7:11 - loss: 0.6809 - acc: 0.5871
1408/5677 [======>.......................] - ETA: 7:06 - loss: 0.6797 - acc: 0.5888
1472/5677 [======>.......................] - ETA: 7:00 - loss: 0.6779 - acc: 0.5917
1536/5677 [=======>......................] - ETA: 6:54 - loss: 0.6785 - acc: 0.5885
1600/5677 [=======>......................] - ETA: 6:45 - loss: 0.6780 - acc: 0.5887
1664/5677 [=======>......................] - ETA: 6:37 - loss: 0.6781 - acc: 0.5889
1728/5677 [========>.....................] - ETA: 6:28 - loss: 0.6774 - acc: 0.5920
1792/5677 [========>.....................] - ETA: 6:21 - loss: 0.6775 - acc: 0.5898
1856/5677 [========>.....................] - ETA: 6:13 - loss: 0.6771 - acc: 0.5911
1920/5677 [=========>....................] - ETA: 6:05 - loss: 0.6774 - acc: 0.5906
1984/5677 [=========>....................] - ETA: 6:00 - loss: 0.6789 - acc: 0.5887
2048/5677 [=========>....................] - ETA: 5:56 - loss: 0.6793 - acc: 0.5864
2112/5677 [==========>...................] - ETA: 5:51 - loss: 0.6785 - acc: 0.5881
2176/5677 [==========>...................] - ETA: 5:44 - loss: 0.6795 - acc: 0.5864
2240/5677 [==========>...................] - ETA: 5:37 - loss: 0.6786 - acc: 0.5888
2304/5677 [===========>..................] - ETA: 5:31 - loss: 0.6782 - acc: 0.5881
2368/5677 [===========>..................] - ETA: 5:24 - loss: 0.6792 - acc: 0.5861
2432/5677 [===========>..................] - ETA: 5:17 - loss: 0.6797 - acc: 0.5855
2496/5677 [============>.................] - ETA: 5:09 - loss: 0.6785 - acc: 0.5869
2560/5677 [============>.................] - ETA: 5:02 - loss: 0.6785 - acc: 0.5887
2624/5677 [============>.................] - ETA: 4:55 - loss: 0.6780 - acc: 0.5884
2688/5677 [=============>................] - ETA: 4:49 - loss: 0.6794 - acc: 0.5856
2752/5677 [=============>................] - ETA: 4:45 - loss: 0.6780 - acc: 0.5879
2816/5677 [=============>................] - ETA: 4:40 - loss: 0.6782 - acc: 0.5859
2880/5677 [==============>...............] - ETA: 4:34 - loss: 0.6786 - acc: 0.5844
2944/5677 [==============>...............] - ETA: 4:28 - loss: 0.6776 - acc: 0.5859
3008/5677 [==============>...............] - ETA: 4:21 - loss: 0.6791 - acc: 0.5821
3072/5677 [===============>..............] - ETA: 4:14 - loss: 0.6792 - acc: 0.5827
3136/5677 [===============>..............] - ETA: 4:07 - loss: 0.6800 - acc: 0.5807
3200/5677 [===============>..............] - ETA: 4:00 - loss: 0.6804 - acc: 0.5813
3264/5677 [================>.............] - ETA: 3:54 - loss: 0.6806 - acc: 0.5806
3328/5677 [================>.............] - ETA: 3:47 - loss: 0.6804 - acc: 0.5802
3392/5677 [================>.............] - ETA: 3:41 - loss: 0.6801 - acc: 0.5817
3456/5677 [=================>............] - ETA: 3:35 - loss: 0.6800 - acc: 0.5822
3520/5677 [=================>............] - ETA: 3:29 - loss: 0.6796 - acc: 0.5824
3584/5677 [=================>............] - ETA: 3:24 - loss: 0.6794 - acc: 0.5812
3648/5677 [==================>...........] - ETA: 3:18 - loss: 0.6793 - acc: 0.5809
3712/5677 [==================>...........] - ETA: 3:11 - loss: 0.6794 - acc: 0.5800
3776/5677 [==================>...........] - ETA: 3:04 - loss: 0.6796 - acc: 0.5781
3840/5677 [===================>..........] - ETA: 2:58 - loss: 0.6796 - acc: 0.5794
3904/5677 [===================>..........] - ETA: 2:51 - loss: 0.6806 - acc: 0.5774
3968/5677 [===================>..........] - ETA: 2:45 - loss: 0.6810 - acc: 0.5769
4032/5677 [====================>.........] - ETA: 2:38 - loss: 0.6816 - acc: 0.5754
4096/5677 [====================>.........] - ETA: 2:33 - loss: 0.6817 - acc: 0.5747
4160/5677 [====================>.........] - ETA: 2:27 - loss: 0.6819 - acc: 0.5738
4224/5677 [=====================>........] - ETA: 2:20 - loss: 0.6822 - acc: 0.5739
4288/5677 [=====================>........] - ETA: 2:14 - loss: 0.6823 - acc: 0.5739
4352/5677 [=====================>........] - ETA: 2:08 - loss: 0.6825 - acc: 0.5728
4416/5677 [======================>.......] - ETA: 2:02 - loss: 0.6824 - acc: 0.5725
4480/5677 [======================>.......] - ETA: 1:55 - loss: 0.6825 - acc: 0.5721
4544/5677 [=======================>......] - ETA: 1:49 - loss: 0.6821 - acc: 0.5733
4608/5677 [=======================>......] - ETA: 1:42 - loss: 0.6820 - acc: 0.5731
4672/5677 [=======================>......] - ETA: 1:36 - loss: 0.6819 - acc: 0.5738
4736/5677 [========================>.....] - ETA: 1:30 - loss: 0.6817 - acc: 0.5747
4800/5677 [========================>.....] - ETA: 1:23 - loss: 0.6810 - acc: 0.5771
4864/5677 [========================>.....] - ETA: 1:17 - loss: 0.6809 - acc: 0.5773
4928/5677 [=========================>....] - ETA: 1:11 - loss: 0.6807 - acc: 0.5777
4992/5677 [=========================>....] - ETA: 1:05 - loss: 0.6805 - acc: 0.5785
5056/5677 [=========================>....] - ETA: 59s - loss: 0.6801 - acc: 0.5791 
5120/5677 [==========================>...] - ETA: 53s - loss: 0.6804 - acc: 0.5787
5184/5677 [==========================>...] - ETA: 47s - loss: 0.6808 - acc: 0.5772
5248/5677 [==========================>...] - ETA: 40s - loss: 0.6807 - acc: 0.5766
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6806 - acc: 0.5776
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6805 - acc: 0.5778
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6799 - acc: 0.5789
5504/5677 [============================>.] - ETA: 16s - loss: 0.6799 - acc: 0.5794
5568/5677 [============================>.] - ETA: 10s - loss: 0.6794 - acc: 0.5796
5632/5677 [============================>.] - ETA: 4s - loss: 0.6796 - acc: 0.5792 
5677/5677 [==============================] - 561s 99ms/step - loss: 0.6794 - acc: 0.5795 - val_loss: 0.6998 - val_acc: 0.5420

Epoch 00006: val_acc did not improve from 0.57528
Epoch 7/10

  64/5677 [..............................] - ETA: 9:13 - loss: 0.6924 - acc: 0.5625
 128/5677 [..............................] - ETA: 9:29 - loss: 0.6868 - acc: 0.5703
 192/5677 [>.............................] - ETA: 9:25 - loss: 0.6722 - acc: 0.5938
 256/5677 [>.............................] - ETA: 9:08 - loss: 0.6711 - acc: 0.6016
 320/5677 [>.............................] - ETA: 9:04 - loss: 0.6684 - acc: 0.6000
 384/5677 [=>............................] - ETA: 9:00 - loss: 0.6720 - acc: 0.5833
 448/5677 [=>............................] - ETA: 9:04 - loss: 0.6697 - acc: 0.5893
 512/5677 [=>............................] - ETA: 8:56 - loss: 0.6697 - acc: 0.5820
 576/5677 [==>...........................] - ETA: 8:48 - loss: 0.6752 - acc: 0.5764
 640/5677 [==>...........................] - ETA: 8:46 - loss: 0.6713 - acc: 0.5859
 704/5677 [==>...........................] - ETA: 8:41 - loss: 0.6734 - acc: 0.5852
 768/5677 [===>..........................] - ETA: 8:37 - loss: 0.6705 - acc: 0.5911
 832/5677 [===>..........................] - ETA: 8:32 - loss: 0.6731 - acc: 0.5877
 896/5677 [===>..........................] - ETA: 8:25 - loss: 0.6703 - acc: 0.5938
 960/5677 [====>.........................] - ETA: 8:18 - loss: 0.6754 - acc: 0.5802
1024/5677 [====>.........................] - ETA: 8:11 - loss: 0.6750 - acc: 0.5811
1088/5677 [====>.........................] - ETA: 8:08 - loss: 0.6743 - acc: 0.5827
1152/5677 [=====>........................] - ETA: 8:00 - loss: 0.6759 - acc: 0.5816
1216/5677 [=====>........................] - ETA: 7:54 - loss: 0.6771 - acc: 0.5773
1280/5677 [=====>........................] - ETA: 7:47 - loss: 0.6750 - acc: 0.5820
1344/5677 [======>.......................] - ETA: 7:38 - loss: 0.6759 - acc: 0.5781
1408/5677 [======>.......................] - ETA: 7:31 - loss: 0.6776 - acc: 0.5724
1472/5677 [======>.......................] - ETA: 7:24 - loss: 0.6769 - acc: 0.5734
1536/5677 [=======>......................] - ETA: 7:16 - loss: 0.6768 - acc: 0.5729
1600/5677 [=======>......................] - ETA: 7:12 - loss: 0.6772 - acc: 0.5706
1664/5677 [=======>......................] - ETA: 7:04 - loss: 0.6763 - acc: 0.5709
1728/5677 [========>.....................] - ETA: 6:58 - loss: 0.6767 - acc: 0.5694
1792/5677 [========>.....................] - ETA: 6:51 - loss: 0.6765 - acc: 0.5709
1856/5677 [========>.....................] - ETA: 6:43 - loss: 0.6766 - acc: 0.5706
1920/5677 [=========>....................] - ETA: 6:35 - loss: 0.6748 - acc: 0.5734
1984/5677 [=========>....................] - ETA: 6:28 - loss: 0.6740 - acc: 0.5766
2048/5677 [=========>....................] - ETA: 6:21 - loss: 0.6749 - acc: 0.5718
2112/5677 [==========>...................] - ETA: 6:14 - loss: 0.6746 - acc: 0.5734
2176/5677 [==========>...................] - ETA: 6:08 - loss: 0.6754 - acc: 0.5731
2240/5677 [==========>...................] - ETA: 6:00 - loss: 0.6749 - acc: 0.5746
2304/5677 [===========>..................] - ETA: 5:52 - loss: 0.6753 - acc: 0.5742
2368/5677 [===========>..................] - ETA: 5:44 - loss: 0.6749 - acc: 0.5756
2432/5677 [===========>..................] - ETA: 5:36 - loss: 0.6748 - acc: 0.5757
2496/5677 [============>.................] - ETA: 5:27 - loss: 0.6749 - acc: 0.5753
2560/5677 [============>.................] - ETA: 5:19 - loss: 0.6747 - acc: 0.5766
2624/5677 [============>.................] - ETA: 5:11 - loss: 0.6740 - acc: 0.5789
2688/5677 [=============>................] - ETA: 5:03 - loss: 0.6727 - acc: 0.5811
2752/5677 [=============>................] - ETA: 4:55 - loss: 0.6727 - acc: 0.5807
2816/5677 [=============>................] - ETA: 4:49 - loss: 0.6730 - acc: 0.5799
2880/5677 [==============>...............] - ETA: 4:42 - loss: 0.6741 - acc: 0.5792
2944/5677 [==============>...............] - ETA: 4:35 - loss: 0.6739 - acc: 0.5791
3008/5677 [==============>...............] - ETA: 4:28 - loss: 0.6727 - acc: 0.5818
3072/5677 [===============>..............] - ETA: 4:21 - loss: 0.6728 - acc: 0.5837
3136/5677 [===============>..............] - ETA: 4:14 - loss: 0.6720 - acc: 0.5845
3200/5677 [===============>..............] - ETA: 4:07 - loss: 0.6714 - acc: 0.5853
3264/5677 [================>.............] - ETA: 4:00 - loss: 0.6709 - acc: 0.5882
3328/5677 [================>.............] - ETA: 3:53 - loss: 0.6709 - acc: 0.5889
3392/5677 [================>.............] - ETA: 3:48 - loss: 0.6721 - acc: 0.5864
3456/5677 [=================>............] - ETA: 3:42 - loss: 0.6711 - acc: 0.5871
3520/5677 [=================>............] - ETA: 3:35 - loss: 0.6713 - acc: 0.5866
3584/5677 [=================>............] - ETA: 3:29 - loss: 0.6708 - acc: 0.5862
3648/5677 [==================>...........] - ETA: 3:22 - loss: 0.6697 - acc: 0.5874
3712/5677 [==================>...........] - ETA: 3:16 - loss: 0.6707 - acc: 0.5854
3776/5677 [==================>...........] - ETA: 3:09 - loss: 0.6713 - acc: 0.5847
3840/5677 [===================>..........] - ETA: 3:02 - loss: 0.6720 - acc: 0.5844
3904/5677 [===================>..........] - ETA: 2:55 - loss: 0.6722 - acc: 0.5840
3968/5677 [===================>..........] - ETA: 2:49 - loss: 0.6723 - acc: 0.5844
4032/5677 [====================>.........] - ETA: 2:43 - loss: 0.6718 - acc: 0.5858
4096/5677 [====================>.........] - ETA: 2:37 - loss: 0.6721 - acc: 0.5859
4160/5677 [====================>.........] - ETA: 2:30 - loss: 0.6717 - acc: 0.5875
4224/5677 [=====================>........] - ETA: 2:24 - loss: 0.6718 - acc: 0.5871
4288/5677 [=====================>........] - ETA: 2:17 - loss: 0.6718 - acc: 0.5870
4352/5677 [=====================>........] - ETA: 2:11 - loss: 0.6721 - acc: 0.5873
4416/5677 [======================>.......] - ETA: 2:04 - loss: 0.6721 - acc: 0.5872
4480/5677 [======================>.......] - ETA: 1:58 - loss: 0.6719 - acc: 0.5873
4544/5677 [=======================>......] - ETA: 1:51 - loss: 0.6718 - acc: 0.5876
4608/5677 [=======================>......] - ETA: 1:45 - loss: 0.6717 - acc: 0.5875
4672/5677 [=======================>......] - ETA: 1:38 - loss: 0.6726 - acc: 0.5856
4736/5677 [========================>.....] - ETA: 1:32 - loss: 0.6730 - acc: 0.5855
4800/5677 [========================>.....] - ETA: 1:26 - loss: 0.6733 - acc: 0.5854
4864/5677 [========================>.....] - ETA: 1:20 - loss: 0.6736 - acc: 0.5851
4928/5677 [=========================>....] - ETA: 1:13 - loss: 0.6738 - acc: 0.5848
4992/5677 [=========================>....] - ETA: 1:07 - loss: 0.6741 - acc: 0.5839
5056/5677 [=========================>....] - ETA: 1:01 - loss: 0.6742 - acc: 0.5843
5120/5677 [==========================>...] - ETA: 54s - loss: 0.6745 - acc: 0.5836 
5184/5677 [==========================>...] - ETA: 48s - loss: 0.6751 - acc: 0.5816
5248/5677 [==========================>...] - ETA: 42s - loss: 0.6750 - acc: 0.5821
5312/5677 [===========================>..] - ETA: 35s - loss: 0.6747 - acc: 0.5825
5376/5677 [===========================>..] - ETA: 29s - loss: 0.6749 - acc: 0.5815
5440/5677 [===========================>..] - ETA: 23s - loss: 0.6751 - acc: 0.5813
5504/5677 [============================>.] - ETA: 17s - loss: 0.6751 - acc: 0.5803
5568/5677 [============================>.] - ETA: 10s - loss: 0.6749 - acc: 0.5814
5632/5677 [============================>.] - ETA: 4s - loss: 0.6753 - acc: 0.5801 
5677/5677 [==============================] - 579s 102ms/step - loss: 0.6752 - acc: 0.5806 - val_loss: 0.6876 - val_acc: 0.5531

Epoch 00007: val_acc did not improve from 0.57528
Epoch 8/10

  64/5677 [..............................] - ETA: 7:57 - loss: 0.6600 - acc: 0.5469
 128/5677 [..............................] - ETA: 8:43 - loss: 0.6469 - acc: 0.6172
 192/5677 [>.............................] - ETA: 9:20 - loss: 0.6541 - acc: 0.5938
 256/5677 [>.............................] - ETA: 9:10 - loss: 0.6588 - acc: 0.5859
 320/5677 [>.............................] - ETA: 9:10 - loss: 0.6543 - acc: 0.6000
 384/5677 [=>............................] - ETA: 8:57 - loss: 0.6483 - acc: 0.6172
 448/5677 [=>............................] - ETA: 8:42 - loss: 0.6502 - acc: 0.6161
 512/5677 [=>............................] - ETA: 8:30 - loss: 0.6551 - acc: 0.6074
 576/5677 [==>...........................] - ETA: 8:15 - loss: 0.6529 - acc: 0.6163
 640/5677 [==>...........................] - ETA: 8:02 - loss: 0.6524 - acc: 0.6203
 704/5677 [==>...........................] - ETA: 7:49 - loss: 0.6512 - acc: 0.6236
 768/5677 [===>..........................] - ETA: 7:40 - loss: 0.6526 - acc: 0.6198
 832/5677 [===>..........................] - ETA: 7:34 - loss: 0.6543 - acc: 0.6202
 896/5677 [===>..........................] - ETA: 7:33 - loss: 0.6578 - acc: 0.6183
 960/5677 [====>.........................] - ETA: 7:27 - loss: 0.6612 - acc: 0.6094
1024/5677 [====>.........................] - ETA: 7:21 - loss: 0.6595 - acc: 0.6133
1088/5677 [====>.........................] - ETA: 7:16 - loss: 0.6604 - acc: 0.6112
1152/5677 [=====>........................] - ETA: 7:09 - loss: 0.6597 - acc: 0.6146
1216/5677 [=====>........................] - ETA: 7:02 - loss: 0.6656 - acc: 0.6028
1280/5677 [=====>........................] - ETA: 6:55 - loss: 0.6670 - acc: 0.5969
1344/5677 [======>.......................] - ETA: 6:47 - loss: 0.6671 - acc: 0.5975
1408/5677 [======>.......................] - ETA: 6:39 - loss: 0.6677 - acc: 0.5966
1472/5677 [======>.......................] - ETA: 6:33 - loss: 0.6706 - acc: 0.5897
1536/5677 [=======>......................] - ETA: 6:31 - loss: 0.6703 - acc: 0.5885
1600/5677 [=======>......................] - ETA: 6:26 - loss: 0.6698 - acc: 0.5913
1664/5677 [=======>......................] - ETA: 6:21 - loss: 0.6694 - acc: 0.5913
1728/5677 [========>.....................] - ETA: 6:16 - loss: 0.6697 - acc: 0.5903
1792/5677 [========>.....................] - ETA: 6:11 - loss: 0.6687 - acc: 0.5921
1856/5677 [========>.....................] - ETA: 6:06 - loss: 0.6701 - acc: 0.5884
1920/5677 [=========>....................] - ETA: 6:01 - loss: 0.6698 - acc: 0.5865
1984/5677 [=========>....................] - ETA: 5:56 - loss: 0.6695 - acc: 0.5867
2048/5677 [=========>....................] - ETA: 5:52 - loss: 0.6707 - acc: 0.5854
2112/5677 [==========>...................] - ETA: 5:46 - loss: 0.6711 - acc: 0.5843
2176/5677 [==========>...................] - ETA: 5:40 - loss: 0.6694 - acc: 0.5873
2240/5677 [==========>...................] - ETA: 5:36 - loss: 0.6696 - acc: 0.5866
2304/5677 [===========>..................] - ETA: 5:31 - loss: 0.6683 - acc: 0.5885
2368/5677 [===========>..................] - ETA: 5:25 - loss: 0.6674 - acc: 0.5904
2432/5677 [===========>..................] - ETA: 5:19 - loss: 0.6687 - acc: 0.5892
2496/5677 [============>.................] - ETA: 5:13 - loss: 0.6701 - acc: 0.5857
2560/5677 [============>.................] - ETA: 5:07 - loss: 0.6701 - acc: 0.5848
2624/5677 [============>.................] - ETA: 5:02 - loss: 0.6712 - acc: 0.5819
2688/5677 [=============>................] - ETA: 4:55 - loss: 0.6711 - acc: 0.5833
2752/5677 [=============>................] - ETA: 4:49 - loss: 0.6706 - acc: 0.5836
2816/5677 [=============>................] - ETA: 4:43 - loss: 0.6705 - acc: 0.5831
2880/5677 [==============>...............] - ETA: 4:36 - loss: 0.6709 - acc: 0.5833
2944/5677 [==============>...............] - ETA: 4:31 - loss: 0.6707 - acc: 0.5846
3008/5677 [==============>...............] - ETA: 4:25 - loss: 0.6713 - acc: 0.5821
3072/5677 [===============>..............] - ETA: 4:19 - loss: 0.6722 - acc: 0.5807
3136/5677 [===============>..............] - ETA: 4:12 - loss: 0.6726 - acc: 0.5804
3200/5677 [===============>..............] - ETA: 4:06 - loss: 0.6731 - acc: 0.5784
3264/5677 [================>.............] - ETA: 3:59 - loss: 0.6732 - acc: 0.5797
3328/5677 [================>.............] - ETA: 3:53 - loss: 0.6738 - acc: 0.5766
3392/5677 [================>.............] - ETA: 3:47 - loss: 0.6737 - acc: 0.5764
3456/5677 [=================>............] - ETA: 3:41 - loss: 0.6742 - acc: 0.5761
3520/5677 [=================>............] - ETA: 3:35 - loss: 0.6741 - acc: 0.5764
3584/5677 [=================>............] - ETA: 3:28 - loss: 0.6744 - acc: 0.5759
3648/5677 [==================>...........] - ETA: 3:22 - loss: 0.6744 - acc: 0.5754
3712/5677 [==================>...........] - ETA: 3:16 - loss: 0.6737 - acc: 0.5762
3776/5677 [==================>...........] - ETA: 3:09 - loss: 0.6735 - acc: 0.5763
3840/5677 [===================>..........] - ETA: 3:03 - loss: 0.6732 - acc: 0.5758
3904/5677 [===================>..........] - ETA: 2:56 - loss: 0.6735 - acc: 0.5758
3968/5677 [===================>..........] - ETA: 2:50 - loss: 0.6735 - acc: 0.5748
4032/5677 [====================>.........] - ETA: 2:44 - loss: 0.6736 - acc: 0.5751
4096/5677 [====================>.........] - ETA: 2:37 - loss: 0.6733 - acc: 0.5745
4160/5677 [====================>.........] - ETA: 2:31 - loss: 0.6729 - acc: 0.5767
4224/5677 [=====================>........] - ETA: 2:25 - loss: 0.6729 - acc: 0.5772
4288/5677 [=====================>........] - ETA: 2:18 - loss: 0.6729 - acc: 0.5777
4352/5677 [=====================>........] - ETA: 2:12 - loss: 0.6729 - acc: 0.5781
4416/5677 [======================>.......] - ETA: 2:05 - loss: 0.6724 - acc: 0.5797
4480/5677 [======================>.......] - ETA: 1:59 - loss: 0.6723 - acc: 0.5806
4544/5677 [=======================>......] - ETA: 1:53 - loss: 0.6721 - acc: 0.5808
4608/5677 [=======================>......] - ETA: 1:46 - loss: 0.6721 - acc: 0.5801
4672/5677 [=======================>......] - ETA: 1:40 - loss: 0.6721 - acc: 0.5803
4736/5677 [========================>.....] - ETA: 1:33 - loss: 0.6720 - acc: 0.5802
4800/5677 [========================>.....] - ETA: 1:27 - loss: 0.6721 - acc: 0.5804
4864/5677 [========================>.....] - ETA: 1:20 - loss: 0.6724 - acc: 0.5798
4928/5677 [=========================>....] - ETA: 1:14 - loss: 0.6718 - acc: 0.5810
4992/5677 [=========================>....] - ETA: 1:08 - loss: 0.6721 - acc: 0.5807
5056/5677 [=========================>....] - ETA: 1:01 - loss: 0.6719 - acc: 0.5815
5120/5677 [==========================>...] - ETA: 55s - loss: 0.6722 - acc: 0.5811 
5184/5677 [==========================>...] - ETA: 49s - loss: 0.6724 - acc: 0.5808
5248/5677 [==========================>...] - ETA: 42s - loss: 0.6722 - acc: 0.5812
5312/5677 [===========================>..] - ETA: 36s - loss: 0.6722 - acc: 0.5815
5376/5677 [===========================>..] - ETA: 29s - loss: 0.6726 - acc: 0.5811
5440/5677 [===========================>..] - ETA: 23s - loss: 0.6726 - acc: 0.5814
5504/5677 [============================>.] - ETA: 17s - loss: 0.6725 - acc: 0.5816
5568/5677 [============================>.] - ETA: 10s - loss: 0.6730 - acc: 0.5806
5632/5677 [============================>.] - ETA: 4s - loss: 0.6730 - acc: 0.5811 
5677/5677 [==============================] - 584s 103ms/step - loss: 0.6732 - acc: 0.5806 - val_loss: 0.6883 - val_acc: 0.5610

Epoch 00008: val_acc did not improve from 0.57528
Epoch 9/10

  64/5677 [..............................] - ETA: 7:19 - loss: 0.6903 - acc: 0.5938
 128/5677 [..............................] - ETA: 7:19 - loss: 0.6848 - acc: 0.6094
 192/5677 [>.............................] - ETA: 7:20 - loss: 0.7012 - acc: 0.5573
 256/5677 [>.............................] - ETA: 7:17 - loss: 0.7082 - acc: 0.5469
 320/5677 [>.............................] - ETA: 7:04 - loss: 0.7019 - acc: 0.5531
 384/5677 [=>............................] - ETA: 6:58 - loss: 0.6940 - acc: 0.5729
 448/5677 [=>............................] - ETA: 6:53 - loss: 0.6900 - acc: 0.5848
 512/5677 [=>............................] - ETA: 6:57 - loss: 0.6903 - acc: 0.5703
 576/5677 [==>...........................] - ETA: 7:01 - loss: 0.6859 - acc: 0.5694
 640/5677 [==>...........................] - ETA: 7:00 - loss: 0.6836 - acc: 0.5766
 704/5677 [==>...........................] - ETA: 7:02 - loss: 0.6846 - acc: 0.5795
 768/5677 [===>..........................] - ETA: 6:56 - loss: 0.6881 - acc: 0.5703
 832/5677 [===>..........................] - ETA: 6:51 - loss: 0.6897 - acc: 0.5685
 896/5677 [===>..........................] - ETA: 6:45 - loss: 0.6908 - acc: 0.5670
 960/5677 [====>.........................] - ETA: 6:40 - loss: 0.6895 - acc: 0.5656
1024/5677 [====>.........................] - ETA: 6:33 - loss: 0.6910 - acc: 0.5586
1088/5677 [====>.........................] - ETA: 6:30 - loss: 0.6885 - acc: 0.5643
1152/5677 [=====>........................] - ETA: 6:28 - loss: 0.6871 - acc: 0.5660
1216/5677 [=====>........................] - ETA: 6:26 - loss: 0.6862 - acc: 0.5683
1280/5677 [=====>........................] - ETA: 6:22 - loss: 0.6848 - acc: 0.5719
1344/5677 [======>.......................] - ETA: 6:18 - loss: 0.6853 - acc: 0.5670
1408/5677 [======>.......................] - ETA: 6:12 - loss: 0.6858 - acc: 0.5653
1472/5677 [======>.......................] - ETA: 6:06 - loss: 0.6842 - acc: 0.5693
1536/5677 [=======>......................] - ETA: 5:59 - loss: 0.6828 - acc: 0.5697
1600/5677 [=======>......................] - ETA: 5:52 - loss: 0.6831 - acc: 0.5687
1664/5677 [=======>......................] - ETA: 5:47 - loss: 0.6819 - acc: 0.5727
1728/5677 [========>.....................] - ETA: 5:41 - loss: 0.6827 - acc: 0.5706
1792/5677 [========>.....................] - ETA: 5:35 - loss: 0.6816 - acc: 0.5725
1856/5677 [========>.....................] - ETA: 5:32 - loss: 0.6811 - acc: 0.5744
1920/5677 [=========>....................] - ETA: 5:27 - loss: 0.6799 - acc: 0.5750
1984/5677 [=========>....................] - ETA: 5:22 - loss: 0.6804 - acc: 0.5726
2048/5677 [=========>....................] - ETA: 5:16 - loss: 0.6792 - acc: 0.5742
2112/5677 [==========>...................] - ETA: 5:10 - loss: 0.6799 - acc: 0.5720
2176/5677 [==========>...................] - ETA: 5:04 - loss: 0.6790 - acc: 0.5731
2240/5677 [==========>...................] - ETA: 4:57 - loss: 0.6792 - acc: 0.5728
2304/5677 [===========>..................] - ETA: 4:51 - loss: 0.6785 - acc: 0.5747
2368/5677 [===========>..................] - ETA: 4:45 - loss: 0.6791 - acc: 0.5739
2432/5677 [===========>..................] - ETA: 4:39 - loss: 0.6782 - acc: 0.5761
2496/5677 [============>.................] - ETA: 4:35 - loss: 0.6784 - acc: 0.5769
2560/5677 [============>.................] - ETA: 4:31 - loss: 0.6796 - acc: 0.5742
2624/5677 [============>.................] - ETA: 4:25 - loss: 0.6785 - acc: 0.5762
2688/5677 [=============>................] - ETA: 4:21 - loss: 0.6784 - acc: 0.5763
2752/5677 [=============>................] - ETA: 4:17 - loss: 0.6779 - acc: 0.5774
2816/5677 [=============>................] - ETA: 4:10 - loss: 0.6776 - acc: 0.5781
2880/5677 [==============>...............] - ETA: 4:05 - loss: 0.6771 - acc: 0.5778
2944/5677 [==============>...............] - ETA: 3:59 - loss: 0.6777 - acc: 0.5785
3008/5677 [==============>...............] - ETA: 3:52 - loss: 0.6773 - acc: 0.5785
3072/5677 [===============>..............] - ETA: 3:46 - loss: 0.6764 - acc: 0.5804
3136/5677 [===============>..............] - ETA: 3:40 - loss: 0.6770 - acc: 0.5804
3200/5677 [===============>..............] - ETA: 3:34 - loss: 0.6786 - acc: 0.5772
3264/5677 [================>.............] - ETA: 3:30 - loss: 0.6789 - acc: 0.5778
3328/5677 [================>.............] - ETA: 3:25 - loss: 0.6785 - acc: 0.5769
3392/5677 [================>.............] - ETA: 3:20 - loss: 0.6799 - acc: 0.5755
3456/5677 [=================>............] - ETA: 3:15 - loss: 0.6795 - acc: 0.5761
3520/5677 [=================>............] - ETA: 3:09 - loss: 0.6798 - acc: 0.5747
3584/5677 [=================>............] - ETA: 3:03 - loss: 0.6795 - acc: 0.5753
3648/5677 [==================>...........] - ETA: 2:58 - loss: 0.6790 - acc: 0.5768
3712/5677 [==================>...........] - ETA: 2:53 - loss: 0.6783 - acc: 0.5773
3776/5677 [==================>...........] - ETA: 2:48 - loss: 0.6783 - acc: 0.5773
3840/5677 [===================>..........] - ETA: 2:42 - loss: 0.6785 - acc: 0.5763
3904/5677 [===================>..........] - ETA: 2:37 - loss: 0.6791 - acc: 0.5745
3968/5677 [===================>..........] - ETA: 2:32 - loss: 0.6795 - acc: 0.5733
4032/5677 [====================>.........] - ETA: 2:27 - loss: 0.6793 - acc: 0.5734
4096/5677 [====================>.........] - ETA: 2:22 - loss: 0.6792 - acc: 0.5732
4160/5677 [====================>.........] - ETA: 2:17 - loss: 0.6792 - acc: 0.5733
4224/5677 [=====================>........] - ETA: 2:11 - loss: 0.6796 - acc: 0.5715
4288/5677 [=====================>........] - ETA: 2:05 - loss: 0.6794 - acc: 0.5721
4352/5677 [=====================>........] - ETA: 2:00 - loss: 0.6792 - acc: 0.5710
4416/5677 [======================>.......] - ETA: 1:54 - loss: 0.6791 - acc: 0.5716
4480/5677 [======================>.......] - ETA: 1:48 - loss: 0.6790 - acc: 0.5717
4544/5677 [=======================>......] - ETA: 1:43 - loss: 0.6784 - acc: 0.5733
4608/5677 [=======================>......] - ETA: 1:37 - loss: 0.6783 - acc: 0.5727
4672/5677 [=======================>......] - ETA: 1:32 - loss: 0.6781 - acc: 0.5732
4736/5677 [========================>.....] - ETA: 1:26 - loss: 0.6776 - acc: 0.5741
4800/5677 [========================>.....] - ETA: 1:20 - loss: 0.6777 - acc: 0.5735
4864/5677 [========================>.....] - ETA: 1:14 - loss: 0.6776 - acc: 0.5740
4928/5677 [=========================>....] - ETA: 1:08 - loss: 0.6770 - acc: 0.5755
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6771 - acc: 0.5759
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6773 - acc: 0.5763 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6768 - acc: 0.5777
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6761 - acc: 0.5793
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6757 - acc: 0.5802
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6761 - acc: 0.5798
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6764 - acc: 0.5792
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6757 - acc: 0.5805
5504/5677 [============================>.] - ETA: 16s - loss: 0.6750 - acc: 0.5809
5568/5677 [============================>.] - ETA: 10s - loss: 0.6759 - acc: 0.5797
5632/5677 [============================>.] - ETA: 4s - loss: 0.6755 - acc: 0.5808 
5677/5677 [==============================] - 562s 99ms/step - loss: 0.6755 - acc: 0.5809 - val_loss: 0.6966 - val_acc: 0.5468

Epoch 00009: val_acc did not improve from 0.57528
Epoch 10/10

  64/5677 [..............................] - ETA: 11:24 - loss: 0.6733 - acc: 0.6094
 128/5677 [..............................] - ETA: 10:49 - loss: 0.6772 - acc: 0.5938
 192/5677 [>.............................] - ETA: 10:30 - loss: 0.6813 - acc: 0.5990
 256/5677 [>.............................] - ETA: 10:20 - loss: 0.6832 - acc: 0.5781
 320/5677 [>.............................] - ETA: 10:23 - loss: 0.6842 - acc: 0.5781
 384/5677 [=>............................] - ETA: 10:23 - loss: 0.6888 - acc: 0.5677
 448/5677 [=>............................] - ETA: 10:14 - loss: 0.6889 - acc: 0.5625
 512/5677 [=>............................] - ETA: 10:01 - loss: 0.6859 - acc: 0.5664
 576/5677 [==>...........................] - ETA: 9:53 - loss: 0.6823 - acc: 0.5712 
 640/5677 [==>...........................] - ETA: 9:45 - loss: 0.6816 - acc: 0.5703
 704/5677 [==>...........................] - ETA: 9:37 - loss: 0.6835 - acc: 0.5668
 768/5677 [===>..........................] - ETA: 9:28 - loss: 0.6819 - acc: 0.5703
 832/5677 [===>..........................] - ETA: 9:19 - loss: 0.6792 - acc: 0.5733
 896/5677 [===>..........................] - ETA: 9:08 - loss: 0.6793 - acc: 0.5692
 960/5677 [====>.........................] - ETA: 8:58 - loss: 0.6791 - acc: 0.5656
1024/5677 [====>.........................] - ETA: 8:52 - loss: 0.6783 - acc: 0.5635
1088/5677 [====>.........................] - ETA: 8:46 - loss: 0.6773 - acc: 0.5662
1152/5677 [=====>........................] - ETA: 8:37 - loss: 0.6756 - acc: 0.5677
1216/5677 [=====>........................] - ETA: 8:30 - loss: 0.6762 - acc: 0.5658
1280/5677 [=====>........................] - ETA: 8:20 - loss: 0.6752 - acc: 0.5656
1344/5677 [======>.......................] - ETA: 8:12 - loss: 0.6761 - acc: 0.5640
1408/5677 [======>.......................] - ETA: 8:07 - loss: 0.6753 - acc: 0.5682
1472/5677 [======>.......................] - ETA: 8:00 - loss: 0.6758 - acc: 0.5693
1536/5677 [=======>......................] - ETA: 7:52 - loss: 0.6755 - acc: 0.5697
1600/5677 [=======>......................] - ETA: 7:45 - loss: 0.6760 - acc: 0.5694
1664/5677 [=======>......................] - ETA: 7:37 - loss: 0.6769 - acc: 0.5691
1728/5677 [========>.....................] - ETA: 7:30 - loss: 0.6764 - acc: 0.5706
1792/5677 [========>.....................] - ETA: 7:23 - loss: 0.6754 - acc: 0.5742
1856/5677 [========>.....................] - ETA: 7:15 - loss: 0.6751 - acc: 0.5738
1920/5677 [=========>....................] - ETA: 7:08 - loss: 0.6744 - acc: 0.5766
1984/5677 [=========>....................] - ETA: 7:01 - loss: 0.6742 - acc: 0.5776
2048/5677 [=========>....................] - ETA: 6:53 - loss: 0.6727 - acc: 0.5820
2112/5677 [==========>...................] - ETA: 6:46 - loss: 0.6722 - acc: 0.5833
2176/5677 [==========>...................] - ETA: 6:38 - loss: 0.6724 - acc: 0.5836
2240/5677 [==========>...................] - ETA: 6:30 - loss: 0.6721 - acc: 0.5848
2304/5677 [===========>..................] - ETA: 6:21 - loss: 0.6716 - acc: 0.5864
2368/5677 [===========>..................] - ETA: 6:13 - loss: 0.6723 - acc: 0.5853
2432/5677 [===========>..................] - ETA: 6:06 - loss: 0.6716 - acc: 0.5859
2496/5677 [============>.................] - ETA: 5:58 - loss: 0.6730 - acc: 0.5849
2560/5677 [============>.................] - ETA: 5:50 - loss: 0.6728 - acc: 0.5855
2624/5677 [============>.................] - ETA: 5:43 - loss: 0.6733 - acc: 0.5861
2688/5677 [=============>................] - ETA: 5:35 - loss: 0.6727 - acc: 0.5871
2752/5677 [=============>................] - ETA: 5:28 - loss: 0.6733 - acc: 0.5861
2816/5677 [=============>................] - ETA: 5:20 - loss: 0.6729 - acc: 0.5866
2880/5677 [==============>...............] - ETA: 5:13 - loss: 0.6728 - acc: 0.5865
2944/5677 [==============>...............] - ETA: 5:05 - loss: 0.6732 - acc: 0.5863
3008/5677 [==============>...............] - ETA: 4:59 - loss: 0.6739 - acc: 0.5868
3072/5677 [===============>..............] - ETA: 4:52 - loss: 0.6739 - acc: 0.5889
3136/5677 [===============>..............] - ETA: 4:44 - loss: 0.6733 - acc: 0.5896
3200/5677 [===============>..............] - ETA: 4:36 - loss: 0.6739 - acc: 0.5872
3264/5677 [================>.............] - ETA: 4:29 - loss: 0.6746 - acc: 0.5861
3328/5677 [================>.............] - ETA: 4:22 - loss: 0.6746 - acc: 0.5853
3392/5677 [================>.............] - ETA: 4:15 - loss: 0.6755 - acc: 0.5840
3456/5677 [=================>............] - ETA: 4:08 - loss: 0.6754 - acc: 0.5839
3520/5677 [=================>............] - ETA: 4:01 - loss: 0.6752 - acc: 0.5847
3584/5677 [=================>............] - ETA: 3:54 - loss: 0.6753 - acc: 0.5840
3648/5677 [==================>...........] - ETA: 3:47 - loss: 0.6749 - acc: 0.5842
3712/5677 [==================>...........] - ETA: 3:39 - loss: 0.6746 - acc: 0.5841
3776/5677 [==================>...........] - ETA: 3:32 - loss: 0.6742 - acc: 0.5850
3840/5677 [===================>..........] - ETA: 3:25 - loss: 0.6737 - acc: 0.5865
3904/5677 [===================>..........] - ETA: 3:18 - loss: 0.6739 - acc: 0.5861
3968/5677 [===================>..........] - ETA: 3:10 - loss: 0.6733 - acc: 0.5877
4032/5677 [====================>.........] - ETA: 3:03 - loss: 0.6740 - acc: 0.5863
4096/5677 [====================>.........] - ETA: 2:56 - loss: 0.6741 - acc: 0.5862
4160/5677 [====================>.........] - ETA: 2:49 - loss: 0.6738 - acc: 0.5858
4224/5677 [=====================>........] - ETA: 2:42 - loss: 0.6738 - acc: 0.5848
4288/5677 [=====================>........] - ETA: 2:34 - loss: 0.6739 - acc: 0.5847
4352/5677 [=====================>........] - ETA: 2:27 - loss: 0.6736 - acc: 0.5857
4416/5677 [======================>.......] - ETA: 2:20 - loss: 0.6739 - acc: 0.5854
4480/5677 [======================>.......] - ETA: 2:13 - loss: 0.6742 - acc: 0.5842
4544/5677 [=======================>......] - ETA: 2:06 - loss: 0.6738 - acc: 0.5838
4608/5677 [=======================>......] - ETA: 1:58 - loss: 0.6745 - acc: 0.5818
4672/5677 [=======================>......] - ETA: 1:51 - loss: 0.6743 - acc: 0.5818
4736/5677 [========================>.....] - ETA: 1:44 - loss: 0.6746 - acc: 0.5807
4800/5677 [========================>.....] - ETA: 1:37 - loss: 0.6749 - acc: 0.5796
4864/5677 [========================>.....] - ETA: 1:29 - loss: 0.6753 - acc: 0.5787
4928/5677 [=========================>....] - ETA: 1:22 - loss: 0.6752 - acc: 0.5791
4992/5677 [=========================>....] - ETA: 1:15 - loss: 0.6748 - acc: 0.5799
5056/5677 [=========================>....] - ETA: 1:08 - loss: 0.6743 - acc: 0.5811
5120/5677 [==========================>...] - ETA: 1:01 - loss: 0.6741 - acc: 0.5818
5184/5677 [==========================>...] - ETA: 54s - loss: 0.6742 - acc: 0.5810 
5248/5677 [==========================>...] - ETA: 47s - loss: 0.6740 - acc: 0.5814
5312/5677 [===========================>..] - ETA: 40s - loss: 0.6740 - acc: 0.5811
5376/5677 [===========================>..] - ETA: 33s - loss: 0.6739 - acc: 0.5811
5440/5677 [===========================>..] - ETA: 26s - loss: 0.6734 - acc: 0.5827
5504/5677 [============================>.] - ETA: 19s - loss: 0.6736 - acc: 0.5814
5568/5677 [============================>.] - ETA: 11s - loss: 0.6730 - acc: 0.5826
5632/5677 [============================>.] - ETA: 4s - loss: 0.6732 - acc: 0.5826 
5677/5677 [==============================] - 647s 114ms/step - loss: 0.6732 - acc: 0.5823 - val_loss: 0.6799 - val_acc: 0.5784

Epoch 00010: val_acc improved from 0.57528 to 0.57845, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window17/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe74ecc6c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe74ecc6c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe74ebc4a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe74ebc4a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe114132ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe114132ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe745758dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe745758dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74574a650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74574a650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf385f9b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf385f9b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe745774ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe745774ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7457581d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7457581d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe745625990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe745625990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7454c6b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe7454c6b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe745373f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe745373f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf3852c490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf3852c490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf185f9a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf185f9a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74527bb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74527bb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74536b390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74536b390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdeb44673d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdeb44673d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf38609910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf38609910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7452e6210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7452e6210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe744f6d7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe744f6d7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdeb4291810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdeb4291810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744ec6610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744ec6610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe744ec6a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe744ec6a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744ff9510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744ff9510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe745064b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe745064b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe744cd0c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe744cd0c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744d24390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744d24390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe744f58f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe744f58f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744c09090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744c09090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74492fa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74492fa50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74489cb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74489cb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744923150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744923150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe74492fa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe74492fa90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74491dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74491dfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74474aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74474aed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe744924b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe744924b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744654b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744654b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe744532390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe744532390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7445a3cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7445a3cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74438d990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74438d990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe744210dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe744210dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7445a3950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7445a3950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe74438d750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe74438d750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744210110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744210110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7441133d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe7441133d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe743fe2790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe743fe2790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744263890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe744263890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe744168d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe744168d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74425b810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74425b810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe743d58650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe743d58650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe743c29b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe743c29b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743caa610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743caa610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe744055790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe744055790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743c7fc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743c7fc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe743a19cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe743a19cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe743bd2210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe743bd2210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7439dbcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7439dbcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe743c20c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe743c20c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743bcf510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe743bcf510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe339ba2790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe339ba2790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe094242b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe094242b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0941ac690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0941ac690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe094487890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe094487890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe745262b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe745262b50>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 2:24
 128/1578 [=>............................] - ETA: 1:34
 192/1578 [==>...........................] - ETA: 1:20
 256/1578 [===>..........................] - ETA: 1:09
 320/1578 [=====>........................] - ETA: 1:01
 384/1578 [======>.......................] - ETA: 55s 
 448/1578 [=======>......................] - ETA: 50s
 512/1578 [========>.....................] - ETA: 46s
 576/1578 [=========>....................] - ETA: 43s
 640/1578 [===========>..................] - ETA: 40s
 704/1578 [============>.................] - ETA: 36s
 768/1578 [=============>................] - ETA: 32s
 832/1578 [==============>...............] - ETA: 30s
 896/1578 [================>.............] - ETA: 27s
 960/1578 [=================>............] - ETA: 24s
1024/1578 [==================>...........] - ETA: 21s
1088/1578 [===================>..........] - ETA: 19s
1152/1578 [====================>.........] - ETA: 16s
1216/1578 [======================>.......] - ETA: 14s
1280/1578 [=======================>......] - ETA: 12s
1344/1578 [========================>.....] - ETA: 9s 
1408/1578 [=========================>....] - ETA: 6s
1472/1578 [==========================>...] - ETA: 4s
1536/1578 [============================>.] - ETA: 1s
1578/1578 [==============================] - 64s 40ms/step
loss: 0.6706765581899723
acc: 0.5944233204324285
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fdeb4280d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fdeb4280d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fdeb4201350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fdeb4201350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe114164590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe114164590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe054211550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe054211550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe05432ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe05432ded0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0940b68d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0940b68d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe054211690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe054211690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7458a93d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe7458a93d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74ebf7250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74ebf7250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0747118d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe0747118d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0741ac550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0741ac550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0543cf050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0543cf050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74ec36b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74ec36b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74507ac10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74507ac10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74ec64b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe74ec64b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdeb405d890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdeb405d890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde947e6050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde947e6050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde94694150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde94694150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74ec36d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe74ec36d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde9438a990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde9438a990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde944e2750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde944e2750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde947daed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde947daed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74ec64450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe74ec64450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde9449e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde9449e550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde94196dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde94196dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde941af690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde941af690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde942a4350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde942a4350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde94382710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde94382710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde94097ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde94097ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde74534a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde74534a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde940b9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde940b9e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde94088bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde94088bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde745301d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde745301d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde7441f990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde7441f990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde741d1090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde741d1090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde7427b2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde7427b2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde7442a0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde7442a0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde7421b910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde7421b910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde742327d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde742327d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde54713110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde54713110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde545aee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde545aee10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde742328d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde742328d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde5470edd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde5470edd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde544b8dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde544b8dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde547909d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde547909d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde544cf450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde544cf450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde544b8310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde544b8310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde541f8110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde541f8110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde5414f650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde5414f650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde5438bb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde5438bb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde54241f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde54241f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde5414f7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde5414f7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde347323d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde347323d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd54696450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd54696450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdd546966d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdd546966d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd5463d6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd5463d6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd54696f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd54696f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd5454a850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd5454a850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd543a9ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd543a9ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdd542c2190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdd542c2190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd54169c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd54169c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd5444ac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd5444ac90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd543995d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd543995d0>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 43:09 - loss: 0.8562 - acc: 0.4219
 128/5677 [..............................] - ETA: 27:32 - loss: 0.8294 - acc: 0.4531
 192/5677 [>.............................] - ETA: 21:59 - loss: 0.7887 - acc: 0.4792
 256/5677 [>.............................] - ETA: 19:32 - loss: 0.8007 - acc: 0.4570
 320/5677 [>.............................] - ETA: 17:44 - loss: 0.7778 - acc: 0.4719
 384/5677 [=>............................] - ETA: 16:37 - loss: 0.7767 - acc: 0.4766
 448/5677 [=>............................] - ETA: 15:38 - loss: 0.7610 - acc: 0.4888
 512/5677 [=>............................] - ETA: 14:55 - loss: 0.7555 - acc: 0.4980
 576/5677 [==>...........................] - ETA: 14:19 - loss: 0.7472 - acc: 0.5052
 640/5677 [==>...........................] - ETA: 13:52 - loss: 0.7416 - acc: 0.5094
 704/5677 [==>...........................] - ETA: 13:24 - loss: 0.7376 - acc: 0.5085
 768/5677 [===>..........................] - ETA: 12:59 - loss: 0.7392 - acc: 0.5013
 832/5677 [===>..........................] - ETA: 12:40 - loss: 0.7409 - acc: 0.5000
 896/5677 [===>..........................] - ETA: 12:25 - loss: 0.7398 - acc: 0.5011
 960/5677 [====>.........................] - ETA: 12:05 - loss: 0.7411 - acc: 0.5021
1024/5677 [====>.........................] - ETA: 11:48 - loss: 0.7391 - acc: 0.5059
1088/5677 [====>.........................] - ETA: 11:34 - loss: 0.7424 - acc: 0.5009
1152/5677 [=====>........................] - ETA: 11:21 - loss: 0.7461 - acc: 0.4965
1216/5677 [=====>........................] - ETA: 11:05 - loss: 0.7490 - acc: 0.4918
1280/5677 [=====>........................] - ETA: 10:53 - loss: 0.7484 - acc: 0.4914
1344/5677 [======>.......................] - ETA: 10:41 - loss: 0.7479 - acc: 0.4911
1408/5677 [======>.......................] - ETA: 10:31 - loss: 0.7486 - acc: 0.4915
1472/5677 [======>.......................] - ETA: 10:17 - loss: 0.7498 - acc: 0.4878
1536/5677 [=======>......................] - ETA: 10:04 - loss: 0.7490 - acc: 0.4889
1600/5677 [=======>......................] - ETA: 9:53 - loss: 0.7465 - acc: 0.4938 
1664/5677 [=======>......................] - ETA: 9:40 - loss: 0.7483 - acc: 0.4880
1728/5677 [========>.....................] - ETA: 9:30 - loss: 0.7459 - acc: 0.4919
1792/5677 [========>.....................] - ETA: 9:19 - loss: 0.7441 - acc: 0.4961
1856/5677 [========>.....................] - ETA: 9:08 - loss: 0.7431 - acc: 0.4968
1920/5677 [=========>....................] - ETA: 8:58 - loss: 0.7417 - acc: 0.4984
1984/5677 [=========>....................] - ETA: 8:47 - loss: 0.7409 - acc: 0.5005
2048/5677 [=========>....................] - ETA: 8:36 - loss: 0.7404 - acc: 0.5005
2112/5677 [==========>...................] - ETA: 8:25 - loss: 0.7404 - acc: 0.4995
2176/5677 [==========>...................] - ETA: 8:14 - loss: 0.7390 - acc: 0.5000
2240/5677 [==========>...................] - ETA: 8:04 - loss: 0.7377 - acc: 0.5004
2304/5677 [===========>..................] - ETA: 7:54 - loss: 0.7357 - acc: 0.5009
2368/5677 [===========>..................] - ETA: 7:44 - loss: 0.7344 - acc: 0.5030
2432/5677 [===========>..................] - ETA: 7:34 - loss: 0.7333 - acc: 0.5033
2496/5677 [============>.................] - ETA: 7:25 - loss: 0.7324 - acc: 0.5044
2560/5677 [============>.................] - ETA: 7:15 - loss: 0.7312 - acc: 0.5055
2624/5677 [============>.................] - ETA: 7:05 - loss: 0.7311 - acc: 0.5027
2688/5677 [=============>................] - ETA: 6:54 - loss: 0.7305 - acc: 0.5030
2752/5677 [=============>................] - ETA: 6:44 - loss: 0.7300 - acc: 0.5033
2816/5677 [=============>................] - ETA: 6:35 - loss: 0.7302 - acc: 0.5028
2880/5677 [==============>...............] - ETA: 6:25 - loss: 0.7299 - acc: 0.5024
2944/5677 [==============>...............] - ETA: 6:15 - loss: 0.7298 - acc: 0.5017
3008/5677 [==============>...............] - ETA: 6:06 - loss: 0.7298 - acc: 0.4993
3072/5677 [===============>..............] - ETA: 5:58 - loss: 0.7290 - acc: 0.4990
3136/5677 [===============>..............] - ETA: 5:48 - loss: 0.7291 - acc: 0.4984
3200/5677 [===============>..............] - ETA: 5:40 - loss: 0.7286 - acc: 0.4981
3264/5677 [================>.............] - ETA: 5:31 - loss: 0.7275 - acc: 0.4991
3328/5677 [================>.............] - ETA: 5:23 - loss: 0.7270 - acc: 0.5006
3392/5677 [================>.............] - ETA: 5:15 - loss: 0.7266 - acc: 0.5009
3456/5677 [=================>............] - ETA: 5:06 - loss: 0.7268 - acc: 0.5000
3520/5677 [=================>............] - ETA: 4:57 - loss: 0.7268 - acc: 0.5003
3584/5677 [=================>............] - ETA: 4:48 - loss: 0.7263 - acc: 0.5014
3648/5677 [==================>...........] - ETA: 4:40 - loss: 0.7267 - acc: 0.5000
3712/5677 [==================>...........] - ETA: 4:31 - loss: 0.7259 - acc: 0.5016
3776/5677 [==================>...........] - ETA: 4:22 - loss: 0.7259 - acc: 0.5013
3840/5677 [===================>..........] - ETA: 4:13 - loss: 0.7256 - acc: 0.5010
3904/5677 [===================>..........] - ETA: 4:04 - loss: 0.7254 - acc: 0.5005
3968/5677 [===================>..........] - ETA: 3:55 - loss: 0.7253 - acc: 0.4992
4032/5677 [====================>.........] - ETA: 3:46 - loss: 0.7255 - acc: 0.4975
4096/5677 [====================>.........] - ETA: 3:37 - loss: 0.7255 - acc: 0.4973
4160/5677 [====================>.........] - ETA: 3:28 - loss: 0.7257 - acc: 0.4964
4224/5677 [=====================>........] - ETA: 3:19 - loss: 0.7257 - acc: 0.4955
4288/5677 [=====================>........] - ETA: 3:10 - loss: 0.7253 - acc: 0.4956
4352/5677 [=====================>........] - ETA: 3:01 - loss: 0.7251 - acc: 0.4954
4416/5677 [======================>.......] - ETA: 2:53 - loss: 0.7255 - acc: 0.4934
4480/5677 [======================>.......] - ETA: 2:43 - loss: 0.7250 - acc: 0.4946
4544/5677 [=======================>......] - ETA: 2:34 - loss: 0.7242 - acc: 0.4965
4608/5677 [=======================>......] - ETA: 2:25 - loss: 0.7236 - acc: 0.4972
4672/5677 [=======================>......] - ETA: 2:17 - loss: 0.7236 - acc: 0.4970
4736/5677 [========================>.....] - ETA: 2:08 - loss: 0.7241 - acc: 0.4954
4800/5677 [========================>.....] - ETA: 1:59 - loss: 0.7239 - acc: 0.4952
4864/5677 [========================>.....] - ETA: 1:50 - loss: 0.7237 - acc: 0.4953
4928/5677 [=========================>....] - ETA: 1:41 - loss: 0.7235 - acc: 0.4949
4992/5677 [=========================>....] - ETA: 1:32 - loss: 0.7229 - acc: 0.4960
5056/5677 [=========================>....] - ETA: 1:24 - loss: 0.7222 - acc: 0.4974
5120/5677 [==========================>...] - ETA: 1:15 - loss: 0.7216 - acc: 0.4982
5184/5677 [==========================>...] - ETA: 1:06 - loss: 0.7216 - acc: 0.4985
5248/5677 [==========================>...] - ETA: 58s - loss: 0.7218 - acc: 0.4994 
5312/5677 [===========================>..] - ETA: 49s - loss: 0.7209 - acc: 0.5019
5376/5677 [===========================>..] - ETA: 40s - loss: 0.7207 - acc: 0.5022
5440/5677 [===========================>..] - ETA: 32s - loss: 0.7210 - acc: 0.5018
5504/5677 [============================>.] - ETA: 23s - loss: 0.7207 - acc: 0.5016
5568/5677 [============================>.] - ETA: 14s - loss: 0.7201 - acc: 0.5027
5632/5677 [============================>.] - ETA: 6s - loss: 0.7198 - acc: 0.5034 
5677/5677 [==============================] - 809s 143ms/step - loss: 0.7200 - acc: 0.5027 - val_loss: 0.6814 - val_acc: 0.5626

Epoch 00001: val_acc improved from -inf to 0.56260, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window18/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 14:02 - loss: 0.7413 - acc: 0.4844
 128/5677 [..............................] - ETA: 12:50 - loss: 0.7256 - acc: 0.4844
 192/5677 [>.............................] - ETA: 12:49 - loss: 0.7254 - acc: 0.4844
 256/5677 [>.............................] - ETA: 12:01 - loss: 0.7191 - acc: 0.4805
 320/5677 [>.............................] - ETA: 11:45 - loss: 0.7098 - acc: 0.5031
 384/5677 [=>............................] - ETA: 11:44 - loss: 0.7066 - acc: 0.5026
 448/5677 [=>............................] - ETA: 11:31 - loss: 0.7069 - acc: 0.5022
 512/5677 [=>............................] - ETA: 11:38 - loss: 0.7075 - acc: 0.4980
 576/5677 [==>...........................] - ETA: 11:34 - loss: 0.7047 - acc: 0.5035
 640/5677 [==>...........................] - ETA: 11:30 - loss: 0.7041 - acc: 0.5000
 704/5677 [==>...........................] - ETA: 11:15 - loss: 0.7038 - acc: 0.5014
 768/5677 [===>..........................] - ETA: 11:06 - loss: 0.7050 - acc: 0.4987
 832/5677 [===>..........................] - ETA: 10:53 - loss: 0.7008 - acc: 0.5108
 896/5677 [===>..........................] - ETA: 10:44 - loss: 0.7030 - acc: 0.5078
 960/5677 [====>.........................] - ETA: 10:36 - loss: 0.7028 - acc: 0.5094
1024/5677 [====>.........................] - ETA: 10:27 - loss: 0.7029 - acc: 0.5078
1088/5677 [====>.........................] - ETA: 10:18 - loss: 0.7018 - acc: 0.5110
1152/5677 [=====>........................] - ETA: 10:09 - loss: 0.7022 - acc: 0.5095
1216/5677 [=====>........................] - ETA: 10:00 - loss: 0.7026 - acc: 0.5099
1280/5677 [=====>........................] - ETA: 9:50 - loss: 0.7017 - acc: 0.5109 
1344/5677 [======>.......................] - ETA: 9:40 - loss: 0.7018 - acc: 0.5097
1408/5677 [======>.......................] - ETA: 9:30 - loss: 0.7015 - acc: 0.5107
1472/5677 [======>.......................] - ETA: 9:20 - loss: 0.7020 - acc: 0.5082
1536/5677 [=======>......................] - ETA: 9:10 - loss: 0.7018 - acc: 0.5085
1600/5677 [=======>......................] - ETA: 9:00 - loss: 0.7011 - acc: 0.5119
1664/5677 [=======>......................] - ETA: 8:52 - loss: 0.7004 - acc: 0.5138
1728/5677 [========>.....................] - ETA: 8:42 - loss: 0.6988 - acc: 0.5174
1792/5677 [========>.....................] - ETA: 8:34 - loss: 0.6986 - acc: 0.5195
1856/5677 [========>.....................] - ETA: 8:24 - loss: 0.6993 - acc: 0.5183
1920/5677 [=========>....................] - ETA: 8:14 - loss: 0.6979 - acc: 0.5219
1984/5677 [=========>....................] - ETA: 8:05 - loss: 0.6985 - acc: 0.5207
2048/5677 [=========>....................] - ETA: 7:56 - loss: 0.6985 - acc: 0.5229
2112/5677 [==========>...................] - ETA: 7:46 - loss: 0.6987 - acc: 0.5227
2176/5677 [==========>...................] - ETA: 7:36 - loss: 0.6988 - acc: 0.5225
2240/5677 [==========>...................] - ETA: 7:27 - loss: 0.6988 - acc: 0.5228
2304/5677 [===========>..................] - ETA: 7:18 - loss: 0.6995 - acc: 0.5213
2368/5677 [===========>..................] - ETA: 7:10 - loss: 0.6990 - acc: 0.5211
2432/5677 [===========>..................] - ETA: 7:01 - loss: 0.6990 - acc: 0.5226
2496/5677 [============>.................] - ETA: 6:53 - loss: 0.6981 - acc: 0.5252
2560/5677 [============>.................] - ETA: 6:44 - loss: 0.6981 - acc: 0.5258
2624/5677 [============>.................] - ETA: 6:35 - loss: 0.6974 - acc: 0.5274
2688/5677 [=============>................] - ETA: 6:27 - loss: 0.6975 - acc: 0.5283
2752/5677 [=============>................] - ETA: 6:18 - loss: 0.6974 - acc: 0.5298
2816/5677 [=============>................] - ETA: 6:09 - loss: 0.6976 - acc: 0.5298
2880/5677 [==============>...............] - ETA: 6:01 - loss: 0.6970 - acc: 0.5323
2944/5677 [==============>...............] - ETA: 5:52 - loss: 0.6968 - acc: 0.5333
3008/5677 [==============>...............] - ETA: 5:44 - loss: 0.6972 - acc: 0.5319
3072/5677 [===============>..............] - ETA: 5:35 - loss: 0.6979 - acc: 0.5309
3136/5677 [===============>..............] - ETA: 5:27 - loss: 0.6981 - acc: 0.5316
3200/5677 [===============>..............] - ETA: 5:18 - loss: 0.6973 - acc: 0.5328
3264/5677 [================>.............] - ETA: 5:10 - loss: 0.6965 - acc: 0.5331
3328/5677 [================>.............] - ETA: 5:01 - loss: 0.6971 - acc: 0.5312
3392/5677 [================>.............] - ETA: 4:52 - loss: 0.6966 - acc: 0.5333
3456/5677 [=================>............] - ETA: 4:44 - loss: 0.6962 - acc: 0.5330
3520/5677 [=================>............] - ETA: 4:36 - loss: 0.6963 - acc: 0.5332
3584/5677 [=================>............] - ETA: 4:28 - loss: 0.6966 - acc: 0.5326
3648/5677 [==================>...........] - ETA: 4:20 - loss: 0.6964 - acc: 0.5340
3712/5677 [==================>...........] - ETA: 4:11 - loss: 0.6968 - acc: 0.5329
3776/5677 [==================>...........] - ETA: 4:03 - loss: 0.6967 - acc: 0.5334
3840/5677 [===================>..........] - ETA: 3:55 - loss: 0.6969 - acc: 0.5328
3904/5677 [===================>..........] - ETA: 3:47 - loss: 0.6967 - acc: 0.5346
3968/5677 [===================>..........] - ETA: 3:39 - loss: 0.6969 - acc: 0.5343
4032/5677 [====================>.........] - ETA: 3:31 - loss: 0.6962 - acc: 0.5365
4096/5677 [====================>.........] - ETA: 3:23 - loss: 0.6964 - acc: 0.5366
4160/5677 [====================>.........] - ETA: 3:15 - loss: 0.6958 - acc: 0.5373
4224/5677 [=====================>........] - ETA: 3:06 - loss: 0.6954 - acc: 0.5386
4288/5677 [=====================>........] - ETA: 2:58 - loss: 0.6954 - acc: 0.5382
4352/5677 [=====================>........] - ETA: 2:50 - loss: 0.6953 - acc: 0.5386
4416/5677 [======================>.......] - ETA: 2:41 - loss: 0.6957 - acc: 0.5369
4480/5677 [======================>.......] - ETA: 2:33 - loss: 0.6955 - acc: 0.5375
4544/5677 [=======================>......] - ETA: 2:25 - loss: 0.6955 - acc: 0.5372
4608/5677 [=======================>......] - ETA: 2:16 - loss: 0.6953 - acc: 0.5382
4672/5677 [=======================>......] - ETA: 2:08 - loss: 0.6956 - acc: 0.5366
4736/5677 [========================>.....] - ETA: 2:00 - loss: 0.6956 - acc: 0.5363
4800/5677 [========================>.....] - ETA: 1:52 - loss: 0.6954 - acc: 0.5371
4864/5677 [========================>.....] - ETA: 1:44 - loss: 0.6956 - acc: 0.5362
4928/5677 [=========================>....] - ETA: 1:35 - loss: 0.6958 - acc: 0.5367
4992/5677 [=========================>....] - ETA: 1:27 - loss: 0.6961 - acc: 0.5367
5056/5677 [=========================>....] - ETA: 1:19 - loss: 0.6964 - acc: 0.5354
5120/5677 [==========================>...] - ETA: 1:11 - loss: 0.6968 - acc: 0.5340
5184/5677 [==========================>...] - ETA: 1:03 - loss: 0.6963 - acc: 0.5349
5248/5677 [==========================>...] - ETA: 54s - loss: 0.6960 - acc: 0.5356 
5312/5677 [===========================>..] - ETA: 46s - loss: 0.6958 - acc: 0.5369
5376/5677 [===========================>..] - ETA: 38s - loss: 0.6955 - acc: 0.5374
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6951 - acc: 0.5377
5504/5677 [============================>.] - ETA: 22s - loss: 0.6948 - acc: 0.5385
5568/5677 [============================>.] - ETA: 13s - loss: 0.6950 - acc: 0.5379
5632/5677 [============================>.] - ETA: 5s - loss: 0.6946 - acc: 0.5380 
5677/5677 [==============================] - 752s 132ms/step - loss: 0.6948 - acc: 0.5373 - val_loss: 0.6788 - val_acc: 0.5721

Epoch 00002: val_acc improved from 0.56260 to 0.57211, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window18/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 10:46 - loss: 0.6957 - acc: 0.5625
 128/5677 [..............................] - ETA: 12:39 - loss: 0.6862 - acc: 0.5859
 192/5677 [>.............................] - ETA: 12:14 - loss: 0.6724 - acc: 0.6094
 256/5677 [>.............................] - ETA: 11:49 - loss: 0.6658 - acc: 0.6289
 320/5677 [>.............................] - ETA: 11:52 - loss: 0.6825 - acc: 0.5906
 384/5677 [=>............................] - ETA: 11:32 - loss: 0.6897 - acc: 0.5677
 448/5677 [=>............................] - ETA: 11:17 - loss: 0.6898 - acc: 0.5603
 512/5677 [=>............................] - ETA: 11:03 - loss: 0.6879 - acc: 0.5625
 576/5677 [==>...........................] - ETA: 11:01 - loss: 0.6837 - acc: 0.5625
 640/5677 [==>...........................] - ETA: 11:09 - loss: 0.6867 - acc: 0.5531
 704/5677 [==>...........................] - ETA: 10:59 - loss: 0.6871 - acc: 0.5540
 768/5677 [===>..........................] - ETA: 10:44 - loss: 0.6882 - acc: 0.5469
 832/5677 [===>..........................] - ETA: 10:30 - loss: 0.6892 - acc: 0.5469
 896/5677 [===>..........................] - ETA: 10:16 - loss: 0.6904 - acc: 0.5458
 960/5677 [====>.........................] - ETA: 10:07 - loss: 0.6908 - acc: 0.5417
1024/5677 [====>.........................] - ETA: 10:05 - loss: 0.6908 - acc: 0.5430
1088/5677 [====>.........................] - ETA: 9:59 - loss: 0.6902 - acc: 0.5460 
1152/5677 [=====>........................] - ETA: 9:52 - loss: 0.6906 - acc: 0.5425
1216/5677 [=====>........................] - ETA: 9:43 - loss: 0.6912 - acc: 0.5411
1280/5677 [=====>........................] - ETA: 9:33 - loss: 0.6909 - acc: 0.5445
1344/5677 [======>.......................] - ETA: 9:24 - loss: 0.6904 - acc: 0.5484
1408/5677 [======>.......................] - ETA: 9:16 - loss: 0.6907 - acc: 0.5476
1472/5677 [======>.......................] - ETA: 9:05 - loss: 0.6937 - acc: 0.5414
1536/5677 [=======>......................] - ETA: 8:59 - loss: 0.6948 - acc: 0.5371
1600/5677 [=======>......................] - ETA: 8:53 - loss: 0.6975 - acc: 0.5331
1664/5677 [=======>......................] - ETA: 8:44 - loss: 0.6970 - acc: 0.5343
1728/5677 [========>.....................] - ETA: 8:39 - loss: 0.6962 - acc: 0.5365
1792/5677 [========>.....................] - ETA: 8:31 - loss: 0.6964 - acc: 0.5374
1856/5677 [========>.....................] - ETA: 8:23 - loss: 0.6958 - acc: 0.5366
1920/5677 [=========>....................] - ETA: 8:14 - loss: 0.6961 - acc: 0.5365
1984/5677 [=========>....................] - ETA: 8:05 - loss: 0.6953 - acc: 0.5373
2048/5677 [=========>....................] - ETA: 7:57 - loss: 0.6964 - acc: 0.5356
2112/5677 [==========>...................] - ETA: 7:49 - loss: 0.6970 - acc: 0.5365
2176/5677 [==========>...................] - ETA: 7:43 - loss: 0.6972 - acc: 0.5354
2240/5677 [==========>...................] - ETA: 7:33 - loss: 0.6966 - acc: 0.5366
2304/5677 [===========>..................] - ETA: 7:25 - loss: 0.6969 - acc: 0.5352
2368/5677 [===========>..................] - ETA: 7:16 - loss: 0.6967 - acc: 0.5355
2432/5677 [===========>..................] - ETA: 7:07 - loss: 0.6974 - acc: 0.5345
2496/5677 [============>.................] - ETA: 6:58 - loss: 0.6969 - acc: 0.5353
2560/5677 [============>.................] - ETA: 6:49 - loss: 0.6964 - acc: 0.5359
2624/5677 [============>.................] - ETA: 6:40 - loss: 0.6968 - acc: 0.5351
2688/5677 [=============>................] - ETA: 6:32 - loss: 0.6972 - acc: 0.5327
2752/5677 [=============>................] - ETA: 6:24 - loss: 0.6978 - acc: 0.5309
2816/5677 [=============>................] - ETA: 6:14 - loss: 0.6979 - acc: 0.5302
2880/5677 [==============>...............] - ETA: 6:06 - loss: 0.6976 - acc: 0.5299
2944/5677 [==============>...............] - ETA: 5:58 - loss: 0.6969 - acc: 0.5302
3008/5677 [==============>...............] - ETA: 5:49 - loss: 0.6970 - acc: 0.5286
3072/5677 [===============>..............] - ETA: 5:41 - loss: 0.6970 - acc: 0.5293
3136/5677 [===============>..............] - ETA: 5:33 - loss: 0.6972 - acc: 0.5284
3200/5677 [===============>..............] - ETA: 5:24 - loss: 0.6973 - acc: 0.5281
3264/5677 [================>.............] - ETA: 5:16 - loss: 0.6974 - acc: 0.5279
3328/5677 [================>.............] - ETA: 5:07 - loss: 0.6980 - acc: 0.5273
3392/5677 [================>.............] - ETA: 4:59 - loss: 0.6978 - acc: 0.5268
3456/5677 [=================>............] - ETA: 4:50 - loss: 0.6978 - acc: 0.5263
3520/5677 [=================>............] - ETA: 4:41 - loss: 0.6972 - acc: 0.5276
3584/5677 [=================>............] - ETA: 4:33 - loss: 0.6966 - acc: 0.5285
3648/5677 [==================>...........] - ETA: 4:24 - loss: 0.6961 - acc: 0.5285
3712/5677 [==================>...........] - ETA: 4:16 - loss: 0.6959 - acc: 0.5286
3776/5677 [==================>...........] - ETA: 4:07 - loss: 0.6959 - acc: 0.5286
3840/5677 [===================>..........] - ETA: 3:59 - loss: 0.6958 - acc: 0.5284
3904/5677 [===================>..........] - ETA: 3:50 - loss: 0.6956 - acc: 0.5292
3968/5677 [===================>..........] - ETA: 3:42 - loss: 0.6960 - acc: 0.5275
4032/5677 [====================>.........] - ETA: 3:34 - loss: 0.6957 - acc: 0.5275
4096/5677 [====================>.........] - ETA: 3:25 - loss: 0.6956 - acc: 0.5286
4160/5677 [====================>.........] - ETA: 3:16 - loss: 0.6959 - acc: 0.5272
4224/5677 [=====================>........] - ETA: 3:08 - loss: 0.6953 - acc: 0.5298
4288/5677 [=====================>........] - ETA: 2:59 - loss: 0.6948 - acc: 0.5317
4352/5677 [=====================>........] - ETA: 2:51 - loss: 0.6951 - acc: 0.5299
4416/5677 [======================>.......] - ETA: 2:43 - loss: 0.6952 - acc: 0.5292
4480/5677 [======================>.......] - ETA: 2:35 - loss: 0.6952 - acc: 0.5290
4544/5677 [=======================>......] - ETA: 2:26 - loss: 0.6952 - acc: 0.5288
4608/5677 [=======================>......] - ETA: 2:18 - loss: 0.6952 - acc: 0.5291
4672/5677 [=======================>......] - ETA: 2:10 - loss: 0.6953 - acc: 0.5283
4736/5677 [========================>.....] - ETA: 2:01 - loss: 0.6956 - acc: 0.5279
4800/5677 [========================>.....] - ETA: 1:53 - loss: 0.6954 - acc: 0.5285
4864/5677 [========================>.....] - ETA: 1:45 - loss: 0.6954 - acc: 0.5282
4928/5677 [=========================>....] - ETA: 1:37 - loss: 0.6956 - acc: 0.5284
4992/5677 [=========================>....] - ETA: 1:28 - loss: 0.6952 - acc: 0.5294
5056/5677 [=========================>....] - ETA: 1:20 - loss: 0.6950 - acc: 0.5299
5120/5677 [==========================>...] - ETA: 1:11 - loss: 0.6949 - acc: 0.5299
5184/5677 [==========================>...] - ETA: 1:03 - loss: 0.6946 - acc: 0.5301
5248/5677 [==========================>...] - ETA: 55s - loss: 0.6941 - acc: 0.5322 
5312/5677 [===========================>..] - ETA: 47s - loss: 0.6947 - acc: 0.5309
5376/5677 [===========================>..] - ETA: 38s - loss: 0.6940 - acc: 0.5329
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6936 - acc: 0.5335
5504/5677 [============================>.] - ETA: 22s - loss: 0.6935 - acc: 0.5336
5568/5677 [============================>.] - ETA: 14s - loss: 0.6937 - acc: 0.5330
5632/5677 [============================>.] - ETA: 5s - loss: 0.6935 - acc: 0.5336 
5677/5677 [==============================] - 755s 133ms/step - loss: 0.6931 - acc: 0.5344 - val_loss: 0.6769 - val_acc: 0.5705

Epoch 00003: val_acc did not improve from 0.57211
Epoch 4/10

  64/5677 [..............................] - ETA: 11:00 - loss: 0.6499 - acc: 0.6406
 128/5677 [..............................] - ETA: 11:06 - loss: 0.6630 - acc: 0.6172
 192/5677 [>.............................] - ETA: 11:12 - loss: 0.6784 - acc: 0.5677
 256/5677 [>.............................] - ETA: 11:00 - loss: 0.6748 - acc: 0.5625
 320/5677 [>.............................] - ETA: 10:52 - loss: 0.6801 - acc: 0.5563
 384/5677 [=>............................] - ETA: 10:46 - loss: 0.6753 - acc: 0.5651
 448/5677 [=>............................] - ETA: 10:36 - loss: 0.6770 - acc: 0.5692
 512/5677 [=>............................] - ETA: 10:28 - loss: 0.6797 - acc: 0.5605
 576/5677 [==>...........................] - ETA: 10:17 - loss: 0.6830 - acc: 0.5608
 640/5677 [==>...........................] - ETA: 10:06 - loss: 0.6857 - acc: 0.5500
 704/5677 [==>...........................] - ETA: 10:01 - loss: 0.6852 - acc: 0.5511
 768/5677 [===>..........................] - ETA: 9:55 - loss: 0.6831 - acc: 0.5508 
 832/5677 [===>..........................] - ETA: 9:47 - loss: 0.6833 - acc: 0.5457
 896/5677 [===>..........................] - ETA: 9:36 - loss: 0.6815 - acc: 0.5469
 960/5677 [====>.........................] - ETA: 9:26 - loss: 0.6813 - acc: 0.5469
1024/5677 [====>.........................] - ETA: 9:20 - loss: 0.6810 - acc: 0.5439
1088/5677 [====>.........................] - ETA: 9:08 - loss: 0.6796 - acc: 0.5469
1152/5677 [=====>........................] - ETA: 9:00 - loss: 0.6771 - acc: 0.5556
1216/5677 [=====>........................] - ETA: 8:55 - loss: 0.6773 - acc: 0.5584
1280/5677 [=====>........................] - ETA: 8:46 - loss: 0.6773 - acc: 0.5586
1344/5677 [======>.......................] - ETA: 8:39 - loss: 0.6792 - acc: 0.5580
1408/5677 [======>.......................] - ETA: 8:32 - loss: 0.6778 - acc: 0.5604
1472/5677 [======>.......................] - ETA: 8:26 - loss: 0.6800 - acc: 0.5584
1536/5677 [=======>......................] - ETA: 8:19 - loss: 0.6806 - acc: 0.5599
1600/5677 [=======>......................] - ETA: 8:11 - loss: 0.6801 - acc: 0.5606
1664/5677 [=======>......................] - ETA: 8:02 - loss: 0.6813 - acc: 0.5571
1728/5677 [========>.....................] - ETA: 7:54 - loss: 0.6810 - acc: 0.5579
1792/5677 [========>.....................] - ETA: 7:46 - loss: 0.6800 - acc: 0.5586
1856/5677 [========>.....................] - ETA: 7:40 - loss: 0.6812 - acc: 0.5560
1920/5677 [=========>....................] - ETA: 7:32 - loss: 0.6804 - acc: 0.5573
1984/5677 [=========>....................] - ETA: 7:25 - loss: 0.6813 - acc: 0.5565
2048/5677 [=========>....................] - ETA: 7:16 - loss: 0.6820 - acc: 0.5552
2112/5677 [==========>...................] - ETA: 7:10 - loss: 0.6834 - acc: 0.5507
2176/5677 [==========>...................] - ETA: 7:03 - loss: 0.6835 - acc: 0.5510
2240/5677 [==========>...................] - ETA: 6:55 - loss: 0.6833 - acc: 0.5527
2304/5677 [===========>..................] - ETA: 6:47 - loss: 0.6832 - acc: 0.5538
2368/5677 [===========>..................] - ETA: 6:39 - loss: 0.6838 - acc: 0.5536
2432/5677 [===========>..................] - ETA: 6:30 - loss: 0.6840 - acc: 0.5518
2496/5677 [============>.................] - ETA: 6:22 - loss: 0.6837 - acc: 0.5541
2560/5677 [============>.................] - ETA: 6:15 - loss: 0.6840 - acc: 0.5527
2624/5677 [============>.................] - ETA: 6:07 - loss: 0.6845 - acc: 0.5530
2688/5677 [=============>................] - ETA: 6:00 - loss: 0.6851 - acc: 0.5528
2752/5677 [=============>................] - ETA: 5:51 - loss: 0.6842 - acc: 0.5552
2816/5677 [=============>................] - ETA: 5:43 - loss: 0.6835 - acc: 0.5568
2880/5677 [==============>...............] - ETA: 5:35 - loss: 0.6834 - acc: 0.5573
2944/5677 [==============>...............] - ETA: 5:27 - loss: 0.6827 - acc: 0.5581
3008/5677 [==============>...............] - ETA: 5:19 - loss: 0.6827 - acc: 0.5585
3072/5677 [===============>..............] - ETA: 5:11 - loss: 0.6823 - acc: 0.5579
3136/5677 [===============>..............] - ETA: 5:03 - loss: 0.6819 - acc: 0.5596
3200/5677 [===============>..............] - ETA: 4:55 - loss: 0.6823 - acc: 0.5603
3264/5677 [================>.............] - ETA: 4:47 - loss: 0.6818 - acc: 0.5600
3328/5677 [================>.............] - ETA: 4:40 - loss: 0.6819 - acc: 0.5598
3392/5677 [================>.............] - ETA: 4:32 - loss: 0.6827 - acc: 0.5590
3456/5677 [=================>............] - ETA: 4:25 - loss: 0.6831 - acc: 0.5582
3520/5677 [=================>............] - ETA: 4:17 - loss: 0.6829 - acc: 0.5580
3584/5677 [=================>............] - ETA: 4:09 - loss: 0.6824 - acc: 0.5592
3648/5677 [==================>...........] - ETA: 4:01 - loss: 0.6819 - acc: 0.5600
3712/5677 [==================>...........] - ETA: 3:54 - loss: 0.6814 - acc: 0.5609
3776/5677 [==================>...........] - ETA: 3:47 - loss: 0.6810 - acc: 0.5630
3840/5677 [===================>..........] - ETA: 3:40 - loss: 0.6818 - acc: 0.5609
3904/5677 [===================>..........] - ETA: 3:32 - loss: 0.6817 - acc: 0.5615
3968/5677 [===================>..........] - ETA: 3:25 - loss: 0.6821 - acc: 0.5600
4032/5677 [====================>.........] - ETA: 3:17 - loss: 0.6818 - acc: 0.5600
4096/5677 [====================>.........] - ETA: 3:10 - loss: 0.6817 - acc: 0.5593
4160/5677 [====================>.........] - ETA: 3:02 - loss: 0.6822 - acc: 0.5579
4224/5677 [=====================>........] - ETA: 2:54 - loss: 0.6828 - acc: 0.5573
4288/5677 [=====================>........] - ETA: 2:47 - loss: 0.6834 - acc: 0.5571
4352/5677 [=====================>........] - ETA: 2:39 - loss: 0.6839 - acc: 0.5568
4416/5677 [======================>.......] - ETA: 2:32 - loss: 0.6829 - acc: 0.5580
4480/5677 [======================>.......] - ETA: 2:24 - loss: 0.6833 - acc: 0.5569
4544/5677 [=======================>......] - ETA: 2:17 - loss: 0.6837 - acc: 0.5561
4608/5677 [=======================>......] - ETA: 2:09 - loss: 0.6836 - acc: 0.5547
4672/5677 [=======================>......] - ETA: 2:01 - loss: 0.6833 - acc: 0.5561
4736/5677 [========================>.....] - ETA: 1:53 - loss: 0.6831 - acc: 0.5566
4800/5677 [========================>.....] - ETA: 1:46 - loss: 0.6834 - acc: 0.5558
4864/5677 [========================>.....] - ETA: 1:38 - loss: 0.6838 - acc: 0.5545
4928/5677 [=========================>....] - ETA: 1:31 - loss: 0.6836 - acc: 0.5556
4992/5677 [=========================>....] - ETA: 1:23 - loss: 0.6841 - acc: 0.5547
5056/5677 [=========================>....] - ETA: 1:15 - loss: 0.6839 - acc: 0.5558
5120/5677 [==========================>...] - ETA: 1:07 - loss: 0.6838 - acc: 0.5557
5184/5677 [==========================>...] - ETA: 1:00 - loss: 0.6835 - acc: 0.5571
5248/5677 [==========================>...] - ETA: 52s - loss: 0.6836 - acc: 0.5566 
5312/5677 [===========================>..] - ETA: 44s - loss: 0.6838 - acc: 0.5565
5376/5677 [===========================>..] - ETA: 36s - loss: 0.6837 - acc: 0.5573
5440/5677 [===========================>..] - ETA: 28s - loss: 0.6836 - acc: 0.5570
5504/5677 [============================>.] - ETA: 21s - loss: 0.6834 - acc: 0.5570
5568/5677 [============================>.] - ETA: 13s - loss: 0.6837 - acc: 0.5559
5632/5677 [============================>.] - ETA: 5s - loss: 0.6839 - acc: 0.5552 
5677/5677 [==============================] - 723s 127ms/step - loss: 0.6839 - acc: 0.5552 - val_loss: 0.6795 - val_acc: 0.5721

Epoch 00004: val_acc did not improve from 0.57211
Epoch 5/10

  64/5677 [..............................] - ETA: 11:41 - loss: 0.6740 - acc: 0.5469
 128/5677 [..............................] - ETA: 10:59 - loss: 0.6773 - acc: 0.5391
 192/5677 [>.............................] - ETA: 11:29 - loss: 0.6832 - acc: 0.5625
 256/5677 [>.............................] - ETA: 11:45 - loss: 0.6866 - acc: 0.5547
 320/5677 [>.............................] - ETA: 11:47 - loss: 0.6896 - acc: 0.5500
 384/5677 [=>............................] - ETA: 11:31 - loss: 0.6897 - acc: 0.5495
 448/5677 [=>............................] - ETA: 11:12 - loss: 0.6880 - acc: 0.5558
 512/5677 [=>............................] - ETA: 10:52 - loss: 0.6837 - acc: 0.5625
 576/5677 [==>...........................] - ETA: 10:41 - loss: 0.6852 - acc: 0.5642
 640/5677 [==>...........................] - ETA: 10:33 - loss: 0.6875 - acc: 0.5578
 704/5677 [==>...........................] - ETA: 10:34 - loss: 0.6886 - acc: 0.5625
 768/5677 [===>..........................] - ETA: 10:27 - loss: 0.6864 - acc: 0.5703
 832/5677 [===>..........................] - ETA: 10:27 - loss: 0.6864 - acc: 0.5673
 896/5677 [===>..........................] - ETA: 10:20 - loss: 0.6847 - acc: 0.5670
 960/5677 [====>.........................] - ETA: 10:09 - loss: 0.6828 - acc: 0.5677
1024/5677 [====>.........................] - ETA: 9:56 - loss: 0.6830 - acc: 0.5684 
1088/5677 [====>.........................] - ETA: 9:48 - loss: 0.6837 - acc: 0.5653
1152/5677 [=====>........................] - ETA: 9:40 - loss: 0.6808 - acc: 0.5703
1216/5677 [=====>........................] - ETA: 9:32 - loss: 0.6826 - acc: 0.5674
1280/5677 [=====>........................] - ETA: 9:25 - loss: 0.6820 - acc: 0.5680
1344/5677 [======>.......................] - ETA: 9:19 - loss: 0.6819 - acc: 0.5655
1408/5677 [======>.......................] - ETA: 9:14 - loss: 0.6801 - acc: 0.5696
1472/5677 [======>.......................] - ETA: 9:09 - loss: 0.6790 - acc: 0.5713
1536/5677 [=======>......................] - ETA: 8:57 - loss: 0.6797 - acc: 0.5716
1600/5677 [=======>......................] - ETA: 8:47 - loss: 0.6773 - acc: 0.5750
1664/5677 [=======>......................] - ETA: 8:40 - loss: 0.6790 - acc: 0.5709
1728/5677 [========>.....................] - ETA: 8:31 - loss: 0.6798 - acc: 0.5689
1792/5677 [========>.....................] - ETA: 8:25 - loss: 0.6801 - acc: 0.5698
1856/5677 [========>.....................] - ETA: 8:17 - loss: 0.6798 - acc: 0.5706
1920/5677 [=========>....................] - ETA: 8:09 - loss: 0.6794 - acc: 0.5734
1984/5677 [=========>....................] - ETA: 8:00 - loss: 0.6811 - acc: 0.5721
2048/5677 [=========>....................] - ETA: 7:52 - loss: 0.6809 - acc: 0.5713
2112/5677 [==========>...................] - ETA: 7:45 - loss: 0.6800 - acc: 0.5729
2176/5677 [==========>...................] - ETA: 7:36 - loss: 0.6803 - acc: 0.5726
2240/5677 [==========>...................] - ETA: 7:28 - loss: 0.6796 - acc: 0.5732
2304/5677 [===========>..................] - ETA: 7:20 - loss: 0.6802 - acc: 0.5707
2368/5677 [===========>..................] - ETA: 7:12 - loss: 0.6807 - acc: 0.5680
2432/5677 [===========>..................] - ETA: 7:04 - loss: 0.6805 - acc: 0.5695
2496/5677 [============>.................] - ETA: 6:55 - loss: 0.6805 - acc: 0.5713
2560/5677 [============>.................] - ETA: 6:44 - loss: 0.6815 - acc: 0.5715
2624/5677 [============>.................] - ETA: 6:35 - loss: 0.6810 - acc: 0.5713
2688/5677 [=============>................] - ETA: 6:25 - loss: 0.6803 - acc: 0.5714
2752/5677 [=============>................] - ETA: 6:16 - loss: 0.6804 - acc: 0.5705
2816/5677 [=============>................] - ETA: 6:06 - loss: 0.6797 - acc: 0.5714
2880/5677 [==============>...............] - ETA: 5:57 - loss: 0.6792 - acc: 0.5712
2944/5677 [==============>...............] - ETA: 5:48 - loss: 0.6787 - acc: 0.5717
3008/5677 [==============>...............] - ETA: 5:40 - loss: 0.6773 - acc: 0.5748
3072/5677 [===============>..............] - ETA: 5:31 - loss: 0.6763 - acc: 0.5765
3136/5677 [===============>..............] - ETA: 5:22 - loss: 0.6758 - acc: 0.5762
3200/5677 [===============>..............] - ETA: 5:13 - loss: 0.6757 - acc: 0.5772
3264/5677 [================>.............] - ETA: 5:05 - loss: 0.6756 - acc: 0.5766
3328/5677 [================>.............] - ETA: 4:56 - loss: 0.6758 - acc: 0.5763
3392/5677 [================>.............] - ETA: 4:47 - loss: 0.6755 - acc: 0.5764
3456/5677 [=================>............] - ETA: 4:39 - loss: 0.6751 - acc: 0.5781
3520/5677 [=================>............] - ETA: 4:31 - loss: 0.6761 - acc: 0.5778
3584/5677 [=================>............] - ETA: 4:23 - loss: 0.6758 - acc: 0.5781
3648/5677 [==================>...........] - ETA: 4:14 - loss: 0.6774 - acc: 0.5773
3712/5677 [==================>...........] - ETA: 4:05 - loss: 0.6775 - acc: 0.5768
3776/5677 [==================>...........] - ETA: 3:57 - loss: 0.6787 - acc: 0.5744
3840/5677 [===================>..........] - ETA: 3:50 - loss: 0.6794 - acc: 0.5742
3904/5677 [===================>..........] - ETA: 3:42 - loss: 0.6799 - acc: 0.5730
3968/5677 [===================>..........] - ETA: 3:33 - loss: 0.6795 - acc: 0.5723
4032/5677 [====================>.........] - ETA: 3:25 - loss: 0.6800 - acc: 0.5712
4096/5677 [====================>.........] - ETA: 3:17 - loss: 0.6801 - acc: 0.5706
4160/5677 [====================>.........] - ETA: 3:09 - loss: 0.6802 - acc: 0.5700
4224/5677 [=====================>........] - ETA: 3:00 - loss: 0.6801 - acc: 0.5696
4288/5677 [=====================>........] - ETA: 2:52 - loss: 0.6801 - acc: 0.5702
4352/5677 [=====================>........] - ETA: 2:43 - loss: 0.6803 - acc: 0.5696
4416/5677 [======================>.......] - ETA: 2:35 - loss: 0.6802 - acc: 0.5695
4480/5677 [======================>.......] - ETA: 2:27 - loss: 0.6801 - acc: 0.5694
4544/5677 [=======================>......] - ETA: 2:19 - loss: 0.6801 - acc: 0.5700
4608/5677 [=======================>......] - ETA: 2:10 - loss: 0.6794 - acc: 0.5710
4672/5677 [=======================>......] - ETA: 2:02 - loss: 0.6799 - acc: 0.5700
4736/5677 [========================>.....] - ETA: 1:54 - loss: 0.6805 - acc: 0.5686
4800/5677 [========================>.....] - ETA: 1:46 - loss: 0.6806 - acc: 0.5675
4864/5677 [========================>.....] - ETA: 1:38 - loss: 0.6809 - acc: 0.5660
4928/5677 [=========================>....] - ETA: 1:31 - loss: 0.6807 - acc: 0.5662
4992/5677 [=========================>....] - ETA: 1:23 - loss: 0.6812 - acc: 0.5653
5056/5677 [=========================>....] - ETA: 1:15 - loss: 0.6811 - acc: 0.5647
5120/5677 [==========================>...] - ETA: 1:07 - loss: 0.6811 - acc: 0.5645
5184/5677 [==========================>...] - ETA: 59s - loss: 0.6812 - acc: 0.5639 
5248/5677 [==========================>...] - ETA: 51s - loss: 0.6815 - acc: 0.5633
5312/5677 [===========================>..] - ETA: 43s - loss: 0.6817 - acc: 0.5631
5376/5677 [===========================>..] - ETA: 36s - loss: 0.6817 - acc: 0.5636
5440/5677 [===========================>..] - ETA: 28s - loss: 0.6813 - acc: 0.5645
5504/5677 [============================>.] - ETA: 20s - loss: 0.6817 - acc: 0.5641
5568/5677 [============================>.] - ETA: 13s - loss: 0.6812 - acc: 0.5647
5632/5677 [============================>.] - ETA: 5s - loss: 0.6811 - acc: 0.5645 
5677/5677 [==============================] - 706s 124ms/step - loss: 0.6810 - acc: 0.5653 - val_loss: 0.6737 - val_acc: 0.5959

Epoch 00005: val_acc improved from 0.57211 to 0.59588, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window18/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 11:18 - loss: 0.6639 - acc: 0.5625
 128/5677 [..............................] - ETA: 11:04 - loss: 0.6792 - acc: 0.5391
 192/5677 [>.............................] - ETA: 10:44 - loss: 0.6881 - acc: 0.5417
 256/5677 [>.............................] - ETA: 10:26 - loss: 0.6859 - acc: 0.5391
 320/5677 [>.............................] - ETA: 10:22 - loss: 0.6850 - acc: 0.5406
 384/5677 [=>............................] - ETA: 10:08 - loss: 0.6867 - acc: 0.5391
 448/5677 [=>............................] - ETA: 10:05 - loss: 0.6797 - acc: 0.5580
 512/5677 [=>............................] - ETA: 9:54 - loss: 0.6845 - acc: 0.5527 
 576/5677 [==>...........................] - ETA: 9:41 - loss: 0.6837 - acc: 0.5573
 640/5677 [==>...........................] - ETA: 9:38 - loss: 0.6853 - acc: 0.5547
 704/5677 [==>...........................] - ETA: 9:29 - loss: 0.6839 - acc: 0.5568
 768/5677 [===>..........................] - ETA: 9:17 - loss: 0.6848 - acc: 0.5573
 832/5677 [===>..........................] - ETA: 9:06 - loss: 0.6809 - acc: 0.5685
 896/5677 [===>..........................] - ETA: 8:56 - loss: 0.6808 - acc: 0.5714
 960/5677 [====>.........................] - ETA: 8:47 - loss: 0.6812 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 8:41 - loss: 0.6802 - acc: 0.5742
1088/5677 [====>.........................] - ETA: 8:34 - loss: 0.6799 - acc: 0.5735
1152/5677 [=====>........................] - ETA: 8:27 - loss: 0.6798 - acc: 0.5747
1216/5677 [=====>........................] - ETA: 8:21 - loss: 0.6797 - acc: 0.5732
1280/5677 [=====>........................] - ETA: 8:14 - loss: 0.6796 - acc: 0.5742
1344/5677 [======>.......................] - ETA: 8:05 - loss: 0.6799 - acc: 0.5737
1408/5677 [======>.......................] - ETA: 7:56 - loss: 0.6830 - acc: 0.5675
1472/5677 [======>.......................] - ETA: 7:48 - loss: 0.6837 - acc: 0.5673
1536/5677 [=======>......................] - ETA: 7:41 - loss: 0.6822 - acc: 0.5684
1600/5677 [=======>......................] - ETA: 7:33 - loss: 0.6828 - acc: 0.5681
1664/5677 [=======>......................] - ETA: 7:26 - loss: 0.6821 - acc: 0.5685
1728/5677 [========>.....................] - ETA: 7:19 - loss: 0.6820 - acc: 0.5671
1792/5677 [========>.....................] - ETA: 7:13 - loss: 0.6826 - acc: 0.5658
1856/5677 [========>.....................] - ETA: 7:06 - loss: 0.6826 - acc: 0.5636
1920/5677 [=========>....................] - ETA: 6:59 - loss: 0.6825 - acc: 0.5656
1984/5677 [=========>....................] - ETA: 6:53 - loss: 0.6820 - acc: 0.5660
2048/5677 [=========>....................] - ETA: 6:46 - loss: 0.6815 - acc: 0.5674
2112/5677 [==========>...................] - ETA: 6:40 - loss: 0.6822 - acc: 0.5658
2176/5677 [==========>...................] - ETA: 6:32 - loss: 0.6823 - acc: 0.5662
2240/5677 [==========>...................] - ETA: 6:25 - loss: 0.6846 - acc: 0.5625
2304/5677 [===========>..................] - ETA: 6:17 - loss: 0.6843 - acc: 0.5634
2368/5677 [===========>..................] - ETA: 6:09 - loss: 0.6847 - acc: 0.5617
2432/5677 [===========>..................] - ETA: 6:03 - loss: 0.6853 - acc: 0.5592
2496/5677 [============>.................] - ETA: 5:56 - loss: 0.6853 - acc: 0.5585
2560/5677 [============>.................] - ETA: 5:48 - loss: 0.6859 - acc: 0.5563
2624/5677 [============>.................] - ETA: 5:41 - loss: 0.6866 - acc: 0.5545
2688/5677 [=============>................] - ETA: 5:34 - loss: 0.6865 - acc: 0.5543
2752/5677 [=============>................] - ETA: 5:26 - loss: 0.6868 - acc: 0.5523
2816/5677 [=============>................] - ETA: 5:19 - loss: 0.6870 - acc: 0.5526
2880/5677 [==============>...............] - ETA: 5:12 - loss: 0.6872 - acc: 0.5517
2944/5677 [==============>...............] - ETA: 5:05 - loss: 0.6865 - acc: 0.5537
3008/5677 [==============>...............] - ETA: 4:57 - loss: 0.6861 - acc: 0.5529
3072/5677 [===============>..............] - ETA: 4:50 - loss: 0.6862 - acc: 0.5531
3136/5677 [===============>..............] - ETA: 4:43 - loss: 0.6864 - acc: 0.5533
3200/5677 [===============>..............] - ETA: 4:38 - loss: 0.6863 - acc: 0.5544
3264/5677 [================>.............] - ETA: 4:31 - loss: 0.6865 - acc: 0.5527
3328/5677 [================>.............] - ETA: 4:24 - loss: 0.6867 - acc: 0.5526
3392/5677 [================>.............] - ETA: 4:17 - loss: 0.6866 - acc: 0.5540
3456/5677 [=================>............] - ETA: 4:10 - loss: 0.6869 - acc: 0.5530
3520/5677 [=================>............] - ETA: 4:03 - loss: 0.6876 - acc: 0.5520
3584/5677 [=================>............] - ETA: 3:56 - loss: 0.6881 - acc: 0.5505
3648/5677 [==================>...........] - ETA: 3:48 - loss: 0.6877 - acc: 0.5513
3712/5677 [==================>...........] - ETA: 3:41 - loss: 0.6875 - acc: 0.5512
3776/5677 [==================>...........] - ETA: 3:34 - loss: 0.6869 - acc: 0.5519
3840/5677 [===================>..........] - ETA: 3:27 - loss: 0.6867 - acc: 0.5529
3904/5677 [===================>..........] - ETA: 3:20 - loss: 0.6867 - acc: 0.5533
3968/5677 [===================>..........] - ETA: 3:13 - loss: 0.6865 - acc: 0.5529
4032/5677 [====================>.........] - ETA: 3:06 - loss: 0.6859 - acc: 0.5543
4096/5677 [====================>.........] - ETA: 2:58 - loss: 0.6858 - acc: 0.5557
4160/5677 [====================>.........] - ETA: 2:51 - loss: 0.6856 - acc: 0.5558
4224/5677 [=====================>........] - ETA: 2:44 - loss: 0.6851 - acc: 0.5580
4288/5677 [=====================>........] - ETA: 2:37 - loss: 0.6847 - acc: 0.5592
4352/5677 [=====================>........] - ETA: 2:30 - loss: 0.6849 - acc: 0.5597
4416/5677 [======================>.......] - ETA: 2:23 - loss: 0.6858 - acc: 0.5584
4480/5677 [======================>.......] - ETA: 2:15 - loss: 0.6857 - acc: 0.5587
4544/5677 [=======================>......] - ETA: 2:08 - loss: 0.6855 - acc: 0.5596
4608/5677 [=======================>......] - ETA: 2:01 - loss: 0.6853 - acc: 0.5601
4672/5677 [=======================>......] - ETA: 1:54 - loss: 0.6853 - acc: 0.5593
4736/5677 [========================>.....] - ETA: 1:47 - loss: 0.6851 - acc: 0.5585
4800/5677 [========================>.....] - ETA: 1:39 - loss: 0.6848 - acc: 0.5596
4864/5677 [========================>.....] - ETA: 1:32 - loss: 0.6848 - acc: 0.5588
4928/5677 [=========================>....] - ETA: 1:25 - loss: 0.6845 - acc: 0.5593
4992/5677 [=========================>....] - ETA: 1:18 - loss: 0.6844 - acc: 0.5593
5056/5677 [=========================>....] - ETA: 1:11 - loss: 0.6846 - acc: 0.5585
5120/5677 [==========================>...] - ETA: 1:03 - loss: 0.6849 - acc: 0.5574
5184/5677 [==========================>...] - ETA: 56s - loss: 0.6852 - acc: 0.5565 
5248/5677 [==========================>...] - ETA: 49s - loss: 0.6850 - acc: 0.5575
5312/5677 [===========================>..] - ETA: 41s - loss: 0.6850 - acc: 0.5576
5376/5677 [===========================>..] - ETA: 34s - loss: 0.6848 - acc: 0.5580
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6845 - acc: 0.5583
5504/5677 [============================>.] - ETA: 19s - loss: 0.6844 - acc: 0.5592
5568/5677 [============================>.] - ETA: 12s - loss: 0.6845 - acc: 0.5587
5632/5677 [============================>.] - ETA: 5s - loss: 0.6844 - acc: 0.5584 
5677/5677 [==============================] - 680s 120ms/step - loss: 0.6841 - acc: 0.5595 - val_loss: 0.6690 - val_acc: 0.5959

Epoch 00006: val_acc improved from 0.59588 to 0.59588, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window18/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 7/10

  64/5677 [..............................] - ETA: 10:29 - loss: 0.6845 - acc: 0.5312
 128/5677 [..............................] - ETA: 9:53 - loss: 0.6798 - acc: 0.5625 
 192/5677 [>.............................] - ETA: 10:19 - loss: 0.6731 - acc: 0.5625
 256/5677 [>.............................] - ETA: 10:19 - loss: 0.6909 - acc: 0.5352
 320/5677 [>.............................] - ETA: 10:22 - loss: 0.6843 - acc: 0.5563
 384/5677 [=>............................] - ETA: 10:21 - loss: 0.6688 - acc: 0.5911
 448/5677 [=>............................] - ETA: 10:20 - loss: 0.6713 - acc: 0.5871
 512/5677 [=>............................] - ETA: 10:03 - loss: 0.6716 - acc: 0.5801
 576/5677 [==>...........................] - ETA: 10:00 - loss: 0.6805 - acc: 0.5694
 640/5677 [==>...........................] - ETA: 9:47 - loss: 0.6785 - acc: 0.5687 
 704/5677 [==>...........................] - ETA: 9:42 - loss: 0.6777 - acc: 0.5739
 768/5677 [===>..........................] - ETA: 9:32 - loss: 0.6779 - acc: 0.5716
 832/5677 [===>..........................] - ETA: 9:26 - loss: 0.6770 - acc: 0.5721
 896/5677 [===>..........................] - ETA: 9:26 - loss: 0.6763 - acc: 0.5759
 960/5677 [====>.........................] - ETA: 9:17 - loss: 0.6788 - acc: 0.5698
1024/5677 [====>.........................] - ETA: 9:14 - loss: 0.6780 - acc: 0.5693
1088/5677 [====>.........................] - ETA: 9:06 - loss: 0.6772 - acc: 0.5689
1152/5677 [=====>........................] - ETA: 8:56 - loss: 0.6759 - acc: 0.5686
1216/5677 [=====>........................] - ETA: 8:45 - loss: 0.6741 - acc: 0.5707
1280/5677 [=====>........................] - ETA: 8:38 - loss: 0.6728 - acc: 0.5750
1344/5677 [======>.......................] - ETA: 8:26 - loss: 0.6705 - acc: 0.5818
1408/5677 [======>.......................] - ETA: 8:19 - loss: 0.6703 - acc: 0.5845
1472/5677 [======>.......................] - ETA: 8:09 - loss: 0.6692 - acc: 0.5842
1536/5677 [=======>......................] - ETA: 8:06 - loss: 0.6679 - acc: 0.5846
1600/5677 [=======>......................] - ETA: 8:00 - loss: 0.6671 - acc: 0.5869
1664/5677 [=======>......................] - ETA: 7:54 - loss: 0.6692 - acc: 0.5859
1728/5677 [========>.....................] - ETA: 7:48 - loss: 0.6699 - acc: 0.5839
1792/5677 [========>.....................] - ETA: 7:41 - loss: 0.6707 - acc: 0.5837
1856/5677 [========>.....................] - ETA: 7:32 - loss: 0.6735 - acc: 0.5787
1920/5677 [=========>....................] - ETA: 7:24 - loss: 0.6746 - acc: 0.5771
1984/5677 [=========>....................] - ETA: 7:17 - loss: 0.6745 - acc: 0.5756
2048/5677 [=========>....................] - ETA: 7:10 - loss: 0.6739 - acc: 0.5767
2112/5677 [==========>...................] - ETA: 7:03 - loss: 0.6749 - acc: 0.5739
2176/5677 [==========>...................] - ETA: 6:55 - loss: 0.6746 - acc: 0.5754
2240/5677 [==========>...................] - ETA: 6:48 - loss: 0.6752 - acc: 0.5737
2304/5677 [===========>..................] - ETA: 6:39 - loss: 0.6770 - acc: 0.5720
2368/5677 [===========>..................] - ETA: 6:32 - loss: 0.6757 - acc: 0.5756
2432/5677 [===========>..................] - ETA: 6:25 - loss: 0.6764 - acc: 0.5740
2496/5677 [============>.................] - ETA: 6:18 - loss: 0.6762 - acc: 0.5741
2560/5677 [============>.................] - ETA: 6:11 - loss: 0.6758 - acc: 0.5746
2624/5677 [============>.................] - ETA: 6:04 - loss: 0.6760 - acc: 0.5739
2688/5677 [=============>................] - ETA: 5:55 - loss: 0.6761 - acc: 0.5748
2752/5677 [=============>................] - ETA: 5:48 - loss: 0.6769 - acc: 0.5730
2816/5677 [=============>................] - ETA: 5:41 - loss: 0.6762 - acc: 0.5742
2880/5677 [==============>...............] - ETA: 5:34 - loss: 0.6766 - acc: 0.5743
2944/5677 [==============>...............] - ETA: 5:27 - loss: 0.6761 - acc: 0.5754
3008/5677 [==============>...............] - ETA: 5:19 - loss: 0.6770 - acc: 0.5748
3072/5677 [===============>..............] - ETA: 5:12 - loss: 0.6772 - acc: 0.5745
3136/5677 [===============>..............] - ETA: 5:04 - loss: 0.6782 - acc: 0.5727
3200/5677 [===============>..............] - ETA: 4:57 - loss: 0.6780 - acc: 0.5741
3264/5677 [================>.............] - ETA: 4:48 - loss: 0.6776 - acc: 0.5741
3328/5677 [================>.............] - ETA: 4:41 - loss: 0.6775 - acc: 0.5730
3392/5677 [================>.............] - ETA: 4:33 - loss: 0.6774 - acc: 0.5731
3456/5677 [=================>............] - ETA: 4:24 - loss: 0.6789 - acc: 0.5703
3520/5677 [=================>............] - ETA: 4:17 - loss: 0.6791 - acc: 0.5693
3584/5677 [=================>............] - ETA: 4:09 - loss: 0.6785 - acc: 0.5709
3648/5677 [==================>...........] - ETA: 4:02 - loss: 0.6783 - acc: 0.5702
3712/5677 [==================>...........] - ETA: 3:54 - loss: 0.6784 - acc: 0.5709
3776/5677 [==================>...........] - ETA: 3:47 - loss: 0.6786 - acc: 0.5697
3840/5677 [===================>..........] - ETA: 3:39 - loss: 0.6788 - acc: 0.5701
3904/5677 [===================>..........] - ETA: 3:31 - loss: 0.6790 - acc: 0.5704
3968/5677 [===================>..........] - ETA: 3:23 - loss: 0.6789 - acc: 0.5703
4032/5677 [====================>.........] - ETA: 3:16 - loss: 0.6781 - acc: 0.5722
4096/5677 [====================>.........] - ETA: 3:09 - loss: 0.6782 - acc: 0.5718
4160/5677 [====================>.........] - ETA: 3:01 - loss: 0.6780 - acc: 0.5714
4224/5677 [=====================>........] - ETA: 2:53 - loss: 0.6785 - acc: 0.5708
4288/5677 [=====================>........] - ETA: 2:45 - loss: 0.6791 - acc: 0.5693
4352/5677 [=====================>........] - ETA: 2:38 - loss: 0.6794 - acc: 0.5701
4416/5677 [======================>.......] - ETA: 2:30 - loss: 0.6787 - acc: 0.5713
4480/5677 [======================>.......] - ETA: 2:22 - loss: 0.6790 - acc: 0.5703
4544/5677 [=======================>......] - ETA: 2:15 - loss: 0.6783 - acc: 0.5722
4608/5677 [=======================>......] - ETA: 2:07 - loss: 0.6784 - acc: 0.5716
4672/5677 [=======================>......] - ETA: 1:59 - loss: 0.6781 - acc: 0.5728
4736/5677 [========================>.....] - ETA: 1:52 - loss: 0.6784 - acc: 0.5722
4800/5677 [========================>.....] - ETA: 1:44 - loss: 0.6781 - acc: 0.5719
4864/5677 [========================>.....] - ETA: 1:36 - loss: 0.6776 - acc: 0.5730
4928/5677 [=========================>....] - ETA: 1:29 - loss: 0.6778 - acc: 0.5728
4992/5677 [=========================>....] - ETA: 1:21 - loss: 0.6777 - acc: 0.5727
5056/5677 [=========================>....] - ETA: 1:14 - loss: 0.6781 - acc: 0.5730
5120/5677 [==========================>...] - ETA: 1:06 - loss: 0.6780 - acc: 0.5729
5184/5677 [==========================>...] - ETA: 59s - loss: 0.6783 - acc: 0.5725 
5248/5677 [==========================>...] - ETA: 51s - loss: 0.6786 - acc: 0.5718
5312/5677 [===========================>..] - ETA: 43s - loss: 0.6787 - acc: 0.5715
5376/5677 [===========================>..] - ETA: 36s - loss: 0.6784 - acc: 0.5720
5440/5677 [===========================>..] - ETA: 28s - loss: 0.6786 - acc: 0.5713
5504/5677 [============================>.] - ETA: 20s - loss: 0.6780 - acc: 0.5730
5568/5677 [============================>.] - ETA: 13s - loss: 0.6783 - acc: 0.5727
5632/5677 [============================>.] - ETA: 5s - loss: 0.6790 - acc: 0.5710 
5677/5677 [==============================] - 718s 127ms/step - loss: 0.6790 - acc: 0.5709 - val_loss: 0.6699 - val_acc: 0.5895

Epoch 00007: val_acc did not improve from 0.59588
Epoch 8/10

  64/5677 [..............................] - ETA: 14:09 - loss: 0.6932 - acc: 0.5000
 128/5677 [..............................] - ETA: 13:43 - loss: 0.7154 - acc: 0.5000
 192/5677 [>.............................] - ETA: 12:58 - loss: 0.6794 - acc: 0.5781
 256/5677 [>.............................] - ETA: 12:31 - loss: 0.6664 - acc: 0.5938
 320/5677 [>.............................] - ETA: 12:12 - loss: 0.6622 - acc: 0.6094
 384/5677 [=>............................] - ETA: 11:49 - loss: 0.6762 - acc: 0.5859
 448/5677 [=>............................] - ETA: 11:40 - loss: 0.6759 - acc: 0.5804
 512/5677 [=>............................] - ETA: 11:26 - loss: 0.6819 - acc: 0.5664
 576/5677 [==>...........................] - ETA: 11:07 - loss: 0.6852 - acc: 0.5608
 640/5677 [==>...........................] - ETA: 10:59 - loss: 0.6830 - acc: 0.5687
 704/5677 [==>...........................] - ETA: 10:42 - loss: 0.6796 - acc: 0.5767
 768/5677 [===>..........................] - ETA: 10:31 - loss: 0.6798 - acc: 0.5729
 832/5677 [===>..........................] - ETA: 10:21 - loss: 0.6786 - acc: 0.5709
 896/5677 [===>..........................] - ETA: 10:11 - loss: 0.6777 - acc: 0.5714
 960/5677 [====>.........................] - ETA: 10:02 - loss: 0.6778 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 9:55 - loss: 0.6796 - acc: 0.5625 
1088/5677 [====>.........................] - ETA: 9:45 - loss: 0.6764 - acc: 0.5689
1152/5677 [=====>........................] - ETA: 9:37 - loss: 0.6764 - acc: 0.5712
1216/5677 [=====>........................] - ETA: 9:29 - loss: 0.6767 - acc: 0.5707
1280/5677 [=====>........................] - ETA: 9:21 - loss: 0.6770 - acc: 0.5680
1344/5677 [======>.......................] - ETA: 9:14 - loss: 0.6768 - acc: 0.5699
1408/5677 [======>.......................] - ETA: 9:04 - loss: 0.6766 - acc: 0.5732
1472/5677 [======>.......................] - ETA: 8:58 - loss: 0.6774 - acc: 0.5713
1536/5677 [=======>......................] - ETA: 8:47 - loss: 0.6786 - acc: 0.5716
1600/5677 [=======>......................] - ETA: 8:38 - loss: 0.6792 - acc: 0.5700
1664/5677 [=======>......................] - ETA: 8:28 - loss: 0.6799 - acc: 0.5703
1728/5677 [========>.....................] - ETA: 8:19 - loss: 0.6782 - acc: 0.5723
1792/5677 [========>.....................] - ETA: 8:09 - loss: 0.6794 - acc: 0.5703
1856/5677 [========>.....................] - ETA: 8:01 - loss: 0.6777 - acc: 0.5722
1920/5677 [=========>....................] - ETA: 7:52 - loss: 0.6781 - acc: 0.5703
1984/5677 [=========>....................] - ETA: 7:43 - loss: 0.6781 - acc: 0.5701
2048/5677 [=========>....................] - ETA: 7:35 - loss: 0.6785 - acc: 0.5688
2112/5677 [==========>...................] - ETA: 7:28 - loss: 0.6778 - acc: 0.5705
2176/5677 [==========>...................] - ETA: 7:19 - loss: 0.6802 - acc: 0.5662
2240/5677 [==========>...................] - ETA: 7:11 - loss: 0.6810 - acc: 0.5656
2304/5677 [===========>..................] - ETA: 7:03 - loss: 0.6808 - acc: 0.5655
2368/5677 [===========>..................] - ETA: 6:55 - loss: 0.6798 - acc: 0.5688
2432/5677 [===========>..................] - ETA: 6:48 - loss: 0.6801 - acc: 0.5678
2496/5677 [============>.................] - ETA: 6:40 - loss: 0.6796 - acc: 0.5693
2560/5677 [============>.................] - ETA: 6:32 - loss: 0.6799 - acc: 0.5691
2624/5677 [============>.................] - ETA: 6:23 - loss: 0.6799 - acc: 0.5690
2688/5677 [=============>................] - ETA: 6:16 - loss: 0.6801 - acc: 0.5688
2752/5677 [=============>................] - ETA: 6:08 - loss: 0.6800 - acc: 0.5690
2816/5677 [=============>................] - ETA: 6:00 - loss: 0.6801 - acc: 0.5700
2880/5677 [==============>...............] - ETA: 5:51 - loss: 0.6801 - acc: 0.5698
2944/5677 [==============>...............] - ETA: 5:42 - loss: 0.6802 - acc: 0.5690
3008/5677 [==============>...............] - ETA: 5:34 - loss: 0.6798 - acc: 0.5708
3072/5677 [===============>..............] - ETA: 5:26 - loss: 0.6800 - acc: 0.5687
3136/5677 [===============>..............] - ETA: 5:18 - loss: 0.6798 - acc: 0.5695
3200/5677 [===============>..............] - ETA: 5:09 - loss: 0.6800 - acc: 0.5697
3264/5677 [================>.............] - ETA: 5:01 - loss: 0.6803 - acc: 0.5683
3328/5677 [================>.............] - ETA: 4:52 - loss: 0.6810 - acc: 0.5658
3392/5677 [================>.............] - ETA: 4:43 - loss: 0.6805 - acc: 0.5672
3456/5677 [=================>............] - ETA: 4:35 - loss: 0.6801 - acc: 0.5683
3520/5677 [=================>............] - ETA: 4:27 - loss: 0.6803 - acc: 0.5682
3584/5677 [=================>............] - ETA: 4:19 - loss: 0.6803 - acc: 0.5681
3648/5677 [==================>...........] - ETA: 4:11 - loss: 0.6808 - acc: 0.5680
3712/5677 [==================>...........] - ETA: 4:03 - loss: 0.6810 - acc: 0.5673
3776/5677 [==================>...........] - ETA: 3:55 - loss: 0.6813 - acc: 0.5657
3840/5677 [===================>..........] - ETA: 3:47 - loss: 0.6811 - acc: 0.5661
3904/5677 [===================>..........] - ETA: 3:39 - loss: 0.6805 - acc: 0.5679
3968/5677 [===================>..........] - ETA: 3:30 - loss: 0.6804 - acc: 0.5683
4032/5677 [====================>.........] - ETA: 3:23 - loss: 0.6802 - acc: 0.5689
4096/5677 [====================>.........] - ETA: 3:15 - loss: 0.6801 - acc: 0.5688
4160/5677 [====================>.........] - ETA: 3:07 - loss: 0.6800 - acc: 0.5687
4224/5677 [=====================>........] - ETA: 2:59 - loss: 0.6800 - acc: 0.5682
4288/5677 [=====================>........] - ETA: 2:51 - loss: 0.6798 - acc: 0.5690
4352/5677 [=====================>........] - ETA: 2:43 - loss: 0.6796 - acc: 0.5703
4416/5677 [======================>.......] - ETA: 2:35 - loss: 0.6798 - acc: 0.5709
4480/5677 [======================>.......] - ETA: 2:27 - loss: 0.6795 - acc: 0.5717
4544/5677 [=======================>......] - ETA: 2:19 - loss: 0.6790 - acc: 0.5735
4608/5677 [=======================>......] - ETA: 2:11 - loss: 0.6790 - acc: 0.5744
4672/5677 [=======================>......] - ETA: 2:03 - loss: 0.6791 - acc: 0.5745
4736/5677 [========================>.....] - ETA: 1:55 - loss: 0.6788 - acc: 0.5754
4800/5677 [========================>.....] - ETA: 1:47 - loss: 0.6789 - acc: 0.5748
4864/5677 [========================>.....] - ETA: 1:39 - loss: 0.6783 - acc: 0.5763
4928/5677 [=========================>....] - ETA: 1:31 - loss: 0.6782 - acc: 0.5757
4992/5677 [=========================>....] - ETA: 1:23 - loss: 0.6782 - acc: 0.5753
5056/5677 [=========================>....] - ETA: 1:16 - loss: 0.6780 - acc: 0.5756
5120/5677 [==========================>...] - ETA: 1:08 - loss: 0.6774 - acc: 0.5775
5184/5677 [==========================>...] - ETA: 1:00 - loss: 0.6769 - acc: 0.5779
5248/5677 [==========================>...] - ETA: 52s - loss: 0.6776 - acc: 0.5768 
5312/5677 [===========================>..] - ETA: 44s - loss: 0.6772 - acc: 0.5777
5376/5677 [===========================>..] - ETA: 36s - loss: 0.6772 - acc: 0.5776
5440/5677 [===========================>..] - ETA: 29s - loss: 0.6769 - acc: 0.5778
5504/5677 [============================>.] - ETA: 21s - loss: 0.6767 - acc: 0.5785
5568/5677 [============================>.] - ETA: 13s - loss: 0.6771 - acc: 0.5776
5632/5677 [============================>.] - ETA: 5s - loss: 0.6766 - acc: 0.5787 
5677/5677 [==============================] - 727s 128ms/step - loss: 0.6767 - acc: 0.5787 - val_loss: 0.6649 - val_acc: 0.5943

Epoch 00008: val_acc did not improve from 0.59588
Epoch 9/10

  64/5677 [..............................] - ETA: 13:45 - loss: 0.6671 - acc: 0.5938
 128/5677 [..............................] - ETA: 12:55 - loss: 0.6298 - acc: 0.6641
 192/5677 [>.............................] - ETA: 11:56 - loss: 0.6444 - acc: 0.6354
 256/5677 [>.............................] - ETA: 11:54 - loss: 0.6480 - acc: 0.6211
 320/5677 [>.............................] - ETA: 12:05 - loss: 0.6451 - acc: 0.6281
 384/5677 [=>............................] - ETA: 11:45 - loss: 0.6606 - acc: 0.6094
 448/5677 [=>............................] - ETA: 11:20 - loss: 0.6670 - acc: 0.5982
 512/5677 [=>............................] - ETA: 11:10 - loss: 0.6733 - acc: 0.5898
 576/5677 [==>...........................] - ETA: 10:48 - loss: 0.6760 - acc: 0.5851
 640/5677 [==>...........................] - ETA: 10:39 - loss: 0.6791 - acc: 0.5828
 704/5677 [==>...........................] - ETA: 10:47 - loss: 0.6768 - acc: 0.5881
 768/5677 [===>..........................] - ETA: 10:42 - loss: 0.6746 - acc: 0.5885
 832/5677 [===>..........................] - ETA: 10:36 - loss: 0.6732 - acc: 0.5889
 896/5677 [===>..........................] - ETA: 10:34 - loss: 0.6733 - acc: 0.5882
 960/5677 [====>.........................] - ETA: 10:26 - loss: 0.6693 - acc: 0.5979
1024/5677 [====>.........................] - ETA: 10:18 - loss: 0.6692 - acc: 0.5986
1088/5677 [====>.........................] - ETA: 10:07 - loss: 0.6701 - acc: 0.5965
1152/5677 [=====>........................] - ETA: 9:53 - loss: 0.6691 - acc: 0.6007 
1216/5677 [=====>........................] - ETA: 9:42 - loss: 0.6680 - acc: 0.6028
1280/5677 [=====>........................] - ETA: 9:31 - loss: 0.6708 - acc: 0.5953
1344/5677 [======>.......................] - ETA: 9:25 - loss: 0.6742 - acc: 0.5878
1408/5677 [======>.......................] - ETA: 9:19 - loss: 0.6726 - acc: 0.5902
1472/5677 [======>.......................] - ETA: 9:11 - loss: 0.6749 - acc: 0.5876
1536/5677 [=======>......................] - ETA: 8:59 - loss: 0.6753 - acc: 0.5846
1600/5677 [=======>......................] - ETA: 8:48 - loss: 0.6769 - acc: 0.5825
1664/5677 [=======>......................] - ETA: 8:39 - loss: 0.6767 - acc: 0.5823
1728/5677 [========>.....................] - ETA: 8:30 - loss: 0.6772 - acc: 0.5816
1792/5677 [========>.....................] - ETA: 8:23 - loss: 0.6768 - acc: 0.5798
1856/5677 [========>.....................] - ETA: 8:16 - loss: 0.6771 - acc: 0.5792
1920/5677 [=========>....................] - ETA: 8:09 - loss: 0.6767 - acc: 0.5797
1984/5677 [=========>....................] - ETA: 7:59 - loss: 0.6774 - acc: 0.5771
2048/5677 [=========>....................] - ETA: 7:50 - loss: 0.6777 - acc: 0.5771
2112/5677 [==========>...................] - ETA: 7:41 - loss: 0.6764 - acc: 0.5814
2176/5677 [==========>...................] - ETA: 7:34 - loss: 0.6763 - acc: 0.5823
2240/5677 [==========>...................] - ETA: 7:25 - loss: 0.6766 - acc: 0.5808
2304/5677 [===========>..................] - ETA: 7:19 - loss: 0.6767 - acc: 0.5803
2368/5677 [===========>..................] - ETA: 7:11 - loss: 0.6780 - acc: 0.5769
2432/5677 [===========>..................] - ETA: 7:03 - loss: 0.6788 - acc: 0.5740
2496/5677 [============>.................] - ETA: 6:55 - loss: 0.6785 - acc: 0.5749
2560/5677 [============>.................] - ETA: 6:45 - loss: 0.6779 - acc: 0.5746
2624/5677 [============>.................] - ETA: 6:36 - loss: 0.6780 - acc: 0.5743
2688/5677 [=============>................] - ETA: 6:27 - loss: 0.6783 - acc: 0.5733
2752/5677 [=============>................] - ETA: 6:18 - loss: 0.6795 - acc: 0.5698
2816/5677 [=============>................] - ETA: 6:10 - loss: 0.6789 - acc: 0.5735
2880/5677 [==============>...............] - ETA: 6:02 - loss: 0.6787 - acc: 0.5736
2944/5677 [==============>...............] - ETA: 5:55 - loss: 0.6785 - acc: 0.5751
3008/5677 [==============>...............] - ETA: 5:47 - loss: 0.6786 - acc: 0.5741
3072/5677 [===============>..............] - ETA: 5:39 - loss: 0.6774 - acc: 0.5771
3136/5677 [===============>..............] - ETA: 5:31 - loss: 0.6771 - acc: 0.5781
3200/5677 [===============>..............] - ETA: 5:22 - loss: 0.6767 - acc: 0.5791
3264/5677 [================>.............] - ETA: 5:13 - loss: 0.6776 - acc: 0.5769
3328/5677 [================>.............] - ETA: 5:04 - loss: 0.6775 - acc: 0.5778
3392/5677 [================>.............] - ETA: 4:56 - loss: 0.6786 - acc: 0.5767
3456/5677 [=================>............] - ETA: 4:47 - loss: 0.6784 - acc: 0.5764
3520/5677 [=================>............] - ETA: 4:39 - loss: 0.6779 - acc: 0.5778
3584/5677 [=================>............] - ETA: 4:31 - loss: 0.6777 - acc: 0.5787
3648/5677 [==================>...........] - ETA: 4:22 - loss: 0.6772 - acc: 0.5792
3712/5677 [==================>...........] - ETA: 4:14 - loss: 0.6766 - acc: 0.5800
3776/5677 [==================>...........] - ETA: 4:06 - loss: 0.6759 - acc: 0.5805
3840/5677 [===================>..........] - ETA: 3:58 - loss: 0.6758 - acc: 0.5810
3904/5677 [===================>..........] - ETA: 3:50 - loss: 0.6763 - acc: 0.5802
3968/5677 [===================>..........] - ETA: 3:41 - loss: 0.6763 - acc: 0.5814
4032/5677 [====================>.........] - ETA: 3:34 - loss: 0.6761 - acc: 0.5818
4096/5677 [====================>.........] - ETA: 3:25 - loss: 0.6762 - acc: 0.5811
4160/5677 [====================>.........] - ETA: 3:17 - loss: 0.6762 - acc: 0.5817
4224/5677 [=====================>........] - ETA: 3:09 - loss: 0.6756 - acc: 0.5833
4288/5677 [=====================>........] - ETA: 3:00 - loss: 0.6752 - acc: 0.5833
4352/5677 [=====================>........] - ETA: 2:52 - loss: 0.6753 - acc: 0.5827
4416/5677 [======================>.......] - ETA: 2:44 - loss: 0.6754 - acc: 0.5820
4480/5677 [======================>.......] - ETA: 2:35 - loss: 0.6753 - acc: 0.5826
4544/5677 [=======================>......] - ETA: 2:27 - loss: 0.6758 - acc: 0.5821
4608/5677 [=======================>......] - ETA: 2:19 - loss: 0.6754 - acc: 0.5818
4672/5677 [=======================>......] - ETA: 2:10 - loss: 0.6751 - acc: 0.5824
4736/5677 [========================>.....] - ETA: 2:02 - loss: 0.6749 - acc: 0.5830
4800/5677 [========================>.....] - ETA: 1:53 - loss: 0.6747 - acc: 0.5840
4864/5677 [========================>.....] - ETA: 1:45 - loss: 0.6749 - acc: 0.5835
4928/5677 [=========================>....] - ETA: 1:37 - loss: 0.6746 - acc: 0.5846
4992/5677 [=========================>....] - ETA: 1:28 - loss: 0.6745 - acc: 0.5849
5056/5677 [=========================>....] - ETA: 1:20 - loss: 0.6739 - acc: 0.5860
5120/5677 [==========================>...] - ETA: 1:12 - loss: 0.6739 - acc: 0.5863
5184/5677 [==========================>...] - ETA: 1:04 - loss: 0.6740 - acc: 0.5866
5248/5677 [==========================>...] - ETA: 55s - loss: 0.6737 - acc: 0.5861 
5312/5677 [===========================>..] - ETA: 47s - loss: 0.6735 - acc: 0.5857
5376/5677 [===========================>..] - ETA: 39s - loss: 0.6739 - acc: 0.5856
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6734 - acc: 0.5864
5504/5677 [============================>.] - ETA: 22s - loss: 0.6738 - acc: 0.5861
5568/5677 [============================>.] - ETA: 14s - loss: 0.6742 - acc: 0.5857
5632/5677 [============================>.] - ETA: 5s - loss: 0.6747 - acc: 0.5845 
5677/5677 [==============================] - 765s 135ms/step - loss: 0.6748 - acc: 0.5838 - val_loss: 0.6671 - val_acc: 0.5880

Epoch 00009: val_acc did not improve from 0.59588
Epoch 10/10

  64/5677 [..............................] - ETA: 12:13 - loss: 0.6669 - acc: 0.6250
 128/5677 [..............................] - ETA: 11:16 - loss: 0.6878 - acc: 0.5781
 192/5677 [>.............................] - ETA: 10:55 - loss: 0.6809 - acc: 0.5625
 256/5677 [>.............................] - ETA: 10:35 - loss: 0.6763 - acc: 0.5820
 320/5677 [>.............................] - ETA: 10:26 - loss: 0.6816 - acc: 0.5719
 384/5677 [=>............................] - ETA: 10:21 - loss: 0.6789 - acc: 0.5781
 448/5677 [=>............................] - ETA: 10:06 - loss: 0.6779 - acc: 0.5893
 512/5677 [=>............................] - ETA: 9:57 - loss: 0.6758 - acc: 0.5918 
 576/5677 [==>...........................] - ETA: 9:56 - loss: 0.6751 - acc: 0.5955
 640/5677 [==>...........................] - ETA: 9:50 - loss: 0.6747 - acc: 0.5953
 704/5677 [==>...........................] - ETA: 9:48 - loss: 0.6735 - acc: 0.5966
 768/5677 [===>..........................] - ETA: 9:41 - loss: 0.6735 - acc: 0.5924
 832/5677 [===>..........................] - ETA: 9:31 - loss: 0.6716 - acc: 0.5950
 896/5677 [===>..........................] - ETA: 9:18 - loss: 0.6738 - acc: 0.5915
 960/5677 [====>.........................] - ETA: 9:15 - loss: 0.6746 - acc: 0.5865
1024/5677 [====>.........................] - ETA: 9:06 - loss: 0.6749 - acc: 0.5859
1088/5677 [====>.........................] - ETA: 8:57 - loss: 0.6728 - acc: 0.5864
1152/5677 [=====>........................] - ETA: 8:51 - loss: 0.6720 - acc: 0.5894
1216/5677 [=====>........................] - ETA: 8:44 - loss: 0.6729 - acc: 0.5839
1280/5677 [=====>........................] - ETA: 8:40 - loss: 0.6742 - acc: 0.5789
1344/5677 [======>.......................] - ETA: 8:31 - loss: 0.6731 - acc: 0.5826
1408/5677 [======>.......................] - ETA: 8:26 - loss: 0.6727 - acc: 0.5831
1472/5677 [======>.......................] - ETA: 8:18 - loss: 0.6762 - acc: 0.5768
1536/5677 [=======>......................] - ETA: 8:10 - loss: 0.6771 - acc: 0.5749
1600/5677 [=======>......................] - ETA: 8:02 - loss: 0.6776 - acc: 0.5706
1664/5677 [=======>......................] - ETA: 7:55 - loss: 0.6768 - acc: 0.5739
1728/5677 [========>.....................] - ETA: 7:48 - loss: 0.6775 - acc: 0.5718
1792/5677 [========>.....................] - ETA: 7:42 - loss: 0.6777 - acc: 0.5720
1856/5677 [========>.....................] - ETA: 7:34 - loss: 0.6775 - acc: 0.5727
1920/5677 [=========>....................] - ETA: 7:27 - loss: 0.6770 - acc: 0.5745
1984/5677 [=========>....................] - ETA: 7:19 - loss: 0.6773 - acc: 0.5731
2048/5677 [=========>....................] - ETA: 7:10 - loss: 0.6768 - acc: 0.5732
2112/5677 [==========>...................] - ETA: 7:01 - loss: 0.6771 - acc: 0.5715
2176/5677 [==========>...................] - ETA: 6:54 - loss: 0.6772 - acc: 0.5722
2240/5677 [==========>...................] - ETA: 6:46 - loss: 0.6773 - acc: 0.5723
2304/5677 [===========>..................] - ETA: 6:39 - loss: 0.6774 - acc: 0.5725
2368/5677 [===========>..................] - ETA: 6:32 - loss: 0.6771 - acc: 0.5722
2432/5677 [===========>..................] - ETA: 6:25 - loss: 0.6772 - acc: 0.5711
2496/5677 [============>.................] - ETA: 6:17 - loss: 0.6774 - acc: 0.5713
2560/5677 [============>.................] - ETA: 6:10 - loss: 0.6767 - acc: 0.5746
2624/5677 [============>.................] - ETA: 6:02 - loss: 0.6766 - acc: 0.5751
2688/5677 [=============>................] - ETA: 5:55 - loss: 0.6758 - acc: 0.5759
2752/5677 [=============>................] - ETA: 5:46 - loss: 0.6753 - acc: 0.5770
2816/5677 [=============>................] - ETA: 5:39 - loss: 0.6752 - acc: 0.5771
2880/5677 [==============>...............] - ETA: 5:31 - loss: 0.6751 - acc: 0.5774
2944/5677 [==============>...............] - ETA: 5:23 - loss: 0.6749 - acc: 0.5771
3008/5677 [==============>...............] - ETA: 5:16 - loss: 0.6759 - acc: 0.5751
3072/5677 [===============>..............] - ETA: 5:08 - loss: 0.6754 - acc: 0.5755
3136/5677 [===============>..............] - ETA: 5:00 - loss: 0.6753 - acc: 0.5772
3200/5677 [===============>..............] - ETA: 4:53 - loss: 0.6752 - acc: 0.5787
3264/5677 [================>.............] - ETA: 4:46 - loss: 0.6758 - acc: 0.5775
3328/5677 [================>.............] - ETA: 4:38 - loss: 0.6752 - acc: 0.5781
3392/5677 [================>.............] - ETA: 4:31 - loss: 0.6748 - acc: 0.5790
3456/5677 [=================>............] - ETA: 4:23 - loss: 0.6754 - acc: 0.5773
3520/5677 [=================>............] - ETA: 4:16 - loss: 0.6754 - acc: 0.5767
3584/5677 [=================>............] - ETA: 4:08 - loss: 0.6749 - acc: 0.5784
3648/5677 [==================>...........] - ETA: 4:00 - loss: 0.6750 - acc: 0.5784
3712/5677 [==================>...........] - ETA: 3:52 - loss: 0.6751 - acc: 0.5795
3776/5677 [==================>...........] - ETA: 3:44 - loss: 0.6743 - acc: 0.5810
3840/5677 [===================>..........] - ETA: 3:36 - loss: 0.6743 - acc: 0.5802
3904/5677 [===================>..........] - ETA: 3:28 - loss: 0.6747 - acc: 0.5802
3968/5677 [===================>..........] - ETA: 3:20 - loss: 0.6743 - acc: 0.5806
4032/5677 [====================>.........] - ETA: 3:13 - loss: 0.6737 - acc: 0.5809
4096/5677 [====================>.........] - ETA: 3:05 - loss: 0.6743 - acc: 0.5793
4160/5677 [====================>.........] - ETA: 2:57 - loss: 0.6746 - acc: 0.5788
4224/5677 [=====================>........] - ETA: 2:49 - loss: 0.6743 - acc: 0.5791
4288/5677 [=====================>........] - ETA: 2:42 - loss: 0.6741 - acc: 0.5793
4352/5677 [=====================>........] - ETA: 2:35 - loss: 0.6737 - acc: 0.5804
4416/5677 [======================>.......] - ETA: 2:27 - loss: 0.6733 - acc: 0.5808
4480/5677 [======================>.......] - ETA: 2:20 - loss: 0.6733 - acc: 0.5815
4544/5677 [=======================>......] - ETA: 2:12 - loss: 0.6727 - acc: 0.5816
4608/5677 [=======================>......] - ETA: 2:05 - loss: 0.6722 - acc: 0.5829
4672/5677 [=======================>......] - ETA: 1:57 - loss: 0.6723 - acc: 0.5830
4736/5677 [========================>.....] - ETA: 1:50 - loss: 0.6728 - acc: 0.5817
4800/5677 [========================>.....] - ETA: 1:42 - loss: 0.6729 - acc: 0.5815
4864/5677 [========================>.....] - ETA: 1:35 - loss: 0.6731 - acc: 0.5820
4928/5677 [=========================>....] - ETA: 1:27 - loss: 0.6727 - acc: 0.5822
4992/5677 [=========================>....] - ETA: 1:20 - loss: 0.6728 - acc: 0.5821
5056/5677 [=========================>....] - ETA: 1:13 - loss: 0.6720 - acc: 0.5833
5120/5677 [==========================>...] - ETA: 1:05 - loss: 0.6725 - acc: 0.5828
5184/5677 [==========================>...] - ETA: 58s - loss: 0.6731 - acc: 0.5818 
5248/5677 [==========================>...] - ETA: 50s - loss: 0.6732 - acc: 0.5819
5312/5677 [===========================>..] - ETA: 43s - loss: 0.6734 - acc: 0.5817
5376/5677 [===========================>..] - ETA: 35s - loss: 0.6737 - acc: 0.5811
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6741 - acc: 0.5801
5504/5677 [============================>.] - ETA: 20s - loss: 0.6738 - acc: 0.5812
5568/5677 [============================>.] - ETA: 12s - loss: 0.6736 - acc: 0.5819
5632/5677 [============================>.] - ETA: 5s - loss: 0.6736 - acc: 0.5820 
5677/5677 [==============================] - 693s 122ms/step - loss: 0.6734 - acc: 0.5827 - val_loss: 0.6729 - val_acc: 0.5784

Epoch 00010: val_acc did not improve from 0.59588
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe0544adf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fe0544adf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe054449910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fe054449910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe054306d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe054306d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe03c7ac650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe03c7ac650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe034636ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe034636ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c24be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c24be10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe03c7ac5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe03c7ac5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe034605890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe034605890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd740d5810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd740d5810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdd543eb950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdd543eb950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c5e0b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c5e0b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd5471c5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd5471c5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c3b6810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c3b6810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe03c5e0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe03c5e0e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe03c1a5ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe03c1a5ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c19c690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c19c690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe03c379250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe03c379250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c18c5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c18c5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0347745d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0347745d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe034675150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe034675150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c535d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c535d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe03c22a3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe03c22a3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c0599d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03c0599d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe03440ba10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe03440ba10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe03429fd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe03429fd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03469c110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe03469c110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0344e6650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe0344e6650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe034188090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe034188090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe034086810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe034086810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdff8739a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdff8739a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0340c3a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0340c3a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe034086c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fe034086c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdff8739410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdff8739410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdff8506510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdff8506510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdff8474690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdff8474690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdff85063d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdff85063d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdff87d4790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdff87d4790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdff841f390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdff841f390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdff8235690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdff8235690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdff8242250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdff8242250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdff829bf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdff829bf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdff859a450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdff859a450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfd472a9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfd472a9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdfd4680a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdfd4680a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdfd45ac310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdfd45ac310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfd45eabd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfd45eabd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdfd4725790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdfd4725790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfd47282d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfd47282d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdfd46d48d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdfd46d48d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdfd438d650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdfd438d650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfd44a1e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfd44a1e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdfd43d0450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdfd43d0450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfd429fa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfd429fa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdfd403ba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdfd403ba50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdfd432b3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdfd432b3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfcc62dd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfcc62dd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdfd403b550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdfd403b550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfcc784f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfcc784f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdfd432b3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdfd432b3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdfcc702d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdfcc702d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfcc4f3e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfcc4f3e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdfcc5571d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdfcc5571d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfcc426310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdfcc426310>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 5:46
 128/1578 [=>............................] - ETA: 3:21
 192/1578 [==>...........................] - ETA: 2:28
 256/1578 [===>..........................] - ETA: 1:59
 320/1578 [=====>........................] - ETA: 1:41
 384/1578 [======>.......................] - ETA: 1:28
 448/1578 [=======>......................] - ETA: 1:17
 512/1578 [========>.....................] - ETA: 1:10
 576/1578 [=========>....................] - ETA: 1:04
 640/1578 [===========>..................] - ETA: 57s 
 704/1578 [============>.................] - ETA: 52s
 768/1578 [=============>................] - ETA: 47s
 832/1578 [==============>...............] - ETA: 42s
 896/1578 [================>.............] - ETA: 37s
 960/1578 [=================>............] - ETA: 33s
1024/1578 [==================>...........] - ETA: 29s
1088/1578 [===================>..........] - ETA: 26s
1152/1578 [====================>.........] - ETA: 22s
1216/1578 [======================>.......] - ETA: 18s
1280/1578 [=======================>......] - ETA: 15s
1344/1578 [========================>.....] - ETA: 11s
1408/1578 [=========================>....] - ETA: 8s 
1472/1578 [==========================>...] - ETA: 5s
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 78s 49ms/step
loss: 0.6754449012161661
acc: 0.574144487220827
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fdcd8443c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fdcd8443c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fdc5409c090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fdc5409c090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf78155810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf78155810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe054045ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe054045ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf78314b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf78314b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf7810d990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf7810d990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf782899d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf782899d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0543745d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0543745d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe054219950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe054219950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe054260950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fe054260950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0541dccd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe0541dccd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdcd83e5890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdcd83e5890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe054188510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe054188510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0541fe4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fe0541fe4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdcd818d110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdcd818d110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdcd8210b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdcd8210b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdcd812e850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdcd812e850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdcd8123350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdcd8123350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc9c6a1e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc9c6a1e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf185c1ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf185c1ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdcd844d890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdcd844d890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdcd81d40d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdcd81d40d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdcd848a910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdcd848a910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc9c5939d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc9c5939d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc9c285ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc9c285ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc9c1a6690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc9c1a6690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc9c593390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc9c593390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc9c5a6e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc9c5a6e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc9c285690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc9c285690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc9c302690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc9c302690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc9460f890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc9460f890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc9c1b3dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc9c1b3dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc94760a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc94760a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc94520590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc94520590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc944e2dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc944e2dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc94774750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc94774750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc945209d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc945209d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdff8512ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdff8512ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc944e2ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc944e2ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc9431aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc9431aa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc9446a990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc9446a990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc94322790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc94322790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc94118910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc94118910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc546f5e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc546f5e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc545b3f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc545b3f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc5447cf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc5447cf50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc546f5650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc546f5650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc5460add0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc5460add0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc5469d450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc5469d450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc5457e7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc5457e7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc54476f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc54476f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc54449510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc54449510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc540e6750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc540e6750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc54115f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc54115f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdb5476c250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdb5476c250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb7408a150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb7408a150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb74080090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb74080090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc54734090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc54734090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdb5464ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdb5464ded0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdb54454d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdb54454d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb5466b350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb5466b350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb5473de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb5473de10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc540f1150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc540f1150>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 1:11:43 - loss: 0.7423 - acc: 0.5312
 128/5677 [..............................] - ETA: 42:37 - loss: 0.7705 - acc: 0.5469  
 192/5677 [>.............................] - ETA: 32:23 - loss: 0.7570 - acc: 0.5417
 256/5677 [>.............................] - ETA: 27:19 - loss: 0.7612 - acc: 0.5117
 320/5677 [>.............................] - ETA: 24:16 - loss: 0.7381 - acc: 0.5437
 384/5677 [=>............................] - ETA: 22:01 - loss: 0.7508 - acc: 0.5286
 448/5677 [=>............................] - ETA: 20:23 - loss: 0.7546 - acc: 0.5290
 512/5677 [=>............................] - ETA: 19:11 - loss: 0.7522 - acc: 0.5273
 576/5677 [==>...........................] - ETA: 18:15 - loss: 0.7475 - acc: 0.5295
 640/5677 [==>...........................] - ETA: 17:28 - loss: 0.7411 - acc: 0.5328
 704/5677 [==>...........................] - ETA: 16:44 - loss: 0.7417 - acc: 0.5213
 768/5677 [===>..........................] - ETA: 16:09 - loss: 0.7402 - acc: 0.5182
 832/5677 [===>..........................] - ETA: 15:35 - loss: 0.7397 - acc: 0.5156
 896/5677 [===>..........................] - ETA: 15:05 - loss: 0.7369 - acc: 0.5145
 960/5677 [====>.........................] - ETA: 14:40 - loss: 0.7347 - acc: 0.5198
1024/5677 [====>.........................] - ETA: 14:18 - loss: 0.7338 - acc: 0.5205
1088/5677 [====>.........................] - ETA: 13:52 - loss: 0.7299 - acc: 0.5230
1152/5677 [=====>........................] - ETA: 13:32 - loss: 0.7299 - acc: 0.5243
1216/5677 [=====>........................] - ETA: 13:10 - loss: 0.7279 - acc: 0.5247
1280/5677 [=====>........................] - ETA: 12:52 - loss: 0.7270 - acc: 0.5242
1344/5677 [======>.......................] - ETA: 12:35 - loss: 0.7246 - acc: 0.5275
1408/5677 [======>.......................] - ETA: 12:21 - loss: 0.7247 - acc: 0.5270
1472/5677 [======>.......................] - ETA: 12:03 - loss: 0.7243 - acc: 0.5272
1536/5677 [=======>......................] - ETA: 11:48 - loss: 0.7249 - acc: 0.5286
1600/5677 [=======>......................] - ETA: 11:33 - loss: 0.7237 - acc: 0.5306
1664/5677 [=======>......................] - ETA: 11:18 - loss: 0.7224 - acc: 0.5319
1728/5677 [========>.....................] - ETA: 11:06 - loss: 0.7217 - acc: 0.5318
1792/5677 [========>.....................] - ETA: 10:53 - loss: 0.7214 - acc: 0.5318
1856/5677 [========>.....................] - ETA: 10:39 - loss: 0.7218 - acc: 0.5280
1920/5677 [=========>....................] - ETA: 10:26 - loss: 0.7228 - acc: 0.5266
1984/5677 [=========>....................] - ETA: 10:13 - loss: 0.7235 - acc: 0.5247
2048/5677 [=========>....................] - ETA: 10:01 - loss: 0.7256 - acc: 0.5195
2112/5677 [==========>...................] - ETA: 9:48 - loss: 0.7263 - acc: 0.5185 
2176/5677 [==========>...................] - ETA: 9:36 - loss: 0.7248 - acc: 0.5202
2240/5677 [==========>...................] - ETA: 9:22 - loss: 0.7240 - acc: 0.5214
2304/5677 [===========>..................] - ETA: 9:10 - loss: 0.7232 - acc: 0.5243
2368/5677 [===========>..................] - ETA: 8:57 - loss: 0.7244 - acc: 0.5215
2432/5677 [===========>..................] - ETA: 8:45 - loss: 0.7229 - acc: 0.5226
2496/5677 [============>.................] - ETA: 8:34 - loss: 0.7238 - acc: 0.5212
2560/5677 [============>.................] - ETA: 8:22 - loss: 0.7235 - acc: 0.5207
2624/5677 [============>.................] - ETA: 8:11 - loss: 0.7235 - acc: 0.5194
2688/5677 [=============>................] - ETA: 7:59 - loss: 0.7228 - acc: 0.5190
2752/5677 [=============>................] - ETA: 7:48 - loss: 0.7220 - acc: 0.5203
2816/5677 [=============>................] - ETA: 7:37 - loss: 0.7221 - acc: 0.5188
2880/5677 [==============>...............] - ETA: 7:25 - loss: 0.7224 - acc: 0.5170
2944/5677 [==============>...............] - ETA: 7:14 - loss: 0.7225 - acc: 0.5160
3008/5677 [==============>...............] - ETA: 7:04 - loss: 0.7227 - acc: 0.5143
3072/5677 [===============>..............] - ETA: 6:52 - loss: 0.7224 - acc: 0.5127
3136/5677 [===============>..............] - ETA: 6:40 - loss: 0.7224 - acc: 0.5124
3200/5677 [===============>..............] - ETA: 6:30 - loss: 0.7217 - acc: 0.5131
3264/5677 [================>.............] - ETA: 6:19 - loss: 0.7215 - acc: 0.5126
3328/5677 [================>.............] - ETA: 6:08 - loss: 0.7220 - acc: 0.5099
3392/5677 [================>.............] - ETA: 5:57 - loss: 0.7210 - acc: 0.5112
3456/5677 [=================>............] - ETA: 5:46 - loss: 0.7212 - acc: 0.5113
3520/5677 [=================>............] - ETA: 5:35 - loss: 0.7221 - acc: 0.5108
3584/5677 [=================>............] - ETA: 5:24 - loss: 0.7219 - acc: 0.5106
3648/5677 [==================>...........] - ETA: 5:14 - loss: 0.7217 - acc: 0.5123
3712/5677 [==================>...........] - ETA: 5:03 - loss: 0.7216 - acc: 0.5124
3776/5677 [==================>...........] - ETA: 4:53 - loss: 0.7214 - acc: 0.5127
3840/5677 [===================>..........] - ETA: 4:43 - loss: 0.7213 - acc: 0.5120
3904/5677 [===================>..........] - ETA: 4:33 - loss: 0.7208 - acc: 0.5123
3968/5677 [===================>..........] - ETA: 4:23 - loss: 0.7209 - acc: 0.5123
4032/5677 [====================>.........] - ETA: 4:13 - loss: 0.7199 - acc: 0.5139
4096/5677 [====================>.........] - ETA: 4:03 - loss: 0.7201 - acc: 0.5137
4160/5677 [====================>.........] - ETA: 3:53 - loss: 0.7197 - acc: 0.5147
4224/5677 [=====================>........] - ETA: 3:43 - loss: 0.7194 - acc: 0.5154
4288/5677 [=====================>........] - ETA: 3:33 - loss: 0.7196 - acc: 0.5149
4352/5677 [=====================>........] - ETA: 3:23 - loss: 0.7191 - acc: 0.5154
4416/5677 [======================>.......] - ETA: 3:14 - loss: 0.7195 - acc: 0.5145
4480/5677 [======================>.......] - ETA: 3:03 - loss: 0.7190 - acc: 0.5150
4544/5677 [=======================>......] - ETA: 2:53 - loss: 0.7185 - acc: 0.5154
4608/5677 [=======================>......] - ETA: 2:43 - loss: 0.7179 - acc: 0.5163
4672/5677 [=======================>......] - ETA: 2:34 - loss: 0.7176 - acc: 0.5171
4736/5677 [========================>.....] - ETA: 2:24 - loss: 0.7179 - acc: 0.5160
4800/5677 [========================>.....] - ETA: 2:14 - loss: 0.7182 - acc: 0.5142
4864/5677 [========================>.....] - ETA: 2:04 - loss: 0.7181 - acc: 0.5148
4928/5677 [=========================>....] - ETA: 1:54 - loss: 0.7180 - acc: 0.5144
4992/5677 [=========================>....] - ETA: 1:44 - loss: 0.7176 - acc: 0.5158
5056/5677 [=========================>....] - ETA: 1:34 - loss: 0.7173 - acc: 0.5164
5120/5677 [==========================>...] - ETA: 1:25 - loss: 0.7168 - acc: 0.5170
5184/5677 [==========================>...] - ETA: 1:15 - loss: 0.7163 - acc: 0.5177
5248/5677 [==========================>...] - ETA: 1:05 - loss: 0.7157 - acc: 0.5185
5312/5677 [===========================>..] - ETA: 55s - loss: 0.7158 - acc: 0.5186 
5376/5677 [===========================>..] - ETA: 45s - loss: 0.7155 - acc: 0.5188
5440/5677 [===========================>..] - ETA: 36s - loss: 0.7151 - acc: 0.5184
5504/5677 [============================>.] - ETA: 26s - loss: 0.7148 - acc: 0.5180
5568/5677 [============================>.] - ETA: 16s - loss: 0.7143 - acc: 0.5183
5632/5677 [============================>.] - ETA: 6s - loss: 0.7144 - acc: 0.5174 
5677/5677 [==============================] - 903s 159ms/step - loss: 0.7141 - acc: 0.5172 - val_loss: 0.6985 - val_acc: 0.5309

Epoch 00001: val_acc improved from -inf to 0.53090, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window19/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 14:28 - loss: 0.6982 - acc: 0.5156
 128/5677 [..............................] - ETA: 14:23 - loss: 0.6995 - acc: 0.5469
 192/5677 [>.............................] - ETA: 13:48 - loss: 0.6908 - acc: 0.5677
 256/5677 [>.............................] - ETA: 13:16 - loss: 0.6869 - acc: 0.5703
 320/5677 [>.............................] - ETA: 13:22 - loss: 0.6848 - acc: 0.5875
 384/5677 [=>............................] - ETA: 13:00 - loss: 0.6992 - acc: 0.5625
 448/5677 [=>............................] - ETA: 12:42 - loss: 0.7000 - acc: 0.5491
 512/5677 [=>............................] - ETA: 12:31 - loss: 0.7010 - acc: 0.5469
 576/5677 [==>...........................] - ETA: 12:22 - loss: 0.7048 - acc: 0.5347
 640/5677 [==>...........................] - ETA: 12:18 - loss: 0.7050 - acc: 0.5312
 704/5677 [==>...........................] - ETA: 12:05 - loss: 0.7074 - acc: 0.5270
 768/5677 [===>..........................] - ETA: 11:58 - loss: 0.7089 - acc: 0.5273
 832/5677 [===>..........................] - ETA: 11:47 - loss: 0.7048 - acc: 0.5349
 896/5677 [===>..........................] - ETA: 11:33 - loss: 0.7068 - acc: 0.5324
 960/5677 [====>.........................] - ETA: 11:23 - loss: 0.7061 - acc: 0.5375
1024/5677 [====>.........................] - ETA: 11:15 - loss: 0.7031 - acc: 0.5449
1088/5677 [====>.........................] - ETA: 11:08 - loss: 0.7065 - acc: 0.5404
1152/5677 [=====>........................] - ETA: 11:00 - loss: 0.7060 - acc: 0.5408
1216/5677 [=====>........................] - ETA: 10:45 - loss: 0.7055 - acc: 0.5395
1280/5677 [=====>........................] - ETA: 10:36 - loss: 0.7042 - acc: 0.5406
1344/5677 [======>.......................] - ETA: 10:25 - loss: 0.7054 - acc: 0.5365
1408/5677 [======>.......................] - ETA: 10:14 - loss: 0.7047 - acc: 0.5384
1472/5677 [======>.......................] - ETA: 10:02 - loss: 0.7040 - acc: 0.5380
1536/5677 [=======>......................] - ETA: 9:54 - loss: 0.7022 - acc: 0.5404 
1600/5677 [=======>......................] - ETA: 9:47 - loss: 0.7017 - acc: 0.5387
1664/5677 [=======>......................] - ETA: 9:39 - loss: 0.7006 - acc: 0.5409
1728/5677 [========>.....................] - ETA: 9:31 - loss: 0.7008 - acc: 0.5399
1792/5677 [========>.....................] - ETA: 9:23 - loss: 0.7007 - acc: 0.5379
1856/5677 [========>.....................] - ETA: 9:15 - loss: 0.7004 - acc: 0.5388
1920/5677 [=========>....................] - ETA: 9:05 - loss: 0.6999 - acc: 0.5391
1984/5677 [=========>....................] - ETA: 8:58 - loss: 0.7006 - acc: 0.5378
2048/5677 [=========>....................] - ETA: 8:49 - loss: 0.7004 - acc: 0.5352
2112/5677 [==========>...................] - ETA: 8:41 - loss: 0.7008 - acc: 0.5331
2176/5677 [==========>...................] - ETA: 8:34 - loss: 0.7007 - acc: 0.5331
2240/5677 [==========>...................] - ETA: 8:25 - loss: 0.7010 - acc: 0.5326
2304/5677 [===========>..................] - ETA: 8:17 - loss: 0.7006 - acc: 0.5326
2368/5677 [===========>..................] - ETA: 8:06 - loss: 0.7014 - acc: 0.5296
2432/5677 [===========>..................] - ETA: 7:56 - loss: 0.7012 - acc: 0.5304
2496/5677 [============>.................] - ETA: 7:46 - loss: 0.7011 - acc: 0.5312
2560/5677 [============>.................] - ETA: 7:36 - loss: 0.7005 - acc: 0.5324
2624/5677 [============>.................] - ETA: 7:25 - loss: 0.7011 - acc: 0.5305
2688/5677 [=============>................] - ETA: 7:17 - loss: 0.7010 - acc: 0.5320
2752/5677 [=============>................] - ETA: 7:07 - loss: 0.7006 - acc: 0.5316
2816/5677 [=============>................] - ETA: 6:58 - loss: 0.7004 - acc: 0.5305
2880/5677 [==============>...............] - ETA: 6:48 - loss: 0.6999 - acc: 0.5309
2944/5677 [==============>...............] - ETA: 6:38 - loss: 0.6998 - acc: 0.5306
3008/5677 [==============>...............] - ETA: 6:29 - loss: 0.7007 - acc: 0.5276
3072/5677 [===============>..............] - ETA: 6:20 - loss: 0.7009 - acc: 0.5267
3136/5677 [===============>..............] - ETA: 6:11 - loss: 0.7013 - acc: 0.5242
3200/5677 [===============>..............] - ETA: 6:02 - loss: 0.7012 - acc: 0.5247
3264/5677 [================>.............] - ETA: 5:53 - loss: 0.7008 - acc: 0.5254
3328/5677 [================>.............] - ETA: 5:43 - loss: 0.7004 - acc: 0.5252
3392/5677 [================>.............] - ETA: 5:33 - loss: 0.7006 - acc: 0.5245
3456/5677 [=================>............] - ETA: 5:24 - loss: 0.6999 - acc: 0.5266
3520/5677 [=================>............] - ETA: 5:14 - loss: 0.7002 - acc: 0.5250
3584/5677 [=================>............] - ETA: 5:05 - loss: 0.6995 - acc: 0.5276
3648/5677 [==================>...........] - ETA: 4:56 - loss: 0.6994 - acc: 0.5274
3712/5677 [==================>...........] - ETA: 4:47 - loss: 0.6994 - acc: 0.5267
3776/5677 [==================>...........] - ETA: 4:38 - loss: 0.6989 - acc: 0.5281
3840/5677 [===================>..........] - ETA: 4:28 - loss: 0.6990 - acc: 0.5276
3904/5677 [===================>..........] - ETA: 4:19 - loss: 0.6993 - acc: 0.5277
3968/5677 [===================>..........] - ETA: 4:09 - loss: 0.6994 - acc: 0.5270
4032/5677 [====================>.........] - ETA: 4:00 - loss: 0.6991 - acc: 0.5268
4096/5677 [====================>.........] - ETA: 3:51 - loss: 0.6994 - acc: 0.5256
4160/5677 [====================>.........] - ETA: 3:42 - loss: 0.6993 - acc: 0.5257
4224/5677 [=====================>........] - ETA: 3:32 - loss: 0.6994 - acc: 0.5249
4288/5677 [=====================>........] - ETA: 3:23 - loss: 0.6992 - acc: 0.5257
4352/5677 [=====================>........] - ETA: 3:14 - loss: 0.6988 - acc: 0.5273
4416/5677 [======================>.......] - ETA: 3:05 - loss: 0.6988 - acc: 0.5276
4480/5677 [======================>.......] - ETA: 2:56 - loss: 0.6989 - acc: 0.5268
4544/5677 [=======================>......] - ETA: 2:46 - loss: 0.6989 - acc: 0.5264
4608/5677 [=======================>......] - ETA: 2:37 - loss: 0.6990 - acc: 0.5256
4672/5677 [=======================>......] - ETA: 2:27 - loss: 0.6990 - acc: 0.5255
4736/5677 [========================>.....] - ETA: 2:18 - loss: 0.6994 - acc: 0.5251
4800/5677 [========================>.....] - ETA: 2:09 - loss: 0.6988 - acc: 0.5262
4864/5677 [========================>.....] - ETA: 1:59 - loss: 0.6990 - acc: 0.5261
4928/5677 [=========================>....] - ETA: 1:50 - loss: 0.6990 - acc: 0.5266
4992/5677 [=========================>....] - ETA: 1:40 - loss: 0.6990 - acc: 0.5268
5056/5677 [=========================>....] - ETA: 1:31 - loss: 0.6990 - acc: 0.5261
5120/5677 [==========================>...] - ETA: 1:22 - loss: 0.6988 - acc: 0.5264
5184/5677 [==========================>...] - ETA: 1:12 - loss: 0.6990 - acc: 0.5266
5248/5677 [==========================>...] - ETA: 1:03 - loss: 0.6993 - acc: 0.5261
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6994 - acc: 0.5260 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6992 - acc: 0.5257
5440/5677 [===========================>..] - ETA: 35s - loss: 0.6991 - acc: 0.5267
5504/5677 [============================>.] - ETA: 25s - loss: 0.6991 - acc: 0.5265
5568/5677 [============================>.] - ETA: 16s - loss: 0.6994 - acc: 0.5259
5632/5677 [============================>.] - ETA: 6s - loss: 0.6996 - acc: 0.5254 
5677/5677 [==============================] - 878s 155ms/step - loss: 0.6994 - acc: 0.5255 - val_loss: 0.6850 - val_acc: 0.5563

Epoch 00002: val_acc improved from 0.53090 to 0.55626, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window19/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 14:43 - loss: 0.6844 - acc: 0.6094
 128/5677 [..............................] - ETA: 14:31 - loss: 0.6862 - acc: 0.6172
 192/5677 [>.............................] - ETA: 14:21 - loss: 0.6945 - acc: 0.5781
 256/5677 [>.............................] - ETA: 14:03 - loss: 0.6958 - acc: 0.5430
 320/5677 [>.............................] - ETA: 13:45 - loss: 0.6962 - acc: 0.5469
 384/5677 [=>............................] - ETA: 13:32 - loss: 0.6927 - acc: 0.5469
 448/5677 [=>............................] - ETA: 13:09 - loss: 0.6928 - acc: 0.5469
 512/5677 [=>............................] - ETA: 12:50 - loss: 0.6914 - acc: 0.5488
 576/5677 [==>...........................] - ETA: 12:37 - loss: 0.6947 - acc: 0.5469
 640/5677 [==>...........................] - ETA: 12:28 - loss: 0.6949 - acc: 0.5453
 704/5677 [==>...........................] - ETA: 12:18 - loss: 0.6953 - acc: 0.5455
 768/5677 [===>..........................] - ETA: 12:08 - loss: 0.6945 - acc: 0.5430
 832/5677 [===>..........................] - ETA: 11:57 - loss: 0.6985 - acc: 0.5385
 896/5677 [===>..........................] - ETA: 11:47 - loss: 0.6954 - acc: 0.5413
 960/5677 [====>.........................] - ETA: 11:36 - loss: 0.6956 - acc: 0.5365
1024/5677 [====>.........................] - ETA: 11:25 - loss: 0.6964 - acc: 0.5332
1088/5677 [====>.........................] - ETA: 11:13 - loss: 0.6965 - acc: 0.5303
1152/5677 [=====>........................] - ETA: 11:04 - loss: 0.6967 - acc: 0.5304
1216/5677 [=====>........................] - ETA: 10:52 - loss: 0.6972 - acc: 0.5304
1280/5677 [=====>........................] - ETA: 10:43 - loss: 0.6965 - acc: 0.5312
1344/5677 [======>.......................] - ETA: 10:34 - loss: 0.6956 - acc: 0.5342
1408/5677 [======>.......................] - ETA: 10:27 - loss: 0.6936 - acc: 0.5384
1472/5677 [======>.......................] - ETA: 10:17 - loss: 0.6950 - acc: 0.5360
1536/5677 [=======>......................] - ETA: 10:06 - loss: 0.6950 - acc: 0.5378
1600/5677 [=======>......................] - ETA: 9:57 - loss: 0.6959 - acc: 0.5350 
1664/5677 [=======>......................] - ETA: 9:48 - loss: 0.6956 - acc: 0.5331
1728/5677 [========>.....................] - ETA: 9:39 - loss: 0.6945 - acc: 0.5353
1792/5677 [========>.....................] - ETA: 9:28 - loss: 0.6954 - acc: 0.5335
1856/5677 [========>.....................] - ETA: 9:20 - loss: 0.6955 - acc: 0.5329
1920/5677 [=========>....................] - ETA: 9:11 - loss: 0.6946 - acc: 0.5354
1984/5677 [=========>....................] - ETA: 9:02 - loss: 0.6956 - acc: 0.5343
2048/5677 [=========>....................] - ETA: 8:52 - loss: 0.6947 - acc: 0.5361
2112/5677 [==========>...................] - ETA: 8:42 - loss: 0.6936 - acc: 0.5379
2176/5677 [==========>...................] - ETA: 8:32 - loss: 0.6948 - acc: 0.5363
2240/5677 [==========>...................] - ETA: 8:23 - loss: 0.6949 - acc: 0.5362
2304/5677 [===========>..................] - ETA: 8:15 - loss: 0.6944 - acc: 0.5369
2368/5677 [===========>..................] - ETA: 8:07 - loss: 0.6941 - acc: 0.5372
2432/5677 [===========>..................] - ETA: 7:58 - loss: 0.6937 - acc: 0.5374
2496/5677 [============>.................] - ETA: 7:48 - loss: 0.6935 - acc: 0.5357
2560/5677 [============>.................] - ETA: 7:39 - loss: 0.6930 - acc: 0.5371
2624/5677 [============>.................] - ETA: 7:29 - loss: 0.6924 - acc: 0.5377
2688/5677 [=============>................] - ETA: 7:18 - loss: 0.6924 - acc: 0.5368
2752/5677 [=============>................] - ETA: 7:09 - loss: 0.6920 - acc: 0.5374
2816/5677 [=============>................] - ETA: 6:59 - loss: 0.6918 - acc: 0.5373
2880/5677 [==============>...............] - ETA: 6:50 - loss: 0.6919 - acc: 0.5372
2944/5677 [==============>...............] - ETA: 6:40 - loss: 0.6910 - acc: 0.5387
3008/5677 [==============>...............] - ETA: 6:30 - loss: 0.6907 - acc: 0.5392
3072/5677 [===============>..............] - ETA: 6:21 - loss: 0.6913 - acc: 0.5374
3136/5677 [===============>..............] - ETA: 6:11 - loss: 0.6905 - acc: 0.5399
3200/5677 [===============>..............] - ETA: 6:01 - loss: 0.6893 - acc: 0.5437
3264/5677 [================>.............] - ETA: 5:52 - loss: 0.6894 - acc: 0.5435
3328/5677 [================>.............] - ETA: 5:42 - loss: 0.6887 - acc: 0.5445
3392/5677 [================>.............] - ETA: 5:32 - loss: 0.6894 - acc: 0.5442
3456/5677 [=================>............] - ETA: 5:23 - loss: 0.6891 - acc: 0.5446
3520/5677 [=================>............] - ETA: 5:13 - loss: 0.6894 - acc: 0.5449
3584/5677 [=================>............] - ETA: 5:03 - loss: 0.6894 - acc: 0.5455
3648/5677 [==================>...........] - ETA: 4:53 - loss: 0.6893 - acc: 0.5463
3712/5677 [==================>...........] - ETA: 4:44 - loss: 0.6890 - acc: 0.5469
3776/5677 [==================>...........] - ETA: 4:34 - loss: 0.6890 - acc: 0.5474
3840/5677 [===================>..........] - ETA: 4:24 - loss: 0.6887 - acc: 0.5482
3904/5677 [===================>..........] - ETA: 4:15 - loss: 0.6886 - acc: 0.5476
3968/5677 [===================>..........] - ETA: 4:06 - loss: 0.6884 - acc: 0.5484
4032/5677 [====================>.........] - ETA: 3:56 - loss: 0.6884 - acc: 0.5474
4096/5677 [====================>.........] - ETA: 3:47 - loss: 0.6884 - acc: 0.5479
4160/5677 [====================>.........] - ETA: 3:38 - loss: 0.6887 - acc: 0.5469
4224/5677 [=====================>........] - ETA: 3:28 - loss: 0.6883 - acc: 0.5478
4288/5677 [=====================>........] - ETA: 3:19 - loss: 0.6882 - acc: 0.5485
4352/5677 [=====================>........] - ETA: 3:10 - loss: 0.6884 - acc: 0.5492
4416/5677 [======================>.......] - ETA: 3:01 - loss: 0.6881 - acc: 0.5507
4480/5677 [======================>.......] - ETA: 2:51 - loss: 0.6882 - acc: 0.5509
4544/5677 [=======================>......] - ETA: 2:42 - loss: 0.6881 - acc: 0.5515
4608/5677 [=======================>......] - ETA: 2:33 - loss: 0.6886 - acc: 0.5506
4672/5677 [=======================>......] - ETA: 2:23 - loss: 0.6883 - acc: 0.5514
4736/5677 [========================>.....] - ETA: 2:14 - loss: 0.6884 - acc: 0.5513
4800/5677 [========================>.....] - ETA: 2:05 - loss: 0.6880 - acc: 0.5521
4864/5677 [========================>.....] - ETA: 1:56 - loss: 0.6882 - acc: 0.5518
4928/5677 [=========================>....] - ETA: 1:47 - loss: 0.6884 - acc: 0.5505
4992/5677 [=========================>....] - ETA: 1:38 - loss: 0.6885 - acc: 0.5497
5056/5677 [=========================>....] - ETA: 1:29 - loss: 0.6883 - acc: 0.5506
5120/5677 [==========================>...] - ETA: 1:19 - loss: 0.6879 - acc: 0.5520
5184/5677 [==========================>...] - ETA: 1:10 - loss: 0.6879 - acc: 0.5523
5248/5677 [==========================>...] - ETA: 1:01 - loss: 0.6879 - acc: 0.5518
5312/5677 [===========================>..] - ETA: 52s - loss: 0.6879 - acc: 0.5520 
5376/5677 [===========================>..] - ETA: 42s - loss: 0.6883 - acc: 0.5517
5440/5677 [===========================>..] - ETA: 33s - loss: 0.6880 - acc: 0.5526
5504/5677 [============================>.] - ETA: 24s - loss: 0.6884 - acc: 0.5512
5568/5677 [============================>.] - ETA: 15s - loss: 0.6885 - acc: 0.5512
5632/5677 [============================>.] - ETA: 6s - loss: 0.6881 - acc: 0.5513 
5677/5677 [==============================] - 842s 148ms/step - loss: 0.6881 - acc: 0.5510 - val_loss: 0.6857 - val_acc: 0.5578

Epoch 00003: val_acc improved from 0.55626 to 0.55784, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window19/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 4/10

  64/5677 [..............................] - ETA: 13:51 - loss: 0.7165 - acc: 0.4375
 128/5677 [..............................] - ETA: 13:53 - loss: 0.7039 - acc: 0.5234
 192/5677 [>.............................] - ETA: 13:51 - loss: 0.6921 - acc: 0.5312
 256/5677 [>.............................] - ETA: 13:36 - loss: 0.6772 - acc: 0.5625
 320/5677 [>.............................] - ETA: 13:21 - loss: 0.6761 - acc: 0.5781
 384/5677 [=>............................] - ETA: 13:03 - loss: 0.6719 - acc: 0.5781
 448/5677 [=>............................] - ETA: 12:51 - loss: 0.6807 - acc: 0.5491
 512/5677 [=>............................] - ETA: 12:43 - loss: 0.6774 - acc: 0.5566
 576/5677 [==>...........................] - ETA: 12:30 - loss: 0.6836 - acc: 0.5486
 640/5677 [==>...........................] - ETA: 12:19 - loss: 0.6817 - acc: 0.5516
 704/5677 [==>...........................] - ETA: 12:13 - loss: 0.6825 - acc: 0.5568
 768/5677 [===>..........................] - ETA: 12:01 - loss: 0.6839 - acc: 0.5521
 832/5677 [===>..........................] - ETA: 11:48 - loss: 0.6862 - acc: 0.5481
 896/5677 [===>..........................] - ETA: 11:42 - loss: 0.6840 - acc: 0.5580
 960/5677 [====>.........................] - ETA: 11:33 - loss: 0.6853 - acc: 0.5573
1024/5677 [====>.........................] - ETA: 11:21 - loss: 0.6820 - acc: 0.5605
1088/5677 [====>.........................] - ETA: 11:10 - loss: 0.6809 - acc: 0.5616
1152/5677 [=====>........................] - ETA: 10:58 - loss: 0.6811 - acc: 0.5564
1216/5677 [=====>........................] - ETA: 10:48 - loss: 0.6829 - acc: 0.5567
1280/5677 [=====>........................] - ETA: 10:36 - loss: 0.6834 - acc: 0.5555
1344/5677 [======>.......................] - ETA: 10:30 - loss: 0.6839 - acc: 0.5543
1408/5677 [======>.......................] - ETA: 10:18 - loss: 0.6831 - acc: 0.5561
1472/5677 [======>.......................] - ETA: 10:11 - loss: 0.6854 - acc: 0.5516
1536/5677 [=======>......................] - ETA: 10:01 - loss: 0.6867 - acc: 0.5469
1600/5677 [=======>......................] - ETA: 9:51 - loss: 0.6858 - acc: 0.5487 
1664/5677 [=======>......................] - ETA: 9:42 - loss: 0.6869 - acc: 0.5469
1728/5677 [========>.....................] - ETA: 9:30 - loss: 0.6882 - acc: 0.5440
1792/5677 [========>.....................] - ETA: 9:21 - loss: 0.6889 - acc: 0.5419
1856/5677 [========>.....................] - ETA: 9:13 - loss: 0.6887 - acc: 0.5436
1920/5677 [=========>....................] - ETA: 9:03 - loss: 0.6883 - acc: 0.5448
1984/5677 [=========>....................] - ETA: 8:53 - loss: 0.6881 - acc: 0.5459
2048/5677 [=========>....................] - ETA: 8:48 - loss: 0.6877 - acc: 0.5479
2112/5677 [==========>...................] - ETA: 8:37 - loss: 0.6865 - acc: 0.5511
2176/5677 [==========>...................] - ETA: 8:26 - loss: 0.6861 - acc: 0.5528
2240/5677 [==========>...................] - ETA: 8:16 - loss: 0.6852 - acc: 0.5554
2304/5677 [===========>..................] - ETA: 8:07 - loss: 0.6844 - acc: 0.5564
2368/5677 [===========>..................] - ETA: 7:59 - loss: 0.6851 - acc: 0.5570
2432/5677 [===========>..................] - ETA: 7:49 - loss: 0.6853 - acc: 0.5555
2496/5677 [============>.................] - ETA: 7:38 - loss: 0.6859 - acc: 0.5545
2560/5677 [============>.................] - ETA: 7:28 - loss: 0.6866 - acc: 0.5527
2624/5677 [============>.................] - ETA: 7:19 - loss: 0.6874 - acc: 0.5518
2688/5677 [=============>................] - ETA: 7:10 - loss: 0.6875 - acc: 0.5513
2752/5677 [=============>................] - ETA: 7:01 - loss: 0.6867 - acc: 0.5534
2816/5677 [=============>................] - ETA: 6:52 - loss: 0.6873 - acc: 0.5515
2880/5677 [==============>...............] - ETA: 6:42 - loss: 0.6876 - acc: 0.5503
2944/5677 [==============>...............] - ETA: 6:33 - loss: 0.6875 - acc: 0.5506
3008/5677 [==============>...............] - ETA: 6:23 - loss: 0.6872 - acc: 0.5519
3072/5677 [===============>..............] - ETA: 6:14 - loss: 0.6873 - acc: 0.5518
3136/5677 [===============>..............] - ETA: 6:05 - loss: 0.6873 - acc: 0.5513
3200/5677 [===============>..............] - ETA: 5:55 - loss: 0.6874 - acc: 0.5516
3264/5677 [================>.............] - ETA: 5:45 - loss: 0.6869 - acc: 0.5530
3328/5677 [================>.............] - ETA: 5:36 - loss: 0.6874 - acc: 0.5517
3392/5677 [================>.............] - ETA: 5:26 - loss: 0.6879 - acc: 0.5498
3456/5677 [=================>............] - ETA: 5:16 - loss: 0.6884 - acc: 0.5486
3520/5677 [=================>............] - ETA: 5:07 - loss: 0.6882 - acc: 0.5489
3584/5677 [=================>............] - ETA: 4:59 - loss: 0.6877 - acc: 0.5488
3648/5677 [==================>...........] - ETA: 4:50 - loss: 0.6875 - acc: 0.5499
3712/5677 [==================>...........] - ETA: 4:41 - loss: 0.6875 - acc: 0.5498
3776/5677 [==================>...........] - ETA: 4:31 - loss: 0.6879 - acc: 0.5493
3840/5677 [===================>..........] - ETA: 4:22 - loss: 0.6881 - acc: 0.5490
3904/5677 [===================>..........] - ETA: 4:13 - loss: 0.6883 - acc: 0.5487
3968/5677 [===================>..........] - ETA: 4:04 - loss: 0.6876 - acc: 0.5509
4032/5677 [====================>.........] - ETA: 3:55 - loss: 0.6869 - acc: 0.5521
4096/5677 [====================>.........] - ETA: 3:46 - loss: 0.6870 - acc: 0.5518
4160/5677 [====================>.........] - ETA: 3:38 - loss: 0.6870 - acc: 0.5514
4224/5677 [=====================>........] - ETA: 3:28 - loss: 0.6868 - acc: 0.5526
4288/5677 [=====================>........] - ETA: 3:19 - loss: 0.6863 - acc: 0.5534
4352/5677 [=====================>........] - ETA: 3:10 - loss: 0.6857 - acc: 0.5540
4416/5677 [======================>.......] - ETA: 3:01 - loss: 0.6855 - acc: 0.5550
4480/5677 [======================>.......] - ETA: 2:51 - loss: 0.6857 - acc: 0.5549
4544/5677 [=======================>......] - ETA: 2:42 - loss: 0.6853 - acc: 0.5555
4608/5677 [=======================>......] - ETA: 2:34 - loss: 0.6854 - acc: 0.5549
4672/5677 [=======================>......] - ETA: 2:25 - loss: 0.6850 - acc: 0.5557
4736/5677 [========================>.....] - ETA: 2:15 - loss: 0.6850 - acc: 0.5560
4800/5677 [========================>.....] - ETA: 2:06 - loss: 0.6849 - acc: 0.5556
4864/5677 [========================>.....] - ETA: 1:57 - loss: 0.6852 - acc: 0.5551
4928/5677 [=========================>....] - ETA: 1:48 - loss: 0.6852 - acc: 0.5558
4992/5677 [=========================>....] - ETA: 1:39 - loss: 0.6854 - acc: 0.5555
5056/5677 [=========================>....] - ETA: 1:30 - loss: 0.6854 - acc: 0.5558
5120/5677 [==========================>...] - ETA: 1:20 - loss: 0.6849 - acc: 0.5568
5184/5677 [==========================>...] - ETA: 1:11 - loss: 0.6850 - acc: 0.5563
5248/5677 [==========================>...] - ETA: 1:02 - loss: 0.6847 - acc: 0.5572
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6846 - acc: 0.5569 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6848 - acc: 0.5564
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6846 - acc: 0.5564
5504/5677 [============================>.] - ETA: 25s - loss: 0.6844 - acc: 0.5574
5568/5677 [============================>.] - ETA: 16s - loss: 0.6847 - acc: 0.5564
5632/5677 [============================>.] - ETA: 6s - loss: 0.6847 - acc: 0.5574 
5677/5677 [==============================] - 875s 154ms/step - loss: 0.6846 - acc: 0.5580 - val_loss: 0.6858 - val_acc: 0.5578

Epoch 00004: val_acc did not improve from 0.55784
Epoch 5/10

  64/5677 [..............................] - ETA: 15:12 - loss: 0.6889 - acc: 0.5625
 128/5677 [..............................] - ETA: 14:29 - loss: 0.6958 - acc: 0.5391
 192/5677 [>.............................] - ETA: 14:22 - loss: 0.6967 - acc: 0.5365
 256/5677 [>.............................] - ETA: 14:29 - loss: 0.6927 - acc: 0.5352
 320/5677 [>.............................] - ETA: 14:04 - loss: 0.6938 - acc: 0.5406
 384/5677 [=>............................] - ETA: 13:53 - loss: 0.6860 - acc: 0.5599
 448/5677 [=>............................] - ETA: 13:45 - loss: 0.6911 - acc: 0.5513
 512/5677 [=>............................] - ETA: 13:31 - loss: 0.6897 - acc: 0.5508
 576/5677 [==>...........................] - ETA: 13:20 - loss: 0.6863 - acc: 0.5590
 640/5677 [==>...........................] - ETA: 13:08 - loss: 0.6871 - acc: 0.5547
 704/5677 [==>...........................] - ETA: 13:03 - loss: 0.6874 - acc: 0.5554
 768/5677 [===>..........................] - ETA: 12:52 - loss: 0.6874 - acc: 0.5534
 832/5677 [===>..........................] - ETA: 12:47 - loss: 0.6866 - acc: 0.5481
 896/5677 [===>..........................] - ETA: 12:40 - loss: 0.6893 - acc: 0.5469
 960/5677 [====>.........................] - ETA: 12:31 - loss: 0.6908 - acc: 0.5458
1024/5677 [====>.........................] - ETA: 12:26 - loss: 0.6885 - acc: 0.5498
1088/5677 [====>.........................] - ETA: 12:19 - loss: 0.6904 - acc: 0.5478
1152/5677 [=====>........................] - ETA: 12:09 - loss: 0.6903 - acc: 0.5486
1216/5677 [=====>........................] - ETA: 12:01 - loss: 0.6874 - acc: 0.5584
1280/5677 [=====>........................] - ETA: 11:51 - loss: 0.6850 - acc: 0.5641
1344/5677 [======>.......................] - ETA: 11:40 - loss: 0.6850 - acc: 0.5670
1408/5677 [======>.......................] - ETA: 11:29 - loss: 0.6846 - acc: 0.5675
1472/5677 [======>.......................] - ETA: 11:17 - loss: 0.6864 - acc: 0.5645
1536/5677 [=======>......................] - ETA: 11:06 - loss: 0.6853 - acc: 0.5677
1600/5677 [=======>......................] - ETA: 10:55 - loss: 0.6838 - acc: 0.5706
1664/5677 [=======>......................] - ETA: 10:43 - loss: 0.6834 - acc: 0.5691
1728/5677 [========>.....................] - ETA: 10:32 - loss: 0.6845 - acc: 0.5660
1792/5677 [========>.....................] - ETA: 10:19 - loss: 0.6833 - acc: 0.5664
1856/5677 [========>.....................] - ETA: 10:06 - loss: 0.6830 - acc: 0.5673
1920/5677 [=========>....................] - ETA: 9:55 - loss: 0.6830 - acc: 0.5646 
1984/5677 [=========>....................] - ETA: 9:43 - loss: 0.6831 - acc: 0.5655
2048/5677 [=========>....................] - ETA: 9:31 - loss: 0.6828 - acc: 0.5664
2112/5677 [==========>...................] - ETA: 9:18 - loss: 0.6825 - acc: 0.5677
2176/5677 [==========>...................] - ETA: 9:04 - loss: 0.6823 - acc: 0.5689
2240/5677 [==========>...................] - ETA: 8:51 - loss: 0.6828 - acc: 0.5670
2304/5677 [===========>..................] - ETA: 8:38 - loss: 0.6834 - acc: 0.5664
2368/5677 [===========>..................] - ETA: 8:26 - loss: 0.6840 - acc: 0.5671
2432/5677 [===========>..................] - ETA: 8:14 - loss: 0.6841 - acc: 0.5662
2496/5677 [============>.................] - ETA: 8:02 - loss: 0.6845 - acc: 0.5657
2560/5677 [============>.................] - ETA: 7:51 - loss: 0.6849 - acc: 0.5652
2624/5677 [============>.................] - ETA: 7:39 - loss: 0.6852 - acc: 0.5629
2688/5677 [=============>................] - ETA: 7:27 - loss: 0.6862 - acc: 0.5606
2752/5677 [=============>................] - ETA: 7:16 - loss: 0.6856 - acc: 0.5618
2816/5677 [=============>................] - ETA: 7:04 - loss: 0.6847 - acc: 0.5632
2880/5677 [==============>...............] - ETA: 6:53 - loss: 0.6849 - acc: 0.5635
2944/5677 [==============>...............] - ETA: 6:43 - loss: 0.6852 - acc: 0.5608
3008/5677 [==============>...............] - ETA: 6:33 - loss: 0.6861 - acc: 0.5585
3072/5677 [===============>..............] - ETA: 6:22 - loss: 0.6855 - acc: 0.5609
3136/5677 [===============>..............] - ETA: 6:12 - loss: 0.6854 - acc: 0.5609
3200/5677 [===============>..............] - ETA: 6:02 - loss: 0.6853 - acc: 0.5613
3264/5677 [================>.............] - ETA: 5:52 - loss: 0.6849 - acc: 0.5613
3328/5677 [================>.............] - ETA: 5:42 - loss: 0.6854 - acc: 0.5598
3392/5677 [================>.............] - ETA: 5:32 - loss: 0.6854 - acc: 0.5601
3456/5677 [=================>............] - ETA: 5:22 - loss: 0.6854 - acc: 0.5611
3520/5677 [=================>............] - ETA: 5:12 - loss: 0.6854 - acc: 0.5605
3584/5677 [=================>............] - ETA: 5:02 - loss: 0.6853 - acc: 0.5608
3648/5677 [==================>...........] - ETA: 4:53 - loss: 0.6847 - acc: 0.5625
3712/5677 [==================>...........] - ETA: 4:43 - loss: 0.6852 - acc: 0.5612
3776/5677 [==================>...........] - ETA: 4:34 - loss: 0.6854 - acc: 0.5596
3840/5677 [===================>..........] - ETA: 4:24 - loss: 0.6853 - acc: 0.5591
3904/5677 [===================>..........] - ETA: 4:15 - loss: 0.6856 - acc: 0.5574
3968/5677 [===================>..........] - ETA: 4:05 - loss: 0.6850 - acc: 0.5585
4032/5677 [====================>.........] - ETA: 3:56 - loss: 0.6849 - acc: 0.5590
4096/5677 [====================>.........] - ETA: 3:46 - loss: 0.6849 - acc: 0.5591
4160/5677 [====================>.........] - ETA: 3:37 - loss: 0.6852 - acc: 0.5582
4224/5677 [=====================>........] - ETA: 3:27 - loss: 0.6854 - acc: 0.5573
4288/5677 [=====================>........] - ETA: 3:18 - loss: 0.6854 - acc: 0.5571
4352/5677 [=====================>........] - ETA: 3:08 - loss: 0.6854 - acc: 0.5568
4416/5677 [======================>.......] - ETA: 2:59 - loss: 0.6849 - acc: 0.5573
4480/5677 [======================>.......] - ETA: 2:50 - loss: 0.6853 - acc: 0.5556
4544/5677 [=======================>......] - ETA: 2:40 - loss: 0.6851 - acc: 0.5566
4608/5677 [=======================>......] - ETA: 2:31 - loss: 0.6851 - acc: 0.5564
4672/5677 [=======================>......] - ETA: 2:22 - loss: 0.6848 - acc: 0.5571
4736/5677 [========================>.....] - ETA: 2:13 - loss: 0.6848 - acc: 0.5566
4800/5677 [========================>.....] - ETA: 2:04 - loss: 0.6848 - acc: 0.5573
4864/5677 [========================>.....] - ETA: 1:54 - loss: 0.6848 - acc: 0.5576
4928/5677 [=========================>....] - ETA: 1:45 - loss: 0.6843 - acc: 0.5582
4992/5677 [=========================>....] - ETA: 1:36 - loss: 0.6843 - acc: 0.5587
5056/5677 [=========================>....] - ETA: 1:27 - loss: 0.6844 - acc: 0.5585
5120/5677 [==========================>...] - ETA: 1:18 - loss: 0.6839 - acc: 0.5592
5184/5677 [==========================>...] - ETA: 1:09 - loss: 0.6839 - acc: 0.5592
5248/5677 [==========================>...] - ETA: 1:00 - loss: 0.6834 - acc: 0.5606
5312/5677 [===========================>..] - ETA: 51s - loss: 0.6826 - acc: 0.5623 
5376/5677 [===========================>..] - ETA: 42s - loss: 0.6826 - acc: 0.5627
5440/5677 [===========================>..] - ETA: 33s - loss: 0.6824 - acc: 0.5634
5504/5677 [============================>.] - ETA: 24s - loss: 0.6823 - acc: 0.5630
5568/5677 [============================>.] - ETA: 15s - loss: 0.6820 - acc: 0.5639
5632/5677 [============================>.] - ETA: 6s - loss: 0.6814 - acc: 0.5655 
5677/5677 [==============================] - 822s 145ms/step - loss: 0.6811 - acc: 0.5663 - val_loss: 0.7167 - val_acc: 0.5325

Epoch 00005: val_acc did not improve from 0.55784
Epoch 6/10

  64/5677 [..............................] - ETA: 12:03 - loss: 0.6942 - acc: 0.5312
 128/5677 [..............................] - ETA: 12:16 - loss: 0.6635 - acc: 0.5938
 192/5677 [>.............................] - ETA: 11:50 - loss: 0.6685 - acc: 0.5885
 256/5677 [>.............................] - ETA: 11:35 - loss: 0.6826 - acc: 0.5703
 320/5677 [>.............................] - ETA: 11:30 - loss: 0.6854 - acc: 0.5687
 384/5677 [=>............................] - ETA: 11:20 - loss: 0.6896 - acc: 0.5625
 448/5677 [=>............................] - ETA: 11:04 - loss: 0.6900 - acc: 0.5625
 512/5677 [=>............................] - ETA: 10:57 - loss: 0.6857 - acc: 0.5684
 576/5677 [==>...........................] - ETA: 10:54 - loss: 0.6847 - acc: 0.5677
 640/5677 [==>...........................] - ETA: 10:39 - loss: 0.6806 - acc: 0.5781
 704/5677 [==>...........................] - ETA: 10:38 - loss: 0.6826 - acc: 0.5753
 768/5677 [===>..........................] - ETA: 10:38 - loss: 0.6838 - acc: 0.5729
 832/5677 [===>..........................] - ETA: 10:32 - loss: 0.6834 - acc: 0.5733
 896/5677 [===>..........................] - ETA: 10:25 - loss: 0.6862 - acc: 0.5647
 960/5677 [====>.........................] - ETA: 10:20 - loss: 0.6848 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 10:17 - loss: 0.6863 - acc: 0.5645
1088/5677 [====>.........................] - ETA: 10:13 - loss: 0.6848 - acc: 0.5699
1152/5677 [=====>........................] - ETA: 10:12 - loss: 0.6870 - acc: 0.5616
1216/5677 [=====>........................] - ETA: 10:08 - loss: 0.6871 - acc: 0.5633
1280/5677 [=====>........................] - ETA: 10:02 - loss: 0.6885 - acc: 0.5578
1344/5677 [======>.......................] - ETA: 9:57 - loss: 0.6862 - acc: 0.5618 
1408/5677 [======>.......................] - ETA: 9:54 - loss: 0.6860 - acc: 0.5604
1472/5677 [======>.......................] - ETA: 9:47 - loss: 0.6866 - acc: 0.5598
1536/5677 [=======>......................] - ETA: 9:41 - loss: 0.6862 - acc: 0.5618
1600/5677 [=======>......................] - ETA: 9:36 - loss: 0.6858 - acc: 0.5613
1664/5677 [=======>......................] - ETA: 9:29 - loss: 0.6868 - acc: 0.5583
1728/5677 [========>.....................] - ETA: 9:18 - loss: 0.6864 - acc: 0.5556
1792/5677 [========>.....................] - ETA: 9:11 - loss: 0.6860 - acc: 0.5575
1856/5677 [========>.....................] - ETA: 9:03 - loss: 0.6861 - acc: 0.5577
1920/5677 [=========>....................] - ETA: 8:52 - loss: 0.6848 - acc: 0.5615
1984/5677 [=========>....................] - ETA: 8:42 - loss: 0.6828 - acc: 0.5635
2048/5677 [=========>....................] - ETA: 8:33 - loss: 0.6821 - acc: 0.5649
2112/5677 [==========>...................] - ETA: 8:25 - loss: 0.6810 - acc: 0.5682
2176/5677 [==========>...................] - ETA: 8:18 - loss: 0.6791 - acc: 0.5712
2240/5677 [==========>...................] - ETA: 8:09 - loss: 0.6777 - acc: 0.5728
2304/5677 [===========>..................] - ETA: 8:00 - loss: 0.6774 - acc: 0.5729
2368/5677 [===========>..................] - ETA: 7:49 - loss: 0.6780 - acc: 0.5714
2432/5677 [===========>..................] - ETA: 7:40 - loss: 0.6774 - acc: 0.5715
2496/5677 [============>.................] - ETA: 7:31 - loss: 0.6773 - acc: 0.5717
2560/5677 [============>.................] - ETA: 7:21 - loss: 0.6770 - acc: 0.5730
2624/5677 [============>.................] - ETA: 7:11 - loss: 0.6774 - acc: 0.5732
2688/5677 [=============>................] - ETA: 7:02 - loss: 0.6773 - acc: 0.5733
2752/5677 [=============>................] - ETA: 6:53 - loss: 0.6763 - acc: 0.5752
2816/5677 [=============>................] - ETA: 6:46 - loss: 0.6773 - acc: 0.5728
2880/5677 [==============>...............] - ETA: 6:37 - loss: 0.6769 - acc: 0.5733
2944/5677 [==============>...............] - ETA: 6:26 - loss: 0.6782 - acc: 0.5710
3008/5677 [==============>...............] - ETA: 6:17 - loss: 0.6788 - acc: 0.5691
3072/5677 [===============>..............] - ETA: 6:07 - loss: 0.6801 - acc: 0.5680
3136/5677 [===============>..............] - ETA: 5:58 - loss: 0.6814 - acc: 0.5654
3200/5677 [===============>..............] - ETA: 5:49 - loss: 0.6809 - acc: 0.5669
3264/5677 [================>.............] - ETA: 5:41 - loss: 0.6811 - acc: 0.5668
3328/5677 [================>.............] - ETA: 5:33 - loss: 0.6816 - acc: 0.5658
3392/5677 [================>.............] - ETA: 5:25 - loss: 0.6814 - acc: 0.5660
3456/5677 [=================>............] - ETA: 5:15 - loss: 0.6813 - acc: 0.5654
3520/5677 [=================>............] - ETA: 5:06 - loss: 0.6810 - acc: 0.5668
3584/5677 [=================>............] - ETA: 4:57 - loss: 0.6808 - acc: 0.5667
3648/5677 [==================>...........] - ETA: 4:47 - loss: 0.6810 - acc: 0.5658
3712/5677 [==================>...........] - ETA: 4:37 - loss: 0.6807 - acc: 0.5663
3776/5677 [==================>...........] - ETA: 4:28 - loss: 0.6808 - acc: 0.5665
3840/5677 [===================>..........] - ETA: 4:19 - loss: 0.6814 - acc: 0.5651
3904/5677 [===================>..........] - ETA: 4:10 - loss: 0.6810 - acc: 0.5658
3968/5677 [===================>..........] - ETA: 4:01 - loss: 0.6803 - acc: 0.5683
4032/5677 [====================>.........] - ETA: 3:52 - loss: 0.6802 - acc: 0.5689
4096/5677 [====================>.........] - ETA: 3:43 - loss: 0.6795 - acc: 0.5701
4160/5677 [====================>.........] - ETA: 3:34 - loss: 0.6798 - acc: 0.5697
4224/5677 [=====================>........] - ETA: 3:25 - loss: 0.6793 - acc: 0.5698
4288/5677 [=====================>........] - ETA: 3:15 - loss: 0.6794 - acc: 0.5704
4352/5677 [=====================>........] - ETA: 3:07 - loss: 0.6793 - acc: 0.5705
4416/5677 [======================>.......] - ETA: 2:58 - loss: 0.6794 - acc: 0.5707
4480/5677 [======================>.......] - ETA: 2:49 - loss: 0.6792 - acc: 0.5703
4544/5677 [=======================>......] - ETA: 2:41 - loss: 0.6791 - acc: 0.5704
4608/5677 [=======================>......] - ETA: 2:32 - loss: 0.6791 - acc: 0.5701
4672/5677 [=======================>......] - ETA: 2:23 - loss: 0.6792 - acc: 0.5704
4736/5677 [========================>.....] - ETA: 2:14 - loss: 0.6792 - acc: 0.5703
4800/5677 [========================>.....] - ETA: 2:05 - loss: 0.6790 - acc: 0.5706
4864/5677 [========================>.....] - ETA: 1:56 - loss: 0.6791 - acc: 0.5711
4928/5677 [=========================>....] - ETA: 1:47 - loss: 0.6787 - acc: 0.5722
4992/5677 [=========================>....] - ETA: 1:38 - loss: 0.6782 - acc: 0.5735
5056/5677 [=========================>....] - ETA: 1:29 - loss: 0.6779 - acc: 0.5740
5120/5677 [==========================>...] - ETA: 1:20 - loss: 0.6776 - acc: 0.5756
5184/5677 [==========================>...] - ETA: 1:11 - loss: 0.6775 - acc: 0.5764
5248/5677 [==========================>...] - ETA: 1:01 - loss: 0.6771 - acc: 0.5770
5312/5677 [===========================>..] - ETA: 52s - loss: 0.6772 - acc: 0.5768 
5376/5677 [===========================>..] - ETA: 43s - loss: 0.6768 - acc: 0.5774
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6763 - acc: 0.5778
5504/5677 [============================>.] - ETA: 24s - loss: 0.6767 - acc: 0.5778
5568/5677 [============================>.] - ETA: 15s - loss: 0.6761 - acc: 0.5783
5632/5677 [============================>.] - ETA: 6s - loss: 0.6762 - acc: 0.5788 
5677/5677 [==============================] - 847s 149ms/step - loss: 0.6766 - acc: 0.5781 - val_loss: 0.6892 - val_acc: 0.5578

Epoch 00006: val_acc did not improve from 0.55784
Epoch 7/10

  64/5677 [..............................] - ETA: 12:16 - loss: 0.6822 - acc: 0.5781
 128/5677 [..............................] - ETA: 12:45 - loss: 0.6802 - acc: 0.5859
 192/5677 [>.............................] - ETA: 13:00 - loss: 0.6935 - acc: 0.5781
 256/5677 [>.............................] - ETA: 12:59 - loss: 0.7072 - acc: 0.5547
 320/5677 [>.............................] - ETA: 12:42 - loss: 0.6946 - acc: 0.5687
 384/5677 [=>............................] - ETA: 12:33 - loss: 0.6996 - acc: 0.5547
 448/5677 [=>............................] - ETA: 12:23 - loss: 0.6979 - acc: 0.5536
 512/5677 [=>............................] - ETA: 12:11 - loss: 0.6907 - acc: 0.5645
 576/5677 [==>...........................] - ETA: 11:52 - loss: 0.6886 - acc: 0.5642
 640/5677 [==>...........................] - ETA: 11:37 - loss: 0.6881 - acc: 0.5594
 704/5677 [==>...........................] - ETA: 11:35 - loss: 0.6878 - acc: 0.5611
 768/5677 [===>..........................] - ETA: 11:32 - loss: 0.6856 - acc: 0.5625
 832/5677 [===>..........................] - ETA: 11:28 - loss: 0.6823 - acc: 0.5673
 896/5677 [===>..........................] - ETA: 11:23 - loss: 0.6840 - acc: 0.5614
 960/5677 [====>.........................] - ETA: 11:11 - loss: 0.6826 - acc: 0.5625
1024/5677 [====>.........................] - ETA: 10:58 - loss: 0.6802 - acc: 0.5674
1088/5677 [====>.........................] - ETA: 10:43 - loss: 0.6793 - acc: 0.5717
1152/5677 [=====>........................] - ETA: 10:34 - loss: 0.6799 - acc: 0.5720
1216/5677 [=====>........................] - ETA: 10:23 - loss: 0.6801 - acc: 0.5724
1280/5677 [=====>........................] - ETA: 10:13 - loss: 0.6802 - acc: 0.5719
1344/5677 [======>.......................] - ETA: 10:07 - loss: 0.6802 - acc: 0.5744
1408/5677 [======>.......................] - ETA: 10:04 - loss: 0.6803 - acc: 0.5732
1472/5677 [======>.......................] - ETA: 9:56 - loss: 0.6808 - acc: 0.5720 
1536/5677 [=======>......................] - ETA: 9:48 - loss: 0.6809 - acc: 0.5697
1600/5677 [=======>......................] - ETA: 9:41 - loss: 0.6812 - acc: 0.5669
1664/5677 [=======>......................] - ETA: 9:33 - loss: 0.6828 - acc: 0.5637
1728/5677 [========>.....................] - ETA: 9:25 - loss: 0.6829 - acc: 0.5648
1792/5677 [========>.....................] - ETA: 9:18 - loss: 0.6816 - acc: 0.5686
1856/5677 [========>.....................] - ETA: 9:09 - loss: 0.6819 - acc: 0.5673
1920/5677 [=========>....................] - ETA: 9:01 - loss: 0.6811 - acc: 0.5682
1984/5677 [=========>....................] - ETA: 8:53 - loss: 0.6810 - acc: 0.5696
2048/5677 [=========>....................] - ETA: 8:44 - loss: 0.6817 - acc: 0.5703
2112/5677 [==========>...................] - ETA: 8:35 - loss: 0.6813 - acc: 0.5682
2176/5677 [==========>...................] - ETA: 8:26 - loss: 0.6826 - acc: 0.5662
2240/5677 [==========>...................] - ETA: 8:18 - loss: 0.6821 - acc: 0.5665
2304/5677 [===========>..................] - ETA: 8:09 - loss: 0.6824 - acc: 0.5660
2368/5677 [===========>..................] - ETA: 8:01 - loss: 0.6826 - acc: 0.5646
2432/5677 [===========>..................] - ETA: 7:53 - loss: 0.6831 - acc: 0.5646
2496/5677 [============>.................] - ETA: 7:45 - loss: 0.6831 - acc: 0.5649
2560/5677 [============>.................] - ETA: 7:36 - loss: 0.6820 - acc: 0.5660
2624/5677 [============>.................] - ETA: 7:25 - loss: 0.6816 - acc: 0.5663
2688/5677 [=============>................] - ETA: 7:16 - loss: 0.6815 - acc: 0.5666
2752/5677 [=============>................] - ETA: 7:07 - loss: 0.6812 - acc: 0.5683
2816/5677 [=============>................] - ETA: 6:58 - loss: 0.6810 - acc: 0.5707
2880/5677 [==============>...............] - ETA: 6:49 - loss: 0.6808 - acc: 0.5708
2944/5677 [==============>...............] - ETA: 6:40 - loss: 0.6805 - acc: 0.5724
3008/5677 [==============>...............] - ETA: 6:32 - loss: 0.6812 - acc: 0.5698
3072/5677 [===============>..............] - ETA: 6:22 - loss: 0.6819 - acc: 0.5690
3136/5677 [===============>..............] - ETA: 6:13 - loss: 0.6808 - acc: 0.5721
3200/5677 [===============>..............] - ETA: 6:03 - loss: 0.6805 - acc: 0.5731
3264/5677 [================>.............] - ETA: 5:55 - loss: 0.6810 - acc: 0.5711
3328/5677 [================>.............] - ETA: 5:46 - loss: 0.6797 - acc: 0.5745
3392/5677 [================>.............] - ETA: 5:37 - loss: 0.6803 - acc: 0.5728
3456/5677 [=================>............] - ETA: 5:27 - loss: 0.6806 - acc: 0.5723
3520/5677 [=================>............] - ETA: 5:17 - loss: 0.6811 - acc: 0.5713
3584/5677 [=================>............] - ETA: 5:08 - loss: 0.6807 - acc: 0.5725
3648/5677 [==================>...........] - ETA: 4:58 - loss: 0.6807 - acc: 0.5715
3712/5677 [==================>...........] - ETA: 4:49 - loss: 0.6800 - acc: 0.5738
3776/5677 [==================>...........] - ETA: 4:40 - loss: 0.6804 - acc: 0.5728
3840/5677 [===================>..........] - ETA: 4:30 - loss: 0.6800 - acc: 0.5742
3904/5677 [===================>..........] - ETA: 4:21 - loss: 0.6798 - acc: 0.5735
3968/5677 [===================>..........] - ETA: 4:12 - loss: 0.6790 - acc: 0.5751
4032/5677 [====================>.........] - ETA: 4:02 - loss: 0.6783 - acc: 0.5754
4096/5677 [====================>.........] - ETA: 3:52 - loss: 0.6779 - acc: 0.5759
4160/5677 [====================>.........] - ETA: 3:43 - loss: 0.6781 - acc: 0.5755
4224/5677 [=====================>........] - ETA: 3:33 - loss: 0.6782 - acc: 0.5765
4288/5677 [=====================>........] - ETA: 3:23 - loss: 0.6778 - acc: 0.5774
4352/5677 [=====================>........] - ETA: 3:13 - loss: 0.6779 - acc: 0.5774
4416/5677 [======================>.......] - ETA: 3:04 - loss: 0.6784 - acc: 0.5759
4480/5677 [======================>.......] - ETA: 2:54 - loss: 0.6782 - acc: 0.5766
4544/5677 [=======================>......] - ETA: 2:45 - loss: 0.6787 - acc: 0.5757
4608/5677 [=======================>......] - ETA: 2:35 - loss: 0.6790 - acc: 0.5760
4672/5677 [=======================>......] - ETA: 2:26 - loss: 0.6784 - acc: 0.5771
4736/5677 [========================>.....] - ETA: 2:17 - loss: 0.6784 - acc: 0.5773
4800/5677 [========================>.....] - ETA: 2:07 - loss: 0.6782 - acc: 0.5783
4864/5677 [========================>.....] - ETA: 1:58 - loss: 0.6777 - acc: 0.5798
4928/5677 [=========================>....] - ETA: 1:49 - loss: 0.6777 - acc: 0.5795
4992/5677 [=========================>....] - ETA: 1:39 - loss: 0.6778 - acc: 0.5791
5056/5677 [=========================>....] - ETA: 1:30 - loss: 0.6776 - acc: 0.5789
5120/5677 [==========================>...] - ETA: 1:21 - loss: 0.6772 - acc: 0.5795
5184/5677 [==========================>...] - ETA: 1:11 - loss: 0.6772 - acc: 0.5787
5248/5677 [==========================>...] - ETA: 1:02 - loss: 0.6775 - acc: 0.5789
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6768 - acc: 0.5806 
5376/5677 [===========================>..] - ETA: 43s - loss: 0.6767 - acc: 0.5817
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6772 - acc: 0.5805
5504/5677 [============================>.] - ETA: 25s - loss: 0.6769 - acc: 0.5807
5568/5677 [============================>.] - ETA: 15s - loss: 0.6768 - acc: 0.5810
5632/5677 [============================>.] - ETA: 6s - loss: 0.6766 - acc: 0.5817 
5677/5677 [==============================] - 856s 151ms/step - loss: 0.6759 - acc: 0.5832 - val_loss: 0.7022 - val_acc: 0.5563

Epoch 00007: val_acc did not improve from 0.55784
Epoch 8/10

  64/5677 [..............................] - ETA: 14:07 - loss: 0.6959 - acc: 0.5625
 128/5677 [..............................] - ETA: 13:45 - loss: 0.6867 - acc: 0.5703
 192/5677 [>.............................] - ETA: 13:35 - loss: 0.6893 - acc: 0.5625
 256/5677 [>.............................] - ETA: 13:32 - loss: 0.6872 - acc: 0.5547
 320/5677 [>.............................] - ETA: 13:06 - loss: 0.6824 - acc: 0.5563
 384/5677 [=>............................] - ETA: 13:07 - loss: 0.6675 - acc: 0.5885
 448/5677 [=>............................] - ETA: 13:00 - loss: 0.6687 - acc: 0.5759
 512/5677 [=>............................] - ETA: 12:52 - loss: 0.6687 - acc: 0.5781
 576/5677 [==>...........................] - ETA: 12:37 - loss: 0.6645 - acc: 0.5885
 640/5677 [==>...........................] - ETA: 12:29 - loss: 0.6704 - acc: 0.5750
 704/5677 [==>...........................] - ETA: 12:16 - loss: 0.6785 - acc: 0.5653
 768/5677 [===>..........................] - ETA: 12:09 - loss: 0.6784 - acc: 0.5664
 832/5677 [===>..........................] - ETA: 12:00 - loss: 0.6760 - acc: 0.5721
 896/5677 [===>..........................] - ETA: 11:49 - loss: 0.6774 - acc: 0.5725
 960/5677 [====>.........................] - ETA: 11:37 - loss: 0.6773 - acc: 0.5719
1024/5677 [====>.........................] - ETA: 11:28 - loss: 0.6756 - acc: 0.5771
1088/5677 [====>.........................] - ETA: 11:21 - loss: 0.6784 - acc: 0.5689
1152/5677 [=====>........................] - ETA: 11:12 - loss: 0.6817 - acc: 0.5668
1216/5677 [=====>........................] - ETA: 11:04 - loss: 0.6802 - acc: 0.5699
1280/5677 [=====>........................] - ETA: 10:53 - loss: 0.6792 - acc: 0.5734
1344/5677 [======>.......................] - ETA: 10:42 - loss: 0.6777 - acc: 0.5789
1408/5677 [======>.......................] - ETA: 10:34 - loss: 0.6767 - acc: 0.5810
1472/5677 [======>.......................] - ETA: 10:24 - loss: 0.6780 - acc: 0.5802
1536/5677 [=======>......................] - ETA: 10:14 - loss: 0.6780 - acc: 0.5814
1600/5677 [=======>......................] - ETA: 10:03 - loss: 0.6798 - acc: 0.5769
1664/5677 [=======>......................] - ETA: 9:51 - loss: 0.6786 - acc: 0.5775 
1728/5677 [========>.....................] - ETA: 9:43 - loss: 0.6783 - acc: 0.5787
1792/5677 [========>.....................] - ETA: 9:34 - loss: 0.6773 - acc: 0.5787
1856/5677 [========>.....................] - ETA: 9:24 - loss: 0.6766 - acc: 0.5803
1920/5677 [=========>....................] - ETA: 9:15 - loss: 0.6759 - acc: 0.5807
1984/5677 [=========>....................] - ETA: 9:04 - loss: 0.6756 - acc: 0.5827
2048/5677 [=========>....................] - ETA: 8:55 - loss: 0.6766 - acc: 0.5806
2112/5677 [==========>...................] - ETA: 8:46 - loss: 0.6763 - acc: 0.5824
2176/5677 [==========>...................] - ETA: 8:36 - loss: 0.6755 - acc: 0.5836
2240/5677 [==========>...................] - ETA: 8:27 - loss: 0.6760 - acc: 0.5826
2304/5677 [===========>..................] - ETA: 8:17 - loss: 0.6767 - acc: 0.5812
2368/5677 [===========>..................] - ETA: 8:08 - loss: 0.6767 - acc: 0.5815
2432/5677 [===========>..................] - ETA: 7:58 - loss: 0.6768 - acc: 0.5810
2496/5677 [============>.................] - ETA: 7:48 - loss: 0.6767 - acc: 0.5805
2560/5677 [============>.................] - ETA: 7:39 - loss: 0.6769 - acc: 0.5820
2624/5677 [============>.................] - ETA: 7:29 - loss: 0.6769 - acc: 0.5846
2688/5677 [=============>................] - ETA: 7:19 - loss: 0.6770 - acc: 0.5856
2752/5677 [=============>................] - ETA: 7:09 - loss: 0.6766 - acc: 0.5858
2816/5677 [=============>................] - ETA: 7:00 - loss: 0.6774 - acc: 0.5849
2880/5677 [==============>...............] - ETA: 6:51 - loss: 0.6775 - acc: 0.5844
2944/5677 [==============>...............] - ETA: 6:42 - loss: 0.6771 - acc: 0.5849
3008/5677 [==============>...............] - ETA: 6:32 - loss: 0.6763 - acc: 0.5864
3072/5677 [===============>..............] - ETA: 6:23 - loss: 0.6762 - acc: 0.5853
3136/5677 [===============>..............] - ETA: 6:14 - loss: 0.6757 - acc: 0.5874
3200/5677 [===============>..............] - ETA: 6:05 - loss: 0.6755 - acc: 0.5884
3264/5677 [================>.............] - ETA: 5:57 - loss: 0.6755 - acc: 0.5885
3328/5677 [================>.............] - ETA: 5:46 - loss: 0.6748 - acc: 0.5895
3392/5677 [================>.............] - ETA: 5:38 - loss: 0.6752 - acc: 0.5896
3456/5677 [=================>............] - ETA: 5:28 - loss: 0.6757 - acc: 0.5883
3520/5677 [=================>............] - ETA: 5:18 - loss: 0.6754 - acc: 0.5884
3584/5677 [=================>............] - ETA: 5:09 - loss: 0.6752 - acc: 0.5882
3648/5677 [==================>...........] - ETA: 4:59 - loss: 0.6744 - acc: 0.5896
3712/5677 [==================>...........] - ETA: 4:50 - loss: 0.6736 - acc: 0.5913
3776/5677 [==================>...........] - ETA: 4:40 - loss: 0.6740 - acc: 0.5906
3840/5677 [===================>..........] - ETA: 4:30 - loss: 0.6738 - acc: 0.5919
3904/5677 [===================>..........] - ETA: 4:21 - loss: 0.6739 - acc: 0.5914
3968/5677 [===================>..........] - ETA: 4:11 - loss: 0.6741 - acc: 0.5905
4032/5677 [====================>.........] - ETA: 4:02 - loss: 0.6745 - acc: 0.5888
4096/5677 [====================>.........] - ETA: 3:53 - loss: 0.6744 - acc: 0.5881
4160/5677 [====================>.........] - ETA: 3:43 - loss: 0.6740 - acc: 0.5885
4224/5677 [=====================>........] - ETA: 3:33 - loss: 0.6748 - acc: 0.5866
4288/5677 [=====================>........] - ETA: 3:24 - loss: 0.6758 - acc: 0.5849
4352/5677 [=====================>........] - ETA: 3:15 - loss: 0.6754 - acc: 0.5855
4416/5677 [======================>.......] - ETA: 3:05 - loss: 0.6755 - acc: 0.5856
4480/5677 [======================>.......] - ETA: 2:56 - loss: 0.6755 - acc: 0.5859
4544/5677 [=======================>......] - ETA: 2:46 - loss: 0.6752 - acc: 0.5860
4608/5677 [=======================>......] - ETA: 2:37 - loss: 0.6748 - acc: 0.5870
4672/5677 [=======================>......] - ETA: 2:28 - loss: 0.6754 - acc: 0.5852
4736/5677 [========================>.....] - ETA: 2:18 - loss: 0.6755 - acc: 0.5853
4800/5677 [========================>.....] - ETA: 2:09 - loss: 0.6761 - acc: 0.5850
4864/5677 [========================>.....] - ETA: 1:59 - loss: 0.6760 - acc: 0.5853
4928/5677 [=========================>....] - ETA: 1:50 - loss: 0.6754 - acc: 0.5858
4992/5677 [=========================>....] - ETA: 1:40 - loss: 0.6749 - acc: 0.5867
5056/5677 [=========================>....] - ETA: 1:31 - loss: 0.6749 - acc: 0.5858
5120/5677 [==========================>...] - ETA: 1:22 - loss: 0.6746 - acc: 0.5867
5184/5677 [==========================>...] - ETA: 1:12 - loss: 0.6753 - acc: 0.5853
5248/5677 [==========================>...] - ETA: 1:03 - loss: 0.6750 - acc: 0.5863
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6749 - acc: 0.5866 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6747 - acc: 0.5876
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6746 - acc: 0.5869
5504/5677 [============================>.] - ETA: 25s - loss: 0.6741 - acc: 0.5879
5568/5677 [============================>.] - ETA: 16s - loss: 0.6737 - acc: 0.5885
5632/5677 [============================>.] - ETA: 6s - loss: 0.6741 - acc: 0.5886 
5677/5677 [==============================] - 870s 153ms/step - loss: 0.6743 - acc: 0.5880 - val_loss: 0.7161 - val_acc: 0.5325

Epoch 00008: val_acc did not improve from 0.55784
Epoch 9/10

  64/5677 [..............................] - ETA: 13:24 - loss: 0.6900 - acc: 0.5156
 128/5677 [..............................] - ETA: 13:34 - loss: 0.6593 - acc: 0.6094
 192/5677 [>.............................] - ETA: 13:23 - loss: 0.6611 - acc: 0.5990
 256/5677 [>.............................] - ETA: 12:59 - loss: 0.6623 - acc: 0.6055
 320/5677 [>.............................] - ETA: 12:56 - loss: 0.6663 - acc: 0.5938
 384/5677 [=>............................] - ETA: 12:53 - loss: 0.6724 - acc: 0.5755
 448/5677 [=>............................] - ETA: 12:45 - loss: 0.6706 - acc: 0.5670
 512/5677 [=>............................] - ETA: 12:39 - loss: 0.6715 - acc: 0.5742
 576/5677 [==>...........................] - ETA: 12:34 - loss: 0.6746 - acc: 0.5712
 640/5677 [==>...........................] - ETA: 12:28 - loss: 0.6710 - acc: 0.5766
 704/5677 [==>...........................] - ETA: 12:10 - loss: 0.6736 - acc: 0.5795
 768/5677 [===>..........................] - ETA: 11:56 - loss: 0.6733 - acc: 0.5833
 832/5677 [===>..........................] - ETA: 11:49 - loss: 0.6719 - acc: 0.5829
 896/5677 [===>..........................] - ETA: 11:42 - loss: 0.6737 - acc: 0.5770
 960/5677 [====>.........................] - ETA: 11:34 - loss: 0.6733 - acc: 0.5823
1024/5677 [====>.........................] - ETA: 11:23 - loss: 0.6705 - acc: 0.5898
1088/5677 [====>.........................] - ETA: 11:15 - loss: 0.6696 - acc: 0.5947
1152/5677 [=====>........................] - ETA: 11:08 - loss: 0.6699 - acc: 0.5946
1216/5677 [=====>........................] - ETA: 11:03 - loss: 0.6686 - acc: 0.5987
1280/5677 [=====>........................] - ETA: 10:54 - loss: 0.6685 - acc: 0.6016
1344/5677 [======>.......................] - ETA: 10:43 - loss: 0.6690 - acc: 0.6012
1408/5677 [======>.......................] - ETA: 10:34 - loss: 0.6704 - acc: 0.5987
1472/5677 [======>.......................] - ETA: 10:20 - loss: 0.6710 - acc: 0.5971
1536/5677 [=======>......................] - ETA: 10:09 - loss: 0.6705 - acc: 0.5990
1600/5677 [=======>......................] - ETA: 9:55 - loss: 0.6710 - acc: 0.5975 
1664/5677 [=======>......................] - ETA: 9:45 - loss: 0.6686 - acc: 0.6022
1728/5677 [========>.....................] - ETA: 9:35 - loss: 0.6703 - acc: 0.6013
1792/5677 [========>.....................] - ETA: 9:28 - loss: 0.6688 - acc: 0.6032
1856/5677 [========>.....................] - ETA: 9:16 - loss: 0.6705 - acc: 0.5981
1920/5677 [=========>....................] - ETA: 9:04 - loss: 0.6715 - acc: 0.5964
1984/5677 [=========>....................] - ETA: 8:52 - loss: 0.6730 - acc: 0.5927
2048/5677 [=========>....................] - ETA: 8:41 - loss: 0.6752 - acc: 0.5894
2112/5677 [==========>...................] - ETA: 8:31 - loss: 0.6756 - acc: 0.5890
2176/5677 [==========>...................] - ETA: 8:22 - loss: 0.6746 - acc: 0.5905
2240/5677 [==========>...................] - ETA: 8:13 - loss: 0.6745 - acc: 0.5924
2304/5677 [===========>..................] - ETA: 8:03 - loss: 0.6731 - acc: 0.5933
2368/5677 [===========>..................] - ETA: 7:53 - loss: 0.6737 - acc: 0.5925
2432/5677 [===========>..................] - ETA: 7:43 - loss: 0.6725 - acc: 0.5954
2496/5677 [============>.................] - ETA: 7:34 - loss: 0.6733 - acc: 0.5933
2560/5677 [============>.................] - ETA: 7:25 - loss: 0.6735 - acc: 0.5922
2624/5677 [============>.................] - ETA: 7:16 - loss: 0.6734 - acc: 0.5915
2688/5677 [=============>................] - ETA: 7:06 - loss: 0.6734 - acc: 0.5911
2752/5677 [=============>................] - ETA: 6:58 - loss: 0.6738 - acc: 0.5894
2816/5677 [=============>................] - ETA: 6:49 - loss: 0.6736 - acc: 0.5891
2880/5677 [==============>...............] - ETA: 6:40 - loss: 0.6743 - acc: 0.5878
2944/5677 [==============>...............] - ETA: 6:31 - loss: 0.6749 - acc: 0.5870
3008/5677 [==============>...............] - ETA: 6:21 - loss: 0.6743 - acc: 0.5878
3072/5677 [===============>..............] - ETA: 6:11 - loss: 0.6740 - acc: 0.5879
3136/5677 [===============>..............] - ETA: 6:02 - loss: 0.6735 - acc: 0.5890
3200/5677 [===============>..............] - ETA: 5:53 - loss: 0.6737 - acc: 0.5887
3264/5677 [================>.............] - ETA: 5:44 - loss: 0.6733 - acc: 0.5895
3328/5677 [================>.............] - ETA: 5:35 - loss: 0.6734 - acc: 0.5895
3392/5677 [================>.............] - ETA: 5:26 - loss: 0.6732 - acc: 0.5884
3456/5677 [=================>............] - ETA: 5:17 - loss: 0.6725 - acc: 0.5911
3520/5677 [=================>............] - ETA: 5:08 - loss: 0.6724 - acc: 0.5915
3584/5677 [=================>............] - ETA: 5:00 - loss: 0.6720 - acc: 0.5918
3648/5677 [==================>...........] - ETA: 4:51 - loss: 0.6720 - acc: 0.5924
3712/5677 [==================>...........] - ETA: 4:43 - loss: 0.6725 - acc: 0.5900
3776/5677 [==================>...........] - ETA: 4:33 - loss: 0.6717 - acc: 0.5916
3840/5677 [===================>..........] - ETA: 4:24 - loss: 0.6715 - acc: 0.5911
3904/5677 [===================>..........] - ETA: 4:15 - loss: 0.6716 - acc: 0.5912
3968/5677 [===================>..........] - ETA: 4:06 - loss: 0.6720 - acc: 0.5907
4032/5677 [====================>.........] - ETA: 3:57 - loss: 0.6722 - acc: 0.5903
4096/5677 [====================>.........] - ETA: 3:47 - loss: 0.6721 - acc: 0.5901
4160/5677 [====================>.........] - ETA: 3:38 - loss: 0.6718 - acc: 0.5911
4224/5677 [=====================>........] - ETA: 3:28 - loss: 0.6711 - acc: 0.5933
4288/5677 [=====================>........] - ETA: 3:19 - loss: 0.6709 - acc: 0.5933
4352/5677 [=====================>........] - ETA: 3:10 - loss: 0.6716 - acc: 0.5931
4416/5677 [======================>.......] - ETA: 3:01 - loss: 0.6723 - acc: 0.5919
4480/5677 [======================>.......] - ETA: 2:52 - loss: 0.6720 - acc: 0.5931
4544/5677 [=======================>......] - ETA: 2:43 - loss: 0.6726 - acc: 0.5922
4608/5677 [=======================>......] - ETA: 2:34 - loss: 0.6730 - acc: 0.5914
4672/5677 [=======================>......] - ETA: 2:24 - loss: 0.6730 - acc: 0.5912
4736/5677 [========================>.....] - ETA: 2:15 - loss: 0.6734 - acc: 0.5902
4800/5677 [========================>.....] - ETA: 2:06 - loss: 0.6736 - acc: 0.5894
4864/5677 [========================>.....] - ETA: 1:57 - loss: 0.6739 - acc: 0.5882
4928/5677 [=========================>....] - ETA: 1:48 - loss: 0.6740 - acc: 0.5881
4992/5677 [=========================>....] - ETA: 1:39 - loss: 0.6743 - acc: 0.5865
5056/5677 [=========================>....] - ETA: 1:29 - loss: 0.6749 - acc: 0.5850
5120/5677 [==========================>...] - ETA: 1:20 - loss: 0.6745 - acc: 0.5859
5184/5677 [==========================>...] - ETA: 1:11 - loss: 0.6748 - acc: 0.5847
5248/5677 [==========================>...] - ETA: 1:02 - loss: 0.6752 - acc: 0.5838
5312/5677 [===========================>..] - ETA: 52s - loss: 0.6752 - acc: 0.5838 
5376/5677 [===========================>..] - ETA: 43s - loss: 0.6745 - acc: 0.5852
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6746 - acc: 0.5851
5504/5677 [============================>.] - ETA: 25s - loss: 0.6745 - acc: 0.5850
5568/5677 [============================>.] - ETA: 15s - loss: 0.6743 - acc: 0.5860
5632/5677 [============================>.] - ETA: 6s - loss: 0.6742 - acc: 0.5852 
5677/5677 [==============================] - 855s 151ms/step - loss: 0.6738 - acc: 0.5864 - val_loss: 0.7152 - val_acc: 0.5087

Epoch 00009: val_acc did not improve from 0.55784
Epoch 10/10

  64/5677 [..............................] - ETA: 13:53 - loss: 0.6284 - acc: 0.6875
 128/5677 [..............................] - ETA: 13:31 - loss: 0.6516 - acc: 0.6484
 192/5677 [>.............................] - ETA: 13:36 - loss: 0.6608 - acc: 0.5990
 256/5677 [>.............................] - ETA: 13:43 - loss: 0.6600 - acc: 0.6016
 320/5677 [>.............................] - ETA: 13:31 - loss: 0.6694 - acc: 0.5687
 384/5677 [=>............................] - ETA: 13:28 - loss: 0.6746 - acc: 0.5521
 448/5677 [=>............................] - ETA: 13:08 - loss: 0.6741 - acc: 0.5536
 512/5677 [=>............................] - ETA: 12:51 - loss: 0.6698 - acc: 0.5664
 576/5677 [==>...........................] - ETA: 12:27 - loss: 0.6698 - acc: 0.5677
 640/5677 [==>...........................] - ETA: 12:11 - loss: 0.6696 - acc: 0.5719
 704/5677 [==>...........................] - ETA: 11:58 - loss: 0.6678 - acc: 0.5824
 768/5677 [===>..........................] - ETA: 11:44 - loss: 0.6707 - acc: 0.5807
 832/5677 [===>..........................] - ETA: 11:34 - loss: 0.6681 - acc: 0.5853
 896/5677 [===>..........................] - ETA: 11:24 - loss: 0.6672 - acc: 0.5882
 960/5677 [====>.........................] - ETA: 11:14 - loss: 0.6661 - acc: 0.5875
1024/5677 [====>.........................] - ETA: 11:01 - loss: 0.6701 - acc: 0.5811
1088/5677 [====>.........................] - ETA: 10:51 - loss: 0.6672 - acc: 0.5836
1152/5677 [=====>........................] - ETA: 10:41 - loss: 0.6689 - acc: 0.5807
1216/5677 [=====>........................] - ETA: 10:31 - loss: 0.6680 - acc: 0.5814
1280/5677 [=====>........................] - ETA: 10:27 - loss: 0.6697 - acc: 0.5758
1344/5677 [======>.......................] - ETA: 10:18 - loss: 0.6713 - acc: 0.5751
1408/5677 [======>.......................] - ETA: 10:10 - loss: 0.6719 - acc: 0.5788
1472/5677 [======>.......................] - ETA: 10:02 - loss: 0.6729 - acc: 0.5774
1536/5677 [=======>......................] - ETA: 9:56 - loss: 0.6744 - acc: 0.5755 
1600/5677 [=======>......................] - ETA: 9:48 - loss: 0.6748 - acc: 0.5763
1664/5677 [=======>......................] - ETA: 9:41 - loss: 0.6743 - acc: 0.5763
1728/5677 [========>.....................] - ETA: 9:34 - loss: 0.6742 - acc: 0.5770
1792/5677 [========>.....................] - ETA: 9:27 - loss: 0.6738 - acc: 0.5765
1856/5677 [========>.....................] - ETA: 9:19 - loss: 0.6739 - acc: 0.5770
1920/5677 [=========>....................] - ETA: 9:12 - loss: 0.6740 - acc: 0.5766
1984/5677 [=========>....................] - ETA: 9:04 - loss: 0.6741 - acc: 0.5766
2048/5677 [=========>....................] - ETA: 8:52 - loss: 0.6747 - acc: 0.5762
2112/5677 [==========>...................] - ETA: 8:44 - loss: 0.6761 - acc: 0.5743
2176/5677 [==========>...................] - ETA: 8:34 - loss: 0.6766 - acc: 0.5726
2240/5677 [==========>...................] - ETA: 8:26 - loss: 0.6756 - acc: 0.5750
2304/5677 [===========>..................] - ETA: 8:17 - loss: 0.6756 - acc: 0.5742
2368/5677 [===========>..................] - ETA: 8:08 - loss: 0.6762 - acc: 0.5726
2432/5677 [===========>..................] - ETA: 7:59 - loss: 0.6761 - acc: 0.5736
2496/5677 [============>.................] - ETA: 7:50 - loss: 0.6755 - acc: 0.5729
2560/5677 [============>.................] - ETA: 7:40 - loss: 0.6758 - acc: 0.5711
2624/5677 [============>.................] - ETA: 7:30 - loss: 0.6761 - acc: 0.5697
2688/5677 [=============>................] - ETA: 7:21 - loss: 0.6758 - acc: 0.5711
2752/5677 [=============>................] - ETA: 7:11 - loss: 0.6754 - acc: 0.5712
2816/5677 [=============>................] - ETA: 7:01 - loss: 0.6752 - acc: 0.5707
2880/5677 [==============>...............] - ETA: 6:52 - loss: 0.6757 - acc: 0.5712
2944/5677 [==============>...............] - ETA: 6:42 - loss: 0.6748 - acc: 0.5744
3008/5677 [==============>...............] - ETA: 6:32 - loss: 0.6740 - acc: 0.5765
3072/5677 [===============>..............] - ETA: 6:22 - loss: 0.6743 - acc: 0.5762
3136/5677 [===============>..............] - ETA: 6:13 - loss: 0.6742 - acc: 0.5781
3200/5677 [===============>..............] - ETA: 6:04 - loss: 0.6738 - acc: 0.5803
3264/5677 [================>.............] - ETA: 5:55 - loss: 0.6727 - acc: 0.5827
3328/5677 [================>.............] - ETA: 5:46 - loss: 0.6721 - acc: 0.5835
3392/5677 [================>.............] - ETA: 5:36 - loss: 0.6717 - acc: 0.5846
3456/5677 [=================>............] - ETA: 5:27 - loss: 0.6718 - acc: 0.5839
3520/5677 [=================>............] - ETA: 5:18 - loss: 0.6708 - acc: 0.5861
3584/5677 [=================>............] - ETA: 5:09 - loss: 0.6714 - acc: 0.5851
3648/5677 [==================>...........] - ETA: 4:59 - loss: 0.6713 - acc: 0.5847
3712/5677 [==================>...........] - ETA: 4:49 - loss: 0.6714 - acc: 0.5849
3776/5677 [==================>...........] - ETA: 4:40 - loss: 0.6718 - acc: 0.5845
3840/5677 [===================>..........] - ETA: 4:31 - loss: 0.6717 - acc: 0.5841
3904/5677 [===================>..........] - ETA: 4:21 - loss: 0.6720 - acc: 0.5830
3968/5677 [===================>..........] - ETA: 4:11 - loss: 0.6717 - acc: 0.5824
4032/5677 [====================>.........] - ETA: 4:02 - loss: 0.6718 - acc: 0.5833
4096/5677 [====================>.........] - ETA: 3:53 - loss: 0.6720 - acc: 0.5830
4160/5677 [====================>.........] - ETA: 3:44 - loss: 0.6709 - acc: 0.5849
4224/5677 [=====================>........] - ETA: 3:34 - loss: 0.6712 - acc: 0.5840
4288/5677 [=====================>........] - ETA: 3:25 - loss: 0.6709 - acc: 0.5851
4352/5677 [=====================>........] - ETA: 3:15 - loss: 0.6709 - acc: 0.5843
4416/5677 [======================>.......] - ETA: 3:06 - loss: 0.6706 - acc: 0.5858
4480/5677 [======================>.......] - ETA: 2:57 - loss: 0.6706 - acc: 0.5857
4544/5677 [=======================>......] - ETA: 2:47 - loss: 0.6711 - acc: 0.5852
4608/5677 [=======================>......] - ETA: 2:38 - loss: 0.6713 - acc: 0.5851
4672/5677 [=======================>......] - ETA: 2:28 - loss: 0.6705 - acc: 0.5873
4736/5677 [========================>.....] - ETA: 2:19 - loss: 0.6709 - acc: 0.5868
4800/5677 [========================>.....] - ETA: 2:10 - loss: 0.6709 - acc: 0.5869
4864/5677 [========================>.....] - ETA: 2:00 - loss: 0.6714 - acc: 0.5861
4928/5677 [=========================>....] - ETA: 1:51 - loss: 0.6713 - acc: 0.5860
4992/5677 [=========================>....] - ETA: 1:41 - loss: 0.6712 - acc: 0.5871
5056/5677 [=========================>....] - ETA: 1:32 - loss: 0.6710 - acc: 0.5872
5120/5677 [==========================>...] - ETA: 1:22 - loss: 0.6711 - acc: 0.5873
5184/5677 [==========================>...] - ETA: 1:13 - loss: 0.6714 - acc: 0.5868
5248/5677 [==========================>...] - ETA: 1:03 - loss: 0.6717 - acc: 0.5861
5312/5677 [===========================>..] - ETA: 54s - loss: 0.6712 - acc: 0.5870 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6716 - acc: 0.5869
5440/5677 [===========================>..] - ETA: 35s - loss: 0.6718 - acc: 0.5866
5504/5677 [============================>.] - ETA: 25s - loss: 0.6718 - acc: 0.5872
5568/5677 [============================>.] - ETA: 16s - loss: 0.6717 - acc: 0.5869
5632/5677 [============================>.] - ETA: 6s - loss: 0.6717 - acc: 0.5875 
5677/5677 [==============================] - 877s 155ms/step - loss: 0.6716 - acc: 0.5875 - val_loss: 0.6919 - val_acc: 0.5642

Epoch 00010: val_acc improved from 0.55784 to 0.56418, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window19/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fdf783a9410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fdf783a9410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fdf782d15d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fdf782d15d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf782833d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf782833d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf7063e090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf7063e090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdcd8400c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdcd8400c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf70692750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf70692750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf7063ef10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf7063ef10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf705df250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf705df250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf70695f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf70695f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf705689d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf705689d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb7413ae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb7413ae50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf7065a350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf7065a350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf705f4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf705f4fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf701f1f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf701f1f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf7015df50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf7015df50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf7005b0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf7005b0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf70280050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf70280050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf701cc150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf701cc150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf385afe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf385afe50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf38588d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf38588d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf7005e190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf7005e190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf385b0850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf385b0850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf3859a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf3859a1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde3457b710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde3457b710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde3440f910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde3440f910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde343a4ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde343a4ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde34621bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde34621bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde342a7190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde342a7190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde34356750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde34356750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde342644d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde342644d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde342b7ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde342b7ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde3408b310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde3408b310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde343dfd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde343dfd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde1470c290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde1470c290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde14702150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde14702150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde142bffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde142bffd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde1458aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde1458aed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde144dd110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde144dd110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde141bbcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde141bbcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde1415d510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fde1415d510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdab8605810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdab8605810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde142ee7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fde142ee7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde140cf110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde140cf110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde142d90d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fde142d90d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fddfc547a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fddfc547a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde143e9790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fde143e9790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdfcc41df90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdfcc41df90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddfc588590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddfc588590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fddfc3392d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fddfc3392d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fddfc3aba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fddfc3aba90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddfc149490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddfc149490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fddfc3398d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fddfc3398d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddfc2b1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddfc2b1c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fddfc3b2a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fddfc3b2a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fddd46f9310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fddd46f9310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddfc25ad90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddfc25ad90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fddd478fdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fddd478fdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddd4798350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddd4798350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fddd455d350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fddd455d350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fddd43e4690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fddd43e4690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddd45108d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddd45108d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fddd4798c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fddd4798c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddd44be7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddd44be7d0>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 9:37
 128/1578 [=>............................] - ETA: 5:14
 192/1578 [==>...........................] - ETA: 3:42
 256/1578 [===>..........................] - ETA: 2:55
 320/1578 [=====>........................] - ETA: 2:27
 384/1578 [======>.......................] - ETA: 2:07
 448/1578 [=======>......................] - ETA: 1:51
 512/1578 [========>.....................] - ETA: 1:40
 576/1578 [=========>....................] - ETA: 1:31
 640/1578 [===========>..................] - ETA: 1:21
 704/1578 [============>.................] - ETA: 1:14
 768/1578 [=============>................] - ETA: 1:07
 832/1578 [==============>...............] - ETA: 1:00
 896/1578 [================>.............] - ETA: 54s 
 960/1578 [=================>............] - ETA: 49s
1024/1578 [==================>...........] - ETA: 43s
1088/1578 [===================>..........] - ETA: 37s
1152/1578 [====================>.........] - ETA: 32s
1216/1578 [======================>.......] - ETA: 27s
1280/1578 [=======================>......] - ETA: 22s
1344/1578 [========================>.....] - ETA: 17s
1408/1578 [=========================>....] - ETA: 12s
1472/1578 [==========================>...] - ETA: 7s 
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 112s 71ms/step
loss: 0.6802077799544437
acc: 0.5849176169348307
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fdab8638950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fdab8638950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fdcd8472ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fdcd8472ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddac227150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddac227150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd746da450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd746da450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd98c32f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd98c32f610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf707b0910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf707b0910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf707b07d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf707b07d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdab85d8550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdab85d8550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fddac211310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fddac211310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf781ff690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdf781ff690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf781c1410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf781c1410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf78200150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdf78200150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf780fe090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf780fe090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf7810be90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdf7810be90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdab823c150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdab823c150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf78143dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdf78143dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdab8387210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdab8387210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdab8397890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdab8397890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdab81a2910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdab81a2910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdab8089810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdab8089810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdab80d52d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdab80d52d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdab80d9510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdab80d9510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda987f52d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda987f52d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda986788d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda986788d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda9850e650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda9850e650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda985daed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda985daed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda98678550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda98678550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda98404850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda98404850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda982331d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda982331d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda98255690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda98255690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda9846bb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda9846bb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda98678450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda98678450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda9810ca50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda9810ca50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda981156d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda981156d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda7468aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda7468aed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda98105510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda98105510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda98115250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda98115250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda74603650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda74603650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda7439ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda7439ec50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda743b0510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda743b0510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda74342310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda74342310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda74666cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda74666cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda7414f850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda7414f850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda740f17d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda740f17d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda7428a090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda7428a090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda7403cb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda7403cb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda6c6fe310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda6c6fe310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda74795410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda74795410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda6c5c7890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda6c5c7890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda6c4aa2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda6c4aa2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda6c5d0d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda6c5d0d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda6c5c7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda6c5c7290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda6c421cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda6c421cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda6c3d00d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda6c3d00d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd98c242150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd98c242150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda6c42a5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda6c42a5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda6c2bb090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda6c2bb090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd98c1cbd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd98c1cbd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda6c41b090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda6c41b090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd98c1b8810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd98c1b8810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd95472bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd95472bfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd98c1b8d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd98c1b8d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9544cf3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9544cf3d0>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 2:23:40 - loss: 0.6590 - acc: 0.5781
 128/5677 [..............................] - ETA: 1:18:51 - loss: 0.7256 - acc: 0.5391
 192/5677 [>.............................] - ETA: 57:00 - loss: 0.7413 - acc: 0.5417  
 256/5677 [>.............................] - ETA: 46:28 - loss: 0.7525 - acc: 0.5156
 320/5677 [>.............................] - ETA: 39:43 - loss: 0.7523 - acc: 0.5125
 384/5677 [=>............................] - ETA: 35:21 - loss: 0.7358 - acc: 0.5312
 448/5677 [=>............................] - ETA: 31:55 - loss: 0.7300 - acc: 0.5357
 512/5677 [=>............................] - ETA: 29:19 - loss: 0.7454 - acc: 0.5215
 576/5677 [==>...........................] - ETA: 27:13 - loss: 0.7408 - acc: 0.5260
 640/5677 [==>...........................] - ETA: 25:24 - loss: 0.7417 - acc: 0.5266
 704/5677 [==>...........................] - ETA: 24:08 - loss: 0.7499 - acc: 0.5128
 768/5677 [===>..........................] - ETA: 22:58 - loss: 0.7511 - acc: 0.5091
 832/5677 [===>..........................] - ETA: 22:01 - loss: 0.7492 - acc: 0.5144
 896/5677 [===>..........................] - ETA: 21:03 - loss: 0.7473 - acc: 0.5167
 960/5677 [====>.........................] - ETA: 20:11 - loss: 0.7470 - acc: 0.5198
1024/5677 [====>.........................] - ETA: 19:38 - loss: 0.7518 - acc: 0.5137
1088/5677 [====>.........................] - ETA: 18:56 - loss: 0.7514 - acc: 0.5110
1152/5677 [=====>........................] - ETA: 18:17 - loss: 0.7472 - acc: 0.5165
1216/5677 [=====>........................] - ETA: 17:41 - loss: 0.7488 - acc: 0.5099
1280/5677 [=====>........................] - ETA: 17:07 - loss: 0.7490 - acc: 0.5078
1344/5677 [======>.......................] - ETA: 16:35 - loss: 0.7476 - acc: 0.5097
1408/5677 [======>.......................] - ETA: 16:05 - loss: 0.7459 - acc: 0.5107
1472/5677 [======>.......................] - ETA: 15:38 - loss: 0.7466 - acc: 0.5088
1536/5677 [=======>......................] - ETA: 15:13 - loss: 0.7456 - acc: 0.5104
1600/5677 [=======>......................] - ETA: 14:50 - loss: 0.7453 - acc: 0.5112
1664/5677 [=======>......................] - ETA: 14:27 - loss: 0.7444 - acc: 0.5138
1728/5677 [========>.....................] - ETA: 14:05 - loss: 0.7438 - acc: 0.5145
1792/5677 [========>.....................] - ETA: 13:43 - loss: 0.7421 - acc: 0.5128
1856/5677 [========>.....................] - ETA: 13:24 - loss: 0.7406 - acc: 0.5135
1920/5677 [=========>....................] - ETA: 13:05 - loss: 0.7415 - acc: 0.5125
1984/5677 [=========>....................] - ETA: 12:45 - loss: 0.7391 - acc: 0.5161
2048/5677 [=========>....................] - ETA: 12:28 - loss: 0.7397 - acc: 0.5137
2112/5677 [==========>...................] - ETA: 12:11 - loss: 0.7381 - acc: 0.5147
2176/5677 [==========>...................] - ETA: 11:53 - loss: 0.7379 - acc: 0.5138
2240/5677 [==========>...................] - ETA: 11:36 - loss: 0.7360 - acc: 0.5161
2304/5677 [===========>..................] - ETA: 11:19 - loss: 0.7346 - acc: 0.5174
2368/5677 [===========>..................] - ETA: 11:04 - loss: 0.7355 - acc: 0.5127
2432/5677 [===========>..................] - ETA: 10:46 - loss: 0.7338 - acc: 0.5136
2496/5677 [============>.................] - ETA: 10:31 - loss: 0.7332 - acc: 0.5132
2560/5677 [============>.................] - ETA: 10:15 - loss: 0.7314 - acc: 0.5164
2624/5677 [============>.................] - ETA: 10:00 - loss: 0.7313 - acc: 0.5160
2688/5677 [=============>................] - ETA: 9:45 - loss: 0.7300 - acc: 0.5167 
2752/5677 [=============>................] - ETA: 9:30 - loss: 0.7297 - acc: 0.5156
2816/5677 [=============>................] - ETA: 9:16 - loss: 0.7301 - acc: 0.5153
2880/5677 [==============>...............] - ETA: 9:02 - loss: 0.7308 - acc: 0.5135
2944/5677 [==============>...............] - ETA: 8:48 - loss: 0.7306 - acc: 0.5132
3008/5677 [==============>...............] - ETA: 8:35 - loss: 0.7306 - acc: 0.5150
3072/5677 [===============>..............] - ETA: 8:22 - loss: 0.7290 - acc: 0.5173
3136/5677 [===============>..............] - ETA: 8:08 - loss: 0.7284 - acc: 0.5169
3200/5677 [===============>..............] - ETA: 7:54 - loss: 0.7281 - acc: 0.5169
3264/5677 [================>.............] - ETA: 7:40 - loss: 0.7273 - acc: 0.5169
3328/5677 [================>.............] - ETA: 7:27 - loss: 0.7277 - acc: 0.5153
3392/5677 [================>.............] - ETA: 7:14 - loss: 0.7269 - acc: 0.5156
3456/5677 [=================>............] - ETA: 7:00 - loss: 0.7271 - acc: 0.5150
3520/5677 [=================>............] - ETA: 6:48 - loss: 0.7264 - acc: 0.5165
3584/5677 [=================>............] - ETA: 6:35 - loss: 0.7263 - acc: 0.5162
3648/5677 [==================>...........] - ETA: 6:22 - loss: 0.7258 - acc: 0.5173
3712/5677 [==================>...........] - ETA: 6:09 - loss: 0.7263 - acc: 0.5162
3776/5677 [==================>...........] - ETA: 5:56 - loss: 0.7264 - acc: 0.5154
3840/5677 [===================>..........] - ETA: 5:44 - loss: 0.7265 - acc: 0.5151
3904/5677 [===================>..........] - ETA: 5:31 - loss: 0.7272 - acc: 0.5136
3968/5677 [===================>..........] - ETA: 5:19 - loss: 0.7270 - acc: 0.5134
4032/5677 [====================>.........] - ETA: 5:06 - loss: 0.7270 - acc: 0.5131
4096/5677 [====================>.........] - ETA: 4:53 - loss: 0.7266 - acc: 0.5142
4160/5677 [====================>.........] - ETA: 4:41 - loss: 0.7273 - acc: 0.5130
4224/5677 [=====================>........] - ETA: 4:28 - loss: 0.7272 - acc: 0.5130
4288/5677 [=====================>........] - ETA: 4:16 - loss: 0.7263 - acc: 0.5142
4352/5677 [=====================>........] - ETA: 4:04 - loss: 0.7267 - acc: 0.5131
4416/5677 [======================>.......] - ETA: 3:52 - loss: 0.7267 - acc: 0.5129
4480/5677 [======================>.......] - ETA: 3:40 - loss: 0.7265 - acc: 0.5125
4544/5677 [=======================>......] - ETA: 3:28 - loss: 0.7266 - acc: 0.5121
4608/5677 [=======================>......] - ETA: 3:16 - loss: 0.7262 - acc: 0.5128
4672/5677 [=======================>......] - ETA: 3:04 - loss: 0.7257 - acc: 0.5126
4736/5677 [========================>.....] - ETA: 2:52 - loss: 0.7256 - acc: 0.5125
4800/5677 [========================>.....] - ETA: 2:40 - loss: 0.7251 - acc: 0.5123
4864/5677 [========================>.....] - ETA: 2:28 - loss: 0.7245 - acc: 0.5130
4928/5677 [=========================>....] - ETA: 2:16 - loss: 0.7239 - acc: 0.5150
4992/5677 [=========================>....] - ETA: 2:04 - loss: 0.7238 - acc: 0.5152
5056/5677 [=========================>....] - ETA: 1:53 - loss: 0.7236 - acc: 0.5156
5120/5677 [==========================>...] - ETA: 1:41 - loss: 0.7233 - acc: 0.5154
5184/5677 [==========================>...] - ETA: 1:29 - loss: 0.7231 - acc: 0.5149
5248/5677 [==========================>...] - ETA: 1:17 - loss: 0.7226 - acc: 0.5154
5312/5677 [===========================>..] - ETA: 1:06 - loss: 0.7220 - acc: 0.5154
5376/5677 [===========================>..] - ETA: 54s - loss: 0.7222 - acc: 0.5143 
5440/5677 [===========================>..] - ETA: 42s - loss: 0.7222 - acc: 0.5149
5504/5677 [============================>.] - ETA: 31s - loss: 0.7215 - acc: 0.5164
5568/5677 [============================>.] - ETA: 19s - loss: 0.7209 - acc: 0.5174
5632/5677 [============================>.] - ETA: 8s - loss: 0.7205 - acc: 0.5178 
5677/5677 [==============================] - 1072s 189ms/step - loss: 0.7206 - acc: 0.5174 - val_loss: 0.6899 - val_acc: 0.5372

Epoch 00001: val_acc improved from -inf to 0.53724, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window20/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 18:20 - loss: 0.6892 - acc: 0.5312
 128/5677 [..............................] - ETA: 16:35 - loss: 0.7118 - acc: 0.5000
 192/5677 [>.............................] - ETA: 16:23 - loss: 0.7167 - acc: 0.5052
 256/5677 [>.............................] - ETA: 15:47 - loss: 0.7029 - acc: 0.5156
 320/5677 [>.............................] - ETA: 15:39 - loss: 0.7018 - acc: 0.5062
 384/5677 [=>............................] - ETA: 15:12 - loss: 0.6969 - acc: 0.5286
 448/5677 [=>............................] - ETA: 14:55 - loss: 0.7018 - acc: 0.5156
 512/5677 [=>............................] - ETA: 14:37 - loss: 0.7061 - acc: 0.5059
 576/5677 [==>...........................] - ETA: 14:27 - loss: 0.7039 - acc: 0.5104
 640/5677 [==>...........................] - ETA: 14:22 - loss: 0.7022 - acc: 0.5156
 704/5677 [==>...........................] - ETA: 14:07 - loss: 0.7005 - acc: 0.5170
 768/5677 [===>..........................] - ETA: 13:48 - loss: 0.7013 - acc: 0.5156
 832/5677 [===>..........................] - ETA: 13:33 - loss: 0.7000 - acc: 0.5192
 896/5677 [===>..........................] - ETA: 13:15 - loss: 0.6996 - acc: 0.5190
 960/5677 [====>.........................] - ETA: 13:00 - loss: 0.6998 - acc: 0.5156
1024/5677 [====>.........................] - ETA: 12:51 - loss: 0.6989 - acc: 0.5156
1088/5677 [====>.........................] - ETA: 12:41 - loss: 0.6980 - acc: 0.5193
1152/5677 [=====>........................] - ETA: 12:34 - loss: 0.6981 - acc: 0.5182
1216/5677 [=====>........................] - ETA: 12:19 - loss: 0.6992 - acc: 0.5173
1280/5677 [=====>........................] - ETA: 12:08 - loss: 0.7009 - acc: 0.5164
1344/5677 [======>.......................] - ETA: 11:58 - loss: 0.7021 - acc: 0.5141
1408/5677 [======>.......................] - ETA: 11:48 - loss: 0.7018 - acc: 0.5178
1472/5677 [======>.......................] - ETA: 11:38 - loss: 0.7012 - acc: 0.5190
1536/5677 [=======>......................] - ETA: 11:30 - loss: 0.7011 - acc: 0.5182
1600/5677 [=======>......................] - ETA: 11:19 - loss: 0.7012 - acc: 0.5200
1664/5677 [=======>......................] - ETA: 11:09 - loss: 0.7012 - acc: 0.5192
1728/5677 [========>.....................] - ETA: 10:56 - loss: 0.7017 - acc: 0.5179
1792/5677 [========>.....................] - ETA: 10:46 - loss: 0.7018 - acc: 0.5190
1856/5677 [========>.....................] - ETA: 10:36 - loss: 0.7015 - acc: 0.5199
1920/5677 [=========>....................] - ETA: 10:27 - loss: 0.7014 - acc: 0.5193
1984/5677 [=========>....................] - ETA: 10:19 - loss: 0.7016 - acc: 0.5197
2048/5677 [=========>....................] - ETA: 10:08 - loss: 0.7027 - acc: 0.5181
2112/5677 [==========>...................] - ETA: 9:59 - loss: 0.7020 - acc: 0.5189 
2176/5677 [==========>...................] - ETA: 9:48 - loss: 0.7031 - acc: 0.5165
2240/5677 [==========>...................] - ETA: 9:39 - loss: 0.7022 - acc: 0.5179
2304/5677 [===========>..................] - ETA: 9:28 - loss: 0.7012 - acc: 0.5217
2368/5677 [===========>..................] - ETA: 9:16 - loss: 0.7024 - acc: 0.5198
2432/5677 [===========>..................] - ETA: 9:07 - loss: 0.7032 - acc: 0.5189
2496/5677 [============>.................] - ETA: 8:55 - loss: 0.7040 - acc: 0.5180
2560/5677 [============>.................] - ETA: 8:44 - loss: 0.7045 - acc: 0.5164
2624/5677 [============>.................] - ETA: 8:35 - loss: 0.7038 - acc: 0.5168
2688/5677 [=============>................] - ETA: 8:24 - loss: 0.7031 - acc: 0.5167
2752/5677 [=============>................] - ETA: 8:13 - loss: 0.7030 - acc: 0.5167
2816/5677 [=============>................] - ETA: 8:02 - loss: 0.7031 - acc: 0.5170
2880/5677 [==============>...............] - ETA: 7:51 - loss: 0.7047 - acc: 0.5139
2944/5677 [==============>...............] - ETA: 7:42 - loss: 0.7038 - acc: 0.5166
3008/5677 [==============>...............] - ETA: 7:30 - loss: 0.7036 - acc: 0.5160
3072/5677 [===============>..............] - ETA: 7:19 - loss: 0.7036 - acc: 0.5166
3136/5677 [===============>..............] - ETA: 7:08 - loss: 0.7032 - acc: 0.5185
3200/5677 [===============>..............] - ETA: 6:57 - loss: 0.7025 - acc: 0.5197
3264/5677 [================>.............] - ETA: 6:46 - loss: 0.7027 - acc: 0.5202
3328/5677 [================>.............] - ETA: 6:36 - loss: 0.7031 - acc: 0.5192
3392/5677 [================>.............] - ETA: 6:24 - loss: 0.7029 - acc: 0.5206
3456/5677 [=================>............] - ETA: 6:14 - loss: 0.7029 - acc: 0.5208
3520/5677 [=================>............] - ETA: 6:03 - loss: 0.7030 - acc: 0.5193
3584/5677 [=================>............] - ETA: 5:53 - loss: 0.7029 - acc: 0.5193
3648/5677 [==================>...........] - ETA: 5:42 - loss: 0.7028 - acc: 0.5195
3712/5677 [==================>...........] - ETA: 5:31 - loss: 0.7025 - acc: 0.5207
3776/5677 [==================>...........] - ETA: 5:20 - loss: 0.7025 - acc: 0.5199
3840/5677 [===================>..........] - ETA: 5:09 - loss: 0.7028 - acc: 0.5193
3904/5677 [===================>..........] - ETA: 4:58 - loss: 0.7023 - acc: 0.5210
3968/5677 [===================>..........] - ETA: 4:48 - loss: 0.7023 - acc: 0.5212
4032/5677 [====================>.........] - ETA: 4:37 - loss: 0.7024 - acc: 0.5213
4096/5677 [====================>.........] - ETA: 4:26 - loss: 0.7027 - acc: 0.5215
4160/5677 [====================>.........] - ETA: 4:15 - loss: 0.7025 - acc: 0.5216
4224/5677 [=====================>........] - ETA: 4:04 - loss: 0.7018 - acc: 0.5237
4288/5677 [=====================>........] - ETA: 3:54 - loss: 0.7022 - acc: 0.5226
4352/5677 [=====================>........] - ETA: 3:43 - loss: 0.7017 - acc: 0.5225
4416/5677 [======================>.......] - ETA: 3:32 - loss: 0.7015 - acc: 0.5226
4480/5677 [======================>.......] - ETA: 3:21 - loss: 0.7014 - acc: 0.5234
4544/5677 [=======================>......] - ETA: 3:10 - loss: 0.7016 - acc: 0.5229
4608/5677 [=======================>......] - ETA: 2:59 - loss: 0.7021 - acc: 0.5211
4672/5677 [=======================>......] - ETA: 2:48 - loss: 0.7015 - acc: 0.5225
4736/5677 [========================>.....] - ETA: 2:38 - loss: 0.7014 - acc: 0.5226
4800/5677 [========================>.....] - ETA: 2:27 - loss: 0.7014 - acc: 0.5219
4864/5677 [========================>.....] - ETA: 2:16 - loss: 0.7010 - acc: 0.5230
4928/5677 [=========================>....] - ETA: 2:05 - loss: 0.7011 - acc: 0.5229
4992/5677 [=========================>....] - ETA: 1:54 - loss: 0.7015 - acc: 0.5226
5056/5677 [=========================>....] - ETA: 1:44 - loss: 0.7016 - acc: 0.5227
5120/5677 [==========================>...] - ETA: 1:33 - loss: 0.7022 - acc: 0.5211
5184/5677 [==========================>...] - ETA: 1:22 - loss: 0.7020 - acc: 0.5216
5248/5677 [==========================>...] - ETA: 1:11 - loss: 0.7019 - acc: 0.5213
5312/5677 [===========================>..] - ETA: 1:01 - loss: 0.7014 - acc: 0.5230
5376/5677 [===========================>..] - ETA: 50s - loss: 0.7015 - acc: 0.5225 
5440/5677 [===========================>..] - ETA: 39s - loss: 0.7011 - acc: 0.5233
5504/5677 [============================>.] - ETA: 28s - loss: 0.7010 - acc: 0.5240
5568/5677 [============================>.] - ETA: 18s - loss: 0.7009 - acc: 0.5244
5632/5677 [============================>.] - ETA: 7s - loss: 0.7018 - acc: 0.5215 
5677/5677 [==============================] - 984s 173ms/step - loss: 0.7018 - acc: 0.5214 - val_loss: 0.6903 - val_acc: 0.5246

Epoch 00002: val_acc did not improve from 0.53724
Epoch 3/10

  64/5677 [..............................] - ETA: 14:28 - loss: 0.6951 - acc: 0.5156
 128/5677 [..............................] - ETA: 14:32 - loss: 0.6867 - acc: 0.5391
 192/5677 [>.............................] - ETA: 14:44 - loss: 0.6770 - acc: 0.5573
 256/5677 [>.............................] - ETA: 14:23 - loss: 0.6785 - acc: 0.5703
 320/5677 [>.............................] - ETA: 14:09 - loss: 0.6745 - acc: 0.5813
 384/5677 [=>............................] - ETA: 13:57 - loss: 0.6832 - acc: 0.5573
 448/5677 [=>............................] - ETA: 13:54 - loss: 0.6892 - acc: 0.5446
 512/5677 [=>............................] - ETA: 13:36 - loss: 0.6902 - acc: 0.5430
 576/5677 [==>...........................] - ETA: 13:31 - loss: 0.6933 - acc: 0.5399
 640/5677 [==>...........................] - ETA: 13:17 - loss: 0.6913 - acc: 0.5500
 704/5677 [==>...........................] - ETA: 13:08 - loss: 0.6970 - acc: 0.5369
 768/5677 [===>..........................] - ETA: 12:57 - loss: 0.6961 - acc: 0.5352
 832/5677 [===>..........................] - ETA: 12:46 - loss: 0.6952 - acc: 0.5337
 896/5677 [===>..........................] - ETA: 12:34 - loss: 0.6995 - acc: 0.5324
 960/5677 [====>.........................] - ETA: 12:19 - loss: 0.6997 - acc: 0.5312
1024/5677 [====>.........................] - ETA: 12:11 - loss: 0.7003 - acc: 0.5254
1088/5677 [====>.........................] - ETA: 12:03 - loss: 0.6979 - acc: 0.5303
1152/5677 [=====>........................] - ETA: 11:55 - loss: 0.6966 - acc: 0.5304
1216/5677 [=====>........................] - ETA: 11:46 - loss: 0.6951 - acc: 0.5321
1280/5677 [=====>........................] - ETA: 11:35 - loss: 0.6939 - acc: 0.5375
1344/5677 [======>.......................] - ETA: 11:23 - loss: 0.6935 - acc: 0.5402
1408/5677 [======>.......................] - ETA: 11:12 - loss: 0.6926 - acc: 0.5433
1472/5677 [======>.......................] - ETA: 11:02 - loss: 0.6932 - acc: 0.5414
1536/5677 [=======>......................] - ETA: 10:53 - loss: 0.6934 - acc: 0.5397
1600/5677 [=======>......................] - ETA: 10:42 - loss: 0.6952 - acc: 0.5344
1664/5677 [=======>......................] - ETA: 10:30 - loss: 0.6953 - acc: 0.5361
1728/5677 [========>.....................] - ETA: 10:20 - loss: 0.6951 - acc: 0.5359
1792/5677 [========>.....................] - ETA: 10:10 - loss: 0.6961 - acc: 0.5318
1856/5677 [========>.....................] - ETA: 10:01 - loss: 0.6952 - acc: 0.5345
1920/5677 [=========>....................] - ETA: 9:51 - loss: 0.6950 - acc: 0.5359 
1984/5677 [=========>....................] - ETA: 9:41 - loss: 0.6946 - acc: 0.5383
2048/5677 [=========>....................] - ETA: 9:31 - loss: 0.6951 - acc: 0.5371
2112/5677 [==========>...................] - ETA: 9:21 - loss: 0.6943 - acc: 0.5402
2176/5677 [==========>...................] - ETA: 9:10 - loss: 0.6949 - acc: 0.5377
2240/5677 [==========>...................] - ETA: 8:59 - loss: 0.6956 - acc: 0.5366
2304/5677 [===========>..................] - ETA: 8:49 - loss: 0.6964 - acc: 0.5347
2368/5677 [===========>..................] - ETA: 8:38 - loss: 0.6959 - acc: 0.5372
2432/5677 [===========>..................] - ETA: 8:27 - loss: 0.6958 - acc: 0.5366
2496/5677 [============>.................] - ETA: 8:17 - loss: 0.6960 - acc: 0.5353
2560/5677 [============>.................] - ETA: 8:07 - loss: 0.6959 - acc: 0.5332
2624/5677 [============>.................] - ETA: 7:57 - loss: 0.6960 - acc: 0.5328
2688/5677 [=============>................] - ETA: 7:47 - loss: 0.6957 - acc: 0.5327
2752/5677 [=============>................] - ETA: 7:38 - loss: 0.6955 - acc: 0.5334
2816/5677 [=============>................] - ETA: 7:27 - loss: 0.6957 - acc: 0.5344
2880/5677 [==============>...............] - ETA: 7:17 - loss: 0.6953 - acc: 0.5351
2944/5677 [==============>...............] - ETA: 7:07 - loss: 0.6948 - acc: 0.5360
3008/5677 [==============>...............] - ETA: 6:57 - loss: 0.6950 - acc: 0.5372
3072/5677 [===============>..............] - ETA: 6:47 - loss: 0.6949 - acc: 0.5371
3136/5677 [===============>..............] - ETA: 6:36 - loss: 0.6961 - acc: 0.5344
3200/5677 [===============>..............] - ETA: 6:26 - loss: 0.6968 - acc: 0.5328
3264/5677 [================>.............] - ETA: 6:16 - loss: 0.6963 - acc: 0.5337
3328/5677 [================>.............] - ETA: 6:07 - loss: 0.6956 - acc: 0.5349
3392/5677 [================>.............] - ETA: 5:56 - loss: 0.6951 - acc: 0.5363
3456/5677 [=================>............] - ETA: 5:47 - loss: 0.6951 - acc: 0.5365
3520/5677 [=================>............] - ETA: 5:37 - loss: 0.6955 - acc: 0.5358
3584/5677 [=================>............] - ETA: 5:26 - loss: 0.6957 - acc: 0.5346
3648/5677 [==================>...........] - ETA: 5:16 - loss: 0.6949 - acc: 0.5362
3712/5677 [==================>...........] - ETA: 5:06 - loss: 0.6943 - acc: 0.5369
3776/5677 [==================>...........] - ETA: 4:56 - loss: 0.6943 - acc: 0.5360
3840/5677 [===================>..........] - ETA: 4:46 - loss: 0.6944 - acc: 0.5359
3904/5677 [===================>..........] - ETA: 4:36 - loss: 0.6945 - acc: 0.5359
3968/5677 [===================>..........] - ETA: 4:26 - loss: 0.6944 - acc: 0.5373
4032/5677 [====================>.........] - ETA: 4:16 - loss: 0.6942 - acc: 0.5370
4096/5677 [====================>.........] - ETA: 4:06 - loss: 0.6942 - acc: 0.5376
4160/5677 [====================>.........] - ETA: 3:56 - loss: 0.6942 - acc: 0.5373
4224/5677 [=====================>........] - ETA: 3:46 - loss: 0.6937 - acc: 0.5376
4288/5677 [=====================>........] - ETA: 3:36 - loss: 0.6933 - acc: 0.5385
4352/5677 [=====================>........] - ETA: 3:26 - loss: 0.6936 - acc: 0.5379
4416/5677 [======================>.......] - ETA: 3:16 - loss: 0.6933 - acc: 0.5389
4480/5677 [======================>.......] - ETA: 3:06 - loss: 0.6933 - acc: 0.5393
4544/5677 [=======================>......] - ETA: 2:56 - loss: 0.6936 - acc: 0.5385
4608/5677 [=======================>......] - ETA: 2:46 - loss: 0.6939 - acc: 0.5378
4672/5677 [=======================>......] - ETA: 2:35 - loss: 0.6939 - acc: 0.5372
4736/5677 [========================>.....] - ETA: 2:26 - loss: 0.6938 - acc: 0.5380
4800/5677 [========================>.....] - ETA: 2:15 - loss: 0.6932 - acc: 0.5392
4864/5677 [========================>.....] - ETA: 2:06 - loss: 0.6931 - acc: 0.5395
4928/5677 [=========================>....] - ETA: 1:56 - loss: 0.6932 - acc: 0.5392
4992/5677 [=========================>....] - ETA: 1:46 - loss: 0.6928 - acc: 0.5399
5056/5677 [=========================>....] - ETA: 1:36 - loss: 0.6930 - acc: 0.5388
5120/5677 [==========================>...] - ETA: 1:26 - loss: 0.6924 - acc: 0.5395
5184/5677 [==========================>...] - ETA: 1:16 - loss: 0.6920 - acc: 0.5403
5248/5677 [==========================>...] - ETA: 1:06 - loss: 0.6927 - acc: 0.5389
5312/5677 [===========================>..] - ETA: 56s - loss: 0.6932 - acc: 0.5382 
5376/5677 [===========================>..] - ETA: 46s - loss: 0.6929 - acc: 0.5387
5440/5677 [===========================>..] - ETA: 36s - loss: 0.6929 - acc: 0.5390
5504/5677 [============================>.] - ETA: 26s - loss: 0.6930 - acc: 0.5385
5568/5677 [============================>.] - ETA: 16s - loss: 0.6928 - acc: 0.5392
5632/5677 [============================>.] - ETA: 6s - loss: 0.6932 - acc: 0.5384 
5677/5677 [==============================] - 912s 161ms/step - loss: 0.6931 - acc: 0.5383 - val_loss: 0.6875 - val_acc: 0.5420

Epoch 00003: val_acc improved from 0.53724 to 0.54200, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window20/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 4/10

  64/5677 [..............................] - ETA: 13:58 - loss: 0.6750 - acc: 0.5469
 128/5677 [..............................] - ETA: 13:40 - loss: 0.6709 - acc: 0.5781
 192/5677 [>.............................] - ETA: 13:38 - loss: 0.6682 - acc: 0.5729
 256/5677 [>.............................] - ETA: 13:22 - loss: 0.6744 - acc: 0.5586
 320/5677 [>.............................] - ETA: 13:13 - loss: 0.6792 - acc: 0.5563
 384/5677 [=>............................] - ETA: 12:59 - loss: 0.6813 - acc: 0.5469
 448/5677 [=>............................] - ETA: 12:48 - loss: 0.6874 - acc: 0.5312
 512/5677 [=>............................] - ETA: 12:54 - loss: 0.6914 - acc: 0.5215
 576/5677 [==>...........................] - ETA: 12:42 - loss: 0.6866 - acc: 0.5399
 640/5677 [==>...........................] - ETA: 12:28 - loss: 0.6870 - acc: 0.5375
 704/5677 [==>...........................] - ETA: 12:17 - loss: 0.6875 - acc: 0.5412
 768/5677 [===>..........................] - ETA: 12:06 - loss: 0.6887 - acc: 0.5404
 832/5677 [===>..........................] - ETA: 11:53 - loss: 0.6894 - acc: 0.5361
 896/5677 [===>..........................] - ETA: 11:46 - loss: 0.6896 - acc: 0.5357
 960/5677 [====>.........................] - ETA: 11:35 - loss: 0.6894 - acc: 0.5375
1024/5677 [====>.........................] - ETA: 11:29 - loss: 0.6863 - acc: 0.5479
1088/5677 [====>.........................] - ETA: 11:19 - loss: 0.6859 - acc: 0.5496
1152/5677 [=====>........................] - ETA: 11:09 - loss: 0.6883 - acc: 0.5425
1216/5677 [=====>........................] - ETA: 11:01 - loss: 0.6896 - acc: 0.5419
1280/5677 [=====>........................] - ETA: 10:53 - loss: 0.6882 - acc: 0.5461
1344/5677 [======>.......................] - ETA: 10:45 - loss: 0.6884 - acc: 0.5454
1408/5677 [======>.......................] - ETA: 10:36 - loss: 0.6883 - acc: 0.5455
1472/5677 [======>.......................] - ETA: 10:26 - loss: 0.6892 - acc: 0.5435
1536/5677 [=======>......................] - ETA: 10:14 - loss: 0.6902 - acc: 0.5410
1600/5677 [=======>......................] - ETA: 10:07 - loss: 0.6920 - acc: 0.5363
1664/5677 [=======>......................] - ETA: 9:56 - loss: 0.6907 - acc: 0.5397 
1728/5677 [========>.....................] - ETA: 9:47 - loss: 0.6891 - acc: 0.5451
1792/5677 [========>.....................] - ETA: 9:38 - loss: 0.6904 - acc: 0.5430
1856/5677 [========>.....................] - ETA: 9:28 - loss: 0.6900 - acc: 0.5426
1920/5677 [=========>....................] - ETA: 9:19 - loss: 0.6894 - acc: 0.5427
1984/5677 [=========>....................] - ETA: 9:09 - loss: 0.6895 - acc: 0.5444
2048/5677 [=========>....................] - ETA: 8:59 - loss: 0.6892 - acc: 0.5420
2112/5677 [==========>...................] - ETA: 8:49 - loss: 0.6907 - acc: 0.5398
2176/5677 [==========>...................] - ETA: 8:40 - loss: 0.6913 - acc: 0.5395
2240/5677 [==========>...................] - ETA: 8:30 - loss: 0.6910 - acc: 0.5406
2304/5677 [===========>..................] - ETA: 8:21 - loss: 0.6905 - acc: 0.5421
2368/5677 [===========>..................] - ETA: 8:12 - loss: 0.6911 - acc: 0.5405
2432/5677 [===========>..................] - ETA: 8:04 - loss: 0.6904 - acc: 0.5411
2496/5677 [============>.................] - ETA: 7:54 - loss: 0.6908 - acc: 0.5417
2560/5677 [============>.................] - ETA: 7:44 - loss: 0.6903 - acc: 0.5437
2624/5677 [============>.................] - ETA: 7:36 - loss: 0.6901 - acc: 0.5438
2688/5677 [=============>................] - ETA: 7:26 - loss: 0.6899 - acc: 0.5432
2752/5677 [=============>................] - ETA: 7:17 - loss: 0.6902 - acc: 0.5425
2816/5677 [=============>................] - ETA: 7:07 - loss: 0.6900 - acc: 0.5433
2880/5677 [==============>...............] - ETA: 6:58 - loss: 0.6901 - acc: 0.5427
2944/5677 [==============>...............] - ETA: 6:48 - loss: 0.6903 - acc: 0.5414
3008/5677 [==============>...............] - ETA: 6:39 - loss: 0.6895 - acc: 0.5445
3072/5677 [===============>..............] - ETA: 6:30 - loss: 0.6898 - acc: 0.5449
3136/5677 [===============>..............] - ETA: 6:20 - loss: 0.6890 - acc: 0.5469
3200/5677 [===============>..............] - ETA: 6:11 - loss: 0.6886 - acc: 0.5475
3264/5677 [================>.............] - ETA: 6:01 - loss: 0.6886 - acc: 0.5478
3328/5677 [================>.............] - ETA: 5:52 - loss: 0.6884 - acc: 0.5475
3392/5677 [================>.............] - ETA: 5:42 - loss: 0.6878 - acc: 0.5486
3456/5677 [=================>............] - ETA: 5:33 - loss: 0.6880 - acc: 0.5480
3520/5677 [=================>............] - ETA: 5:23 - loss: 0.6888 - acc: 0.5457
3584/5677 [=================>............] - ETA: 5:14 - loss: 0.6895 - acc: 0.5446
3648/5677 [==================>...........] - ETA: 5:04 - loss: 0.6891 - acc: 0.5461
3712/5677 [==================>...........] - ETA: 4:55 - loss: 0.6885 - acc: 0.5477
3776/5677 [==================>...........] - ETA: 4:45 - loss: 0.6884 - acc: 0.5479
3840/5677 [===================>..........] - ETA: 4:35 - loss: 0.6879 - acc: 0.5497
3904/5677 [===================>..........] - ETA: 4:26 - loss: 0.6879 - acc: 0.5497
3968/5677 [===================>..........] - ETA: 4:16 - loss: 0.6877 - acc: 0.5504
4032/5677 [====================>.........] - ETA: 4:07 - loss: 0.6874 - acc: 0.5506
4096/5677 [====================>.........] - ETA: 3:57 - loss: 0.6871 - acc: 0.5505
4160/5677 [====================>.........] - ETA: 3:48 - loss: 0.6869 - acc: 0.5512
4224/5677 [=====================>........] - ETA: 3:38 - loss: 0.6872 - acc: 0.5514
4288/5677 [=====================>........] - ETA: 3:29 - loss: 0.6867 - acc: 0.5520
4352/5677 [=====================>........] - ETA: 3:19 - loss: 0.6869 - acc: 0.5517
4416/5677 [======================>.......] - ETA: 3:09 - loss: 0.6868 - acc: 0.5523
4480/5677 [======================>.......] - ETA: 3:00 - loss: 0.6867 - acc: 0.5522
4544/5677 [=======================>......] - ETA: 2:51 - loss: 0.6875 - acc: 0.5526
4608/5677 [=======================>......] - ETA: 2:42 - loss: 0.6877 - acc: 0.5525
4672/5677 [=======================>......] - ETA: 2:32 - loss: 0.6882 - acc: 0.5509
4736/5677 [========================>.....] - ETA: 2:22 - loss: 0.6880 - acc: 0.5515
4800/5677 [========================>.....] - ETA: 2:13 - loss: 0.6878 - acc: 0.5521
4864/5677 [========================>.....] - ETA: 2:03 - loss: 0.6878 - acc: 0.5514
4928/5677 [=========================>....] - ETA: 1:53 - loss: 0.6871 - acc: 0.5526
4992/5677 [=========================>....] - ETA: 1:44 - loss: 0.6868 - acc: 0.5527
5056/5677 [=========================>....] - ETA: 1:34 - loss: 0.6871 - acc: 0.5524
5120/5677 [==========================>...] - ETA: 1:24 - loss: 0.6870 - acc: 0.5527
5184/5677 [==========================>...] - ETA: 1:15 - loss: 0.6867 - acc: 0.5529
5248/5677 [==========================>...] - ETA: 1:05 - loss: 0.6864 - acc: 0.5543
5312/5677 [===========================>..] - ETA: 55s - loss: 0.6866 - acc: 0.5533 
5376/5677 [===========================>..] - ETA: 46s - loss: 0.6869 - acc: 0.5523
5440/5677 [===========================>..] - ETA: 36s - loss: 0.6870 - acc: 0.5518
5504/5677 [============================>.] - ETA: 26s - loss: 0.6871 - acc: 0.5521
5568/5677 [============================>.] - ETA: 16s - loss: 0.6869 - acc: 0.5526
5632/5677 [============================>.] - ETA: 6s - loss: 0.6870 - acc: 0.5531 
5677/5677 [==============================] - 905s 159ms/step - loss: 0.6870 - acc: 0.5535 - val_loss: 0.6947 - val_acc: 0.5071

Epoch 00004: val_acc did not improve from 0.54200
Epoch 5/10

  64/5677 [..............................] - ETA: 15:13 - loss: 0.6837 - acc: 0.5469
 128/5677 [..............................] - ETA: 14:44 - loss: 0.6844 - acc: 0.5391
 192/5677 [>.............................] - ETA: 14:33 - loss: 0.6752 - acc: 0.5677
 256/5677 [>.............................] - ETA: 14:08 - loss: 0.6791 - acc: 0.5664
 320/5677 [>.............................] - ETA: 13:58 - loss: 0.6878 - acc: 0.5500
 384/5677 [=>............................] - ETA: 13:54 - loss: 0.6868 - acc: 0.5469
 448/5677 [=>............................] - ETA: 13:44 - loss: 0.6870 - acc: 0.5491
 512/5677 [=>............................] - ETA: 13:32 - loss: 0.6847 - acc: 0.5547
 576/5677 [==>...........................] - ETA: 13:23 - loss: 0.6850 - acc: 0.5556
 640/5677 [==>...........................] - ETA: 13:13 - loss: 0.6847 - acc: 0.5500
 704/5677 [==>...........................] - ETA: 12:57 - loss: 0.6820 - acc: 0.5554
 768/5677 [===>..........................] - ETA: 12:49 - loss: 0.6782 - acc: 0.5599
 832/5677 [===>..........................] - ETA: 12:33 - loss: 0.6805 - acc: 0.5553
 896/5677 [===>..........................] - ETA: 12:24 - loss: 0.6791 - acc: 0.5569
 960/5677 [====>.........................] - ETA: 12:12 - loss: 0.6827 - acc: 0.5521
1024/5677 [====>.........................] - ETA: 12:07 - loss: 0.6815 - acc: 0.5547
1088/5677 [====>.........................] - ETA: 11:56 - loss: 0.6818 - acc: 0.5506
1152/5677 [=====>........................] - ETA: 11:48 - loss: 0.6821 - acc: 0.5530
1216/5677 [=====>........................] - ETA: 11:36 - loss: 0.6811 - acc: 0.5510
1280/5677 [=====>........................] - ETA: 11:27 - loss: 0.6817 - acc: 0.5531
1344/5677 [======>.......................] - ETA: 11:16 - loss: 0.6827 - acc: 0.5506
1408/5677 [======>.......................] - ETA: 11:06 - loss: 0.6847 - acc: 0.5490
1472/5677 [======>.......................] - ETA: 10:54 - loss: 0.6853 - acc: 0.5482
1536/5677 [=======>......................] - ETA: 10:42 - loss: 0.6844 - acc: 0.5469
1600/5677 [=======>......................] - ETA: 10:30 - loss: 0.6826 - acc: 0.5487
1664/5677 [=======>......................] - ETA: 10:22 - loss: 0.6825 - acc: 0.5499
1728/5677 [========>.....................] - ETA: 10:13 - loss: 0.6813 - acc: 0.5515
1792/5677 [========>.....................] - ETA: 10:05 - loss: 0.6808 - acc: 0.5547
1856/5677 [========>.....................] - ETA: 9:54 - loss: 0.6800 - acc: 0.5560 
1920/5677 [=========>....................] - ETA: 9:43 - loss: 0.6798 - acc: 0.5557
1984/5677 [=========>....................] - ETA: 9:34 - loss: 0.6799 - acc: 0.5554
2048/5677 [=========>....................] - ETA: 9:24 - loss: 0.6797 - acc: 0.5557
2112/5677 [==========>...................] - ETA: 9:14 - loss: 0.6789 - acc: 0.5568
2176/5677 [==========>...................] - ETA: 9:04 - loss: 0.6785 - acc: 0.5607
2240/5677 [==========>...................] - ETA: 8:55 - loss: 0.6781 - acc: 0.5603
2304/5677 [===========>..................] - ETA: 8:44 - loss: 0.6785 - acc: 0.5582
2368/5677 [===========>..................] - ETA: 8:35 - loss: 0.6799 - acc: 0.5557
2432/5677 [===========>..................] - ETA: 8:25 - loss: 0.6811 - acc: 0.5518
2496/5677 [============>.................] - ETA: 8:15 - loss: 0.6816 - acc: 0.5513
2560/5677 [============>.................] - ETA: 8:05 - loss: 0.6812 - acc: 0.5516
2624/5677 [============>.................] - ETA: 7:55 - loss: 0.6818 - acc: 0.5499
2688/5677 [=============>................] - ETA: 7:45 - loss: 0.6806 - acc: 0.5525
2752/5677 [=============>................] - ETA: 7:35 - loss: 0.6811 - acc: 0.5523
2816/5677 [=============>................] - ETA: 7:25 - loss: 0.6816 - acc: 0.5508
2880/5677 [==============>...............] - ETA: 7:15 - loss: 0.6812 - acc: 0.5510
2944/5677 [==============>...............] - ETA: 7:04 - loss: 0.6802 - acc: 0.5547
3008/5677 [==============>...............] - ETA: 6:54 - loss: 0.6801 - acc: 0.5542
3072/5677 [===============>..............] - ETA: 6:45 - loss: 0.6799 - acc: 0.5553
3136/5677 [===============>..............] - ETA: 6:35 - loss: 0.6794 - acc: 0.5580
3200/5677 [===============>..............] - ETA: 6:26 - loss: 0.6797 - acc: 0.5591
3264/5677 [================>.............] - ETA: 6:15 - loss: 0.6798 - acc: 0.5591
3328/5677 [================>.............] - ETA: 6:05 - loss: 0.6797 - acc: 0.5604
3392/5677 [================>.............] - ETA: 5:56 - loss: 0.6801 - acc: 0.5598
3456/5677 [=================>............] - ETA: 5:45 - loss: 0.6808 - acc: 0.5596
3520/5677 [=================>............] - ETA: 5:36 - loss: 0.6803 - acc: 0.5605
3584/5677 [=================>............] - ETA: 5:25 - loss: 0.6805 - acc: 0.5617
3648/5677 [==================>...........] - ETA: 5:16 - loss: 0.6808 - acc: 0.5617
3712/5677 [==================>...........] - ETA: 5:05 - loss: 0.6803 - acc: 0.5630
3776/5677 [==================>...........] - ETA: 4:55 - loss: 0.6812 - acc: 0.5636
3840/5677 [===================>..........] - ETA: 4:46 - loss: 0.6818 - acc: 0.5635
3904/5677 [===================>..........] - ETA: 4:36 - loss: 0.6822 - acc: 0.5630
3968/5677 [===================>..........] - ETA: 4:26 - loss: 0.6835 - acc: 0.5612
4032/5677 [====================>.........] - ETA: 4:16 - loss: 0.6846 - acc: 0.5595
4096/5677 [====================>.........] - ETA: 4:06 - loss: 0.6846 - acc: 0.5591
4160/5677 [====================>.........] - ETA: 3:56 - loss: 0.6847 - acc: 0.5589
4224/5677 [=====================>........] - ETA: 3:47 - loss: 0.6841 - acc: 0.5601
4288/5677 [=====================>........] - ETA: 3:37 - loss: 0.6842 - acc: 0.5602
4352/5677 [=====================>........] - ETA: 3:27 - loss: 0.6840 - acc: 0.5614
4416/5677 [======================>.......] - ETA: 3:17 - loss: 0.6842 - acc: 0.5607
4480/5677 [======================>.......] - ETA: 3:07 - loss: 0.6849 - acc: 0.5596
4544/5677 [=======================>......] - ETA: 2:57 - loss: 0.6848 - acc: 0.5601
4608/5677 [=======================>......] - ETA: 2:47 - loss: 0.6849 - acc: 0.5603
4672/5677 [=======================>......] - ETA: 2:37 - loss: 0.6848 - acc: 0.5604
4736/5677 [========================>.....] - ETA: 2:27 - loss: 0.6847 - acc: 0.5608
4800/5677 [========================>.....] - ETA: 2:17 - loss: 0.6848 - acc: 0.5604
4864/5677 [========================>.....] - ETA: 2:07 - loss: 0.6848 - acc: 0.5604
4928/5677 [=========================>....] - ETA: 1:57 - loss: 0.6846 - acc: 0.5607
4992/5677 [=========================>....] - ETA: 1:47 - loss: 0.6843 - acc: 0.5613
5056/5677 [=========================>....] - ETA: 1:37 - loss: 0.6844 - acc: 0.5607
5120/5677 [==========================>...] - ETA: 1:27 - loss: 0.6849 - acc: 0.5584
5184/5677 [==========================>...] - ETA: 1:17 - loss: 0.6851 - acc: 0.5573
5248/5677 [==========================>...] - ETA: 1:07 - loss: 0.6852 - acc: 0.5568
5312/5677 [===========================>..] - ETA: 57s - loss: 0.6851 - acc: 0.5569 
5376/5677 [===========================>..] - ETA: 47s - loss: 0.6850 - acc: 0.5569
5440/5677 [===========================>..] - ETA: 37s - loss: 0.6848 - acc: 0.5566
5504/5677 [============================>.] - ETA: 27s - loss: 0.6848 - acc: 0.5560
5568/5677 [============================>.] - ETA: 17s - loss: 0.6844 - acc: 0.5569
5632/5677 [============================>.] - ETA: 7s - loss: 0.6843 - acc: 0.5577 
5677/5677 [==============================] - 930s 164ms/step - loss: 0.6841 - acc: 0.5580 - val_loss: 0.6940 - val_acc: 0.5468

Epoch 00005: val_acc improved from 0.54200 to 0.54675, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window20/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 13:30 - loss: 0.7057 - acc: 0.5000
 128/5677 [..............................] - ETA: 14:08 - loss: 0.6947 - acc: 0.5312
 192/5677 [>.............................] - ETA: 13:57 - loss: 0.6842 - acc: 0.5573
 256/5677 [>.............................] - ETA: 14:07 - loss: 0.6788 - acc: 0.5742
 320/5677 [>.............................] - ETA: 14:00 - loss: 0.6908 - acc: 0.5656
 384/5677 [=>............................] - ETA: 13:44 - loss: 0.6865 - acc: 0.5625
 448/5677 [=>............................] - ETA: 13:32 - loss: 0.6807 - acc: 0.5625
 512/5677 [=>............................] - ETA: 13:27 - loss: 0.6790 - acc: 0.5684
 576/5677 [==>...........................] - ETA: 13:24 - loss: 0.6762 - acc: 0.5781
 640/5677 [==>...........................] - ETA: 13:17 - loss: 0.6775 - acc: 0.5813
 704/5677 [==>...........................] - ETA: 13:10 - loss: 0.6769 - acc: 0.5781
 768/5677 [===>..........................] - ETA: 13:01 - loss: 0.6768 - acc: 0.5742
 832/5677 [===>..........................] - ETA: 12:52 - loss: 0.6752 - acc: 0.5793
 896/5677 [===>..........................] - ETA: 12:43 - loss: 0.6760 - acc: 0.5781
 960/5677 [====>.........................] - ETA: 12:35 - loss: 0.6770 - acc: 0.5792
1024/5677 [====>.........................] - ETA: 12:23 - loss: 0.6811 - acc: 0.5684
1088/5677 [====>.........................] - ETA: 12:13 - loss: 0.6805 - acc: 0.5662
1152/5677 [=====>........................] - ETA: 12:01 - loss: 0.6824 - acc: 0.5599
1216/5677 [=====>........................] - ETA: 11:53 - loss: 0.6824 - acc: 0.5609
1280/5677 [=====>........................] - ETA: 11:43 - loss: 0.6823 - acc: 0.5594
1344/5677 [======>.......................] - ETA: 11:34 - loss: 0.6832 - acc: 0.5595
1408/5677 [======>.......................] - ETA: 11:23 - loss: 0.6815 - acc: 0.5646
1472/5677 [======>.......................] - ETA: 11:14 - loss: 0.6808 - acc: 0.5659
1536/5677 [=======>......................] - ETA: 11:05 - loss: 0.6798 - acc: 0.5677
1600/5677 [=======>......................] - ETA: 10:55 - loss: 0.6815 - acc: 0.5650
1664/5677 [=======>......................] - ETA: 10:46 - loss: 0.6812 - acc: 0.5661
1728/5677 [========>.....................] - ETA: 10:36 - loss: 0.6816 - acc: 0.5660
1792/5677 [========>.....................] - ETA: 10:27 - loss: 0.6810 - acc: 0.5698
1856/5677 [========>.....................] - ETA: 10:16 - loss: 0.6807 - acc: 0.5684
1920/5677 [=========>....................] - ETA: 10:05 - loss: 0.6809 - acc: 0.5672
1984/5677 [=========>....................] - ETA: 9:55 - loss: 0.6799 - acc: 0.5706 
2048/5677 [=========>....................] - ETA: 9:45 - loss: 0.6795 - acc: 0.5708
2112/5677 [==========>...................] - ETA: 9:35 - loss: 0.6788 - acc: 0.5739
2176/5677 [==========>...................] - ETA: 9:25 - loss: 0.6789 - acc: 0.5726
2240/5677 [==========>...................] - ETA: 9:15 - loss: 0.6797 - acc: 0.5692
2304/5677 [===========>..................] - ETA: 9:05 - loss: 0.6785 - acc: 0.5716
2368/5677 [===========>..................] - ETA: 8:55 - loss: 0.6782 - acc: 0.5731
2432/5677 [===========>..................] - ETA: 8:43 - loss: 0.6773 - acc: 0.5740
2496/5677 [============>.................] - ETA: 8:32 - loss: 0.6773 - acc: 0.5753
2560/5677 [============>.................] - ETA: 8:22 - loss: 0.6786 - acc: 0.5707
2624/5677 [============>.................] - ETA: 8:11 - loss: 0.6791 - acc: 0.5709
2688/5677 [=============>................] - ETA: 8:01 - loss: 0.6791 - acc: 0.5707
2752/5677 [=============>................] - ETA: 7:50 - loss: 0.6786 - acc: 0.5709
2816/5677 [=============>................] - ETA: 7:40 - loss: 0.6778 - acc: 0.5728
2880/5677 [==============>...............] - ETA: 7:30 - loss: 0.6773 - acc: 0.5726
2944/5677 [==============>...............] - ETA: 7:21 - loss: 0.6771 - acc: 0.5724
3008/5677 [==============>...............] - ETA: 7:10 - loss: 0.6770 - acc: 0.5718
3072/5677 [===============>..............] - ETA: 7:01 - loss: 0.6771 - acc: 0.5729
3136/5677 [===============>..............] - ETA: 6:51 - loss: 0.6767 - acc: 0.5740
3200/5677 [===============>..............] - ETA: 6:40 - loss: 0.6766 - acc: 0.5747
3264/5677 [================>.............] - ETA: 6:30 - loss: 0.6771 - acc: 0.5738
3328/5677 [================>.............] - ETA: 6:20 - loss: 0.6780 - acc: 0.5724
3392/5677 [================>.............] - ETA: 6:10 - loss: 0.6786 - acc: 0.5725
3456/5677 [=================>............] - ETA: 6:00 - loss: 0.6792 - acc: 0.5726
3520/5677 [=================>............] - ETA: 5:49 - loss: 0.6803 - acc: 0.5710
3584/5677 [=================>............] - ETA: 5:40 - loss: 0.6804 - acc: 0.5703
3648/5677 [==================>...........] - ETA: 5:29 - loss: 0.6804 - acc: 0.5696
3712/5677 [==================>...........] - ETA: 5:19 - loss: 0.6808 - acc: 0.5676
3776/5677 [==================>...........] - ETA: 5:09 - loss: 0.6813 - acc: 0.5675
3840/5677 [===================>..........] - ETA: 4:58 - loss: 0.6811 - acc: 0.5682
3904/5677 [===================>..........] - ETA: 4:48 - loss: 0.6807 - acc: 0.5686
3968/5677 [===================>..........] - ETA: 4:37 - loss: 0.6809 - acc: 0.5683
4032/5677 [====================>.........] - ETA: 4:27 - loss: 0.6807 - acc: 0.5677
4096/5677 [====================>.........] - ETA: 4:16 - loss: 0.6805 - acc: 0.5691
4160/5677 [====================>.........] - ETA: 4:06 - loss: 0.6801 - acc: 0.5707
4224/5677 [=====================>........] - ETA: 3:56 - loss: 0.6803 - acc: 0.5694
4288/5677 [=====================>........] - ETA: 3:45 - loss: 0.6799 - acc: 0.5700
4352/5677 [=====================>........] - ETA: 3:34 - loss: 0.6800 - acc: 0.5696
4416/5677 [======================>.......] - ETA: 3:24 - loss: 0.6801 - acc: 0.5695
4480/5677 [======================>.......] - ETA: 3:14 - loss: 0.6802 - acc: 0.5701
4544/5677 [=======================>......] - ETA: 3:04 - loss: 0.6801 - acc: 0.5706
4608/5677 [=======================>......] - ETA: 2:53 - loss: 0.6797 - acc: 0.5714
4672/5677 [=======================>......] - ETA: 2:43 - loss: 0.6801 - acc: 0.5700
4736/5677 [========================>.....] - ETA: 2:32 - loss: 0.6801 - acc: 0.5703
4800/5677 [========================>.....] - ETA: 2:22 - loss: 0.6805 - acc: 0.5694
4864/5677 [========================>.....] - ETA: 2:12 - loss: 0.6803 - acc: 0.5697
4928/5677 [=========================>....] - ETA: 2:01 - loss: 0.6804 - acc: 0.5692
4992/5677 [=========================>....] - ETA: 1:51 - loss: 0.6802 - acc: 0.5695
5056/5677 [=========================>....] - ETA: 1:40 - loss: 0.6802 - acc: 0.5698
5120/5677 [==========================>...] - ETA: 1:30 - loss: 0.6803 - acc: 0.5697
5184/5677 [==========================>...] - ETA: 1:20 - loss: 0.6805 - acc: 0.5691
5248/5677 [==========================>...] - ETA: 1:09 - loss: 0.6802 - acc: 0.5692
5312/5677 [===========================>..] - ETA: 59s - loss: 0.6804 - acc: 0.5689 
5376/5677 [===========================>..] - ETA: 48s - loss: 0.6798 - acc: 0.5694
5440/5677 [===========================>..] - ETA: 38s - loss: 0.6803 - acc: 0.5686
5504/5677 [============================>.] - ETA: 27s - loss: 0.6802 - acc: 0.5689
5568/5677 [============================>.] - ETA: 17s - loss: 0.6795 - acc: 0.5708
5632/5677 [============================>.] - ETA: 7s - loss: 0.6796 - acc: 0.5705 
5677/5677 [==============================] - 952s 168ms/step - loss: 0.6798 - acc: 0.5698 - val_loss: 0.7009 - val_acc: 0.5309

Epoch 00006: val_acc did not improve from 0.54675
Epoch 7/10

  64/5677 [..............................] - ETA: 14:40 - loss: 0.6382 - acc: 0.7031
 128/5677 [..............................] - ETA: 14:41 - loss: 0.6567 - acc: 0.6328
 192/5677 [>.............................] - ETA: 14:10 - loss: 0.6616 - acc: 0.6354
 256/5677 [>.............................] - ETA: 13:54 - loss: 0.6672 - acc: 0.6211
 320/5677 [>.............................] - ETA: 13:09 - loss: 0.6731 - acc: 0.5938
 384/5677 [=>............................] - ETA: 12:57 - loss: 0.6684 - acc: 0.6016
 448/5677 [=>............................] - ETA: 12:46 - loss: 0.6745 - acc: 0.5893
 512/5677 [=>............................] - ETA: 12:38 - loss: 0.6747 - acc: 0.5859
 576/5677 [==>...........................] - ETA: 12:32 - loss: 0.6762 - acc: 0.5764
 640/5677 [==>...........................] - ETA: 12:28 - loss: 0.6754 - acc: 0.5813
 704/5677 [==>...........................] - ETA: 12:19 - loss: 0.6756 - acc: 0.5795
 768/5677 [===>..........................] - ETA: 12:10 - loss: 0.6774 - acc: 0.5716
 832/5677 [===>..........................] - ETA: 12:00 - loss: 0.6733 - acc: 0.5769
 896/5677 [===>..........................] - ETA: 11:50 - loss: 0.6761 - acc: 0.5714
 960/5677 [====>.........................] - ETA: 11:39 - loss: 0.6781 - acc: 0.5635
1024/5677 [====>.........................] - ETA: 11:29 - loss: 0.6768 - acc: 0.5645
1088/5677 [====>.........................] - ETA: 11:20 - loss: 0.6748 - acc: 0.5689
1152/5677 [=====>........................] - ETA: 11:10 - loss: 0.6756 - acc: 0.5694
1216/5677 [=====>........................] - ETA: 11:06 - loss: 0.6749 - acc: 0.5715
1280/5677 [=====>........................] - ETA: 11:01 - loss: 0.6768 - acc: 0.5695
1344/5677 [======>.......................] - ETA: 10:56 - loss: 0.6748 - acc: 0.5744
1408/5677 [======>.......................] - ETA: 10:47 - loss: 0.6744 - acc: 0.5767
1472/5677 [======>.......................] - ETA: 10:40 - loss: 0.6741 - acc: 0.5768
1536/5677 [=======>......................] - ETA: 10:34 - loss: 0.6736 - acc: 0.5794
1600/5677 [=======>......................] - ETA: 10:27 - loss: 0.6739 - acc: 0.5781
1664/5677 [=======>......................] - ETA: 10:19 - loss: 0.6732 - acc: 0.5811
1728/5677 [========>.....................] - ETA: 10:10 - loss: 0.6736 - acc: 0.5822
1792/5677 [========>.....................] - ETA: 10:01 - loss: 0.6748 - acc: 0.5787
1856/5677 [========>.....................] - ETA: 9:54 - loss: 0.6755 - acc: 0.5776 
1920/5677 [=========>....................] - ETA: 9:45 - loss: 0.6757 - acc: 0.5760
1984/5677 [=========>....................] - ETA: 9:37 - loss: 0.6754 - acc: 0.5771
2048/5677 [=========>....................] - ETA: 9:26 - loss: 0.6748 - acc: 0.5771
2112/5677 [==========>...................] - ETA: 9:19 - loss: 0.6753 - acc: 0.5748
2176/5677 [==========>...................] - ETA: 9:12 - loss: 0.6746 - acc: 0.5758
2240/5677 [==========>...................] - ETA: 9:03 - loss: 0.6747 - acc: 0.5768
2304/5677 [===========>..................] - ETA: 8:54 - loss: 0.6750 - acc: 0.5755
2368/5677 [===========>..................] - ETA: 8:45 - loss: 0.6746 - acc: 0.5760
2432/5677 [===========>..................] - ETA: 8:34 - loss: 0.6763 - acc: 0.5732
2496/5677 [============>.................] - ETA: 8:26 - loss: 0.6775 - acc: 0.5697
2560/5677 [============>.................] - ETA: 8:16 - loss: 0.6784 - acc: 0.5676
2624/5677 [============>.................] - ETA: 8:08 - loss: 0.6773 - acc: 0.5682
2688/5677 [=============>................] - ETA: 7:59 - loss: 0.6772 - acc: 0.5688
2752/5677 [=============>................] - ETA: 7:48 - loss: 0.6782 - acc: 0.5680
2816/5677 [=============>................] - ETA: 7:38 - loss: 0.6777 - acc: 0.5707
2880/5677 [==============>...............] - ETA: 7:28 - loss: 0.6773 - acc: 0.5705
2944/5677 [==============>...............] - ETA: 7:17 - loss: 0.6772 - acc: 0.5696
3008/5677 [==============>...............] - ETA: 7:07 - loss: 0.6768 - acc: 0.5701
3072/5677 [===============>..............] - ETA: 6:56 - loss: 0.6764 - acc: 0.5706
3136/5677 [===============>..............] - ETA: 6:46 - loss: 0.6767 - acc: 0.5708
3200/5677 [===============>..............] - ETA: 6:36 - loss: 0.6766 - acc: 0.5709
3264/5677 [================>.............] - ETA: 6:26 - loss: 0.6766 - acc: 0.5711
3328/5677 [================>.............] - ETA: 6:16 - loss: 0.6776 - acc: 0.5694
3392/5677 [================>.............] - ETA: 6:06 - loss: 0.6776 - acc: 0.5696
3456/5677 [=================>............] - ETA: 5:56 - loss: 0.6780 - acc: 0.5700
3520/5677 [=================>............] - ETA: 5:45 - loss: 0.6769 - acc: 0.5733
3584/5677 [=================>............] - ETA: 5:35 - loss: 0.6771 - acc: 0.5734
3648/5677 [==================>...........] - ETA: 5:24 - loss: 0.6773 - acc: 0.5735
3712/5677 [==================>...........] - ETA: 5:14 - loss: 0.6766 - acc: 0.5757
3776/5677 [==================>...........] - ETA: 5:04 - loss: 0.6768 - acc: 0.5760
3840/5677 [===================>..........] - ETA: 4:54 - loss: 0.6764 - acc: 0.5760
3904/5677 [===================>..........] - ETA: 4:43 - loss: 0.6758 - acc: 0.5761
3968/5677 [===================>..........] - ETA: 4:33 - loss: 0.6767 - acc: 0.5759
4032/5677 [====================>.........] - ETA: 4:23 - loss: 0.6769 - acc: 0.5749
4096/5677 [====================>.........] - ETA: 4:12 - loss: 0.6773 - acc: 0.5742
4160/5677 [====================>.........] - ETA: 4:02 - loss: 0.6769 - acc: 0.5755
4224/5677 [=====================>........] - ETA: 3:52 - loss: 0.6772 - acc: 0.5743
4288/5677 [=====================>........] - ETA: 3:42 - loss: 0.6769 - acc: 0.5753
4352/5677 [=====================>........] - ETA: 3:32 - loss: 0.6767 - acc: 0.5761
4416/5677 [======================>.......] - ETA: 3:21 - loss: 0.6763 - acc: 0.5770
4480/5677 [======================>.......] - ETA: 3:11 - loss: 0.6765 - acc: 0.5768
4544/5677 [=======================>......] - ETA: 3:01 - loss: 0.6769 - acc: 0.5757
4608/5677 [=======================>......] - ETA: 2:51 - loss: 0.6772 - acc: 0.5753
4672/5677 [=======================>......] - ETA: 2:41 - loss: 0.6776 - acc: 0.5753
4736/5677 [========================>.....] - ETA: 2:30 - loss: 0.6776 - acc: 0.5754
4800/5677 [========================>.....] - ETA: 2:20 - loss: 0.6777 - acc: 0.5748
4864/5677 [========================>.....] - ETA: 2:10 - loss: 0.6778 - acc: 0.5742
4928/5677 [=========================>....] - ETA: 2:00 - loss: 0.6785 - acc: 0.5724
4992/5677 [=========================>....] - ETA: 1:49 - loss: 0.6787 - acc: 0.5723
5056/5677 [=========================>....] - ETA: 1:39 - loss: 0.6786 - acc: 0.5724
5120/5677 [==========================>...] - ETA: 1:29 - loss: 0.6787 - acc: 0.5725
5184/5677 [==========================>...] - ETA: 1:19 - loss: 0.6791 - acc: 0.5718
5248/5677 [==========================>...] - ETA: 1:08 - loss: 0.6790 - acc: 0.5715
5312/5677 [===========================>..] - ETA: 58s - loss: 0.6788 - acc: 0.5717 
5376/5677 [===========================>..] - ETA: 48s - loss: 0.6788 - acc: 0.5711
5440/5677 [===========================>..] - ETA: 38s - loss: 0.6787 - acc: 0.5719
5504/5677 [============================>.] - ETA: 27s - loss: 0.6788 - acc: 0.5725
5568/5677 [============================>.] - ETA: 17s - loss: 0.6792 - acc: 0.5715
5632/5677 [============================>.] - ETA: 7s - loss: 0.6793 - acc: 0.5710 
5677/5677 [==============================] - 951s 168ms/step - loss: 0.6792 - acc: 0.5714 - val_loss: 0.6865 - val_acc: 0.5515

Epoch 00007: val_acc improved from 0.54675 to 0.55151, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window20/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 8/10

  64/5677 [..............................] - ETA: 15:44 - loss: 0.6836 - acc: 0.5156
 128/5677 [..............................] - ETA: 15:36 - loss: 0.6829 - acc: 0.5547
 192/5677 [>.............................] - ETA: 15:20 - loss: 0.6752 - acc: 0.5885
 256/5677 [>.............................] - ETA: 14:53 - loss: 0.6668 - acc: 0.6133
 320/5677 [>.............................] - ETA: 14:45 - loss: 0.6775 - acc: 0.5875
 384/5677 [=>............................] - ETA: 14:38 - loss: 0.6869 - acc: 0.5729
 448/5677 [=>............................] - ETA: 14:12 - loss: 0.6852 - acc: 0.5848
 512/5677 [=>............................] - ETA: 14:00 - loss: 0.6789 - acc: 0.5879
 576/5677 [==>...........................] - ETA: 13:51 - loss: 0.6747 - acc: 0.5938
 640/5677 [==>...........................] - ETA: 13:35 - loss: 0.6778 - acc: 0.5844
 704/5677 [==>...........................] - ETA: 13:23 - loss: 0.6782 - acc: 0.5838
 768/5677 [===>..........................] - ETA: 13:12 - loss: 0.6848 - acc: 0.5703
 832/5677 [===>..........................] - ETA: 13:03 - loss: 0.6857 - acc: 0.5709
 896/5677 [===>..........................] - ETA: 12:53 - loss: 0.6859 - acc: 0.5692
 960/5677 [====>.........................] - ETA: 12:41 - loss: 0.6854 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 12:36 - loss: 0.6837 - acc: 0.5752
1088/5677 [====>.........................] - ETA: 12:26 - loss: 0.6842 - acc: 0.5772
1152/5677 [=====>........................] - ETA: 12:19 - loss: 0.6879 - acc: 0.5720
1216/5677 [=====>........................] - ETA: 12:08 - loss: 0.6887 - acc: 0.5699
1280/5677 [=====>........................] - ETA: 12:01 - loss: 0.6864 - acc: 0.5742
1344/5677 [======>.......................] - ETA: 11:48 - loss: 0.6860 - acc: 0.5737
1408/5677 [======>.......................] - ETA: 11:38 - loss: 0.6859 - acc: 0.5753
1472/5677 [======>.......................] - ETA: 11:28 - loss: 0.6860 - acc: 0.5734
1536/5677 [=======>......................] - ETA: 11:13 - loss: 0.6845 - acc: 0.5775
1600/5677 [=======>......................] - ETA: 11:05 - loss: 0.6833 - acc: 0.5806
1664/5677 [=======>......................] - ETA: 10:53 - loss: 0.6827 - acc: 0.5775
1728/5677 [========>.....................] - ETA: 10:44 - loss: 0.6831 - acc: 0.5781
1792/5677 [========>.....................] - ETA: 10:33 - loss: 0.6837 - acc: 0.5748
1856/5677 [========>.....................] - ETA: 10:24 - loss: 0.6835 - acc: 0.5765
1920/5677 [=========>....................] - ETA: 10:12 - loss: 0.6837 - acc: 0.5750
1984/5677 [=========>....................] - ETA: 10:02 - loss: 0.6832 - acc: 0.5771
2048/5677 [=========>....................] - ETA: 9:52 - loss: 0.6824 - acc: 0.5771 
2112/5677 [==========>...................] - ETA: 9:41 - loss: 0.6821 - acc: 0.5772
2176/5677 [==========>...................] - ETA: 9:30 - loss: 0.6816 - acc: 0.5781
2240/5677 [==========>...................] - ETA: 9:20 - loss: 0.6816 - acc: 0.5790
2304/5677 [===========>..................] - ETA: 9:09 - loss: 0.6817 - acc: 0.5790
2368/5677 [===========>..................] - ETA: 8:58 - loss: 0.6814 - acc: 0.5794
2432/5677 [===========>..................] - ETA: 8:48 - loss: 0.6814 - acc: 0.5802
2496/5677 [============>.................] - ETA: 8:38 - loss: 0.6811 - acc: 0.5797
2560/5677 [============>.................] - ETA: 8:27 - loss: 0.6825 - acc: 0.5758
2624/5677 [============>.................] - ETA: 8:17 - loss: 0.6828 - acc: 0.5751
2688/5677 [=============>................] - ETA: 8:06 - loss: 0.6825 - acc: 0.5737
2752/5677 [=============>................] - ETA: 7:57 - loss: 0.6811 - acc: 0.5767
2816/5677 [=============>................] - ETA: 7:45 - loss: 0.6814 - acc: 0.5767
2880/5677 [==============>...............] - ETA: 7:35 - loss: 0.6818 - acc: 0.5760
2944/5677 [==============>...............] - ETA: 7:24 - loss: 0.6813 - acc: 0.5774
3008/5677 [==============>...............] - ETA: 7:13 - loss: 0.6813 - acc: 0.5768
3072/5677 [===============>..............] - ETA: 7:03 - loss: 0.6819 - acc: 0.5758
3136/5677 [===============>..............] - ETA: 6:52 - loss: 0.6815 - acc: 0.5765
3200/5677 [===============>..............] - ETA: 6:42 - loss: 0.6809 - acc: 0.5781
3264/5677 [================>.............] - ETA: 6:31 - loss: 0.6814 - acc: 0.5766
3328/5677 [================>.............] - ETA: 6:20 - loss: 0.6812 - acc: 0.5760
3392/5677 [================>.............] - ETA: 6:10 - loss: 0.6808 - acc: 0.5767
3456/5677 [=================>............] - ETA: 6:00 - loss: 0.6808 - acc: 0.5764
3520/5677 [=================>............] - ETA: 5:50 - loss: 0.6803 - acc: 0.5773
3584/5677 [=================>............] - ETA: 5:39 - loss: 0.6805 - acc: 0.5767
3648/5677 [==================>...........] - ETA: 5:28 - loss: 0.6804 - acc: 0.5784
3712/5677 [==================>...........] - ETA: 5:17 - loss: 0.6799 - acc: 0.5789
3776/5677 [==================>...........] - ETA: 5:07 - loss: 0.6797 - acc: 0.5792
3840/5677 [===================>..........] - ETA: 4:56 - loss: 0.6801 - acc: 0.5789
3904/5677 [===================>..........] - ETA: 4:45 - loss: 0.6801 - acc: 0.5779
3968/5677 [===================>..........] - ETA: 4:35 - loss: 0.6801 - acc: 0.5781
4032/5677 [====================>.........] - ETA: 4:24 - loss: 0.6803 - acc: 0.5776
4096/5677 [====================>.........] - ETA: 4:14 - loss: 0.6811 - acc: 0.5764
4160/5677 [====================>.........] - ETA: 4:04 - loss: 0.6809 - acc: 0.5774
4224/5677 [=====================>........] - ETA: 3:53 - loss: 0.6810 - acc: 0.5774
4288/5677 [=====================>........] - ETA: 3:43 - loss: 0.6813 - acc: 0.5767
4352/5677 [=====================>........] - ETA: 3:32 - loss: 0.6810 - acc: 0.5767
4416/5677 [======================>.......] - ETA: 3:22 - loss: 0.6806 - acc: 0.5772
4480/5677 [======================>.......] - ETA: 3:11 - loss: 0.6812 - acc: 0.5761
4544/5677 [=======================>......] - ETA: 3:01 - loss: 0.6808 - acc: 0.5766
4608/5677 [=======================>......] - ETA: 2:51 - loss: 0.6808 - acc: 0.5764
4672/5677 [=======================>......] - ETA: 2:40 - loss: 0.6807 - acc: 0.5773
4736/5677 [========================>.....] - ETA: 2:30 - loss: 0.6813 - acc: 0.5762
4800/5677 [========================>.....] - ETA: 2:19 - loss: 0.6813 - acc: 0.5758
4864/5677 [========================>.....] - ETA: 2:09 - loss: 0.6811 - acc: 0.5767
4928/5677 [=========================>....] - ETA: 1:59 - loss: 0.6810 - acc: 0.5763
4992/5677 [=========================>....] - ETA: 1:49 - loss: 0.6807 - acc: 0.5765
5056/5677 [=========================>....] - ETA: 1:38 - loss: 0.6807 - acc: 0.5761
5120/5677 [==========================>...] - ETA: 1:28 - loss: 0.6810 - acc: 0.5750
5184/5677 [==========================>...] - ETA: 1:18 - loss: 0.6807 - acc: 0.5756
5248/5677 [==========================>...] - ETA: 1:08 - loss: 0.6802 - acc: 0.5764
5312/5677 [===========================>..] - ETA: 57s - loss: 0.6802 - acc: 0.5759 
5376/5677 [===========================>..] - ETA: 47s - loss: 0.6800 - acc: 0.5759
5440/5677 [===========================>..] - ETA: 37s - loss: 0.6799 - acc: 0.5752
5504/5677 [============================>.] - ETA: 27s - loss: 0.6796 - acc: 0.5758
5568/5677 [============================>.] - ETA: 17s - loss: 0.6793 - acc: 0.5756
5632/5677 [============================>.] - ETA: 7s - loss: 0.6789 - acc: 0.5765 
5677/5677 [==============================] - 939s 165ms/step - loss: 0.6789 - acc: 0.5764 - val_loss: 0.6785 - val_acc: 0.5737

Epoch 00008: val_acc improved from 0.55151 to 0.57369, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window20/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 9/10

  64/5677 [..............................] - ETA: 13:07 - loss: 0.6948 - acc: 0.4688
 128/5677 [..............................] - ETA: 13:10 - loss: 0.6673 - acc: 0.5781
 192/5677 [>.............................] - ETA: 12:33 - loss: 0.6667 - acc: 0.5885
 256/5677 [>.............................] - ETA: 12:33 - loss: 0.6747 - acc: 0.5742
 320/5677 [>.............................] - ETA: 12:41 - loss: 0.6739 - acc: 0.5719
 384/5677 [=>............................] - ETA: 12:26 - loss: 0.6712 - acc: 0.5755
 448/5677 [=>............................] - ETA: 12:16 - loss: 0.6703 - acc: 0.5826
 512/5677 [=>............................] - ETA: 12:11 - loss: 0.6691 - acc: 0.5820
 576/5677 [==>...........................] - ETA: 12:06 - loss: 0.6709 - acc: 0.5799
 640/5677 [==>...........................] - ETA: 12:01 - loss: 0.6768 - acc: 0.5703
 704/5677 [==>...........................] - ETA: 11:52 - loss: 0.6768 - acc: 0.5724
 768/5677 [===>..........................] - ETA: 11:35 - loss: 0.6774 - acc: 0.5703
 832/5677 [===>..........................] - ETA: 11:28 - loss: 0.6780 - acc: 0.5673
 896/5677 [===>..........................] - ETA: 11:16 - loss: 0.6750 - acc: 0.5737
 960/5677 [====>.........................] - ETA: 11:09 - loss: 0.6742 - acc: 0.5760
1024/5677 [====>.........................] - ETA: 11:06 - loss: 0.6741 - acc: 0.5771
1088/5677 [====>.........................] - ETA: 10:59 - loss: 0.6726 - acc: 0.5809
1152/5677 [=====>........................] - ETA: 10:55 - loss: 0.6736 - acc: 0.5807
1216/5677 [=====>........................] - ETA: 10:48 - loss: 0.6743 - acc: 0.5839
1280/5677 [=====>........................] - ETA: 10:40 - loss: 0.6744 - acc: 0.5852
1344/5677 [======>.......................] - ETA: 10:34 - loss: 0.6757 - acc: 0.5863
1408/5677 [======>.......................] - ETA: 10:28 - loss: 0.6757 - acc: 0.5888
1472/5677 [======>.......................] - ETA: 10:24 - loss: 0.6741 - acc: 0.5938
1536/5677 [=======>......................] - ETA: 10:16 - loss: 0.6754 - acc: 0.5931
1600/5677 [=======>......................] - ETA: 10:08 - loss: 0.6748 - acc: 0.5938
1664/5677 [=======>......................] - ETA: 10:00 - loss: 0.6768 - acc: 0.5895
1728/5677 [========>.....................] - ETA: 9:52 - loss: 0.6767 - acc: 0.5932 
1792/5677 [========>.....................] - ETA: 9:43 - loss: 0.6769 - acc: 0.5921
1856/5677 [========>.....................] - ETA: 9:35 - loss: 0.6777 - acc: 0.5889
1920/5677 [=========>....................] - ETA: 9:26 - loss: 0.6779 - acc: 0.5891
1984/5677 [=========>....................] - ETA: 9:15 - loss: 0.6774 - acc: 0.5887
2048/5677 [=========>....................] - ETA: 9:05 - loss: 0.6780 - acc: 0.5879
2112/5677 [==========>...................] - ETA: 8:55 - loss: 0.6777 - acc: 0.5876
2176/5677 [==========>...................] - ETA: 8:47 - loss: 0.6771 - acc: 0.5892
2240/5677 [==========>...................] - ETA: 8:38 - loss: 0.6780 - acc: 0.5862
2304/5677 [===========>..................] - ETA: 8:29 - loss: 0.6788 - acc: 0.5851
2368/5677 [===========>..................] - ETA: 8:19 - loss: 0.6780 - acc: 0.5861
2432/5677 [===========>..................] - ETA: 8:11 - loss: 0.6778 - acc: 0.5863
2496/5677 [============>.................] - ETA: 8:01 - loss: 0.6774 - acc: 0.5857
2560/5677 [============>.................] - ETA: 7:51 - loss: 0.6771 - acc: 0.5863
2624/5677 [============>.................] - ETA: 7:43 - loss: 0.6770 - acc: 0.5854
2688/5677 [=============>................] - ETA: 7:34 - loss: 0.6769 - acc: 0.5844
2752/5677 [=============>................] - ETA: 7:25 - loss: 0.6779 - acc: 0.5825
2816/5677 [=============>................] - ETA: 7:16 - loss: 0.6777 - acc: 0.5817
2880/5677 [==============>...............] - ETA: 7:07 - loss: 0.6772 - acc: 0.5837
2944/5677 [==============>...............] - ETA: 6:57 - loss: 0.6768 - acc: 0.5859
3008/5677 [==============>...............] - ETA: 6:47 - loss: 0.6760 - acc: 0.5874
3072/5677 [===============>..............] - ETA: 6:38 - loss: 0.6755 - acc: 0.5876
3136/5677 [===============>..............] - ETA: 6:28 - loss: 0.6757 - acc: 0.5871
3200/5677 [===============>..............] - ETA: 6:19 - loss: 0.6758 - acc: 0.5869
3264/5677 [================>.............] - ETA: 6:10 - loss: 0.6756 - acc: 0.5879
3328/5677 [================>.............] - ETA: 6:00 - loss: 0.6756 - acc: 0.5868
3392/5677 [================>.............] - ETA: 5:50 - loss: 0.6753 - acc: 0.5876
3456/5677 [=================>............] - ETA: 5:42 - loss: 0.6745 - acc: 0.5897
3520/5677 [=================>............] - ETA: 5:33 - loss: 0.6739 - acc: 0.5898
3584/5677 [=================>............] - ETA: 5:23 - loss: 0.6746 - acc: 0.5887
3648/5677 [==================>...........] - ETA: 5:13 - loss: 0.6749 - acc: 0.5880
3712/5677 [==================>...........] - ETA: 5:03 - loss: 0.6749 - acc: 0.5881
3776/5677 [==================>...........] - ETA: 4:53 - loss: 0.6744 - acc: 0.5892
3840/5677 [===================>..........] - ETA: 4:44 - loss: 0.6752 - acc: 0.5883
3904/5677 [===================>..........] - ETA: 4:35 - loss: 0.6757 - acc: 0.5876
3968/5677 [===================>..........] - ETA: 4:25 - loss: 0.6759 - acc: 0.5880
4032/5677 [====================>.........] - ETA: 4:16 - loss: 0.6760 - acc: 0.5871
4096/5677 [====================>.........] - ETA: 4:06 - loss: 0.6755 - acc: 0.5881
4160/5677 [====================>.........] - ETA: 3:56 - loss: 0.6755 - acc: 0.5875
4224/5677 [=====================>........] - ETA: 3:46 - loss: 0.6760 - acc: 0.5871
4288/5677 [=====================>........] - ETA: 3:36 - loss: 0.6760 - acc: 0.5875
4352/5677 [=====================>........] - ETA: 3:26 - loss: 0.6761 - acc: 0.5859
4416/5677 [======================>.......] - ETA: 3:16 - loss: 0.6760 - acc: 0.5863
4480/5677 [======================>.......] - ETA: 3:07 - loss: 0.6757 - acc: 0.5864
4544/5677 [=======================>......] - ETA: 2:57 - loss: 0.6761 - acc: 0.5854
4608/5677 [=======================>......] - ETA: 2:47 - loss: 0.6761 - acc: 0.5864
4672/5677 [=======================>......] - ETA: 2:37 - loss: 0.6761 - acc: 0.5848
4736/5677 [========================>.....] - ETA: 2:28 - loss: 0.6761 - acc: 0.5842
4800/5677 [========================>.....] - ETA: 2:18 - loss: 0.6759 - acc: 0.5846
4864/5677 [========================>.....] - ETA: 2:08 - loss: 0.6756 - acc: 0.5847
4928/5677 [=========================>....] - ETA: 1:58 - loss: 0.6755 - acc: 0.5854
4992/5677 [=========================>....] - ETA: 1:48 - loss: 0.6755 - acc: 0.5845
5056/5677 [=========================>....] - ETA: 1:38 - loss: 0.6754 - acc: 0.5848
5120/5677 [==========================>...] - ETA: 1:28 - loss: 0.6752 - acc: 0.5846
5184/5677 [==========================>...] - ETA: 1:18 - loss: 0.6752 - acc: 0.5847
5248/5677 [==========================>...] - ETA: 1:08 - loss: 0.6757 - acc: 0.5842
5312/5677 [===========================>..] - ETA: 58s - loss: 0.6760 - acc: 0.5838 
5376/5677 [===========================>..] - ETA: 48s - loss: 0.6760 - acc: 0.5833
5440/5677 [===========================>..] - ETA: 38s - loss: 0.6763 - acc: 0.5825
5504/5677 [============================>.] - ETA: 27s - loss: 0.6761 - acc: 0.5832
5568/5677 [============================>.] - ETA: 17s - loss: 0.6761 - acc: 0.5828
5632/5677 [============================>.] - ETA: 7s - loss: 0.6762 - acc: 0.5829 
5677/5677 [==============================] - 957s 169ms/step - loss: 0.6763 - acc: 0.5827 - val_loss: 0.6807 - val_acc: 0.5705

Epoch 00009: val_acc did not improve from 0.57369
Epoch 10/10

  64/5677 [..............................] - ETA: 17:40 - loss: 0.6202 - acc: 0.6719
 128/5677 [..............................] - ETA: 16:59 - loss: 0.6391 - acc: 0.6641
 192/5677 [>.............................] - ETA: 17:05 - loss: 0.6547 - acc: 0.6458
 256/5677 [>.............................] - ETA: 16:31 - loss: 0.6706 - acc: 0.6133
 320/5677 [>.............................] - ETA: 16:08 - loss: 0.6709 - acc: 0.6062
 384/5677 [=>............................] - ETA: 15:50 - loss: 0.6798 - acc: 0.5833
 448/5677 [=>............................] - ETA: 15:35 - loss: 0.6777 - acc: 0.5848
 512/5677 [=>............................] - ETA: 15:24 - loss: 0.6749 - acc: 0.5879
 576/5677 [==>...........................] - ETA: 15:08 - loss: 0.6747 - acc: 0.5885
 640/5677 [==>...........................] - ETA: 14:55 - loss: 0.6771 - acc: 0.5859
 704/5677 [==>...........................] - ETA: 14:43 - loss: 0.6752 - acc: 0.5909
 768/5677 [===>..........................] - ETA: 14:28 - loss: 0.6779 - acc: 0.5807
 832/5677 [===>..........................] - ETA: 14:17 - loss: 0.6791 - acc: 0.5769
 896/5677 [===>..........................] - ETA: 14:05 - loss: 0.6799 - acc: 0.5781
 960/5677 [====>.........................] - ETA: 13:52 - loss: 0.6782 - acc: 0.5792
1024/5677 [====>.........................] - ETA: 13:46 - loss: 0.6786 - acc: 0.5742
1088/5677 [====>.........................] - ETA: 13:37 - loss: 0.6784 - acc: 0.5744
1152/5677 [=====>........................] - ETA: 13:25 - loss: 0.6783 - acc: 0.5755
1216/5677 [=====>........................] - ETA: 13:12 - loss: 0.6776 - acc: 0.5773
1280/5677 [=====>........................] - ETA: 13:00 - loss: 0.6772 - acc: 0.5781
1344/5677 [======>.......................] - ETA: 12:47 - loss: 0.6763 - acc: 0.5796
1408/5677 [======>.......................] - ETA: 12:36 - loss: 0.6737 - acc: 0.5881
1472/5677 [======>.......................] - ETA: 12:24 - loss: 0.6727 - acc: 0.5897
1536/5677 [=======>......................] - ETA: 12:12 - loss: 0.6732 - acc: 0.5885
1600/5677 [=======>......................] - ETA: 12:01 - loss: 0.6729 - acc: 0.5894
1664/5677 [=======>......................] - ETA: 11:49 - loss: 0.6727 - acc: 0.5889
1728/5677 [========>.....................] - ETA: 11:38 - loss: 0.6737 - acc: 0.5851
1792/5677 [========>.....................] - ETA: 11:26 - loss: 0.6744 - acc: 0.5815
1856/5677 [========>.....................] - ETA: 11:13 - loss: 0.6754 - acc: 0.5808
1920/5677 [=========>....................] - ETA: 11:00 - loss: 0.6746 - acc: 0.5839
1984/5677 [=========>....................] - ETA: 10:47 - loss: 0.6740 - acc: 0.5852
2048/5677 [=========>....................] - ETA: 10:38 - loss: 0.6759 - acc: 0.5820
2112/5677 [==========>...................] - ETA: 10:26 - loss: 0.6765 - acc: 0.5824
2176/5677 [==========>...................] - ETA: 10:12 - loss: 0.6764 - acc: 0.5813
2240/5677 [==========>...................] - ETA: 10:00 - loss: 0.6760 - acc: 0.5813
2304/5677 [===========>..................] - ETA: 9:47 - loss: 0.6758 - acc: 0.5803 
2368/5677 [===========>..................] - ETA: 9:36 - loss: 0.6757 - acc: 0.5802
2432/5677 [===========>..................] - ETA: 9:25 - loss: 0.6756 - acc: 0.5806
2496/5677 [============>.................] - ETA: 9:12 - loss: 0.6749 - acc: 0.5809
2560/5677 [============>.................] - ETA: 9:01 - loss: 0.6754 - acc: 0.5781
2624/5677 [============>.................] - ETA: 8:49 - loss: 0.6738 - acc: 0.5816
2688/5677 [=============>................] - ETA: 8:38 - loss: 0.6732 - acc: 0.5833
2752/5677 [=============>................] - ETA: 8:26 - loss: 0.6732 - acc: 0.5836
2816/5677 [=============>................] - ETA: 8:14 - loss: 0.6726 - acc: 0.5845
2880/5677 [==============>...............] - ETA: 8:03 - loss: 0.6731 - acc: 0.5826
2944/5677 [==============>...............] - ETA: 7:51 - loss: 0.6734 - acc: 0.5819
3008/5677 [==============>...............] - ETA: 7:40 - loss: 0.6733 - acc: 0.5808
3072/5677 [===============>..............] - ETA: 7:28 - loss: 0.6730 - acc: 0.5814
3136/5677 [===============>..............] - ETA: 7:16 - loss: 0.6729 - acc: 0.5807
3200/5677 [===============>..............] - ETA: 7:03 - loss: 0.6722 - acc: 0.5831
3264/5677 [================>.............] - ETA: 6:52 - loss: 0.6718 - acc: 0.5833
3328/5677 [================>.............] - ETA: 6:39 - loss: 0.6725 - acc: 0.5802
3392/5677 [================>.............] - ETA: 6:28 - loss: 0.6726 - acc: 0.5790
3456/5677 [=================>............] - ETA: 6:16 - loss: 0.6729 - acc: 0.5764
3520/5677 [=================>............] - ETA: 6:05 - loss: 0.6733 - acc: 0.5750
3584/5677 [=================>............] - ETA: 5:53 - loss: 0.6729 - acc: 0.5756
3648/5677 [==================>...........] - ETA: 5:42 - loss: 0.6729 - acc: 0.5757
3712/5677 [==================>...........] - ETA: 5:30 - loss: 0.6731 - acc: 0.5754
3776/5677 [==================>...........] - ETA: 5:19 - loss: 0.6731 - acc: 0.5763
3840/5677 [===================>..........] - ETA: 5:08 - loss: 0.6723 - acc: 0.5776
3904/5677 [===================>..........] - ETA: 4:57 - loss: 0.6721 - acc: 0.5791
3968/5677 [===================>..........] - ETA: 4:46 - loss: 0.6717 - acc: 0.5806
4032/5677 [====================>.........] - ETA: 4:35 - loss: 0.6719 - acc: 0.5813
4096/5677 [====================>.........] - ETA: 4:24 - loss: 0.6720 - acc: 0.5815
4160/5677 [====================>.........] - ETA: 4:12 - loss: 0.6718 - acc: 0.5820
4224/5677 [=====================>........] - ETA: 4:02 - loss: 0.6717 - acc: 0.5817
4288/5677 [=====================>........] - ETA: 3:51 - loss: 0.6714 - acc: 0.5826
4352/5677 [=====================>........] - ETA: 3:40 - loss: 0.6722 - acc: 0.5813
4416/5677 [======================>.......] - ETA: 3:29 - loss: 0.6726 - acc: 0.5808
4480/5677 [======================>.......] - ETA: 3:18 - loss: 0.6728 - acc: 0.5806
4544/5677 [=======================>......] - ETA: 3:08 - loss: 0.6724 - acc: 0.5810
4608/5677 [=======================>......] - ETA: 2:57 - loss: 0.6735 - acc: 0.5790
4672/5677 [=======================>......] - ETA: 2:46 - loss: 0.6741 - acc: 0.5794
4736/5677 [========================>.....] - ETA: 2:35 - loss: 0.6746 - acc: 0.5777
4800/5677 [========================>.....] - ETA: 2:24 - loss: 0.6742 - acc: 0.5796
4864/5677 [========================>.....] - ETA: 2:14 - loss: 0.6746 - acc: 0.5792
4928/5677 [=========================>....] - ETA: 2:03 - loss: 0.6744 - acc: 0.5797
4992/5677 [=========================>....] - ETA: 1:52 - loss: 0.6743 - acc: 0.5807
5056/5677 [=========================>....] - ETA: 1:42 - loss: 0.6743 - acc: 0.5813
5120/5677 [==========================>...] - ETA: 1:31 - loss: 0.6742 - acc: 0.5814
5184/5677 [==========================>...] - ETA: 1:21 - loss: 0.6745 - acc: 0.5808
5248/5677 [==========================>...] - ETA: 1:10 - loss: 0.6741 - acc: 0.5806
5312/5677 [===========================>..] - ETA: 1:00 - loss: 0.6745 - acc: 0.5798
5376/5677 [===========================>..] - ETA: 49s - loss: 0.6749 - acc: 0.5787 
5440/5677 [===========================>..] - ETA: 38s - loss: 0.6747 - acc: 0.5796
5504/5677 [============================>.] - ETA: 28s - loss: 0.6745 - acc: 0.5799
5568/5677 [============================>.] - ETA: 17s - loss: 0.6747 - acc: 0.5790
5632/5677 [============================>.] - ETA: 7s - loss: 0.6752 - acc: 0.5783 
5677/5677 [==============================] - 963s 170ms/step - loss: 0.6757 - acc: 0.5774 - val_loss: 0.6908 - val_acc: 0.5626

Epoch 00010: val_acc did not improve from 0.57369
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fddac3da150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fddac3da150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fddac1d4050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fddac1d4050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75aed19d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe75aed19d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd746704d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd746704d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdd74665850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdd74665850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd743868d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd743868d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fddac132d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fddac132d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9543d3e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9543d3e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd74618650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd74618650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd98c3be450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd98c3be450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd74582d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd74582d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd74618290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd74618290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd74405710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd74405710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd74247b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd74247b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b47813d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b47813d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd7449c7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd7449c7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd74247e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd74247e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8d4045510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8d4045510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd38353a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd38353a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc5406db50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc5406db50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd7410eed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd7410eed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdab8641d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdab8641d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc4c7726d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc4c7726d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd3828a850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdd3828a850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc4c6303d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc4c6303d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc4c64a4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc4c64a4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd38290650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdd38290650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc4c5af110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc4c5af110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc4c5a8290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc4c5a8290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc4c2476d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc4c2476d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc4c24e210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc4c24e210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc4c57e810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc4c57e810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc4c1eafd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc4c1eafd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc147d4e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc147d4e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc4c0e7e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc4c0e7e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc147b2610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc147b2610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc4c224c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc4c224c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc146a3ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc146a3ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc14445b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc14445b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc146537d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc146537d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc1456e690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc1456e690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc14445f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc14445f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc14685690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc14685690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc146955d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc146955d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc14177710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc14177710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc046d9bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc046d9bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc1445af50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc1445af50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc1435c090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc1435c090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc045dcdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc045dcdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc04625b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc04625b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc14063e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc14063e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc1419c610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc1419c610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc044c1c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc044c1c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc042ed890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc042ed890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc041dfc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdc041dfc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc04279f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc04279f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc041fa390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc041fa390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc0447c550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc0447c550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc04200cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdc04200cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdbd4752b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdbd4752b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbd473bd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbd473bd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc040a33d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdc040a33d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbd46e3a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbd46e3a10>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 7:01
 128/1578 [=>............................] - ETA: 3:55
 192/1578 [==>...........................] - ETA: 2:52
 256/1578 [===>..........................] - ETA: 2:16
 320/1578 [=====>........................] - ETA: 1:54
 384/1578 [======>.......................] - ETA: 1:40
 448/1578 [=======>......................] - ETA: 1:28
 512/1578 [========>.....................] - ETA: 1:19
 576/1578 [=========>....................] - ETA: 1:12
 640/1578 [===========>..................] - ETA: 1:04
 704/1578 [============>.................] - ETA: 58s 
 768/1578 [=============>................] - ETA: 52s
 832/1578 [==============>...............] - ETA: 47s
 896/1578 [================>.............] - ETA: 42s
 960/1578 [=================>............] - ETA: 37s
1024/1578 [==================>...........] - ETA: 33s
1088/1578 [===================>..........] - ETA: 28s
1152/1578 [====================>.........] - ETA: 24s
1216/1578 [======================>.......] - ETA: 20s
1280/1578 [=======================>......] - ETA: 16s
1344/1578 [========================>.....] - ETA: 13s
1408/1578 [=========================>....] - ETA: 9s 
1472/1578 [==========================>...] - ETA: 5s
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 86s 55ms/step
loss: 0.6666206963766361
acc: 0.6001267424101159
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd8b47bf990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fd8b47bf990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd8b47f5490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fd8b47f5490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbb43417d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbb43417d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdbb4341510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdbb4341510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fddac38ee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fddac38ee10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd74766310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd74766310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdbb4341850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdbb4341850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd74768150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdd74768150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fddac231b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fddac231b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b4771710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b4771710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddac317950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fddac317950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fddac231d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fddac231d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b474b750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b474b750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdbb415fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdbb415fc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b449b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b449b1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b473ea10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b473ea10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8b45d9490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8b45d9490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b45d0450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b45d0450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8b43443d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8b43443d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b4101790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b4101790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b4073fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b4073fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8b4492a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8b4492a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b448db50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8b448db50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8b425d910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8b425d910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd894626e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd894626e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8946c4510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8946c4510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8b425dc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd8b425dc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8946c4210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8946c4210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd89471fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd89471fe10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd894617950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd894617950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc045ae550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdc045ae550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd89462c990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd89462c990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8943d3bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8943d3bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8942ed610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8942ed610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd874757210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd874757210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd89410d410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd89410d410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd89413af10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd89413af10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd89406c8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd89406c8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd87466c510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd87466c510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd874542110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd874542110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87466c0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87466c0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd894082310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd894082310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87451ced0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87451ced0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8741fffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd8741fffd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8741ffd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8741ffd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87423f750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87423f750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd87446bd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd87446bd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd874278cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd874278cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd874175550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd874175550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8545c0510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8545c0510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8741f3d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd8741f3d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd874146510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd874146510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd874266a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd874266a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd77846fa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd77846fa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd778469e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd778469e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87415c190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd87415c190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd77846f550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd77846f550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd77820b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd77820b1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd778207110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd778207110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd77810ef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd77810ef90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd7646cf990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd7646cf990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd778469390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd778469390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd778073350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd778073350>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 1:31:16 - loss: 0.7792 - acc: 0.4375
 128/5677 [..............................] - ETA: 53:17 - loss: 0.7946 - acc: 0.4766  
 192/5677 [>.............................] - ETA: 40:21 - loss: 0.7737 - acc: 0.5052
 256/5677 [>.............................] - ETA: 33:47 - loss: 0.7667 - acc: 0.4961
 320/5677 [>.............................] - ETA: 29:55 - loss: 0.7620 - acc: 0.5062
 384/5677 [=>............................] - ETA: 27:15 - loss: 0.7549 - acc: 0.5182
 448/5677 [=>............................] - ETA: 25:22 - loss: 0.7502 - acc: 0.5246
 512/5677 [=>............................] - ETA: 23:46 - loss: 0.7471 - acc: 0.5273
 576/5677 [==>...........................] - ETA: 22:28 - loss: 0.7410 - acc: 0.5295
 640/5677 [==>...........................] - ETA: 21:23 - loss: 0.7381 - acc: 0.5312
 704/5677 [==>...........................] - ETA: 20:31 - loss: 0.7408 - acc: 0.5341
 768/5677 [===>..........................] - ETA: 19:43 - loss: 0.7403 - acc: 0.5299
 832/5677 [===>..........................] - ETA: 19:02 - loss: 0.7409 - acc: 0.5349
 896/5677 [===>..........................] - ETA: 18:27 - loss: 0.7387 - acc: 0.5346
 960/5677 [====>.........................] - ETA: 17:53 - loss: 0.7401 - acc: 0.5323
1024/5677 [====>.........................] - ETA: 17:23 - loss: 0.7382 - acc: 0.5332
1088/5677 [====>.........................] - ETA: 16:55 - loss: 0.7325 - acc: 0.5377
1152/5677 [=====>........................] - ETA: 16:28 - loss: 0.7292 - acc: 0.5399
1216/5677 [=====>........................] - ETA: 16:05 - loss: 0.7272 - acc: 0.5378
1280/5677 [=====>........................] - ETA: 15:44 - loss: 0.7308 - acc: 0.5312
1344/5677 [======>.......................] - ETA: 15:23 - loss: 0.7322 - acc: 0.5268
1408/5677 [======>.......................] - ETA: 15:02 - loss: 0.7320 - acc: 0.5291
1472/5677 [======>.......................] - ETA: 14:43 - loss: 0.7314 - acc: 0.5292
1536/5677 [=======>......................] - ETA: 14:24 - loss: 0.7299 - acc: 0.5293
1600/5677 [=======>......................] - ETA: 14:06 - loss: 0.7307 - acc: 0.5269
1664/5677 [=======>......................] - ETA: 13:48 - loss: 0.7311 - acc: 0.5258
1728/5677 [========>.....................] - ETA: 13:31 - loss: 0.7336 - acc: 0.5214
1792/5677 [========>.....................] - ETA: 13:15 - loss: 0.7365 - acc: 0.5184
1856/5677 [========>.....................] - ETA: 12:59 - loss: 0.7351 - acc: 0.5205
1920/5677 [=========>....................] - ETA: 12:41 - loss: 0.7345 - acc: 0.5214
1984/5677 [=========>....................] - ETA: 12:26 - loss: 0.7333 - acc: 0.5212
2048/5677 [=========>....................] - ETA: 12:10 - loss: 0.7329 - acc: 0.5200
2112/5677 [==========>...................] - ETA: 11:54 - loss: 0.7331 - acc: 0.5185
2176/5677 [==========>...................] - ETA: 11:38 - loss: 0.7340 - acc: 0.5170
2240/5677 [==========>...................] - ETA: 11:22 - loss: 0.7340 - acc: 0.5183
2304/5677 [===========>..................] - ETA: 11:07 - loss: 0.7357 - acc: 0.5148
2368/5677 [===========>..................] - ETA: 10:52 - loss: 0.7348 - acc: 0.5165
2432/5677 [===========>..................] - ETA: 10:38 - loss: 0.7332 - acc: 0.5189
2496/5677 [============>.................] - ETA: 10:23 - loss: 0.7326 - acc: 0.5192
2560/5677 [============>.................] - ETA: 10:08 - loss: 0.7317 - acc: 0.5191
2624/5677 [============>.................] - ETA: 9:55 - loss: 0.7306 - acc: 0.5202 
2688/5677 [=============>................] - ETA: 9:41 - loss: 0.7301 - acc: 0.5190
2752/5677 [=============>................] - ETA: 9:28 - loss: 0.7298 - acc: 0.5193
2816/5677 [=============>................] - ETA: 9:14 - loss: 0.7298 - acc: 0.5174
2880/5677 [==============>...............] - ETA: 9:01 - loss: 0.7285 - acc: 0.5181
2944/5677 [==============>...............] - ETA: 8:48 - loss: 0.7282 - acc: 0.5177
3008/5677 [==============>...............] - ETA: 8:35 - loss: 0.7275 - acc: 0.5186
3072/5677 [===============>..............] - ETA: 8:21 - loss: 0.7268 - acc: 0.5192
3136/5677 [===============>..............] - ETA: 8:09 - loss: 0.7270 - acc: 0.5195
3200/5677 [===============>..............] - ETA: 7:56 - loss: 0.7255 - acc: 0.5209
3264/5677 [================>.............] - ETA: 7:43 - loss: 0.7254 - acc: 0.5211
3328/5677 [================>.............] - ETA: 7:30 - loss: 0.7251 - acc: 0.5198
3392/5677 [================>.............] - ETA: 7:18 - loss: 0.7242 - acc: 0.5209
3456/5677 [=================>............] - ETA: 7:05 - loss: 0.7230 - acc: 0.5220
3520/5677 [=================>............] - ETA: 6:52 - loss: 0.7226 - acc: 0.5230
3584/5677 [=================>............] - ETA: 6:39 - loss: 0.7230 - acc: 0.5204
3648/5677 [==================>...........] - ETA: 6:26 - loss: 0.7237 - acc: 0.5184
3712/5677 [==================>...........] - ETA: 6:13 - loss: 0.7235 - acc: 0.5180
3776/5677 [==================>...........] - ETA: 6:00 - loss: 0.7232 - acc: 0.5177
3840/5677 [===================>..........] - ETA: 5:47 - loss: 0.7226 - acc: 0.5198
3904/5677 [===================>..........] - ETA: 5:34 - loss: 0.7222 - acc: 0.5205
3968/5677 [===================>..........] - ETA: 5:22 - loss: 0.7225 - acc: 0.5199
4032/5677 [====================>.........] - ETA: 5:09 - loss: 0.7222 - acc: 0.5198
4096/5677 [====================>.........] - ETA: 4:56 - loss: 0.7219 - acc: 0.5188
4160/5677 [====================>.........] - ETA: 4:44 - loss: 0.7226 - acc: 0.5166
4224/5677 [=====================>........] - ETA: 4:32 - loss: 0.7228 - acc: 0.5161
4288/5677 [=====================>........] - ETA: 4:19 - loss: 0.7223 - acc: 0.5161
4352/5677 [=====================>........] - ETA: 4:07 - loss: 0.7216 - acc: 0.5170
4416/5677 [======================>.......] - ETA: 3:54 - loss: 0.7218 - acc: 0.5156
4480/5677 [======================>.......] - ETA: 3:42 - loss: 0.7216 - acc: 0.5152
4544/5677 [=======================>......] - ETA: 3:30 - loss: 0.7214 - acc: 0.5163
4608/5677 [=======================>......] - ETA: 3:18 - loss: 0.7211 - acc: 0.5163
4672/5677 [=======================>......] - ETA: 3:06 - loss: 0.7205 - acc: 0.5169
4736/5677 [========================>.....] - ETA: 2:54 - loss: 0.7203 - acc: 0.5173
4800/5677 [========================>.....] - ETA: 2:42 - loss: 0.7211 - acc: 0.5148
4864/5677 [========================>.....] - ETA: 2:30 - loss: 0.7209 - acc: 0.5152
4928/5677 [=========================>....] - ETA: 2:18 - loss: 0.7211 - acc: 0.5150
4992/5677 [=========================>....] - ETA: 2:06 - loss: 0.7208 - acc: 0.5146
5056/5677 [=========================>....] - ETA: 1:54 - loss: 0.7207 - acc: 0.5150
5120/5677 [==========================>...] - ETA: 1:42 - loss: 0.7207 - acc: 0.5141
5184/5677 [==========================>...] - ETA: 1:30 - loss: 0.7201 - acc: 0.5145
5248/5677 [==========================>...] - ETA: 1:18 - loss: 0.7198 - acc: 0.5143
5312/5677 [===========================>..] - ETA: 1:06 - loss: 0.7201 - acc: 0.5132
5376/5677 [===========================>..] - ETA: 55s - loss: 0.7204 - acc: 0.5117 
5440/5677 [===========================>..] - ETA: 43s - loss: 0.7203 - acc: 0.5112
5504/5677 [============================>.] - ETA: 31s - loss: 0.7201 - acc: 0.5107
5568/5677 [============================>.] - ETA: 19s - loss: 0.7196 - acc: 0.5110
5632/5677 [============================>.] - ETA: 8s - loss: 0.7193 - acc: 0.5115 
5677/5677 [==============================] - 1078s 190ms/step - loss: 0.7193 - acc: 0.5110 - val_loss: 0.6851 - val_acc: 0.5547

Epoch 00001: val_acc improved from -inf to 0.55468, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window21/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 16:37 - loss: 0.7165 - acc: 0.4844
 128/5677 [..............................] - ETA: 16:21 - loss: 0.6912 - acc: 0.5547
 192/5677 [>.............................] - ETA: 15:58 - loss: 0.6925 - acc: 0.5417
 256/5677 [>.............................] - ETA: 15:35 - loss: 0.6989 - acc: 0.5234
 320/5677 [>.............................] - ETA: 15:21 - loss: 0.6924 - acc: 0.5344
 384/5677 [=>............................] - ETA: 15:04 - loss: 0.6926 - acc: 0.5312
 448/5677 [=>............................] - ETA: 14:49 - loss: 0.6943 - acc: 0.5379
 512/5677 [=>............................] - ETA: 14:43 - loss: 0.6970 - acc: 0.5293
 576/5677 [==>...........................] - ETA: 14:32 - loss: 0.6952 - acc: 0.5312
 640/5677 [==>...........................] - ETA: 14:17 - loss: 0.6945 - acc: 0.5312
 704/5677 [==>...........................] - ETA: 14:11 - loss: 0.6941 - acc: 0.5312
 768/5677 [===>..........................] - ETA: 14:03 - loss: 0.6971 - acc: 0.5260
 832/5677 [===>..........................] - ETA: 13:56 - loss: 0.6981 - acc: 0.5228
 896/5677 [===>..........................] - ETA: 13:40 - loss: 0.7016 - acc: 0.5167
 960/5677 [====>.........................] - ETA: 13:35 - loss: 0.7008 - acc: 0.5188
1024/5677 [====>.........................] - ETA: 13:20 - loss: 0.7014 - acc: 0.5127
1088/5677 [====>.........................] - ETA: 13:08 - loss: 0.7005 - acc: 0.5156
1152/5677 [=====>........................] - ETA: 12:57 - loss: 0.7017 - acc: 0.5122
1216/5677 [=====>........................] - ETA: 12:48 - loss: 0.7024 - acc: 0.5123
1280/5677 [=====>........................] - ETA: 12:35 - loss: 0.7019 - acc: 0.5141
1344/5677 [======>.......................] - ETA: 12:21 - loss: 0.7018 - acc: 0.5156
1408/5677 [======>.......................] - ETA: 12:11 - loss: 0.7024 - acc: 0.5170
1472/5677 [======>.......................] - ETA: 11:57 - loss: 0.7023 - acc: 0.5190
1536/5677 [=======>......................] - ETA: 11:46 - loss: 0.7019 - acc: 0.5189
1600/5677 [=======>......................] - ETA: 11:34 - loss: 0.7010 - acc: 0.5200
1664/5677 [=======>......................] - ETA: 11:25 - loss: 0.7007 - acc: 0.5186
1728/5677 [========>.....................] - ETA: 11:14 - loss: 0.7012 - acc: 0.5174
1792/5677 [========>.....................] - ETA: 11:03 - loss: 0.7028 - acc: 0.5145
1856/5677 [========>.....................] - ETA: 10:52 - loss: 0.7025 - acc: 0.5156
1920/5677 [=========>....................] - ETA: 10:42 - loss: 0.7031 - acc: 0.5156
1984/5677 [=========>....................] - ETA: 10:32 - loss: 0.7031 - acc: 0.5176
2048/5677 [=========>....................] - ETA: 10:20 - loss: 0.7030 - acc: 0.5176
2112/5677 [==========>...................] - ETA: 10:08 - loss: 0.7020 - acc: 0.5194
2176/5677 [==========>...................] - ETA: 9:57 - loss: 0.7019 - acc: 0.5188 
2240/5677 [==========>...................] - ETA: 9:46 - loss: 0.7028 - acc: 0.5161
2304/5677 [===========>..................] - ETA: 9:36 - loss: 0.7043 - acc: 0.5117
2368/5677 [===========>..................] - ETA: 9:25 - loss: 0.7047 - acc: 0.5110
2432/5677 [===========>..................] - ETA: 9:13 - loss: 0.7052 - acc: 0.5115
2496/5677 [============>.................] - ETA: 9:01 - loss: 0.7047 - acc: 0.5132
2560/5677 [============>.................] - ETA: 8:50 - loss: 0.7047 - acc: 0.5125
2624/5677 [============>.................] - ETA: 8:38 - loss: 0.7046 - acc: 0.5130
2688/5677 [=============>................] - ETA: 8:28 - loss: 0.7036 - acc: 0.5153
2752/5677 [=============>................] - ETA: 8:18 - loss: 0.7034 - acc: 0.5145
2816/5677 [=============>................] - ETA: 8:07 - loss: 0.7029 - acc: 0.5142
2880/5677 [==============>...............] - ETA: 7:56 - loss: 0.7035 - acc: 0.5135
2944/5677 [==============>...............] - ETA: 7:45 - loss: 0.7031 - acc: 0.5136
3008/5677 [==============>...............] - ETA: 7:34 - loss: 0.7027 - acc: 0.5133
3072/5677 [===============>..............] - ETA: 7:23 - loss: 0.7022 - acc: 0.5156
3136/5677 [===============>..............] - ETA: 7:12 - loss: 0.7024 - acc: 0.5147
3200/5677 [===============>..............] - ETA: 7:01 - loss: 0.7017 - acc: 0.5166
3264/5677 [================>.............] - ETA: 6:50 - loss: 0.7021 - acc: 0.5147
3328/5677 [================>.............] - ETA: 6:39 - loss: 0.7017 - acc: 0.5147
3392/5677 [================>.............] - ETA: 6:27 - loss: 0.7013 - acc: 0.5159
3456/5677 [=================>............] - ETA: 6:16 - loss: 0.7021 - acc: 0.5165
3520/5677 [=================>............] - ETA: 6:05 - loss: 0.7021 - acc: 0.5170
3584/5677 [=================>............] - ETA: 5:54 - loss: 0.7021 - acc: 0.5167
3648/5677 [==================>...........] - ETA: 5:42 - loss: 0.7025 - acc: 0.5156
3712/5677 [==================>...........] - ETA: 5:31 - loss: 0.7026 - acc: 0.5156
3776/5677 [==================>...........] - ETA: 5:21 - loss: 0.7029 - acc: 0.5148
3840/5677 [===================>..........] - ETA: 5:10 - loss: 0.7028 - acc: 0.5148
3904/5677 [===================>..........] - ETA: 4:59 - loss: 0.7033 - acc: 0.5136
3968/5677 [===================>..........] - ETA: 4:48 - loss: 0.7025 - acc: 0.5149
4032/5677 [====================>.........] - ETA: 4:37 - loss: 0.7028 - acc: 0.5139
4096/5677 [====================>.........] - ETA: 4:27 - loss: 0.7027 - acc: 0.5137
4160/5677 [====================>.........] - ETA: 4:16 - loss: 0.7027 - acc: 0.5132
4224/5677 [=====================>........] - ETA: 4:05 - loss: 0.7028 - acc: 0.5133
4288/5677 [=====================>........] - ETA: 3:54 - loss: 0.7025 - acc: 0.5133
4352/5677 [=====================>........] - ETA: 3:44 - loss: 0.7027 - acc: 0.5131
4416/5677 [======================>.......] - ETA: 3:33 - loss: 0.7018 - acc: 0.5152
4480/5677 [======================>.......] - ETA: 3:21 - loss: 0.7017 - acc: 0.5154
4544/5677 [=======================>......] - ETA: 3:11 - loss: 0.7021 - acc: 0.5158
4608/5677 [=======================>......] - ETA: 3:00 - loss: 0.7014 - acc: 0.5182
4672/5677 [=======================>......] - ETA: 2:49 - loss: 0.7012 - acc: 0.5193
4736/5677 [========================>.....] - ETA: 2:38 - loss: 0.7013 - acc: 0.5188
4800/5677 [========================>.....] - ETA: 2:28 - loss: 0.7014 - acc: 0.5192
4864/5677 [========================>.....] - ETA: 2:17 - loss: 0.7015 - acc: 0.5195
4928/5677 [=========================>....] - ETA: 2:06 - loss: 0.7018 - acc: 0.5195
4992/5677 [=========================>....] - ETA: 1:55 - loss: 0.7013 - acc: 0.5202
5056/5677 [=========================>....] - ETA: 1:44 - loss: 0.7009 - acc: 0.5214
5120/5677 [==========================>...] - ETA: 1:33 - loss: 0.7011 - acc: 0.5215
5184/5677 [==========================>...] - ETA: 1:22 - loss: 0.7011 - acc: 0.5212
5248/5677 [==========================>...] - ETA: 1:12 - loss: 0.7009 - acc: 0.5219
5312/5677 [===========================>..] - ETA: 1:01 - loss: 0.7004 - acc: 0.5230
5376/5677 [===========================>..] - ETA: 50s - loss: 0.7000 - acc: 0.5238 
5440/5677 [===========================>..] - ETA: 39s - loss: 0.6996 - acc: 0.5244
5504/5677 [============================>.] - ETA: 29s - loss: 0.6995 - acc: 0.5243
5568/5677 [============================>.] - ETA: 18s - loss: 0.6993 - acc: 0.5246
5632/5677 [============================>.] - ETA: 7s - loss: 0.6994 - acc: 0.5245 
5677/5677 [==============================] - 992s 175ms/step - loss: 0.6993 - acc: 0.5246 - val_loss: 0.6836 - val_acc: 0.5325

Epoch 00002: val_acc did not improve from 0.55468
Epoch 3/10

  64/5677 [..............................] - ETA: 15:50 - loss: 0.7239 - acc: 0.5000
 128/5677 [..............................] - ETA: 15:28 - loss: 0.7059 - acc: 0.5156
 192/5677 [>.............................] - ETA: 15:35 - loss: 0.7115 - acc: 0.5260
 256/5677 [>.............................] - ETA: 15:03 - loss: 0.7049 - acc: 0.5352
 320/5677 [>.............................] - ETA: 14:47 - loss: 0.7072 - acc: 0.5406
 384/5677 [=>............................] - ETA: 14:36 - loss: 0.7031 - acc: 0.5391
 448/5677 [=>............................] - ETA: 14:31 - loss: 0.7024 - acc: 0.5312
 512/5677 [=>............................] - ETA: 14:13 - loss: 0.6995 - acc: 0.5371
 576/5677 [==>...........................] - ETA: 14:09 - loss: 0.6968 - acc: 0.5434
 640/5677 [==>...........................] - ETA: 13:59 - loss: 0.6946 - acc: 0.5422
 704/5677 [==>...........................] - ETA: 13:39 - loss: 0.6896 - acc: 0.5526
 768/5677 [===>..........................] - ETA: 13:24 - loss: 0.6929 - acc: 0.5430
 832/5677 [===>..........................] - ETA: 13:17 - loss: 0.6942 - acc: 0.5421
 896/5677 [===>..........................] - ETA: 13:05 - loss: 0.6947 - acc: 0.5391
 960/5677 [====>.........................] - ETA: 13:06 - loss: 0.6953 - acc: 0.5406
1024/5677 [====>.........................] - ETA: 12:50 - loss: 0.6962 - acc: 0.5371
1088/5677 [====>.........................] - ETA: 12:42 - loss: 0.6969 - acc: 0.5331
1152/5677 [=====>........................] - ETA: 12:32 - loss: 0.6972 - acc: 0.5286
1216/5677 [=====>........................] - ETA: 12:22 - loss: 0.6951 - acc: 0.5288
1280/5677 [=====>........................] - ETA: 12:09 - loss: 0.6952 - acc: 0.5305
1344/5677 [======>.......................] - ETA: 11:57 - loss: 0.6947 - acc: 0.5290
1408/5677 [======>.......................] - ETA: 11:48 - loss: 0.6940 - acc: 0.5277
1472/5677 [======>.......................] - ETA: 11:36 - loss: 0.6938 - acc: 0.5292
1536/5677 [=======>......................] - ETA: 11:25 - loss: 0.6933 - acc: 0.5312
1600/5677 [=======>......................] - ETA: 11:16 - loss: 0.6940 - acc: 0.5281
1664/5677 [=======>......................] - ETA: 11:07 - loss: 0.6962 - acc: 0.5240
1728/5677 [========>.....................] - ETA: 10:56 - loss: 0.6948 - acc: 0.5255
1792/5677 [========>.....................] - ETA: 10:45 - loss: 0.6942 - acc: 0.5257
1856/5677 [========>.....................] - ETA: 10:35 - loss: 0.6938 - acc: 0.5275
1920/5677 [=========>....................] - ETA: 10:23 - loss: 0.6936 - acc: 0.5286
1984/5677 [=========>....................] - ETA: 10:12 - loss: 0.6933 - acc: 0.5297
2048/5677 [=========>....................] - ETA: 10:02 - loss: 0.6928 - acc: 0.5308
2112/5677 [==========>...................] - ETA: 9:51 - loss: 0.6931 - acc: 0.5322 
2176/5677 [==========>...................] - ETA: 9:40 - loss: 0.6943 - acc: 0.5294
2240/5677 [==========>...................] - ETA: 9:30 - loss: 0.6932 - acc: 0.5299
2304/5677 [===========>..................] - ETA: 9:18 - loss: 0.6932 - acc: 0.5299
2368/5677 [===========>..................] - ETA: 9:08 - loss: 0.6934 - acc: 0.5296
2432/5677 [===========>..................] - ETA: 8:58 - loss: 0.6951 - acc: 0.5275
2496/5677 [============>.................] - ETA: 8:47 - loss: 0.6951 - acc: 0.5300
2560/5677 [============>.................] - ETA: 8:36 - loss: 0.6946 - acc: 0.5316
2624/5677 [============>.................] - ETA: 8:26 - loss: 0.6952 - acc: 0.5297
2688/5677 [=============>................] - ETA: 8:14 - loss: 0.6945 - acc: 0.5327
2752/5677 [=============>................] - ETA: 8:04 - loss: 0.6942 - acc: 0.5345
2816/5677 [=============>................] - ETA: 7:53 - loss: 0.6933 - acc: 0.5362
2880/5677 [==============>...............] - ETA: 7:43 - loss: 0.6940 - acc: 0.5347
2944/5677 [==============>...............] - ETA: 7:33 - loss: 0.6948 - acc: 0.5323
3008/5677 [==============>...............] - ETA: 7:23 - loss: 0.6946 - acc: 0.5329
3072/5677 [===============>..............] - ETA: 7:12 - loss: 0.6943 - acc: 0.5348
3136/5677 [===============>..............] - ETA: 7:02 - loss: 0.6942 - acc: 0.5348
3200/5677 [===============>..............] - ETA: 6:50 - loss: 0.6941 - acc: 0.5350
3264/5677 [================>.............] - ETA: 6:40 - loss: 0.6936 - acc: 0.5352
3328/5677 [================>.............] - ETA: 6:29 - loss: 0.6941 - acc: 0.5340
3392/5677 [================>.............] - ETA: 6:18 - loss: 0.6947 - acc: 0.5342
3456/5677 [=================>............] - ETA: 6:08 - loss: 0.6951 - acc: 0.5336
3520/5677 [=================>............] - ETA: 5:58 - loss: 0.6958 - acc: 0.5310
3584/5677 [=================>............] - ETA: 5:47 - loss: 0.6963 - acc: 0.5304
3648/5677 [==================>...........] - ETA: 5:36 - loss: 0.6961 - acc: 0.5312
3712/5677 [==================>...........] - ETA: 5:25 - loss: 0.6960 - acc: 0.5326
3776/5677 [==================>...........] - ETA: 5:15 - loss: 0.6960 - acc: 0.5320
3840/5677 [===================>..........] - ETA: 5:04 - loss: 0.6961 - acc: 0.5318
3904/5677 [===================>..........] - ETA: 4:53 - loss: 0.6960 - acc: 0.5325
3968/5677 [===================>..........] - ETA: 4:43 - loss: 0.6962 - acc: 0.5325
4032/5677 [====================>.........] - ETA: 4:32 - loss: 0.6961 - acc: 0.5325
4096/5677 [====================>.........] - ETA: 4:21 - loss: 0.6964 - acc: 0.5327
4160/5677 [====================>.........] - ETA: 4:11 - loss: 0.6964 - acc: 0.5332
4224/5677 [=====================>........] - ETA: 4:00 - loss: 0.6961 - acc: 0.5341
4288/5677 [=====================>........] - ETA: 3:50 - loss: 0.6967 - acc: 0.5329
4352/5677 [=====================>........] - ETA: 3:39 - loss: 0.6962 - acc: 0.5333
4416/5677 [======================>.......] - ETA: 3:28 - loss: 0.6965 - acc: 0.5333
4480/5677 [======================>.......] - ETA: 3:18 - loss: 0.6964 - acc: 0.5324
4544/5677 [=======================>......] - ETA: 3:07 - loss: 0.6965 - acc: 0.5330
4608/5677 [=======================>......] - ETA: 2:57 - loss: 0.6965 - acc: 0.5330
4672/5677 [=======================>......] - ETA: 2:46 - loss: 0.6965 - acc: 0.5338
4736/5677 [========================>.....] - ETA: 2:36 - loss: 0.6969 - acc: 0.5332
4800/5677 [========================>.....] - ETA: 2:25 - loss: 0.6975 - acc: 0.5319
4864/5677 [========================>.....] - ETA: 2:14 - loss: 0.6972 - acc: 0.5335
4928/5677 [=========================>....] - ETA: 2:04 - loss: 0.6971 - acc: 0.5337
4992/5677 [=========================>....] - ETA: 1:53 - loss: 0.6969 - acc: 0.5347
5056/5677 [=========================>....] - ETA: 1:43 - loss: 0.6968 - acc: 0.5350
5120/5677 [==========================>...] - ETA: 1:32 - loss: 0.6964 - acc: 0.5363
5184/5677 [==========================>...] - ETA: 1:21 - loss: 0.6964 - acc: 0.5351
5248/5677 [==========================>...] - ETA: 1:11 - loss: 0.6960 - acc: 0.5362
5312/5677 [===========================>..] - ETA: 1:00 - loss: 0.6959 - acc: 0.5367
5376/5677 [===========================>..] - ETA: 50s - loss: 0.6958 - acc: 0.5363 
5440/5677 [===========================>..] - ETA: 39s - loss: 0.6956 - acc: 0.5366
5504/5677 [============================>.] - ETA: 28s - loss: 0.6958 - acc: 0.5363
5568/5677 [============================>.] - ETA: 18s - loss: 0.6962 - acc: 0.5365
5632/5677 [============================>.] - ETA: 7s - loss: 0.6962 - acc: 0.5359 
5677/5677 [==============================] - 981s 173ms/step - loss: 0.6963 - acc: 0.5358 - val_loss: 0.6807 - val_acc: 0.5658

Epoch 00003: val_acc improved from 0.55468 to 0.56577, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window21/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 4/10

  64/5677 [..............................] - ETA: 14:46 - loss: 0.6982 - acc: 0.5469
 128/5677 [..............................] - ETA: 14:50 - loss: 0.6998 - acc: 0.5391
 192/5677 [>.............................] - ETA: 14:42 - loss: 0.6988 - acc: 0.5365
 256/5677 [>.............................] - ETA: 14:41 - loss: 0.6966 - acc: 0.5508
 320/5677 [>.............................] - ETA: 14:37 - loss: 0.6964 - acc: 0.5500
 384/5677 [=>............................] - ETA: 14:28 - loss: 0.6964 - acc: 0.5495
 448/5677 [=>............................] - ETA: 14:36 - loss: 0.6891 - acc: 0.5625
 512/5677 [=>............................] - ETA: 14:22 - loss: 0.6870 - acc: 0.5723
 576/5677 [==>...........................] - ETA: 14:08 - loss: 0.6863 - acc: 0.5729
 640/5677 [==>...........................] - ETA: 14:00 - loss: 0.6866 - acc: 0.5719
 704/5677 [==>...........................] - ETA: 13:52 - loss: 0.6856 - acc: 0.5668
 768/5677 [===>..........................] - ETA: 13:42 - loss: 0.6885 - acc: 0.5599
 832/5677 [===>..........................] - ETA: 13:29 - loss: 0.6883 - acc: 0.5565
 896/5677 [===>..........................] - ETA: 13:17 - loss: 0.6887 - acc: 0.5558
 960/5677 [====>.........................] - ETA: 13:07 - loss: 0.6877 - acc: 0.5573
1024/5677 [====>.........................] - ETA: 12:56 - loss: 0.6873 - acc: 0.5625
1088/5677 [====>.........................] - ETA: 12:45 - loss: 0.6879 - acc: 0.5579
1152/5677 [=====>........................] - ETA: 12:33 - loss: 0.6882 - acc: 0.5547
1216/5677 [=====>........................] - ETA: 12:23 - loss: 0.6882 - acc: 0.5535
1280/5677 [=====>........................] - ETA: 12:13 - loss: 0.6864 - acc: 0.5539
1344/5677 [======>.......................] - ETA: 12:02 - loss: 0.6870 - acc: 0.5499
1408/5677 [======>.......................] - ETA: 11:49 - loss: 0.6871 - acc: 0.5476
1472/5677 [======>.......................] - ETA: 11:36 - loss: 0.6880 - acc: 0.5442
1536/5677 [=======>......................] - ETA: 11:25 - loss: 0.6902 - acc: 0.5410
1600/5677 [=======>......................] - ETA: 11:13 - loss: 0.6908 - acc: 0.5375
1664/5677 [=======>......................] - ETA: 11:04 - loss: 0.6897 - acc: 0.5415
1728/5677 [========>.....................] - ETA: 10:52 - loss: 0.6881 - acc: 0.5446
1792/5677 [========>.....................] - ETA: 10:41 - loss: 0.6870 - acc: 0.5469
1856/5677 [========>.....................] - ETA: 10:31 - loss: 0.6878 - acc: 0.5431
1920/5677 [=========>....................] - ETA: 10:21 - loss: 0.6884 - acc: 0.5411
1984/5677 [=========>....................] - ETA: 10:12 - loss: 0.6887 - acc: 0.5393
2048/5677 [=========>....................] - ETA: 10:02 - loss: 0.6880 - acc: 0.5410
2112/5677 [==========>...................] - ETA: 9:50 - loss: 0.6877 - acc: 0.5402 
2176/5677 [==========>...................] - ETA: 9:39 - loss: 0.6877 - acc: 0.5427
2240/5677 [==========>...................] - ETA: 9:29 - loss: 0.6875 - acc: 0.5442
2304/5677 [===========>..................] - ETA: 9:18 - loss: 0.6876 - acc: 0.5438
2368/5677 [===========>..................] - ETA: 9:07 - loss: 0.6871 - acc: 0.5456
2432/5677 [===========>..................] - ETA: 8:56 - loss: 0.6868 - acc: 0.5461
2496/5677 [============>.................] - ETA: 8:44 - loss: 0.6865 - acc: 0.5469
2560/5677 [============>.................] - ETA: 8:34 - loss: 0.6871 - acc: 0.5449
2624/5677 [============>.................] - ETA: 8:22 - loss: 0.6876 - acc: 0.5442
2688/5677 [=============>................] - ETA: 8:10 - loss: 0.6888 - acc: 0.5417
2752/5677 [=============>................] - ETA: 7:59 - loss: 0.6887 - acc: 0.5422
2816/5677 [=============>................] - ETA: 7:49 - loss: 0.6892 - acc: 0.5423
2880/5677 [==============>...............] - ETA: 7:38 - loss: 0.6894 - acc: 0.5424
2944/5677 [==============>...............] - ETA: 7:27 - loss: 0.6901 - acc: 0.5418
3008/5677 [==============>...............] - ETA: 7:16 - loss: 0.6904 - acc: 0.5412
3072/5677 [===============>..............] - ETA: 7:04 - loss: 0.6894 - acc: 0.5439
3136/5677 [===============>..............] - ETA: 6:53 - loss: 0.6905 - acc: 0.5402
3200/5677 [===============>..............] - ETA: 6:42 - loss: 0.6914 - acc: 0.5384
3264/5677 [================>.............] - ETA: 6:31 - loss: 0.6913 - acc: 0.5383
3328/5677 [================>.............] - ETA: 6:21 - loss: 0.6925 - acc: 0.5361
3392/5677 [================>.............] - ETA: 6:10 - loss: 0.6924 - acc: 0.5366
3456/5677 [=================>............] - ETA: 6:00 - loss: 0.6920 - acc: 0.5376
3520/5677 [=================>............] - ETA: 5:50 - loss: 0.6921 - acc: 0.5366
3584/5677 [=================>............] - ETA: 5:39 - loss: 0.6918 - acc: 0.5377
3648/5677 [==================>...........] - ETA: 5:28 - loss: 0.6918 - acc: 0.5376
3712/5677 [==================>...........] - ETA: 5:18 - loss: 0.6915 - acc: 0.5383
3776/5677 [==================>...........] - ETA: 5:07 - loss: 0.6915 - acc: 0.5371
3840/5677 [===================>..........] - ETA: 4:56 - loss: 0.6910 - acc: 0.5385
3904/5677 [===================>..........] - ETA: 4:46 - loss: 0.6908 - acc: 0.5405
3968/5677 [===================>..........] - ETA: 4:35 - loss: 0.6913 - acc: 0.5393
4032/5677 [====================>.........] - ETA: 4:25 - loss: 0.6915 - acc: 0.5397
4096/5677 [====================>.........] - ETA: 4:14 - loss: 0.6918 - acc: 0.5388
4160/5677 [====================>.........] - ETA: 4:04 - loss: 0.6917 - acc: 0.5397
4224/5677 [=====================>........] - ETA: 3:53 - loss: 0.6913 - acc: 0.5407
4288/5677 [=====================>........] - ETA: 3:43 - loss: 0.6913 - acc: 0.5394
4352/5677 [=====================>........] - ETA: 3:32 - loss: 0.6914 - acc: 0.5398
4416/5677 [======================>.......] - ETA: 3:22 - loss: 0.6914 - acc: 0.5396
4480/5677 [======================>.......] - ETA: 3:11 - loss: 0.6914 - acc: 0.5404
4544/5677 [=======================>......] - ETA: 3:01 - loss: 0.6915 - acc: 0.5398
4608/5677 [=======================>......] - ETA: 2:51 - loss: 0.6910 - acc: 0.5412
4672/5677 [=======================>......] - ETA: 2:40 - loss: 0.6907 - acc: 0.5422
4736/5677 [========================>.....] - ETA: 2:30 - loss: 0.6904 - acc: 0.5433
4800/5677 [========================>.....] - ETA: 2:20 - loss: 0.6909 - acc: 0.5429
4864/5677 [========================>.....] - ETA: 2:10 - loss: 0.6905 - acc: 0.5438
4928/5677 [=========================>....] - ETA: 1:59 - loss: 0.6902 - acc: 0.5446
4992/5677 [=========================>....] - ETA: 1:49 - loss: 0.6904 - acc: 0.5443
5056/5677 [=========================>....] - ETA: 1:39 - loss: 0.6900 - acc: 0.5459
5120/5677 [==========================>...] - ETA: 1:29 - loss: 0.6896 - acc: 0.5477
5184/5677 [==========================>...] - ETA: 1:19 - loss: 0.6896 - acc: 0.5471
5248/5677 [==========================>...] - ETA: 1:08 - loss: 0.6897 - acc: 0.5465
5312/5677 [===========================>..] - ETA: 58s - loss: 0.6894 - acc: 0.5476 
5376/5677 [===========================>..] - ETA: 48s - loss: 0.6896 - acc: 0.5469
5440/5677 [===========================>..] - ETA: 38s - loss: 0.6892 - acc: 0.5476
5504/5677 [============================>.] - ETA: 27s - loss: 0.6893 - acc: 0.5478
5568/5677 [============================>.] - ETA: 17s - loss: 0.6894 - acc: 0.5474
5632/5677 [============================>.] - ETA: 7s - loss: 0.6889 - acc: 0.5485 
5677/5677 [==============================] - 950s 167ms/step - loss: 0.6887 - acc: 0.5484 - val_loss: 0.6959 - val_acc: 0.5420

Epoch 00004: val_acc did not improve from 0.56577
Epoch 5/10

  64/5677 [..............................] - ETA: 14:58 - loss: 0.6745 - acc: 0.6094
 128/5677 [..............................] - ETA: 14:37 - loss: 0.6870 - acc: 0.5781
 192/5677 [>.............................] - ETA: 14:39 - loss: 0.6798 - acc: 0.5781
 256/5677 [>.............................] - ETA: 14:35 - loss: 0.6839 - acc: 0.5547
 320/5677 [>.............................] - ETA: 14:40 - loss: 0.6868 - acc: 0.5625
 384/5677 [=>............................] - ETA: 14:28 - loss: 0.6841 - acc: 0.5651
 448/5677 [=>............................] - ETA: 14:20 - loss: 0.6845 - acc: 0.5625
 512/5677 [=>............................] - ETA: 14:30 - loss: 0.6777 - acc: 0.5742
 576/5677 [==>...........................] - ETA: 14:20 - loss: 0.6750 - acc: 0.5816
 640/5677 [==>...........................] - ETA: 14:04 - loss: 0.6789 - acc: 0.5781
 704/5677 [==>...........................] - ETA: 14:01 - loss: 0.6807 - acc: 0.5696
 768/5677 [===>..........................] - ETA: 13:52 - loss: 0.6799 - acc: 0.5742
 832/5677 [===>..........................] - ETA: 13:36 - loss: 0.6806 - acc: 0.5685
 896/5677 [===>..........................] - ETA: 13:29 - loss: 0.6827 - acc: 0.5580
 960/5677 [====>.........................] - ETA: 13:16 - loss: 0.6830 - acc: 0.5594
1024/5677 [====>.........................] - ETA: 13:03 - loss: 0.6809 - acc: 0.5625
1088/5677 [====>.........................] - ETA: 12:50 - loss: 0.6853 - acc: 0.5570
1152/5677 [=====>........................] - ETA: 12:43 - loss: 0.6821 - acc: 0.5634
1216/5677 [=====>........................] - ETA: 12:35 - loss: 0.6834 - acc: 0.5625
1280/5677 [=====>........................] - ETA: 12:24 - loss: 0.6823 - acc: 0.5625
1344/5677 [======>.......................] - ETA: 12:16 - loss: 0.6804 - acc: 0.5647
1408/5677 [======>.......................] - ETA: 12:05 - loss: 0.6799 - acc: 0.5653
1472/5677 [======>.......................] - ETA: 11:56 - loss: 0.6809 - acc: 0.5645
1536/5677 [=======>......................] - ETA: 11:46 - loss: 0.6819 - acc: 0.5612
1600/5677 [=======>......................] - ETA: 11:34 - loss: 0.6823 - acc: 0.5606
1664/5677 [=======>......................] - ETA: 11:23 - loss: 0.6833 - acc: 0.5625
1728/5677 [========>.....................] - ETA: 11:13 - loss: 0.6826 - acc: 0.5637
1792/5677 [========>.....................] - ETA: 11:01 - loss: 0.6840 - acc: 0.5592
1856/5677 [========>.....................] - ETA: 10:50 - loss: 0.6840 - acc: 0.5598
1920/5677 [=========>....................] - ETA: 10:38 - loss: 0.6845 - acc: 0.5589
1984/5677 [=========>....................] - ETA: 10:27 - loss: 0.6851 - acc: 0.5580
2048/5677 [=========>....................] - ETA: 10:15 - loss: 0.6854 - acc: 0.5566
2112/5677 [==========>...................] - ETA: 10:04 - loss: 0.6859 - acc: 0.5540
2176/5677 [==========>...................] - ETA: 9:54 - loss: 0.6865 - acc: 0.5515 
2240/5677 [==========>...................] - ETA: 9:44 - loss: 0.6863 - acc: 0.5500
2304/5677 [===========>..................] - ETA: 9:33 - loss: 0.6849 - acc: 0.5543
2368/5677 [===========>..................] - ETA: 9:21 - loss: 0.6854 - acc: 0.5524
2432/5677 [===========>..................] - ETA: 9:09 - loss: 0.6849 - acc: 0.5539
2496/5677 [============>.................] - ETA: 8:57 - loss: 0.6842 - acc: 0.5573
2560/5677 [============>.................] - ETA: 8:47 - loss: 0.6850 - acc: 0.5559
2624/5677 [============>.................] - ETA: 8:37 - loss: 0.6845 - acc: 0.5575
2688/5677 [=============>................] - ETA: 8:25 - loss: 0.6849 - acc: 0.5573
2752/5677 [=============>................] - ETA: 8:14 - loss: 0.6852 - acc: 0.5556
2816/5677 [=============>................] - ETA: 8:03 - loss: 0.6864 - acc: 0.5536
2880/5677 [==============>...............] - ETA: 7:52 - loss: 0.6865 - acc: 0.5524
2944/5677 [==============>...............] - ETA: 7:42 - loss: 0.6860 - acc: 0.5547
3008/5677 [==============>...............] - ETA: 7:31 - loss: 0.6855 - acc: 0.5559
3072/5677 [===============>..............] - ETA: 7:21 - loss: 0.6855 - acc: 0.5563
3136/5677 [===============>..............] - ETA: 7:10 - loss: 0.6850 - acc: 0.5564
3200/5677 [===============>..............] - ETA: 6:59 - loss: 0.6847 - acc: 0.5566
3264/5677 [================>.............] - ETA: 6:47 - loss: 0.6843 - acc: 0.5582
3328/5677 [================>.............] - ETA: 6:37 - loss: 0.6841 - acc: 0.5577
3392/5677 [================>.............] - ETA: 6:26 - loss: 0.6841 - acc: 0.5572
3456/5677 [=================>............] - ETA: 6:15 - loss: 0.6846 - acc: 0.5564
3520/5677 [=================>............] - ETA: 6:04 - loss: 0.6847 - acc: 0.5563
3584/5677 [=================>............] - ETA: 5:54 - loss: 0.6845 - acc: 0.5589
3648/5677 [==================>...........] - ETA: 5:43 - loss: 0.6841 - acc: 0.5606
3712/5677 [==================>...........] - ETA: 5:33 - loss: 0.6845 - acc: 0.5590
3776/5677 [==================>...........] - ETA: 5:22 - loss: 0.6842 - acc: 0.5599
3840/5677 [===================>..........] - ETA: 5:11 - loss: 0.6838 - acc: 0.5609
3904/5677 [===================>..........] - ETA: 5:00 - loss: 0.6837 - acc: 0.5620
3968/5677 [===================>..........] - ETA: 4:49 - loss: 0.6841 - acc: 0.5615
4032/5677 [====================>.........] - ETA: 4:38 - loss: 0.6842 - acc: 0.5600
4096/5677 [====================>.........] - ETA: 4:26 - loss: 0.6844 - acc: 0.5593
4160/5677 [====================>.........] - ETA: 4:16 - loss: 0.6846 - acc: 0.5587
4224/5677 [=====================>........] - ETA: 4:05 - loss: 0.6845 - acc: 0.5597
4288/5677 [=====================>........] - ETA: 3:54 - loss: 0.6840 - acc: 0.5606
4352/5677 [=====================>........] - ETA: 3:43 - loss: 0.6847 - acc: 0.5600
4416/5677 [======================>.......] - ETA: 3:32 - loss: 0.6847 - acc: 0.5607
4480/5677 [======================>.......] - ETA: 3:21 - loss: 0.6847 - acc: 0.5607
4544/5677 [=======================>......] - ETA: 3:10 - loss: 0.6849 - acc: 0.5607
4608/5677 [=======================>......] - ETA: 2:59 - loss: 0.6852 - acc: 0.5595
4672/5677 [=======================>......] - ETA: 2:49 - loss: 0.6848 - acc: 0.5604
4736/5677 [========================>.....] - ETA: 2:38 - loss: 0.6847 - acc: 0.5606
4800/5677 [========================>.....] - ETA: 2:27 - loss: 0.6845 - acc: 0.5617
4864/5677 [========================>.....] - ETA: 2:16 - loss: 0.6849 - acc: 0.5611
4928/5677 [=========================>....] - ETA: 2:05 - loss: 0.6846 - acc: 0.5619
4992/5677 [=========================>....] - ETA: 1:55 - loss: 0.6842 - acc: 0.5619
5056/5677 [=========================>....] - ETA: 1:44 - loss: 0.6847 - acc: 0.5613
5120/5677 [==========================>...] - ETA: 1:33 - loss: 0.6848 - acc: 0.5607
5184/5677 [==========================>...] - ETA: 1:23 - loss: 0.6848 - acc: 0.5608
5248/5677 [==========================>...] - ETA: 1:12 - loss: 0.6848 - acc: 0.5614
5312/5677 [===========================>..] - ETA: 1:01 - loss: 0.6850 - acc: 0.5606
5376/5677 [===========================>..] - ETA: 50s - loss: 0.6854 - acc: 0.5595 
5440/5677 [===========================>..] - ETA: 39s - loss: 0.6854 - acc: 0.5594
5504/5677 [============================>.] - ETA: 29s - loss: 0.6856 - acc: 0.5583
5568/5677 [============================>.] - ETA: 18s - loss: 0.6859 - acc: 0.5577
5632/5677 [============================>.] - ETA: 7s - loss: 0.6859 - acc: 0.5575 
5677/5677 [==============================] - 992s 175ms/step - loss: 0.6857 - acc: 0.5580 - val_loss: 0.6848 - val_acc: 0.5515

Epoch 00005: val_acc did not improve from 0.56577
Epoch 6/10

  64/5677 [..............................] - ETA: 16:17 - loss: 0.6672 - acc: 0.6406
 128/5677 [..............................] - ETA: 15:39 - loss: 0.6906 - acc: 0.5625
 192/5677 [>.............................] - ETA: 15:35 - loss: 0.6941 - acc: 0.5260
 256/5677 [>.............................] - ETA: 15:20 - loss: 0.6932 - acc: 0.5430
 320/5677 [>.............................] - ETA: 15:05 - loss: 0.6887 - acc: 0.5469
 384/5677 [=>............................] - ETA: 14:47 - loss: 0.6877 - acc: 0.5495
 448/5677 [=>............................] - ETA: 14:39 - loss: 0.6861 - acc: 0.5558
 512/5677 [=>............................] - ETA: 14:27 - loss: 0.6852 - acc: 0.5664
 576/5677 [==>...........................] - ETA: 14:22 - loss: 0.6876 - acc: 0.5608
 640/5677 [==>...........................] - ETA: 14:08 - loss: 0.6865 - acc: 0.5656
 704/5677 [==>...........................] - ETA: 13:56 - loss: 0.6824 - acc: 0.5739
 768/5677 [===>..........................] - ETA: 13:46 - loss: 0.6825 - acc: 0.5703
 832/5677 [===>..........................] - ETA: 13:37 - loss: 0.6823 - acc: 0.5697
 896/5677 [===>..........................] - ETA: 13:25 - loss: 0.6810 - acc: 0.5725
 960/5677 [====>.........................] - ETA: 13:16 - loss: 0.6794 - acc: 0.5760
1024/5677 [====>.........................] - ETA: 13:09 - loss: 0.6816 - acc: 0.5752
1088/5677 [====>.........................] - ETA: 13:01 - loss: 0.6803 - acc: 0.5772
1152/5677 [=====>........................] - ETA: 12:46 - loss: 0.6811 - acc: 0.5747
1216/5677 [=====>........................] - ETA: 12:32 - loss: 0.6823 - acc: 0.5715
1280/5677 [=====>........................] - ETA: 12:24 - loss: 0.6830 - acc: 0.5742
1344/5677 [======>.......................] - ETA: 12:15 - loss: 0.6849 - acc: 0.5714
1408/5677 [======>.......................] - ETA: 12:09 - loss: 0.6861 - acc: 0.5682
1472/5677 [======>.......................] - ETA: 11:59 - loss: 0.6872 - acc: 0.5639
1536/5677 [=======>......................] - ETA: 11:50 - loss: 0.6849 - acc: 0.5690
1600/5677 [=======>......................] - ETA: 11:39 - loss: 0.6827 - acc: 0.5731
1664/5677 [=======>......................] - ETA: 11:29 - loss: 0.6829 - acc: 0.5721
1728/5677 [========>.....................] - ETA: 11:20 - loss: 0.6814 - acc: 0.5741
1792/5677 [========>.....................] - ETA: 11:08 - loss: 0.6820 - acc: 0.5748
1856/5677 [========>.....................] - ETA: 10:56 - loss: 0.6823 - acc: 0.5738
1920/5677 [=========>....................] - ETA: 10:46 - loss: 0.6820 - acc: 0.5740
1984/5677 [=========>....................] - ETA: 10:34 - loss: 0.6824 - acc: 0.5726
2048/5677 [=========>....................] - ETA: 10:21 - loss: 0.6823 - acc: 0.5728
2112/5677 [==========>...................] - ETA: 10:09 - loss: 0.6813 - acc: 0.5753
2176/5677 [==========>...................] - ETA: 9:57 - loss: 0.6813 - acc: 0.5744 
2240/5677 [==========>...................] - ETA: 9:47 - loss: 0.6807 - acc: 0.5763
2304/5677 [===========>..................] - ETA: 9:36 - loss: 0.6810 - acc: 0.5764
2368/5677 [===========>..................] - ETA: 9:24 - loss: 0.6809 - acc: 0.5756
2432/5677 [===========>..................] - ETA: 9:11 - loss: 0.6807 - acc: 0.5773
2496/5677 [============>.................] - ETA: 8:59 - loss: 0.6827 - acc: 0.5741
2560/5677 [============>.................] - ETA: 8:49 - loss: 0.6817 - acc: 0.5758
2624/5677 [============>.................] - ETA: 8:37 - loss: 0.6820 - acc: 0.5751
2688/5677 [=============>................] - ETA: 8:27 - loss: 0.6816 - acc: 0.5763
2752/5677 [=============>................] - ETA: 8:15 - loss: 0.6819 - acc: 0.5759
2816/5677 [=============>................] - ETA: 8:04 - loss: 0.6812 - acc: 0.5767
2880/5677 [==============>...............] - ETA: 7:53 - loss: 0.6809 - acc: 0.5785
2944/5677 [==============>...............] - ETA: 7:42 - loss: 0.6804 - acc: 0.5791
3008/5677 [==============>...............] - ETA: 7:31 - loss: 0.6819 - acc: 0.5771
3072/5677 [===============>..............] - ETA: 7:20 - loss: 0.6809 - acc: 0.5791
3136/5677 [===============>..............] - ETA: 7:08 - loss: 0.6812 - acc: 0.5778
3200/5677 [===============>..............] - ETA: 6:57 - loss: 0.6812 - acc: 0.5766
3264/5677 [================>.............] - ETA: 6:47 - loss: 0.6803 - acc: 0.5775
3328/5677 [================>.............] - ETA: 6:36 - loss: 0.6805 - acc: 0.5760
3392/5677 [================>.............] - ETA: 6:24 - loss: 0.6801 - acc: 0.5764
3456/5677 [=================>............] - ETA: 6:13 - loss: 0.6804 - acc: 0.5755
3520/5677 [=================>............] - ETA: 6:02 - loss: 0.6805 - acc: 0.5759
3584/5677 [=================>............] - ETA: 5:51 - loss: 0.6798 - acc: 0.5765
3648/5677 [==================>...........] - ETA: 5:40 - loss: 0.6797 - acc: 0.5770
3712/5677 [==================>...........] - ETA: 5:30 - loss: 0.6808 - acc: 0.5749
3776/5677 [==================>...........] - ETA: 5:20 - loss: 0.6813 - acc: 0.5736
3840/5677 [===================>..........] - ETA: 5:10 - loss: 0.6811 - acc: 0.5734
3904/5677 [===================>..........] - ETA: 4:59 - loss: 0.6820 - acc: 0.5715
3968/5677 [===================>..........] - ETA: 4:47 - loss: 0.6813 - acc: 0.5731
4032/5677 [====================>.........] - ETA: 4:36 - loss: 0.6816 - acc: 0.5729
4096/5677 [====================>.........] - ETA: 4:25 - loss: 0.6815 - acc: 0.5725
4160/5677 [====================>.........] - ETA: 4:14 - loss: 0.6816 - acc: 0.5719
4224/5677 [=====================>........] - ETA: 4:04 - loss: 0.6817 - acc: 0.5720
4288/5677 [=====================>........] - ETA: 3:53 - loss: 0.6819 - acc: 0.5714
4352/5677 [=====================>........] - ETA: 3:43 - loss: 0.6819 - acc: 0.5717
4416/5677 [======================>.......] - ETA: 3:32 - loss: 0.6828 - acc: 0.5702
4480/5677 [======================>.......] - ETA: 3:21 - loss: 0.6828 - acc: 0.5696
4544/5677 [=======================>......] - ETA: 3:10 - loss: 0.6830 - acc: 0.5684
4608/5677 [=======================>......] - ETA: 2:59 - loss: 0.6829 - acc: 0.5688
4672/5677 [=======================>......] - ETA: 2:49 - loss: 0.6826 - acc: 0.5696
4736/5677 [========================>.....] - ETA: 2:38 - loss: 0.6824 - acc: 0.5703
4800/5677 [========================>.....] - ETA: 2:27 - loss: 0.6826 - acc: 0.5704
4864/5677 [========================>.....] - ETA: 2:16 - loss: 0.6830 - acc: 0.5691
4928/5677 [=========================>....] - ETA: 2:05 - loss: 0.6832 - acc: 0.5684
4992/5677 [=========================>....] - ETA: 1:55 - loss: 0.6835 - acc: 0.5673
5056/5677 [=========================>....] - ETA: 1:44 - loss: 0.6831 - acc: 0.5674
5120/5677 [==========================>...] - ETA: 1:33 - loss: 0.6835 - acc: 0.5670
5184/5677 [==========================>...] - ETA: 1:23 - loss: 0.6832 - acc: 0.5673
5248/5677 [==========================>...] - ETA: 1:12 - loss: 0.6837 - acc: 0.5663
5312/5677 [===========================>..] - ETA: 1:01 - loss: 0.6839 - acc: 0.5659
5376/5677 [===========================>..] - ETA: 50s - loss: 0.6837 - acc: 0.5668 
5440/5677 [===========================>..] - ETA: 39s - loss: 0.6837 - acc: 0.5673
5504/5677 [============================>.] - ETA: 29s - loss: 0.6837 - acc: 0.5676
5568/5677 [============================>.] - ETA: 18s - loss: 0.6831 - acc: 0.5691
5632/5677 [============================>.] - ETA: 7s - loss: 0.6829 - acc: 0.5700 
5677/5677 [==============================] - 992s 175ms/step - loss: 0.6826 - acc: 0.5705 - val_loss: 0.6797 - val_acc: 0.5610

Epoch 00006: val_acc did not improve from 0.56577
Epoch 7/10

  64/5677 [..............................] - ETA: 18:55 - loss: 0.6722 - acc: 0.5938
 128/5677 [..............................] - ETA: 17:15 - loss: 0.6773 - acc: 0.5781
 192/5677 [>.............................] - ETA: 16:43 - loss: 0.6815 - acc: 0.5781
 256/5677 [>.............................] - ETA: 16:29 - loss: 0.6783 - acc: 0.5508
 320/5677 [>.............................] - ETA: 16:18 - loss: 0.6757 - acc: 0.5656
 384/5677 [=>............................] - ETA: 16:00 - loss: 0.6750 - acc: 0.5677
 448/5677 [=>............................] - ETA: 16:00 - loss: 0.6761 - acc: 0.5714
 512/5677 [=>............................] - ETA: 15:53 - loss: 0.6786 - acc: 0.5605
 576/5677 [==>...........................] - ETA: 15:43 - loss: 0.6773 - acc: 0.5642
 640/5677 [==>...........................] - ETA: 15:37 - loss: 0.6793 - acc: 0.5594
 704/5677 [==>...........................] - ETA: 15:15 - loss: 0.6784 - acc: 0.5582
 768/5677 [===>..........................] - ETA: 14:58 - loss: 0.6773 - acc: 0.5586
 832/5677 [===>..........................] - ETA: 14:40 - loss: 0.6754 - acc: 0.5601
 896/5677 [===>..........................] - ETA: 14:29 - loss: 0.6764 - acc: 0.5614
 960/5677 [====>.........................] - ETA: 14:13 - loss: 0.6760 - acc: 0.5646
1024/5677 [====>.........................] - ETA: 14:01 - loss: 0.6764 - acc: 0.5605
1088/5677 [====>.........................] - ETA: 13:51 - loss: 0.6762 - acc: 0.5634
1152/5677 [=====>........................] - ETA: 13:35 - loss: 0.6752 - acc: 0.5677
1216/5677 [=====>........................] - ETA: 13:23 - loss: 0.6731 - acc: 0.5724
1280/5677 [=====>........................] - ETA: 13:10 - loss: 0.6734 - acc: 0.5734
1344/5677 [======>.......................] - ETA: 13:00 - loss: 0.6750 - acc: 0.5714
1408/5677 [======>.......................] - ETA: 12:46 - loss: 0.6743 - acc: 0.5717
1472/5677 [======>.......................] - ETA: 12:32 - loss: 0.6761 - acc: 0.5700
1536/5677 [=======>......................] - ETA: 12:20 - loss: 0.6775 - acc: 0.5690
1600/5677 [=======>......................] - ETA: 12:08 - loss: 0.6767 - acc: 0.5719
1664/5677 [=======>......................] - ETA: 11:57 - loss: 0.6779 - acc: 0.5703
1728/5677 [========>.....................] - ETA: 11:44 - loss: 0.6764 - acc: 0.5758
1792/5677 [========>.....................] - ETA: 11:32 - loss: 0.6765 - acc: 0.5765
1856/5677 [========>.....................] - ETA: 11:23 - loss: 0.6740 - acc: 0.5797
1920/5677 [=========>....................] - ETA: 11:11 - loss: 0.6742 - acc: 0.5781
1984/5677 [=========>....................] - ETA: 10:58 - loss: 0.6736 - acc: 0.5791
2048/5677 [=========>....................] - ETA: 10:45 - loss: 0.6742 - acc: 0.5796
2112/5677 [==========>...................] - ETA: 10:33 - loss: 0.6754 - acc: 0.5786
2176/5677 [==========>...................] - ETA: 10:22 - loss: 0.6756 - acc: 0.5795
2240/5677 [==========>...................] - ETA: 10:11 - loss: 0.6771 - acc: 0.5772
2304/5677 [===========>..................] - ETA: 9:58 - loss: 0.6773 - acc: 0.5781 
2368/5677 [===========>..................] - ETA: 9:46 - loss: 0.6774 - acc: 0.5781
2432/5677 [===========>..................] - ETA: 9:33 - loss: 0.6772 - acc: 0.5794
2496/5677 [============>.................] - ETA: 9:20 - loss: 0.6768 - acc: 0.5789
2560/5677 [============>.................] - ETA: 9:10 - loss: 0.6777 - acc: 0.5777
2624/5677 [============>.................] - ETA: 8:58 - loss: 0.6782 - acc: 0.5774
2688/5677 [=============>................] - ETA: 8:47 - loss: 0.6779 - acc: 0.5774
2752/5677 [=============>................] - ETA: 8:36 - loss: 0.6777 - acc: 0.5763
2816/5677 [=============>................] - ETA: 8:24 - loss: 0.6780 - acc: 0.5753
2880/5677 [==============>...............] - ETA: 8:11 - loss: 0.6778 - acc: 0.5760
2944/5677 [==============>...............] - ETA: 8:00 - loss: 0.6773 - acc: 0.5778
3008/5677 [==============>...............] - ETA: 7:48 - loss: 0.6785 - acc: 0.5748
3072/5677 [===============>..............] - ETA: 7:37 - loss: 0.6780 - acc: 0.5752
3136/5677 [===============>..............] - ETA: 7:25 - loss: 0.6773 - acc: 0.5772
3200/5677 [===============>..............] - ETA: 7:13 - loss: 0.6770 - acc: 0.5781
3264/5677 [================>.............] - ETA: 7:02 - loss: 0.6763 - acc: 0.5806
3328/5677 [================>.............] - ETA: 6:51 - loss: 0.6764 - acc: 0.5814
3392/5677 [================>.............] - ETA: 6:39 - loss: 0.6771 - acc: 0.5790
3456/5677 [=================>............] - ETA: 6:29 - loss: 0.6777 - acc: 0.5787
3520/5677 [=================>............] - ETA: 6:17 - loss: 0.6777 - acc: 0.5790
3584/5677 [=================>............] - ETA: 6:06 - loss: 0.6780 - acc: 0.5790
3648/5677 [==================>...........] - ETA: 5:55 - loss: 0.6782 - acc: 0.5784
3712/5677 [==================>...........] - ETA: 5:44 - loss: 0.6782 - acc: 0.5781
3776/5677 [==================>...........] - ETA: 5:32 - loss: 0.6786 - acc: 0.5773
3840/5677 [===================>..........] - ETA: 5:21 - loss: 0.6786 - acc: 0.5776
3904/5677 [===================>..........] - ETA: 5:10 - loss: 0.6787 - acc: 0.5771
3968/5677 [===================>..........] - ETA: 4:59 - loss: 0.6789 - acc: 0.5769
4032/5677 [====================>.........] - ETA: 4:48 - loss: 0.6791 - acc: 0.5764
4096/5677 [====================>.........] - ETA: 4:36 - loss: 0.6791 - acc: 0.5754
4160/5677 [====================>.........] - ETA: 4:25 - loss: 0.6795 - acc: 0.5750
4224/5677 [=====================>........] - ETA: 4:14 - loss: 0.6796 - acc: 0.5755
4288/5677 [=====================>........] - ETA: 4:02 - loss: 0.6798 - acc: 0.5758
4352/5677 [=====================>........] - ETA: 3:51 - loss: 0.6798 - acc: 0.5763
4416/5677 [======================>.......] - ETA: 3:40 - loss: 0.6791 - acc: 0.5772
4480/5677 [======================>.......] - ETA: 3:29 - loss: 0.6798 - acc: 0.5754
4544/5677 [=======================>......] - ETA: 3:18 - loss: 0.6797 - acc: 0.5748
4608/5677 [=======================>......] - ETA: 3:07 - loss: 0.6796 - acc: 0.5751
4672/5677 [=======================>......] - ETA: 2:55 - loss: 0.6795 - acc: 0.5758
4736/5677 [========================>.....] - ETA: 2:44 - loss: 0.6797 - acc: 0.5747
4800/5677 [========================>.....] - ETA: 2:33 - loss: 0.6791 - acc: 0.5763
4864/5677 [========================>.....] - ETA: 2:22 - loss: 0.6795 - acc: 0.5755
4928/5677 [=========================>....] - ETA: 2:10 - loss: 0.6798 - acc: 0.5747
4992/5677 [=========================>....] - ETA: 1:59 - loss: 0.6802 - acc: 0.5747
5056/5677 [=========================>....] - ETA: 1:48 - loss: 0.6805 - acc: 0.5740
5120/5677 [==========================>...] - ETA: 1:37 - loss: 0.6802 - acc: 0.5742
5184/5677 [==========================>...] - ETA: 1:26 - loss: 0.6799 - acc: 0.5747
5248/5677 [==========================>...] - ETA: 1:14 - loss: 0.6801 - acc: 0.5741
5312/5677 [===========================>..] - ETA: 1:03 - loss: 0.6805 - acc: 0.5730
5376/5677 [===========================>..] - ETA: 52s - loss: 0.6805 - acc: 0.5731 
5440/5677 [===========================>..] - ETA: 41s - loss: 0.6803 - acc: 0.5733
5504/5677 [============================>.] - ETA: 30s - loss: 0.6804 - acc: 0.5729
5568/5677 [============================>.] - ETA: 19s - loss: 0.6808 - acc: 0.5720
5632/5677 [============================>.] - ETA: 7s - loss: 0.6807 - acc: 0.5719 
5677/5677 [==============================] - 1028s 181ms/step - loss: 0.6808 - acc: 0.5709 - val_loss: 0.6821 - val_acc: 0.5626

Epoch 00007: val_acc did not improve from 0.56577
Epoch 8/10

  64/5677 [..............................] - ETA: 16:35 - loss: 0.6824 - acc: 0.5469
 128/5677 [..............................] - ETA: 16:19 - loss: 0.6810 - acc: 0.5781
 192/5677 [>.............................] - ETA: 16:09 - loss: 0.6784 - acc: 0.5781
 256/5677 [>.............................] - ETA: 16:08 - loss: 0.6778 - acc: 0.5703
 320/5677 [>.............................] - ETA: 15:54 - loss: 0.6766 - acc: 0.5813
 384/5677 [=>............................] - ETA: 15:54 - loss: 0.6730 - acc: 0.5938
 448/5677 [=>............................] - ETA: 15:51 - loss: 0.6758 - acc: 0.5804
 512/5677 [=>............................] - ETA: 15:40 - loss: 0.6783 - acc: 0.5723
 576/5677 [==>...........................] - ETA: 15:28 - loss: 0.6760 - acc: 0.5747
 640/5677 [==>...........................] - ETA: 15:13 - loss: 0.6758 - acc: 0.5719
 704/5677 [==>...........................] - ETA: 15:08 - loss: 0.6762 - acc: 0.5739
 768/5677 [===>..........................] - ETA: 14:58 - loss: 0.6777 - acc: 0.5703
 832/5677 [===>..........................] - ETA: 14:47 - loss: 0.6779 - acc: 0.5685
 896/5677 [===>..........................] - ETA: 14:34 - loss: 0.6779 - acc: 0.5703
 960/5677 [====>.........................] - ETA: 14:21 - loss: 0.6776 - acc: 0.5719
1024/5677 [====>.........................] - ETA: 14:07 - loss: 0.6766 - acc: 0.5732
1088/5677 [====>.........................] - ETA: 13:56 - loss: 0.6758 - acc: 0.5726
1152/5677 [=====>........................] - ETA: 13:42 - loss: 0.6748 - acc: 0.5738
1216/5677 [=====>........................] - ETA: 13:30 - loss: 0.6767 - acc: 0.5699
1280/5677 [=====>........................] - ETA: 13:19 - loss: 0.6782 - acc: 0.5703
1344/5677 [======>.......................] - ETA: 13:04 - loss: 0.6762 - acc: 0.5744
1408/5677 [======>.......................] - ETA: 12:52 - loss: 0.6767 - acc: 0.5732
1472/5677 [======>.......................] - ETA: 12:39 - loss: 0.6754 - acc: 0.5761
1536/5677 [=======>......................] - ETA: 12:24 - loss: 0.6772 - acc: 0.5736
1600/5677 [=======>......................] - ETA: 12:11 - loss: 0.6760 - acc: 0.5756
1664/5677 [=======>......................] - ETA: 12:02 - loss: 0.6763 - acc: 0.5751
1728/5677 [========>.....................] - ETA: 11:51 - loss: 0.6759 - acc: 0.5764
1792/5677 [========>.....................] - ETA: 11:38 - loss: 0.6755 - acc: 0.5770
1856/5677 [========>.....................] - ETA: 11:27 - loss: 0.6768 - acc: 0.5744
1920/5677 [=========>....................] - ETA: 11:15 - loss: 0.6769 - acc: 0.5734
1984/5677 [=========>....................] - ETA: 11:01 - loss: 0.6779 - acc: 0.5731
2048/5677 [=========>....................] - ETA: 10:49 - loss: 0.6797 - acc: 0.5698
2112/5677 [==========>...................] - ETA: 10:38 - loss: 0.6802 - acc: 0.5691
2176/5677 [==========>...................] - ETA: 10:27 - loss: 0.6805 - acc: 0.5689
2240/5677 [==========>...................] - ETA: 10:15 - loss: 0.6820 - acc: 0.5661
2304/5677 [===========>..................] - ETA: 10:03 - loss: 0.6828 - acc: 0.5651
2368/5677 [===========>..................] - ETA: 9:51 - loss: 0.6822 - acc: 0.5659 
2432/5677 [===========>..................] - ETA: 9:39 - loss: 0.6826 - acc: 0.5650
2496/5677 [============>.................] - ETA: 9:28 - loss: 0.6831 - acc: 0.5649
2560/5677 [============>.................] - ETA: 9:16 - loss: 0.6835 - acc: 0.5645
2624/5677 [============>.................] - ETA: 9:04 - loss: 0.6831 - acc: 0.5648
2688/5677 [=============>................] - ETA: 8:53 - loss: 0.6834 - acc: 0.5636
2752/5677 [=============>................] - ETA: 8:40 - loss: 0.6833 - acc: 0.5629
2816/5677 [=============>................] - ETA: 8:28 - loss: 0.6830 - acc: 0.5639
2880/5677 [==============>...............] - ETA: 8:16 - loss: 0.6829 - acc: 0.5628
2944/5677 [==============>...............] - ETA: 8:04 - loss: 0.6839 - acc: 0.5618
3008/5677 [==============>...............] - ETA: 7:53 - loss: 0.6836 - acc: 0.5625
3072/5677 [===============>..............] - ETA: 7:41 - loss: 0.6834 - acc: 0.5622
3136/5677 [===============>..............] - ETA: 7:30 - loss: 0.6835 - acc: 0.5628
3200/5677 [===============>..............] - ETA: 7:18 - loss: 0.6840 - acc: 0.5606
3264/5677 [================>.............] - ETA: 7:06 - loss: 0.6837 - acc: 0.5613
3328/5677 [================>.............] - ETA: 6:54 - loss: 0.6833 - acc: 0.5616
3392/5677 [================>.............] - ETA: 6:43 - loss: 0.6830 - acc: 0.5613
3456/5677 [=================>............] - ETA: 6:31 - loss: 0.6831 - acc: 0.5613
3520/5677 [=================>............] - ETA: 6:18 - loss: 0.6836 - acc: 0.5605
3584/5677 [=================>............] - ETA: 6:07 - loss: 0.6835 - acc: 0.5608
3648/5677 [==================>...........] - ETA: 5:55 - loss: 0.6831 - acc: 0.5617
3712/5677 [==================>...........] - ETA: 5:44 - loss: 0.6835 - acc: 0.5609
3776/5677 [==================>...........] - ETA: 5:32 - loss: 0.6829 - acc: 0.5625
3840/5677 [===================>..........] - ETA: 5:20 - loss: 0.6826 - acc: 0.5622
3904/5677 [===================>..........] - ETA: 5:09 - loss: 0.6821 - acc: 0.5633
3968/5677 [===================>..........] - ETA: 4:58 - loss: 0.6822 - acc: 0.5628
4032/5677 [====================>.........] - ETA: 4:46 - loss: 0.6818 - acc: 0.5637
4096/5677 [====================>.........] - ETA: 4:34 - loss: 0.6814 - acc: 0.5647
4160/5677 [====================>.........] - ETA: 4:23 - loss: 0.6813 - acc: 0.5649
4224/5677 [=====================>........] - ETA: 4:12 - loss: 0.6814 - acc: 0.5649
4288/5677 [=====================>........] - ETA: 4:00 - loss: 0.6811 - acc: 0.5660
4352/5677 [=====================>........] - ETA: 3:49 - loss: 0.6807 - acc: 0.5662
4416/5677 [======================>.......] - ETA: 3:38 - loss: 0.6810 - acc: 0.5654
4480/5677 [======================>.......] - ETA: 3:28 - loss: 0.6806 - acc: 0.5656
4544/5677 [=======================>......] - ETA: 3:17 - loss: 0.6809 - acc: 0.5658
4608/5677 [=======================>......] - ETA: 3:06 - loss: 0.6809 - acc: 0.5666
4672/5677 [=======================>......] - ETA: 2:55 - loss: 0.6808 - acc: 0.5672
4736/5677 [========================>.....] - ETA: 2:43 - loss: 0.6810 - acc: 0.5667
4800/5677 [========================>.....] - ETA: 2:32 - loss: 0.6815 - acc: 0.5652
4864/5677 [========================>.....] - ETA: 2:21 - loss: 0.6818 - acc: 0.5654
4928/5677 [=========================>....] - ETA: 2:10 - loss: 0.6817 - acc: 0.5651
4992/5677 [=========================>....] - ETA: 1:59 - loss: 0.6819 - acc: 0.5647
5056/5677 [=========================>....] - ETA: 1:48 - loss: 0.6814 - acc: 0.5657
5120/5677 [==========================>...] - ETA: 1:37 - loss: 0.6813 - acc: 0.5652
5184/5677 [==========================>...] - ETA: 1:25 - loss: 0.6816 - acc: 0.5644
5248/5677 [==========================>...] - ETA: 1:14 - loss: 0.6820 - acc: 0.5638
5312/5677 [===========================>..] - ETA: 1:03 - loss: 0.6822 - acc: 0.5633
5376/5677 [===========================>..] - ETA: 52s - loss: 0.6830 - acc: 0.5621 
5440/5677 [===========================>..] - ETA: 41s - loss: 0.6829 - acc: 0.5621
5504/5677 [============================>.] - ETA: 30s - loss: 0.6825 - acc: 0.5627
5568/5677 [============================>.] - ETA: 19s - loss: 0.6821 - acc: 0.5636
5632/5677 [============================>.] - ETA: 7s - loss: 0.6819 - acc: 0.5637 
5677/5677 [==============================] - 1029s 181ms/step - loss: 0.6815 - acc: 0.5647 - val_loss: 0.6796 - val_acc: 0.5689

Epoch 00008: val_acc improved from 0.56577 to 0.56894, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window21/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 9/10

  64/5677 [..............................] - ETA: 17:19 - loss: 0.6708 - acc: 0.5938
 128/5677 [..............................] - ETA: 16:51 - loss: 0.6731 - acc: 0.5781
 192/5677 [>.............................] - ETA: 16:02 - loss: 0.6924 - acc: 0.5573
 256/5677 [>.............................] - ETA: 15:30 - loss: 0.6954 - acc: 0.5586
 320/5677 [>.............................] - ETA: 15:38 - loss: 0.6934 - acc: 0.5594
 384/5677 [=>............................] - ETA: 15:08 - loss: 0.6957 - acc: 0.5573
 448/5677 [=>............................] - ETA: 15:14 - loss: 0.6922 - acc: 0.5625
 512/5677 [=>............................] - ETA: 14:56 - loss: 0.6899 - acc: 0.5664
 576/5677 [==>...........................] - ETA: 14:34 - loss: 0.6915 - acc: 0.5642
 640/5677 [==>...........................] - ETA: 14:27 - loss: 0.6915 - acc: 0.5656
 704/5677 [==>...........................] - ETA: 14:12 - loss: 0.6920 - acc: 0.5625
 768/5677 [===>..........................] - ETA: 14:04 - loss: 0.6950 - acc: 0.5495
 832/5677 [===>..........................] - ETA: 13:53 - loss: 0.6911 - acc: 0.5613
 896/5677 [===>..........................] - ETA: 13:43 - loss: 0.6917 - acc: 0.5614
 960/5677 [====>.........................] - ETA: 13:34 - loss: 0.6902 - acc: 0.5667
1024/5677 [====>.........................] - ETA: 13:21 - loss: 0.6898 - acc: 0.5635
1088/5677 [====>.........................] - ETA: 13:10 - loss: 0.6903 - acc: 0.5616
1152/5677 [=====>........................] - ETA: 13:06 - loss: 0.6887 - acc: 0.5616
1216/5677 [=====>........................] - ETA: 12:52 - loss: 0.6896 - acc: 0.5625
1280/5677 [=====>........................] - ETA: 12:41 - loss: 0.6887 - acc: 0.5656
1344/5677 [======>.......................] - ETA: 12:28 - loss: 0.6889 - acc: 0.5625
1408/5677 [======>.......................] - ETA: 12:17 - loss: 0.6883 - acc: 0.5618
1472/5677 [======>.......................] - ETA: 12:06 - loss: 0.6889 - acc: 0.5598
1536/5677 [=======>......................] - ETA: 11:54 - loss: 0.6896 - acc: 0.5579
1600/5677 [=======>......................] - ETA: 11:44 - loss: 0.6896 - acc: 0.5594
1664/5677 [=======>......................] - ETA: 11:31 - loss: 0.6896 - acc: 0.5583
1728/5677 [========>.....................] - ETA: 11:19 - loss: 0.6880 - acc: 0.5613
1792/5677 [========>.....................] - ETA: 11:07 - loss: 0.6885 - acc: 0.5597
1856/5677 [========>.....................] - ETA: 10:56 - loss: 0.6874 - acc: 0.5598
1920/5677 [=========>....................] - ETA: 10:46 - loss: 0.6862 - acc: 0.5630
1984/5677 [=========>....................] - ETA: 10:36 - loss: 0.6851 - acc: 0.5655
2048/5677 [=========>....................] - ETA: 10:24 - loss: 0.6852 - acc: 0.5645
2112/5677 [==========>...................] - ETA: 10:13 - loss: 0.6848 - acc: 0.5672
2176/5677 [==========>...................] - ETA: 10:00 - loss: 0.6845 - acc: 0.5676
2240/5677 [==========>...................] - ETA: 9:49 - loss: 0.6837 - acc: 0.5683 
2304/5677 [===========>..................] - ETA: 9:37 - loss: 0.6841 - acc: 0.5655
2368/5677 [===========>..................] - ETA: 9:25 - loss: 0.6834 - acc: 0.5667
2432/5677 [===========>..................] - ETA: 9:15 - loss: 0.6827 - acc: 0.5687
2496/5677 [============>.................] - ETA: 9:03 - loss: 0.6824 - acc: 0.5705
2560/5677 [============>.................] - ETA: 8:51 - loss: 0.6818 - acc: 0.5715
2624/5677 [============>.................] - ETA: 8:40 - loss: 0.6813 - acc: 0.5728
2688/5677 [=============>................] - ETA: 8:28 - loss: 0.6814 - acc: 0.5725
2752/5677 [=============>................] - ETA: 8:18 - loss: 0.6809 - acc: 0.5741
2816/5677 [=============>................] - ETA: 8:08 - loss: 0.6805 - acc: 0.5746
2880/5677 [==============>...............] - ETA: 7:57 - loss: 0.6797 - acc: 0.5760
2944/5677 [==============>...............] - ETA: 7:46 - loss: 0.6799 - acc: 0.5754
3008/5677 [==============>...............] - ETA: 7:35 - loss: 0.6796 - acc: 0.5751
3072/5677 [===============>..............] - ETA: 7:24 - loss: 0.6793 - acc: 0.5755
3136/5677 [===============>..............] - ETA: 7:13 - loss: 0.6788 - acc: 0.5765
3200/5677 [===============>..............] - ETA: 7:03 - loss: 0.6794 - acc: 0.5753
3264/5677 [================>.............] - ETA: 6:53 - loss: 0.6793 - acc: 0.5748
3328/5677 [================>.............] - ETA: 6:42 - loss: 0.6792 - acc: 0.5754
3392/5677 [================>.............] - ETA: 6:32 - loss: 0.6789 - acc: 0.5743
3456/5677 [=================>............] - ETA: 6:21 - loss: 0.6786 - acc: 0.5749
3520/5677 [=================>............] - ETA: 6:09 - loss: 0.6783 - acc: 0.5764
3584/5677 [=================>............] - ETA: 5:58 - loss: 0.6793 - acc: 0.5748
3648/5677 [==================>...........] - ETA: 5:47 - loss: 0.6796 - acc: 0.5748
3712/5677 [==================>...........] - ETA: 5:36 - loss: 0.6794 - acc: 0.5746
3776/5677 [==================>...........] - ETA: 5:25 - loss: 0.6799 - acc: 0.5744
3840/5677 [===================>..........] - ETA: 5:14 - loss: 0.6796 - acc: 0.5758
3904/5677 [===================>..........] - ETA: 5:03 - loss: 0.6802 - acc: 0.5748
3968/5677 [===================>..........] - ETA: 4:52 - loss: 0.6797 - acc: 0.5764
4032/5677 [====================>.........] - ETA: 4:40 - loss: 0.6795 - acc: 0.5759
4096/5677 [====================>.........] - ETA: 4:29 - loss: 0.6796 - acc: 0.5750
4160/5677 [====================>.........] - ETA: 4:18 - loss: 0.6801 - acc: 0.5743
4224/5677 [=====================>........] - ETA: 4:07 - loss: 0.6801 - acc: 0.5746
4288/5677 [=====================>........] - ETA: 3:56 - loss: 0.6798 - acc: 0.5735
4352/5677 [=====================>........] - ETA: 3:46 - loss: 0.6790 - acc: 0.5749
4416/5677 [======================>.......] - ETA: 3:35 - loss: 0.6787 - acc: 0.5752
4480/5677 [======================>.......] - ETA: 3:24 - loss: 0.6783 - acc: 0.5757
4544/5677 [=======================>......] - ETA: 3:14 - loss: 0.6780 - acc: 0.5757
4608/5677 [=======================>......] - ETA: 3:03 - loss: 0.6775 - acc: 0.5764
4672/5677 [=======================>......] - ETA: 2:52 - loss: 0.6775 - acc: 0.5773
4736/5677 [========================>.....] - ETA: 2:41 - loss: 0.6772 - acc: 0.5773
4800/5677 [========================>.....] - ETA: 2:31 - loss: 0.6768 - acc: 0.5779
4864/5677 [========================>.....] - ETA: 2:20 - loss: 0.6771 - acc: 0.5777
4928/5677 [=========================>....] - ETA: 2:09 - loss: 0.6782 - acc: 0.5757
4992/5677 [=========================>....] - ETA: 1:58 - loss: 0.6786 - acc: 0.5751
5056/5677 [=========================>....] - ETA: 1:47 - loss: 0.6788 - acc: 0.5752
5120/5677 [==========================>...] - ETA: 1:36 - loss: 0.6794 - acc: 0.5736
5184/5677 [==========================>...] - ETA: 1:25 - loss: 0.6801 - acc: 0.5727
5248/5677 [==========================>...] - ETA: 1:14 - loss: 0.6800 - acc: 0.5732
5312/5677 [===========================>..] - ETA: 1:03 - loss: 0.6803 - acc: 0.5713
5376/5677 [===========================>..] - ETA: 52s - loss: 0.6806 - acc: 0.5711 
5440/5677 [===========================>..] - ETA: 40s - loss: 0.6806 - acc: 0.5713
5504/5677 [============================>.] - ETA: 29s - loss: 0.6809 - acc: 0.5710
5568/5677 [============================>.] - ETA: 18s - loss: 0.6809 - acc: 0.5713
5632/5677 [============================>.] - ETA: 7s - loss: 0.6806 - acc: 0.5717 
5677/5677 [==============================] - 1013s 178ms/step - loss: 0.6807 - acc: 0.5707 - val_loss: 0.6818 - val_acc: 0.5594

Epoch 00009: val_acc did not improve from 0.56894
Epoch 10/10

  64/5677 [..............................] - ETA: 16:01 - loss: 0.6890 - acc: 0.5000
 128/5677 [..............................] - ETA: 15:43 - loss: 0.6880 - acc: 0.5469
 192/5677 [>.............................] - ETA: 15:46 - loss: 0.6866 - acc: 0.5312
 256/5677 [>.............................] - ETA: 15:29 - loss: 0.6815 - acc: 0.5625
 320/5677 [>.............................] - ETA: 15:12 - loss: 0.6837 - acc: 0.5563
 384/5677 [=>............................] - ETA: 15:01 - loss: 0.6842 - acc: 0.5599
 448/5677 [=>............................] - ETA: 14:49 - loss: 0.6822 - acc: 0.5670
 512/5677 [=>............................] - ETA: 14:36 - loss: 0.6822 - acc: 0.5742
 576/5677 [==>...........................] - ETA: 14:23 - loss: 0.6802 - acc: 0.5799
 640/5677 [==>...........................] - ETA: 14:12 - loss: 0.6822 - acc: 0.5734
 704/5677 [==>...........................] - ETA: 13:56 - loss: 0.6839 - acc: 0.5668
 768/5677 [===>..........................] - ETA: 13:46 - loss: 0.6828 - acc: 0.5703
 832/5677 [===>..........................] - ETA: 13:43 - loss: 0.6814 - acc: 0.5709
 896/5677 [===>..........................] - ETA: 13:28 - loss: 0.6809 - acc: 0.5670
 960/5677 [====>.........................] - ETA: 13:19 - loss: 0.6814 - acc: 0.5625
1024/5677 [====>.........................] - ETA: 13:05 - loss: 0.6829 - acc: 0.5566
1088/5677 [====>.........................] - ETA: 13:00 - loss: 0.6827 - acc: 0.5579
1152/5677 [=====>........................] - ETA: 12:53 - loss: 0.6813 - acc: 0.5677
1216/5677 [=====>........................] - ETA: 12:41 - loss: 0.6817 - acc: 0.5650
1280/5677 [=====>........................] - ETA: 12:31 - loss: 0.6825 - acc: 0.5648
1344/5677 [======>.......................] - ETA: 12:21 - loss: 0.6820 - acc: 0.5677
1408/5677 [======>.......................] - ETA: 12:11 - loss: 0.6832 - acc: 0.5675
1472/5677 [======>.......................] - ETA: 12:01 - loss: 0.6834 - acc: 0.5659
1536/5677 [=======>......................] - ETA: 11:51 - loss: 0.6838 - acc: 0.5671
1600/5677 [=======>......................] - ETA: 11:40 - loss: 0.6831 - acc: 0.5669
1664/5677 [=======>......................] - ETA: 11:31 - loss: 0.6832 - acc: 0.5655
1728/5677 [========>.....................] - ETA: 11:20 - loss: 0.6841 - acc: 0.5642
1792/5677 [========>.....................] - ETA: 11:09 - loss: 0.6830 - acc: 0.5670
1856/5677 [========>.....................] - ETA: 10:59 - loss: 0.6815 - acc: 0.5711
1920/5677 [=========>....................] - ETA: 10:47 - loss: 0.6830 - acc: 0.5672
1984/5677 [=========>....................] - ETA: 10:37 - loss: 0.6829 - acc: 0.5665
2048/5677 [=========>....................] - ETA: 10:25 - loss: 0.6808 - acc: 0.5728
2112/5677 [==========>...................] - ETA: 10:13 - loss: 0.6812 - acc: 0.5696
2176/5677 [==========>...................] - ETA: 10:02 - loss: 0.6805 - acc: 0.5699
2240/5677 [==========>...................] - ETA: 9:50 - loss: 0.6798 - acc: 0.5728 
2304/5677 [===========>..................] - ETA: 9:38 - loss: 0.6807 - acc: 0.5716
2368/5677 [===========>..................] - ETA: 9:26 - loss: 0.6812 - acc: 0.5714
2432/5677 [===========>..................] - ETA: 9:14 - loss: 0.6813 - acc: 0.5711
2496/5677 [============>.................] - ETA: 9:03 - loss: 0.6821 - acc: 0.5705
2560/5677 [============>.................] - ETA: 8:52 - loss: 0.6815 - acc: 0.5719
2624/5677 [============>.................] - ETA: 8:40 - loss: 0.6817 - acc: 0.5713
2688/5677 [=============>................] - ETA: 8:29 - loss: 0.6829 - acc: 0.5688
2752/5677 [=============>................] - ETA: 8:18 - loss: 0.6829 - acc: 0.5687
2816/5677 [=============>................] - ETA: 8:08 - loss: 0.6825 - acc: 0.5689
2880/5677 [==============>...............] - ETA: 7:57 - loss: 0.6816 - acc: 0.5691
2944/5677 [==============>...............] - ETA: 7:46 - loss: 0.6804 - acc: 0.5720
3008/5677 [==============>...............] - ETA: 7:35 - loss: 0.6797 - acc: 0.5725
3072/5677 [===============>..............] - ETA: 7:24 - loss: 0.6796 - acc: 0.5723
3136/5677 [===============>..............] - ETA: 7:13 - loss: 0.6800 - acc: 0.5711
3200/5677 [===============>..............] - ETA: 7:02 - loss: 0.6799 - acc: 0.5722
3264/5677 [================>.............] - ETA: 6:52 - loss: 0.6802 - acc: 0.5714
3328/5677 [================>.............] - ETA: 6:40 - loss: 0.6805 - acc: 0.5703
3392/5677 [================>.............] - ETA: 6:30 - loss: 0.6802 - acc: 0.5713
3456/5677 [=================>............] - ETA: 6:19 - loss: 0.6804 - acc: 0.5700
3520/5677 [=================>............] - ETA: 6:08 - loss: 0.6807 - acc: 0.5699
3584/5677 [=================>............] - ETA: 5:57 - loss: 0.6807 - acc: 0.5695
3648/5677 [==================>...........] - ETA: 5:46 - loss: 0.6812 - acc: 0.5685
3712/5677 [==================>...........] - ETA: 5:35 - loss: 0.6809 - acc: 0.5692
3776/5677 [==================>...........] - ETA: 5:24 - loss: 0.6805 - acc: 0.5689
3840/5677 [===================>..........] - ETA: 5:14 - loss: 0.6803 - acc: 0.5687
3904/5677 [===================>..........] - ETA: 5:02 - loss: 0.6803 - acc: 0.5679
3968/5677 [===================>..........] - ETA: 4:52 - loss: 0.6804 - acc: 0.5673
4032/5677 [====================>.........] - ETA: 4:40 - loss: 0.6806 - acc: 0.5667
4096/5677 [====================>.........] - ETA: 4:29 - loss: 0.6807 - acc: 0.5662
4160/5677 [====================>.........] - ETA: 4:18 - loss: 0.6807 - acc: 0.5663
4224/5677 [=====================>........] - ETA: 4:07 - loss: 0.6806 - acc: 0.5670
4288/5677 [=====================>........] - ETA: 3:56 - loss: 0.6809 - acc: 0.5676
4352/5677 [=====================>........] - ETA: 3:45 - loss: 0.6807 - acc: 0.5682
4416/5677 [======================>.......] - ETA: 3:34 - loss: 0.6799 - acc: 0.5695
4480/5677 [======================>.......] - ETA: 3:23 - loss: 0.6799 - acc: 0.5687
4544/5677 [=======================>......] - ETA: 3:13 - loss: 0.6799 - acc: 0.5687
4608/5677 [=======================>......] - ETA: 3:02 - loss: 0.6805 - acc: 0.5686
4672/5677 [=======================>......] - ETA: 2:51 - loss: 0.6801 - acc: 0.5685
4736/5677 [========================>.....] - ETA: 2:40 - loss: 0.6798 - acc: 0.5695
4800/5677 [========================>.....] - ETA: 2:29 - loss: 0.6793 - acc: 0.5700
4864/5677 [========================>.....] - ETA: 2:18 - loss: 0.6791 - acc: 0.5703
4928/5677 [=========================>....] - ETA: 2:07 - loss: 0.6784 - acc: 0.5716
4992/5677 [=========================>....] - ETA: 1:56 - loss: 0.6788 - acc: 0.5715
5056/5677 [=========================>....] - ETA: 1:46 - loss: 0.6784 - acc: 0.5724
5120/5677 [==========================>...] - ETA: 1:35 - loss: 0.6778 - acc: 0.5727
5184/5677 [==========================>...] - ETA: 1:24 - loss: 0.6773 - acc: 0.5737
5248/5677 [==========================>...] - ETA: 1:13 - loss: 0.6774 - acc: 0.5732
5312/5677 [===========================>..] - ETA: 1:02 - loss: 0.6781 - acc: 0.5713
5376/5677 [===========================>..] - ETA: 51s - loss: 0.6780 - acc: 0.5718 
5440/5677 [===========================>..] - ETA: 40s - loss: 0.6779 - acc: 0.5715
5504/5677 [============================>.] - ETA: 29s - loss: 0.6782 - acc: 0.5710
5568/5677 [============================>.] - ETA: 18s - loss: 0.6780 - acc: 0.5717
5632/5677 [============================>.] - ETA: 7s - loss: 0.6782 - acc: 0.5712 
5677/5677 [==============================] - 1005s 177ms/step - loss: 0.6782 - acc: 0.5713 - val_loss: 0.6840 - val_acc: 0.5563

Epoch 00010: val_acc did not improve from 0.56894
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fdbb45ca890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fdbb45ca890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fddac222890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fddac222890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe76b328790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fe76b328790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdbb410d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdbb410d8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdbb403e910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdbb403e910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbb409db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbb409db90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdbb410d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdbb410d390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbb4303e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbb4303e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdb747c7ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdb747c7ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdb7472e790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdb7472e790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb74789ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb74789ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb747c79d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb747c79d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb7471f050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb7471f050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdb742df2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdb742df2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdb7427f250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fdb7427f250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb74193bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb74193bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb746de0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb746de0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbd42dac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdbd42dac50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd764405290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd764405290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b477c250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd8b477c250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdcd849c9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdcd849c9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb54407910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb54407910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb3c414d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fdb3c414d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdb3c51c190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fdb3c51c190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda6c05b4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda6c05b4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda6c056d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda6c056d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb3c54dc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fdb3c54dc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda38559350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda38559350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda38534e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda38534e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda38487e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda38487e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda382d3d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda382d3d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda38634a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda38634a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda385159d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda385159d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda38431a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda38431a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda3815af50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda3815af50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda38431b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda38431b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda381cbe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda381cbe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda3814dd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda3814dd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda1469ef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda1469ef90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda380caad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda380caad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda1459f0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda1459f0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda1469e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda1469e550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda1468b2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda1468b2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda14341b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda14341b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda145a9910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fda145a9910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda1435d250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda1435d250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda143d59d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda143d59d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda142d7650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda142d7650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda1404fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fda1404fc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd9f4710190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd9f4710190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9f4725c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9f4725c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda1404f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fda1404f1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9f45e72d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9f45e72d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd9f472e3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd9f472e3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd9f43bf0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd9f43bf0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda14064ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fda14064ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd9f46fcf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd9f46fcf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9f455cd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9f455cd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd9f42f6110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fd9f42f6110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd9f407e810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fd9f407e810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9f41f0c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9f41f0c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd9f42a2110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fd9f42a2110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9f40eb290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fd9f40eb290>>: AttributeError: module 'gast' has no attribute 'Str'
04window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 10:44
 128/1578 [=>............................] - ETA: 5:44 
 192/1578 [==>...........................] - ETA: 4:04
 256/1578 [===>..........................] - ETA: 3:10
 320/1578 [=====>........................] - ETA: 2:37
 384/1578 [======>.......................] - ETA: 2:13
 448/1578 [=======>......................] - ETA: 1:57
 512/1578 [========>.....................] - ETA: 1:46
 576/1578 [=========>....................] - ETA: 1:36
 640/1578 [===========>..................] - ETA: 1:27
 704/1578 [============>.................] - ETA: 1:18
 768/1578 [=============>................] - ETA: 1:10
 832/1578 [==============>...............] - ETA: 1:03
 896/1578 [================>.............] - ETA: 55s 
 960/1578 [=================>............] - ETA: 49s
1024/1578 [==================>...........] - ETA: 43s
1088/1578 [===================>..........] - ETA: 37s
1152/1578 [====================>.........] - ETA: 31s
1216/1578 [======================>.......] - ETA: 26s
1280/1578 [=======================>......] - ETA: 21s
1344/1578 [========================>.....] - ETA: 16s
1408/1578 [=========================>....] - ETA: 11s
1472/1578 [==========================>...] - ETA: 7s 
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 108s 68ms/step
loss: 0.6762326732183439
acc: 0.5811153357926431
