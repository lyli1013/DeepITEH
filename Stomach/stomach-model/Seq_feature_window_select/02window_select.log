nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 3154
样本个数 6308
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9d3a27f350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9d3a27f350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9dc1d19490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9dc1d19490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db97c5b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db97c5b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db9864990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db9864990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9db9855750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9db9855750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d3a2116d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d3a2116d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d3a1e02d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d3a1e02d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d3a028710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d3a028710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d3a061950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d3a061950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d39f65e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d39f65e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39eef450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39eef450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d39eefa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d39eefa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db98624d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db98624d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d39f65d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d39f65d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d39caff50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d39caff50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db9862b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db9862b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d3a246290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d3a246290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39c0fb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39c0fb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d39ab63d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d39ab63d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d398a4110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d398a4110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39abedd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39abedd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d398ae6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d398ae6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39866bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39866bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d396ef390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d396ef390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d3951dd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d3951dd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d3970ad10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d3970ad10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d397c2ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d397c2ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39639d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39639d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d293835d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d293835d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d292ddf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d292ddf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d293820d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d293820d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d39585350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d39585350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d29191250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d29191250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d29231cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d29231cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d290bf1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d290bf1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d29085090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d29085090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d39f9d250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d39f9d250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d3a1e0dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d3a1e0dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d28d343d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d28d343d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d28d88510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d28d88510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d28f79810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d28f79810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d28f794d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d28f794d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d28db5e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d28db5e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d209e1390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d209e1390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d2094dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d2094dc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d28c21250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d28c21250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d209e1ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d209e1ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d209afe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d209afe50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d206f6810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d206f6810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d20948d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d20948d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d206f6bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d206f6bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d20959290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d20959290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39427510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d39427510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d203e1410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d203e1410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d18396050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d18396050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d20663910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d20663910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d2067ad10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d2067ad10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d18303a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d18303a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d180af3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d180af3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d1809f150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d1809f150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d17fd8050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d17fd8050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d18307d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d18307d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d17eb5bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d17eb5bd0>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-28 22:30:21.974026: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-28 22:30:22.185267: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-28 22:30:22.267955: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562ba4d8a1c0 executing computations on platform Host. Devices:
2022-11-28 22:30:22.268178: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-28 22:30:23.482403: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 41:56 - loss: 0.7474 - acc: 0.4531
 128/5677 [..............................] - ETA: 26:55 - loss: 0.8770 - acc: 0.4141
 192/5677 [>.............................] - ETA: 21:22 - loss: 0.8470 - acc: 0.4427
 256/5677 [>.............................] - ETA: 18:51 - loss: 0.8312 - acc: 0.4609
 320/5677 [>.............................] - ETA: 17:50 - loss: 0.8076 - acc: 0.4750
 384/5677 [=>............................] - ETA: 16:32 - loss: 0.7999 - acc: 0.4583
 448/5677 [=>............................] - ETA: 15:34 - loss: 0.7970 - acc: 0.4554
 512/5677 [=>............................] - ETA: 16:03 - loss: 0.7809 - acc: 0.4727
 576/5677 [==>...........................] - ETA: 15:52 - loss: 0.7737 - acc: 0.4809
 640/5677 [==>...........................] - ETA: 15:07 - loss: 0.7744 - acc: 0.4719
 704/5677 [==>...........................] - ETA: 14:27 - loss: 0.7703 - acc: 0.4759
 768/5677 [===>..........................] - ETA: 13:49 - loss: 0.7750 - acc: 0.4701
 832/5677 [===>..........................] - ETA: 13:12 - loss: 0.7717 - acc: 0.4760
 896/5677 [===>..........................] - ETA: 12:40 - loss: 0.7691 - acc: 0.4754
 960/5677 [====>.........................] - ETA: 12:12 - loss: 0.7661 - acc: 0.4760
1024/5677 [====>.........................] - ETA: 11:47 - loss: 0.7634 - acc: 0.4756
1088/5677 [====>.........................] - ETA: 11:22 - loss: 0.7632 - acc: 0.4779
1152/5677 [=====>........................] - ETA: 11:01 - loss: 0.7613 - acc: 0.4800
1216/5677 [=====>........................] - ETA: 10:41 - loss: 0.7606 - acc: 0.4811
1280/5677 [=====>........................] - ETA: 10:21 - loss: 0.7596 - acc: 0.4844
1344/5677 [======>.......................] - ETA: 10:05 - loss: 0.7605 - acc: 0.4807
1408/5677 [======>.......................] - ETA: 9:47 - loss: 0.7592 - acc: 0.4822 
1472/5677 [======>.......................] - ETA: 9:31 - loss: 0.7566 - acc: 0.4844
1536/5677 [=======>......................] - ETA: 9:16 - loss: 0.7548 - acc: 0.4870
1600/5677 [=======>......................] - ETA: 9:02 - loss: 0.7524 - acc: 0.4888
1664/5677 [=======>......................] - ETA: 8:48 - loss: 0.7523 - acc: 0.4886
1728/5677 [========>.....................] - ETA: 8:35 - loss: 0.7521 - acc: 0.4878
1792/5677 [========>.....................] - ETA: 8:22 - loss: 0.7489 - acc: 0.4900
1856/5677 [========>.....................] - ETA: 8:10 - loss: 0.7475 - acc: 0.4925
1920/5677 [=========>....................] - ETA: 7:58 - loss: 0.7466 - acc: 0.4938
1984/5677 [=========>....................] - ETA: 7:47 - loss: 0.7457 - acc: 0.4940
2048/5677 [=========>....................] - ETA: 7:36 - loss: 0.7441 - acc: 0.4966
2112/5677 [==========>...................] - ETA: 7:24 - loss: 0.7447 - acc: 0.4957
2176/5677 [==========>...................] - ETA: 7:14 - loss: 0.7445 - acc: 0.4954
2240/5677 [==========>...................] - ETA: 7:04 - loss: 0.7444 - acc: 0.4933
2304/5677 [===========>..................] - ETA: 6:55 - loss: 0.7429 - acc: 0.4957
2368/5677 [===========>..................] - ETA: 6:45 - loss: 0.7429 - acc: 0.4937
2432/5677 [===========>..................] - ETA: 6:35 - loss: 0.7422 - acc: 0.4951
2496/5677 [============>.................] - ETA: 6:25 - loss: 0.7418 - acc: 0.4948
2560/5677 [============>.................] - ETA: 6:15 - loss: 0.7413 - acc: 0.4945
2624/5677 [============>.................] - ETA: 6:06 - loss: 0.7416 - acc: 0.4931
2688/5677 [=============>................] - ETA: 5:56 - loss: 0.7408 - acc: 0.4940
2752/5677 [=============>................] - ETA: 5:47 - loss: 0.7391 - acc: 0.4964
2816/5677 [=============>................] - ETA: 5:38 - loss: 0.7381 - acc: 0.4982
2880/5677 [==============>...............] - ETA: 5:30 - loss: 0.7371 - acc: 0.4993
2944/5677 [==============>...............] - ETA: 5:21 - loss: 0.7359 - acc: 0.4997
3008/5677 [==============>...............] - ETA: 5:12 - loss: 0.7370 - acc: 0.4983
3072/5677 [===============>..............] - ETA: 5:04 - loss: 0.7365 - acc: 0.4984
3136/5677 [===============>..............] - ETA: 4:56 - loss: 0.7364 - acc: 0.4974
3200/5677 [===============>..............] - ETA: 4:48 - loss: 0.7352 - acc: 0.5003
3264/5677 [================>.............] - ETA: 4:40 - loss: 0.7354 - acc: 0.5006
3328/5677 [================>.............] - ETA: 4:32 - loss: 0.7363 - acc: 0.4988
3392/5677 [================>.............] - ETA: 4:24 - loss: 0.7359 - acc: 0.4994
3456/5677 [=================>............] - ETA: 4:17 - loss: 0.7360 - acc: 0.4986
3520/5677 [=================>............] - ETA: 4:09 - loss: 0.7360 - acc: 0.4989
3584/5677 [=================>............] - ETA: 4:01 - loss: 0.7360 - acc: 0.4989
3648/5677 [==================>...........] - ETA: 3:53 - loss: 0.7360 - acc: 0.4989
3712/5677 [==================>...........] - ETA: 3:45 - loss: 0.7357 - acc: 0.4981
3776/5677 [==================>...........] - ETA: 3:37 - loss: 0.7347 - acc: 0.4981
3840/5677 [===================>..........] - ETA: 3:29 - loss: 0.7345 - acc: 0.4982
3904/5677 [===================>..........] - ETA: 3:21 - loss: 0.7342 - acc: 0.4985
3968/5677 [===================>..........] - ETA: 3:13 - loss: 0.7344 - acc: 0.4970
4032/5677 [====================>.........] - ETA: 3:05 - loss: 0.7335 - acc: 0.4978
4096/5677 [====================>.........] - ETA: 2:57 - loss: 0.7327 - acc: 0.4985
4160/5677 [====================>.........] - ETA: 2:49 - loss: 0.7328 - acc: 0.4969
4224/5677 [=====================>........] - ETA: 2:41 - loss: 0.7329 - acc: 0.4983
4288/5677 [=====================>........] - ETA: 2:33 - loss: 0.7324 - acc: 0.4986
4352/5677 [=====================>........] - ETA: 2:25 - loss: 0.7318 - acc: 0.4995
4416/5677 [======================>.......] - ETA: 2:18 - loss: 0.7308 - acc: 0.5000
4480/5677 [======================>.......] - ETA: 2:10 - loss: 0.7310 - acc: 0.4998
4544/5677 [=======================>......] - ETA: 2:03 - loss: 0.7306 - acc: 0.4996
4608/5677 [=======================>......] - ETA: 1:55 - loss: 0.7300 - acc: 0.5000
4672/5677 [=======================>......] - ETA: 1:48 - loss: 0.7293 - acc: 0.5017
4736/5677 [========================>.....] - ETA: 1:41 - loss: 0.7285 - acc: 0.5025
4800/5677 [========================>.....] - ETA: 1:34 - loss: 0.7285 - acc: 0.5015
4864/5677 [========================>.....] - ETA: 1:26 - loss: 0.7282 - acc: 0.5010
4928/5677 [=========================>....] - ETA: 1:19 - loss: 0.7281 - acc: 0.5006
4992/5677 [=========================>....] - ETA: 1:12 - loss: 0.7274 - acc: 0.5018
5056/5677 [=========================>....] - ETA: 1:05 - loss: 0.7267 - acc: 0.5030
5120/5677 [==========================>...] - ETA: 58s - loss: 0.7263 - acc: 0.5037 
5184/5677 [==========================>...] - ETA: 51s - loss: 0.7263 - acc: 0.5031
5248/5677 [==========================>...] - ETA: 44s - loss: 0.7257 - acc: 0.5040
5312/5677 [===========================>..] - ETA: 38s - loss: 0.7255 - acc: 0.5053
5376/5677 [===========================>..] - ETA: 31s - loss: 0.7251 - acc: 0.5056
5440/5677 [===========================>..] - ETA: 24s - loss: 0.7248 - acc: 0.5059
5504/5677 [============================>.] - ETA: 17s - loss: 0.7247 - acc: 0.5060
5568/5677 [============================>.] - ETA: 11s - loss: 0.7241 - acc: 0.5068
5632/5677 [============================>.] - ETA: 4s - loss: 0.7239 - acc: 0.5059 
5677/5677 [==============================] - 606s 107ms/step - loss: 0.7236 - acc: 0.5064 - val_loss: 0.6913 - val_acc: 0.5214

Epoch 00001: val_acc improved from -inf to 0.52139, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window06/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 8:40 - loss: 0.6849 - acc: 0.5938
 128/5677 [..............................] - ETA: 9:03 - loss: 0.6970 - acc: 0.5234
 192/5677 [>.............................] - ETA: 8:31 - loss: 0.7037 - acc: 0.5104
 256/5677 [>.............................] - ETA: 7:48 - loss: 0.7024 - acc: 0.5156
 320/5677 [>.............................] - ETA: 7:40 - loss: 0.6953 - acc: 0.5219
 384/5677 [=>............................] - ETA: 7:51 - loss: 0.6961 - acc: 0.5182
 448/5677 [=>............................] - ETA: 7:41 - loss: 0.6969 - acc: 0.5179
 512/5677 [=>............................] - ETA: 7:24 - loss: 0.7005 - acc: 0.5117
 576/5677 [==>...........................] - ETA: 7:11 - loss: 0.6992 - acc: 0.5174
 640/5677 [==>...........................] - ETA: 7:03 - loss: 0.7020 - acc: 0.5078
 704/5677 [==>...........................] - ETA: 7:05 - loss: 0.7018 - acc: 0.5114
 768/5677 [===>..........................] - ETA: 7:06 - loss: 0.7034 - acc: 0.5078
 832/5677 [===>..........................] - ETA: 7:00 - loss: 0.7035 - acc: 0.5060
 896/5677 [===>..........................] - ETA: 6:49 - loss: 0.7024 - acc: 0.5123
 960/5677 [====>.........................] - ETA: 6:40 - loss: 0.7001 - acc: 0.5208
1024/5677 [====>.........................] - ETA: 6:32 - loss: 0.7007 - acc: 0.5234
1088/5677 [====>.........................] - ETA: 6:26 - loss: 0.7025 - acc: 0.5202
1152/5677 [=====>........................] - ETA: 6:26 - loss: 0.7026 - acc: 0.5234
1216/5677 [=====>........................] - ETA: 6:23 - loss: 0.7041 - acc: 0.5230
1280/5677 [=====>........................] - ETA: 6:20 - loss: 0.7044 - acc: 0.5227
1344/5677 [======>.......................] - ETA: 6:12 - loss: 0.7053 - acc: 0.5223
1408/5677 [======>.......................] - ETA: 6:04 - loss: 0.7034 - acc: 0.5263
1472/5677 [======>.......................] - ETA: 5:56 - loss: 0.7049 - acc: 0.5245
1536/5677 [=======>......................] - ETA: 5:48 - loss: 0.7041 - acc: 0.5241
1600/5677 [=======>......................] - ETA: 5:40 - loss: 0.7049 - acc: 0.5250
1664/5677 [=======>......................] - ETA: 5:36 - loss: 0.7042 - acc: 0.5270
1728/5677 [========>.....................] - ETA: 5:34 - loss: 0.7040 - acc: 0.5260
1792/5677 [========>.....................] - ETA: 5:30 - loss: 0.7041 - acc: 0.5257
1856/5677 [========>.....................] - ETA: 5:25 - loss: 0.7050 - acc: 0.5237
1920/5677 [=========>....................] - ETA: 5:21 - loss: 0.7046 - acc: 0.5250
1984/5677 [=========>....................] - ETA: 5:16 - loss: 0.7055 - acc: 0.5212
2048/5677 [=========>....................] - ETA: 5:08 - loss: 0.7054 - acc: 0.5220
2112/5677 [==========>...................] - ETA: 5:01 - loss: 0.7056 - acc: 0.5208
2176/5677 [==========>...................] - ETA: 4:54 - loss: 0.7057 - acc: 0.5202
2240/5677 [==========>...................] - ETA: 4:48 - loss: 0.7066 - acc: 0.5170
2304/5677 [===========>..................] - ETA: 4:43 - loss: 0.7055 - acc: 0.5191
2368/5677 [===========>..................] - ETA: 4:40 - loss: 0.7051 - acc: 0.5186
2432/5677 [===========>..................] - ETA: 4:36 - loss: 0.7037 - acc: 0.5206
2496/5677 [============>.................] - ETA: 4:32 - loss: 0.7029 - acc: 0.5208
2560/5677 [============>.................] - ETA: 4:27 - loss: 0.7030 - acc: 0.5203
2624/5677 [============>.................] - ETA: 4:20 - loss: 0.7033 - acc: 0.5206
2688/5677 [=============>................] - ETA: 4:14 - loss: 0.7030 - acc: 0.5227
2752/5677 [=============>................] - ETA: 4:08 - loss: 0.7029 - acc: 0.5222
2816/5677 [=============>................] - ETA: 4:01 - loss: 0.7050 - acc: 0.5195
2880/5677 [==============>...............] - ETA: 3:55 - loss: 0.7060 - acc: 0.5188
2944/5677 [==============>...............] - ETA: 3:50 - loss: 0.7054 - acc: 0.5194
3008/5677 [==============>...............] - ETA: 3:46 - loss: 0.7054 - acc: 0.5180
3072/5677 [===============>..............] - ETA: 3:42 - loss: 0.7053 - acc: 0.5169
3136/5677 [===============>..............] - ETA: 3:37 - loss: 0.7051 - acc: 0.5172
3200/5677 [===============>..............] - ETA: 3:32 - loss: 0.7046 - acc: 0.5188
3264/5677 [================>.............] - ETA: 3:27 - loss: 0.7048 - acc: 0.5190
3328/5677 [================>.............] - ETA: 3:21 - loss: 0.7050 - acc: 0.5183
3392/5677 [================>.............] - ETA: 3:15 - loss: 0.7043 - acc: 0.5180
3456/5677 [=================>............] - ETA: 3:09 - loss: 0.7047 - acc: 0.5168
3520/5677 [=================>............] - ETA: 3:03 - loss: 0.7050 - acc: 0.5153
3584/5677 [=================>............] - ETA: 2:57 - loss: 0.7050 - acc: 0.5153
3648/5677 [==================>...........] - ETA: 2:52 - loss: 0.7047 - acc: 0.5156
3712/5677 [==================>...........] - ETA: 2:47 - loss: 0.7045 - acc: 0.5164
3776/5677 [==================>...........] - ETA: 2:42 - loss: 0.7039 - acc: 0.5169
3840/5677 [===================>..........] - ETA: 2:37 - loss: 0.7042 - acc: 0.5164
3904/5677 [===================>..........] - ETA: 2:32 - loss: 0.7034 - acc: 0.5172
3968/5677 [===================>..........] - ETA: 2:27 - loss: 0.7035 - acc: 0.5179
4032/5677 [====================>.........] - ETA: 2:21 - loss: 0.7032 - acc: 0.5188
4096/5677 [====================>.........] - ETA: 2:16 - loss: 0.7035 - acc: 0.5178
4160/5677 [====================>.........] - ETA: 2:11 - loss: 0.7030 - acc: 0.5188
4224/5677 [=====================>........] - ETA: 2:06 - loss: 0.7029 - acc: 0.5189
4288/5677 [=====================>........] - ETA: 2:01 - loss: 0.7022 - acc: 0.5212
4352/5677 [=====================>........] - ETA: 1:56 - loss: 0.7020 - acc: 0.5216
4416/5677 [======================>.......] - ETA: 1:50 - loss: 0.7024 - acc: 0.5204
4480/5677 [======================>.......] - ETA: 1:45 - loss: 0.7023 - acc: 0.5210
4544/5677 [=======================>......] - ETA: 1:39 - loss: 0.7023 - acc: 0.5207
4608/5677 [=======================>......] - ETA: 1:34 - loss: 0.7021 - acc: 0.5211
4672/5677 [=======================>......] - ETA: 1:28 - loss: 0.7018 - acc: 0.5212
4736/5677 [========================>.....] - ETA: 1:23 - loss: 0.7020 - acc: 0.5203
4800/5677 [========================>.....] - ETA: 1:17 - loss: 0.7022 - acc: 0.5204
4864/5677 [========================>.....] - ETA: 1:11 - loss: 0.7021 - acc: 0.5212
4928/5677 [=========================>....] - ETA: 1:06 - loss: 0.7024 - acc: 0.5201
4992/5677 [=========================>....] - ETA: 1:00 - loss: 0.7020 - acc: 0.5208
5056/5677 [=========================>....] - ETA: 55s - loss: 0.7018 - acc: 0.5210 
5120/5677 [==========================>...] - ETA: 49s - loss: 0.7016 - acc: 0.5213
5184/5677 [==========================>...] - ETA: 43s - loss: 0.7016 - acc: 0.5218
5248/5677 [==========================>...] - ETA: 38s - loss: 0.7014 - acc: 0.5221
5312/5677 [===========================>..] - ETA: 32s - loss: 0.7022 - acc: 0.5205
5376/5677 [===========================>..] - ETA: 26s - loss: 0.7027 - acc: 0.5192
5440/5677 [===========================>..] - ETA: 20s - loss: 0.7033 - acc: 0.5176
5504/5677 [============================>.] - ETA: 15s - loss: 0.7033 - acc: 0.5176
5568/5677 [============================>.] - ETA: 9s - loss: 0.7034 - acc: 0.5176 
5632/5677 [============================>.] - ETA: 3s - loss: 0.7033 - acc: 0.5178
5677/5677 [==============================] - 515s 91ms/step - loss: 0.7029 - acc: 0.5189 - val_loss: 0.6851 - val_acc: 0.5959

Epoch 00002: val_acc improved from 0.52139 to 0.59588, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window06/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 6:35 - loss: 0.6752 - acc: 0.5938
 128/5677 [..............................] - ETA: 6:06 - loss: 0.6755 - acc: 0.5938
 192/5677 [>.............................] - ETA: 5:55 - loss: 0.6890 - acc: 0.5417
 256/5677 [>.............................] - ETA: 5:51 - loss: 0.6974 - acc: 0.5312
 320/5677 [>.............................] - ETA: 6:29 - loss: 0.6923 - acc: 0.5469
 384/5677 [=>............................] - ETA: 6:40 - loss: 0.6913 - acc: 0.5417
 448/5677 [=>............................] - ETA: 6:40 - loss: 0.6951 - acc: 0.5312
 512/5677 [=>............................] - ETA: 6:40 - loss: 0.6893 - acc: 0.5508
 576/5677 [==>...........................] - ETA: 6:34 - loss: 0.6911 - acc: 0.5434
 640/5677 [==>...........................] - ETA: 6:23 - loss: 0.6936 - acc: 0.5391
 704/5677 [==>...........................] - ETA: 6:15 - loss: 0.6939 - acc: 0.5412
 768/5677 [===>..........................] - ETA: 6:06 - loss: 0.6936 - acc: 0.5430
 832/5677 [===>..........................] - ETA: 6:01 - loss: 0.6938 - acc: 0.5397
 896/5677 [===>..........................] - ETA: 5:53 - loss: 0.6942 - acc: 0.5324
 960/5677 [====>.........................] - ETA: 5:47 - loss: 0.6971 - acc: 0.5271
1024/5677 [====>.........................] - ETA: 5:48 - loss: 0.6985 - acc: 0.5244
1088/5677 [====>.........................] - ETA: 5:48 - loss: 0.6975 - acc: 0.5294
1152/5677 [=====>........................] - ETA: 5:44 - loss: 0.6962 - acc: 0.5321
1216/5677 [=====>........................] - ETA: 5:39 - loss: 0.6952 - acc: 0.5337
1280/5677 [=====>........................] - ETA: 5:34 - loss: 0.6957 - acc: 0.5312
1344/5677 [======>.......................] - ETA: 5:29 - loss: 0.6965 - acc: 0.5275
1408/5677 [======>.......................] - ETA: 5:22 - loss: 0.6949 - acc: 0.5284
1472/5677 [======>.......................] - ETA: 5:16 - loss: 0.6934 - acc: 0.5340
1536/5677 [=======>......................] - ETA: 5:16 - loss: 0.6929 - acc: 0.5358
1600/5677 [=======>......................] - ETA: 5:13 - loss: 0.6931 - acc: 0.5356
1664/5677 [=======>......................] - ETA: 5:10 - loss: 0.6935 - acc: 0.5343
1728/5677 [========>.....................] - ETA: 5:05 - loss: 0.6946 - acc: 0.5324
1792/5677 [========>.....................] - ETA: 5:01 - loss: 0.6933 - acc: 0.5340
1856/5677 [========>.....................] - ETA: 4:56 - loss: 0.6938 - acc: 0.5356
1920/5677 [=========>....................] - ETA: 4:51 - loss: 0.6944 - acc: 0.5349
1984/5677 [=========>....................] - ETA: 4:47 - loss: 0.6939 - acc: 0.5388
2048/5677 [=========>....................] - ETA: 4:43 - loss: 0.6937 - acc: 0.5405
2112/5677 [==========>...................] - ETA: 4:38 - loss: 0.6944 - acc: 0.5402
2176/5677 [==========>...................] - ETA: 4:34 - loss: 0.6943 - acc: 0.5395
2240/5677 [==========>...................] - ETA: 4:29 - loss: 0.6936 - acc: 0.5411
2304/5677 [===========>..................] - ETA: 4:24 - loss: 0.6921 - acc: 0.5434
2368/5677 [===========>..................] - ETA: 4:19 - loss: 0.6919 - acc: 0.5439
2432/5677 [===========>..................] - ETA: 4:14 - loss: 0.6921 - acc: 0.5448
2496/5677 [============>.................] - ETA: 4:10 - loss: 0.6918 - acc: 0.5437
2560/5677 [============>.................] - ETA: 4:05 - loss: 0.6914 - acc: 0.5437
2624/5677 [============>.................] - ETA: 4:00 - loss: 0.6911 - acc: 0.5446
2688/5677 [=============>................] - ETA: 3:55 - loss: 0.6914 - acc: 0.5439
2752/5677 [=============>................] - ETA: 3:50 - loss: 0.6915 - acc: 0.5443
2816/5677 [=============>................] - ETA: 3:46 - loss: 0.6911 - acc: 0.5451
2880/5677 [==============>...............] - ETA: 3:40 - loss: 0.6905 - acc: 0.5458
2944/5677 [==============>...............] - ETA: 3:35 - loss: 0.6914 - acc: 0.5448
3008/5677 [==============>...............] - ETA: 3:30 - loss: 0.6918 - acc: 0.5445
3072/5677 [===============>..............] - ETA: 3:25 - loss: 0.6917 - acc: 0.5430
3136/5677 [===============>..............] - ETA: 3:20 - loss: 0.6921 - acc: 0.5424
3200/5677 [===============>..............] - ETA: 3:15 - loss: 0.6921 - acc: 0.5422
3264/5677 [================>.............] - ETA: 3:10 - loss: 0.6924 - acc: 0.5423
3328/5677 [================>.............] - ETA: 3:05 - loss: 0.6930 - acc: 0.5415
3392/5677 [================>.............] - ETA: 2:59 - loss: 0.6927 - acc: 0.5425
3456/5677 [=================>............] - ETA: 2:54 - loss: 0.6928 - acc: 0.5422
3520/5677 [=================>............] - ETA: 2:49 - loss: 0.6925 - acc: 0.5420
3584/5677 [=================>............] - ETA: 2:44 - loss: 0.6925 - acc: 0.5419
3648/5677 [==================>...........] - ETA: 2:39 - loss: 0.6922 - acc: 0.5428
3712/5677 [==================>...........] - ETA: 2:34 - loss: 0.6921 - acc: 0.5434
3776/5677 [==================>...........] - ETA: 2:29 - loss: 0.6929 - acc: 0.5418
3840/5677 [===================>..........] - ETA: 2:24 - loss: 0.6926 - acc: 0.5411
3904/5677 [===================>..........] - ETA: 2:19 - loss: 0.6932 - acc: 0.5410
3968/5677 [===================>..........] - ETA: 2:13 - loss: 0.6935 - acc: 0.5398
4032/5677 [====================>.........] - ETA: 2:08 - loss: 0.6935 - acc: 0.5392
4096/5677 [====================>.........] - ETA: 2:03 - loss: 0.6940 - acc: 0.5378
4160/5677 [====================>.........] - ETA: 1:58 - loss: 0.6934 - acc: 0.5406
4224/5677 [=====================>........] - ETA: 1:53 - loss: 0.6942 - acc: 0.5386
4288/5677 [=====================>........] - ETA: 1:48 - loss: 0.6948 - acc: 0.5378
4352/5677 [=====================>........] - ETA: 1:43 - loss: 0.6945 - acc: 0.5384
4416/5677 [======================>.......] - ETA: 1:38 - loss: 0.6943 - acc: 0.5389
4480/5677 [======================>.......] - ETA: 1:33 - loss: 0.6937 - acc: 0.5400
4544/5677 [=======================>......] - ETA: 1:28 - loss: 0.6933 - acc: 0.5409
4608/5677 [=======================>......] - ETA: 1:23 - loss: 0.6932 - acc: 0.5417
4672/5677 [=======================>......] - ETA: 1:18 - loss: 0.6930 - acc: 0.5415
4736/5677 [========================>.....] - ETA: 1:13 - loss: 0.6930 - acc: 0.5418
4800/5677 [========================>.....] - ETA: 1:08 - loss: 0.6927 - acc: 0.5425
4864/5677 [========================>.....] - ETA: 1:03 - loss: 0.6929 - acc: 0.5428
4928/5677 [=========================>....] - ETA: 58s - loss: 0.6933 - acc: 0.5418 
4992/5677 [=========================>....] - ETA: 53s - loss: 0.6931 - acc: 0.5427
5056/5677 [=========================>....] - ETA: 48s - loss: 0.6929 - acc: 0.5431
5120/5677 [==========================>...] - ETA: 43s - loss: 0.6931 - acc: 0.5432
5184/5677 [==========================>...] - ETA: 38s - loss: 0.6930 - acc: 0.5428
5248/5677 [==========================>...] - ETA: 33s - loss: 0.6929 - acc: 0.5433
5312/5677 [===========================>..] - ETA: 28s - loss: 0.6930 - acc: 0.5422
5376/5677 [===========================>..] - ETA: 23s - loss: 0.6929 - acc: 0.5419
5440/5677 [===========================>..] - ETA: 18s - loss: 0.6926 - acc: 0.5425
5504/5677 [============================>.] - ETA: 13s - loss: 0.6928 - acc: 0.5420
5568/5677 [============================>.] - ETA: 8s - loss: 0.6933 - acc: 0.5413 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6933 - acc: 0.5407
5677/5677 [==============================] - 462s 81ms/step - loss: 0.6934 - acc: 0.5401 - val_loss: 0.6819 - val_acc: 0.5689

Epoch 00003: val_acc did not improve from 0.59588
Epoch 4/10

  64/5677 [..............................] - ETA: 7:15 - loss: 0.6451 - acc: 0.6719
 128/5677 [..............................] - ETA: 7:11 - loss: 0.6591 - acc: 0.6328
 192/5677 [>.............................] - ETA: 6:59 - loss: 0.6681 - acc: 0.6094
 256/5677 [>.............................] - ETA: 6:48 - loss: 0.6699 - acc: 0.5859
 320/5677 [>.............................] - ETA: 6:37 - loss: 0.6796 - acc: 0.5625
 384/5677 [=>............................] - ETA: 6:33 - loss: 0.6865 - acc: 0.5573
 448/5677 [=>............................] - ETA: 6:34 - loss: 0.6810 - acc: 0.5692
 512/5677 [=>............................] - ETA: 6:30 - loss: 0.6799 - acc: 0.5703
 576/5677 [==>...........................] - ETA: 6:29 - loss: 0.6800 - acc: 0.5625
 640/5677 [==>...........................] - ETA: 6:23 - loss: 0.6789 - acc: 0.5641
 704/5677 [==>...........................] - ETA: 6:15 - loss: 0.6787 - acc: 0.5668
 768/5677 [===>..........................] - ETA: 6:11 - loss: 0.6781 - acc: 0.5638
 832/5677 [===>..........................] - ETA: 6:04 - loss: 0.6772 - acc: 0.5613
 896/5677 [===>..........................] - ETA: 5:57 - loss: 0.6765 - acc: 0.5636
 960/5677 [====>.........................] - ETA: 5:48 - loss: 0.6769 - acc: 0.5667
1024/5677 [====>.........................] - ETA: 5:40 - loss: 0.6770 - acc: 0.5684
1088/5677 [====>.........................] - ETA: 5:33 - loss: 0.6764 - acc: 0.5717
1152/5677 [=====>........................] - ETA: 5:26 - loss: 0.6771 - acc: 0.5720
1216/5677 [=====>........................] - ETA: 5:22 - loss: 0.6781 - acc: 0.5724
1280/5677 [=====>........................] - ETA: 5:16 - loss: 0.6803 - acc: 0.5703
1344/5677 [======>.......................] - ETA: 5:10 - loss: 0.6805 - acc: 0.5707
1408/5677 [======>.......................] - ETA: 5:05 - loss: 0.6794 - acc: 0.5732
1472/5677 [======>.......................] - ETA: 5:00 - loss: 0.6782 - acc: 0.5761
1536/5677 [=======>......................] - ETA: 4:54 - loss: 0.6800 - acc: 0.5710
1600/5677 [=======>......................] - ETA: 4:50 - loss: 0.6805 - acc: 0.5700
1664/5677 [=======>......................] - ETA: 4:45 - loss: 0.6815 - acc: 0.5667
1728/5677 [========>.....................] - ETA: 4:40 - loss: 0.6819 - acc: 0.5648
1792/5677 [========>.....................] - ETA: 4:34 - loss: 0.6816 - acc: 0.5664
1856/5677 [========>.....................] - ETA: 4:29 - loss: 0.6814 - acc: 0.5679
1920/5677 [=========>....................] - ETA: 4:24 - loss: 0.6822 - acc: 0.5635
1984/5677 [=========>....................] - ETA: 4:19 - loss: 0.6813 - acc: 0.5645
2048/5677 [=========>....................] - ETA: 4:15 - loss: 0.6823 - acc: 0.5630
2112/5677 [==========>...................] - ETA: 4:10 - loss: 0.6818 - acc: 0.5634
2176/5677 [==========>...................] - ETA: 4:05 - loss: 0.6826 - acc: 0.5611
2240/5677 [==========>...................] - ETA: 4:00 - loss: 0.6830 - acc: 0.5594
2304/5677 [===========>..................] - ETA: 3:56 - loss: 0.6829 - acc: 0.5612
2368/5677 [===========>..................] - ETA: 3:51 - loss: 0.6835 - acc: 0.5617
2432/5677 [===========>..................] - ETA: 3:47 - loss: 0.6849 - acc: 0.5592
2496/5677 [============>.................] - ETA: 3:42 - loss: 0.6847 - acc: 0.5597
2560/5677 [============>.................] - ETA: 3:37 - loss: 0.6840 - acc: 0.5609
2624/5677 [============>.................] - ETA: 3:33 - loss: 0.6840 - acc: 0.5621
2688/5677 [=============>................] - ETA: 3:28 - loss: 0.6843 - acc: 0.5603
2752/5677 [=============>................] - ETA: 3:24 - loss: 0.6839 - acc: 0.5614
2816/5677 [=============>................] - ETA: 3:19 - loss: 0.6846 - acc: 0.5593
2880/5677 [==============>...............] - ETA: 3:15 - loss: 0.6852 - acc: 0.5583
2944/5677 [==============>...............] - ETA: 3:10 - loss: 0.6852 - acc: 0.5588
3008/5677 [==============>...............] - ETA: 3:05 - loss: 0.6858 - acc: 0.5585
3072/5677 [===============>..............] - ETA: 3:01 - loss: 0.6855 - acc: 0.5589
3136/5677 [===============>..............] - ETA: 2:56 - loss: 0.6851 - acc: 0.5599
3200/5677 [===============>..............] - ETA: 2:51 - loss: 0.6856 - acc: 0.5584
3264/5677 [================>.............] - ETA: 2:47 - loss: 0.6856 - acc: 0.5588
3328/5677 [================>.............] - ETA: 2:42 - loss: 0.6853 - acc: 0.5604
3392/5677 [================>.............] - ETA: 2:38 - loss: 0.6856 - acc: 0.5590
3456/5677 [=================>............] - ETA: 2:33 - loss: 0.6849 - acc: 0.5590
3520/5677 [=================>............] - ETA: 2:29 - loss: 0.6858 - acc: 0.5588
3584/5677 [=================>............] - ETA: 2:24 - loss: 0.6860 - acc: 0.5575
3648/5677 [==================>...........] - ETA: 2:20 - loss: 0.6861 - acc: 0.5581
3712/5677 [==================>...........] - ETA: 2:15 - loss: 0.6865 - acc: 0.5574
3776/5677 [==================>...........] - ETA: 2:11 - loss: 0.6863 - acc: 0.5572
3840/5677 [===================>..........] - ETA: 2:06 - loss: 0.6857 - acc: 0.5586
3904/5677 [===================>..........] - ETA: 2:02 - loss: 0.6857 - acc: 0.5587
3968/5677 [===================>..........] - ETA: 1:57 - loss: 0.6862 - acc: 0.5575
4032/5677 [====================>.........] - ETA: 1:53 - loss: 0.6865 - acc: 0.5561
4096/5677 [====================>.........] - ETA: 1:48 - loss: 0.6868 - acc: 0.5552
4160/5677 [====================>.........] - ETA: 1:44 - loss: 0.6865 - acc: 0.5563
4224/5677 [=====================>........] - ETA: 1:39 - loss: 0.6865 - acc: 0.5559
4288/5677 [=====================>........] - ETA: 1:35 - loss: 0.6864 - acc: 0.5548
4352/5677 [=====================>........] - ETA: 1:30 - loss: 0.6866 - acc: 0.5547
4416/5677 [======================>.......] - ETA: 1:26 - loss: 0.6867 - acc: 0.5539
4480/5677 [======================>.......] - ETA: 1:22 - loss: 0.6864 - acc: 0.5547
4544/5677 [=======================>......] - ETA: 1:17 - loss: 0.6868 - acc: 0.5528
4608/5677 [=======================>......] - ETA: 1:13 - loss: 0.6870 - acc: 0.5530
4672/5677 [=======================>......] - ETA: 1:08 - loss: 0.6870 - acc: 0.5533
4736/5677 [========================>.....] - ETA: 1:04 - loss: 0.6872 - acc: 0.5519
4800/5677 [========================>.....] - ETA: 59s - loss: 0.6869 - acc: 0.5527 
4864/5677 [========================>.....] - ETA: 55s - loss: 0.6869 - acc: 0.5528
4928/5677 [=========================>....] - ETA: 51s - loss: 0.6869 - acc: 0.5536
4992/5677 [=========================>....] - ETA: 46s - loss: 0.6875 - acc: 0.5519
5056/5677 [=========================>....] - ETA: 42s - loss: 0.6878 - acc: 0.5512
5120/5677 [==========================>...] - ETA: 38s - loss: 0.6875 - acc: 0.5512
5184/5677 [==========================>...] - ETA: 33s - loss: 0.6874 - acc: 0.5511
5248/5677 [==========================>...] - ETA: 29s - loss: 0.6880 - acc: 0.5495
5312/5677 [===========================>..] - ETA: 24s - loss: 0.6887 - acc: 0.5480
5376/5677 [===========================>..] - ETA: 20s - loss: 0.6888 - acc: 0.5472
5440/5677 [===========================>..] - ETA: 16s - loss: 0.6888 - acc: 0.5476
5504/5677 [============================>.] - ETA: 11s - loss: 0.6889 - acc: 0.5465
5568/5677 [============================>.] - ETA: 7s - loss: 0.6889 - acc: 0.5467 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6890 - acc: 0.5455
5677/5677 [==============================] - 406s 72ms/step - loss: 0.6887 - acc: 0.5461 - val_loss: 0.6878 - val_acc: 0.5531

Epoch 00004: val_acc did not improve from 0.59588
Epoch 5/10

  64/5677 [..............................] - ETA: 7:30 - loss: 0.6853 - acc: 0.5469
 128/5677 [..............................] - ETA: 7:30 - loss: 0.6823 - acc: 0.5469
 192/5677 [>.............................] - ETA: 7:26 - loss: 0.6880 - acc: 0.5365
 256/5677 [>.............................] - ETA: 7:26 - loss: 0.6907 - acc: 0.5156
 320/5677 [>.............................] - ETA: 7:22 - loss: 0.6837 - acc: 0.5469
 384/5677 [=>............................] - ETA: 7:18 - loss: 0.6879 - acc: 0.5286
 448/5677 [=>............................] - ETA: 7:11 - loss: 0.6856 - acc: 0.5357
 512/5677 [=>............................] - ETA: 7:06 - loss: 0.6891 - acc: 0.5312
 576/5677 [==>...........................] - ETA: 7:12 - loss: 0.6871 - acc: 0.5434
 640/5677 [==>...........................] - ETA: 7:14 - loss: 0.6879 - acc: 0.5391
 704/5677 [==>...........................] - ETA: 7:09 - loss: 0.6854 - acc: 0.5526
 768/5677 [===>..........................] - ETA: 7:07 - loss: 0.6833 - acc: 0.5547
 832/5677 [===>..........................] - ETA: 7:05 - loss: 0.6863 - acc: 0.5505
 896/5677 [===>..........................] - ETA: 7:00 - loss: 0.6874 - acc: 0.5435
 960/5677 [====>.........................] - ETA: 7:03 - loss: 0.6892 - acc: 0.5427
1024/5677 [====>.........................] - ETA: 6:55 - loss: 0.6896 - acc: 0.5420
1088/5677 [====>.........................] - ETA: 6:54 - loss: 0.6891 - acc: 0.5432
1152/5677 [=====>........................] - ETA: 6:47 - loss: 0.6892 - acc: 0.5443
1216/5677 [=====>........................] - ETA: 6:40 - loss: 0.6884 - acc: 0.5469
1280/5677 [=====>........................] - ETA: 6:39 - loss: 0.6880 - acc: 0.5477
1344/5677 [======>.......................] - ETA: 6:32 - loss: 0.6869 - acc: 0.5484
1408/5677 [======>.......................] - ETA: 6:25 - loss: 0.6872 - acc: 0.5469
1472/5677 [======>.......................] - ETA: 6:21 - loss: 0.6873 - acc: 0.5469
1536/5677 [=======>......................] - ETA: 6:17 - loss: 0.6871 - acc: 0.5475
1600/5677 [=======>......................] - ETA: 6:09 - loss: 0.6862 - acc: 0.5481
1664/5677 [=======>......................] - ETA: 6:02 - loss: 0.6867 - acc: 0.5481
1728/5677 [========>.....................] - ETA: 5:56 - loss: 0.6869 - acc: 0.5469
1792/5677 [========>.....................] - ETA: 5:54 - loss: 0.6867 - acc: 0.5463
1856/5677 [========>.....................] - ETA: 5:49 - loss: 0.6878 - acc: 0.5436
1920/5677 [=========>....................] - ETA: 5:42 - loss: 0.6893 - acc: 0.5401
1984/5677 [=========>....................] - ETA: 5:34 - loss: 0.6891 - acc: 0.5403
2048/5677 [=========>....................] - ETA: 5:28 - loss: 0.6894 - acc: 0.5400
2112/5677 [==========>...................] - ETA: 5:24 - loss: 0.6892 - acc: 0.5407
2176/5677 [==========>...................] - ETA: 5:19 - loss: 0.6906 - acc: 0.5400
2240/5677 [==========>...................] - ETA: 5:14 - loss: 0.6902 - acc: 0.5402
2304/5677 [===========>..................] - ETA: 5:07 - loss: 0.6916 - acc: 0.5391
2368/5677 [===========>..................] - ETA: 5:00 - loss: 0.6910 - acc: 0.5401
2432/5677 [===========>..................] - ETA: 4:54 - loss: 0.6915 - acc: 0.5411
2496/5677 [============>.................] - ETA: 4:47 - loss: 0.6916 - acc: 0.5421
2560/5677 [============>.................] - ETA: 4:43 - loss: 0.6917 - acc: 0.5414
2624/5677 [============>.................] - ETA: 4:39 - loss: 0.6917 - acc: 0.5400
2688/5677 [=============>................] - ETA: 4:33 - loss: 0.6914 - acc: 0.5394
2752/5677 [=============>................] - ETA: 4:27 - loss: 0.6916 - acc: 0.5403
2816/5677 [=============>................] - ETA: 4:20 - loss: 0.6918 - acc: 0.5408
2880/5677 [==============>...............] - ETA: 4:14 - loss: 0.6907 - acc: 0.5444
2944/5677 [==============>...............] - ETA: 4:07 - loss: 0.6907 - acc: 0.5431
3008/5677 [==============>...............] - ETA: 4:02 - loss: 0.6903 - acc: 0.5436
3072/5677 [===============>..............] - ETA: 3:57 - loss: 0.6912 - acc: 0.5407
3136/5677 [===============>..............] - ETA: 3:51 - loss: 0.6910 - acc: 0.5408
3200/5677 [===============>..............] - ETA: 3:46 - loss: 0.6909 - acc: 0.5409
3264/5677 [================>.............] - ETA: 3:40 - loss: 0.6914 - acc: 0.5404
3328/5677 [================>.............] - ETA: 3:34 - loss: 0.6918 - acc: 0.5382
3392/5677 [================>.............] - ETA: 3:28 - loss: 0.6920 - acc: 0.5369
3456/5677 [=================>............] - ETA: 3:22 - loss: 0.6915 - acc: 0.5382
3520/5677 [=================>............] - ETA: 3:16 - loss: 0.6919 - acc: 0.5372
3584/5677 [=================>............] - ETA: 3:10 - loss: 0.6920 - acc: 0.5368
3648/5677 [==================>...........] - ETA: 3:05 - loss: 0.6913 - acc: 0.5397
3712/5677 [==================>...........] - ETA: 3:00 - loss: 0.6917 - acc: 0.5383
3776/5677 [==================>...........] - ETA: 2:54 - loss: 0.6916 - acc: 0.5389
3840/5677 [===================>..........] - ETA: 2:48 - loss: 0.6917 - acc: 0.5380
3904/5677 [===================>..........] - ETA: 2:42 - loss: 0.6920 - acc: 0.5374
3968/5677 [===================>..........] - ETA: 2:36 - loss: 0.6922 - acc: 0.5373
4032/5677 [====================>.........] - ETA: 2:30 - loss: 0.6922 - acc: 0.5384
4096/5677 [====================>.........] - ETA: 2:23 - loss: 0.6926 - acc: 0.5371
4160/5677 [====================>.........] - ETA: 2:17 - loss: 0.6926 - acc: 0.5370
4224/5677 [=====================>........] - ETA: 2:12 - loss: 0.6926 - acc: 0.5372
4288/5677 [=====================>........] - ETA: 2:06 - loss: 0.6928 - acc: 0.5364
4352/5677 [=====================>........] - ETA: 2:01 - loss: 0.6928 - acc: 0.5361
4416/5677 [======================>.......] - ETA: 1:55 - loss: 0.6927 - acc: 0.5367
4480/5677 [======================>.......] - ETA: 1:49 - loss: 0.6930 - acc: 0.5359
4544/5677 [=======================>......] - ETA: 1:43 - loss: 0.6924 - acc: 0.5376
4608/5677 [=======================>......] - ETA: 1:37 - loss: 0.6923 - acc: 0.5375
4672/5677 [=======================>......] - ETA: 1:31 - loss: 0.6921 - acc: 0.5390
4736/5677 [========================>.....] - ETA: 1:25 - loss: 0.6916 - acc: 0.5405
4800/5677 [========================>.....] - ETA: 1:19 - loss: 0.6913 - acc: 0.5408
4864/5677 [========================>.....] - ETA: 1:14 - loss: 0.6914 - acc: 0.5403
4928/5677 [=========================>....] - ETA: 1:08 - loss: 0.6915 - acc: 0.5396
4992/5677 [=========================>....] - ETA: 1:02 - loss: 0.6912 - acc: 0.5403
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6912 - acc: 0.5400 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6912 - acc: 0.5402
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6912 - acc: 0.5403
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6904 - acc: 0.5417
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6901 - acc: 0.5422
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6905 - acc: 0.5407
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6905 - acc: 0.5414
5504/5677 [============================>.] - ETA: 16s - loss: 0.6906 - acc: 0.5418
5568/5677 [============================>.] - ETA: 10s - loss: 0.6903 - acc: 0.5420
5632/5677 [============================>.] - ETA: 4s - loss: 0.6903 - acc: 0.5421 
5677/5677 [==============================] - 550s 97ms/step - loss: 0.6903 - acc: 0.5417 - val_loss: 0.6855 - val_acc: 0.5594

Epoch 00005: val_acc did not improve from 0.59588
Epoch 6/10

  64/5677 [..............................] - ETA: 9:50 - loss: 0.6892 - acc: 0.5156
 128/5677 [..............................] - ETA: 9:27 - loss: 0.6866 - acc: 0.5547
 192/5677 [>.............................] - ETA: 9:35 - loss: 0.6902 - acc: 0.5573
 256/5677 [>.............................] - ETA: 9:20 - loss: 0.6920 - acc: 0.5508
 320/5677 [>.............................] - ETA: 9:08 - loss: 0.6910 - acc: 0.5437
 384/5677 [=>............................] - ETA: 9:04 - loss: 0.6881 - acc: 0.5443
 448/5677 [=>............................] - ETA: 8:53 - loss: 0.6883 - acc: 0.5446
 512/5677 [=>............................] - ETA: 8:40 - loss: 0.6885 - acc: 0.5449
 576/5677 [==>...........................] - ETA: 8:23 - loss: 0.6886 - acc: 0.5417
 640/5677 [==>...........................] - ETA: 8:10 - loss: 0.6856 - acc: 0.5531
 704/5677 [==>...........................] - ETA: 7:57 - loss: 0.6897 - acc: 0.5455
 768/5677 [===>..........................] - ETA: 7:48 - loss: 0.6889 - acc: 0.5469
 832/5677 [===>..........................] - ETA: 7:35 - loss: 0.6884 - acc: 0.5493
 896/5677 [===>..........................] - ETA: 7:27 - loss: 0.6858 - acc: 0.5558
 960/5677 [====>.........................] - ETA: 7:22 - loss: 0.6840 - acc: 0.5604
1024/5677 [====>.........................] - ETA: 7:20 - loss: 0.6851 - acc: 0.5566
1088/5677 [====>.........................] - ETA: 7:13 - loss: 0.6853 - acc: 0.5579
1152/5677 [=====>........................] - ETA: 7:06 - loss: 0.6850 - acc: 0.5590
1216/5677 [=====>........................] - ETA: 6:56 - loss: 0.6865 - acc: 0.5535
1280/5677 [=====>........................] - ETA: 6:48 - loss: 0.6865 - acc: 0.5547
1344/5677 [======>.......................] - ETA: 6:40 - loss: 0.6855 - acc: 0.5595
1408/5677 [======>.......................] - ETA: 6:33 - loss: 0.6835 - acc: 0.5646
1472/5677 [======>.......................] - ETA: 6:31 - loss: 0.6818 - acc: 0.5673
1536/5677 [=======>......................] - ETA: 6:27 - loss: 0.6813 - acc: 0.5684
1600/5677 [=======>......................] - ETA: 6:22 - loss: 0.6827 - acc: 0.5656
1664/5677 [=======>......................] - ETA: 6:17 - loss: 0.6818 - acc: 0.5673
1728/5677 [========>.....................] - ETA: 6:10 - loss: 0.6818 - acc: 0.5683
1792/5677 [========>.....................] - ETA: 6:02 - loss: 0.6818 - acc: 0.5692
1856/5677 [========>.....................] - ETA: 5:55 - loss: 0.6809 - acc: 0.5727
1920/5677 [=========>....................] - ETA: 5:47 - loss: 0.6800 - acc: 0.5740
1984/5677 [=========>....................] - ETA: 5:40 - loss: 0.6796 - acc: 0.5746
2048/5677 [=========>....................] - ETA: 5:34 - loss: 0.6788 - acc: 0.5747
2112/5677 [==========>...................] - ETA: 5:30 - loss: 0.6784 - acc: 0.5739
2176/5677 [==========>...................] - ETA: 5:25 - loss: 0.6777 - acc: 0.5744
2240/5677 [==========>...................] - ETA: 5:19 - loss: 0.6772 - acc: 0.5754
2304/5677 [===========>..................] - ETA: 5:13 - loss: 0.6778 - acc: 0.5734
2368/5677 [===========>..................] - ETA: 5:07 - loss: 0.6774 - acc: 0.5752
2432/5677 [===========>..................] - ETA: 5:00 - loss: 0.6768 - acc: 0.5761
2496/5677 [============>.................] - ETA: 4:54 - loss: 0.6766 - acc: 0.5765
2560/5677 [============>.................] - ETA: 4:48 - loss: 0.6767 - acc: 0.5773
2624/5677 [============>.................] - ETA: 4:43 - loss: 0.6776 - acc: 0.5762
2688/5677 [=============>................] - ETA: 4:38 - loss: 0.6778 - acc: 0.5751
2752/5677 [=============>................] - ETA: 4:33 - loss: 0.6778 - acc: 0.5759
2816/5677 [=============>................] - ETA: 4:27 - loss: 0.6781 - acc: 0.5749
2880/5677 [==============>...............] - ETA: 4:22 - loss: 0.6783 - acc: 0.5743
2944/5677 [==============>...............] - ETA: 4:16 - loss: 0.6782 - acc: 0.5744
3008/5677 [==============>...............] - ETA: 4:11 - loss: 0.6788 - acc: 0.5721
3072/5677 [===============>..............] - ETA: 4:05 - loss: 0.6784 - acc: 0.5736
3136/5677 [===============>..............] - ETA: 3:59 - loss: 0.6783 - acc: 0.5743
3200/5677 [===============>..............] - ETA: 3:53 - loss: 0.6784 - acc: 0.5747
3264/5677 [================>.............] - ETA: 3:47 - loss: 0.6784 - acc: 0.5741
3328/5677 [================>.............] - ETA: 3:42 - loss: 0.6789 - acc: 0.5742
3392/5677 [================>.............] - ETA: 3:36 - loss: 0.6787 - acc: 0.5740
3456/5677 [=================>............] - ETA: 3:31 - loss: 0.6789 - acc: 0.5732
3520/5677 [=================>............] - ETA: 3:25 - loss: 0.6794 - acc: 0.5710
3584/5677 [=================>............] - ETA: 3:19 - loss: 0.6791 - acc: 0.5709
3648/5677 [==================>...........] - ETA: 3:13 - loss: 0.6797 - acc: 0.5696
3712/5677 [==================>...........] - ETA: 3:07 - loss: 0.6794 - acc: 0.5709
3776/5677 [==================>...........] - ETA: 3:01 - loss: 0.6797 - acc: 0.5697
3840/5677 [===================>..........] - ETA: 2:55 - loss: 0.6795 - acc: 0.5701
3904/5677 [===================>..........] - ETA: 2:49 - loss: 0.6797 - acc: 0.5697
3968/5677 [===================>..........] - ETA: 2:43 - loss: 0.6793 - acc: 0.5696
4032/5677 [====================>.........] - ETA: 2:37 - loss: 0.6807 - acc: 0.5680
4096/5677 [====================>.........] - ETA: 2:31 - loss: 0.6809 - acc: 0.5671
4160/5677 [====================>.........] - ETA: 2:25 - loss: 0.6809 - acc: 0.5666
4224/5677 [=====================>........] - ETA: 2:19 - loss: 0.6812 - acc: 0.5661
4288/5677 [=====================>........] - ETA: 2:12 - loss: 0.6811 - acc: 0.5669
4352/5677 [=====================>........] - ETA: 2:06 - loss: 0.6808 - acc: 0.5671
4416/5677 [======================>.......] - ETA: 2:00 - loss: 0.6813 - acc: 0.5666
4480/5677 [======================>.......] - ETA: 1:54 - loss: 0.6812 - acc: 0.5663
4544/5677 [=======================>......] - ETA: 1:48 - loss: 0.6817 - acc: 0.5651
4608/5677 [=======================>......] - ETA: 1:42 - loss: 0.6817 - acc: 0.5651
4672/5677 [=======================>......] - ETA: 1:36 - loss: 0.6821 - acc: 0.5644
4736/5677 [========================>.....] - ETA: 1:30 - loss: 0.6821 - acc: 0.5648
4800/5677 [========================>.....] - ETA: 1:24 - loss: 0.6819 - acc: 0.5656
4864/5677 [========================>.....] - ETA: 1:17 - loss: 0.6820 - acc: 0.5654
4928/5677 [=========================>....] - ETA: 1:11 - loss: 0.6821 - acc: 0.5653
4992/5677 [=========================>....] - ETA: 1:05 - loss: 0.6816 - acc: 0.5667
5056/5677 [=========================>....] - ETA: 59s - loss: 0.6816 - acc: 0.5669 
5120/5677 [==========================>...] - ETA: 53s - loss: 0.6817 - acc: 0.5664
5184/5677 [==========================>...] - ETA: 47s - loss: 0.6815 - acc: 0.5662
5248/5677 [==========================>...] - ETA: 41s - loss: 0.6812 - acc: 0.5669
5312/5677 [===========================>..] - ETA: 35s - loss: 0.6810 - acc: 0.5674
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6809 - acc: 0.5679
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6809 - acc: 0.5680
5504/5677 [============================>.] - ETA: 16s - loss: 0.6812 - acc: 0.5670
5568/5677 [============================>.] - ETA: 10s - loss: 0.6809 - acc: 0.5673
5632/5677 [============================>.] - ETA: 4s - loss: 0.6808 - acc: 0.5684 
5677/5677 [==============================] - 572s 101ms/step - loss: 0.6806 - acc: 0.5686 - val_loss: 0.6979 - val_acc: 0.5404

Epoch 00006: val_acc did not improve from 0.59588
Epoch 7/10

  64/5677 [..............................] - ETA: 9:26 - loss: 0.7030 - acc: 0.4844
 128/5677 [..............................] - ETA: 9:58 - loss: 0.7010 - acc: 0.5234
 192/5677 [>.............................] - ETA: 9:33 - loss: 0.7083 - acc: 0.5208
 256/5677 [>.............................] - ETA: 9:27 - loss: 0.7038 - acc: 0.5430
 320/5677 [>.............................] - ETA: 9:15 - loss: 0.6972 - acc: 0.5469
 384/5677 [=>............................] - ETA: 9:06 - loss: 0.6913 - acc: 0.5547
 448/5677 [=>............................] - ETA: 8:58 - loss: 0.6956 - acc: 0.5558
 512/5677 [=>............................] - ETA: 8:49 - loss: 0.6956 - acc: 0.5605
 576/5677 [==>...........................] - ETA: 8:41 - loss: 0.6958 - acc: 0.5573
 640/5677 [==>...........................] - ETA: 8:35 - loss: 0.6934 - acc: 0.5703
 704/5677 [==>...........................] - ETA: 8:28 - loss: 0.6928 - acc: 0.5710
 768/5677 [===>..........................] - ETA: 8:21 - loss: 0.6925 - acc: 0.5729
 832/5677 [===>..........................] - ETA: 8:14 - loss: 0.6949 - acc: 0.5649
 896/5677 [===>..........................] - ETA: 8:08 - loss: 0.6909 - acc: 0.5737
 960/5677 [====>.........................] - ETA: 8:02 - loss: 0.6918 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 8:02 - loss: 0.6920 - acc: 0.5674
1088/5677 [====>.........................] - ETA: 8:00 - loss: 0.6915 - acc: 0.5643
1152/5677 [=====>........................] - ETA: 7:49 - loss: 0.6901 - acc: 0.5668
1216/5677 [=====>........................] - ETA: 7:37 - loss: 0.6894 - acc: 0.5691
1280/5677 [=====>........................] - ETA: 7:27 - loss: 0.6892 - acc: 0.5672
1344/5677 [======>.......................] - ETA: 7:15 - loss: 0.6894 - acc: 0.5647
1408/5677 [======>.......................] - ETA: 7:05 - loss: 0.6885 - acc: 0.5675
1472/5677 [======>.......................] - ETA: 6:55 - loss: 0.6879 - acc: 0.5666
1536/5677 [=======>......................] - ETA: 6:46 - loss: 0.6864 - acc: 0.5723
1600/5677 [=======>......................] - ETA: 6:36 - loss: 0.6862 - acc: 0.5737
1664/5677 [=======>......................] - ETA: 6:29 - loss: 0.6860 - acc: 0.5739
1728/5677 [========>.....................] - ETA: 6:20 - loss: 0.6863 - acc: 0.5729
1792/5677 [========>.....................] - ETA: 6:13 - loss: 0.6860 - acc: 0.5737
1856/5677 [========>.....................] - ETA: 6:05 - loss: 0.6871 - acc: 0.5717
1920/5677 [=========>....................] - ETA: 5:57 - loss: 0.6874 - acc: 0.5724
1984/5677 [=========>....................] - ETA: 5:50 - loss: 0.6865 - acc: 0.5716
2048/5677 [=========>....................] - ETA: 5:43 - loss: 0.6858 - acc: 0.5718
2112/5677 [==========>...................] - ETA: 5:35 - loss: 0.6859 - acc: 0.5705
2176/5677 [==========>...................] - ETA: 5:28 - loss: 0.6859 - acc: 0.5694
2240/5677 [==========>...................] - ETA: 5:21 - loss: 0.6864 - acc: 0.5683
2304/5677 [===========>..................] - ETA: 5:14 - loss: 0.6872 - acc: 0.5668
2368/5677 [===========>..................] - ETA: 5:07 - loss: 0.6867 - acc: 0.5680
2432/5677 [===========>..................] - ETA: 5:00 - loss: 0.6868 - acc: 0.5687
2496/5677 [============>.................] - ETA: 4:53 - loss: 0.6869 - acc: 0.5677
2560/5677 [============>.................] - ETA: 4:46 - loss: 0.6860 - acc: 0.5687
2624/5677 [============>.................] - ETA: 4:39 - loss: 0.6854 - acc: 0.5694
2688/5677 [=============>................] - ETA: 4:32 - loss: 0.6852 - acc: 0.5681
2752/5677 [=============>................] - ETA: 4:26 - loss: 0.6851 - acc: 0.5676
2816/5677 [=============>................] - ETA: 4:19 - loss: 0.6854 - acc: 0.5668
2880/5677 [==============>...............] - ETA: 4:13 - loss: 0.6846 - acc: 0.5677
2944/5677 [==============>...............] - ETA: 4:07 - loss: 0.6852 - acc: 0.5662
3008/5677 [==============>...............] - ETA: 4:01 - loss: 0.6850 - acc: 0.5662
3072/5677 [===============>..............] - ETA: 3:55 - loss: 0.6853 - acc: 0.5658
3136/5677 [===============>..............] - ETA: 3:48 - loss: 0.6849 - acc: 0.5666
3200/5677 [===============>..............] - ETA: 3:42 - loss: 0.6849 - acc: 0.5656
3264/5677 [================>.............] - ETA: 3:36 - loss: 0.6857 - acc: 0.5637
3328/5677 [================>.............] - ETA: 3:30 - loss: 0.6860 - acc: 0.5622
3392/5677 [================>.............] - ETA: 3:24 - loss: 0.6867 - acc: 0.5607
3456/5677 [=================>............] - ETA: 3:18 - loss: 0.6865 - acc: 0.5616
3520/5677 [=================>............] - ETA: 3:12 - loss: 0.6867 - acc: 0.5602
3584/5677 [=================>............] - ETA: 3:06 - loss: 0.6863 - acc: 0.5617
3648/5677 [==================>...........] - ETA: 3:00 - loss: 0.6860 - acc: 0.5622
3712/5677 [==================>...........] - ETA: 2:54 - loss: 0.6854 - acc: 0.5633
3776/5677 [==================>...........] - ETA: 2:49 - loss: 0.6852 - acc: 0.5641
3840/5677 [===================>..........] - ETA: 2:43 - loss: 0.6849 - acc: 0.5646
3904/5677 [===================>..........] - ETA: 2:37 - loss: 0.6846 - acc: 0.5658
3968/5677 [===================>..........] - ETA: 2:31 - loss: 0.6848 - acc: 0.5655
4032/5677 [====================>.........] - ETA: 2:25 - loss: 0.6849 - acc: 0.5655
4096/5677 [====================>.........] - ETA: 2:19 - loss: 0.6842 - acc: 0.5669
4160/5677 [====================>.........] - ETA: 2:13 - loss: 0.6843 - acc: 0.5661
4224/5677 [=====================>........] - ETA: 2:07 - loss: 0.6841 - acc: 0.5663
4288/5677 [=====================>........] - ETA: 2:01 - loss: 0.6841 - acc: 0.5665
4352/5677 [=====================>........] - ETA: 1:56 - loss: 0.6840 - acc: 0.5671
4416/5677 [======================>.......] - ETA: 1:50 - loss: 0.6840 - acc: 0.5673
4480/5677 [======================>.......] - ETA: 1:44 - loss: 0.6838 - acc: 0.5676
4544/5677 [=======================>......] - ETA: 1:39 - loss: 0.6837 - acc: 0.5678
4608/5677 [=======================>......] - ETA: 1:33 - loss: 0.6836 - acc: 0.5681
4672/5677 [=======================>......] - ETA: 1:27 - loss: 0.6834 - acc: 0.5685
4736/5677 [========================>.....] - ETA: 1:22 - loss: 0.6831 - acc: 0.5699
4800/5677 [========================>.....] - ETA: 1:16 - loss: 0.6828 - acc: 0.5704
4864/5677 [========================>.....] - ETA: 1:10 - loss: 0.6826 - acc: 0.5705
4928/5677 [=========================>....] - ETA: 1:05 - loss: 0.6828 - acc: 0.5698
4992/5677 [=========================>....] - ETA: 59s - loss: 0.6834 - acc: 0.5683 
5056/5677 [=========================>....] - ETA: 54s - loss: 0.6838 - acc: 0.5672
5120/5677 [==========================>...] - ETA: 48s - loss: 0.6835 - acc: 0.5672
5184/5677 [==========================>...] - ETA: 42s - loss: 0.6841 - acc: 0.5654
5248/5677 [==========================>...] - ETA: 37s - loss: 0.6837 - acc: 0.5663
5312/5677 [===========================>..] - ETA: 31s - loss: 0.6837 - acc: 0.5659
5376/5677 [===========================>..] - ETA: 26s - loss: 0.6836 - acc: 0.5660
5440/5677 [===========================>..] - ETA: 20s - loss: 0.6837 - acc: 0.5658
5504/5677 [============================>.] - ETA: 15s - loss: 0.6840 - acc: 0.5650
5568/5677 [============================>.] - ETA: 9s - loss: 0.6844 - acc: 0.5641 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6841 - acc: 0.5650
5677/5677 [==============================] - 516s 91ms/step - loss: 0.6837 - acc: 0.5660 - val_loss: 0.7037 - val_acc: 0.5182

Epoch 00007: val_acc did not improve from 0.59588
Epoch 8/10

  64/5677 [..............................] - ETA: 7:17 - loss: 0.7057 - acc: 0.5469
 128/5677 [..............................] - ETA: 8:02 - loss: 0.7159 - acc: 0.5234
 192/5677 [>.............................] - ETA: 8:39 - loss: 0.6941 - acc: 0.5729
 256/5677 [>.............................] - ETA: 8:43 - loss: 0.6945 - acc: 0.5469
 320/5677 [>.............................] - ETA: 8:21 - loss: 0.6902 - acc: 0.5563
 384/5677 [=>............................] - ETA: 8:05 - loss: 0.6978 - acc: 0.5365
 448/5677 [=>............................] - ETA: 8:03 - loss: 0.6981 - acc: 0.5268
 512/5677 [=>............................] - ETA: 8:03 - loss: 0.6960 - acc: 0.5410
 576/5677 [==>...........................] - ETA: 7:56 - loss: 0.6965 - acc: 0.5382
 640/5677 [==>...........................] - ETA: 7:47 - loss: 0.6968 - acc: 0.5344
 704/5677 [==>...........................] - ETA: 7:33 - loss: 0.6946 - acc: 0.5398
 768/5677 [===>..........................] - ETA: 7:27 - loss: 0.6949 - acc: 0.5404
 832/5677 [===>..........................] - ETA: 7:26 - loss: 0.6943 - acc: 0.5373
 896/5677 [===>..........................] - ETA: 7:27 - loss: 0.6919 - acc: 0.5458
 960/5677 [====>.........................] - ETA: 7:23 - loss: 0.6909 - acc: 0.5510
1024/5677 [====>.........................] - ETA: 7:18 - loss: 0.6885 - acc: 0.5557
1088/5677 [====>.........................] - ETA: 7:10 - loss: 0.6894 - acc: 0.5487
1152/5677 [=====>........................] - ETA: 7:00 - loss: 0.6897 - acc: 0.5460
1216/5677 [=====>........................] - ETA: 6:52 - loss: 0.6899 - acc: 0.5452
1280/5677 [=====>........................] - ETA: 6:45 - loss: 0.6905 - acc: 0.5406
1344/5677 [======>.......................] - ETA: 6:43 - loss: 0.6907 - acc: 0.5394
1408/5677 [======>.......................] - ETA: 6:39 - loss: 0.6901 - acc: 0.5419
1472/5677 [======>.......................] - ETA: 6:34 - loss: 0.6895 - acc: 0.5435
1536/5677 [=======>......................] - ETA: 6:29 - loss: 0.6898 - acc: 0.5423
1600/5677 [=======>......................] - ETA: 6:23 - loss: 0.6910 - acc: 0.5394
1664/5677 [=======>......................] - ETA: 6:14 - loss: 0.6897 - acc: 0.5421
1728/5677 [========>.....................] - ETA: 6:07 - loss: 0.6892 - acc: 0.5440
1792/5677 [========>.....................] - ETA: 6:00 - loss: 0.6888 - acc: 0.5458
1856/5677 [========>.....................] - ETA: 5:52 - loss: 0.6899 - acc: 0.5431
1920/5677 [=========>....................] - ETA: 5:48 - loss: 0.6892 - acc: 0.5437
1984/5677 [=========>....................] - ETA: 5:44 - loss: 0.6896 - acc: 0.5398
2048/5677 [=========>....................] - ETA: 5:38 - loss: 0.6882 - acc: 0.5415
2112/5677 [==========>...................] - ETA: 5:33 - loss: 0.6885 - acc: 0.5412
2176/5677 [==========>...................] - ETA: 5:28 - loss: 0.6882 - acc: 0.5427
2240/5677 [==========>...................] - ETA: 5:22 - loss: 0.6885 - acc: 0.5420
2304/5677 [===========>..................] - ETA: 5:17 - loss: 0.6874 - acc: 0.5464
2368/5677 [===========>..................] - ETA: 5:11 - loss: 0.6879 - acc: 0.5469
2432/5677 [===========>..................] - ETA: 5:05 - loss: 0.6872 - acc: 0.5485
2496/5677 [============>.................] - ETA: 4:59 - loss: 0.6878 - acc: 0.5477
2560/5677 [============>.................] - ETA: 4:53 - loss: 0.6872 - acc: 0.5484
2624/5677 [============>.................] - ETA: 4:47 - loss: 0.6863 - acc: 0.5518
2688/5677 [=============>................] - ETA: 4:42 - loss: 0.6862 - acc: 0.5513
2752/5677 [=============>................] - ETA: 4:36 - loss: 0.6853 - acc: 0.5541
2816/5677 [=============>................] - ETA: 4:30 - loss: 0.6847 - acc: 0.5554
2880/5677 [==============>...............] - ETA: 4:25 - loss: 0.6856 - acc: 0.5542
2944/5677 [==============>...............] - ETA: 4:19 - loss: 0.6854 - acc: 0.5547
3008/5677 [==============>...............] - ETA: 4:13 - loss: 0.6847 - acc: 0.5562
3072/5677 [===============>..............] - ETA: 4:06 - loss: 0.6845 - acc: 0.5547
3136/5677 [===============>..............] - ETA: 3:59 - loss: 0.6838 - acc: 0.5577
3200/5677 [===============>..............] - ETA: 3:53 - loss: 0.6838 - acc: 0.5581
3264/5677 [================>.............] - ETA: 3:47 - loss: 0.6834 - acc: 0.5600
3328/5677 [================>.............] - ETA: 3:41 - loss: 0.6834 - acc: 0.5601
3392/5677 [================>.............] - ETA: 3:35 - loss: 0.6839 - acc: 0.5598
3456/5677 [=================>............] - ETA: 3:29 - loss: 0.6836 - acc: 0.5602
3520/5677 [=================>............] - ETA: 3:23 - loss: 0.6837 - acc: 0.5597
3584/5677 [=================>............] - ETA: 3:17 - loss: 0.6828 - acc: 0.5611
3648/5677 [==================>...........] - ETA: 3:11 - loss: 0.6829 - acc: 0.5600
3712/5677 [==================>...........] - ETA: 3:06 - loss: 0.6825 - acc: 0.5598
3776/5677 [==================>...........] - ETA: 3:00 - loss: 0.6821 - acc: 0.5599
3840/5677 [===================>..........] - ETA: 2:54 - loss: 0.6821 - acc: 0.5599
3904/5677 [===================>..........] - ETA: 2:48 - loss: 0.6823 - acc: 0.5597
3968/5677 [===================>..........] - ETA: 2:42 - loss: 0.6823 - acc: 0.5600
4032/5677 [====================>.........] - ETA: 2:36 - loss: 0.6820 - acc: 0.5605
4096/5677 [====================>.........] - ETA: 2:30 - loss: 0.6821 - acc: 0.5598
4160/5677 [====================>.........] - ETA: 2:24 - loss: 0.6820 - acc: 0.5591
4224/5677 [=====================>........] - ETA: 2:18 - loss: 0.6815 - acc: 0.5601
4288/5677 [=====================>........] - ETA: 2:12 - loss: 0.6818 - acc: 0.5592
4352/5677 [=====================>........] - ETA: 2:05 - loss: 0.6810 - acc: 0.5607
4416/5677 [======================>.......] - ETA: 1:59 - loss: 0.6816 - acc: 0.5602
4480/5677 [======================>.......] - ETA: 1:53 - loss: 0.6819 - acc: 0.5598
4544/5677 [=======================>......] - ETA: 1:46 - loss: 0.6813 - acc: 0.5601
4608/5677 [=======================>......] - ETA: 1:40 - loss: 0.6811 - acc: 0.5612
4672/5677 [=======================>......] - ETA: 1:34 - loss: 0.6810 - acc: 0.5614
4736/5677 [========================>.....] - ETA: 1:28 - loss: 0.6808 - acc: 0.5614
4800/5677 [========================>.....] - ETA: 1:22 - loss: 0.6808 - acc: 0.5625
4864/5677 [========================>.....] - ETA: 1:16 - loss: 0.6801 - acc: 0.5639
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.6803 - acc: 0.5627
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6801 - acc: 0.5627
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6804 - acc: 0.5625 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6808 - acc: 0.5613
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6813 - acc: 0.5606
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6812 - acc: 0.5602
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6809 - acc: 0.5612
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6810 - acc: 0.5603
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6810 - acc: 0.5601
5504/5677 [============================>.] - ETA: 15s - loss: 0.6809 - acc: 0.5603
5568/5677 [============================>.] - ETA: 10s - loss: 0.6805 - acc: 0.5611
5632/5677 [============================>.] - ETA: 4s - loss: 0.6809 - acc: 0.5602 
5677/5677 [==============================] - 540s 95ms/step - loss: 0.6811 - acc: 0.5600 - val_loss: 0.7024 - val_acc: 0.5071

Epoch 00008: val_acc did not improve from 0.59588
Epoch 9/10

  64/5677 [..............................] - ETA: 7:44 - loss: 0.7105 - acc: 0.5000
 128/5677 [..............................] - ETA: 7:34 - loss: 0.6946 - acc: 0.5234
 192/5677 [>.............................] - ETA: 7:29 - loss: 0.6862 - acc: 0.5677
 256/5677 [>.............................] - ETA: 7:23 - loss: 0.6838 - acc: 0.5781
 320/5677 [>.............................] - ETA: 7:23 - loss: 0.6844 - acc: 0.5656
 384/5677 [=>............................] - ETA: 7:27 - loss: 0.6832 - acc: 0.5677
 448/5677 [=>............................] - ETA: 7:23 - loss: 0.6870 - acc: 0.5692
 512/5677 [=>............................] - ETA: 7:23 - loss: 0.6811 - acc: 0.5820
 576/5677 [==>...........................] - ETA: 7:26 - loss: 0.6774 - acc: 0.5938
 640/5677 [==>...........................] - ETA: 7:17 - loss: 0.6794 - acc: 0.5906
 704/5677 [==>...........................] - ETA: 7:19 - loss: 0.6781 - acc: 0.5952
 768/5677 [===>..........................] - ETA: 7:18 - loss: 0.6785 - acc: 0.5938
 832/5677 [===>..........................] - ETA: 7:13 - loss: 0.6798 - acc: 0.5865
 896/5677 [===>..........................] - ETA: 7:12 - loss: 0.6803 - acc: 0.5882
 960/5677 [====>.........................] - ETA: 7:09 - loss: 0.6791 - acc: 0.5906
1024/5677 [====>.........................] - ETA: 7:06 - loss: 0.6795 - acc: 0.5879
1088/5677 [====>.........................] - ETA: 7:00 - loss: 0.6786 - acc: 0.5919
1152/5677 [=====>........................] - ETA: 6:52 - loss: 0.6785 - acc: 0.5920
1216/5677 [=====>........................] - ETA: 6:46 - loss: 0.6779 - acc: 0.5913
1280/5677 [=====>........................] - ETA: 6:42 - loss: 0.6807 - acc: 0.5828
1344/5677 [======>.......................] - ETA: 6:37 - loss: 0.6806 - acc: 0.5826
1408/5677 [======>.......................] - ETA: 6:31 - loss: 0.6820 - acc: 0.5774
1472/5677 [======>.......................] - ETA: 6:23 - loss: 0.6811 - acc: 0.5781
1536/5677 [=======>......................] - ETA: 6:17 - loss: 0.6819 - acc: 0.5742
1600/5677 [=======>......................] - ETA: 6:09 - loss: 0.6815 - acc: 0.5763
1664/5677 [=======>......................] - ETA: 6:03 - loss: 0.6809 - acc: 0.5763
1728/5677 [========>.....................] - ETA: 5:55 - loss: 0.6802 - acc: 0.5758
1792/5677 [========>.....................] - ETA: 5:48 - loss: 0.6797 - acc: 0.5765
1856/5677 [========>.....................] - ETA: 5:41 - loss: 0.6781 - acc: 0.5781
1920/5677 [=========>....................] - ETA: 5:35 - loss: 0.6772 - acc: 0.5786
1984/5677 [=========>....................] - ETA: 5:27 - loss: 0.6778 - acc: 0.5781
2048/5677 [=========>....................] - ETA: 5:22 - loss: 0.6778 - acc: 0.5771
2112/5677 [==========>...................] - ETA: 5:17 - loss: 0.6784 - acc: 0.5772
2176/5677 [==========>...................] - ETA: 5:12 - loss: 0.6775 - acc: 0.5786
2240/5677 [==========>...................] - ETA: 5:06 - loss: 0.6771 - acc: 0.5768
2304/5677 [===========>..................] - ETA: 5:00 - loss: 0.6771 - acc: 0.5760
2368/5677 [===========>..................] - ETA: 4:54 - loss: 0.6775 - acc: 0.5747
2432/5677 [===========>..................] - ETA: 4:48 - loss: 0.6777 - acc: 0.5744
2496/5677 [============>.................] - ETA: 4:42 - loss: 0.6781 - acc: 0.5741
2560/5677 [============>.................] - ETA: 4:36 - loss: 0.6779 - acc: 0.5754
2624/5677 [============>.................] - ETA: 4:30 - loss: 0.6786 - acc: 0.5732
2688/5677 [=============>................] - ETA: 4:24 - loss: 0.6780 - acc: 0.5759
2752/5677 [=============>................] - ETA: 4:17 - loss: 0.6785 - acc: 0.5752
2816/5677 [=============>................] - ETA: 4:12 - loss: 0.6783 - acc: 0.5749
2880/5677 [==============>...............] - ETA: 4:06 - loss: 0.6776 - acc: 0.5753
2944/5677 [==============>...............] - ETA: 4:00 - loss: 0.6779 - acc: 0.5740
3008/5677 [==============>...............] - ETA: 3:55 - loss: 0.6778 - acc: 0.5738
3072/5677 [===============>..............] - ETA: 3:49 - loss: 0.6779 - acc: 0.5736
3136/5677 [===============>..............] - ETA: 3:43 - loss: 0.6778 - acc: 0.5740
3200/5677 [===============>..............] - ETA: 3:37 - loss: 0.6781 - acc: 0.5719
3264/5677 [================>.............] - ETA: 3:32 - loss: 0.6776 - acc: 0.5732
3328/5677 [================>.............] - ETA: 3:26 - loss: 0.6776 - acc: 0.5730
3392/5677 [================>.............] - ETA: 3:21 - loss: 0.6779 - acc: 0.5728
3456/5677 [=================>............] - ETA: 3:16 - loss: 0.6783 - acc: 0.5726
3520/5677 [=================>............] - ETA: 3:10 - loss: 0.6785 - acc: 0.5724
3584/5677 [=================>............] - ETA: 3:04 - loss: 0.6783 - acc: 0.5731
3648/5677 [==================>...........] - ETA: 2:59 - loss: 0.6786 - acc: 0.5724
3712/5677 [==================>...........] - ETA: 2:53 - loss: 0.6787 - acc: 0.5730
3776/5677 [==================>...........] - ETA: 2:48 - loss: 0.6791 - acc: 0.5720
3840/5677 [===================>..........] - ETA: 2:42 - loss: 0.6796 - acc: 0.5706
3904/5677 [===================>..........] - ETA: 2:37 - loss: 0.6794 - acc: 0.5717
3968/5677 [===================>..........] - ETA: 2:31 - loss: 0.6785 - acc: 0.5738
4032/5677 [====================>.........] - ETA: 2:25 - loss: 0.6779 - acc: 0.5749
4096/5677 [====================>.........] - ETA: 2:20 - loss: 0.6782 - acc: 0.5745
4160/5677 [====================>.........] - ETA: 2:15 - loss: 0.6780 - acc: 0.5745
4224/5677 [=====================>........] - ETA: 2:09 - loss: 0.6783 - acc: 0.5739
4288/5677 [=====================>........] - ETA: 2:03 - loss: 0.6782 - acc: 0.5742
4352/5677 [=====================>........] - ETA: 1:58 - loss: 0.6780 - acc: 0.5744
4416/5677 [======================>.......] - ETA: 1:52 - loss: 0.6780 - acc: 0.5750
4480/5677 [======================>.......] - ETA: 1:46 - loss: 0.6779 - acc: 0.5748
4544/5677 [=======================>......] - ETA: 1:40 - loss: 0.6778 - acc: 0.5744
4608/5677 [=======================>......] - ETA: 1:35 - loss: 0.6778 - acc: 0.5749
4672/5677 [=======================>......] - ETA: 1:29 - loss: 0.6785 - acc: 0.5738
4736/5677 [========================>.....] - ETA: 1:24 - loss: 0.6779 - acc: 0.5754
4800/5677 [========================>.....] - ETA: 1:18 - loss: 0.6782 - acc: 0.5744
4864/5677 [========================>.....] - ETA: 1:12 - loss: 0.6784 - acc: 0.5744
4928/5677 [=========================>....] - ETA: 1:06 - loss: 0.6781 - acc: 0.5753
4992/5677 [=========================>....] - ETA: 1:01 - loss: 0.6780 - acc: 0.5755
5056/5677 [=========================>....] - ETA: 55s - loss: 0.6783 - acc: 0.5752 
5120/5677 [==========================>...] - ETA: 49s - loss: 0.6787 - acc: 0.5746
5184/5677 [==========================>...] - ETA: 44s - loss: 0.6785 - acc: 0.5750
5248/5677 [==========================>...] - ETA: 38s - loss: 0.6792 - acc: 0.5732
5312/5677 [===========================>..] - ETA: 32s - loss: 0.6794 - acc: 0.5730
5376/5677 [===========================>..] - ETA: 26s - loss: 0.6792 - acc: 0.5738
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6796 - acc: 0.5724
5504/5677 [============================>.] - ETA: 15s - loss: 0.6801 - acc: 0.5709
5568/5677 [============================>.] - ETA: 9s - loss: 0.6797 - acc: 0.5720 
5632/5677 [============================>.] - ETA: 4s - loss: 0.6793 - acc: 0.5724
5677/5677 [==============================] - 530s 93ms/step - loss: 0.6791 - acc: 0.5725 - val_loss: 0.6875 - val_acc: 0.5452

Epoch 00009: val_acc did not improve from 0.59588
Epoch 10/10

  64/5677 [..............................] - ETA: 7:42 - loss: 0.6612 - acc: 0.6406
 128/5677 [..............................] - ETA: 7:30 - loss: 0.6716 - acc: 0.6016
 192/5677 [>.............................] - ETA: 7:29 - loss: 0.6681 - acc: 0.5990
 256/5677 [>.............................] - ETA: 7:18 - loss: 0.6831 - acc: 0.5547
 320/5677 [>.............................] - ETA: 7:53 - loss: 0.6855 - acc: 0.5500
 384/5677 [=>............................] - ETA: 8:00 - loss: 0.6834 - acc: 0.5469
 448/5677 [=>............................] - ETA: 7:58 - loss: 0.6873 - acc: 0.5469
 512/5677 [=>............................] - ETA: 7:56 - loss: 0.6874 - acc: 0.5352
 576/5677 [==>...........................] - ETA: 7:45 - loss: 0.6859 - acc: 0.5451
 640/5677 [==>...........................] - ETA: 7:34 - loss: 0.6822 - acc: 0.5516
 704/5677 [==>...........................] - ETA: 7:24 - loss: 0.6829 - acc: 0.5554
 768/5677 [===>..........................] - ETA: 7:14 - loss: 0.6833 - acc: 0.5560
 832/5677 [===>..........................] - ETA: 7:09 - loss: 0.6805 - acc: 0.5649
 896/5677 [===>..........................] - ETA: 7:12 - loss: 0.6805 - acc: 0.5625
 960/5677 [====>.........................] - ETA: 7:10 - loss: 0.6843 - acc: 0.5542
1024/5677 [====>.........................] - ETA: 7:07 - loss: 0.6819 - acc: 0.5605
1088/5677 [====>.........................] - ETA: 7:02 - loss: 0.6825 - acc: 0.5597
1152/5677 [=====>........................] - ETA: 6:57 - loss: 0.6814 - acc: 0.5634
1216/5677 [=====>........................] - ETA: 6:54 - loss: 0.6830 - acc: 0.5617
1280/5677 [=====>........................] - ETA: 6:50 - loss: 0.6828 - acc: 0.5633
1344/5677 [======>.......................] - ETA: 6:45 - loss: 0.6836 - acc: 0.5595
1408/5677 [======>.......................] - ETA: 6:40 - loss: 0.6825 - acc: 0.5625
1472/5677 [======>.......................] - ETA: 6:35 - loss: 0.6827 - acc: 0.5639
1536/5677 [=======>......................] - ETA: 6:31 - loss: 0.6814 - acc: 0.5671
1600/5677 [=======>......................] - ETA: 6:26 - loss: 0.6826 - acc: 0.5644
1664/5677 [=======>......................] - ETA: 6:19 - loss: 0.6817 - acc: 0.5655
1728/5677 [========>.....................] - ETA: 6:14 - loss: 0.6813 - acc: 0.5677
1792/5677 [========>.....................] - ETA: 6:09 - loss: 0.6789 - acc: 0.5742
1856/5677 [========>.....................] - ETA: 6:04 - loss: 0.6809 - acc: 0.5679
1920/5677 [=========>....................] - ETA: 5:59 - loss: 0.6814 - acc: 0.5667
1984/5677 [=========>....................] - ETA: 5:53 - loss: 0.6805 - acc: 0.5706
2048/5677 [=========>....................] - ETA: 5:47 - loss: 0.6806 - acc: 0.5718
2112/5677 [==========>...................] - ETA: 5:39 - loss: 0.6807 - acc: 0.5720
2176/5677 [==========>...................] - ETA: 5:32 - loss: 0.6799 - acc: 0.5726
2240/5677 [==========>...................] - ETA: 5:26 - loss: 0.6791 - acc: 0.5746
2304/5677 [===========>..................] - ETA: 5:21 - loss: 0.6797 - acc: 0.5725
2368/5677 [===========>..................] - ETA: 5:15 - loss: 0.6790 - acc: 0.5739
2432/5677 [===========>..................] - ETA: 5:09 - loss: 0.6791 - acc: 0.5732
2496/5677 [============>.................] - ETA: 5:03 - loss: 0.6791 - acc: 0.5749
2560/5677 [============>.................] - ETA: 4:57 - loss: 0.6779 - acc: 0.5770
2624/5677 [============>.................] - ETA: 4:51 - loss: 0.6787 - acc: 0.5755
2688/5677 [=============>................] - ETA: 4:45 - loss: 0.6786 - acc: 0.5759
2752/5677 [=============>................] - ETA: 4:39 - loss: 0.6783 - acc: 0.5759
2816/5677 [=============>................] - ETA: 4:34 - loss: 0.6767 - acc: 0.5788
2880/5677 [==============>...............] - ETA: 4:28 - loss: 0.6774 - acc: 0.5774
2944/5677 [==============>...............] - ETA: 4:22 - loss: 0.6773 - acc: 0.5774
3008/5677 [==============>...............] - ETA: 4:16 - loss: 0.6771 - acc: 0.5781
3072/5677 [===============>..............] - ETA: 4:09 - loss: 0.6762 - acc: 0.5791
3136/5677 [===============>..............] - ETA: 4:03 - loss: 0.6765 - acc: 0.5784
3200/5677 [===============>..............] - ETA: 3:56 - loss: 0.6767 - acc: 0.5787
3264/5677 [================>.............] - ETA: 3:50 - loss: 0.6767 - acc: 0.5772
3328/5677 [================>.............] - ETA: 3:43 - loss: 0.6763 - acc: 0.5778
3392/5677 [================>.............] - ETA: 3:37 - loss: 0.6765 - acc: 0.5775
3456/5677 [=================>............] - ETA: 3:30 - loss: 0.6772 - acc: 0.5773
3520/5677 [=================>............] - ETA: 3:24 - loss: 0.6768 - acc: 0.5787
3584/5677 [=================>............] - ETA: 3:18 - loss: 0.6770 - acc: 0.5776
3648/5677 [==================>...........] - ETA: 3:12 - loss: 0.6768 - acc: 0.5789
3712/5677 [==================>...........] - ETA: 3:06 - loss: 0.6764 - acc: 0.5800
3776/5677 [==================>...........] - ETA: 3:00 - loss: 0.6764 - acc: 0.5797
3840/5677 [===================>..........] - ETA: 2:54 - loss: 0.6767 - acc: 0.5799
3904/5677 [===================>..........] - ETA: 2:48 - loss: 0.6767 - acc: 0.5791
3968/5677 [===================>..........] - ETA: 2:42 - loss: 0.6764 - acc: 0.5801
4032/5677 [====================>.........] - ETA: 2:35 - loss: 0.6772 - acc: 0.5779
4096/5677 [====================>.........] - ETA: 2:29 - loss: 0.6770 - acc: 0.5784
4160/5677 [====================>.........] - ETA: 2:23 - loss: 0.6773 - acc: 0.5779
4224/5677 [=====================>........] - ETA: 2:17 - loss: 0.6774 - acc: 0.5779
4288/5677 [=====================>........] - ETA: 2:11 - loss: 0.6779 - acc: 0.5767
4352/5677 [=====================>........] - ETA: 2:05 - loss: 0.6773 - acc: 0.5774
4416/5677 [======================>.......] - ETA: 1:59 - loss: 0.6772 - acc: 0.5774
4480/5677 [======================>.......] - ETA: 1:53 - loss: 0.6776 - acc: 0.5763
4544/5677 [=======================>......] - ETA: 1:47 - loss: 0.6781 - acc: 0.5750
4608/5677 [=======================>......] - ETA: 1:40 - loss: 0.6783 - acc: 0.5751
4672/5677 [=======================>......] - ETA: 1:35 - loss: 0.6786 - acc: 0.5745
4736/5677 [========================>.....] - ETA: 1:28 - loss: 0.6788 - acc: 0.5737
4800/5677 [========================>.....] - ETA: 1:22 - loss: 0.6786 - acc: 0.5746
4864/5677 [========================>.....] - ETA: 1:16 - loss: 0.6785 - acc: 0.5752
4928/5677 [=========================>....] - ETA: 1:10 - loss: 0.6787 - acc: 0.5749
4992/5677 [=========================>....] - ETA: 1:04 - loss: 0.6786 - acc: 0.5753
5056/5677 [=========================>....] - ETA: 58s - loss: 0.6784 - acc: 0.5763 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6781 - acc: 0.5770
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6781 - acc: 0.5768
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6781 - acc: 0.5764
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6785 - acc: 0.5755
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6788 - acc: 0.5759
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6792 - acc: 0.5748
5504/5677 [============================>.] - ETA: 15s - loss: 0.6795 - acc: 0.5741
5568/5677 [============================>.] - ETA: 10s - loss: 0.6788 - acc: 0.5758
5632/5677 [============================>.] - ETA: 4s - loss: 0.6791 - acc: 0.5748 
5677/5677 [==============================] - 541s 95ms/step - loss: 0.6788 - acc: 0.5753 - val_loss: 0.6976 - val_acc: 0.5214

Epoch 00010: val_acc did not improve from 0.59588
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9dc1daadd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9dc1daadd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9dc1ccb790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9dc1ccb790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db9b08890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db9b08890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9dc1ccbe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9dc1ccbe90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9db978d810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9db978d810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db98eba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db98eba50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9dc1ccbe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9dc1ccbe10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db92362d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db92362d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db9648790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db9648790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9db9637fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9db9637fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db965b310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db965b310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9db9648cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9db9648cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db94fdf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db94fdf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db92b0f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db92b0f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9db9353890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9db9353890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db934af10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db934af10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9db92b05d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9db92b05d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db9529a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db9529a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db9563890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db9563890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9db8e41a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9db8e41a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db919bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db919bfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9db9042e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9db9042e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db8f2d990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db8f2d990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db8ded190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db8ded190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9da8b62290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9da8b62290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d2070b150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d2070b150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9db8ded4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9db8ded4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db8be7ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db8be7ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db8bef0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9db8bef0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9da8891ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9da8891ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da892cfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da892cfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9db8bef110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9db8bef110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da871cfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da871cfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9da85c0a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9da85c0a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9da852a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9da852a1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da8524050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da8524050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9da87b1190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9da87b1190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da864b510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da864b510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9da02dd390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9da02dd390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9da0325a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9da0325a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da01bc650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da01bc650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9da02f7790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9da02f7790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da01c3590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da01c3590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d9ff7dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d9ff7dfd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d9feeff90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d9feeff90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da01c3510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9da01c3510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d9ff7df90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d9ff7df90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d9fe95110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d9fe95110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d9fc7c410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d9fc7c410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d97b6d110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d97b6d110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d9fc16a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d9fc16a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d9fc7ce10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d9fc7ce10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d9fd3b190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d9fd3b190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d9fc9de50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d9fc9de50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d978bc1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d978bc1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d9798d5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d9798d5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d97b81b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d97b81b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d977569d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d977569d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d9787e5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d9787e5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d9754c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d9754c710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d97644f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d97644f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d97688e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d97688e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d973ffad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d973ffad0>>: AttributeError: module 'gast' has no attribute 'Str'
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 2:05
 128/1578 [=>............................] - ETA: 1:21
 192/1578 [==>...........................] - ETA: 1:08
 256/1578 [===>..........................] - ETA: 59s 
 320/1578 [=====>........................] - ETA: 54s
 384/1578 [======>.......................] - ETA: 50s
 448/1578 [=======>......................] - ETA: 45s
 512/1578 [========>.....................] - ETA: 41s
 576/1578 [=========>....................] - ETA: 38s
 640/1578 [===========>..................] - ETA: 35s
 704/1578 [============>.................] - ETA: 32s
 768/1578 [=============>................] - ETA: 30s
 832/1578 [==============>...............] - ETA: 28s
 896/1578 [================>.............] - ETA: 26s
 960/1578 [=================>............] - ETA: 24s
1024/1578 [==================>...........] - ETA: 21s
1088/1578 [===================>..........] - ETA: 19s
1152/1578 [====================>.........] - ETA: 16s
1216/1578 [======================>.......] - ETA: 14s
1280/1578 [=======================>......] - ETA: 11s
1344/1578 [========================>.....] - ETA: 9s 
1408/1578 [=========================>....] - ETA: 6s
1472/1578 [==========================>...] - ETA: 4s
1536/1578 [============================>.] - ETA: 1s
1578/1578 [==============================] - 62s 39ms/step
loss: 0.6838671635312272
acc: 0.5792141957125887
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f97d02b1e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f97d02b1e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f97d0238090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f97d0238090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db97c97d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db97c97d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9594589890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9594589890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d642f5d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d642f5d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db99ba550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db99ba550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9594589c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9594589c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d642f5d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d642f5d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d97688a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d97688a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97d01cde90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97d01cde90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d86501b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d86501b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9dc1c4cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9dc1c4cf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d86607490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d86607490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d20948d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d20948d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97c0692ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97c0692ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d97b38950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d97b38950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97c07e9790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97c07e9790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c0717190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c0717190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97c0443750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97c0443750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97c04dd090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97c04dd090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c063cd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c063cd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97c0443950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97c0443950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c03b0910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c03b0910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97c03a2090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97c03a2090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97c0040690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97c0040690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c03933d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c03933d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97c04ddc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97c04ddc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c03a90d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c03a90d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97605cfb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97605cfb50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97604dbcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97604dbcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c01339d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97c01339d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97605cfc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97605cfc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f976052ba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f976052ba50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97602fef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97602fef50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97602df7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97602df7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9760255950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9760255950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97602fe6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97602fe6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9760257bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9760257bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f976017b490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f976017b490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97347e23d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97347e23d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97346a8310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97346a8310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97602dfc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97602dfc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97601c8510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97601c8510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f973447d690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f973447d690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9734482150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9734482150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97346a6f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97346a6f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9734546710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9734546710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9734375890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9734375890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9734190690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9734190690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9734373090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9734373090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97187d4210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97187d4210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9734461f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9734461f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f971869afd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f971869afd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95944ffbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95944ffbd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9594436f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9594436f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9594403850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9594403850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95944ff650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95944ff650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9594577990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9594577990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f959446c4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f959446c4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95940dfc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95940dfc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f959417e790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f959417e790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9594495790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9594495790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f959419e210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f959419e210>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 37:55 - loss: 0.6924 - acc: 0.5156
 128/5677 [..............................] - ETA: 24:13 - loss: 0.6909 - acc: 0.5469
 192/5677 [>.............................] - ETA: 19:28 - loss: 0.7314 - acc: 0.5365
 256/5677 [>.............................] - ETA: 17:21 - loss: 0.7302 - acc: 0.5430
 320/5677 [>.............................] - ETA: 15:53 - loss: 0.7254 - acc: 0.5344
 384/5677 [=>............................] - ETA: 14:51 - loss: 0.7244 - acc: 0.5286
 448/5677 [=>............................] - ETA: 13:58 - loss: 0.7274 - acc: 0.5179
 512/5677 [=>............................] - ETA: 13:16 - loss: 0.7317 - acc: 0.5137
 576/5677 [==>...........................] - ETA: 12:47 - loss: 0.7367 - acc: 0.5035
 640/5677 [==>...........................] - ETA: 12:17 - loss: 0.7356 - acc: 0.4984
 704/5677 [==>...........................] - ETA: 11:54 - loss: 0.7386 - acc: 0.4957
 768/5677 [===>..........................] - ETA: 11:36 - loss: 0.7357 - acc: 0.5000
 832/5677 [===>..........................] - ETA: 11:25 - loss: 0.7327 - acc: 0.5036
 896/5677 [===>..........................] - ETA: 11:08 - loss: 0.7328 - acc: 0.5011
 960/5677 [====>.........................] - ETA: 10:57 - loss: 0.7291 - acc: 0.5073
1024/5677 [====>.........................] - ETA: 10:41 - loss: 0.7319 - acc: 0.5039
1088/5677 [====>.........................] - ETA: 10:27 - loss: 0.7272 - acc: 0.5074
1152/5677 [=====>........................] - ETA: 10:12 - loss: 0.7262 - acc: 0.5078
1216/5677 [=====>........................] - ETA: 10:00 - loss: 0.7263 - acc: 0.5041
1280/5677 [=====>........................] - ETA: 9:46 - loss: 0.7266 - acc: 0.5047 
1344/5677 [======>.......................] - ETA: 9:33 - loss: 0.7289 - acc: 0.5022
1408/5677 [======>.......................] - ETA: 9:21 - loss: 0.7288 - acc: 0.5021
1472/5677 [======>.......................] - ETA: 9:14 - loss: 0.7293 - acc: 0.4993
1536/5677 [=======>......................] - ETA: 9:04 - loss: 0.7274 - acc: 0.5052
1600/5677 [=======>......................] - ETA: 8:54 - loss: 0.7271 - acc: 0.5062
1664/5677 [=======>......................] - ETA: 8:44 - loss: 0.7265 - acc: 0.5072
1728/5677 [========>.....................] - ETA: 8:34 - loss: 0.7266 - acc: 0.5064
1792/5677 [========>.....................] - ETA: 8:23 - loss: 0.7250 - acc: 0.5073
1856/5677 [========>.....................] - ETA: 8:13 - loss: 0.7239 - acc: 0.5081
1920/5677 [=========>....................] - ETA: 8:03 - loss: 0.7218 - acc: 0.5109
1984/5677 [=========>....................] - ETA: 7:52 - loss: 0.7235 - acc: 0.5106
2048/5677 [=========>....................] - ETA: 7:44 - loss: 0.7248 - acc: 0.5117
2112/5677 [==========>...................] - ETA: 7:35 - loss: 0.7256 - acc: 0.5118
2176/5677 [==========>...................] - ETA: 7:27 - loss: 0.7259 - acc: 0.5115
2240/5677 [==========>...................] - ETA: 7:18 - loss: 0.7252 - acc: 0.5125
2304/5677 [===========>..................] - ETA: 7:09 - loss: 0.7241 - acc: 0.5135
2368/5677 [===========>..................] - ETA: 7:01 - loss: 0.7237 - acc: 0.5139
2432/5677 [===========>..................] - ETA: 6:52 - loss: 0.7240 - acc: 0.5144
2496/5677 [============>.................] - ETA: 6:44 - loss: 0.7245 - acc: 0.5132
2560/5677 [============>.................] - ETA: 6:36 - loss: 0.7241 - acc: 0.5137
2624/5677 [============>.................] - ETA: 6:28 - loss: 0.7220 - acc: 0.5187
2688/5677 [=============>................] - ETA: 6:21 - loss: 0.7224 - acc: 0.5179
2752/5677 [=============>................] - ETA: 6:12 - loss: 0.7220 - acc: 0.5193
2816/5677 [=============>................] - ETA: 6:03 - loss: 0.7212 - acc: 0.5217
2880/5677 [==============>...............] - ETA: 5:56 - loss: 0.7209 - acc: 0.5219
2944/5677 [==============>...............] - ETA: 5:49 - loss: 0.7196 - acc: 0.5241
3008/5677 [==============>...............] - ETA: 5:40 - loss: 0.7198 - acc: 0.5236
3072/5677 [===============>..............] - ETA: 5:32 - loss: 0.7197 - acc: 0.5251
3136/5677 [===============>..............] - ETA: 5:24 - loss: 0.7200 - acc: 0.5249
3200/5677 [===============>..............] - ETA: 5:16 - loss: 0.7204 - acc: 0.5241
3264/5677 [================>.............] - ETA: 5:08 - loss: 0.7200 - acc: 0.5251
3328/5677 [================>.............] - ETA: 5:00 - loss: 0.7209 - acc: 0.5234
3392/5677 [================>.............] - ETA: 4:51 - loss: 0.7212 - acc: 0.5236
3456/5677 [=================>............] - ETA: 4:43 - loss: 0.7225 - acc: 0.5223
3520/5677 [=================>............] - ETA: 4:35 - loss: 0.7220 - acc: 0.5236
3584/5677 [=================>............] - ETA: 4:26 - loss: 0.7230 - acc: 0.5209
3648/5677 [==================>...........] - ETA: 4:18 - loss: 0.7233 - acc: 0.5195
3712/5677 [==================>...........] - ETA: 4:10 - loss: 0.7236 - acc: 0.5189
3776/5677 [==================>...........] - ETA: 4:01 - loss: 0.7236 - acc: 0.5183
3840/5677 [===================>..........] - ETA: 3:53 - loss: 0.7236 - acc: 0.5177
3904/5677 [===================>..........] - ETA: 3:45 - loss: 0.7237 - acc: 0.5177
3968/5677 [===================>..........] - ETA: 3:37 - loss: 0.7238 - acc: 0.5169
4032/5677 [====================>.........] - ETA: 3:28 - loss: 0.7238 - acc: 0.5171
4096/5677 [====================>.........] - ETA: 3:20 - loss: 0.7240 - acc: 0.5176
4160/5677 [====================>.........] - ETA: 3:12 - loss: 0.7230 - acc: 0.5190
4224/5677 [=====================>........] - ETA: 3:04 - loss: 0.7228 - acc: 0.5192
4288/5677 [=====================>........] - ETA: 2:56 - loss: 0.7225 - acc: 0.5189
4352/5677 [=====================>........] - ETA: 2:48 - loss: 0.7221 - acc: 0.5191
4416/5677 [======================>.......] - ETA: 2:40 - loss: 0.7226 - acc: 0.5183
4480/5677 [======================>.......] - ETA: 2:32 - loss: 0.7225 - acc: 0.5172
4544/5677 [=======================>......] - ETA: 2:23 - loss: 0.7220 - acc: 0.5178
4608/5677 [=======================>......] - ETA: 2:15 - loss: 0.7219 - acc: 0.5182
4672/5677 [=======================>......] - ETA: 2:07 - loss: 0.7222 - acc: 0.5180
4736/5677 [========================>.....] - ETA: 1:59 - loss: 0.7220 - acc: 0.5182
4800/5677 [========================>.....] - ETA: 1:51 - loss: 0.7214 - acc: 0.5185
4864/5677 [========================>.....] - ETA: 1:43 - loss: 0.7209 - acc: 0.5187
4928/5677 [=========================>....] - ETA: 1:34 - loss: 0.7203 - acc: 0.5193
4992/5677 [=========================>....] - ETA: 1:26 - loss: 0.7206 - acc: 0.5180
5056/5677 [=========================>....] - ETA: 1:18 - loss: 0.7202 - acc: 0.5188
5120/5677 [==========================>...] - ETA: 1:10 - loss: 0.7205 - acc: 0.5180
5184/5677 [==========================>...] - ETA: 1:02 - loss: 0.7202 - acc: 0.5193
5248/5677 [==========================>...] - ETA: 54s - loss: 0.7203 - acc: 0.5189 
5312/5677 [===========================>..] - ETA: 46s - loss: 0.7204 - acc: 0.5184
5376/5677 [===========================>..] - ETA: 38s - loss: 0.7203 - acc: 0.5190
5440/5677 [===========================>..] - ETA: 30s - loss: 0.7201 - acc: 0.5193
5504/5677 [============================>.] - ETA: 21s - loss: 0.7196 - acc: 0.5193
5568/5677 [============================>.] - ETA: 13s - loss: 0.7190 - acc: 0.5201
5632/5677 [============================>.] - ETA: 5s - loss: 0.7191 - acc: 0.5197 
5677/5677 [==============================] - 753s 133ms/step - loss: 0.7186 - acc: 0.5203 - val_loss: 0.6808 - val_acc: 0.5705

Epoch 00001: val_acc improved from -inf to 0.57052, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window07/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 12:07 - loss: 0.6770 - acc: 0.5312
 128/5677 [..............................] - ETA: 11:42 - loss: 0.7265 - acc: 0.4844
 192/5677 [>.............................] - ETA: 11:18 - loss: 0.7239 - acc: 0.4792
 256/5677 [>.............................] - ETA: 11:08 - loss: 0.7364 - acc: 0.4688
 320/5677 [>.............................] - ETA: 10:53 - loss: 0.7279 - acc: 0.4813
 384/5677 [=>............................] - ETA: 10:42 - loss: 0.7220 - acc: 0.4922
 448/5677 [=>............................] - ETA: 10:38 - loss: 0.7164 - acc: 0.4955
 512/5677 [=>............................] - ETA: 10:30 - loss: 0.7098 - acc: 0.5137
 576/5677 [==>...........................] - ETA: 10:24 - loss: 0.7068 - acc: 0.5156
 640/5677 [==>...........................] - ETA: 10:16 - loss: 0.7071 - acc: 0.5172
 704/5677 [==>...........................] - ETA: 10:10 - loss: 0.7043 - acc: 0.5199
 768/5677 [===>..........................] - ETA: 10:05 - loss: 0.7029 - acc: 0.5260
 832/5677 [===>..........................] - ETA: 9:56 - loss: 0.7063 - acc: 0.5264 
 896/5677 [===>..........................] - ETA: 9:51 - loss: 0.7094 - acc: 0.5234
 960/5677 [====>.........................] - ETA: 9:45 - loss: 0.7085 - acc: 0.5260
1024/5677 [====>.........................] - ETA: 9:39 - loss: 0.7064 - acc: 0.5293
1088/5677 [====>.........................] - ETA: 9:31 - loss: 0.7033 - acc: 0.5349
1152/5677 [=====>........................] - ETA: 9:25 - loss: 0.7056 - acc: 0.5339
1216/5677 [=====>........................] - ETA: 9:17 - loss: 0.7032 - acc: 0.5378
1280/5677 [=====>........................] - ETA: 9:10 - loss: 0.7041 - acc: 0.5383
1344/5677 [======>.......................] - ETA: 9:03 - loss: 0.7041 - acc: 0.5409
1408/5677 [======>.......................] - ETA: 8:53 - loss: 0.7036 - acc: 0.5405
1472/5677 [======>.......................] - ETA: 8:45 - loss: 0.7025 - acc: 0.5435
1536/5677 [=======>......................] - ETA: 8:36 - loss: 0.7038 - acc: 0.5430
1600/5677 [=======>......................] - ETA: 8:28 - loss: 0.7036 - acc: 0.5406
1664/5677 [=======>......................] - ETA: 8:22 - loss: 0.7042 - acc: 0.5379
1728/5677 [========>.....................] - ETA: 8:14 - loss: 0.7034 - acc: 0.5370
1792/5677 [========>.....................] - ETA: 8:06 - loss: 0.7049 - acc: 0.5324
1856/5677 [========>.....................] - ETA: 7:57 - loss: 0.7045 - acc: 0.5334
1920/5677 [=========>....................] - ETA: 7:48 - loss: 0.7049 - acc: 0.5297
1984/5677 [=========>....................] - ETA: 7:40 - loss: 0.7047 - acc: 0.5292
2048/5677 [=========>....................] - ETA: 7:31 - loss: 0.7036 - acc: 0.5298
2112/5677 [==========>...................] - ETA: 7:23 - loss: 0.7032 - acc: 0.5303
2176/5677 [==========>...................] - ETA: 7:15 - loss: 0.7048 - acc: 0.5276
2240/5677 [==========>...................] - ETA: 7:05 - loss: 0.7053 - acc: 0.5241
2304/5677 [===========>..................] - ETA: 6:57 - loss: 0.7064 - acc: 0.5213
2368/5677 [===========>..................] - ETA: 6:49 - loss: 0.7075 - acc: 0.5203
2432/5677 [===========>..................] - ETA: 6:41 - loss: 0.7070 - acc: 0.5210
2496/5677 [============>.................] - ETA: 6:32 - loss: 0.7072 - acc: 0.5192
2560/5677 [============>.................] - ETA: 6:24 - loss: 0.7085 - acc: 0.5172
2624/5677 [============>.................] - ETA: 6:17 - loss: 0.7077 - acc: 0.5183
2688/5677 [=============>................] - ETA: 6:09 - loss: 0.7075 - acc: 0.5197
2752/5677 [=============>................] - ETA: 6:00 - loss: 0.7065 - acc: 0.5214
2816/5677 [=============>................] - ETA: 5:53 - loss: 0.7057 - acc: 0.5234
2880/5677 [==============>...............] - ETA: 5:44 - loss: 0.7063 - acc: 0.5226
2944/5677 [==============>...............] - ETA: 5:37 - loss: 0.7060 - acc: 0.5241
3008/5677 [==============>...............] - ETA: 5:28 - loss: 0.7052 - acc: 0.5249
3072/5677 [===============>..............] - ETA: 5:20 - loss: 0.7055 - acc: 0.5247
3136/5677 [===============>..............] - ETA: 5:11 - loss: 0.7043 - acc: 0.5261
3200/5677 [===============>..............] - ETA: 5:03 - loss: 0.7045 - acc: 0.5266
3264/5677 [================>.............] - ETA: 4:55 - loss: 0.7043 - acc: 0.5267
3328/5677 [================>.............] - ETA: 4:47 - loss: 0.7046 - acc: 0.5252
3392/5677 [================>.............] - ETA: 4:39 - loss: 0.7057 - acc: 0.5221
3456/5677 [=================>............] - ETA: 4:31 - loss: 0.7057 - acc: 0.5211
3520/5677 [=================>............] - ETA: 4:23 - loss: 0.7057 - acc: 0.5202
3584/5677 [=================>............] - ETA: 4:15 - loss: 0.7058 - acc: 0.5190
3648/5677 [==================>...........] - ETA: 4:07 - loss: 0.7058 - acc: 0.5189
3712/5677 [==================>...........] - ETA: 3:59 - loss: 0.7057 - acc: 0.5189
3776/5677 [==================>...........] - ETA: 3:51 - loss: 0.7062 - acc: 0.5177
3840/5677 [===================>..........] - ETA: 3:44 - loss: 0.7061 - acc: 0.5188
3904/5677 [===================>..........] - ETA: 3:36 - loss: 0.7062 - acc: 0.5197
3968/5677 [===================>..........] - ETA: 3:28 - loss: 0.7063 - acc: 0.5207
4032/5677 [====================>.........] - ETA: 3:20 - loss: 0.7060 - acc: 0.5198
4096/5677 [====================>.........] - ETA: 3:12 - loss: 0.7058 - acc: 0.5205
4160/5677 [====================>.........] - ETA: 3:04 - loss: 0.7046 - acc: 0.5231
4224/5677 [=====================>........] - ETA: 2:56 - loss: 0.7040 - acc: 0.5239
4288/5677 [=====================>........] - ETA: 2:48 - loss: 0.7036 - acc: 0.5250
4352/5677 [=====================>........] - ETA: 2:40 - loss: 0.7035 - acc: 0.5241
4416/5677 [======================>.......] - ETA: 2:32 - loss: 0.7034 - acc: 0.5240
4480/5677 [======================>.......] - ETA: 2:24 - loss: 0.7029 - acc: 0.5237
4544/5677 [=======================>......] - ETA: 2:16 - loss: 0.7024 - acc: 0.5253
4608/5677 [=======================>......] - ETA: 2:08 - loss: 0.7018 - acc: 0.5267
4672/5677 [=======================>......] - ETA: 2:00 - loss: 0.7021 - acc: 0.5259
4736/5677 [========================>.....] - ETA: 1:52 - loss: 0.7013 - acc: 0.5279
4800/5677 [========================>.....] - ETA: 1:44 - loss: 0.7012 - acc: 0.5283
4864/5677 [========================>.....] - ETA: 1:37 - loss: 0.7009 - acc: 0.5298
4928/5677 [=========================>....] - ETA: 1:29 - loss: 0.7010 - acc: 0.5306
4992/5677 [=========================>....] - ETA: 1:21 - loss: 0.7017 - acc: 0.5300
5056/5677 [=========================>....] - ETA: 1:13 - loss: 0.7020 - acc: 0.5295
5120/5677 [==========================>...] - ETA: 1:06 - loss: 0.7020 - acc: 0.5303
5184/5677 [==========================>...] - ETA: 58s - loss: 0.7018 - acc: 0.5318 
5248/5677 [==========================>...] - ETA: 51s - loss: 0.7015 - acc: 0.5326
5312/5677 [===========================>..] - ETA: 43s - loss: 0.7012 - acc: 0.5337
5376/5677 [===========================>..] - ETA: 35s - loss: 0.7014 - acc: 0.5329
5440/5677 [===========================>..] - ETA: 28s - loss: 0.7015 - acc: 0.5331
5504/5677 [============================>.] - ETA: 20s - loss: 0.7014 - acc: 0.5331
5568/5677 [============================>.] - ETA: 12s - loss: 0.7012 - acc: 0.5339
5632/5677 [============================>.] - ETA: 5s - loss: 0.7011 - acc: 0.5337 
5677/5677 [==============================] - 702s 124ms/step - loss: 0.7016 - acc: 0.5330 - val_loss: 0.6835 - val_acc: 0.5436

Epoch 00002: val_acc did not improve from 0.57052
Epoch 3/10

  64/5677 [..............................] - ETA: 11:31 - loss: 0.7429 - acc: 0.4375
 128/5677 [..............................] - ETA: 11:29 - loss: 0.7123 - acc: 0.5078
 192/5677 [>.............................] - ETA: 11:04 - loss: 0.7157 - acc: 0.4896
 256/5677 [>.............................] - ETA: 10:56 - loss: 0.7098 - acc: 0.4883
 320/5677 [>.............................] - ETA: 10:41 - loss: 0.7117 - acc: 0.4813
 384/5677 [=>............................] - ETA: 10:35 - loss: 0.7054 - acc: 0.4974
 448/5677 [=>............................] - ETA: 10:29 - loss: 0.7027 - acc: 0.5089
 512/5677 [=>............................] - ETA: 10:17 - loss: 0.7015 - acc: 0.5117
 576/5677 [==>...........................] - ETA: 10:14 - loss: 0.7006 - acc: 0.5191
 640/5677 [==>...........................] - ETA: 10:00 - loss: 0.7024 - acc: 0.5141
 704/5677 [==>...........................] - ETA: 9:49 - loss: 0.6998 - acc: 0.5170 
 768/5677 [===>..........................] - ETA: 9:39 - loss: 0.7028 - acc: 0.5117
 832/5677 [===>..........................] - ETA: 9:33 - loss: 0.7053 - acc: 0.5084
 896/5677 [===>..........................] - ETA: 9:28 - loss: 0.7058 - acc: 0.5078
 960/5677 [====>.........................] - ETA: 9:22 - loss: 0.7079 - acc: 0.5083
1024/5677 [====>.........................] - ETA: 9:15 - loss: 0.7068 - acc: 0.5068
1088/5677 [====>.........................] - ETA: 9:06 - loss: 0.7089 - acc: 0.5000
1152/5677 [=====>........................] - ETA: 8:59 - loss: 0.7071 - acc: 0.5035
1216/5677 [=====>........................] - ETA: 8:53 - loss: 0.7057 - acc: 0.5066
1280/5677 [=====>........................] - ETA: 8:48 - loss: 0.7046 - acc: 0.5109
1344/5677 [======>.......................] - ETA: 8:40 - loss: 0.7065 - acc: 0.5067
1408/5677 [======>.......................] - ETA: 8:30 - loss: 0.7055 - acc: 0.5057
1472/5677 [======>.......................] - ETA: 8:21 - loss: 0.7058 - acc: 0.5061
1536/5677 [=======>......................] - ETA: 8:13 - loss: 0.7040 - acc: 0.5091
1600/5677 [=======>......................] - ETA: 8:05 - loss: 0.7020 - acc: 0.5138
1664/5677 [=======>......................] - ETA: 7:58 - loss: 0.6999 - acc: 0.5174
1728/5677 [========>.....................] - ETA: 7:50 - loss: 0.6994 - acc: 0.5179
1792/5677 [========>.....................] - ETA: 7:41 - loss: 0.6991 - acc: 0.5206
1856/5677 [========>.....................] - ETA: 7:33 - loss: 0.6996 - acc: 0.5183
1920/5677 [=========>....................] - ETA: 7:24 - loss: 0.7004 - acc: 0.5167
1984/5677 [=========>....................] - ETA: 7:16 - loss: 0.7000 - acc: 0.5181
2048/5677 [=========>....................] - ETA: 7:09 - loss: 0.7002 - acc: 0.5181
2112/5677 [==========>...................] - ETA: 7:03 - loss: 0.6999 - acc: 0.5194
2176/5677 [==========>...................] - ETA: 6:56 - loss: 0.6996 - acc: 0.5207
2240/5677 [==========>...................] - ETA: 6:48 - loss: 0.6987 - acc: 0.5228
2304/5677 [===========>..................] - ETA: 6:40 - loss: 0.6978 - acc: 0.5239
2368/5677 [===========>..................] - ETA: 6:31 - loss: 0.6976 - acc: 0.5236
2432/5677 [===========>..................] - ETA: 6:22 - loss: 0.6977 - acc: 0.5255
2496/5677 [============>.................] - ETA: 6:12 - loss: 0.6978 - acc: 0.5236
2560/5677 [============>.................] - ETA: 6:02 - loss: 0.6968 - acc: 0.5250
2624/5677 [============>.................] - ETA: 5:52 - loss: 0.6972 - acc: 0.5248
2688/5677 [=============>................] - ETA: 5:43 - loss: 0.6969 - acc: 0.5257
2752/5677 [=============>................] - ETA: 5:36 - loss: 0.6966 - acc: 0.5273
2816/5677 [=============>................] - ETA: 5:28 - loss: 0.6973 - acc: 0.5252
2880/5677 [==============>...............] - ETA: 5:20 - loss: 0.6965 - acc: 0.5281
2944/5677 [==============>...............] - ETA: 5:11 - loss: 0.6969 - acc: 0.5275
3008/5677 [==============>...............] - ETA: 5:03 - loss: 0.6956 - acc: 0.5306
3072/5677 [===============>..............] - ETA: 4:56 - loss: 0.6947 - acc: 0.5332
3136/5677 [===============>..............] - ETA: 4:48 - loss: 0.6946 - acc: 0.5341
3200/5677 [===============>..............] - ETA: 4:39 - loss: 0.6944 - acc: 0.5341
3264/5677 [================>.............] - ETA: 4:31 - loss: 0.6937 - acc: 0.5358
3328/5677 [================>.............] - ETA: 4:23 - loss: 0.6932 - acc: 0.5370
3392/5677 [================>.............] - ETA: 4:14 - loss: 0.6921 - acc: 0.5392
3456/5677 [=================>............] - ETA: 4:06 - loss: 0.6916 - acc: 0.5408
3520/5677 [=================>............] - ETA: 3:58 - loss: 0.6921 - acc: 0.5398
3584/5677 [=================>............] - ETA: 3:51 - loss: 0.6921 - acc: 0.5393
3648/5677 [==================>...........] - ETA: 3:44 - loss: 0.6918 - acc: 0.5414
3712/5677 [==================>...........] - ETA: 3:37 - loss: 0.6928 - acc: 0.5407
3776/5677 [==================>...........] - ETA: 3:29 - loss: 0.6931 - acc: 0.5403
3840/5677 [===================>..........] - ETA: 3:22 - loss: 0.6925 - acc: 0.5414
3904/5677 [===================>..........] - ETA: 3:15 - loss: 0.6923 - acc: 0.5420
3968/5677 [===================>..........] - ETA: 3:08 - loss: 0.6917 - acc: 0.5436
4032/5677 [====================>.........] - ETA: 3:00 - loss: 0.6919 - acc: 0.5441
4096/5677 [====================>.........] - ETA: 2:53 - loss: 0.6915 - acc: 0.5449
4160/5677 [====================>.........] - ETA: 2:46 - loss: 0.6914 - acc: 0.5452
4224/5677 [=====================>........] - ETA: 2:39 - loss: 0.6911 - acc: 0.5464
4288/5677 [=====================>........] - ETA: 2:32 - loss: 0.6907 - acc: 0.5473
4352/5677 [=====================>........] - ETA: 2:25 - loss: 0.6907 - acc: 0.5466
4416/5677 [======================>.......] - ETA: 2:18 - loss: 0.6907 - acc: 0.5464
4480/5677 [======================>.......] - ETA: 2:11 - loss: 0.6913 - acc: 0.5458
4544/5677 [=======================>......] - ETA: 2:04 - loss: 0.6907 - acc: 0.5464
4608/5677 [=======================>......] - ETA: 1:56 - loss: 0.6903 - acc: 0.5471
4672/5677 [=======================>......] - ETA: 1:49 - loss: 0.6897 - acc: 0.5479
4736/5677 [========================>.....] - ETA: 1:42 - loss: 0.6893 - acc: 0.5481
4800/5677 [========================>.....] - ETA: 1:35 - loss: 0.6893 - acc: 0.5485
4864/5677 [========================>.....] - ETA: 1:28 - loss: 0.6897 - acc: 0.5477
4928/5677 [=========================>....] - ETA: 1:21 - loss: 0.6899 - acc: 0.5473
4992/5677 [=========================>....] - ETA: 1:14 - loss: 0.6897 - acc: 0.5481
5056/5677 [=========================>....] - ETA: 1:07 - loss: 0.6896 - acc: 0.5471
5120/5677 [==========================>...] - ETA: 1:00 - loss: 0.6898 - acc: 0.5459
5184/5677 [==========================>...] - ETA: 53s - loss: 0.6896 - acc: 0.5461 
5248/5677 [==========================>...] - ETA: 46s - loss: 0.6900 - acc: 0.5457
5312/5677 [===========================>..] - ETA: 39s - loss: 0.6897 - acc: 0.5467
5376/5677 [===========================>..] - ETA: 32s - loss: 0.6898 - acc: 0.5461
5440/5677 [===========================>..] - ETA: 25s - loss: 0.6896 - acc: 0.5469
5504/5677 [============================>.] - ETA: 18s - loss: 0.6895 - acc: 0.5471
5568/5677 [============================>.] - ETA: 11s - loss: 0.6894 - acc: 0.5476
5632/5677 [============================>.] - ETA: 4s - loss: 0.6897 - acc: 0.5462 
5677/5677 [==============================] - 638s 112ms/step - loss: 0.6898 - acc: 0.5461 - val_loss: 0.7205 - val_acc: 0.4865

Epoch 00003: val_acc did not improve from 0.57052
Epoch 4/10

  64/5677 [..............................] - ETA: 10:24 - loss: 0.7003 - acc: 0.5469
 128/5677 [..............................] - ETA: 9:50 - loss: 0.6619 - acc: 0.6328 
 192/5677 [>.............................] - ETA: 9:25 - loss: 0.6666 - acc: 0.6250
 256/5677 [>.............................] - ETA: 9:23 - loss: 0.6629 - acc: 0.6289
 320/5677 [>.............................] - ETA: 9:22 - loss: 0.6687 - acc: 0.6094
 384/5677 [=>............................] - ETA: 9:15 - loss: 0.6712 - acc: 0.6146
 448/5677 [=>............................] - ETA: 9:10 - loss: 0.6711 - acc: 0.6138
 512/5677 [=>............................] - ETA: 9:06 - loss: 0.6733 - acc: 0.5996
 576/5677 [==>...........................] - ETA: 8:50 - loss: 0.6716 - acc: 0.5990
 640/5677 [==>...........................] - ETA: 8:42 - loss: 0.6735 - acc: 0.5891
 704/5677 [==>...........................] - ETA: 8:32 - loss: 0.6766 - acc: 0.5852
 768/5677 [===>..........................] - ETA: 8:24 - loss: 0.6796 - acc: 0.5794
 832/5677 [===>..........................] - ETA: 8:16 - loss: 0.6840 - acc: 0.5721
 896/5677 [===>..........................] - ETA: 8:09 - loss: 0.6834 - acc: 0.5714
 960/5677 [====>.........................] - ETA: 8:01 - loss: 0.6809 - acc: 0.5750
1024/5677 [====>.........................] - ETA: 7:54 - loss: 0.6844 - acc: 0.5664
1088/5677 [====>.........................] - ETA: 7:49 - loss: 0.6841 - acc: 0.5662
1152/5677 [=====>........................] - ETA: 7:43 - loss: 0.6827 - acc: 0.5677
1216/5677 [=====>........................] - ETA: 7:38 - loss: 0.6826 - acc: 0.5674
1280/5677 [=====>........................] - ETA: 7:33 - loss: 0.6824 - acc: 0.5656
1344/5677 [======>.......................] - ETA: 7:26 - loss: 0.6835 - acc: 0.5640
1408/5677 [======>.......................] - ETA: 7:19 - loss: 0.6824 - acc: 0.5639
1472/5677 [======>.......................] - ETA: 7:13 - loss: 0.6805 - acc: 0.5686
1536/5677 [=======>......................] - ETA: 7:06 - loss: 0.6811 - acc: 0.5671
1600/5677 [=======>......................] - ETA: 6:58 - loss: 0.6812 - acc: 0.5650
1664/5677 [=======>......................] - ETA: 6:51 - loss: 0.6809 - acc: 0.5649
1728/5677 [========>.....................] - ETA: 6:44 - loss: 0.6808 - acc: 0.5671
1792/5677 [========>.....................] - ETA: 6:37 - loss: 0.6812 - acc: 0.5664
1856/5677 [========>.....................] - ETA: 6:30 - loss: 0.6799 - acc: 0.5673
1920/5677 [=========>....................] - ETA: 6:22 - loss: 0.6800 - acc: 0.5667
1984/5677 [=========>....................] - ETA: 6:16 - loss: 0.6790 - acc: 0.5691
2048/5677 [=========>....................] - ETA: 6:09 - loss: 0.6792 - acc: 0.5688
2112/5677 [==========>...................] - ETA: 6:02 - loss: 0.6787 - acc: 0.5710
2176/5677 [==========>...................] - ETA: 5:55 - loss: 0.6791 - acc: 0.5689
2240/5677 [==========>...................] - ETA: 5:48 - loss: 0.6791 - acc: 0.5705
2304/5677 [===========>..................] - ETA: 5:40 - loss: 0.6799 - acc: 0.5694
2368/5677 [===========>..................] - ETA: 5:33 - loss: 0.6798 - acc: 0.5680
2432/5677 [===========>..................] - ETA: 5:27 - loss: 0.6801 - acc: 0.5683
2496/5677 [============>.................] - ETA: 5:20 - loss: 0.6819 - acc: 0.5629
2560/5677 [============>.................] - ETA: 5:13 - loss: 0.6815 - acc: 0.5641
2624/5677 [============>.................] - ETA: 5:06 - loss: 0.6816 - acc: 0.5640
2688/5677 [=============>................] - ETA: 4:59 - loss: 0.6817 - acc: 0.5640
2752/5677 [=============>................] - ETA: 4:52 - loss: 0.6812 - acc: 0.5640
2816/5677 [=============>................] - ETA: 4:44 - loss: 0.6814 - acc: 0.5625
2880/5677 [==============>...............] - ETA: 4:38 - loss: 0.6814 - acc: 0.5615
2944/5677 [==============>...............] - ETA: 4:30 - loss: 0.6807 - acc: 0.5618
3008/5677 [==============>...............] - ETA: 4:24 - loss: 0.6818 - acc: 0.5592
3072/5677 [===============>..............] - ETA: 4:17 - loss: 0.6826 - acc: 0.5589
3136/5677 [===============>..............] - ETA: 4:11 - loss: 0.6820 - acc: 0.5609
3200/5677 [===============>..............] - ETA: 4:04 - loss: 0.6840 - acc: 0.5559
3264/5677 [================>.............] - ETA: 3:57 - loss: 0.6843 - acc: 0.5555
3328/5677 [================>.............] - ETA: 3:50 - loss: 0.6838 - acc: 0.5565
3392/5677 [================>.............] - ETA: 3:43 - loss: 0.6839 - acc: 0.5560
3456/5677 [=================>............] - ETA: 3:37 - loss: 0.6843 - acc: 0.5561
3520/5677 [=================>............] - ETA: 3:30 - loss: 0.6848 - acc: 0.5554
3584/5677 [=================>............] - ETA: 3:24 - loss: 0.6847 - acc: 0.5555
3648/5677 [==================>...........] - ETA: 3:18 - loss: 0.6848 - acc: 0.5556
3712/5677 [==================>...........] - ETA: 3:11 - loss: 0.6845 - acc: 0.5563
3776/5677 [==================>...........] - ETA: 3:04 - loss: 0.6847 - acc: 0.5564
3840/5677 [===================>..........] - ETA: 2:58 - loss: 0.6845 - acc: 0.5576
3904/5677 [===================>..........] - ETA: 2:52 - loss: 0.6846 - acc: 0.5576
3968/5677 [===================>..........] - ETA: 2:46 - loss: 0.6847 - acc: 0.5580
4032/5677 [====================>.........] - ETA: 2:39 - loss: 0.6849 - acc: 0.5575
4096/5677 [====================>.........] - ETA: 2:33 - loss: 0.6852 - acc: 0.5574
4160/5677 [====================>.........] - ETA: 2:27 - loss: 0.6848 - acc: 0.5579
4224/5677 [=====================>........] - ETA: 2:21 - loss: 0.6843 - acc: 0.5585
4288/5677 [=====================>........] - ETA: 2:14 - loss: 0.6850 - acc: 0.5569
4352/5677 [=====================>........] - ETA: 2:08 - loss: 0.6850 - acc: 0.5574
4416/5677 [======================>.......] - ETA: 2:02 - loss: 0.6851 - acc: 0.5577
4480/5677 [======================>.......] - ETA: 1:55 - loss: 0.6848 - acc: 0.5576
4544/5677 [=======================>......] - ETA: 1:49 - loss: 0.6849 - acc: 0.5581
4608/5677 [=======================>......] - ETA: 1:43 - loss: 0.6848 - acc: 0.5590
4672/5677 [=======================>......] - ETA: 1:37 - loss: 0.6849 - acc: 0.5580
4736/5677 [========================>.....] - ETA: 1:30 - loss: 0.6853 - acc: 0.5564
4800/5677 [========================>.....] - ETA: 1:24 - loss: 0.6851 - acc: 0.5565
4864/5677 [========================>.....] - ETA: 1:18 - loss: 0.6852 - acc: 0.5557
4928/5677 [=========================>....] - ETA: 1:12 - loss: 0.6855 - acc: 0.5552
4992/5677 [=========================>....] - ETA: 1:06 - loss: 0.6854 - acc: 0.5557
5056/5677 [=========================>....] - ETA: 59s - loss: 0.6856 - acc: 0.5554 
5120/5677 [==========================>...] - ETA: 53s - loss: 0.6851 - acc: 0.5549
5184/5677 [==========================>...] - ETA: 47s - loss: 0.6848 - acc: 0.5552
5248/5677 [==========================>...] - ETA: 41s - loss: 0.6846 - acc: 0.5562
5312/5677 [===========================>..] - ETA: 35s - loss: 0.6848 - acc: 0.5565
5376/5677 [===========================>..] - ETA: 29s - loss: 0.6854 - acc: 0.5554
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6858 - acc: 0.5546
5504/5677 [============================>.] - ETA: 16s - loss: 0.6856 - acc: 0.5554
5568/5677 [============================>.] - ETA: 10s - loss: 0.6859 - acc: 0.5546
5632/5677 [============================>.] - ETA: 4s - loss: 0.6858 - acc: 0.5545 
5677/5677 [==============================] - 572s 101ms/step - loss: 0.6859 - acc: 0.5542 - val_loss: 0.6697 - val_acc: 0.6054

Epoch 00004: val_acc improved from 0.57052 to 0.60539, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window07/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 5/10

  64/5677 [..............................] - ETA: 8:51 - loss: 0.6636 - acc: 0.6406
 128/5677 [..............................] - ETA: 8:22 - loss: 0.6800 - acc: 0.6016
 192/5677 [>.............................] - ETA: 9:14 - loss: 0.6650 - acc: 0.6250
 256/5677 [>.............................] - ETA: 9:01 - loss: 0.6726 - acc: 0.6055
 320/5677 [>.............................] - ETA: 8:44 - loss: 0.6802 - acc: 0.5906
 384/5677 [=>............................] - ETA: 8:43 - loss: 0.6821 - acc: 0.5859
 448/5677 [=>............................] - ETA: 8:44 - loss: 0.6812 - acc: 0.5915
 512/5677 [=>............................] - ETA: 8:30 - loss: 0.6823 - acc: 0.5781
 576/5677 [==>...........................] - ETA: 8:19 - loss: 0.6774 - acc: 0.5920
 640/5677 [==>...........................] - ETA: 8:08 - loss: 0.6799 - acc: 0.5828
 704/5677 [==>...........................] - ETA: 8:05 - loss: 0.6804 - acc: 0.5838
 768/5677 [===>..........................] - ETA: 8:05 - loss: 0.6785 - acc: 0.5846
 832/5677 [===>..........................] - ETA: 7:58 - loss: 0.6777 - acc: 0.5913
 896/5677 [===>..........................] - ETA: 7:51 - loss: 0.6765 - acc: 0.5904
 960/5677 [====>.........................] - ETA: 7:42 - loss: 0.6773 - acc: 0.5865
1024/5677 [====>.........................] - ETA: 7:35 - loss: 0.6790 - acc: 0.5850
1088/5677 [====>.........................] - ETA: 7:29 - loss: 0.6801 - acc: 0.5790
1152/5677 [=====>........................] - ETA: 7:24 - loss: 0.6785 - acc: 0.5807
1216/5677 [=====>........................] - ETA: 7:20 - loss: 0.6808 - acc: 0.5781
1280/5677 [=====>........................] - ETA: 7:16 - loss: 0.6827 - acc: 0.5758
1344/5677 [======>.......................] - ETA: 7:10 - loss: 0.6809 - acc: 0.5818
1408/5677 [======>.......................] - ETA: 7:01 - loss: 0.6805 - acc: 0.5817
1472/5677 [======>.......................] - ETA: 6:53 - loss: 0.6801 - acc: 0.5849
1536/5677 [=======>......................] - ETA: 6:45 - loss: 0.6806 - acc: 0.5827
1600/5677 [=======>......................] - ETA: 6:37 - loss: 0.6794 - acc: 0.5850
1664/5677 [=======>......................] - ETA: 6:31 - loss: 0.6781 - acc: 0.5847
1728/5677 [========>.....................] - ETA: 6:26 - loss: 0.6797 - acc: 0.5804
1792/5677 [========>.....................] - ETA: 6:21 - loss: 0.6788 - acc: 0.5826
1856/5677 [========>.....................] - ETA: 6:15 - loss: 0.6789 - acc: 0.5808
1920/5677 [=========>....................] - ETA: 6:08 - loss: 0.6793 - acc: 0.5802
1984/5677 [=========>....................] - ETA: 6:01 - loss: 0.6806 - acc: 0.5771
2048/5677 [=========>....................] - ETA: 5:53 - loss: 0.6827 - acc: 0.5742
2112/5677 [==========>...................] - ETA: 5:46 - loss: 0.6823 - acc: 0.5767
2176/5677 [==========>...................] - ETA: 5:41 - loss: 0.6837 - acc: 0.5749
2240/5677 [==========>...................] - ETA: 5:36 - loss: 0.6845 - acc: 0.5741
2304/5677 [===========>..................] - ETA: 5:29 - loss: 0.6851 - acc: 0.5707
2368/5677 [===========>..................] - ETA: 5:23 - loss: 0.6853 - acc: 0.5688
2432/5677 [===========>..................] - ETA: 5:16 - loss: 0.6840 - acc: 0.5707
2496/5677 [============>.................] - ETA: 5:10 - loss: 0.6837 - acc: 0.5697
2560/5677 [============>.................] - ETA: 5:04 - loss: 0.6830 - acc: 0.5711
2624/5677 [============>.................] - ETA: 4:58 - loss: 0.6827 - acc: 0.5728
2688/5677 [=============>................] - ETA: 4:53 - loss: 0.6835 - acc: 0.5707
2752/5677 [=============>................] - ETA: 4:47 - loss: 0.6833 - acc: 0.5709
2816/5677 [=============>................] - ETA: 4:41 - loss: 0.6834 - acc: 0.5710
2880/5677 [==============>...............] - ETA: 4:35 - loss: 0.6834 - acc: 0.5705
2944/5677 [==============>...............] - ETA: 4:29 - loss: 0.6842 - acc: 0.5683
3008/5677 [==============>...............] - ETA: 4:23 - loss: 0.6845 - acc: 0.5665
3072/5677 [===============>..............] - ETA: 4:16 - loss: 0.6848 - acc: 0.5661
3136/5677 [===============>..............] - ETA: 4:10 - loss: 0.6850 - acc: 0.5654
3200/5677 [===============>..............] - ETA: 4:04 - loss: 0.6854 - acc: 0.5653
3264/5677 [================>.............] - ETA: 3:58 - loss: 0.6847 - acc: 0.5677
3328/5677 [================>.............] - ETA: 3:51 - loss: 0.6842 - acc: 0.5682
3392/5677 [================>.............] - ETA: 3:45 - loss: 0.6837 - acc: 0.5684
3456/5677 [=================>............] - ETA: 3:38 - loss: 0.6842 - acc: 0.5663
3520/5677 [=================>............] - ETA: 3:32 - loss: 0.6844 - acc: 0.5651
3584/5677 [=================>............] - ETA: 3:26 - loss: 0.6851 - acc: 0.5645
3648/5677 [==================>...........] - ETA: 3:20 - loss: 0.6849 - acc: 0.5639
3712/5677 [==================>...........] - ETA: 3:13 - loss: 0.6845 - acc: 0.5647
3776/5677 [==================>...........] - ETA: 3:07 - loss: 0.6845 - acc: 0.5644
3840/5677 [===================>..........] - ETA: 3:01 - loss: 0.6844 - acc: 0.5646
3904/5677 [===================>..........] - ETA: 2:55 - loss: 0.6845 - acc: 0.5653
3968/5677 [===================>..........] - ETA: 2:49 - loss: 0.6843 - acc: 0.5655
4032/5677 [====================>.........] - ETA: 2:43 - loss: 0.6840 - acc: 0.5657
4096/5677 [====================>.........] - ETA: 2:37 - loss: 0.6840 - acc: 0.5652
4160/5677 [====================>.........] - ETA: 2:31 - loss: 0.6840 - acc: 0.5644
4224/5677 [=====================>........] - ETA: 2:24 - loss: 0.6832 - acc: 0.5653
4288/5677 [=====================>........] - ETA: 2:18 - loss: 0.6826 - acc: 0.5662
4352/5677 [=====================>........] - ETA: 2:12 - loss: 0.6825 - acc: 0.5662
4416/5677 [======================>.......] - ETA: 2:05 - loss: 0.6823 - acc: 0.5659
4480/5677 [======================>.......] - ETA: 1:59 - loss: 0.6818 - acc: 0.5667
4544/5677 [=======================>......] - ETA: 1:52 - loss: 0.6813 - acc: 0.5678
4608/5677 [=======================>......] - ETA: 1:46 - loss: 0.6815 - acc: 0.5666
4672/5677 [=======================>......] - ETA: 1:39 - loss: 0.6815 - acc: 0.5668
4736/5677 [========================>.....] - ETA: 1:33 - loss: 0.6818 - acc: 0.5657
4800/5677 [========================>.....] - ETA: 1:26 - loss: 0.6821 - acc: 0.5644
4864/5677 [========================>.....] - ETA: 1:20 - loss: 0.6818 - acc: 0.5648
4928/5677 [=========================>....] - ETA: 1:14 - loss: 0.6820 - acc: 0.5639
4992/5677 [=========================>....] - ETA: 1:07 - loss: 0.6825 - acc: 0.5637
5056/5677 [=========================>....] - ETA: 1:01 - loss: 0.6824 - acc: 0.5641
5120/5677 [==========================>...] - ETA: 54s - loss: 0.6829 - acc: 0.5635 
5184/5677 [==========================>...] - ETA: 48s - loss: 0.6826 - acc: 0.5640
5248/5677 [==========================>...] - ETA: 42s - loss: 0.6825 - acc: 0.5638
5312/5677 [===========================>..] - ETA: 35s - loss: 0.6827 - acc: 0.5638
5376/5677 [===========================>..] - ETA: 29s - loss: 0.6825 - acc: 0.5638
5440/5677 [===========================>..] - ETA: 23s - loss: 0.6826 - acc: 0.5640
5504/5677 [============================>.] - ETA: 17s - loss: 0.6827 - acc: 0.5643
5568/5677 [============================>.] - ETA: 10s - loss: 0.6827 - acc: 0.5647
5632/5677 [============================>.] - ETA: 4s - loss: 0.6828 - acc: 0.5645 
5677/5677 [==============================] - 579s 102ms/step - loss: 0.6827 - acc: 0.5649 - val_loss: 0.6689 - val_acc: 0.6165

Epoch 00005: val_acc improved from 0.60539 to 0.61648, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window07/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 8:13 - loss: 0.7437 - acc: 0.4531
 128/5677 [..............................] - ETA: 8:24 - loss: 0.7173 - acc: 0.5391
 192/5677 [>.............................] - ETA: 8:25 - loss: 0.6939 - acc: 0.5729
 256/5677 [>.............................] - ETA: 8:20 - loss: 0.6956 - acc: 0.5664
 320/5677 [>.............................] - ETA: 8:10 - loss: 0.6830 - acc: 0.5844
 384/5677 [=>............................] - ETA: 8:07 - loss: 0.6720 - acc: 0.6016
 448/5677 [=>............................] - ETA: 7:59 - loss: 0.6755 - acc: 0.5938
 512/5677 [=>............................] - ETA: 7:55 - loss: 0.6750 - acc: 0.5879
 576/5677 [==>...........................] - ETA: 7:46 - loss: 0.6783 - acc: 0.5833
 640/5677 [==>...........................] - ETA: 7:31 - loss: 0.6773 - acc: 0.5844
 704/5677 [==>...........................] - ETA: 7:24 - loss: 0.6803 - acc: 0.5795
 768/5677 [===>..........................] - ETA: 7:14 - loss: 0.6794 - acc: 0.5807
 832/5677 [===>..........................] - ETA: 7:01 - loss: 0.6798 - acc: 0.5805
 896/5677 [===>..........................] - ETA: 6:51 - loss: 0.6786 - acc: 0.5815
 960/5677 [====>.........................] - ETA: 6:41 - loss: 0.6809 - acc: 0.5740
1024/5677 [====>.........................] - ETA: 6:33 - loss: 0.6794 - acc: 0.5791
1088/5677 [====>.........................] - ETA: 6:30 - loss: 0.6774 - acc: 0.5873
1152/5677 [=====>........................] - ETA: 6:21 - loss: 0.6791 - acc: 0.5825
1216/5677 [=====>........................] - ETA: 6:16 - loss: 0.6777 - acc: 0.5855
1280/5677 [=====>........................] - ETA: 6:10 - loss: 0.6789 - acc: 0.5797
1344/5677 [======>.......................] - ETA: 6:04 - loss: 0.6787 - acc: 0.5789
1408/5677 [======>.......................] - ETA: 5:57 - loss: 0.6797 - acc: 0.5746
1472/5677 [======>.......................] - ETA: 5:53 - loss: 0.6794 - acc: 0.5761
1536/5677 [=======>......................] - ETA: 5:49 - loss: 0.6791 - acc: 0.5775
1600/5677 [=======>......................] - ETA: 5:44 - loss: 0.6806 - acc: 0.5744
1664/5677 [=======>......................] - ETA: 5:36 - loss: 0.6828 - acc: 0.5709
1728/5677 [========>.....................] - ETA: 5:31 - loss: 0.6843 - acc: 0.5666
1792/5677 [========>.....................] - ETA: 5:25 - loss: 0.6845 - acc: 0.5653
1856/5677 [========>.....................] - ETA: 5:19 - loss: 0.6835 - acc: 0.5673
1920/5677 [=========>....................] - ETA: 5:13 - loss: 0.6832 - acc: 0.5687
1984/5677 [=========>....................] - ETA: 5:08 - loss: 0.6829 - acc: 0.5711
2048/5677 [=========>....................] - ETA: 5:02 - loss: 0.6827 - acc: 0.5698
2112/5677 [==========>...................] - ETA: 4:56 - loss: 0.6832 - acc: 0.5668
2176/5677 [==========>...................] - ETA: 4:50 - loss: 0.6828 - acc: 0.5671
2240/5677 [==========>...................] - ETA: 4:45 - loss: 0.6823 - acc: 0.5670
2304/5677 [===========>..................] - ETA: 4:39 - loss: 0.6828 - acc: 0.5668
2368/5677 [===========>..................] - ETA: 4:34 - loss: 0.6827 - acc: 0.5659
2432/5677 [===========>..................] - ETA: 4:29 - loss: 0.6835 - acc: 0.5650
2496/5677 [============>.................] - ETA: 4:24 - loss: 0.6842 - acc: 0.5661
2560/5677 [============>.................] - ETA: 4:19 - loss: 0.6842 - acc: 0.5660
2624/5677 [============>.................] - ETA: 4:13 - loss: 0.6836 - acc: 0.5663
2688/5677 [=============>................] - ETA: 4:08 - loss: 0.6838 - acc: 0.5651
2752/5677 [=============>................] - ETA: 4:03 - loss: 0.6837 - acc: 0.5643
2816/5677 [=============>................] - ETA: 3:58 - loss: 0.6834 - acc: 0.5650
2880/5677 [==============>...............] - ETA: 3:52 - loss: 0.6840 - acc: 0.5628
2944/5677 [==============>...............] - ETA: 3:46 - loss: 0.6842 - acc: 0.5622
3008/5677 [==============>...............] - ETA: 3:40 - loss: 0.6842 - acc: 0.5615
3072/5677 [===============>..............] - ETA: 3:35 - loss: 0.6844 - acc: 0.5602
3136/5677 [===============>..............] - ETA: 3:29 - loss: 0.6842 - acc: 0.5615
3200/5677 [===============>..............] - ETA: 3:24 - loss: 0.6836 - acc: 0.5631
3264/5677 [================>.............] - ETA: 3:19 - loss: 0.6835 - acc: 0.5640
3328/5677 [================>.............] - ETA: 3:14 - loss: 0.6835 - acc: 0.5628
3392/5677 [================>.............] - ETA: 3:08 - loss: 0.6834 - acc: 0.5640
3456/5677 [=================>............] - ETA: 3:02 - loss: 0.6839 - acc: 0.5619
3520/5677 [=================>............] - ETA: 2:57 - loss: 0.6836 - acc: 0.5628
3584/5677 [=================>............] - ETA: 2:52 - loss: 0.6845 - acc: 0.5594
3648/5677 [==================>...........] - ETA: 2:46 - loss: 0.6844 - acc: 0.5592
3712/5677 [==================>...........] - ETA: 2:41 - loss: 0.6835 - acc: 0.5606
3776/5677 [==================>...........] - ETA: 2:36 - loss: 0.6841 - acc: 0.5585
3840/5677 [===================>..........] - ETA: 2:31 - loss: 0.6840 - acc: 0.5589
3904/5677 [===================>..........] - ETA: 2:26 - loss: 0.6840 - acc: 0.5589
3968/5677 [===================>..........] - ETA: 2:20 - loss: 0.6838 - acc: 0.5595
4032/5677 [====================>.........] - ETA: 2:15 - loss: 0.6838 - acc: 0.5593
4096/5677 [====================>.........] - ETA: 2:10 - loss: 0.6833 - acc: 0.5605
4160/5677 [====================>.........] - ETA: 2:04 - loss: 0.6834 - acc: 0.5611
4224/5677 [=====================>........] - ETA: 1:59 - loss: 0.6839 - acc: 0.5601
4288/5677 [=====================>........] - ETA: 1:54 - loss: 0.6837 - acc: 0.5599
4352/5677 [=====================>........] - ETA: 1:49 - loss: 0.6839 - acc: 0.5607
4416/5677 [======================>.......] - ETA: 1:43 - loss: 0.6846 - acc: 0.5591
4480/5677 [======================>.......] - ETA: 1:38 - loss: 0.6838 - acc: 0.5605
4544/5677 [=======================>......] - ETA: 1:33 - loss: 0.6833 - acc: 0.5612
4608/5677 [=======================>......] - ETA: 1:27 - loss: 0.6833 - acc: 0.5612
4672/5677 [=======================>......] - ETA: 1:22 - loss: 0.6828 - acc: 0.5629
4736/5677 [========================>.....] - ETA: 1:17 - loss: 0.6816 - acc: 0.5655
4800/5677 [========================>.....] - ETA: 1:12 - loss: 0.6811 - acc: 0.5663
4864/5677 [========================>.....] - ETA: 1:06 - loss: 0.6808 - acc: 0.5664
4928/5677 [=========================>....] - ETA: 1:01 - loss: 0.6810 - acc: 0.5666
4992/5677 [=========================>....] - ETA: 56s - loss: 0.6808 - acc: 0.5675 
5056/5677 [=========================>....] - ETA: 51s - loss: 0.6807 - acc: 0.5676
5120/5677 [==========================>...] - ETA: 45s - loss: 0.6810 - acc: 0.5676
5184/5677 [==========================>...] - ETA: 40s - loss: 0.6807 - acc: 0.5677
5248/5677 [==========================>...] - ETA: 35s - loss: 0.6816 - acc: 0.5665
5312/5677 [===========================>..] - ETA: 29s - loss: 0.6819 - acc: 0.5665
5376/5677 [===========================>..] - ETA: 24s - loss: 0.6822 - acc: 0.5658
5440/5677 [===========================>..] - ETA: 19s - loss: 0.6824 - acc: 0.5647
5504/5677 [============================>.] - ETA: 14s - loss: 0.6830 - acc: 0.5640
5568/5677 [============================>.] - ETA: 8s - loss: 0.6829 - acc: 0.5643 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6827 - acc: 0.5652
5677/5677 [==============================] - 485s 85ms/step - loss: 0.6828 - acc: 0.5654 - val_loss: 0.6765 - val_acc: 0.5689

Epoch 00006: val_acc did not improve from 0.61648
Epoch 7/10

  64/5677 [..............................] - ETA: 8:07 - loss: 0.6888 - acc: 0.5625
 128/5677 [..............................] - ETA: 7:45 - loss: 0.7039 - acc: 0.5234
 192/5677 [>.............................] - ETA: 7:51 - loss: 0.7079 - acc: 0.5156
 256/5677 [>.............................] - ETA: 7:47 - loss: 0.7043 - acc: 0.5195
 320/5677 [>.............................] - ETA: 7:35 - loss: 0.6991 - acc: 0.5250
 384/5677 [=>............................] - ETA: 7:36 - loss: 0.6925 - acc: 0.5312
 448/5677 [=>............................] - ETA: 7:42 - loss: 0.6905 - acc: 0.5357
 512/5677 [=>............................] - ETA: 7:52 - loss: 0.6937 - acc: 0.5215
 576/5677 [==>...........................] - ETA: 7:41 - loss: 0.6887 - acc: 0.5312
 640/5677 [==>...........................] - ETA: 7:35 - loss: 0.6861 - acc: 0.5437
 704/5677 [==>...........................] - ETA: 7:34 - loss: 0.6862 - acc: 0.5412
 768/5677 [===>..........................] - ETA: 7:24 - loss: 0.6898 - acc: 0.5339
 832/5677 [===>..........................] - ETA: 7:23 - loss: 0.6853 - acc: 0.5457
 896/5677 [===>..........................] - ETA: 7:16 - loss: 0.6872 - acc: 0.5446
 960/5677 [====>.........................] - ETA: 7:13 - loss: 0.6848 - acc: 0.5490
1024/5677 [====>.........................] - ETA: 7:05 - loss: 0.6855 - acc: 0.5459
1088/5677 [====>.........................] - ETA: 6:55 - loss: 0.6837 - acc: 0.5524
1152/5677 [=====>........................] - ETA: 6:48 - loss: 0.6836 - acc: 0.5538
1216/5677 [=====>........................] - ETA: 6:47 - loss: 0.6862 - acc: 0.5477
1280/5677 [=====>........................] - ETA: 6:42 - loss: 0.6851 - acc: 0.5508
1344/5677 [======>.......................] - ETA: 6:34 - loss: 0.6855 - acc: 0.5484
1408/5677 [======>.......................] - ETA: 6:28 - loss: 0.6851 - acc: 0.5476
1472/5677 [======>.......................] - ETA: 6:22 - loss: 0.6854 - acc: 0.5448
1536/5677 [=======>......................] - ETA: 6:16 - loss: 0.6848 - acc: 0.5469
1600/5677 [=======>......................] - ETA: 6:08 - loss: 0.6842 - acc: 0.5475
1664/5677 [=======>......................] - ETA: 6:01 - loss: 0.6839 - acc: 0.5481
1728/5677 [========>.....................] - ETA: 5:55 - loss: 0.6827 - acc: 0.5503
1792/5677 [========>.....................] - ETA: 5:52 - loss: 0.6832 - acc: 0.5458
1856/5677 [========>.....................] - ETA: 5:46 - loss: 0.6837 - acc: 0.5469
1920/5677 [=========>....................] - ETA: 5:40 - loss: 0.6832 - acc: 0.5490
1984/5677 [=========>....................] - ETA: 5:33 - loss: 0.6829 - acc: 0.5484
2048/5677 [=========>....................] - ETA: 5:26 - loss: 0.6824 - acc: 0.5503
2112/5677 [==========>...................] - ETA: 5:20 - loss: 0.6814 - acc: 0.5521
2176/5677 [==========>...................] - ETA: 5:15 - loss: 0.6810 - acc: 0.5538
2240/5677 [==========>...................] - ETA: 5:10 - loss: 0.6817 - acc: 0.5536
2304/5677 [===========>..................] - ETA: 5:05 - loss: 0.6822 - acc: 0.5516
2368/5677 [===========>..................] - ETA: 4:59 - loss: 0.6823 - acc: 0.5507
2432/5677 [===========>..................] - ETA: 4:54 - loss: 0.6828 - acc: 0.5522
2496/5677 [============>.................] - ETA: 4:46 - loss: 0.6827 - acc: 0.5521
2560/5677 [============>.................] - ETA: 4:40 - loss: 0.6821 - acc: 0.5539
2624/5677 [============>.................] - ETA: 4:33 - loss: 0.6821 - acc: 0.5537
2688/5677 [=============>................] - ETA: 4:28 - loss: 0.6825 - acc: 0.5528
2752/5677 [=============>................] - ETA: 4:22 - loss: 0.6824 - acc: 0.5520
2816/5677 [=============>................] - ETA: 4:17 - loss: 0.6829 - acc: 0.5490
2880/5677 [==============>...............] - ETA: 4:11 - loss: 0.6832 - acc: 0.5486
2944/5677 [==============>...............] - ETA: 4:05 - loss: 0.6839 - acc: 0.5482
3008/5677 [==============>...............] - ETA: 3:59 - loss: 0.6838 - acc: 0.5482
3072/5677 [===============>..............] - ETA: 3:53 - loss: 0.6837 - acc: 0.5485
3136/5677 [===============>..............] - ETA: 3:47 - loss: 0.6834 - acc: 0.5491
3200/5677 [===============>..............] - ETA: 3:41 - loss: 0.6830 - acc: 0.5503
3264/5677 [================>.............] - ETA: 3:35 - loss: 0.6824 - acc: 0.5518
3328/5677 [================>.............] - ETA: 3:30 - loss: 0.6827 - acc: 0.5520
3392/5677 [================>.............] - ETA: 3:24 - loss: 0.6823 - acc: 0.5525
3456/5677 [=================>............] - ETA: 3:19 - loss: 0.6830 - acc: 0.5509
3520/5677 [=================>............] - ETA: 3:14 - loss: 0.6826 - acc: 0.5517
3584/5677 [=================>............] - ETA: 3:08 - loss: 0.6825 - acc: 0.5519
3648/5677 [==================>...........] - ETA: 3:01 - loss: 0.6826 - acc: 0.5518
3712/5677 [==================>...........] - ETA: 2:55 - loss: 0.6825 - acc: 0.5531
3776/5677 [==================>...........] - ETA: 2:49 - loss: 0.6820 - acc: 0.5546
3840/5677 [===================>..........] - ETA: 2:43 - loss: 0.6812 - acc: 0.5568
3904/5677 [===================>..........] - ETA: 2:37 - loss: 0.6808 - acc: 0.5581
3968/5677 [===================>..........] - ETA: 2:32 - loss: 0.6806 - acc: 0.5587
4032/5677 [====================>.........] - ETA: 2:26 - loss: 0.6807 - acc: 0.5593
4096/5677 [====================>.........] - ETA: 2:21 - loss: 0.6805 - acc: 0.5596
4160/5677 [====================>.........] - ETA: 2:15 - loss: 0.6808 - acc: 0.5589
4224/5677 [=====================>........] - ETA: 2:10 - loss: 0.6801 - acc: 0.5604
4288/5677 [=====================>........] - ETA: 2:04 - loss: 0.6802 - acc: 0.5609
4352/5677 [=====================>........] - ETA: 1:58 - loss: 0.6800 - acc: 0.5620
4416/5677 [======================>.......] - ETA: 1:53 - loss: 0.6806 - acc: 0.5605
4480/5677 [======================>.......] - ETA: 1:47 - loss: 0.6800 - acc: 0.5612
4544/5677 [=======================>......] - ETA: 1:41 - loss: 0.6802 - acc: 0.5614
4608/5677 [=======================>......] - ETA: 1:35 - loss: 0.6810 - acc: 0.5601
4672/5677 [=======================>......] - ETA: 1:29 - loss: 0.6811 - acc: 0.5597
4736/5677 [========================>.....] - ETA: 1:23 - loss: 0.6809 - acc: 0.5614
4800/5677 [========================>.....] - ETA: 1:17 - loss: 0.6804 - acc: 0.5627
4864/5677 [========================>.....] - ETA: 1:12 - loss: 0.6804 - acc: 0.5635
4928/5677 [=========================>....] - ETA: 1:06 - loss: 0.6810 - acc: 0.5635
4992/5677 [=========================>....] - ETA: 1:00 - loss: 0.6807 - acc: 0.5647
5056/5677 [=========================>....] - ETA: 55s - loss: 0.6808 - acc: 0.5647 
5120/5677 [==========================>...] - ETA: 49s - loss: 0.6808 - acc: 0.5643
5184/5677 [==========================>...] - ETA: 43s - loss: 0.6810 - acc: 0.5635
5248/5677 [==========================>...] - ETA: 38s - loss: 0.6811 - acc: 0.5627
5312/5677 [===========================>..] - ETA: 32s - loss: 0.6807 - acc: 0.5634
5376/5677 [===========================>..] - ETA: 26s - loss: 0.6808 - acc: 0.5636
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6802 - acc: 0.5649
5504/5677 [============================>.] - ETA: 15s - loss: 0.6799 - acc: 0.5652
5568/5677 [============================>.] - ETA: 9s - loss: 0.6797 - acc: 0.5661 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6793 - acc: 0.5669
5677/5677 [==============================] - 524s 92ms/step - loss: 0.6793 - acc: 0.5672 - val_loss: 0.6736 - val_acc: 0.5689

Epoch 00007: val_acc did not improve from 0.61648
Epoch 8/10

  64/5677 [..............................] - ETA: 8:13 - loss: 0.6508 - acc: 0.6250
 128/5677 [..............................] - ETA: 8:44 - loss: 0.6646 - acc: 0.6016
 192/5677 [>.............................] - ETA: 8:45 - loss: 0.6658 - acc: 0.6094
 256/5677 [>.............................] - ETA: 8:44 - loss: 0.6734 - acc: 0.6055
 320/5677 [>.............................] - ETA: 8:36 - loss: 0.6712 - acc: 0.6000
 384/5677 [=>............................] - ETA: 8:31 - loss: 0.6756 - acc: 0.5938
 448/5677 [=>............................] - ETA: 8:18 - loss: 0.6807 - acc: 0.5871
 512/5677 [=>............................] - ETA: 8:13 - loss: 0.6751 - acc: 0.5918
 576/5677 [==>...........................] - ETA: 8:07 - loss: 0.6727 - acc: 0.5955
 640/5677 [==>...........................] - ETA: 8:05 - loss: 0.6720 - acc: 0.6031
 704/5677 [==>...........................] - ETA: 8:00 - loss: 0.6778 - acc: 0.5909
 768/5677 [===>..........................] - ETA: 8:00 - loss: 0.6786 - acc: 0.5898
 832/5677 [===>..........................] - ETA: 7:54 - loss: 0.6829 - acc: 0.5829
 896/5677 [===>..........................] - ETA: 7:44 - loss: 0.6851 - acc: 0.5737
 960/5677 [====>.........................] - ETA: 7:38 - loss: 0.6840 - acc: 0.5729
1024/5677 [====>.........................] - ETA: 7:30 - loss: 0.6822 - acc: 0.5752
1088/5677 [====>.........................] - ETA: 7:27 - loss: 0.6805 - acc: 0.5781
1152/5677 [=====>........................] - ETA: 7:23 - loss: 0.6796 - acc: 0.5799
1216/5677 [=====>........................] - ETA: 7:14 - loss: 0.6783 - acc: 0.5798
1280/5677 [=====>........................] - ETA: 7:12 - loss: 0.6785 - acc: 0.5781
1344/5677 [======>.......................] - ETA: 7:04 - loss: 0.6776 - acc: 0.5789
1408/5677 [======>.......................] - ETA: 6:57 - loss: 0.6780 - acc: 0.5746
1472/5677 [======>.......................] - ETA: 6:49 - loss: 0.6778 - acc: 0.5768
1536/5677 [=======>......................] - ETA: 6:41 - loss: 0.6786 - acc: 0.5749
1600/5677 [=======>......................] - ETA: 6:31 - loss: 0.6798 - acc: 0.5719
1664/5677 [=======>......................] - ETA: 6:23 - loss: 0.6793 - acc: 0.5733
1728/5677 [========>.....................] - ETA: 6:16 - loss: 0.6776 - acc: 0.5764
1792/5677 [========>.....................] - ETA: 6:08 - loss: 0.6780 - acc: 0.5759
1856/5677 [========>.....................] - ETA: 6:01 - loss: 0.6779 - acc: 0.5760
1920/5677 [=========>....................] - ETA: 5:55 - loss: 0.6773 - acc: 0.5776
1984/5677 [=========>....................] - ETA: 5:50 - loss: 0.6764 - acc: 0.5766
2048/5677 [=========>....................] - ETA: 5:43 - loss: 0.6757 - acc: 0.5771
2112/5677 [==========>...................] - ETA: 5:37 - loss: 0.6760 - acc: 0.5767
2176/5677 [==========>...................] - ETA: 5:30 - loss: 0.6757 - acc: 0.5767
2240/5677 [==========>...................] - ETA: 5:23 - loss: 0.6768 - acc: 0.5754
2304/5677 [===========>..................] - ETA: 5:16 - loss: 0.6763 - acc: 0.5768
2368/5677 [===========>..................] - ETA: 5:10 - loss: 0.6776 - acc: 0.5718
2432/5677 [===========>..................] - ETA: 5:03 - loss: 0.6781 - acc: 0.5699
2496/5677 [============>.................] - ETA: 4:56 - loss: 0.6779 - acc: 0.5709
2560/5677 [============>.................] - ETA: 4:51 - loss: 0.6778 - acc: 0.5703
2624/5677 [============>.................] - ETA: 4:45 - loss: 0.6780 - acc: 0.5701
2688/5677 [=============>................] - ETA: 4:39 - loss: 0.6787 - acc: 0.5696
2752/5677 [=============>................] - ETA: 4:33 - loss: 0.6791 - acc: 0.5690
2816/5677 [=============>................] - ETA: 4:27 - loss: 0.6798 - acc: 0.5671
2880/5677 [==============>...............] - ETA: 4:21 - loss: 0.6801 - acc: 0.5670
2944/5677 [==============>...............] - ETA: 4:16 - loss: 0.6803 - acc: 0.5652
3008/5677 [==============>...............] - ETA: 4:09 - loss: 0.6805 - acc: 0.5655
3072/5677 [===============>..............] - ETA: 4:02 - loss: 0.6802 - acc: 0.5651
3136/5677 [===============>..............] - ETA: 3:56 - loss: 0.6800 - acc: 0.5644
3200/5677 [===============>..............] - ETA: 3:49 - loss: 0.6796 - acc: 0.5653
3264/5677 [================>.............] - ETA: 3:43 - loss: 0.6791 - acc: 0.5668
3328/5677 [================>.............] - ETA: 3:38 - loss: 0.6794 - acc: 0.5661
3392/5677 [================>.............] - ETA: 3:32 - loss: 0.6785 - acc: 0.5678
3456/5677 [=================>............] - ETA: 3:26 - loss: 0.6779 - acc: 0.5686
3520/5677 [=================>............] - ETA: 3:20 - loss: 0.6780 - acc: 0.5673
3584/5677 [=================>............] - ETA: 3:14 - loss: 0.6786 - acc: 0.5664
3648/5677 [==================>...........] - ETA: 3:08 - loss: 0.6783 - acc: 0.5677
3712/5677 [==================>...........] - ETA: 3:02 - loss: 0.6781 - acc: 0.5668
3776/5677 [==================>...........] - ETA: 2:56 - loss: 0.6786 - acc: 0.5659
3840/5677 [===================>..........] - ETA: 2:50 - loss: 0.6786 - acc: 0.5661
3904/5677 [===================>..........] - ETA: 2:43 - loss: 0.6780 - acc: 0.5676
3968/5677 [===================>..........] - ETA: 2:37 - loss: 0.6778 - acc: 0.5683
4032/5677 [====================>.........] - ETA: 2:31 - loss: 0.6779 - acc: 0.5672
4096/5677 [====================>.........] - ETA: 2:26 - loss: 0.6784 - acc: 0.5659
4160/5677 [====================>.........] - ETA: 2:20 - loss: 0.6788 - acc: 0.5661
4224/5677 [=====================>........] - ETA: 2:14 - loss: 0.6787 - acc: 0.5670
4288/5677 [=====================>........] - ETA: 2:09 - loss: 0.6790 - acc: 0.5662
4352/5677 [=====================>........] - ETA: 2:03 - loss: 0.6793 - acc: 0.5657
4416/5677 [======================>.......] - ETA: 1:57 - loss: 0.6791 - acc: 0.5654
4480/5677 [======================>.......] - ETA: 1:50 - loss: 0.6795 - acc: 0.5647
4544/5677 [=======================>......] - ETA: 1:45 - loss: 0.6796 - acc: 0.5638
4608/5677 [=======================>......] - ETA: 1:38 - loss: 0.6794 - acc: 0.5640
4672/5677 [=======================>......] - ETA: 1:32 - loss: 0.6791 - acc: 0.5646
4736/5677 [========================>.....] - ETA: 1:27 - loss: 0.6794 - acc: 0.5642
4800/5677 [========================>.....] - ETA: 1:21 - loss: 0.6795 - acc: 0.5640
4864/5677 [========================>.....] - ETA: 1:15 - loss: 0.6792 - acc: 0.5650
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.6793 - acc: 0.5651
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6795 - acc: 0.5639
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6796 - acc: 0.5643 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6793 - acc: 0.5654
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6787 - acc: 0.5667
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6786 - acc: 0.5673
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6787 - acc: 0.5672
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6787 - acc: 0.5675
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6787 - acc: 0.5675
5504/5677 [============================>.] - ETA: 15s - loss: 0.6792 - acc: 0.5663
5568/5677 [============================>.] - ETA: 9s - loss: 0.6794 - acc: 0.5657 
5632/5677 [============================>.] - ETA: 4s - loss: 0.6791 - acc: 0.5662
5677/5677 [==============================] - 540s 95ms/step - loss: 0.6787 - acc: 0.5668 - val_loss: 0.6757 - val_acc: 0.5832

Epoch 00008: val_acc did not improve from 0.61648
Epoch 9/10

  64/5677 [..............................] - ETA: 6:55 - loss: 0.6452 - acc: 0.5625
 128/5677 [..............................] - ETA: 6:57 - loss: 0.6617 - acc: 0.5859
 192/5677 [>.............................] - ETA: 6:44 - loss: 0.6651 - acc: 0.5885
 256/5677 [>.............................] - ETA: 6:40 - loss: 0.6623 - acc: 0.6016
 320/5677 [>.............................] - ETA: 7:00 - loss: 0.6657 - acc: 0.5906
 384/5677 [=>............................] - ETA: 7:01 - loss: 0.6663 - acc: 0.5964
 448/5677 [=>............................] - ETA: 7:01 - loss: 0.6642 - acc: 0.5982
 512/5677 [=>............................] - ETA: 6:58 - loss: 0.6685 - acc: 0.6074
 576/5677 [==>...........................] - ETA: 6:53 - loss: 0.6706 - acc: 0.5938
 640/5677 [==>...........................] - ETA: 6:49 - loss: 0.6725 - acc: 0.5891
 704/5677 [==>...........................] - ETA: 6:51 - loss: 0.6751 - acc: 0.5838
 768/5677 [===>..........................] - ETA: 6:47 - loss: 0.6757 - acc: 0.5846
 832/5677 [===>..........................] - ETA: 6:40 - loss: 0.6771 - acc: 0.5817
 896/5677 [===>..........................] - ETA: 6:37 - loss: 0.6757 - acc: 0.5859
 960/5677 [====>.........................] - ETA: 6:34 - loss: 0.6774 - acc: 0.5792
1024/5677 [====>.........................] - ETA: 6:33 - loss: 0.6770 - acc: 0.5801
1088/5677 [====>.........................] - ETA: 6:27 - loss: 0.6761 - acc: 0.5809
1152/5677 [=====>........................] - ETA: 6:25 - loss: 0.6743 - acc: 0.5833
1216/5677 [=====>........................] - ETA: 6:21 - loss: 0.6749 - acc: 0.5822
1280/5677 [=====>........................] - ETA: 6:19 - loss: 0.6750 - acc: 0.5820
1344/5677 [======>.......................] - ETA: 6:15 - loss: 0.6743 - acc: 0.5841
1408/5677 [======>.......................] - ETA: 6:11 - loss: 0.6741 - acc: 0.5838
1472/5677 [======>.......................] - ETA: 6:08 - loss: 0.6740 - acc: 0.5842
1536/5677 [=======>......................] - ETA: 6:04 - loss: 0.6752 - acc: 0.5814
1600/5677 [=======>......................] - ETA: 5:59 - loss: 0.6760 - acc: 0.5825
1664/5677 [=======>......................] - ETA: 5:57 - loss: 0.6761 - acc: 0.5811
1728/5677 [========>.....................] - ETA: 5:52 - loss: 0.6772 - acc: 0.5804
1792/5677 [========>.....................] - ETA: 5:48 - loss: 0.6771 - acc: 0.5798
1856/5677 [========>.....................] - ETA: 5:44 - loss: 0.6779 - acc: 0.5781
1920/5677 [=========>....................] - ETA: 5:39 - loss: 0.6783 - acc: 0.5755
1984/5677 [=========>....................] - ETA: 5:35 - loss: 0.6772 - acc: 0.5781
2048/5677 [=========>....................] - ETA: 5:30 - loss: 0.6770 - acc: 0.5781
2112/5677 [==========>...................] - ETA: 5:25 - loss: 0.6762 - acc: 0.5791
2176/5677 [==========>...................] - ETA: 5:20 - loss: 0.6762 - acc: 0.5795
2240/5677 [==========>...................] - ETA: 5:14 - loss: 0.6760 - acc: 0.5808
2304/5677 [===========>..................] - ETA: 5:09 - loss: 0.6752 - acc: 0.5812
2368/5677 [===========>..................] - ETA: 5:03 - loss: 0.6752 - acc: 0.5815
2432/5677 [===========>..................] - ETA: 4:57 - loss: 0.6734 - acc: 0.5839
2496/5677 [============>.................] - ETA: 4:53 - loss: 0.6737 - acc: 0.5825
2560/5677 [============>.................] - ETA: 4:48 - loss: 0.6750 - acc: 0.5805
2624/5677 [============>.................] - ETA: 4:42 - loss: 0.6737 - acc: 0.5816
2688/5677 [=============>................] - ETA: 4:37 - loss: 0.6735 - acc: 0.5822
2752/5677 [=============>................] - ETA: 4:32 - loss: 0.6743 - acc: 0.5814
2816/5677 [=============>................] - ETA: 4:26 - loss: 0.6744 - acc: 0.5817
2880/5677 [==============>...............] - ETA: 4:20 - loss: 0.6737 - acc: 0.5823
2944/5677 [==============>...............] - ETA: 4:14 - loss: 0.6750 - acc: 0.5788
3008/5677 [==============>...............] - ETA: 4:10 - loss: 0.6755 - acc: 0.5788
3072/5677 [===============>..............] - ETA: 4:04 - loss: 0.6753 - acc: 0.5798
3136/5677 [===============>..............] - ETA: 3:58 - loss: 0.6749 - acc: 0.5794
3200/5677 [===============>..............] - ETA: 3:52 - loss: 0.6751 - acc: 0.5787
3264/5677 [================>.............] - ETA: 3:46 - loss: 0.6749 - acc: 0.5784
3328/5677 [================>.............] - ETA: 3:40 - loss: 0.6748 - acc: 0.5793
3392/5677 [================>.............] - ETA: 3:34 - loss: 0.6750 - acc: 0.5778
3456/5677 [=================>............] - ETA: 3:27 - loss: 0.6748 - acc: 0.5787
3520/5677 [=================>............] - ETA: 3:20 - loss: 0.6750 - acc: 0.5776
3584/5677 [=================>............] - ETA: 3:14 - loss: 0.6746 - acc: 0.5784
3648/5677 [==================>...........] - ETA: 3:08 - loss: 0.6744 - acc: 0.5789
3712/5677 [==================>...........] - ETA: 3:01 - loss: 0.6754 - acc: 0.5768
3776/5677 [==================>...........] - ETA: 2:55 - loss: 0.6751 - acc: 0.5757
3840/5677 [===================>..........] - ETA: 2:49 - loss: 0.6755 - acc: 0.5742
3904/5677 [===================>..........] - ETA: 2:43 - loss: 0.6756 - acc: 0.5738
3968/5677 [===================>..........] - ETA: 2:37 - loss: 0.6756 - acc: 0.5731
4032/5677 [====================>.........] - ETA: 2:31 - loss: 0.6751 - acc: 0.5737
4096/5677 [====================>.........] - ETA: 2:25 - loss: 0.6752 - acc: 0.5723
4160/5677 [====================>.........] - ETA: 2:19 - loss: 0.6758 - acc: 0.5709
4224/5677 [=====================>........] - ETA: 2:13 - loss: 0.6755 - acc: 0.5720
4288/5677 [=====================>........] - ETA: 2:06 - loss: 0.6752 - acc: 0.5732
4352/5677 [=====================>........] - ETA: 2:01 - loss: 0.6751 - acc: 0.5726
4416/5677 [======================>.......] - ETA: 1:55 - loss: 0.6754 - acc: 0.5722
4480/5677 [======================>.......] - ETA: 1:49 - loss: 0.6760 - acc: 0.5712
4544/5677 [=======================>......] - ETA: 1:43 - loss: 0.6764 - acc: 0.5709
4608/5677 [=======================>......] - ETA: 1:37 - loss: 0.6764 - acc: 0.5707
4672/5677 [=======================>......] - ETA: 1:32 - loss: 0.6758 - acc: 0.5715
4736/5677 [========================>.....] - ETA: 1:26 - loss: 0.6761 - acc: 0.5709
4800/5677 [========================>.....] - ETA: 1:20 - loss: 0.6763 - acc: 0.5715
4864/5677 [========================>.....] - ETA: 1:14 - loss: 0.6758 - acc: 0.5726
4928/5677 [=========================>....] - ETA: 1:08 - loss: 0.6758 - acc: 0.5716
4992/5677 [=========================>....] - ETA: 1:02 - loss: 0.6758 - acc: 0.5723
5056/5677 [=========================>....] - ETA: 56s - loss: 0.6758 - acc: 0.5728 
5120/5677 [==========================>...] - ETA: 50s - loss: 0.6755 - acc: 0.5734
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6757 - acc: 0.5733
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6755 - acc: 0.5741
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6752 - acc: 0.5747
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6751 - acc: 0.5751
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6760 - acc: 0.5741
5504/5677 [============================>.] - ETA: 15s - loss: 0.6763 - acc: 0.5741
5568/5677 [============================>.] - ETA: 9s - loss: 0.6765 - acc: 0.5740 
5632/5677 [============================>.] - ETA: 4s - loss: 0.6770 - acc: 0.5730
5677/5677 [==============================] - 542s 96ms/step - loss: 0.6777 - acc: 0.5721 - val_loss: 0.6697 - val_acc: 0.6117

Epoch 00009: val_acc did not improve from 0.61648
Epoch 10/10

  64/5677 [..............................] - ETA: 9:19 - loss: 0.6424 - acc: 0.6094
 128/5677 [..............................] - ETA: 9:07 - loss: 0.6432 - acc: 0.6094
 192/5677 [>.............................] - ETA: 8:52 - loss: 0.6579 - acc: 0.5938
 256/5677 [>.............................] - ETA: 8:28 - loss: 0.6681 - acc: 0.5742
 320/5677 [>.............................] - ETA: 8:19 - loss: 0.6778 - acc: 0.5594
 384/5677 [=>............................] - ETA: 8:05 - loss: 0.6799 - acc: 0.5443
 448/5677 [=>............................] - ETA: 7:57 - loss: 0.6873 - acc: 0.5513
 512/5677 [=>............................] - ETA: 7:51 - loss: 0.6872 - acc: 0.5488
 576/5677 [==>...........................] - ETA: 7:58 - loss: 0.6882 - acc: 0.5469
 640/5677 [==>...........................] - ETA: 7:57 - loss: 0.6836 - acc: 0.5609
 704/5677 [==>...........................] - ETA: 7:47 - loss: 0.6838 - acc: 0.5540
 768/5677 [===>..........................] - ETA: 7:42 - loss: 0.6878 - acc: 0.5365
 832/5677 [===>..........................] - ETA: 7:36 - loss: 0.6862 - acc: 0.5385
 896/5677 [===>..........................] - ETA: 7:27 - loss: 0.6843 - acc: 0.5446
 960/5677 [====>.........................] - ETA: 7:22 - loss: 0.6849 - acc: 0.5479
1024/5677 [====>.........................] - ETA: 7:14 - loss: 0.6854 - acc: 0.5469
1088/5677 [====>.........................] - ETA: 7:05 - loss: 0.6841 - acc: 0.5506
1152/5677 [=====>........................] - ETA: 6:57 - loss: 0.6831 - acc: 0.5521
1216/5677 [=====>........................] - ETA: 6:52 - loss: 0.6829 - acc: 0.5543
1280/5677 [=====>........................] - ETA: 6:51 - loss: 0.6824 - acc: 0.5531
1344/5677 [======>.......................] - ETA: 6:45 - loss: 0.6825 - acc: 0.5558
1408/5677 [======>.......................] - ETA: 6:38 - loss: 0.6810 - acc: 0.5604
1472/5677 [======>.......................] - ETA: 6:32 - loss: 0.6817 - acc: 0.5598
1536/5677 [=======>......................] - ETA: 6:26 - loss: 0.6820 - acc: 0.5599
1600/5677 [=======>......................] - ETA: 6:18 - loss: 0.6821 - acc: 0.5587
1664/5677 [=======>......................] - ETA: 6:12 - loss: 0.6825 - acc: 0.5601
1728/5677 [========>.....................] - ETA: 6:05 - loss: 0.6821 - acc: 0.5602
1792/5677 [========>.....................] - ETA: 5:57 - loss: 0.6806 - acc: 0.5664
1856/5677 [========>.....................] - ETA: 5:50 - loss: 0.6805 - acc: 0.5679
1920/5677 [=========>....................] - ETA: 5:43 - loss: 0.6816 - acc: 0.5646
1984/5677 [=========>....................] - ETA: 5:38 - loss: 0.6810 - acc: 0.5670
2048/5677 [=========>....................] - ETA: 5:33 - loss: 0.6803 - acc: 0.5698
2112/5677 [==========>...................] - ETA: 5:27 - loss: 0.6800 - acc: 0.5705
2176/5677 [==========>...................] - ETA: 5:22 - loss: 0.6808 - acc: 0.5671
2240/5677 [==========>...................] - ETA: 5:16 - loss: 0.6823 - acc: 0.5625
2304/5677 [===========>..................] - ETA: 5:10 - loss: 0.6826 - acc: 0.5625
2368/5677 [===========>..................] - ETA: 5:04 - loss: 0.6825 - acc: 0.5642
2432/5677 [===========>..................] - ETA: 4:58 - loss: 0.6829 - acc: 0.5633
2496/5677 [============>.................] - ETA: 4:51 - loss: 0.6829 - acc: 0.5629
2560/5677 [============>.................] - ETA: 4:45 - loss: 0.6829 - acc: 0.5625
2624/5677 [============>.................] - ETA: 4:40 - loss: 0.6822 - acc: 0.5659
2688/5677 [=============>................] - ETA: 4:34 - loss: 0.6829 - acc: 0.5640
2752/5677 [=============>................] - ETA: 4:29 - loss: 0.6843 - acc: 0.5607
2816/5677 [=============>................] - ETA: 4:23 - loss: 0.6839 - acc: 0.5614
2880/5677 [==============>...............] - ETA: 4:17 - loss: 0.6836 - acc: 0.5628
2944/5677 [==============>...............] - ETA: 4:11 - loss: 0.6835 - acc: 0.5628
3008/5677 [==============>...............] - ETA: 4:06 - loss: 0.6838 - acc: 0.5615
3072/5677 [===============>..............] - ETA: 4:01 - loss: 0.6835 - acc: 0.5622
3136/5677 [===============>..............] - ETA: 3:54 - loss: 0.6841 - acc: 0.5612
3200/5677 [===============>..............] - ETA: 3:49 - loss: 0.6842 - acc: 0.5616
3264/5677 [================>.............] - ETA: 3:44 - loss: 0.6842 - acc: 0.5610
3328/5677 [================>.............] - ETA: 3:38 - loss: 0.6834 - acc: 0.5625
3392/5677 [================>.............] - ETA: 3:32 - loss: 0.6830 - acc: 0.5628
3456/5677 [=================>............] - ETA: 3:26 - loss: 0.6832 - acc: 0.5616
3520/5677 [=================>............] - ETA: 3:20 - loss: 0.6830 - acc: 0.5619
3584/5677 [=================>............] - ETA: 3:14 - loss: 0.6826 - acc: 0.5628
3648/5677 [==================>...........] - ETA: 3:08 - loss: 0.6820 - acc: 0.5636
3712/5677 [==================>...........] - ETA: 3:03 - loss: 0.6816 - acc: 0.5644
3776/5677 [==================>...........] - ETA: 2:58 - loss: 0.6813 - acc: 0.5646
3840/5677 [===================>..........] - ETA: 2:52 - loss: 0.6805 - acc: 0.5664
3904/5677 [===================>..........] - ETA: 2:46 - loss: 0.6802 - acc: 0.5669
3968/5677 [===================>..........] - ETA: 2:40 - loss: 0.6808 - acc: 0.5655
4032/5677 [====================>.........] - ETA: 2:34 - loss: 0.6807 - acc: 0.5657
4096/5677 [====================>.........] - ETA: 2:28 - loss: 0.6798 - acc: 0.5679
4160/5677 [====================>.........] - ETA: 2:22 - loss: 0.6797 - acc: 0.5680
4224/5677 [=====================>........] - ETA: 2:16 - loss: 0.6796 - acc: 0.5682
4288/5677 [=====================>........] - ETA: 2:10 - loss: 0.6798 - acc: 0.5674
4352/5677 [=====================>........] - ETA: 2:04 - loss: 0.6800 - acc: 0.5678
4416/5677 [======================>.......] - ETA: 1:58 - loss: 0.6802 - acc: 0.5661
4480/5677 [======================>.......] - ETA: 1:52 - loss: 0.6803 - acc: 0.5665
4544/5677 [=======================>......] - ETA: 1:46 - loss: 0.6806 - acc: 0.5654
4608/5677 [=======================>......] - ETA: 1:40 - loss: 0.6804 - acc: 0.5653
4672/5677 [=======================>......] - ETA: 1:34 - loss: 0.6801 - acc: 0.5661
4736/5677 [========================>.....] - ETA: 1:28 - loss: 0.6798 - acc: 0.5667
4800/5677 [========================>.....] - ETA: 1:22 - loss: 0.6804 - acc: 0.5667
4864/5677 [========================>.....] - ETA: 1:16 - loss: 0.6806 - acc: 0.5664
4928/5677 [=========================>....] - ETA: 1:10 - loss: 0.6808 - acc: 0.5664
4992/5677 [=========================>....] - ETA: 1:04 - loss: 0.6811 - acc: 0.5655
5056/5677 [=========================>....] - ETA: 58s - loss: 0.6807 - acc: 0.5661 
5120/5677 [==========================>...] - ETA: 52s - loss: 0.6802 - acc: 0.5676
5184/5677 [==========================>...] - ETA: 46s - loss: 0.6802 - acc: 0.5677
5248/5677 [==========================>...] - ETA: 40s - loss: 0.6802 - acc: 0.5678
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6805 - acc: 0.5666
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6805 - acc: 0.5670
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6800 - acc: 0.5680
5504/5677 [============================>.] - ETA: 16s - loss: 0.6794 - acc: 0.5694
5568/5677 [============================>.] - ETA: 10s - loss: 0.6793 - acc: 0.5700
5632/5677 [============================>.] - ETA: 4s - loss: 0.6791 - acc: 0.5701 
5677/5677 [==============================] - 554s 98ms/step - loss: 0.6791 - acc: 0.5702 - val_loss: 0.6746 - val_acc: 0.5895

Epoch 00010: val_acc did not improve from 0.61648
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9d865785d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9d865785d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9d8659e6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9d8659e6d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db97c9310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db97c9310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d640c8510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d640c8510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d64070cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d64070cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d64084190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d64084190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d640c8c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d640c8c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d63facc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d63facc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d64036ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d64036ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d63d6ddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d63d6ddd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d63ec74d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d63ec74d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9734653a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9734653a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d63d65850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d63d65850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f973406aad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f973406aad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d9fc658d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d9fc658d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d63bde210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d63bde210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d63e2d7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d63e2d7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d53b60e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d53b60e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d17d383d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d17d383d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d5377bf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d5377bf90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d63bde350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d63bde350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f951c2bf650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f951c2bf650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d53ad1710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d53ad1710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d5358b910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d5358b910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d53570590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d53570590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d535cb850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d535cb850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d537b83d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d537b83d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d5349b590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d5349b590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d534d4510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d534d4510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d53570590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d53570590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d53571850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d53571850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d537cf410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d537cf410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4b186510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4b186510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4afb8b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4afb8b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4ade1a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4ade1a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4b008dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4b008dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4b337410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4b337410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4ae32310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4ae32310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4b183f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4b183f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4ae9d090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4ae9d090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4abe39d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4abe39d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4af62650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4af62650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4ac66c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4ac66c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4abb3f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4abb3f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4ae9d490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4ae9d490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d428d8990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d428d8990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4ae5c810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4ae5c810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d427a4e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d427a4e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d425b4e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d425b4e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d424cef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d424cef50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d42527210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d42527210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d425b4a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d425b4a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4261dc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4261dc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d425a1450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d425a1450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99ac491a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99ac491a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d3a37bd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d3a37bd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4259b810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4259b810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99ac4c8210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99ac4c8210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97184b3350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97184b3350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97183ccb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97183ccb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97184e1290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97184e1290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97184b3810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97184b3810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97183ad850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97183ad850>>: AttributeError: module 'gast' has no attribute 'Str'
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 4:12
 128/1578 [=>............................] - ETA: 2:24
 192/1578 [==>...........................] - ETA: 1:47
 256/1578 [===>..........................] - ETA: 1:31
 320/1578 [=====>........................] - ETA: 1:17
 384/1578 [======>.......................] - ETA: 1:08
 448/1578 [=======>......................] - ETA: 1:01
 512/1578 [========>.....................] - ETA: 54s 
 576/1578 [=========>....................] - ETA: 49s
 640/1578 [===========>..................] - ETA: 45s
 704/1578 [============>.................] - ETA: 41s
 768/1578 [=============>................] - ETA: 37s
 832/1578 [==============>...............] - ETA: 33s
 896/1578 [================>.............] - ETA: 30s
 960/1578 [=================>............] - ETA: 27s
1024/1578 [==================>...........] - ETA: 24s
1088/1578 [===================>..........] - ETA: 21s
1152/1578 [====================>.........] - ETA: 19s
1216/1578 [======================>.......] - ETA: 16s
1280/1578 [=======================>......] - ETA: 13s
1344/1578 [========================>.....] - ETA: 10s
1408/1578 [=========================>....] - ETA: 7s 
1472/1578 [==========================>...] - ETA: 4s
1536/1578 [============================>.] - ETA: 1s
1578/1578 [==============================] - 69s 44ms/step
loss: 0.6770443826422794
acc: 0.5671736381957437
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f957037d190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f957037d190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f951c2c3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f951c2c3c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db9ab5b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db9ab5b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96f034c210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96f034c210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f951c2d1350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f951c2d1350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c71e790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c71e790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96f034c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96f034c390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d64265810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d64265810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d64357f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d64357f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d642802d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d642802d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d864f26d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d864f26d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d864cc990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d864cc990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9570601950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9570601950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f951c07e610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f951c07e610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d86530a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d86530a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d863d85d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d863d85d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f951c11fe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f951c11fe50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc650ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc650ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f951c05df50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f951c05df50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94fc78ea50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94fc78ea50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc43c350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc43c350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94fc6b6f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94fc6b6f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc4b8110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc4b8110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94fc28c690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94fc28c690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94fc754a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94fc754a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc25cd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc25cd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94fc26c610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94fc26c610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc1f8c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc1f8c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94fc1c9890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94fc1c9890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94d8698c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94d8698c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d87d4190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d87d4190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94d86d3310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94d86d3310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d87f4090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d87f4090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94d8635d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94d8635d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94d82912d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94d82912d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d828f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d828f610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94d83ed510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94d83ed510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d84101d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d84101d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94d8146fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94d8146fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94d82db050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94d82db050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d809b050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d809b050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94d81d8910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94d81d8910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d073cbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d073cbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94d056fb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94d056fb50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94d053b9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94d053b9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f951c2fbdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f951c2fbdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94d8058950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94d8058950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d0311950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d0311950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94d0476890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94d0476890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94d01820d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94d01820d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d0192c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d0192c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94d0202810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f94d0202810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d00c6d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d00c6d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94d0592a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94d0592a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93bc6c2f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93bc6c2f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d01692d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94d01692d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93bc74d690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93bc74d690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93bc608390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93bc608390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93bc64dd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93bc64dd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93bc4787d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93bc4787d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93bc3706d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93bc3706d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93bc458490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93bc458490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93bc780290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93bc780290>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 1:00:18 - loss: 0.7843 - acc: 0.5938
 128/5677 [..............................] - ETA: 37:15 - loss: 0.7213 - acc: 0.6094  
 192/5677 [>.............................] - ETA: 28:56 - loss: 0.7533 - acc: 0.5781
 256/5677 [>.............................] - ETA: 24:43 - loss: 0.7460 - acc: 0.5664
 320/5677 [>.............................] - ETA: 22:17 - loss: 0.7354 - acc: 0.5687
 384/5677 [=>............................] - ETA: 20:24 - loss: 0.7359 - acc: 0.5625
 448/5677 [=>............................] - ETA: 19:04 - loss: 0.7329 - acc: 0.5670
 512/5677 [=>............................] - ETA: 17:56 - loss: 0.7323 - acc: 0.5645
 576/5677 [==>...........................] - ETA: 17:02 - loss: 0.7318 - acc: 0.5625
 640/5677 [==>...........................] - ETA: 16:21 - loss: 0.7280 - acc: 0.5594
 704/5677 [==>...........................] - ETA: 15:41 - loss: 0.7307 - acc: 0.5511
 768/5677 [===>..........................] - ETA: 15:10 - loss: 0.7244 - acc: 0.5573
 832/5677 [===>..........................] - ETA: 14:39 - loss: 0.7275 - acc: 0.5541
 896/5677 [===>..........................] - ETA: 14:15 - loss: 0.7266 - acc: 0.5580
 960/5677 [====>.........................] - ETA: 13:53 - loss: 0.7257 - acc: 0.5552
1024/5677 [====>.........................] - ETA: 13:32 - loss: 0.7331 - acc: 0.5430
1088/5677 [====>.........................] - ETA: 13:13 - loss: 0.7323 - acc: 0.5386
1152/5677 [=====>........................] - ETA: 12:53 - loss: 0.7335 - acc: 0.5356
1216/5677 [=====>........................] - ETA: 12:33 - loss: 0.7305 - acc: 0.5395
1280/5677 [=====>........................] - ETA: 12:15 - loss: 0.7314 - acc: 0.5352
1344/5677 [======>.......................] - ETA: 11:59 - loss: 0.7317 - acc: 0.5320
1408/5677 [======>.......................] - ETA: 11:46 - loss: 0.7304 - acc: 0.5320
1472/5677 [======>.......................] - ETA: 11:32 - loss: 0.7308 - acc: 0.5258
1536/5677 [=======>......................] - ETA: 11:18 - loss: 0.7319 - acc: 0.5208
1600/5677 [=======>......................] - ETA: 11:02 - loss: 0.7311 - acc: 0.5194
1664/5677 [=======>......................] - ETA: 10:48 - loss: 0.7302 - acc: 0.5192
1728/5677 [========>.....................] - ETA: 10:33 - loss: 0.7306 - acc: 0.5156
1792/5677 [========>.....................] - ETA: 10:19 - loss: 0.7307 - acc: 0.5151
1856/5677 [========>.....................] - ETA: 10:06 - loss: 0.7284 - acc: 0.5172
1920/5677 [=========>....................] - ETA: 9:55 - loss: 0.7287 - acc: 0.5182 
1984/5677 [=========>....................] - ETA: 9:44 - loss: 0.7266 - acc: 0.5202
2048/5677 [=========>....................] - ETA: 9:33 - loss: 0.7262 - acc: 0.5195
2112/5677 [==========>...................] - ETA: 9:21 - loss: 0.7255 - acc: 0.5208
2176/5677 [==========>...................] - ETA: 9:10 - loss: 0.7239 - acc: 0.5216
2240/5677 [==========>...................] - ETA: 8:58 - loss: 0.7220 - acc: 0.5228
2304/5677 [===========>..................] - ETA: 8:45 - loss: 0.7226 - acc: 0.5230
2368/5677 [===========>..................] - ETA: 8:33 - loss: 0.7235 - acc: 0.5211
2432/5677 [===========>..................] - ETA: 8:21 - loss: 0.7226 - acc: 0.5226
2496/5677 [============>.................] - ETA: 8:11 - loss: 0.7232 - acc: 0.5220
2560/5677 [============>.................] - ETA: 8:00 - loss: 0.7232 - acc: 0.5215
2624/5677 [============>.................] - ETA: 7:52 - loss: 0.7222 - acc: 0.5221
2688/5677 [=============>................] - ETA: 7:41 - loss: 0.7208 - acc: 0.5219
2752/5677 [=============>................] - ETA: 7:32 - loss: 0.7206 - acc: 0.5214
2816/5677 [=============>................] - ETA: 7:21 - loss: 0.7206 - acc: 0.5213
2880/5677 [==============>...............] - ETA: 7:09 - loss: 0.7204 - acc: 0.5194
2944/5677 [==============>...............] - ETA: 6:59 - loss: 0.7214 - acc: 0.5170
3008/5677 [==============>...............] - ETA: 6:49 - loss: 0.7211 - acc: 0.5176
3072/5677 [===============>..............] - ETA: 6:38 - loss: 0.7202 - acc: 0.5186
3136/5677 [===============>..............] - ETA: 6:28 - loss: 0.7193 - acc: 0.5204
3200/5677 [===============>..............] - ETA: 6:17 - loss: 0.7196 - acc: 0.5197
3264/5677 [================>.............] - ETA: 6:06 - loss: 0.7189 - acc: 0.5214
3328/5677 [================>.............] - ETA: 5:56 - loss: 0.7193 - acc: 0.5225
3392/5677 [================>.............] - ETA: 5:46 - loss: 0.7191 - acc: 0.5230
3456/5677 [=================>............] - ETA: 5:35 - loss: 0.7193 - acc: 0.5220
3520/5677 [=================>............] - ETA: 5:26 - loss: 0.7191 - acc: 0.5216
3584/5677 [=================>............] - ETA: 5:15 - loss: 0.7189 - acc: 0.5212
3648/5677 [==================>...........] - ETA: 5:05 - loss: 0.7189 - acc: 0.5208
3712/5677 [==================>...........] - ETA: 4:55 - loss: 0.7180 - acc: 0.5221
3776/5677 [==================>...........] - ETA: 4:45 - loss: 0.7181 - acc: 0.5225
3840/5677 [===================>..........] - ETA: 4:36 - loss: 0.7180 - acc: 0.5216
3904/5677 [===================>..........] - ETA: 4:26 - loss: 0.7170 - acc: 0.5236
3968/5677 [===================>..........] - ETA: 4:16 - loss: 0.7172 - acc: 0.5232
4032/5677 [====================>.........] - ETA: 4:05 - loss: 0.7168 - acc: 0.5250
4096/5677 [====================>.........] - ETA: 3:55 - loss: 0.7166 - acc: 0.5249
4160/5677 [====================>.........] - ETA: 3:46 - loss: 0.7165 - acc: 0.5238
4224/5677 [=====================>........] - ETA: 3:36 - loss: 0.7165 - acc: 0.5227
4288/5677 [=====================>........] - ETA: 3:26 - loss: 0.7164 - acc: 0.5226
4352/5677 [=====================>........] - ETA: 3:16 - loss: 0.7164 - acc: 0.5223
4416/5677 [======================>.......] - ETA: 3:07 - loss: 0.7161 - acc: 0.5222
4480/5677 [======================>.......] - ETA: 2:57 - loss: 0.7162 - acc: 0.5221
4544/5677 [=======================>......] - ETA: 2:48 - loss: 0.7161 - acc: 0.5213
4608/5677 [=======================>......] - ETA: 2:38 - loss: 0.7159 - acc: 0.5211
4672/5677 [=======================>......] - ETA: 2:29 - loss: 0.7157 - acc: 0.5203
4736/5677 [========================>.....] - ETA: 2:19 - loss: 0.7152 - acc: 0.5215
4800/5677 [========================>.....] - ETA: 2:09 - loss: 0.7150 - acc: 0.5225
4864/5677 [========================>.....] - ETA: 2:00 - loss: 0.7146 - acc: 0.5226
4928/5677 [=========================>....] - ETA: 1:50 - loss: 0.7144 - acc: 0.5221
4992/5677 [=========================>....] - ETA: 1:41 - loss: 0.7142 - acc: 0.5224
5056/5677 [=========================>....] - ETA: 1:31 - loss: 0.7138 - acc: 0.5227
5120/5677 [==========================>...] - ETA: 1:22 - loss: 0.7138 - acc: 0.5225
5184/5677 [==========================>...] - ETA: 1:12 - loss: 0.7133 - acc: 0.5233
5248/5677 [==========================>...] - ETA: 1:03 - loss: 0.7128 - acc: 0.5231
5312/5677 [===========================>..] - ETA: 53s - loss: 0.7125 - acc: 0.5239 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.7125 - acc: 0.5240
5440/5677 [===========================>..] - ETA: 34s - loss: 0.7124 - acc: 0.5237
5504/5677 [============================>.] - ETA: 25s - loss: 0.7120 - acc: 0.5247
5568/5677 [============================>.] - ETA: 15s - loss: 0.7117 - acc: 0.5248
5632/5677 [============================>.] - ETA: 6s - loss: 0.7112 - acc: 0.5247 
5677/5677 [==============================] - 869s 153ms/step - loss: 0.7110 - acc: 0.5249 - val_loss: 0.6869 - val_acc: 0.5436

Epoch 00001: val_acc improved from -inf to 0.54358, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window08/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 14:54 - loss: 0.7663 - acc: 0.4062
 128/5677 [..............................] - ETA: 13:22 - loss: 0.7457 - acc: 0.4297
 192/5677 [>.............................] - ETA: 13:55 - loss: 0.7370 - acc: 0.4688
 256/5677 [>.............................] - ETA: 13:13 - loss: 0.7238 - acc: 0.4883
 320/5677 [>.............................] - ETA: 13:06 - loss: 0.7241 - acc: 0.4844
 384/5677 [=>............................] - ETA: 13:08 - loss: 0.7172 - acc: 0.4948
 448/5677 [=>............................] - ETA: 12:59 - loss: 0.7106 - acc: 0.4978
 512/5677 [=>............................] - ETA: 12:34 - loss: 0.7143 - acc: 0.4883
 576/5677 [==>...........................] - ETA: 12:22 - loss: 0.7139 - acc: 0.4948
 640/5677 [==>...........................] - ETA: 12:11 - loss: 0.7114 - acc: 0.5016
 704/5677 [==>...........................] - ETA: 12:05 - loss: 0.7103 - acc: 0.5057
 768/5677 [===>..........................] - ETA: 11:50 - loss: 0.7077 - acc: 0.5117
 832/5677 [===>..........................] - ETA: 11:36 - loss: 0.7074 - acc: 0.5144
 896/5677 [===>..........................] - ETA: 11:29 - loss: 0.7076 - acc: 0.5145
 960/5677 [====>.........................] - ETA: 11:21 - loss: 0.7045 - acc: 0.5219
1024/5677 [====>.........................] - ETA: 11:18 - loss: 0.7037 - acc: 0.5254
1088/5677 [====>.........................] - ETA: 11:06 - loss: 0.7039 - acc: 0.5221
1152/5677 [=====>........................] - ETA: 10:51 - loss: 0.7028 - acc: 0.5252
1216/5677 [=====>........................] - ETA: 10:38 - loss: 0.7017 - acc: 0.5271
1280/5677 [=====>........................] - ETA: 10:25 - loss: 0.7029 - acc: 0.5219
1344/5677 [======>.......................] - ETA: 10:13 - loss: 0.7021 - acc: 0.5231
1408/5677 [======>.......................] - ETA: 10:08 - loss: 0.7005 - acc: 0.5249
1472/5677 [======>.......................] - ETA: 10:00 - loss: 0.6990 - acc: 0.5279
1536/5677 [=======>......................] - ETA: 9:46 - loss: 0.6996 - acc: 0.5267 
1600/5677 [=======>......................] - ETA: 9:35 - loss: 0.7003 - acc: 0.5256
1664/5677 [=======>......................] - ETA: 9:25 - loss: 0.7005 - acc: 0.5246
1728/5677 [========>.....................] - ETA: 9:15 - loss: 0.7018 - acc: 0.5220
1792/5677 [========>.....................] - ETA: 9:08 - loss: 0.7009 - acc: 0.5240
1856/5677 [========>.....................] - ETA: 8:59 - loss: 0.7007 - acc: 0.5248
1920/5677 [=========>....................] - ETA: 8:50 - loss: 0.7017 - acc: 0.5203
1984/5677 [=========>....................] - ETA: 8:41 - loss: 0.7008 - acc: 0.5227
2048/5677 [=========>....................] - ETA: 8:31 - loss: 0.7007 - acc: 0.5220
2112/5677 [==========>...................] - ETA: 8:23 - loss: 0.7002 - acc: 0.5237
2176/5677 [==========>...................] - ETA: 8:14 - loss: 0.6995 - acc: 0.5239
2240/5677 [==========>...................] - ETA: 8:03 - loss: 0.6985 - acc: 0.5259
2304/5677 [===========>..................] - ETA: 7:56 - loss: 0.6981 - acc: 0.5260
2368/5677 [===========>..................] - ETA: 7:48 - loss: 0.6978 - acc: 0.5258
2432/5677 [===========>..................] - ETA: 7:39 - loss: 0.6983 - acc: 0.5259
2496/5677 [============>.................] - ETA: 7:29 - loss: 0.6977 - acc: 0.5276
2560/5677 [============>.................] - ETA: 7:19 - loss: 0.6975 - acc: 0.5277
2624/5677 [============>.................] - ETA: 7:11 - loss: 0.6978 - acc: 0.5290
2688/5677 [=============>................] - ETA: 7:01 - loss: 0.6985 - acc: 0.5275
2752/5677 [=============>................] - ETA: 6:52 - loss: 0.6984 - acc: 0.5276
2816/5677 [=============>................] - ETA: 6:44 - loss: 0.6986 - acc: 0.5263
2880/5677 [==============>...............] - ETA: 6:35 - loss: 0.6981 - acc: 0.5267
2944/5677 [==============>...............] - ETA: 6:27 - loss: 0.6981 - acc: 0.5265
3008/5677 [==============>...............] - ETA: 6:18 - loss: 0.6972 - acc: 0.5283
3072/5677 [===============>..............] - ETA: 6:10 - loss: 0.6973 - acc: 0.5283
3136/5677 [===============>..............] - ETA: 6:01 - loss: 0.6978 - acc: 0.5261
3200/5677 [===============>..............] - ETA: 5:51 - loss: 0.6974 - acc: 0.5275
3264/5677 [================>.............] - ETA: 5:42 - loss: 0.6975 - acc: 0.5267
3328/5677 [================>.............] - ETA: 5:33 - loss: 0.6970 - acc: 0.5276
3392/5677 [================>.............] - ETA: 5:25 - loss: 0.6969 - acc: 0.5274
3456/5677 [=================>............] - ETA: 5:15 - loss: 0.6968 - acc: 0.5272
3520/5677 [=================>............] - ETA: 5:05 - loss: 0.6973 - acc: 0.5259
3584/5677 [=================>............] - ETA: 4:55 - loss: 0.6968 - acc: 0.5265
3648/5677 [==================>...........] - ETA: 4:46 - loss: 0.6974 - acc: 0.5258
3712/5677 [==================>...........] - ETA: 4:37 - loss: 0.6969 - acc: 0.5269
3776/5677 [==================>...........] - ETA: 4:28 - loss: 0.6969 - acc: 0.5281
3840/5677 [===================>..........] - ETA: 4:19 - loss: 0.6970 - acc: 0.5276
3904/5677 [===================>..........] - ETA: 4:10 - loss: 0.6968 - acc: 0.5279
3968/5677 [===================>..........] - ETA: 4:01 - loss: 0.6962 - acc: 0.5287
4032/5677 [====================>.........] - ETA: 3:52 - loss: 0.6959 - acc: 0.5295
4096/5677 [====================>.........] - ETA: 3:43 - loss: 0.6959 - acc: 0.5293
4160/5677 [====================>.........] - ETA: 3:34 - loss: 0.6965 - acc: 0.5276
4224/5677 [=====================>........] - ETA: 3:25 - loss: 0.6967 - acc: 0.5282
4288/5677 [=====================>........] - ETA: 3:15 - loss: 0.6962 - acc: 0.5282
4352/5677 [=====================>........] - ETA: 3:06 - loss: 0.6960 - acc: 0.5292
4416/5677 [======================>.......] - ETA: 2:57 - loss: 0.6964 - acc: 0.5292
4480/5677 [======================>.......] - ETA: 2:48 - loss: 0.6962 - acc: 0.5297
4544/5677 [=======================>......] - ETA: 2:39 - loss: 0.6964 - acc: 0.5297
4608/5677 [=======================>......] - ETA: 2:30 - loss: 0.6956 - acc: 0.5310
4672/5677 [=======================>......] - ETA: 2:21 - loss: 0.6957 - acc: 0.5308
4736/5677 [========================>.....] - ETA: 2:12 - loss: 0.6957 - acc: 0.5298
4800/5677 [========================>.....] - ETA: 2:03 - loss: 0.6958 - acc: 0.5296
4864/5677 [========================>.....] - ETA: 1:54 - loss: 0.6960 - acc: 0.5300
4928/5677 [=========================>....] - ETA: 1:45 - loss: 0.6956 - acc: 0.5306
4992/5677 [=========================>....] - ETA: 1:35 - loss: 0.6955 - acc: 0.5310
5056/5677 [=========================>....] - ETA: 1:26 - loss: 0.6957 - acc: 0.5312
5120/5677 [==========================>...] - ETA: 1:17 - loss: 0.6956 - acc: 0.5318
5184/5677 [==========================>...] - ETA: 1:08 - loss: 0.6963 - acc: 0.5301
5248/5677 [==========================>...] - ETA: 59s - loss: 0.6955 - acc: 0.5314 
5312/5677 [===========================>..] - ETA: 50s - loss: 0.6955 - acc: 0.5324
5376/5677 [===========================>..] - ETA: 41s - loss: 0.6960 - acc: 0.5311
5440/5677 [===========================>..] - ETA: 32s - loss: 0.6966 - acc: 0.5301
5504/5677 [============================>.] - ETA: 23s - loss: 0.6968 - acc: 0.5289
5568/5677 [============================>.] - ETA: 15s - loss: 0.6970 - acc: 0.5284
5632/5677 [============================>.] - ETA: 6s - loss: 0.6970 - acc: 0.5289 
5677/5677 [==============================] - 814s 143ms/step - loss: 0.6969 - acc: 0.5293 - val_loss: 0.6938 - val_acc: 0.5214

Epoch 00002: val_acc did not improve from 0.54358
Epoch 3/10

  64/5677 [..............................] - ETA: 11:36 - loss: 0.6961 - acc: 0.4844
 128/5677 [..............................] - ETA: 11:17 - loss: 0.7089 - acc: 0.4844
 192/5677 [>.............................] - ETA: 11:06 - loss: 0.7126 - acc: 0.4896
 256/5677 [>.............................] - ETA: 10:43 - loss: 0.7153 - acc: 0.5039
 320/5677 [>.............................] - ETA: 11:03 - loss: 0.7111 - acc: 0.4906
 384/5677 [=>............................] - ETA: 10:50 - loss: 0.7091 - acc: 0.4896
 448/5677 [=>............................] - ETA: 10:40 - loss: 0.7081 - acc: 0.4844
 512/5677 [=>............................] - ETA: 10:29 - loss: 0.7082 - acc: 0.4863
 576/5677 [==>...........................] - ETA: 10:23 - loss: 0.7062 - acc: 0.4931
 640/5677 [==>...........................] - ETA: 10:20 - loss: 0.7036 - acc: 0.4906
 704/5677 [==>...........................] - ETA: 10:16 - loss: 0.7032 - acc: 0.4957
 768/5677 [===>..........................] - ETA: 10:10 - loss: 0.7010 - acc: 0.5013
 832/5677 [===>..........................] - ETA: 10:06 - loss: 0.6989 - acc: 0.5048
 896/5677 [===>..........................] - ETA: 9:58 - loss: 0.6981 - acc: 0.5045 
 960/5677 [====>.........................] - ETA: 9:50 - loss: 0.6957 - acc: 0.5094
1024/5677 [====>.........................] - ETA: 9:41 - loss: 0.6970 - acc: 0.5039
1088/5677 [====>.........................] - ETA: 9:32 - loss: 0.6957 - acc: 0.5092
1152/5677 [=====>........................] - ETA: 9:27 - loss: 0.6961 - acc: 0.5104
1216/5677 [=====>........................] - ETA: 9:20 - loss: 0.6963 - acc: 0.5082
1280/5677 [=====>........................] - ETA: 9:12 - loss: 0.6949 - acc: 0.5117
1344/5677 [======>.......................] - ETA: 9:03 - loss: 0.6944 - acc: 0.5134
1408/5677 [======>.......................] - ETA: 8:54 - loss: 0.6951 - acc: 0.5149
1472/5677 [======>.......................] - ETA: 8:50 - loss: 0.6944 - acc: 0.5163
1536/5677 [=======>......................] - ETA: 8:41 - loss: 0.6943 - acc: 0.5156
1600/5677 [=======>......................] - ETA: 8:33 - loss: 0.6944 - acc: 0.5156
1664/5677 [=======>......................] - ETA: 8:25 - loss: 0.6939 - acc: 0.5198
1728/5677 [========>.....................] - ETA: 8:18 - loss: 0.6940 - acc: 0.5214
1792/5677 [========>.....................] - ETA: 8:11 - loss: 0.6946 - acc: 0.5212
1856/5677 [========>.....................] - ETA: 8:03 - loss: 0.6942 - acc: 0.5232
1920/5677 [=========>....................] - ETA: 7:55 - loss: 0.6941 - acc: 0.5229
1984/5677 [=========>....................] - ETA: 7:47 - loss: 0.6934 - acc: 0.5252
2048/5677 [=========>....................] - ETA: 7:40 - loss: 0.6925 - acc: 0.5283
2112/5677 [==========>...................] - ETA: 7:32 - loss: 0.6921 - acc: 0.5289
2176/5677 [==========>...................] - ETA: 7:22 - loss: 0.6929 - acc: 0.5267
2240/5677 [==========>...................] - ETA: 7:14 - loss: 0.6947 - acc: 0.5237
2304/5677 [===========>..................] - ETA: 7:06 - loss: 0.6946 - acc: 0.5243
2368/5677 [===========>..................] - ETA: 6:58 - loss: 0.6944 - acc: 0.5249
2432/5677 [===========>..................] - ETA: 6:50 - loss: 0.6954 - acc: 0.5234
2496/5677 [============>.................] - ETA: 6:41 - loss: 0.6952 - acc: 0.5244
2560/5677 [============>.................] - ETA: 6:36 - loss: 0.6946 - acc: 0.5266
2624/5677 [============>.................] - ETA: 6:29 - loss: 0.6941 - acc: 0.5267
2688/5677 [=============>................] - ETA: 6:20 - loss: 0.6938 - acc: 0.5275
2752/5677 [=============>................] - ETA: 6:13 - loss: 0.6937 - acc: 0.5280
2816/5677 [=============>................] - ETA: 6:04 - loss: 0.6937 - acc: 0.5281
2880/5677 [==============>...............] - ETA: 5:56 - loss: 0.6932 - acc: 0.5292
2944/5677 [==============>...............] - ETA: 5:49 - loss: 0.6924 - acc: 0.5312
3008/5677 [==============>...............] - ETA: 5:41 - loss: 0.6915 - acc: 0.5329
3072/5677 [===============>..............] - ETA: 5:33 - loss: 0.6910 - acc: 0.5345
3136/5677 [===============>..............] - ETA: 5:25 - loss: 0.6912 - acc: 0.5341
3200/5677 [===============>..............] - ETA: 5:17 - loss: 0.6909 - acc: 0.5359
3264/5677 [================>.............] - ETA: 5:08 - loss: 0.6908 - acc: 0.5352
3328/5677 [================>.............] - ETA: 5:00 - loss: 0.6909 - acc: 0.5352
3392/5677 [================>.............] - ETA: 4:51 - loss: 0.6911 - acc: 0.5357
3456/5677 [=================>............] - ETA: 4:44 - loss: 0.6907 - acc: 0.5365
3520/5677 [=================>............] - ETA: 4:36 - loss: 0.6907 - acc: 0.5375
3584/5677 [=================>............] - ETA: 4:29 - loss: 0.6903 - acc: 0.5396
3648/5677 [==================>...........] - ETA: 4:21 - loss: 0.6907 - acc: 0.5389
3712/5677 [==================>...........] - ETA: 4:14 - loss: 0.6914 - acc: 0.5385
3776/5677 [==================>...........] - ETA: 4:06 - loss: 0.6909 - acc: 0.5395
3840/5677 [===================>..........] - ETA: 3:57 - loss: 0.6912 - acc: 0.5396
3904/5677 [===================>..........] - ETA: 3:49 - loss: 0.6913 - acc: 0.5394
3968/5677 [===================>..........] - ETA: 3:40 - loss: 0.6911 - acc: 0.5391
4032/5677 [====================>.........] - ETA: 3:32 - loss: 0.6913 - acc: 0.5382
4096/5677 [====================>.........] - ETA: 3:24 - loss: 0.6914 - acc: 0.5386
4160/5677 [====================>.........] - ETA: 3:16 - loss: 0.6914 - acc: 0.5389
4224/5677 [=====================>........] - ETA: 3:07 - loss: 0.6914 - acc: 0.5388
4288/5677 [=====================>........] - ETA: 2:59 - loss: 0.6916 - acc: 0.5385
4352/5677 [=====================>........] - ETA: 2:51 - loss: 0.6915 - acc: 0.5388
4416/5677 [======================>.......] - ETA: 2:43 - loss: 0.6920 - acc: 0.5378
4480/5677 [======================>.......] - ETA: 2:35 - loss: 0.6920 - acc: 0.5368
4544/5677 [=======================>......] - ETA: 2:27 - loss: 0.6916 - acc: 0.5376
4608/5677 [=======================>......] - ETA: 2:18 - loss: 0.6917 - acc: 0.5378
4672/5677 [=======================>......] - ETA: 2:10 - loss: 0.6913 - acc: 0.5398
4736/5677 [========================>.....] - ETA: 2:01 - loss: 0.6908 - acc: 0.5405
4800/5677 [========================>.....] - ETA: 1:53 - loss: 0.6905 - acc: 0.5419
4864/5677 [========================>.....] - ETA: 1:44 - loss: 0.6905 - acc: 0.5409
4928/5677 [=========================>....] - ETA: 1:36 - loss: 0.6904 - acc: 0.5410
4992/5677 [=========================>....] - ETA: 1:28 - loss: 0.6904 - acc: 0.5417
5056/5677 [=========================>....] - ETA: 1:20 - loss: 0.6900 - acc: 0.5425
5120/5677 [==========================>...] - ETA: 1:11 - loss: 0.6906 - acc: 0.5412
5184/5677 [==========================>...] - ETA: 1:03 - loss: 0.6906 - acc: 0.5407
5248/5677 [==========================>...] - ETA: 55s - loss: 0.6906 - acc: 0.5410 
5312/5677 [===========================>..] - ETA: 47s - loss: 0.6906 - acc: 0.5410
5376/5677 [===========================>..] - ETA: 38s - loss: 0.6906 - acc: 0.5400
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6905 - acc: 0.5408
5504/5677 [============================>.] - ETA: 22s - loss: 0.6900 - acc: 0.5429
5568/5677 [============================>.] - ETA: 14s - loss: 0.6898 - acc: 0.5433
5632/5677 [============================>.] - ETA: 5s - loss: 0.6899 - acc: 0.5431 
5677/5677 [==============================] - 758s 134ms/step - loss: 0.6897 - acc: 0.5441 - val_loss: 0.6869 - val_acc: 0.5563

Epoch 00003: val_acc improved from 0.54358 to 0.55626, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window08/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 4/10

  64/5677 [..............................] - ETA: 11:36 - loss: 0.6670 - acc: 0.6094
 128/5677 [..............................] - ETA: 13:22 - loss: 0.6775 - acc: 0.6094
 192/5677 [>.............................] - ETA: 13:30 - loss: 0.6803 - acc: 0.5990
 256/5677 [>.............................] - ETA: 12:58 - loss: 0.6819 - acc: 0.5820
 320/5677 [>.............................] - ETA: 12:30 - loss: 0.6803 - acc: 0.5813
 384/5677 [=>............................] - ETA: 11:55 - loss: 0.6741 - acc: 0.5911
 448/5677 [=>............................] - ETA: 11:37 - loss: 0.6784 - acc: 0.5714
 512/5677 [=>............................] - ETA: 11:24 - loss: 0.6765 - acc: 0.5762
 576/5677 [==>...........................] - ETA: 11:17 - loss: 0.6734 - acc: 0.5851
 640/5677 [==>...........................] - ETA: 11:10 - loss: 0.6803 - acc: 0.5656
 704/5677 [==>...........................] - ETA: 11:03 - loss: 0.6793 - acc: 0.5682
 768/5677 [===>..........................] - ETA: 10:54 - loss: 0.6789 - acc: 0.5690
 832/5677 [===>..........................] - ETA: 10:47 - loss: 0.6802 - acc: 0.5709
 896/5677 [===>..........................] - ETA: 10:46 - loss: 0.6797 - acc: 0.5725
 960/5677 [====>.........................] - ETA: 10:41 - loss: 0.6800 - acc: 0.5677
1024/5677 [====>.........................] - ETA: 10:32 - loss: 0.6799 - acc: 0.5674
1088/5677 [====>.........................] - ETA: 10:24 - loss: 0.6786 - acc: 0.5699
1152/5677 [=====>........................] - ETA: 10:14 - loss: 0.6799 - acc: 0.5642
1216/5677 [=====>........................] - ETA: 10:06 - loss: 0.6811 - acc: 0.5633
1280/5677 [=====>........................] - ETA: 9:56 - loss: 0.6815 - acc: 0.5641 
1344/5677 [======>.......................] - ETA: 9:43 - loss: 0.6813 - acc: 0.5640
1408/5677 [======>.......................] - ETA: 9:33 - loss: 0.6808 - acc: 0.5646
1472/5677 [======>.......................] - ETA: 9:25 - loss: 0.6822 - acc: 0.5591
1536/5677 [=======>......................] - ETA: 9:18 - loss: 0.6832 - acc: 0.5560
1600/5677 [=======>......................] - ETA: 9:10 - loss: 0.6826 - acc: 0.5581
1664/5677 [=======>......................] - ETA: 9:02 - loss: 0.6827 - acc: 0.5601
1728/5677 [========>.....................] - ETA: 8:52 - loss: 0.6833 - acc: 0.5579
1792/5677 [========>.....................] - ETA: 8:44 - loss: 0.6824 - acc: 0.5597
1856/5677 [========>.....................] - ETA: 8:35 - loss: 0.6823 - acc: 0.5609
1920/5677 [=========>....................] - ETA: 8:25 - loss: 0.6823 - acc: 0.5594
1984/5677 [=========>....................] - ETA: 8:15 - loss: 0.6825 - acc: 0.5605
2048/5677 [=========>....................] - ETA: 8:05 - loss: 0.6829 - acc: 0.5586
2112/5677 [==========>...................] - ETA: 7:56 - loss: 0.6819 - acc: 0.5630
2176/5677 [==========>...................] - ETA: 7:45 - loss: 0.6817 - acc: 0.5639
2240/5677 [==========>...................] - ETA: 7:37 - loss: 0.6811 - acc: 0.5647
2304/5677 [===========>..................] - ETA: 7:26 - loss: 0.6809 - acc: 0.5651
2368/5677 [===========>..................] - ETA: 7:17 - loss: 0.6801 - acc: 0.5667
2432/5677 [===========>..................] - ETA: 7:07 - loss: 0.6808 - acc: 0.5666
2496/5677 [============>.................] - ETA: 6:57 - loss: 0.6807 - acc: 0.5677
2560/5677 [============>.................] - ETA: 6:48 - loss: 0.6806 - acc: 0.5684
2624/5677 [============>.................] - ETA: 6:38 - loss: 0.6806 - acc: 0.5659
2688/5677 [=============>................] - ETA: 6:28 - loss: 0.6808 - acc: 0.5666
2752/5677 [=============>................] - ETA: 6:19 - loss: 0.6802 - acc: 0.5658
2816/5677 [=============>................] - ETA: 6:09 - loss: 0.6802 - acc: 0.5650
2880/5677 [==============>...............] - ETA: 6:00 - loss: 0.6809 - acc: 0.5628
2944/5677 [==============>...............] - ETA: 5:51 - loss: 0.6809 - acc: 0.5632
3008/5677 [==============>...............] - ETA: 5:41 - loss: 0.6807 - acc: 0.5642
3072/5677 [===============>..............] - ETA: 5:33 - loss: 0.6802 - acc: 0.5645
3136/5677 [===============>..............] - ETA: 5:24 - loss: 0.6803 - acc: 0.5635
3200/5677 [===============>..............] - ETA: 5:15 - loss: 0.6798 - acc: 0.5644
3264/5677 [================>.............] - ETA: 5:06 - loss: 0.6802 - acc: 0.5628
3328/5677 [================>.............] - ETA: 4:58 - loss: 0.6814 - acc: 0.5616
3392/5677 [================>.............] - ETA: 4:49 - loss: 0.6814 - acc: 0.5619
3456/5677 [=================>............] - ETA: 4:41 - loss: 0.6816 - acc: 0.5622
3520/5677 [=================>............] - ETA: 4:31 - loss: 0.6819 - acc: 0.5631
3584/5677 [=================>............] - ETA: 4:23 - loss: 0.6815 - acc: 0.5636
3648/5677 [==================>...........] - ETA: 4:15 - loss: 0.6821 - acc: 0.5628
3712/5677 [==================>...........] - ETA: 4:06 - loss: 0.6820 - acc: 0.5628
3776/5677 [==================>...........] - ETA: 3:58 - loss: 0.6815 - acc: 0.5638
3840/5677 [===================>..........] - ETA: 3:50 - loss: 0.6816 - acc: 0.5638
3904/5677 [===================>..........] - ETA: 3:42 - loss: 0.6815 - acc: 0.5628
3968/5677 [===================>..........] - ETA: 3:34 - loss: 0.6816 - acc: 0.5622
4032/5677 [====================>.........] - ETA: 3:25 - loss: 0.6812 - acc: 0.5630
4096/5677 [====================>.........] - ETA: 3:17 - loss: 0.6816 - acc: 0.5627
4160/5677 [====================>.........] - ETA: 3:09 - loss: 0.6823 - acc: 0.5608
4224/5677 [=====================>........] - ETA: 3:01 - loss: 0.6830 - acc: 0.5592
4288/5677 [=====================>........] - ETA: 2:53 - loss: 0.6833 - acc: 0.5588
4352/5677 [=====================>........] - ETA: 2:45 - loss: 0.6836 - acc: 0.5584
4416/5677 [======================>.......] - ETA: 2:36 - loss: 0.6838 - acc: 0.5577
4480/5677 [======================>.......] - ETA: 2:28 - loss: 0.6835 - acc: 0.5578
4544/5677 [=======================>......] - ETA: 2:20 - loss: 0.6835 - acc: 0.5581
4608/5677 [=======================>......] - ETA: 2:12 - loss: 0.6837 - acc: 0.5582
4672/5677 [=======================>......] - ETA: 2:04 - loss: 0.6839 - acc: 0.5578
4736/5677 [========================>.....] - ETA: 1:56 - loss: 0.6842 - acc: 0.5564
4800/5677 [========================>.....] - ETA: 1:48 - loss: 0.6840 - acc: 0.5581
4864/5677 [========================>.....] - ETA: 1:40 - loss: 0.6839 - acc: 0.5576
4928/5677 [=========================>....] - ETA: 1:32 - loss: 0.6844 - acc: 0.5578
4992/5677 [=========================>....] - ETA: 1:24 - loss: 0.6843 - acc: 0.5583
5056/5677 [=========================>....] - ETA: 1:16 - loss: 0.6842 - acc: 0.5595
5120/5677 [==========================>...] - ETA: 1:08 - loss: 0.6836 - acc: 0.5602
5184/5677 [==========================>...] - ETA: 1:00 - loss: 0.6836 - acc: 0.5602
5248/5677 [==========================>...] - ETA: 52s - loss: 0.6837 - acc: 0.5600 
5312/5677 [===========================>..] - ETA: 44s - loss: 0.6841 - acc: 0.5597
5376/5677 [===========================>..] - ETA: 37s - loss: 0.6842 - acc: 0.5597
5440/5677 [===========================>..] - ETA: 29s - loss: 0.6836 - acc: 0.5607
5504/5677 [============================>.] - ETA: 21s - loss: 0.6836 - acc: 0.5612
5568/5677 [============================>.] - ETA: 13s - loss: 0.6835 - acc: 0.5621
5632/5677 [============================>.] - ETA: 5s - loss: 0.6837 - acc: 0.5618 
5677/5677 [==============================] - 725s 128ms/step - loss: 0.6835 - acc: 0.5616 - val_loss: 0.6848 - val_acc: 0.5594

Epoch 00004: val_acc improved from 0.55626 to 0.55943, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window08/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 5/10

  64/5677 [..............................] - ETA: 11:45 - loss: 0.6472 - acc: 0.6719
 128/5677 [..............................] - ETA: 11:58 - loss: 0.6825 - acc: 0.5781
 192/5677 [>.............................] - ETA: 11:35 - loss: 0.6860 - acc: 0.5677
 256/5677 [>.............................] - ETA: 11:08 - loss: 0.6777 - acc: 0.5742
 320/5677 [>.............................] - ETA: 10:55 - loss: 0.6732 - acc: 0.5906
 384/5677 [=>............................] - ETA: 10:47 - loss: 0.6660 - acc: 0.5990
 448/5677 [=>............................] - ETA: 10:43 - loss: 0.6625 - acc: 0.6071
 512/5677 [=>............................] - ETA: 10:30 - loss: 0.6683 - acc: 0.5957
 576/5677 [==>...........................] - ETA: 10:21 - loss: 0.6665 - acc: 0.6007
 640/5677 [==>...........................] - ETA: 10:15 - loss: 0.6712 - acc: 0.5906
 704/5677 [==>...........................] - ETA: 10:08 - loss: 0.6704 - acc: 0.5909
 768/5677 [===>..........................] - ETA: 10:09 - loss: 0.6708 - acc: 0.5951
 832/5677 [===>..........................] - ETA: 10:04 - loss: 0.6709 - acc: 0.5950
 896/5677 [===>..........................] - ETA: 9:54 - loss: 0.6738 - acc: 0.5893 
 960/5677 [====>.........................] - ETA: 9:47 - loss: 0.6750 - acc: 0.5865
1024/5677 [====>.........................] - ETA: 9:39 - loss: 0.6721 - acc: 0.5947
1088/5677 [====>.........................] - ETA: 9:26 - loss: 0.6745 - acc: 0.5919
1152/5677 [=====>........................] - ETA: 9:20 - loss: 0.6712 - acc: 0.5972
1216/5677 [=====>........................] - ETA: 9:11 - loss: 0.6734 - acc: 0.5921
1280/5677 [=====>........................] - ETA: 9:08 - loss: 0.6734 - acc: 0.5906
1344/5677 [======>.......................] - ETA: 8:56 - loss: 0.6709 - acc: 0.5967
1408/5677 [======>.......................] - ETA: 8:47 - loss: 0.6693 - acc: 0.5966
1472/5677 [======>.......................] - ETA: 8:36 - loss: 0.6718 - acc: 0.5904
1536/5677 [=======>......................] - ETA: 8:26 - loss: 0.6710 - acc: 0.5905
1600/5677 [=======>......................] - ETA: 8:19 - loss: 0.6718 - acc: 0.5919
1664/5677 [=======>......................] - ETA: 8:09 - loss: 0.6746 - acc: 0.5871
1728/5677 [========>.....................] - ETA: 7:59 - loss: 0.6748 - acc: 0.5868
1792/5677 [========>.....................] - ETA: 7:49 - loss: 0.6759 - acc: 0.5843
1856/5677 [========>.....................] - ETA: 7:39 - loss: 0.6785 - acc: 0.5803
1920/5677 [=========>....................] - ETA: 7:32 - loss: 0.6802 - acc: 0.5792
1984/5677 [=========>....................] - ETA: 7:24 - loss: 0.6807 - acc: 0.5776
2048/5677 [=========>....................] - ETA: 7:17 - loss: 0.6799 - acc: 0.5786
2112/5677 [==========>...................] - ETA: 7:07 - loss: 0.6787 - acc: 0.5791
2176/5677 [==========>...................] - ETA: 6:58 - loss: 0.6786 - acc: 0.5800
2240/5677 [==========>...................] - ETA: 6:51 - loss: 0.6779 - acc: 0.5817
2304/5677 [===========>..................] - ETA: 6:43 - loss: 0.6781 - acc: 0.5816
2368/5677 [===========>..................] - ETA: 6:38 - loss: 0.6782 - acc: 0.5823
2432/5677 [===========>..................] - ETA: 6:31 - loss: 0.6770 - acc: 0.5855
2496/5677 [============>.................] - ETA: 6:22 - loss: 0.6769 - acc: 0.5853
2560/5677 [============>.................] - ETA: 6:15 - loss: 0.6775 - acc: 0.5844
2624/5677 [============>.................] - ETA: 6:06 - loss: 0.6783 - acc: 0.5835
2688/5677 [=============>................] - ETA: 5:58 - loss: 0.6786 - acc: 0.5826
2752/5677 [=============>................] - ETA: 5:50 - loss: 0.6790 - acc: 0.5821
2816/5677 [=============>................] - ETA: 5:42 - loss: 0.6789 - acc: 0.5824
2880/5677 [==============>...............] - ETA: 5:35 - loss: 0.6792 - acc: 0.5802
2944/5677 [==============>...............] - ETA: 5:28 - loss: 0.6799 - acc: 0.5791
3008/5677 [==============>...............] - ETA: 5:20 - loss: 0.6798 - acc: 0.5791
3072/5677 [===============>..............] - ETA: 5:12 - loss: 0.6791 - acc: 0.5794
3136/5677 [===============>..............] - ETA: 5:04 - loss: 0.6790 - acc: 0.5797
3200/5677 [===============>..............] - ETA: 4:56 - loss: 0.6789 - acc: 0.5803
3264/5677 [================>.............] - ETA: 4:48 - loss: 0.6792 - acc: 0.5794
3328/5677 [================>.............] - ETA: 4:41 - loss: 0.6784 - acc: 0.5805
3392/5677 [================>.............] - ETA: 4:33 - loss: 0.6780 - acc: 0.5808
3456/5677 [=================>............] - ETA: 4:26 - loss: 0.6783 - acc: 0.5810
3520/5677 [=================>............] - ETA: 4:18 - loss: 0.6792 - acc: 0.5793
3584/5677 [=================>............] - ETA: 4:11 - loss: 0.6797 - acc: 0.5776
3648/5677 [==================>...........] - ETA: 4:03 - loss: 0.6796 - acc: 0.5770
3712/5677 [==================>...........] - ETA: 3:56 - loss: 0.6800 - acc: 0.5752
3776/5677 [==================>...........] - ETA: 3:49 - loss: 0.6801 - acc: 0.5742
3840/5677 [===================>..........] - ETA: 3:41 - loss: 0.6795 - acc: 0.5755
3904/5677 [===================>..........] - ETA: 3:34 - loss: 0.6794 - acc: 0.5768
3968/5677 [===================>..........] - ETA: 3:26 - loss: 0.6792 - acc: 0.5771
4032/5677 [====================>.........] - ETA: 3:18 - loss: 0.6787 - acc: 0.5774
4096/5677 [====================>.........] - ETA: 3:10 - loss: 0.6791 - acc: 0.5757
4160/5677 [====================>.........] - ETA: 3:02 - loss: 0.6794 - acc: 0.5755
4224/5677 [=====================>........] - ETA: 2:55 - loss: 0.6799 - acc: 0.5739
4288/5677 [=====================>........] - ETA: 2:47 - loss: 0.6800 - acc: 0.5742
4352/5677 [=====================>........] - ETA: 2:39 - loss: 0.6804 - acc: 0.5735
4416/5677 [======================>.......] - ETA: 2:32 - loss: 0.6802 - acc: 0.5731
4480/5677 [======================>.......] - ETA: 2:24 - loss: 0.6795 - acc: 0.5752
4544/5677 [=======================>......] - ETA: 2:16 - loss: 0.6799 - acc: 0.5737
4608/5677 [=======================>......] - ETA: 2:09 - loss: 0.6799 - acc: 0.5740
4672/5677 [=======================>......] - ETA: 2:01 - loss: 0.6798 - acc: 0.5743
4736/5677 [========================>.....] - ETA: 1:54 - loss: 0.6798 - acc: 0.5735
4800/5677 [========================>.....] - ETA: 1:46 - loss: 0.6800 - acc: 0.5735
4864/5677 [========================>.....] - ETA: 1:38 - loss: 0.6802 - acc: 0.5722
4928/5677 [=========================>....] - ETA: 1:30 - loss: 0.6808 - acc: 0.5708
4992/5677 [=========================>....] - ETA: 1:22 - loss: 0.6808 - acc: 0.5709
5056/5677 [=========================>....] - ETA: 1:15 - loss: 0.6806 - acc: 0.5718
5120/5677 [==========================>...] - ETA: 1:07 - loss: 0.6806 - acc: 0.5717
5184/5677 [==========================>...] - ETA: 59s - loss: 0.6814 - acc: 0.5700 
5248/5677 [==========================>...] - ETA: 51s - loss: 0.6819 - acc: 0.5682
5312/5677 [===========================>..] - ETA: 44s - loss: 0.6818 - acc: 0.5683
5376/5677 [===========================>..] - ETA: 36s - loss: 0.6816 - acc: 0.5681
5440/5677 [===========================>..] - ETA: 28s - loss: 0.6813 - acc: 0.5676
5504/5677 [============================>.] - ETA: 20s - loss: 0.6813 - acc: 0.5681
5568/5677 [============================>.] - ETA: 13s - loss: 0.6812 - acc: 0.5675
5632/5677 [============================>.] - ETA: 5s - loss: 0.6810 - acc: 0.5678 
5677/5677 [==============================] - 708s 125ms/step - loss: 0.6818 - acc: 0.5661 - val_loss: 0.6893 - val_acc: 0.5689

Epoch 00005: val_acc improved from 0.55943 to 0.56894, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window08/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 10:28 - loss: 0.6948 - acc: 0.6094
 128/5677 [..............................] - ETA: 10:11 - loss: 0.6708 - acc: 0.5859
 192/5677 [>.............................] - ETA: 9:58 - loss: 0.6633 - acc: 0.6094 
 256/5677 [>.............................] - ETA: 9:49 - loss: 0.6742 - acc: 0.5938
 320/5677 [>.............................] - ETA: 9:41 - loss: 0.6724 - acc: 0.6000
 384/5677 [=>............................] - ETA: 9:24 - loss: 0.6680 - acc: 0.6016
 448/5677 [=>............................] - ETA: 9:21 - loss: 0.6683 - acc: 0.6049
 512/5677 [=>............................] - ETA: 9:22 - loss: 0.6678 - acc: 0.6016
 576/5677 [==>...........................] - ETA: 9:17 - loss: 0.6648 - acc: 0.6076
 640/5677 [==>...........................] - ETA: 9:17 - loss: 0.6671 - acc: 0.6031
 704/5677 [==>...........................] - ETA: 9:09 - loss: 0.6690 - acc: 0.5966
 768/5677 [===>..........................] - ETA: 9:06 - loss: 0.6686 - acc: 0.6003
 832/5677 [===>..........................] - ETA: 9:00 - loss: 0.6706 - acc: 0.5962
 896/5677 [===>..........................] - ETA: 8:57 - loss: 0.6670 - acc: 0.6027
 960/5677 [====>.........................] - ETA: 8:51 - loss: 0.6680 - acc: 0.5979
1024/5677 [====>.........................] - ETA: 8:41 - loss: 0.6690 - acc: 0.5938
1088/5677 [====>.........................] - ETA: 8:34 - loss: 0.6710 - acc: 0.5892
1152/5677 [=====>........................] - ETA: 8:27 - loss: 0.6698 - acc: 0.5929
1216/5677 [=====>........................] - ETA: 8:18 - loss: 0.6701 - acc: 0.5938
1280/5677 [=====>........................] - ETA: 8:16 - loss: 0.6722 - acc: 0.5914
1344/5677 [======>.......................] - ETA: 8:09 - loss: 0.6724 - acc: 0.5923
1408/5677 [======>.......................] - ETA: 8:01 - loss: 0.6726 - acc: 0.5902
1472/5677 [======>.......................] - ETA: 7:53 - loss: 0.6715 - acc: 0.5904
1536/5677 [=======>......................] - ETA: 7:45 - loss: 0.6737 - acc: 0.5866
1600/5677 [=======>......................] - ETA: 7:39 - loss: 0.6747 - acc: 0.5831
1664/5677 [=======>......................] - ETA: 7:31 - loss: 0.6732 - acc: 0.5871
1728/5677 [========>.....................] - ETA: 7:24 - loss: 0.6726 - acc: 0.5856
1792/5677 [========>.....................] - ETA: 7:18 - loss: 0.6726 - acc: 0.5859
1856/5677 [========>.....................] - ETA: 7:10 - loss: 0.6734 - acc: 0.5851
1920/5677 [=========>....................] - ETA: 7:04 - loss: 0.6735 - acc: 0.5865
1984/5677 [=========>....................] - ETA: 6:58 - loss: 0.6734 - acc: 0.5877
2048/5677 [=========>....................] - ETA: 6:52 - loss: 0.6744 - acc: 0.5889
2112/5677 [==========>...................] - ETA: 6:44 - loss: 0.6734 - acc: 0.5895
2176/5677 [==========>...................] - ETA: 6:38 - loss: 0.6738 - acc: 0.5910
2240/5677 [==========>...................] - ETA: 6:29 - loss: 0.6753 - acc: 0.5853
2304/5677 [===========>..................] - ETA: 6:23 - loss: 0.6760 - acc: 0.5842
2368/5677 [===========>..................] - ETA: 6:16 - loss: 0.6759 - acc: 0.5836
2432/5677 [===========>..................] - ETA: 6:08 - loss: 0.6760 - acc: 0.5814
2496/5677 [============>.................] - ETA: 6:00 - loss: 0.6758 - acc: 0.5829
2560/5677 [============>.................] - ETA: 5:53 - loss: 0.6757 - acc: 0.5832
2624/5677 [============>.................] - ETA: 5:46 - loss: 0.6761 - acc: 0.5823
2688/5677 [=============>................] - ETA: 5:40 - loss: 0.6765 - acc: 0.5807
2752/5677 [=============>................] - ETA: 5:34 - loss: 0.6766 - acc: 0.5803
2816/5677 [=============>................] - ETA: 5:27 - loss: 0.6769 - acc: 0.5788
2880/5677 [==============>...............] - ETA: 5:19 - loss: 0.6770 - acc: 0.5785
2944/5677 [==============>...............] - ETA: 5:13 - loss: 0.6773 - acc: 0.5781
3008/5677 [==============>...............] - ETA: 5:05 - loss: 0.6774 - acc: 0.5788
3072/5677 [===============>..............] - ETA: 4:58 - loss: 0.6774 - acc: 0.5785
3136/5677 [===============>..............] - ETA: 4:50 - loss: 0.6775 - acc: 0.5784
3200/5677 [===============>..............] - ETA: 4:43 - loss: 0.6774 - acc: 0.5787
3264/5677 [================>.............] - ETA: 4:36 - loss: 0.6774 - acc: 0.5775
3328/5677 [================>.............] - ETA: 4:28 - loss: 0.6771 - acc: 0.5790
3392/5677 [================>.............] - ETA: 4:21 - loss: 0.6777 - acc: 0.5767
3456/5677 [=================>............] - ETA: 4:14 - loss: 0.6775 - acc: 0.5755
3520/5677 [=================>............] - ETA: 4:07 - loss: 0.6781 - acc: 0.5753
3584/5677 [=================>............] - ETA: 3:59 - loss: 0.6786 - acc: 0.5734
3648/5677 [==================>...........] - ETA: 3:52 - loss: 0.6784 - acc: 0.5735
3712/5677 [==================>...........] - ETA: 3:45 - loss: 0.6783 - acc: 0.5733
3776/5677 [==================>...........] - ETA: 3:37 - loss: 0.6786 - acc: 0.5728
3840/5677 [===================>..........] - ETA: 3:30 - loss: 0.6784 - acc: 0.5734
3904/5677 [===================>..........] - ETA: 3:22 - loss: 0.6781 - acc: 0.5738
3968/5677 [===================>..........] - ETA: 3:15 - loss: 0.6778 - acc: 0.5759
4032/5677 [====================>.........] - ETA: 3:08 - loss: 0.6776 - acc: 0.5761
4096/5677 [====================>.........] - ETA: 3:01 - loss: 0.6777 - acc: 0.5762
4160/5677 [====================>.........] - ETA: 2:53 - loss: 0.6773 - acc: 0.5774
4224/5677 [=====================>........] - ETA: 2:46 - loss: 0.6772 - acc: 0.5777
4288/5677 [=====================>........] - ETA: 2:39 - loss: 0.6773 - acc: 0.5770
4352/5677 [=====================>........] - ETA: 2:31 - loss: 0.6775 - acc: 0.5772
4416/5677 [======================>.......] - ETA: 2:24 - loss: 0.6773 - acc: 0.5786
4480/5677 [======================>.......] - ETA: 2:16 - loss: 0.6774 - acc: 0.5781
4544/5677 [=======================>......] - ETA: 2:09 - loss: 0.6772 - acc: 0.5788
4608/5677 [=======================>......] - ETA: 2:02 - loss: 0.6773 - acc: 0.5790
4672/5677 [=======================>......] - ETA: 1:54 - loss: 0.6767 - acc: 0.5792
4736/5677 [========================>.....] - ETA: 1:47 - loss: 0.6769 - acc: 0.5790
4800/5677 [========================>.....] - ETA: 1:40 - loss: 0.6769 - acc: 0.5790
4864/5677 [========================>.....] - ETA: 1:32 - loss: 0.6769 - acc: 0.5785
4928/5677 [=========================>....] - ETA: 1:25 - loss: 0.6770 - acc: 0.5783
4992/5677 [=========================>....] - ETA: 1:18 - loss: 0.6777 - acc: 0.5775
5056/5677 [=========================>....] - ETA: 1:10 - loss: 0.6776 - acc: 0.5773
5120/5677 [==========================>...] - ETA: 1:03 - loss: 0.6770 - acc: 0.5783
5184/5677 [==========================>...] - ETA: 56s - loss: 0.6774 - acc: 0.5768 
5248/5677 [==========================>...] - ETA: 49s - loss: 0.6773 - acc: 0.5774
5312/5677 [===========================>..] - ETA: 41s - loss: 0.6768 - acc: 0.5787
5376/5677 [===========================>..] - ETA: 34s - loss: 0.6769 - acc: 0.5791
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6768 - acc: 0.5790
5504/5677 [============================>.] - ETA: 19s - loss: 0.6767 - acc: 0.5792
5568/5677 [============================>.] - ETA: 12s - loss: 0.6760 - acc: 0.5808
5632/5677 [============================>.] - ETA: 5s - loss: 0.6760 - acc: 0.5819 
5677/5677 [==============================] - 676s 119ms/step - loss: 0.6762 - acc: 0.5816 - val_loss: 0.6771 - val_acc: 0.5800

Epoch 00006: val_acc improved from 0.56894 to 0.58003, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window08/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 7/10

  64/5677 [..............................] - ETA: 11:18 - loss: 0.6658 - acc: 0.5781
 128/5677 [..............................] - ETA: 10:51 - loss: 0.6433 - acc: 0.6250
 192/5677 [>.............................] - ETA: 11:07 - loss: 0.6570 - acc: 0.5885
 256/5677 [>.............................] - ETA: 10:49 - loss: 0.6594 - acc: 0.5703
 320/5677 [>.............................] - ETA: 10:37 - loss: 0.6684 - acc: 0.5687
 384/5677 [=>............................] - ETA: 10:39 - loss: 0.6709 - acc: 0.5651
 448/5677 [=>............................] - ETA: 10:30 - loss: 0.6744 - acc: 0.5692
 512/5677 [=>............................] - ETA: 10:23 - loss: 0.6756 - acc: 0.5703
 576/5677 [==>...........................] - ETA: 10:11 - loss: 0.6739 - acc: 0.5712
 640/5677 [==>...........................] - ETA: 9:55 - loss: 0.6796 - acc: 0.5609 
 704/5677 [==>...........................] - ETA: 9:44 - loss: 0.6808 - acc: 0.5611
 768/5677 [===>..........................] - ETA: 9:35 - loss: 0.6830 - acc: 0.5573
 832/5677 [===>..........................] - ETA: 9:29 - loss: 0.6825 - acc: 0.5589
 896/5677 [===>..........................] - ETA: 9:17 - loss: 0.6824 - acc: 0.5603
 960/5677 [====>.........................] - ETA: 9:08 - loss: 0.6784 - acc: 0.5708
1024/5677 [====>.........................] - ETA: 9:05 - loss: 0.6777 - acc: 0.5674
1088/5677 [====>.........................] - ETA: 8:56 - loss: 0.6782 - acc: 0.5699
1152/5677 [=====>........................] - ETA: 8:48 - loss: 0.6784 - acc: 0.5668
1216/5677 [=====>........................] - ETA: 8:41 - loss: 0.6774 - acc: 0.5674
1280/5677 [=====>........................] - ETA: 8:34 - loss: 0.6762 - acc: 0.5687
1344/5677 [======>.......................] - ETA: 8:29 - loss: 0.6746 - acc: 0.5685
1408/5677 [======>.......................] - ETA: 8:23 - loss: 0.6751 - acc: 0.5689
1472/5677 [======>.......................] - ETA: 8:15 - loss: 0.6731 - acc: 0.5747
1536/5677 [=======>......................] - ETA: 8:08 - loss: 0.6732 - acc: 0.5762
1600/5677 [=======>......................] - ETA: 8:02 - loss: 0.6722 - acc: 0.5787
1664/5677 [=======>......................] - ETA: 7:53 - loss: 0.6708 - acc: 0.5811
1728/5677 [========>.....................] - ETA: 7:46 - loss: 0.6708 - acc: 0.5816
1792/5677 [========>.....................] - ETA: 7:39 - loss: 0.6697 - acc: 0.5843
1856/5677 [========>.....................] - ETA: 7:31 - loss: 0.6697 - acc: 0.5857
1920/5677 [=========>....................] - ETA: 7:23 - loss: 0.6691 - acc: 0.5854
1984/5677 [=========>....................] - ETA: 7:17 - loss: 0.6702 - acc: 0.5847
2048/5677 [=========>....................] - ETA: 7:07 - loss: 0.6699 - acc: 0.5854
2112/5677 [==========>...................] - ETA: 6:58 - loss: 0.6712 - acc: 0.5833
2176/5677 [==========>...................] - ETA: 6:50 - loss: 0.6712 - acc: 0.5827
2240/5677 [==========>...................] - ETA: 6:43 - loss: 0.6712 - acc: 0.5839
2304/5677 [===========>..................] - ETA: 6:35 - loss: 0.6720 - acc: 0.5829
2368/5677 [===========>..................] - ETA: 6:27 - loss: 0.6717 - acc: 0.5823
2432/5677 [===========>..................] - ETA: 6:20 - loss: 0.6714 - acc: 0.5839
2496/5677 [============>.................] - ETA: 6:13 - loss: 0.6705 - acc: 0.5845
2560/5677 [============>.................] - ETA: 6:05 - loss: 0.6719 - acc: 0.5820
2624/5677 [============>.................] - ETA: 5:58 - loss: 0.6723 - acc: 0.5793
2688/5677 [=============>................] - ETA: 5:51 - loss: 0.6718 - acc: 0.5815
2752/5677 [=============>................] - ETA: 5:43 - loss: 0.6711 - acc: 0.5832
2816/5677 [=============>................] - ETA: 5:35 - loss: 0.6710 - acc: 0.5838
2880/5677 [==============>...............] - ETA: 5:28 - loss: 0.6713 - acc: 0.5830
2944/5677 [==============>...............] - ETA: 5:20 - loss: 0.6721 - acc: 0.5829
3008/5677 [==============>...............] - ETA: 5:12 - loss: 0.6724 - acc: 0.5818
3072/5677 [===============>..............] - ETA: 5:04 - loss: 0.6725 - acc: 0.5807
3136/5677 [===============>..............] - ETA: 4:57 - loss: 0.6728 - acc: 0.5797
3200/5677 [===============>..............] - ETA: 4:50 - loss: 0.6721 - acc: 0.5809
3264/5677 [================>.............] - ETA: 4:43 - loss: 0.6725 - acc: 0.5815
3328/5677 [================>.............] - ETA: 4:36 - loss: 0.6727 - acc: 0.5811
3392/5677 [================>.............] - ETA: 4:29 - loss: 0.6736 - acc: 0.5796
3456/5677 [=================>............] - ETA: 4:21 - loss: 0.6738 - acc: 0.5796
3520/5677 [=================>............] - ETA: 4:13 - loss: 0.6737 - acc: 0.5801
3584/5677 [=================>............] - ETA: 4:05 - loss: 0.6738 - acc: 0.5809
3648/5677 [==================>...........] - ETA: 3:57 - loss: 0.6735 - acc: 0.5811
3712/5677 [==================>...........] - ETA: 3:49 - loss: 0.6740 - acc: 0.5814
3776/5677 [==================>...........] - ETA: 3:42 - loss: 0.6740 - acc: 0.5813
3840/5677 [===================>..........] - ETA: 3:34 - loss: 0.6735 - acc: 0.5807
3904/5677 [===================>..........] - ETA: 3:27 - loss: 0.6733 - acc: 0.5807
3968/5677 [===================>..........] - ETA: 3:19 - loss: 0.6729 - acc: 0.5811
4032/5677 [====================>.........] - ETA: 3:12 - loss: 0.6738 - acc: 0.5799
4096/5677 [====================>.........] - ETA: 3:04 - loss: 0.6743 - acc: 0.5784
4160/5677 [====================>.........] - ETA: 2:56 - loss: 0.6739 - acc: 0.5791
4224/5677 [=====================>........] - ETA: 2:49 - loss: 0.6743 - acc: 0.5784
4288/5677 [=====================>........] - ETA: 2:42 - loss: 0.6744 - acc: 0.5781
4352/5677 [=====================>........] - ETA: 2:34 - loss: 0.6741 - acc: 0.5790
4416/5677 [======================>.......] - ETA: 2:26 - loss: 0.6740 - acc: 0.5790
4480/5677 [======================>.......] - ETA: 2:19 - loss: 0.6749 - acc: 0.5772
4544/5677 [=======================>......] - ETA: 2:11 - loss: 0.6750 - acc: 0.5764
4608/5677 [=======================>......] - ETA: 2:04 - loss: 0.6752 - acc: 0.5764
4672/5677 [=======================>......] - ETA: 1:56 - loss: 0.6753 - acc: 0.5756
4736/5677 [========================>.....] - ETA: 1:49 - loss: 0.6756 - acc: 0.5754
4800/5677 [========================>.....] - ETA: 1:42 - loss: 0.6751 - acc: 0.5769
4864/5677 [========================>.....] - ETA: 1:34 - loss: 0.6749 - acc: 0.5773
4928/5677 [=========================>....] - ETA: 1:27 - loss: 0.6761 - acc: 0.5745
4992/5677 [=========================>....] - ETA: 1:19 - loss: 0.6758 - acc: 0.5757
5056/5677 [=========================>....] - ETA: 1:12 - loss: 0.6756 - acc: 0.5761
5120/5677 [==========================>...] - ETA: 1:04 - loss: 0.6758 - acc: 0.5756
5184/5677 [==========================>...] - ETA: 57s - loss: 0.6754 - acc: 0.5766 
5248/5677 [==========================>...] - ETA: 50s - loss: 0.6758 - acc: 0.5756
5312/5677 [===========================>..] - ETA: 42s - loss: 0.6757 - acc: 0.5745
5376/5677 [===========================>..] - ETA: 35s - loss: 0.6757 - acc: 0.5750
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6758 - acc: 0.5735
5504/5677 [============================>.] - ETA: 20s - loss: 0.6761 - acc: 0.5729
5568/5677 [============================>.] - ETA: 12s - loss: 0.6760 - acc: 0.5729
5632/5677 [============================>.] - ETA: 5s - loss: 0.6755 - acc: 0.5746 
5677/5677 [==============================] - 688s 121ms/step - loss: 0.6754 - acc: 0.5746 - val_loss: 0.6760 - val_acc: 0.5880

Epoch 00007: val_acc improved from 0.58003 to 0.58796, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window08/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 8/10

  64/5677 [..............................] - ETA: 10:02 - loss: 0.6514 - acc: 0.5938
 128/5677 [..............................] - ETA: 9:58 - loss: 0.6511 - acc: 0.5703 
 192/5677 [>.............................] - ETA: 10:09 - loss: 0.6526 - acc: 0.5781
 256/5677 [>.............................] - ETA: 10:10 - loss: 0.6617 - acc: 0.5625
 320/5677 [>.............................] - ETA: 10:15 - loss: 0.6606 - acc: 0.5781
 384/5677 [=>............................] - ETA: 9:57 - loss: 0.6671 - acc: 0.5729 
 448/5677 [=>............................] - ETA: 9:54 - loss: 0.6690 - acc: 0.5737
 512/5677 [=>............................] - ETA: 9:45 - loss: 0.6766 - acc: 0.5645
 576/5677 [==>...........................] - ETA: 9:40 - loss: 0.6801 - acc: 0.5625
 640/5677 [==>...........................] - ETA: 9:30 - loss: 0.6798 - acc: 0.5625
 704/5677 [==>...........................] - ETA: 9:25 - loss: 0.6787 - acc: 0.5696
 768/5677 [===>..........................] - ETA: 9:22 - loss: 0.6783 - acc: 0.5703
 832/5677 [===>..........................] - ETA: 9:16 - loss: 0.6753 - acc: 0.5769
 896/5677 [===>..........................] - ETA: 9:08 - loss: 0.6720 - acc: 0.5848
 960/5677 [====>.........................] - ETA: 9:03 - loss: 0.6747 - acc: 0.5771
1024/5677 [====>.........................] - ETA: 8:58 - loss: 0.6762 - acc: 0.5684
1088/5677 [====>.........................] - ETA: 8:53 - loss: 0.6775 - acc: 0.5689
1152/5677 [=====>........................] - ETA: 8:47 - loss: 0.6781 - acc: 0.5660
1216/5677 [=====>........................] - ETA: 8:38 - loss: 0.6780 - acc: 0.5674
1280/5677 [=====>........................] - ETA: 8:36 - loss: 0.6801 - acc: 0.5617
1344/5677 [======>.......................] - ETA: 8:26 - loss: 0.6782 - acc: 0.5647
1408/5677 [======>.......................] - ETA: 8:19 - loss: 0.6789 - acc: 0.5632
1472/5677 [======>.......................] - ETA: 8:10 - loss: 0.6788 - acc: 0.5639
1536/5677 [=======>......................] - ETA: 8:01 - loss: 0.6783 - acc: 0.5605
1600/5677 [=======>......................] - ETA: 7:54 - loss: 0.6780 - acc: 0.5600
1664/5677 [=======>......................] - ETA: 7:46 - loss: 0.6784 - acc: 0.5583
1728/5677 [========>.....................] - ETA: 7:38 - loss: 0.6777 - acc: 0.5584
1792/5677 [========>.....................] - ETA: 7:31 - loss: 0.6781 - acc: 0.5586
1856/5677 [========>.....................] - ETA: 7:22 - loss: 0.6765 - acc: 0.5625
1920/5677 [=========>....................] - ETA: 7:12 - loss: 0.6766 - acc: 0.5625
1984/5677 [=========>....................] - ETA: 7:05 - loss: 0.6745 - acc: 0.5655
2048/5677 [=========>....................] - ETA: 6:57 - loss: 0.6739 - acc: 0.5659
2112/5677 [==========>...................] - ETA: 6:48 - loss: 0.6740 - acc: 0.5668
2176/5677 [==========>...................] - ETA: 6:42 - loss: 0.6738 - acc: 0.5671
2240/5677 [==========>...................] - ETA: 6:35 - loss: 0.6745 - acc: 0.5665
2304/5677 [===========>..................] - ETA: 6:28 - loss: 0.6756 - acc: 0.5660
2368/5677 [===========>..................] - ETA: 6:20 - loss: 0.6741 - acc: 0.5671
2432/5677 [===========>..................] - ETA: 6:12 - loss: 0.6737 - acc: 0.5678
2496/5677 [============>.................] - ETA: 6:04 - loss: 0.6729 - acc: 0.5689
2560/5677 [============>.................] - ETA: 5:57 - loss: 0.6729 - acc: 0.5695
2624/5677 [============>.................] - ETA: 5:49 - loss: 0.6734 - acc: 0.5697
2688/5677 [=============>................] - ETA: 5:41 - loss: 0.6732 - acc: 0.5703
2752/5677 [=============>................] - ETA: 5:34 - loss: 0.6740 - acc: 0.5690
2816/5677 [=============>................] - ETA: 5:27 - loss: 0.6740 - acc: 0.5710
2880/5677 [==============>...............] - ETA: 5:20 - loss: 0.6739 - acc: 0.5705
2944/5677 [==============>...............] - ETA: 5:13 - loss: 0.6750 - acc: 0.5669
3008/5677 [==============>...............] - ETA: 5:06 - loss: 0.6739 - acc: 0.5705
3072/5677 [===============>..............] - ETA: 4:59 - loss: 0.6742 - acc: 0.5684
3136/5677 [===============>..............] - ETA: 4:51 - loss: 0.6744 - acc: 0.5695
3200/5677 [===============>..............] - ETA: 4:44 - loss: 0.6749 - acc: 0.5678
3264/5677 [================>.............] - ETA: 4:36 - loss: 0.6746 - acc: 0.5680
3328/5677 [================>.............] - ETA: 4:28 - loss: 0.6743 - acc: 0.5691
3392/5677 [================>.............] - ETA: 4:20 - loss: 0.6737 - acc: 0.5693
3456/5677 [=================>............] - ETA: 4:13 - loss: 0.6745 - acc: 0.5680
3520/5677 [=================>............] - ETA: 4:06 - loss: 0.6758 - acc: 0.5662
3584/5677 [=================>............] - ETA: 3:58 - loss: 0.6764 - acc: 0.5653
3648/5677 [==================>...........] - ETA: 3:51 - loss: 0.6767 - acc: 0.5647
3712/5677 [==================>...........] - ETA: 3:43 - loss: 0.6769 - acc: 0.5641
3776/5677 [==================>...........] - ETA: 3:36 - loss: 0.6767 - acc: 0.5641
3840/5677 [===================>..........] - ETA: 3:29 - loss: 0.6767 - acc: 0.5656
3904/5677 [===================>..........] - ETA: 3:21 - loss: 0.6767 - acc: 0.5658
3968/5677 [===================>..........] - ETA: 3:14 - loss: 0.6772 - acc: 0.5648
4032/5677 [====================>.........] - ETA: 3:07 - loss: 0.6769 - acc: 0.5652
4096/5677 [====================>.........] - ETA: 3:00 - loss: 0.6769 - acc: 0.5647
4160/5677 [====================>.........] - ETA: 2:52 - loss: 0.6774 - acc: 0.5627
4224/5677 [=====================>........] - ETA: 2:45 - loss: 0.6770 - acc: 0.5639
4288/5677 [=====================>........] - ETA: 2:38 - loss: 0.6766 - acc: 0.5651
4352/5677 [=====================>........] - ETA: 2:31 - loss: 0.6767 - acc: 0.5646
4416/5677 [======================>.......] - ETA: 2:24 - loss: 0.6767 - acc: 0.5654
4480/5677 [======================>.......] - ETA: 2:16 - loss: 0.6764 - acc: 0.5661
4544/5677 [=======================>......] - ETA: 2:09 - loss: 0.6772 - acc: 0.5645
4608/5677 [=======================>......] - ETA: 2:02 - loss: 0.6771 - acc: 0.5651
4672/5677 [=======================>......] - ETA: 1:54 - loss: 0.6767 - acc: 0.5659
4736/5677 [========================>.....] - ETA: 1:47 - loss: 0.6770 - acc: 0.5648
4800/5677 [========================>.....] - ETA: 1:40 - loss: 0.6768 - acc: 0.5654
4864/5677 [========================>.....] - ETA: 1:32 - loss: 0.6768 - acc: 0.5658
4928/5677 [=========================>....] - ETA: 1:25 - loss: 0.6774 - acc: 0.5655
4992/5677 [=========================>....] - ETA: 1:18 - loss: 0.6774 - acc: 0.5661
5056/5677 [=========================>....] - ETA: 1:10 - loss: 0.6769 - acc: 0.5667
5120/5677 [==========================>...] - ETA: 1:03 - loss: 0.6768 - acc: 0.5674
5184/5677 [==========================>...] - ETA: 56s - loss: 0.6766 - acc: 0.5681 
5248/5677 [==========================>...] - ETA: 48s - loss: 0.6766 - acc: 0.5684
5312/5677 [===========================>..] - ETA: 41s - loss: 0.6767 - acc: 0.5680
5376/5677 [===========================>..] - ETA: 34s - loss: 0.6765 - acc: 0.5688
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6765 - acc: 0.5691
5504/5677 [============================>.] - ETA: 19s - loss: 0.6764 - acc: 0.5694
5568/5677 [============================>.] - ETA: 12s - loss: 0.6766 - acc: 0.5690
5632/5677 [============================>.] - ETA: 5s - loss: 0.6763 - acc: 0.5698 
5677/5677 [==============================] - 675s 119ms/step - loss: 0.6764 - acc: 0.5695 - val_loss: 0.6759 - val_acc: 0.5911

Epoch 00008: val_acc improved from 0.58796 to 0.59113, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window08/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 9/10

  64/5677 [..............................] - ETA: 10:26 - loss: 0.6814 - acc: 0.5781
 128/5677 [..............................] - ETA: 10:51 - loss: 0.6823 - acc: 0.5781
 192/5677 [>.............................] - ETA: 10:15 - loss: 0.6789 - acc: 0.5833
 256/5677 [>.............................] - ETA: 9:56 - loss: 0.6732 - acc: 0.6016 
 320/5677 [>.............................] - ETA: 9:47 - loss: 0.6800 - acc: 0.5719
 384/5677 [=>............................] - ETA: 9:47 - loss: 0.6772 - acc: 0.5755
 448/5677 [=>............................] - ETA: 9:38 - loss: 0.6768 - acc: 0.5737
 512/5677 [=>............................] - ETA: 9:42 - loss: 0.6732 - acc: 0.5801
 576/5677 [==>...........................] - ETA: 9:30 - loss: 0.6698 - acc: 0.5955
 640/5677 [==>...........................] - ETA: 9:23 - loss: 0.6679 - acc: 0.5969
 704/5677 [==>...........................] - ETA: 9:15 - loss: 0.6663 - acc: 0.5994
 768/5677 [===>..........................] - ETA: 9:09 - loss: 0.6693 - acc: 0.5924
 832/5677 [===>..........................] - ETA: 8:58 - loss: 0.6712 - acc: 0.5865
 896/5677 [===>..........................] - ETA: 8:55 - loss: 0.6729 - acc: 0.5826
 960/5677 [====>.........................] - ETA: 8:48 - loss: 0.6744 - acc: 0.5792
1024/5677 [====>.........................] - ETA: 8:41 - loss: 0.6778 - acc: 0.5742
1088/5677 [====>.........................] - ETA: 8:36 - loss: 0.6775 - acc: 0.5754
1152/5677 [=====>........................] - ETA: 8:28 - loss: 0.6758 - acc: 0.5799
1216/5677 [=====>........................] - ETA: 8:23 - loss: 0.6771 - acc: 0.5765
1280/5677 [=====>........................] - ETA: 8:18 - loss: 0.6753 - acc: 0.5844
1344/5677 [======>.......................] - ETA: 8:10 - loss: 0.6735 - acc: 0.5871
1408/5677 [======>.......................] - ETA: 8:06 - loss: 0.6721 - acc: 0.5888
1472/5677 [======>.......................] - ETA: 7:59 - loss: 0.6711 - acc: 0.5904
1536/5677 [=======>......................] - ETA: 7:51 - loss: 0.6689 - acc: 0.5918
1600/5677 [=======>......................] - ETA: 7:42 - loss: 0.6692 - acc: 0.5919
1664/5677 [=======>......................] - ETA: 7:33 - loss: 0.6676 - acc: 0.5925
1728/5677 [========>.....................] - ETA: 7:26 - loss: 0.6672 - acc: 0.5943
1792/5677 [========>.....................] - ETA: 7:20 - loss: 0.6668 - acc: 0.5949
1856/5677 [========>.....................] - ETA: 7:12 - loss: 0.6678 - acc: 0.5943
1920/5677 [=========>....................] - ETA: 7:05 - loss: 0.6686 - acc: 0.5932
1984/5677 [=========>....................] - ETA: 6:58 - loss: 0.6697 - acc: 0.5917
2048/5677 [=========>....................] - ETA: 6:50 - loss: 0.6693 - acc: 0.5933
2112/5677 [==========>...................] - ETA: 6:43 - loss: 0.6706 - acc: 0.5928
2176/5677 [==========>...................] - ETA: 6:35 - loss: 0.6723 - acc: 0.5905
2240/5677 [==========>...................] - ETA: 6:29 - loss: 0.6730 - acc: 0.5902
2304/5677 [===========>..................] - ETA: 6:19 - loss: 0.6728 - acc: 0.5885
2368/5677 [===========>..................] - ETA: 6:11 - loss: 0.6745 - acc: 0.5861
2432/5677 [===========>..................] - ETA: 6:03 - loss: 0.6753 - acc: 0.5855
2496/5677 [============>.................] - ETA: 5:55 - loss: 0.6748 - acc: 0.5849
2560/5677 [============>.................] - ETA: 5:47 - loss: 0.6745 - acc: 0.5867
2624/5677 [============>.................] - ETA: 5:40 - loss: 0.6750 - acc: 0.5865
2688/5677 [=============>................] - ETA: 5:32 - loss: 0.6755 - acc: 0.5852
2752/5677 [=============>................] - ETA: 5:24 - loss: 0.6763 - acc: 0.5828
2816/5677 [=============>................] - ETA: 5:17 - loss: 0.6756 - acc: 0.5842
2880/5677 [==============>...............] - ETA: 5:09 - loss: 0.6759 - acc: 0.5826
2944/5677 [==============>...............] - ETA: 5:01 - loss: 0.6757 - acc: 0.5829
3008/5677 [==============>...............] - ETA: 4:53 - loss: 0.6754 - acc: 0.5834
3072/5677 [===============>..............] - ETA: 4:45 - loss: 0.6761 - acc: 0.5820
3136/5677 [===============>..............] - ETA: 4:37 - loss: 0.6771 - acc: 0.5791
3200/5677 [===============>..............] - ETA: 4:30 - loss: 0.6774 - acc: 0.5778
3264/5677 [================>.............] - ETA: 4:23 - loss: 0.6782 - acc: 0.5760
3328/5677 [================>.............] - ETA: 4:15 - loss: 0.6781 - acc: 0.5766
3392/5677 [================>.............] - ETA: 4:07 - loss: 0.6769 - acc: 0.5784
3456/5677 [=================>............] - ETA: 4:00 - loss: 0.6771 - acc: 0.5781
3520/5677 [=================>............] - ETA: 3:53 - loss: 0.6771 - acc: 0.5776
3584/5677 [=================>............] - ETA: 3:46 - loss: 0.6764 - acc: 0.5792
3648/5677 [==================>...........] - ETA: 3:38 - loss: 0.6764 - acc: 0.5787
3712/5677 [==================>...........] - ETA: 3:32 - loss: 0.6761 - acc: 0.5803
3776/5677 [==================>...........] - ETA: 3:24 - loss: 0.6763 - acc: 0.5802
3840/5677 [===================>..........] - ETA: 3:17 - loss: 0.6768 - acc: 0.5794
3904/5677 [===================>..........] - ETA: 3:11 - loss: 0.6769 - acc: 0.5791
3968/5677 [===================>..........] - ETA: 3:04 - loss: 0.6774 - acc: 0.5781
4032/5677 [====================>.........] - ETA: 2:57 - loss: 0.6774 - acc: 0.5781
4096/5677 [====================>.........] - ETA: 2:49 - loss: 0.6769 - acc: 0.5791
4160/5677 [====================>.........] - ETA: 2:42 - loss: 0.6766 - acc: 0.5808
4224/5677 [=====================>........] - ETA: 2:35 - loss: 0.6768 - acc: 0.5805
4288/5677 [=====================>........] - ETA: 2:28 - loss: 0.6767 - acc: 0.5807
4352/5677 [=====================>........] - ETA: 2:21 - loss: 0.6764 - acc: 0.5811
4416/5677 [======================>.......] - ETA: 2:14 - loss: 0.6765 - acc: 0.5813
4480/5677 [======================>.......] - ETA: 2:07 - loss: 0.6763 - acc: 0.5813
4544/5677 [=======================>......] - ETA: 2:00 - loss: 0.6760 - acc: 0.5819
4608/5677 [=======================>......] - ETA: 1:53 - loss: 0.6756 - acc: 0.5818
4672/5677 [=======================>......] - ETA: 1:46 - loss: 0.6756 - acc: 0.5813
4736/5677 [========================>.....] - ETA: 1:39 - loss: 0.6755 - acc: 0.5811
4800/5677 [========================>.....] - ETA: 1:32 - loss: 0.6753 - acc: 0.5817
4864/5677 [========================>.....] - ETA: 1:26 - loss: 0.6751 - acc: 0.5824
4928/5677 [=========================>....] - ETA: 1:19 - loss: 0.6746 - acc: 0.5838
4992/5677 [=========================>....] - ETA: 1:12 - loss: 0.6746 - acc: 0.5837
5056/5677 [=========================>....] - ETA: 1:05 - loss: 0.6751 - acc: 0.5833
5120/5677 [==========================>...] - ETA: 58s - loss: 0.6755 - acc: 0.5832 
5184/5677 [==========================>...] - ETA: 51s - loss: 0.6754 - acc: 0.5828
5248/5677 [==========================>...] - ETA: 45s - loss: 0.6754 - acc: 0.5829
5312/5677 [===========================>..] - ETA: 38s - loss: 0.6754 - acc: 0.5823
5376/5677 [===========================>..] - ETA: 31s - loss: 0.6754 - acc: 0.5818
5440/5677 [===========================>..] - ETA: 24s - loss: 0.6754 - acc: 0.5827
5504/5677 [============================>.] - ETA: 18s - loss: 0.6755 - acc: 0.5827
5568/5677 [============================>.] - ETA: 11s - loss: 0.6753 - acc: 0.5828
5632/5677 [============================>.] - ETA: 4s - loss: 0.6754 - acc: 0.5826 
5677/5677 [==============================] - 618s 109ms/step - loss: 0.6755 - acc: 0.5822 - val_loss: 0.7170 - val_acc: 0.5135

Epoch 00009: val_acc did not improve from 0.59113
Epoch 10/10

  64/5677 [..............................] - ETA: 12:06 - loss: 0.7000 - acc: 0.4844
 128/5677 [..............................] - ETA: 11:04 - loss: 0.6749 - acc: 0.5625
 192/5677 [>.............................] - ETA: 10:48 - loss: 0.6829 - acc: 0.5573
 256/5677 [>.............................] - ETA: 10:26 - loss: 0.6820 - acc: 0.5820
 320/5677 [>.............................] - ETA: 9:58 - loss: 0.6751 - acc: 0.5906 
 384/5677 [=>............................] - ETA: 9:42 - loss: 0.6702 - acc: 0.5911
 448/5677 [=>............................] - ETA: 9:26 - loss: 0.6668 - acc: 0.6004
 512/5677 [=>............................] - ETA: 9:13 - loss: 0.6646 - acc: 0.6113
 576/5677 [==>...........................] - ETA: 9:11 - loss: 0.6638 - acc: 0.6128
 640/5677 [==>...........................] - ETA: 8:57 - loss: 0.6649 - acc: 0.6094
 704/5677 [==>...........................] - ETA: 8:49 - loss: 0.6636 - acc: 0.6080
 768/5677 [===>..........................] - ETA: 8:41 - loss: 0.6654 - acc: 0.6068
 832/5677 [===>..........................] - ETA: 8:33 - loss: 0.6682 - acc: 0.6070
 896/5677 [===>..........................] - ETA: 8:23 - loss: 0.6666 - acc: 0.6083
 960/5677 [====>.........................] - ETA: 8:17 - loss: 0.6690 - acc: 0.6052
1024/5677 [====>.........................] - ETA: 8:07 - loss: 0.6703 - acc: 0.5986
1088/5677 [====>.........................] - ETA: 7:58 - loss: 0.6743 - acc: 0.5938
1152/5677 [=====>........................] - ETA: 7:53 - loss: 0.6745 - acc: 0.5903
1216/5677 [=====>........................] - ETA: 7:44 - loss: 0.6757 - acc: 0.5872
1280/5677 [=====>........................] - ETA: 7:39 - loss: 0.6743 - acc: 0.5898
1344/5677 [======>.......................] - ETA: 7:31 - loss: 0.6708 - acc: 0.5967
1408/5677 [======>.......................] - ETA: 7:24 - loss: 0.6720 - acc: 0.5966
1472/5677 [======>.......................] - ETA: 7:17 - loss: 0.6712 - acc: 0.6005
1536/5677 [=======>......................] - ETA: 7:13 - loss: 0.6707 - acc: 0.6016
1600/5677 [=======>......................] - ETA: 7:04 - loss: 0.6695 - acc: 0.6006
1664/5677 [=======>......................] - ETA: 6:58 - loss: 0.6705 - acc: 0.5980
1728/5677 [========>.....................] - ETA: 6:52 - loss: 0.6723 - acc: 0.5938
1792/5677 [========>.....................] - ETA: 6:44 - loss: 0.6734 - acc: 0.5898
1856/5677 [========>.....................] - ETA: 6:36 - loss: 0.6734 - acc: 0.5905
1920/5677 [=========>....................] - ETA: 6:29 - loss: 0.6740 - acc: 0.5896
1984/5677 [=========>....................] - ETA: 6:21 - loss: 0.6742 - acc: 0.5882
2048/5677 [=========>....................] - ETA: 6:13 - loss: 0.6750 - acc: 0.5879
2112/5677 [==========>...................] - ETA: 6:07 - loss: 0.6750 - acc: 0.5857
2176/5677 [==========>...................] - ETA: 6:01 - loss: 0.6741 - acc: 0.5864
2240/5677 [==========>...................] - ETA: 5:54 - loss: 0.6747 - acc: 0.5844
2304/5677 [===========>..................] - ETA: 5:48 - loss: 0.6741 - acc: 0.5833
2368/5677 [===========>..................] - ETA: 5:41 - loss: 0.6738 - acc: 0.5840
2432/5677 [===========>..................] - ETA: 5:35 - loss: 0.6746 - acc: 0.5814
2496/5677 [============>.................] - ETA: 5:27 - loss: 0.6749 - acc: 0.5817
2560/5677 [============>.................] - ETA: 5:20 - loss: 0.6750 - acc: 0.5813
2624/5677 [============>.................] - ETA: 5:13 - loss: 0.6754 - acc: 0.5793
2688/5677 [=============>................] - ETA: 5:07 - loss: 0.6750 - acc: 0.5807
2752/5677 [=============>................] - ETA: 4:59 - loss: 0.6743 - acc: 0.5825
2816/5677 [=============>................] - ETA: 4:53 - loss: 0.6748 - acc: 0.5817
2880/5677 [==============>...............] - ETA: 4:46 - loss: 0.6745 - acc: 0.5830
2944/5677 [==============>...............] - ETA: 4:39 - loss: 0.6737 - acc: 0.5853
3008/5677 [==============>...............] - ETA: 4:33 - loss: 0.6737 - acc: 0.5861
3072/5677 [===============>..............] - ETA: 4:27 - loss: 0.6739 - acc: 0.5863
3136/5677 [===============>..............] - ETA: 4:20 - loss: 0.6744 - acc: 0.5848
3200/5677 [===============>..............] - ETA: 4:14 - loss: 0.6745 - acc: 0.5847
3264/5677 [================>.............] - ETA: 4:07 - loss: 0.6746 - acc: 0.5836
3328/5677 [================>.............] - ETA: 4:00 - loss: 0.6747 - acc: 0.5835
3392/5677 [================>.............] - ETA: 3:53 - loss: 0.6746 - acc: 0.5840
3456/5677 [=================>............] - ETA: 3:46 - loss: 0.6750 - acc: 0.5836
3520/5677 [=================>............] - ETA: 3:40 - loss: 0.6749 - acc: 0.5838
3584/5677 [=================>............] - ETA: 3:34 - loss: 0.6748 - acc: 0.5843
3648/5677 [==================>...........] - ETA: 3:27 - loss: 0.6755 - acc: 0.5825
3712/5677 [==================>...........] - ETA: 3:22 - loss: 0.6758 - acc: 0.5811
3776/5677 [==================>...........] - ETA: 3:16 - loss: 0.6748 - acc: 0.5837
3840/5677 [===================>..........] - ETA: 3:10 - loss: 0.6744 - acc: 0.5831
3904/5677 [===================>..........] - ETA: 3:03 - loss: 0.6742 - acc: 0.5838
3968/5677 [===================>..........] - ETA: 2:56 - loss: 0.6736 - acc: 0.5834
4032/5677 [====================>.........] - ETA: 2:50 - loss: 0.6735 - acc: 0.5838
4096/5677 [====================>.........] - ETA: 2:43 - loss: 0.6731 - acc: 0.5847
4160/5677 [====================>.........] - ETA: 2:37 - loss: 0.6731 - acc: 0.5858
4224/5677 [=====================>........] - ETA: 2:31 - loss: 0.6729 - acc: 0.5859
4288/5677 [=====================>........] - ETA: 2:24 - loss: 0.6730 - acc: 0.5870
4352/5677 [=====================>........] - ETA: 2:17 - loss: 0.6730 - acc: 0.5864
4416/5677 [======================>.......] - ETA: 2:11 - loss: 0.6727 - acc: 0.5856
4480/5677 [======================>.......] - ETA: 2:04 - loss: 0.6729 - acc: 0.5853
4544/5677 [=======================>......] - ETA: 1:57 - loss: 0.6723 - acc: 0.5863
4608/5677 [=======================>......] - ETA: 1:51 - loss: 0.6720 - acc: 0.5864
4672/5677 [=======================>......] - ETA: 1:44 - loss: 0.6714 - acc: 0.5880
4736/5677 [========================>.....] - ETA: 1:37 - loss: 0.6713 - acc: 0.5878
4800/5677 [========================>.....] - ETA: 1:31 - loss: 0.6714 - acc: 0.5881
4864/5677 [========================>.....] - ETA: 1:24 - loss: 0.6709 - acc: 0.5892
4928/5677 [=========================>....] - ETA: 1:17 - loss: 0.6717 - acc: 0.5885
4992/5677 [=========================>....] - ETA: 1:11 - loss: 0.6723 - acc: 0.5881
5056/5677 [=========================>....] - ETA: 1:04 - loss: 0.6723 - acc: 0.5884
5120/5677 [==========================>...] - ETA: 57s - loss: 0.6722 - acc: 0.5891 
5184/5677 [==========================>...] - ETA: 51s - loss: 0.6721 - acc: 0.5887
5248/5677 [==========================>...] - ETA: 44s - loss: 0.6716 - acc: 0.5897
5312/5677 [===========================>..] - ETA: 37s - loss: 0.6721 - acc: 0.5889
5376/5677 [===========================>..] - ETA: 31s - loss: 0.6727 - acc: 0.5874
5440/5677 [===========================>..] - ETA: 24s - loss: 0.6722 - acc: 0.5875
5504/5677 [============================>.] - ETA: 17s - loss: 0.6720 - acc: 0.5883
5568/5677 [============================>.] - ETA: 11s - loss: 0.6718 - acc: 0.5882
5632/5677 [============================>.] - ETA: 4s - loss: 0.6725 - acc: 0.5866 
5677/5677 [==============================] - 616s 108ms/step - loss: 0.6722 - acc: 0.5864 - val_loss: 0.6929 - val_acc: 0.5610

Epoch 00010: val_acc did not improve from 0.59113
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f96f05e4d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f96f05e4d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f96f0447a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f96f0447a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db9ab5390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9db9ab5390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96f04d8510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96f04d8510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96d0797d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96d0797d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc244710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94fc244710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92e41657d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92e41657d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96f0378910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96f0378910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96d06a2950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96d06a2950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96f0048550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96f0048550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96d06ebb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96d06ebb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96d05432d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96d05432d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96d0673e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96d0673e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96d02d1810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96d02d1810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96d01e0610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96d01e0610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96d02ba5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96d02ba5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96d07244d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96d07244d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96d04bc890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96d04bc890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969c7c9c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969c7c9c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f969c662e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f969c662e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c76ed10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c76ed10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f969c791150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f969c791150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c5b7890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c5b7890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969c539450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969c539450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f969c33da90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f969c33da90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c25e890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c25e890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f969c446b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f969c446b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c369310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c369310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969c248f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969c248f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96807cbf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96807cbf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c1ee890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c1ee890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f969c1dfed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f969c1dfed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c18ffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f969c18ffd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969c33d290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f969c33d290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9680697d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9680697d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9680533390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9680533390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96805d0f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96805d0f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96803eef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96803eef90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9680312dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9680312dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9680282750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9680282750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f968018f590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f968018f590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96803e4990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96803e4990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96387c7f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96387c7f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96802dc410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96802dc410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96801e5ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96801e5ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96801922d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96801922d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96802dc350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96802dc350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96386b8fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96386b8fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f963848e890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f963848e890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f963863ee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f963863ee10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9638575990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9638575990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9638488350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9638488350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96383138d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96383138d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96383db8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96383db8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9638201050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9638201050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9638315d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9638315d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9638201b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9638201b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96186db450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96186db450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96185f2d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96185f2d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96184a3dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96184a3dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96184e9190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96184e9190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96185f21d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96185f21d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96184a3dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96184a3dd0>>: AttributeError: module 'gast' has no attribute 'Str'
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 5:34
 128/1578 [=>............................] - ETA: 3:14
 192/1578 [==>...........................] - ETA: 2:27
 256/1578 [===>..........................] - ETA: 2:09
 320/1578 [=====>........................] - ETA: 1:50
 384/1578 [======>.......................] - ETA: 1:36
 448/1578 [=======>......................] - ETA: 1:25
 512/1578 [========>.....................] - ETA: 1:16
 576/1578 [=========>....................] - ETA: 1:09
 640/1578 [===========>..................] - ETA: 1:02
 704/1578 [============>.................] - ETA: 57s 
 768/1578 [=============>................] - ETA: 51s
 832/1578 [==============>...............] - ETA: 46s
 896/1578 [================>.............] - ETA: 41s
 960/1578 [=================>............] - ETA: 36s
1024/1578 [==================>...........] - ETA: 32s
1088/1578 [===================>..........] - ETA: 28s
1152/1578 [====================>.........] - ETA: 24s
1216/1578 [======================>.......] - ETA: 20s
1280/1578 [=======================>......] - ETA: 16s
1344/1578 [========================>.....] - ETA: 12s
1408/1578 [=========================>....] - ETA: 9s 
1472/1578 [==========================>...] - ETA: 5s
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 83s 53ms/step
loss: 0.6799202836360617
acc: 0.5709759188091196
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9318553a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9318553a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f93184bd9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f93184bd9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d4240210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d4240210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93184c0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93184c0e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91d01e2090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91d01e2090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d4373ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d4373ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93184c0f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93184c0f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96f01d1bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96f01d1bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96f01b5650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96f01b5650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96f02ea510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f96f02ea510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96f02c8c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96f02c8c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96f03cdfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f96f03cdfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96f04ba7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f96f04ba7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96f031c910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f96f031c910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93181edd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93181edd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f931820ba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f931820ba50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9318335dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9318335dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f931817c490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f931817c490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93181ed7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93181ed7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9318533750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9318533750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93181e4250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93181e4250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f931065b590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f931065b590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93105576d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93105576d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93105539d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93105539d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93103d72d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93103d72d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9310676790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9310676790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9310553990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9310553990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93103d72d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93103d72d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f931019ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f931019ac50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f931019a0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f931019a0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93100d9350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93100d9350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93101c0b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93101c0b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93100e9390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93100e9390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93100654d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93100654d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f931005dad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f931005dad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92ec6ba110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92ec6ba110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9310065c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9310065c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92ec5b2c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92ec5b2c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92ec30b150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92ec30b150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92ec1d5ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92ec1d5ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92ec5d79d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92ec5d79d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f931019a050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f931019a050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92ec2520d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92ec2520d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92e474b2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92e474b2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92e46775d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92e46775d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92e4797610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92e4797610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92e474b290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92e474b290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92e46b0590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92e46b0590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92ec3e2610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92ec3e2610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92e431aad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92e431aad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92e4450c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92e4450c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92e463ef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92e463ef50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92e43ed810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92e43ed810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92e426bd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92e426bd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91d0076850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91d0076850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91d00af790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91d00af790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92e47b1910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92e47b1910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91d0039610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91d0039610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f919c60f550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f919c60f550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f919c6334d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f919c6334d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f919c4370d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f919c4370d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f919c63fe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f919c63fe50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91d014f4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91d014f4d0>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 1:12:40 - loss: 0.7363 - acc: 0.5781
 128/5677 [..............................] - ETA: 43:08 - loss: 0.7771 - acc: 0.5391  
 192/5677 [>.............................] - ETA: 33:06 - loss: 0.7663 - acc: 0.5156
 256/5677 [>.............................] - ETA: 27:43 - loss: 0.7679 - acc: 0.5156
 320/5677 [>.............................] - ETA: 24:25 - loss: 0.7733 - acc: 0.5094
 384/5677 [=>............................] - ETA: 22:13 - loss: 0.7781 - acc: 0.4974
 448/5677 [=>............................] - ETA: 20:37 - loss: 0.7688 - acc: 0.5089
 512/5677 [=>............................] - ETA: 19:27 - loss: 0.7637 - acc: 0.5176
 576/5677 [==>...........................] - ETA: 18:32 - loss: 0.7555 - acc: 0.5191
 640/5677 [==>...........................] - ETA: 17:40 - loss: 0.7496 - acc: 0.5219
 704/5677 [==>...........................] - ETA: 17:01 - loss: 0.7406 - acc: 0.5284
 768/5677 [===>..........................] - ETA: 16:29 - loss: 0.7381 - acc: 0.5299
 832/5677 [===>..........................] - ETA: 15:54 - loss: 0.7383 - acc: 0.5276
 896/5677 [===>..........................] - ETA: 15:19 - loss: 0.7372 - acc: 0.5246
 960/5677 [====>.........................] - ETA: 14:50 - loss: 0.7414 - acc: 0.5188
1024/5677 [====>.........................] - ETA: 14:28 - loss: 0.7411 - acc: 0.5156
1088/5677 [====>.........................] - ETA: 14:02 - loss: 0.7388 - acc: 0.5147
1152/5677 [=====>........................] - ETA: 13:40 - loss: 0.7388 - acc: 0.5104
1216/5677 [=====>........................] - ETA: 13:21 - loss: 0.7384 - acc: 0.5099
1280/5677 [=====>........................] - ETA: 13:05 - loss: 0.7369 - acc: 0.5125
1344/5677 [======>.......................] - ETA: 12:45 - loss: 0.7366 - acc: 0.5126
1408/5677 [======>.......................] - ETA: 12:29 - loss: 0.7351 - acc: 0.5142
1472/5677 [======>.......................] - ETA: 12:14 - loss: 0.7337 - acc: 0.5156
1536/5677 [=======>......................] - ETA: 11:56 - loss: 0.7339 - acc: 0.5111
1600/5677 [=======>......................] - ETA: 11:41 - loss: 0.7329 - acc: 0.5094
1664/5677 [=======>......................] - ETA: 11:23 - loss: 0.7338 - acc: 0.5078
1728/5677 [========>.....................] - ETA: 11:08 - loss: 0.7346 - acc: 0.5075
1792/5677 [========>.....................] - ETA: 10:51 - loss: 0.7341 - acc: 0.5067
1856/5677 [========>.....................] - ETA: 10:37 - loss: 0.7328 - acc: 0.5070
1920/5677 [=========>....................] - ETA: 10:25 - loss: 0.7308 - acc: 0.5083
1984/5677 [=========>....................] - ETA: 10:11 - loss: 0.7293 - acc: 0.5096
2048/5677 [=========>....................] - ETA: 9:56 - loss: 0.7283 - acc: 0.5107 
2112/5677 [==========>...................] - ETA: 9:44 - loss: 0.7277 - acc: 0.5099
2176/5677 [==========>...................] - ETA: 9:33 - loss: 0.7266 - acc: 0.5092
2240/5677 [==========>...................] - ETA: 9:20 - loss: 0.7264 - acc: 0.5098
2304/5677 [===========>..................] - ETA: 9:07 - loss: 0.7258 - acc: 0.5109
2368/5677 [===========>..................] - ETA: 8:53 - loss: 0.7261 - acc: 0.5114
2432/5677 [===========>..................] - ETA: 8:41 - loss: 0.7268 - acc: 0.5078
2496/5677 [============>.................] - ETA: 8:31 - loss: 0.7259 - acc: 0.5096
2560/5677 [============>.................] - ETA: 8:20 - loss: 0.7256 - acc: 0.5090
2624/5677 [============>.................] - ETA: 8:09 - loss: 0.7239 - acc: 0.5111
2688/5677 [=============>................] - ETA: 7:57 - loss: 0.7239 - acc: 0.5100
2752/5677 [=============>................] - ETA: 7:46 - loss: 0.7239 - acc: 0.5109
2816/5677 [=============>................] - ETA: 7:36 - loss: 0.7225 - acc: 0.5135
2880/5677 [==============>...............] - ETA: 7:25 - loss: 0.7222 - acc: 0.5132
2944/5677 [==============>...............] - ETA: 7:14 - loss: 0.7223 - acc: 0.5129
3008/5677 [==============>...............] - ETA: 7:02 - loss: 0.7224 - acc: 0.5126
3072/5677 [===============>..............] - ETA: 6:52 - loss: 0.7218 - acc: 0.5130
3136/5677 [===============>..............] - ETA: 6:42 - loss: 0.7211 - acc: 0.5131
3200/5677 [===============>..............] - ETA: 6:32 - loss: 0.7207 - acc: 0.5141
3264/5677 [================>.............] - ETA: 6:22 - loss: 0.7210 - acc: 0.5119
3328/5677 [================>.............] - ETA: 6:11 - loss: 0.7213 - acc: 0.5117
3392/5677 [================>.............] - ETA: 6:00 - loss: 0.7215 - acc: 0.5115
3456/5677 [=================>............] - ETA: 5:50 - loss: 0.7218 - acc: 0.5116
3520/5677 [=================>............] - ETA: 5:40 - loss: 0.7214 - acc: 0.5111
3584/5677 [=================>............] - ETA: 5:28 - loss: 0.7214 - acc: 0.5103
3648/5677 [==================>...........] - ETA: 5:18 - loss: 0.7210 - acc: 0.5118
3712/5677 [==================>...........] - ETA: 5:07 - loss: 0.7210 - acc: 0.5121
3776/5677 [==================>...........] - ETA: 4:57 - loss: 0.7211 - acc: 0.5106
3840/5677 [===================>..........] - ETA: 4:47 - loss: 0.7212 - acc: 0.5091
3904/5677 [===================>..........] - ETA: 4:36 - loss: 0.7208 - acc: 0.5092
3968/5677 [===================>..........] - ETA: 4:26 - loss: 0.7209 - acc: 0.5081
4032/5677 [====================>.........] - ETA: 4:15 - loss: 0.7200 - acc: 0.5087
4096/5677 [====================>.........] - ETA: 4:05 - loss: 0.7194 - acc: 0.5107
4160/5677 [====================>.........] - ETA: 3:55 - loss: 0.7191 - acc: 0.5111
4224/5677 [=====================>........] - ETA: 3:44 - loss: 0.7196 - acc: 0.5104
4288/5677 [=====================>........] - ETA: 3:34 - loss: 0.7192 - acc: 0.5114
4352/5677 [=====================>........] - ETA: 3:24 - loss: 0.7190 - acc: 0.5113
4416/5677 [======================>.......] - ETA: 3:13 - loss: 0.7183 - acc: 0.5120
4480/5677 [======================>.......] - ETA: 3:03 - loss: 0.7183 - acc: 0.5125
4544/5677 [=======================>......] - ETA: 2:53 - loss: 0.7188 - acc: 0.5112
4608/5677 [=======================>......] - ETA: 2:43 - loss: 0.7187 - acc: 0.5113
4672/5677 [=======================>......] - ETA: 2:33 - loss: 0.7184 - acc: 0.5116
4736/5677 [========================>.....] - ETA: 2:23 - loss: 0.7180 - acc: 0.5133
4800/5677 [========================>.....] - ETA: 2:13 - loss: 0.7182 - acc: 0.5121
4864/5677 [========================>.....] - ETA: 2:03 - loss: 0.7180 - acc: 0.5123
4928/5677 [=========================>....] - ETA: 1:53 - loss: 0.7173 - acc: 0.5136
4992/5677 [=========================>....] - ETA: 1:43 - loss: 0.7174 - acc: 0.5128
5056/5677 [=========================>....] - ETA: 1:33 - loss: 0.7172 - acc: 0.5133
5120/5677 [==========================>...] - ETA: 1:24 - loss: 0.7181 - acc: 0.5109
5184/5677 [==========================>...] - ETA: 1:14 - loss: 0.7180 - acc: 0.5114
5248/5677 [==========================>...] - ETA: 1:04 - loss: 0.7179 - acc: 0.5114
5312/5677 [===========================>..] - ETA: 54s - loss: 0.7173 - acc: 0.5128 
5376/5677 [===========================>..] - ETA: 45s - loss: 0.7167 - acc: 0.5132
5440/5677 [===========================>..] - ETA: 35s - loss: 0.7165 - acc: 0.5132
5504/5677 [============================>.] - ETA: 25s - loss: 0.7166 - acc: 0.5125
5568/5677 [============================>.] - ETA: 16s - loss: 0.7162 - acc: 0.5131
5632/5677 [============================>.] - ETA: 6s - loss: 0.7160 - acc: 0.5126 
5677/5677 [==============================] - 886s 156ms/step - loss: 0.7158 - acc: 0.5129 - val_loss: 0.6876 - val_acc: 0.5563

Epoch 00001: val_acc improved from -inf to 0.55626, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window09/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 13:09 - loss: 0.7078 - acc: 0.5000
 128/5677 [..............................] - ETA: 12:59 - loss: 0.6924 - acc: 0.5156
 192/5677 [>.............................] - ETA: 12:25 - loss: 0.6905 - acc: 0.5260
 256/5677 [>.............................] - ETA: 12:01 - loss: 0.6975 - acc: 0.5039
 320/5677 [>.............................] - ETA: 11:48 - loss: 0.7027 - acc: 0.5000
 384/5677 [=>............................] - ETA: 11:42 - loss: 0.7020 - acc: 0.4974
 448/5677 [=>............................] - ETA: 11:34 - loss: 0.7010 - acc: 0.4978
 512/5677 [=>............................] - ETA: 11:26 - loss: 0.7023 - acc: 0.4961
 576/5677 [==>...........................] - ETA: 11:10 - loss: 0.6985 - acc: 0.4983
 640/5677 [==>...........................] - ETA: 11:01 - loss: 0.7010 - acc: 0.4938
 704/5677 [==>...........................] - ETA: 10:52 - loss: 0.6992 - acc: 0.4972
 768/5677 [===>..........................] - ETA: 10:45 - loss: 0.6973 - acc: 0.5026
 832/5677 [===>..........................] - ETA: 10:34 - loss: 0.6968 - acc: 0.5072
 896/5677 [===>..........................] - ETA: 10:33 - loss: 0.6981 - acc: 0.5011
 960/5677 [====>.........................] - ETA: 10:24 - loss: 0.6965 - acc: 0.5000
1024/5677 [====>.........................] - ETA: 10:14 - loss: 0.6969 - acc: 0.5010
1088/5677 [====>.........................] - ETA: 10:03 - loss: 0.6968 - acc: 0.5037
1152/5677 [=====>........................] - ETA: 9:54 - loss: 0.6971 - acc: 0.5026 
1216/5677 [=====>........................] - ETA: 9:45 - loss: 0.6973 - acc: 0.5033
1280/5677 [=====>........................] - ETA: 9:35 - loss: 0.6981 - acc: 0.5008
1344/5677 [======>.......................] - ETA: 9:28 - loss: 0.6975 - acc: 0.5037
1408/5677 [======>.......................] - ETA: 9:23 - loss: 0.6952 - acc: 0.5099
1472/5677 [======>.......................] - ETA: 9:15 - loss: 0.6946 - acc: 0.5115
1536/5677 [=======>......................] - ETA: 9:09 - loss: 0.6938 - acc: 0.5137
1600/5677 [=======>......................] - ETA: 9:03 - loss: 0.6950 - acc: 0.5125
1664/5677 [=======>......................] - ETA: 8:55 - loss: 0.6989 - acc: 0.5066
1728/5677 [========>.....................] - ETA: 8:47 - loss: 0.6996 - acc: 0.5075
1792/5677 [========>.....................] - ETA: 8:40 - loss: 0.6986 - acc: 0.5112
1856/5677 [========>.....................] - ETA: 8:33 - loss: 0.6988 - acc: 0.5119
1920/5677 [=========>....................] - ETA: 8:25 - loss: 0.6976 - acc: 0.5161
1984/5677 [=========>....................] - ETA: 8:18 - loss: 0.6981 - acc: 0.5166
2048/5677 [=========>....................] - ETA: 8:09 - loss: 0.6963 - acc: 0.5205
2112/5677 [==========>...................] - ETA: 7:59 - loss: 0.6967 - acc: 0.5208
2176/5677 [==========>...................] - ETA: 7:51 - loss: 0.6964 - acc: 0.5230
2240/5677 [==========>...................] - ETA: 7:42 - loss: 0.6965 - acc: 0.5192
2304/5677 [===========>..................] - ETA: 7:34 - loss: 0.6975 - acc: 0.5178
2368/5677 [===========>..................] - ETA: 7:25 - loss: 0.6969 - acc: 0.5182
2432/5677 [===========>..................] - ETA: 7:16 - loss: 0.6968 - acc: 0.5189
2496/5677 [============>.................] - ETA: 7:06 - loss: 0.6969 - acc: 0.5200
2560/5677 [============>.................] - ETA: 6:57 - loss: 0.6961 - acc: 0.5215
2624/5677 [============>.................] - ETA: 6:49 - loss: 0.6962 - acc: 0.5210
2688/5677 [=============>................] - ETA: 6:40 - loss: 0.6953 - acc: 0.5234
2752/5677 [=============>................] - ETA: 6:31 - loss: 0.6955 - acc: 0.5243
2816/5677 [=============>................] - ETA: 6:23 - loss: 0.6952 - acc: 0.5238
2880/5677 [==============>...............] - ETA: 6:13 - loss: 0.6953 - acc: 0.5236
2944/5677 [==============>...............] - ETA: 6:03 - loss: 0.6951 - acc: 0.5251
3008/5677 [==============>...............] - ETA: 5:53 - loss: 0.6949 - acc: 0.5263
3072/5677 [===============>..............] - ETA: 5:45 - loss: 0.6947 - acc: 0.5267
3136/5677 [===============>..............] - ETA: 5:37 - loss: 0.6945 - acc: 0.5277
3200/5677 [===============>..............] - ETA: 5:30 - loss: 0.6941 - acc: 0.5288
3264/5677 [================>.............] - ETA: 5:21 - loss: 0.6937 - acc: 0.5294
3328/5677 [================>.............] - ETA: 5:12 - loss: 0.6939 - acc: 0.5279
3392/5677 [================>.............] - ETA: 5:03 - loss: 0.6934 - acc: 0.5295
3456/5677 [=================>............] - ETA: 4:54 - loss: 0.6925 - acc: 0.5321
3520/5677 [=================>............] - ETA: 4:46 - loss: 0.6928 - acc: 0.5318
3584/5677 [=================>............] - ETA: 4:37 - loss: 0.6923 - acc: 0.5340
3648/5677 [==================>...........] - ETA: 4:29 - loss: 0.6923 - acc: 0.5351
3712/5677 [==================>...........] - ETA: 4:20 - loss: 0.6920 - acc: 0.5358
3776/5677 [==================>...........] - ETA: 4:11 - loss: 0.6922 - acc: 0.5347
3840/5677 [===================>..........] - ETA: 4:02 - loss: 0.6924 - acc: 0.5344
3904/5677 [===================>..........] - ETA: 3:53 - loss: 0.6926 - acc: 0.5348
3968/5677 [===================>..........] - ETA: 3:45 - loss: 0.6917 - acc: 0.5370
4032/5677 [====================>.........] - ETA: 3:37 - loss: 0.6925 - acc: 0.5357
4096/5677 [====================>.........] - ETA: 3:28 - loss: 0.6929 - acc: 0.5352
4160/5677 [====================>.........] - ETA: 3:20 - loss: 0.6933 - acc: 0.5339
4224/5677 [=====================>........] - ETA: 3:11 - loss: 0.6935 - acc: 0.5331
4288/5677 [=====================>........] - ETA: 3:03 - loss: 0.6935 - acc: 0.5326
4352/5677 [=====================>........] - ETA: 2:55 - loss: 0.6939 - acc: 0.5333
4416/5677 [======================>.......] - ETA: 2:46 - loss: 0.6934 - acc: 0.5349
4480/5677 [======================>.......] - ETA: 2:37 - loss: 0.6936 - acc: 0.5339
4544/5677 [=======================>......] - ETA: 2:29 - loss: 0.6937 - acc: 0.5343
4608/5677 [=======================>......] - ETA: 2:21 - loss: 0.6934 - acc: 0.5341
4672/5677 [=======================>......] - ETA: 2:12 - loss: 0.6933 - acc: 0.5349
4736/5677 [========================>.....] - ETA: 2:04 - loss: 0.6936 - acc: 0.5348
4800/5677 [========================>.....] - ETA: 1:55 - loss: 0.6931 - acc: 0.5358
4864/5677 [========================>.....] - ETA: 1:47 - loss: 0.6931 - acc: 0.5366
4928/5677 [=========================>....] - ETA: 1:38 - loss: 0.6932 - acc: 0.5363
4992/5677 [=========================>....] - ETA: 1:30 - loss: 0.6933 - acc: 0.5361
5056/5677 [=========================>....] - ETA: 1:21 - loss: 0.6937 - acc: 0.5352
5120/5677 [==========================>...] - ETA: 1:13 - loss: 0.6936 - acc: 0.5363
5184/5677 [==========================>...] - ETA: 1:05 - loss: 0.6938 - acc: 0.5363
5248/5677 [==========================>...] - ETA: 56s - loss: 0.6936 - acc: 0.5366 
5312/5677 [===========================>..] - ETA: 48s - loss: 0.6938 - acc: 0.5358
5376/5677 [===========================>..] - ETA: 39s - loss: 0.6943 - acc: 0.5339
5440/5677 [===========================>..] - ETA: 31s - loss: 0.6944 - acc: 0.5342
5504/5677 [============================>.] - ETA: 22s - loss: 0.6944 - acc: 0.5340
5568/5677 [============================>.] - ETA: 14s - loss: 0.6943 - acc: 0.5345
5632/5677 [============================>.] - ETA: 5s - loss: 0.6942 - acc: 0.5341 
5677/5677 [==============================] - 780s 137ms/step - loss: 0.6942 - acc: 0.5343 - val_loss: 0.6817 - val_acc: 0.5626

Epoch 00002: val_acc improved from 0.55626 to 0.56260, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window09/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 13:12 - loss: 0.6569 - acc: 0.5938
 128/5677 [..............................] - ETA: 12:47 - loss: 0.6931 - acc: 0.4844
 192/5677 [>.............................] - ETA: 12:34 - loss: 0.6953 - acc: 0.4792
 256/5677 [>.............................] - ETA: 12:19 - loss: 0.7036 - acc: 0.4961
 320/5677 [>.............................] - ETA: 12:20 - loss: 0.6998 - acc: 0.5156
 384/5677 [=>............................] - ETA: 12:25 - loss: 0.6962 - acc: 0.5286
 448/5677 [=>............................] - ETA: 12:17 - loss: 0.6918 - acc: 0.5446
 512/5677 [=>............................] - ETA: 12:07 - loss: 0.6894 - acc: 0.5508
 576/5677 [==>...........................] - ETA: 11:57 - loss: 0.6898 - acc: 0.5469
 640/5677 [==>...........................] - ETA: 11:38 - loss: 0.6914 - acc: 0.5391
 704/5677 [==>...........................] - ETA: 11:24 - loss: 0.6887 - acc: 0.5455
 768/5677 [===>..........................] - ETA: 11:13 - loss: 0.6890 - acc: 0.5443
 832/5677 [===>..........................] - ETA: 10:54 - loss: 0.6886 - acc: 0.5445
 896/5677 [===>..........................] - ETA: 10:43 - loss: 0.6861 - acc: 0.5536
 960/5677 [====>.........................] - ETA: 10:35 - loss: 0.6835 - acc: 0.5625
1024/5677 [====>.........................] - ETA: 10:24 - loss: 0.6837 - acc: 0.5576
1088/5677 [====>.........................] - ETA: 10:16 - loss: 0.6818 - acc: 0.5625
1152/5677 [=====>........................] - ETA: 10:04 - loss: 0.6817 - acc: 0.5642
1216/5677 [=====>........................] - ETA: 9:55 - loss: 0.6829 - acc: 0.5633 
1280/5677 [=====>........................] - ETA: 9:43 - loss: 0.6832 - acc: 0.5617
1344/5677 [======>.......................] - ETA: 9:33 - loss: 0.6818 - acc: 0.5670
1408/5677 [======>.......................] - ETA: 9:23 - loss: 0.6825 - acc: 0.5668
1472/5677 [======>.......................] - ETA: 9:16 - loss: 0.6826 - acc: 0.5673
1536/5677 [=======>......................] - ETA: 9:08 - loss: 0.6837 - acc: 0.5618
1600/5677 [=======>......................] - ETA: 8:59 - loss: 0.6839 - acc: 0.5606
1664/5677 [=======>......................] - ETA: 8:51 - loss: 0.6844 - acc: 0.5607
1728/5677 [========>.....................] - ETA: 8:43 - loss: 0.6840 - acc: 0.5608
1792/5677 [========>.....................] - ETA: 8:36 - loss: 0.6845 - acc: 0.5586
1856/5677 [========>.....................] - ETA: 8:29 - loss: 0.6832 - acc: 0.5625
1920/5677 [=========>....................] - ETA: 8:22 - loss: 0.6855 - acc: 0.5573
1984/5677 [=========>....................] - ETA: 8:14 - loss: 0.6848 - acc: 0.5595
2048/5677 [=========>....................] - ETA: 8:05 - loss: 0.6847 - acc: 0.5596
2112/5677 [==========>...................] - ETA: 7:59 - loss: 0.6838 - acc: 0.5620
2176/5677 [==========>...................] - ETA: 7:51 - loss: 0.6836 - acc: 0.5620
2240/5677 [==========>...................] - ETA: 7:43 - loss: 0.6825 - acc: 0.5643
2304/5677 [===========>..................] - ETA: 7:36 - loss: 0.6812 - acc: 0.5681
2368/5677 [===========>..................] - ETA: 7:28 - loss: 0.6811 - acc: 0.5680
2432/5677 [===========>..................] - ETA: 7:19 - loss: 0.6810 - acc: 0.5678
2496/5677 [============>.................] - ETA: 7:09 - loss: 0.6807 - acc: 0.5681
2560/5677 [============>.................] - ETA: 7:00 - loss: 0.6804 - acc: 0.5676
2624/5677 [============>.................] - ETA: 6:52 - loss: 0.6808 - acc: 0.5678
2688/5677 [=============>................] - ETA: 6:43 - loss: 0.6797 - acc: 0.5696
2752/5677 [=============>................] - ETA: 6:34 - loss: 0.6796 - acc: 0.5701
2816/5677 [=============>................] - ETA: 6:26 - loss: 0.6797 - acc: 0.5685
2880/5677 [==============>...............] - ETA: 6:17 - loss: 0.6797 - acc: 0.5674
2944/5677 [==============>...............] - ETA: 6:10 - loss: 0.6796 - acc: 0.5683
3008/5677 [==============>...............] - ETA: 6:01 - loss: 0.6791 - acc: 0.5682
3072/5677 [===============>..............] - ETA: 5:53 - loss: 0.6788 - acc: 0.5680
3136/5677 [===============>..............] - ETA: 5:45 - loss: 0.6789 - acc: 0.5682
3200/5677 [===============>..............] - ETA: 5:36 - loss: 0.6794 - acc: 0.5678
3264/5677 [================>.............] - ETA: 5:28 - loss: 0.6802 - acc: 0.5665
3328/5677 [================>.............] - ETA: 5:19 - loss: 0.6793 - acc: 0.5685
3392/5677 [================>.............] - ETA: 5:11 - loss: 0.6802 - acc: 0.5672
3456/5677 [=================>............] - ETA: 5:02 - loss: 0.6805 - acc: 0.5663
3520/5677 [=================>............] - ETA: 4:53 - loss: 0.6803 - acc: 0.5662
3584/5677 [=================>............] - ETA: 4:44 - loss: 0.6808 - acc: 0.5656
3648/5677 [==================>...........] - ETA: 4:36 - loss: 0.6808 - acc: 0.5641
3712/5677 [==================>...........] - ETA: 4:27 - loss: 0.6810 - acc: 0.5641
3776/5677 [==================>...........] - ETA: 4:19 - loss: 0.6814 - acc: 0.5625
3840/5677 [===================>..........] - ETA: 4:10 - loss: 0.6811 - acc: 0.5633
3904/5677 [===================>..........] - ETA: 4:01 - loss: 0.6812 - acc: 0.5628
3968/5677 [===================>..........] - ETA: 3:52 - loss: 0.6808 - acc: 0.5625
4032/5677 [====================>.........] - ETA: 3:44 - loss: 0.6805 - acc: 0.5627
4096/5677 [====================>.........] - ETA: 3:35 - loss: 0.6804 - acc: 0.5632
4160/5677 [====================>.........] - ETA: 3:26 - loss: 0.6803 - acc: 0.5627
4224/5677 [=====================>........] - ETA: 3:18 - loss: 0.6798 - acc: 0.5646
4288/5677 [=====================>........] - ETA: 3:09 - loss: 0.6805 - acc: 0.5639
4352/5677 [=====================>........] - ETA: 3:00 - loss: 0.6806 - acc: 0.5639
4416/5677 [======================>.......] - ETA: 2:51 - loss: 0.6809 - acc: 0.5634
4480/5677 [======================>.......] - ETA: 2:42 - loss: 0.6814 - acc: 0.5623
4544/5677 [=======================>......] - ETA: 2:33 - loss: 0.6815 - acc: 0.5618
4608/5677 [=======================>......] - ETA: 2:25 - loss: 0.6820 - acc: 0.5623
4672/5677 [=======================>......] - ETA: 2:16 - loss: 0.6824 - acc: 0.5614
4736/5677 [========================>.....] - ETA: 2:07 - loss: 0.6826 - acc: 0.5606
4800/5677 [========================>.....] - ETA: 1:59 - loss: 0.6825 - acc: 0.5610
4864/5677 [========================>.....] - ETA: 1:50 - loss: 0.6822 - acc: 0.5617
4928/5677 [=========================>....] - ETA: 1:41 - loss: 0.6827 - acc: 0.5601
4992/5677 [=========================>....] - ETA: 1:33 - loss: 0.6828 - acc: 0.5589
5056/5677 [=========================>....] - ETA: 1:24 - loss: 0.6829 - acc: 0.5581
5120/5677 [==========================>...] - ETA: 1:15 - loss: 0.6829 - acc: 0.5584
5184/5677 [==========================>...] - ETA: 1:07 - loss: 0.6831 - acc: 0.5577
5248/5677 [==========================>...] - ETA: 58s - loss: 0.6832 - acc: 0.5577 
5312/5677 [===========================>..] - ETA: 49s - loss: 0.6834 - acc: 0.5582
5376/5677 [===========================>..] - ETA: 40s - loss: 0.6832 - acc: 0.5588
5440/5677 [===========================>..] - ETA: 32s - loss: 0.6836 - acc: 0.5579
5504/5677 [============================>.] - ETA: 23s - loss: 0.6835 - acc: 0.5587
5568/5677 [============================>.] - ETA: 14s - loss: 0.6835 - acc: 0.5584
5632/5677 [============================>.] - ETA: 6s - loss: 0.6836 - acc: 0.5589 
5677/5677 [==============================] - 801s 141ms/step - loss: 0.6840 - acc: 0.5584 - val_loss: 0.6988 - val_acc: 0.5230

Epoch 00003: val_acc did not improve from 0.56260
Epoch 4/10

  64/5677 [..............................] - ETA: 12:48 - loss: 0.7385 - acc: 0.5312
 128/5677 [..............................] - ETA: 12:50 - loss: 0.7244 - acc: 0.5156
 192/5677 [>.............................] - ETA: 12:24 - loss: 0.7003 - acc: 0.5781
 256/5677 [>.............................] - ETA: 12:18 - loss: 0.6999 - acc: 0.5352
 320/5677 [>.............................] - ETA: 12:05 - loss: 0.6937 - acc: 0.5594
 384/5677 [=>............................] - ETA: 11:58 - loss: 0.6899 - acc: 0.5625
 448/5677 [=>............................] - ETA: 11:48 - loss: 0.6891 - acc: 0.5536
 512/5677 [=>............................] - ETA: 11:39 - loss: 0.6856 - acc: 0.5586
 576/5677 [==>...........................] - ETA: 11:25 - loss: 0.6887 - acc: 0.5556
 640/5677 [==>...........................] - ETA: 11:22 - loss: 0.6840 - acc: 0.5656
 704/5677 [==>...........................] - ETA: 11:15 - loss: 0.6807 - acc: 0.5724
 768/5677 [===>..........................] - ETA: 11:00 - loss: 0.6809 - acc: 0.5664
 832/5677 [===>..........................] - ETA: 10:47 - loss: 0.6835 - acc: 0.5601
 896/5677 [===>..........................] - ETA: 10:32 - loss: 0.6847 - acc: 0.5625
 960/5677 [====>.........................] - ETA: 10:19 - loss: 0.6824 - acc: 0.5667
1024/5677 [====>.........................] - ETA: 10:13 - loss: 0.6823 - acc: 0.5635
1088/5677 [====>.........................] - ETA: 10:00 - loss: 0.6815 - acc: 0.5653
1152/5677 [=====>........................] - ETA: 9:52 - loss: 0.6816 - acc: 0.5634 
1216/5677 [=====>........................] - ETA: 9:44 - loss: 0.6825 - acc: 0.5609
1280/5677 [=====>........................] - ETA: 9:34 - loss: 0.6818 - acc: 0.5633
1344/5677 [======>.......................] - ETA: 9:25 - loss: 0.6816 - acc: 0.5662
1408/5677 [======>.......................] - ETA: 9:18 - loss: 0.6808 - acc: 0.5682
1472/5677 [======>.......................] - ETA: 9:11 - loss: 0.6818 - acc: 0.5639
1536/5677 [=======>......................] - ETA: 9:01 - loss: 0.6817 - acc: 0.5645
1600/5677 [=======>......................] - ETA: 8:51 - loss: 0.6817 - acc: 0.5631
1664/5677 [=======>......................] - ETA: 8:43 - loss: 0.6818 - acc: 0.5613
1728/5677 [========>.....................] - ETA: 8:32 - loss: 0.6816 - acc: 0.5613
1792/5677 [========>.....................] - ETA: 8:24 - loss: 0.6818 - acc: 0.5597
1856/5677 [========>.....................] - ETA: 8:15 - loss: 0.6823 - acc: 0.5587
1920/5677 [=========>....................] - ETA: 8:06 - loss: 0.6820 - acc: 0.5594
1984/5677 [=========>....................] - ETA: 7:58 - loss: 0.6815 - acc: 0.5590
2048/5677 [=========>....................] - ETA: 7:50 - loss: 0.6821 - acc: 0.5571
2112/5677 [==========>...................] - ETA: 7:42 - loss: 0.6817 - acc: 0.5601
2176/5677 [==========>...................] - ETA: 7:35 - loss: 0.6825 - acc: 0.5579
2240/5677 [==========>...................] - ETA: 7:28 - loss: 0.6819 - acc: 0.5576
2304/5677 [===========>..................] - ETA: 7:20 - loss: 0.6815 - acc: 0.5586
2368/5677 [===========>..................] - ETA: 7:12 - loss: 0.6812 - acc: 0.5579
2432/5677 [===========>..................] - ETA: 7:03 - loss: 0.6802 - acc: 0.5604
2496/5677 [============>.................] - ETA: 6:55 - loss: 0.6808 - acc: 0.5597
2560/5677 [============>.................] - ETA: 6:46 - loss: 0.6804 - acc: 0.5609
2624/5677 [============>.................] - ETA: 6:39 - loss: 0.6806 - acc: 0.5614
2688/5677 [=============>................] - ETA: 6:30 - loss: 0.6807 - acc: 0.5614
2752/5677 [=============>................] - ETA: 6:21 - loss: 0.6803 - acc: 0.5629
2816/5677 [=============>................] - ETA: 6:12 - loss: 0.6805 - acc: 0.5618
2880/5677 [==============>...............] - ETA: 6:04 - loss: 0.6811 - acc: 0.5618
2944/5677 [==============>...............] - ETA: 5:55 - loss: 0.6809 - acc: 0.5625
3008/5677 [==============>...............] - ETA: 5:45 - loss: 0.6806 - acc: 0.5632
3072/5677 [===============>..............] - ETA: 5:37 - loss: 0.6819 - acc: 0.5599
3136/5677 [===============>..............] - ETA: 5:28 - loss: 0.6821 - acc: 0.5606
3200/5677 [===============>..............] - ETA: 5:20 - loss: 0.6825 - acc: 0.5594
3264/5677 [================>.............] - ETA: 5:13 - loss: 0.6831 - acc: 0.5582
3328/5677 [================>.............] - ETA: 5:06 - loss: 0.6837 - acc: 0.5562
3392/5677 [================>.............] - ETA: 4:59 - loss: 0.6846 - acc: 0.5548
3456/5677 [=================>............] - ETA: 4:51 - loss: 0.6851 - acc: 0.5535
3520/5677 [=================>............] - ETA: 4:42 - loss: 0.6860 - acc: 0.5509
3584/5677 [=================>............] - ETA: 4:34 - loss: 0.6860 - acc: 0.5511
3648/5677 [==================>...........] - ETA: 4:26 - loss: 0.6856 - acc: 0.5521
3712/5677 [==================>...........] - ETA: 4:17 - loss: 0.6859 - acc: 0.5512
3776/5677 [==================>...........] - ETA: 4:09 - loss: 0.6858 - acc: 0.5516
3840/5677 [===================>..........] - ETA: 4:02 - loss: 0.6860 - acc: 0.5505
3904/5677 [===================>..........] - ETA: 3:53 - loss: 0.6861 - acc: 0.5502
3968/5677 [===================>..........] - ETA: 3:45 - loss: 0.6863 - acc: 0.5494
4032/5677 [====================>.........] - ETA: 3:36 - loss: 0.6862 - acc: 0.5494
4096/5677 [====================>.........] - ETA: 3:29 - loss: 0.6864 - acc: 0.5496
4160/5677 [====================>.........] - ETA: 3:20 - loss: 0.6872 - acc: 0.5486
4224/5677 [=====================>........] - ETA: 3:11 - loss: 0.6867 - acc: 0.5507
4288/5677 [=====================>........] - ETA: 3:03 - loss: 0.6872 - acc: 0.5501
4352/5677 [=====================>........] - ETA: 2:55 - loss: 0.6867 - acc: 0.5515
4416/5677 [======================>.......] - ETA: 2:46 - loss: 0.6869 - acc: 0.5510
4480/5677 [======================>.......] - ETA: 2:38 - loss: 0.6864 - acc: 0.5520
4544/5677 [=======================>......] - ETA: 2:30 - loss: 0.6863 - acc: 0.5519
4608/5677 [=======================>......] - ETA: 2:21 - loss: 0.6859 - acc: 0.5534
4672/5677 [=======================>......] - ETA: 2:13 - loss: 0.6853 - acc: 0.5544
4736/5677 [========================>.....] - ETA: 2:05 - loss: 0.6847 - acc: 0.5557
4800/5677 [========================>.....] - ETA: 1:56 - loss: 0.6847 - acc: 0.5552
4864/5677 [========================>.....] - ETA: 1:48 - loss: 0.6847 - acc: 0.5551
4928/5677 [=========================>....] - ETA: 1:39 - loss: 0.6847 - acc: 0.5552
4992/5677 [=========================>....] - ETA: 1:31 - loss: 0.6845 - acc: 0.5559
5056/5677 [=========================>....] - ETA: 1:22 - loss: 0.6852 - acc: 0.5542
5120/5677 [==========================>...] - ETA: 1:13 - loss: 0.6848 - acc: 0.5555
5184/5677 [==========================>...] - ETA: 1:05 - loss: 0.6845 - acc: 0.5563
5248/5677 [==========================>...] - ETA: 56s - loss: 0.6846 - acc: 0.5556 
5312/5677 [===========================>..] - ETA: 48s - loss: 0.6840 - acc: 0.5574
5376/5677 [===========================>..] - ETA: 39s - loss: 0.6838 - acc: 0.5578
5440/5677 [===========================>..] - ETA: 31s - loss: 0.6837 - acc: 0.5577
5504/5677 [============================>.] - ETA: 22s - loss: 0.6836 - acc: 0.5578
5568/5677 [============================>.] - ETA: 14s - loss: 0.6835 - acc: 0.5577
5632/5677 [============================>.] - ETA: 5s - loss: 0.6842 - acc: 0.5565 
5677/5677 [==============================] - 781s 138ms/step - loss: 0.6840 - acc: 0.5566 - val_loss: 0.7049 - val_acc: 0.5055

Epoch 00004: val_acc did not improve from 0.56260
Epoch 5/10

  64/5677 [..............................] - ETA: 12:35 - loss: 0.6783 - acc: 0.5938
 128/5677 [..............................] - ETA: 12:52 - loss: 0.6788 - acc: 0.5703
 192/5677 [>.............................] - ETA: 12:19 - loss: 0.6857 - acc: 0.5469
 256/5677 [>.............................] - ETA: 12:00 - loss: 0.6882 - acc: 0.5391
 320/5677 [>.............................] - ETA: 11:47 - loss: 0.6905 - acc: 0.5437
 384/5677 [=>............................] - ETA: 11:31 - loss: 0.6957 - acc: 0.5391
 448/5677 [=>............................] - ETA: 11:22 - loss: 0.6903 - acc: 0.5469
 512/5677 [=>............................] - ETA: 11:19 - loss: 0.6888 - acc: 0.5566
 576/5677 [==>...........................] - ETA: 11:14 - loss: 0.6907 - acc: 0.5608
 640/5677 [==>...........................] - ETA: 11:04 - loss: 0.6904 - acc: 0.5594
 704/5677 [==>...........................] - ETA: 10:47 - loss: 0.6879 - acc: 0.5639
 768/5677 [===>..........................] - ETA: 10:38 - loss: 0.6890 - acc: 0.5612
 832/5677 [===>..........................] - ETA: 10:29 - loss: 0.6951 - acc: 0.5469
 896/5677 [===>..........................] - ETA: 10:17 - loss: 0.6904 - acc: 0.5569
 960/5677 [====>.........................] - ETA: 10:13 - loss: 0.6901 - acc: 0.5594
1024/5677 [====>.........................] - ETA: 10:04 - loss: 0.6887 - acc: 0.5596
1088/5677 [====>.........................] - ETA: 9:53 - loss: 0.6881 - acc: 0.5597 
1152/5677 [=====>........................] - ETA: 9:45 - loss: 0.6875 - acc: 0.5616
1216/5677 [=====>........................] - ETA: 9:38 - loss: 0.6865 - acc: 0.5617
1280/5677 [=====>........................] - ETA: 9:28 - loss: 0.6863 - acc: 0.5664
1344/5677 [======>.......................] - ETA: 9:19 - loss: 0.6858 - acc: 0.5692
1408/5677 [======>.......................] - ETA: 9:12 - loss: 0.6858 - acc: 0.5682
1472/5677 [======>.......................] - ETA: 9:03 - loss: 0.6852 - acc: 0.5673
1536/5677 [=======>......................] - ETA: 8:57 - loss: 0.6869 - acc: 0.5638
1600/5677 [=======>......................] - ETA: 8:47 - loss: 0.6853 - acc: 0.5687
1664/5677 [=======>......................] - ETA: 8:36 - loss: 0.6847 - acc: 0.5685
1728/5677 [========>.....................] - ETA: 8:27 - loss: 0.6836 - acc: 0.5712
1792/5677 [========>.....................] - ETA: 8:17 - loss: 0.6846 - acc: 0.5686
1856/5677 [========>.....................] - ETA: 8:08 - loss: 0.6848 - acc: 0.5679
1920/5677 [=========>....................] - ETA: 8:01 - loss: 0.6845 - acc: 0.5687
1984/5677 [=========>....................] - ETA: 7:53 - loss: 0.6839 - acc: 0.5691
2048/5677 [=========>....................] - ETA: 7:46 - loss: 0.6836 - acc: 0.5698
2112/5677 [==========>...................] - ETA: 7:39 - loss: 0.6826 - acc: 0.5710
2176/5677 [==========>...................] - ETA: 7:32 - loss: 0.6832 - acc: 0.5708
2240/5677 [==========>...................] - ETA: 7:25 - loss: 0.6843 - acc: 0.5679
2304/5677 [===========>..................] - ETA: 7:17 - loss: 0.6837 - acc: 0.5677
2368/5677 [===========>..................] - ETA: 7:08 - loss: 0.6844 - acc: 0.5659
2432/5677 [===========>..................] - ETA: 7:00 - loss: 0.6842 - acc: 0.5674
2496/5677 [============>.................] - ETA: 6:52 - loss: 0.6841 - acc: 0.5689
2560/5677 [============>.................] - ETA: 6:44 - loss: 0.6836 - acc: 0.5703
2624/5677 [============>.................] - ETA: 6:35 - loss: 0.6837 - acc: 0.5701
2688/5677 [=============>................] - ETA: 6:28 - loss: 0.6833 - acc: 0.5685
2752/5677 [=============>................] - ETA: 6:20 - loss: 0.6821 - acc: 0.5712
2816/5677 [=============>................] - ETA: 6:11 - loss: 0.6818 - acc: 0.5735
2880/5677 [==============>...............] - ETA: 6:03 - loss: 0.6823 - acc: 0.5733
2944/5677 [==============>...............] - ETA: 5:54 - loss: 0.6817 - acc: 0.5744
3008/5677 [==============>...............] - ETA: 5:46 - loss: 0.6819 - acc: 0.5735
3072/5677 [===============>..............] - ETA: 5:37 - loss: 0.6826 - acc: 0.5739
3136/5677 [===============>..............] - ETA: 5:29 - loss: 0.6826 - acc: 0.5740
3200/5677 [===============>..............] - ETA: 5:21 - loss: 0.6815 - acc: 0.5756
3264/5677 [================>.............] - ETA: 5:13 - loss: 0.6816 - acc: 0.5757
3328/5677 [================>.............] - ETA: 5:05 - loss: 0.6813 - acc: 0.5763
3392/5677 [================>.............] - ETA: 4:57 - loss: 0.6814 - acc: 0.5758
3456/5677 [=================>............] - ETA: 4:49 - loss: 0.6816 - acc: 0.5747
3520/5677 [=================>............] - ETA: 4:41 - loss: 0.6817 - acc: 0.5744
3584/5677 [=================>............] - ETA: 4:32 - loss: 0.6816 - acc: 0.5737
3648/5677 [==================>...........] - ETA: 4:24 - loss: 0.6813 - acc: 0.5737
3712/5677 [==================>...........] - ETA: 4:16 - loss: 0.6816 - acc: 0.5722
3776/5677 [==================>...........] - ETA: 4:07 - loss: 0.6821 - acc: 0.5715
3840/5677 [===================>..........] - ETA: 3:59 - loss: 0.6827 - acc: 0.5714
3904/5677 [===================>..........] - ETA: 3:51 - loss: 0.6827 - acc: 0.5715
3968/5677 [===================>..........] - ETA: 3:42 - loss: 0.6828 - acc: 0.5708
4032/5677 [====================>.........] - ETA: 3:34 - loss: 0.6829 - acc: 0.5702
4096/5677 [====================>.........] - ETA: 3:25 - loss: 0.6825 - acc: 0.5701
4160/5677 [====================>.........] - ETA: 3:17 - loss: 0.6827 - acc: 0.5690
4224/5677 [=====================>........] - ETA: 3:08 - loss: 0.6823 - acc: 0.5696
4288/5677 [=====================>........] - ETA: 3:00 - loss: 0.6821 - acc: 0.5697
4352/5677 [=====================>........] - ETA: 2:51 - loss: 0.6827 - acc: 0.5682
4416/5677 [======================>.......] - ETA: 2:43 - loss: 0.6829 - acc: 0.5686
4480/5677 [======================>.......] - ETA: 2:35 - loss: 0.6828 - acc: 0.5685
4544/5677 [=======================>......] - ETA: 2:26 - loss: 0.6833 - acc: 0.5682
4608/5677 [=======================>......] - ETA: 2:18 - loss: 0.6835 - acc: 0.5686
4672/5677 [=======================>......] - ETA: 2:10 - loss: 0.6830 - acc: 0.5700
4736/5677 [========================>.....] - ETA: 2:01 - loss: 0.6828 - acc: 0.5712
4800/5677 [========================>.....] - ETA: 1:53 - loss: 0.6827 - acc: 0.5713
4864/5677 [========================>.....] - ETA: 1:44 - loss: 0.6830 - acc: 0.5695
4928/5677 [=========================>....] - ETA: 1:36 - loss: 0.6829 - acc: 0.5698
4992/5677 [=========================>....] - ETA: 1:28 - loss: 0.6824 - acc: 0.5711
5056/5677 [=========================>....] - ETA: 1:19 - loss: 0.6818 - acc: 0.5728
5120/5677 [==========================>...] - ETA: 1:11 - loss: 0.6815 - acc: 0.5723
5184/5677 [==========================>...] - ETA: 1:03 - loss: 0.6820 - acc: 0.5708
5248/5677 [==========================>...] - ETA: 54s - loss: 0.6820 - acc: 0.5699 
5312/5677 [===========================>..] - ETA: 46s - loss: 0.6814 - acc: 0.5717
5376/5677 [===========================>..] - ETA: 38s - loss: 0.6819 - acc: 0.5703
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6824 - acc: 0.5699
5504/5677 [============================>.] - ETA: 22s - loss: 0.6820 - acc: 0.5701
5568/5677 [============================>.] - ETA: 13s - loss: 0.6821 - acc: 0.5700
5632/5677 [============================>.] - ETA: 5s - loss: 0.6822 - acc: 0.5696 
5677/5677 [==============================] - 751s 132ms/step - loss: 0.6820 - acc: 0.5695 - val_loss: 0.6774 - val_acc: 0.5658

Epoch 00005: val_acc improved from 0.56260 to 0.56577, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window09/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 10:22 - loss: 0.6902 - acc: 0.5000
 128/5677 [..............................] - ETA: 10:35 - loss: 0.6756 - acc: 0.5859
 192/5677 [>.............................] - ETA: 10:33 - loss: 0.6784 - acc: 0.5781
 256/5677 [>.............................] - ETA: 10:12 - loss: 0.6762 - acc: 0.5859
 320/5677 [>.............................] - ETA: 10:15 - loss: 0.6789 - acc: 0.5750
 384/5677 [=>............................] - ETA: 10:06 - loss: 0.6853 - acc: 0.5521
 448/5677 [=>............................] - ETA: 9:56 - loss: 0.6810 - acc: 0.5670 
 512/5677 [=>............................] - ETA: 9:43 - loss: 0.6800 - acc: 0.5723
 576/5677 [==>...........................] - ETA: 9:41 - loss: 0.6771 - acc: 0.5833
 640/5677 [==>...........................] - ETA: 9:28 - loss: 0.6776 - acc: 0.5844
 704/5677 [==>...........................] - ETA: 9:24 - loss: 0.6786 - acc: 0.5852
 768/5677 [===>..........................] - ETA: 9:17 - loss: 0.6743 - acc: 0.5924
 832/5677 [===>..........................] - ETA: 9:11 - loss: 0.6739 - acc: 0.5889
 896/5677 [===>..........................] - ETA: 9:02 - loss: 0.6734 - acc: 0.5882
 960/5677 [====>.........................] - ETA: 8:58 - loss: 0.6720 - acc: 0.5885
1024/5677 [====>.........................] - ETA: 8:50 - loss: 0.6748 - acc: 0.5820
1088/5677 [====>.........................] - ETA: 8:42 - loss: 0.6758 - acc: 0.5800
1152/5677 [=====>........................] - ETA: 8:33 - loss: 0.6760 - acc: 0.5790
1216/5677 [=====>........................] - ETA: 8:25 - loss: 0.6743 - acc: 0.5831
1280/5677 [=====>........................] - ETA: 8:16 - loss: 0.6754 - acc: 0.5813
1344/5677 [======>.......................] - ETA: 8:09 - loss: 0.6777 - acc: 0.5766
1408/5677 [======>.......................] - ETA: 8:04 - loss: 0.6771 - acc: 0.5781
1472/5677 [======>.......................] - ETA: 7:57 - loss: 0.6754 - acc: 0.5829
1536/5677 [=======>......................] - ETA: 7:52 - loss: 0.6778 - acc: 0.5794
1600/5677 [=======>......................] - ETA: 7:46 - loss: 0.6777 - acc: 0.5794
1664/5677 [=======>......................] - ETA: 7:40 - loss: 0.6792 - acc: 0.5763
1728/5677 [========>.....................] - ETA: 7:34 - loss: 0.6797 - acc: 0.5735
1792/5677 [========>.....................] - ETA: 7:27 - loss: 0.6801 - acc: 0.5725
1856/5677 [========>.....................] - ETA: 7:21 - loss: 0.6800 - acc: 0.5700
1920/5677 [=========>....................] - ETA: 7:15 - loss: 0.6803 - acc: 0.5698
1984/5677 [=========>....................] - ETA: 7:09 - loss: 0.6808 - acc: 0.5691
2048/5677 [=========>....................] - ETA: 7:01 - loss: 0.6821 - acc: 0.5654
2112/5677 [==========>...................] - ETA: 6:54 - loss: 0.6807 - acc: 0.5696
2176/5677 [==========>...................] - ETA: 6:46 - loss: 0.6805 - acc: 0.5689
2240/5677 [==========>...................] - ETA: 6:39 - loss: 0.6807 - acc: 0.5692
2304/5677 [===========>..................] - ETA: 6:32 - loss: 0.6804 - acc: 0.5699
2368/5677 [===========>..................] - ETA: 6:25 - loss: 0.6790 - acc: 0.5722
2432/5677 [===========>..................] - ETA: 6:17 - loss: 0.6797 - acc: 0.5695
2496/5677 [============>.................] - ETA: 6:10 - loss: 0.6800 - acc: 0.5673
2560/5677 [============>.................] - ETA: 6:03 - loss: 0.6794 - acc: 0.5684
2624/5677 [============>.................] - ETA: 5:55 - loss: 0.6791 - acc: 0.5686
2688/5677 [=============>................] - ETA: 5:48 - loss: 0.6784 - acc: 0.5688
2752/5677 [=============>................] - ETA: 5:40 - loss: 0.6792 - acc: 0.5672
2816/5677 [=============>................] - ETA: 5:34 - loss: 0.6797 - acc: 0.5668
2880/5677 [==============>...............] - ETA: 5:26 - loss: 0.6798 - acc: 0.5681
2944/5677 [==============>...............] - ETA: 5:18 - loss: 0.6805 - acc: 0.5652
3008/5677 [==============>...............] - ETA: 5:11 - loss: 0.6803 - acc: 0.5655
3072/5677 [===============>..............] - ETA: 5:03 - loss: 0.6803 - acc: 0.5667
3136/5677 [===============>..............] - ETA: 4:56 - loss: 0.6814 - acc: 0.5631
3200/5677 [===============>..............] - ETA: 4:48 - loss: 0.6807 - acc: 0.5644
3264/5677 [================>.............] - ETA: 4:40 - loss: 0.6799 - acc: 0.5662
3328/5677 [================>.............] - ETA: 4:33 - loss: 0.6803 - acc: 0.5670
3392/5677 [================>.............] - ETA: 4:25 - loss: 0.6799 - acc: 0.5672
3456/5677 [=================>............] - ETA: 4:18 - loss: 0.6803 - acc: 0.5671
3520/5677 [=================>............] - ETA: 4:10 - loss: 0.6805 - acc: 0.5665
3584/5677 [=================>............] - ETA: 4:02 - loss: 0.6803 - acc: 0.5675
3648/5677 [==================>...........] - ETA: 3:55 - loss: 0.6805 - acc: 0.5674
3712/5677 [==================>...........] - ETA: 3:47 - loss: 0.6800 - acc: 0.5687
3776/5677 [==================>...........] - ETA: 3:40 - loss: 0.6797 - acc: 0.5694
3840/5677 [===================>..........] - ETA: 3:32 - loss: 0.6806 - acc: 0.5672
3904/5677 [===================>..........] - ETA: 3:24 - loss: 0.6813 - acc: 0.5653
3968/5677 [===================>..........] - ETA: 3:17 - loss: 0.6815 - acc: 0.5635
4032/5677 [====================>.........] - ETA: 3:10 - loss: 0.6809 - acc: 0.5647
4096/5677 [====================>.........] - ETA: 3:02 - loss: 0.6812 - acc: 0.5637
4160/5677 [====================>.........] - ETA: 2:55 - loss: 0.6815 - acc: 0.5625
4224/5677 [=====================>........] - ETA: 2:48 - loss: 0.6810 - acc: 0.5637
4288/5677 [=====================>........] - ETA: 2:40 - loss: 0.6812 - acc: 0.5637
4352/5677 [=====================>........] - ETA: 2:33 - loss: 0.6808 - acc: 0.5648
4416/5677 [======================>.......] - ETA: 2:26 - loss: 0.6808 - acc: 0.5650
4480/5677 [======================>.......] - ETA: 2:19 - loss: 0.6803 - acc: 0.5650
4544/5677 [=======================>......] - ETA: 2:11 - loss: 0.6803 - acc: 0.5649
4608/5677 [=======================>......] - ETA: 2:04 - loss: 0.6804 - acc: 0.5649
4672/5677 [=======================>......] - ETA: 1:57 - loss: 0.6805 - acc: 0.5638
4736/5677 [========================>.....] - ETA: 1:50 - loss: 0.6803 - acc: 0.5648
4800/5677 [========================>.....] - ETA: 1:42 - loss: 0.6800 - acc: 0.5650
4864/5677 [========================>.....] - ETA: 1:35 - loss: 0.6800 - acc: 0.5652
4928/5677 [=========================>....] - ETA: 1:28 - loss: 0.6797 - acc: 0.5662
4992/5677 [=========================>....] - ETA: 1:20 - loss: 0.6800 - acc: 0.5655
5056/5677 [=========================>....] - ETA: 1:13 - loss: 0.6799 - acc: 0.5661
5120/5677 [==========================>...] - ETA: 1:05 - loss: 0.6800 - acc: 0.5662
5184/5677 [==========================>...] - ETA: 57s - loss: 0.6803 - acc: 0.5669 
5248/5677 [==========================>...] - ETA: 50s - loss: 0.6801 - acc: 0.5669
5312/5677 [===========================>..] - ETA: 42s - loss: 0.6804 - acc: 0.5668
5376/5677 [===========================>..] - ETA: 35s - loss: 0.6807 - acc: 0.5662
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6806 - acc: 0.5665
5504/5677 [============================>.] - ETA: 20s - loss: 0.6807 - acc: 0.5672
5568/5677 [============================>.] - ETA: 12s - loss: 0.6802 - acc: 0.5690
5632/5677 [============================>.] - ETA: 5s - loss: 0.6804 - acc: 0.5692 
5677/5677 [==============================] - 697s 123ms/step - loss: 0.6804 - acc: 0.5698 - val_loss: 0.6749 - val_acc: 0.5689

Epoch 00006: val_acc improved from 0.56577 to 0.56894, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window09/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 7/10

  64/5677 [..............................] - ETA: 10:39 - loss: 0.6427 - acc: 0.6562
 128/5677 [..............................] - ETA: 10:49 - loss: 0.6791 - acc: 0.5781
 192/5677 [>.............................] - ETA: 10:38 - loss: 0.6771 - acc: 0.5573
 256/5677 [>.............................] - ETA: 10:43 - loss: 0.6705 - acc: 0.5859
 320/5677 [>.............................] - ETA: 10:34 - loss: 0.6746 - acc: 0.5813
 384/5677 [=>............................] - ETA: 10:35 - loss: 0.6764 - acc: 0.5781
 448/5677 [=>............................] - ETA: 10:17 - loss: 0.6696 - acc: 0.5893
 512/5677 [=>............................] - ETA: 10:15 - loss: 0.6703 - acc: 0.5840
 576/5677 [==>...........................] - ETA: 9:59 - loss: 0.6701 - acc: 0.5851 
 640/5677 [==>...........................] - ETA: 9:45 - loss: 0.6700 - acc: 0.5859
 704/5677 [==>...........................] - ETA: 9:37 - loss: 0.6727 - acc: 0.5810
 768/5677 [===>..........................] - ETA: 9:33 - loss: 0.6737 - acc: 0.5833
 832/5677 [===>..........................] - ETA: 9:29 - loss: 0.6740 - acc: 0.5829
 896/5677 [===>..........................] - ETA: 9:26 - loss: 0.6737 - acc: 0.5792
 960/5677 [====>.........................] - ETA: 9:21 - loss: 0.6738 - acc: 0.5760
1024/5677 [====>.........................] - ETA: 9:12 - loss: 0.6749 - acc: 0.5732
1088/5677 [====>.........................] - ETA: 9:02 - loss: 0.6769 - acc: 0.5671
1152/5677 [=====>........................] - ETA: 8:55 - loss: 0.6748 - acc: 0.5703
1216/5677 [=====>........................] - ETA: 8:46 - loss: 0.6752 - acc: 0.5691
1280/5677 [=====>........................] - ETA: 8:38 - loss: 0.6768 - acc: 0.5680
1344/5677 [======>.......................] - ETA: 8:32 - loss: 0.6785 - acc: 0.5647
1408/5677 [======>.......................] - ETA: 8:27 - loss: 0.6784 - acc: 0.5682
1472/5677 [======>.......................] - ETA: 8:22 - loss: 0.6783 - acc: 0.5686
1536/5677 [=======>......................] - ETA: 8:15 - loss: 0.6792 - acc: 0.5658
1600/5677 [=======>......................] - ETA: 8:07 - loss: 0.6799 - acc: 0.5650
1664/5677 [=======>......................] - ETA: 8:01 - loss: 0.6787 - acc: 0.5673
1728/5677 [========>.....................] - ETA: 7:54 - loss: 0.6780 - acc: 0.5694
1792/5677 [========>.....................] - ETA: 7:46 - loss: 0.6770 - acc: 0.5698
1856/5677 [========>.....................] - ETA: 7:37 - loss: 0.6757 - acc: 0.5727
1920/5677 [=========>....................] - ETA: 7:28 - loss: 0.6764 - acc: 0.5719
1984/5677 [=========>....................] - ETA: 7:21 - loss: 0.6768 - acc: 0.5711
2048/5677 [=========>....................] - ETA: 7:16 - loss: 0.6755 - acc: 0.5728
2112/5677 [==========>...................] - ETA: 7:08 - loss: 0.6772 - acc: 0.5701
2176/5677 [==========>...................] - ETA: 7:01 - loss: 0.6768 - acc: 0.5717
2240/5677 [==========>...................] - ETA: 6:53 - loss: 0.6759 - acc: 0.5737
2304/5677 [===========>..................] - ETA: 6:44 - loss: 0.6765 - acc: 0.5725
2368/5677 [===========>..................] - ETA: 6:36 - loss: 0.6772 - acc: 0.5701
2432/5677 [===========>..................] - ETA: 6:28 - loss: 0.6767 - acc: 0.5703
2496/5677 [============>.................] - ETA: 6:21 - loss: 0.6758 - acc: 0.5717
2560/5677 [============>.................] - ETA: 6:13 - loss: 0.6753 - acc: 0.5723
2624/5677 [============>.................] - ETA: 6:06 - loss: 0.6755 - acc: 0.5713
2688/5677 [=============>................] - ETA: 5:58 - loss: 0.6756 - acc: 0.5714
2752/5677 [=============>................] - ETA: 5:50 - loss: 0.6753 - acc: 0.5730
2816/5677 [=============>................] - ETA: 5:41 - loss: 0.6748 - acc: 0.5735
2880/5677 [==============>...............] - ETA: 5:34 - loss: 0.6763 - acc: 0.5712
2944/5677 [==============>...............] - ETA: 5:26 - loss: 0.6762 - acc: 0.5713
3008/5677 [==============>...............] - ETA: 5:18 - loss: 0.6757 - acc: 0.5711
3072/5677 [===============>..............] - ETA: 5:10 - loss: 0.6746 - acc: 0.5719
3136/5677 [===============>..............] - ETA: 5:02 - loss: 0.6745 - acc: 0.5705
3200/5677 [===============>..............] - ETA: 4:55 - loss: 0.6761 - acc: 0.5687
3264/5677 [================>.............] - ETA: 4:48 - loss: 0.6762 - acc: 0.5677
3328/5677 [================>.............] - ETA: 4:41 - loss: 0.6765 - acc: 0.5679
3392/5677 [================>.............] - ETA: 4:34 - loss: 0.6773 - acc: 0.5666
3456/5677 [=================>............] - ETA: 4:26 - loss: 0.6764 - acc: 0.5689
3520/5677 [=================>............] - ETA: 4:19 - loss: 0.6767 - acc: 0.5693
3584/5677 [=================>............] - ETA: 4:11 - loss: 0.6772 - acc: 0.5681
3648/5677 [==================>...........] - ETA: 4:04 - loss: 0.6774 - acc: 0.5685
3712/5677 [==================>...........] - ETA: 3:57 - loss: 0.6780 - acc: 0.5676
3776/5677 [==================>...........] - ETA: 3:50 - loss: 0.6777 - acc: 0.5681
3840/5677 [===================>..........] - ETA: 3:42 - loss: 0.6777 - acc: 0.5695
3904/5677 [===================>..........] - ETA: 3:35 - loss: 0.6777 - acc: 0.5692
3968/5677 [===================>..........] - ETA: 3:27 - loss: 0.6781 - acc: 0.5693
4032/5677 [====================>.........] - ETA: 3:20 - loss: 0.6784 - acc: 0.5692
4096/5677 [====================>.........] - ETA: 3:12 - loss: 0.6783 - acc: 0.5696
4160/5677 [====================>.........] - ETA: 3:04 - loss: 0.6790 - acc: 0.5673
4224/5677 [=====================>........] - ETA: 2:56 - loss: 0.6790 - acc: 0.5663
4288/5677 [=====================>........] - ETA: 2:48 - loss: 0.6788 - acc: 0.5679
4352/5677 [=====================>........] - ETA: 2:41 - loss: 0.6784 - acc: 0.5685
4416/5677 [======================>.......] - ETA: 2:33 - loss: 0.6782 - acc: 0.5688
4480/5677 [======================>.......] - ETA: 2:25 - loss: 0.6785 - acc: 0.5685
4544/5677 [=======================>......] - ETA: 2:17 - loss: 0.6786 - acc: 0.5684
4608/5677 [=======================>......] - ETA: 2:09 - loss: 0.6786 - acc: 0.5684
4672/5677 [=======================>......] - ETA: 2:01 - loss: 0.6789 - acc: 0.5681
4736/5677 [========================>.....] - ETA: 1:53 - loss: 0.6789 - acc: 0.5690
4800/5677 [========================>.....] - ETA: 1:46 - loss: 0.6798 - acc: 0.5671
4864/5677 [========================>.....] - ETA: 1:38 - loss: 0.6801 - acc: 0.5672
4928/5677 [=========================>....] - ETA: 1:30 - loss: 0.6797 - acc: 0.5686
4992/5677 [=========================>....] - ETA: 1:22 - loss: 0.6798 - acc: 0.5677
5056/5677 [=========================>....] - ETA: 1:15 - loss: 0.6801 - acc: 0.5670
5120/5677 [==========================>...] - ETA: 1:07 - loss: 0.6798 - acc: 0.5680
5184/5677 [==========================>...] - ETA: 59s - loss: 0.6802 - acc: 0.5671 
5248/5677 [==========================>...] - ETA: 51s - loss: 0.6797 - acc: 0.5678
5312/5677 [===========================>..] - ETA: 44s - loss: 0.6797 - acc: 0.5678
5376/5677 [===========================>..] - ETA: 36s - loss: 0.6795 - acc: 0.5690
5440/5677 [===========================>..] - ETA: 28s - loss: 0.6796 - acc: 0.5687
5504/5677 [============================>.] - ETA: 20s - loss: 0.6793 - acc: 0.5692
5568/5677 [============================>.] - ETA: 13s - loss: 0.6793 - acc: 0.5695
5632/5677 [============================>.] - ETA: 5s - loss: 0.6795 - acc: 0.5685 
5677/5677 [==============================] - 715s 126ms/step - loss: 0.6795 - acc: 0.5697 - val_loss: 0.6824 - val_acc: 0.5689

Epoch 00007: val_acc improved from 0.56894 to 0.56894, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window09/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 8/10

  64/5677 [..............................] - ETA: 12:22 - loss: 0.6801 - acc: 0.6094
 128/5677 [..............................] - ETA: 11:49 - loss: 0.6640 - acc: 0.6328
 192/5677 [>.............................] - ETA: 11:48 - loss: 0.6638 - acc: 0.6198
 256/5677 [>.............................] - ETA: 11:41 - loss: 0.6736 - acc: 0.5898
 320/5677 [>.............................] - ETA: 11:32 - loss: 0.6707 - acc: 0.5906
 384/5677 [=>............................] - ETA: 11:22 - loss: 0.6681 - acc: 0.6016
 448/5677 [=>............................] - ETA: 11:08 - loss: 0.6681 - acc: 0.6049
 512/5677 [=>............................] - ETA: 11:02 - loss: 0.6732 - acc: 0.5957
 576/5677 [==>...........................] - ETA: 11:01 - loss: 0.6741 - acc: 0.5938
 640/5677 [==>...........................] - ETA: 10:52 - loss: 0.6748 - acc: 0.5859
 704/5677 [==>...........................] - ETA: 10:40 - loss: 0.6726 - acc: 0.5909
 768/5677 [===>..........................] - ETA: 10:29 - loss: 0.6714 - acc: 0.5951
 832/5677 [===>..........................] - ETA: 10:19 - loss: 0.6716 - acc: 0.5974
 896/5677 [===>..........................] - ETA: 10:09 - loss: 0.6723 - acc: 0.5926
 960/5677 [====>.........................] - ETA: 10:02 - loss: 0.6738 - acc: 0.5906
1024/5677 [====>.........................] - ETA: 9:51 - loss: 0.6709 - acc: 0.5967 
1088/5677 [====>.........................] - ETA: 9:40 - loss: 0.6701 - acc: 0.5983
1152/5677 [=====>........................] - ETA: 9:32 - loss: 0.6732 - acc: 0.5929
1216/5677 [=====>........................] - ETA: 9:22 - loss: 0.6707 - acc: 0.6012
1280/5677 [=====>........................] - ETA: 9:14 - loss: 0.6729 - acc: 0.5961
1344/5677 [======>.......................] - ETA: 9:06 - loss: 0.6728 - acc: 0.5952
1408/5677 [======>.......................] - ETA: 8:56 - loss: 0.6722 - acc: 0.5938
1472/5677 [======>.......................] - ETA: 8:48 - loss: 0.6718 - acc: 0.5944
1536/5677 [=======>......................] - ETA: 8:40 - loss: 0.6725 - acc: 0.5905
1600/5677 [=======>......................] - ETA: 8:31 - loss: 0.6719 - acc: 0.5931
1664/5677 [=======>......................] - ETA: 8:22 - loss: 0.6737 - acc: 0.5883
1728/5677 [========>.....................] - ETA: 8:14 - loss: 0.6735 - acc: 0.5874
1792/5677 [========>.....................] - ETA: 8:07 - loss: 0.6738 - acc: 0.5871
1856/5677 [========>.....................] - ETA: 8:00 - loss: 0.6732 - acc: 0.5889
1920/5677 [=========>....................] - ETA: 7:53 - loss: 0.6748 - acc: 0.5870
1984/5677 [=========>....................] - ETA: 7:46 - loss: 0.6767 - acc: 0.5837
2048/5677 [=========>....................] - ETA: 7:38 - loss: 0.6790 - acc: 0.5806
2112/5677 [==========>...................] - ETA: 7:31 - loss: 0.6787 - acc: 0.5810
2176/5677 [==========>...................] - ETA: 7:24 - loss: 0.6779 - acc: 0.5827
2240/5677 [==========>...................] - ETA: 7:15 - loss: 0.6770 - acc: 0.5826
2304/5677 [===========>..................] - ETA: 7:07 - loss: 0.6772 - acc: 0.5829
2368/5677 [===========>..................] - ETA: 6:59 - loss: 0.6777 - acc: 0.5832
2432/5677 [===========>..................] - ETA: 6:50 - loss: 0.6781 - acc: 0.5831
2496/5677 [============>.................] - ETA: 6:43 - loss: 0.6779 - acc: 0.5841
2560/5677 [============>.................] - ETA: 6:35 - loss: 0.6771 - acc: 0.5852
2624/5677 [============>.................] - ETA: 6:27 - loss: 0.6764 - acc: 0.5869
2688/5677 [=============>................] - ETA: 6:19 - loss: 0.6768 - acc: 0.5848
2752/5677 [=============>................] - ETA: 6:11 - loss: 0.6766 - acc: 0.5858
2816/5677 [=============>................] - ETA: 6:03 - loss: 0.6760 - acc: 0.5863
2880/5677 [==============>...............] - ETA: 5:56 - loss: 0.6754 - acc: 0.5865
2944/5677 [==============>...............] - ETA: 5:48 - loss: 0.6753 - acc: 0.5863
3008/5677 [==============>...............] - ETA: 5:39 - loss: 0.6747 - acc: 0.5881
3072/5677 [===============>..............] - ETA: 5:31 - loss: 0.6754 - acc: 0.5859
3136/5677 [===============>..............] - ETA: 5:22 - loss: 0.6762 - acc: 0.5851
3200/5677 [===============>..............] - ETA: 5:13 - loss: 0.6757 - acc: 0.5850
3264/5677 [================>.............] - ETA: 5:05 - loss: 0.6762 - acc: 0.5830
3328/5677 [================>.............] - ETA: 4:57 - loss: 0.6760 - acc: 0.5832
3392/5677 [================>.............] - ETA: 4:48 - loss: 0.6759 - acc: 0.5831
3456/5677 [=================>............] - ETA: 4:40 - loss: 0.6755 - acc: 0.5842
3520/5677 [=================>............] - ETA: 4:32 - loss: 0.6759 - acc: 0.5830
3584/5677 [=================>............] - ETA: 4:24 - loss: 0.6754 - acc: 0.5848
3648/5677 [==================>...........] - ETA: 4:16 - loss: 0.6755 - acc: 0.5842
3712/5677 [==================>...........] - ETA: 4:08 - loss: 0.6759 - acc: 0.5835
3776/5677 [==================>...........] - ETA: 3:59 - loss: 0.6760 - acc: 0.5826
3840/5677 [===================>..........] - ETA: 3:51 - loss: 0.6766 - acc: 0.5815
3904/5677 [===================>..........] - ETA: 3:43 - loss: 0.6757 - acc: 0.5827
3968/5677 [===================>..........] - ETA: 3:35 - loss: 0.6752 - acc: 0.5829
4032/5677 [====================>.........] - ETA: 3:26 - loss: 0.6755 - acc: 0.5826
4096/5677 [====================>.........] - ETA: 3:18 - loss: 0.6754 - acc: 0.5828
4160/5677 [====================>.........] - ETA: 3:10 - loss: 0.6754 - acc: 0.5832
4224/5677 [=====================>........] - ETA: 3:02 - loss: 0.6746 - acc: 0.5845
4288/5677 [=====================>........] - ETA: 2:53 - loss: 0.6749 - acc: 0.5833
4352/5677 [=====================>........] - ETA: 2:45 - loss: 0.6749 - acc: 0.5834
4416/5677 [======================>.......] - ETA: 2:37 - loss: 0.6748 - acc: 0.5831
4480/5677 [======================>.......] - ETA: 2:29 - loss: 0.6753 - acc: 0.5819
4544/5677 [=======================>......] - ETA: 2:21 - loss: 0.6752 - acc: 0.5823
4608/5677 [=======================>......] - ETA: 2:13 - loss: 0.6747 - acc: 0.5825
4672/5677 [=======================>......] - ETA: 2:05 - loss: 0.6753 - acc: 0.5820
4736/5677 [========================>.....] - ETA: 1:57 - loss: 0.6754 - acc: 0.5815
4800/5677 [========================>.....] - ETA: 1:49 - loss: 0.6750 - acc: 0.5823
4864/5677 [========================>.....] - ETA: 1:41 - loss: 0.6744 - acc: 0.5833
4928/5677 [=========================>....] - ETA: 1:33 - loss: 0.6742 - acc: 0.5834
4992/5677 [=========================>....] - ETA: 1:25 - loss: 0.6741 - acc: 0.5835
5056/5677 [=========================>....] - ETA: 1:17 - loss: 0.6739 - acc: 0.5835
5120/5677 [==========================>...] - ETA: 1:09 - loss: 0.6744 - acc: 0.5832
5184/5677 [==========================>...] - ETA: 1:01 - loss: 0.6745 - acc: 0.5829
5248/5677 [==========================>...] - ETA: 53s - loss: 0.6747 - acc: 0.5821 
5312/5677 [===========================>..] - ETA: 45s - loss: 0.6749 - acc: 0.5819
5376/5677 [===========================>..] - ETA: 37s - loss: 0.6752 - acc: 0.5813
5440/5677 [===========================>..] - ETA: 29s - loss: 0.6749 - acc: 0.5827
5504/5677 [============================>.] - ETA: 21s - loss: 0.6755 - acc: 0.5823
5568/5677 [============================>.] - ETA: 13s - loss: 0.6758 - acc: 0.5817
5632/5677 [============================>.] - ETA: 5s - loss: 0.6757 - acc: 0.5820 
5677/5677 [==============================] - 737s 130ms/step - loss: 0.6756 - acc: 0.5820 - val_loss: 0.7212 - val_acc: 0.4802

Epoch 00008: val_acc did not improve from 0.56894
Epoch 9/10

  64/5677 [..............................] - ETA: 10:34 - loss: 0.6872 - acc: 0.5625
 128/5677 [..............................] - ETA: 11:24 - loss: 0.6874 - acc: 0.5391
 192/5677 [>.............................] - ETA: 11:16 - loss: 0.6938 - acc: 0.5417
 256/5677 [>.............................] - ETA: 10:48 - loss: 0.6894 - acc: 0.5508
 320/5677 [>.............................] - ETA: 10:42 - loss: 0.6902 - acc: 0.5531
 384/5677 [=>............................] - ETA: 10:35 - loss: 0.6851 - acc: 0.5651
 448/5677 [=>............................] - ETA: 10:36 - loss: 0.6778 - acc: 0.5804
 512/5677 [=>............................] - ETA: 10:26 - loss: 0.6749 - acc: 0.5918
 576/5677 [==>...........................] - ETA: 10:17 - loss: 0.6761 - acc: 0.5920
 640/5677 [==>...........................] - ETA: 10:16 - loss: 0.6765 - acc: 0.5922
 704/5677 [==>...........................] - ETA: 10:05 - loss: 0.6751 - acc: 0.5895
 768/5677 [===>..........................] - ETA: 9:54 - loss: 0.6745 - acc: 0.5846 
 832/5677 [===>..........................] - ETA: 9:49 - loss: 0.6740 - acc: 0.5817
 896/5677 [===>..........................] - ETA: 9:39 - loss: 0.6769 - acc: 0.5826
 960/5677 [====>.........................] - ETA: 9:37 - loss: 0.6752 - acc: 0.5885
1024/5677 [====>.........................] - ETA: 9:29 - loss: 0.6764 - acc: 0.5811
1088/5677 [====>.........................] - ETA: 9:18 - loss: 0.6784 - acc: 0.5781
1152/5677 [=====>........................] - ETA: 9:12 - loss: 0.6779 - acc: 0.5764
1216/5677 [=====>........................] - ETA: 9:03 - loss: 0.6795 - acc: 0.5740
1280/5677 [=====>........................] - ETA: 8:52 - loss: 0.6803 - acc: 0.5719
1344/5677 [======>.......................] - ETA: 8:42 - loss: 0.6809 - acc: 0.5699
1408/5677 [======>.......................] - ETA: 8:31 - loss: 0.6804 - acc: 0.5703
1472/5677 [======>.......................] - ETA: 8:21 - loss: 0.6800 - acc: 0.5713
1536/5677 [=======>......................] - ETA: 8:13 - loss: 0.6785 - acc: 0.5749
1600/5677 [=======>......................] - ETA: 8:03 - loss: 0.6771 - acc: 0.5794
1664/5677 [=======>......................] - ETA: 7:54 - loss: 0.6776 - acc: 0.5775
1728/5677 [========>.....................] - ETA: 7:44 - loss: 0.6790 - acc: 0.5741
1792/5677 [========>.....................] - ETA: 7:37 - loss: 0.6784 - acc: 0.5759
1856/5677 [========>.....................] - ETA: 7:30 - loss: 0.6773 - acc: 0.5787
1920/5677 [=========>....................] - ETA: 7:22 - loss: 0.6780 - acc: 0.5766
1984/5677 [=========>....................] - ETA: 7:13 - loss: 0.6777 - acc: 0.5786
2048/5677 [=========>....................] - ETA: 7:05 - loss: 0.6775 - acc: 0.5786
2112/5677 [==========>...................] - ETA: 6:57 - loss: 0.6770 - acc: 0.5786
2176/5677 [==========>...................] - ETA: 6:50 - loss: 0.6764 - acc: 0.5804
2240/5677 [==========>...................] - ETA: 6:43 - loss: 0.6762 - acc: 0.5813
2304/5677 [===========>..................] - ETA: 6:36 - loss: 0.6753 - acc: 0.5842
2368/5677 [===========>..................] - ETA: 6:30 - loss: 0.6753 - acc: 0.5840
2432/5677 [===========>..................] - ETA: 6:22 - loss: 0.6752 - acc: 0.5843
2496/5677 [============>.................] - ETA: 6:15 - loss: 0.6754 - acc: 0.5849
2560/5677 [============>.................] - ETA: 6:08 - loss: 0.6743 - acc: 0.5871
2624/5677 [============>.................] - ETA: 6:01 - loss: 0.6734 - acc: 0.5892
2688/5677 [=============>................] - ETA: 5:53 - loss: 0.6741 - acc: 0.5874
2752/5677 [=============>................] - ETA: 5:45 - loss: 0.6738 - acc: 0.5883
2816/5677 [=============>................] - ETA: 5:38 - loss: 0.6738 - acc: 0.5884
2880/5677 [==============>...............] - ETA: 5:29 - loss: 0.6740 - acc: 0.5882
2944/5677 [==============>...............] - ETA: 5:22 - loss: 0.6740 - acc: 0.5876
3008/5677 [==============>...............] - ETA: 5:14 - loss: 0.6750 - acc: 0.5858
3072/5677 [===============>..............] - ETA: 5:07 - loss: 0.6758 - acc: 0.5859
3136/5677 [===============>..............] - ETA: 4:59 - loss: 0.6750 - acc: 0.5867
3200/5677 [===============>..............] - ETA: 4:53 - loss: 0.6742 - acc: 0.5878
3264/5677 [================>.............] - ETA: 4:45 - loss: 0.6736 - acc: 0.5885
3328/5677 [================>.............] - ETA: 4:38 - loss: 0.6736 - acc: 0.5874
3392/5677 [================>.............] - ETA: 4:31 - loss: 0.6729 - acc: 0.5876
3456/5677 [=================>............] - ETA: 4:23 - loss: 0.6736 - acc: 0.5859
3520/5677 [=================>............] - ETA: 4:16 - loss: 0.6740 - acc: 0.5852
3584/5677 [=================>............] - ETA: 4:09 - loss: 0.6745 - acc: 0.5840
3648/5677 [==================>...........] - ETA: 4:02 - loss: 0.6737 - acc: 0.5850
3712/5677 [==================>...........] - ETA: 3:55 - loss: 0.6735 - acc: 0.5849
3776/5677 [==================>...........] - ETA: 3:47 - loss: 0.6733 - acc: 0.5834
3840/5677 [===================>..........] - ETA: 3:40 - loss: 0.6729 - acc: 0.5844
3904/5677 [===================>..........] - ETA: 3:32 - loss: 0.6731 - acc: 0.5843
3968/5677 [===================>..........] - ETA: 3:25 - loss: 0.6730 - acc: 0.5849
4032/5677 [====================>.........] - ETA: 3:17 - loss: 0.6737 - acc: 0.5843
4096/5677 [====================>.........] - ETA: 3:10 - loss: 0.6740 - acc: 0.5840
4160/5677 [====================>.........] - ETA: 3:02 - loss: 0.6749 - acc: 0.5822
4224/5677 [=====================>........] - ETA: 2:54 - loss: 0.6751 - acc: 0.5812
4288/5677 [=====================>........] - ETA: 2:46 - loss: 0.6750 - acc: 0.5812
4352/5677 [=====================>........] - ETA: 2:39 - loss: 0.6748 - acc: 0.5816
4416/5677 [======================>.......] - ETA: 2:31 - loss: 0.6748 - acc: 0.5811
4480/5677 [======================>.......] - ETA: 2:23 - loss: 0.6748 - acc: 0.5810
4544/5677 [=======================>......] - ETA: 2:16 - loss: 0.6744 - acc: 0.5819
4608/5677 [=======================>......] - ETA: 2:08 - loss: 0.6738 - acc: 0.5838
4672/5677 [=======================>......] - ETA: 2:00 - loss: 0.6741 - acc: 0.5830
4736/5677 [========================>.....] - ETA: 1:53 - loss: 0.6740 - acc: 0.5840
4800/5677 [========================>.....] - ETA: 1:45 - loss: 0.6739 - acc: 0.5846
4864/5677 [========================>.....] - ETA: 1:37 - loss: 0.6741 - acc: 0.5835
4928/5677 [=========================>....] - ETA: 1:30 - loss: 0.6738 - acc: 0.5848
4992/5677 [=========================>....] - ETA: 1:22 - loss: 0.6739 - acc: 0.5847
5056/5677 [=========================>....] - ETA: 1:14 - loss: 0.6745 - acc: 0.5831
5120/5677 [==========================>...] - ETA: 1:07 - loss: 0.6743 - acc: 0.5832
5184/5677 [==========================>...] - ETA: 59s - loss: 0.6747 - acc: 0.5824 
5248/5677 [==========================>...] - ETA: 51s - loss: 0.6748 - acc: 0.5816
5312/5677 [===========================>..] - ETA: 44s - loss: 0.6746 - acc: 0.5828
5376/5677 [===========================>..] - ETA: 36s - loss: 0.6737 - acc: 0.5843
5440/5677 [===========================>..] - ETA: 28s - loss: 0.6744 - acc: 0.5833
5504/5677 [============================>.] - ETA: 20s - loss: 0.6742 - acc: 0.5834
5568/5677 [============================>.] - ETA: 13s - loss: 0.6743 - acc: 0.5835
5632/5677 [============================>.] - ETA: 5s - loss: 0.6745 - acc: 0.5829 
5677/5677 [==============================] - 710s 125ms/step - loss: 0.6745 - acc: 0.5829 - val_loss: 0.6762 - val_acc: 0.5848

Epoch 00009: val_acc improved from 0.56894 to 0.58479, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window09/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 10/10

  64/5677 [..............................] - ETA: 11:59 - loss: 0.7182 - acc: 0.5469
 128/5677 [..............................] - ETA: 12:21 - loss: 0.6988 - acc: 0.5391
 192/5677 [>.............................] - ETA: 11:28 - loss: 0.6925 - acc: 0.5573
 256/5677 [>.............................] - ETA: 10:49 - loss: 0.6857 - acc: 0.5742
 320/5677 [>.............................] - ETA: 10:32 - loss: 0.6828 - acc: 0.5781
 384/5677 [=>............................] - ETA: 10:29 - loss: 0.6827 - acc: 0.5755
 448/5677 [=>............................] - ETA: 10:27 - loss: 0.6794 - acc: 0.5737
 512/5677 [=>............................] - ETA: 10:23 - loss: 0.6807 - acc: 0.5723
 576/5677 [==>...........................] - ETA: 10:11 - loss: 0.6832 - acc: 0.5608
 640/5677 [==>...........................] - ETA: 10:01 - loss: 0.6855 - acc: 0.5641
 704/5677 [==>...........................] - ETA: 9:54 - loss: 0.6851 - acc: 0.5653 
 768/5677 [===>..........................] - ETA: 9:43 - loss: 0.6848 - acc: 0.5651
 832/5677 [===>..........................] - ETA: 9:38 - loss: 0.6821 - acc: 0.5709
 896/5677 [===>..........................] - ETA: 9:31 - loss: 0.6788 - acc: 0.5737
 960/5677 [====>.........................] - ETA: 9:22 - loss: 0.6785 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 9:13 - loss: 0.6779 - acc: 0.5713
1088/5677 [====>.........................] - ETA: 9:04 - loss: 0.6778 - acc: 0.5735
1152/5677 [=====>........................] - ETA: 8:52 - loss: 0.6797 - acc: 0.5686
1216/5677 [=====>........................] - ETA: 8:42 - loss: 0.6768 - acc: 0.5773
1280/5677 [=====>........................] - ETA: 8:33 - loss: 0.6758 - acc: 0.5773
1344/5677 [======>.......................] - ETA: 8:25 - loss: 0.6768 - acc: 0.5789
1408/5677 [======>.......................] - ETA: 8:19 - loss: 0.6770 - acc: 0.5795
1472/5677 [======>.......................] - ETA: 8:08 - loss: 0.6774 - acc: 0.5795
1536/5677 [=======>......................] - ETA: 8:01 - loss: 0.6758 - acc: 0.5840
1600/5677 [=======>......................] - ETA: 7:51 - loss: 0.6763 - acc: 0.5825
1664/5677 [=======>......................] - ETA: 7:44 - loss: 0.6754 - acc: 0.5847
1728/5677 [========>.....................] - ETA: 7:35 - loss: 0.6752 - acc: 0.5868
1792/5677 [========>.....................] - ETA: 7:28 - loss: 0.6756 - acc: 0.5876
1856/5677 [========>.....................] - ETA: 7:22 - loss: 0.6756 - acc: 0.5867
1920/5677 [=========>....................] - ETA: 7:17 - loss: 0.6765 - acc: 0.5849
1984/5677 [=========>....................] - ETA: 7:11 - loss: 0.6757 - acc: 0.5872
2048/5677 [=========>....................] - ETA: 7:04 - loss: 0.6755 - acc: 0.5879
2112/5677 [==========>...................] - ETA: 6:57 - loss: 0.6749 - acc: 0.5871
2176/5677 [==========>...................] - ETA: 6:50 - loss: 0.6754 - acc: 0.5850
2240/5677 [==========>...................] - ETA: 6:43 - loss: 0.6741 - acc: 0.5871
2304/5677 [===========>..................] - ETA: 6:35 - loss: 0.6738 - acc: 0.5864
2368/5677 [===========>..................] - ETA: 6:28 - loss: 0.6741 - acc: 0.5870
2432/5677 [===========>..................] - ETA: 6:20 - loss: 0.6733 - acc: 0.5888
2496/5677 [============>.................] - ETA: 6:13 - loss: 0.6736 - acc: 0.5861
2560/5677 [============>.................] - ETA: 6:06 - loss: 0.6732 - acc: 0.5867
2624/5677 [============>.................] - ETA: 5:59 - loss: 0.6730 - acc: 0.5880
2688/5677 [=============>................] - ETA: 5:52 - loss: 0.6726 - acc: 0.5889
2752/5677 [=============>................] - ETA: 5:44 - loss: 0.6724 - acc: 0.5894
2816/5677 [=============>................] - ETA: 5:35 - loss: 0.6716 - acc: 0.5916
2880/5677 [==============>...............] - ETA: 5:28 - loss: 0.6720 - acc: 0.5896
2944/5677 [==============>...............] - ETA: 5:20 - loss: 0.6722 - acc: 0.5887
3008/5677 [==============>...............] - ETA: 5:13 - loss: 0.6726 - acc: 0.5878
3072/5677 [===============>..............] - ETA: 5:06 - loss: 0.6731 - acc: 0.5879
3136/5677 [===============>..............] - ETA: 4:59 - loss: 0.6739 - acc: 0.5861
3200/5677 [===============>..............] - ETA: 4:52 - loss: 0.6750 - acc: 0.5834
3264/5677 [================>.............] - ETA: 4:45 - loss: 0.6751 - acc: 0.5839
3328/5677 [================>.............] - ETA: 4:38 - loss: 0.6756 - acc: 0.5826
3392/5677 [================>.............] - ETA: 4:30 - loss: 0.6752 - acc: 0.5820
3456/5677 [=================>............] - ETA: 4:23 - loss: 0.6755 - acc: 0.5813
3520/5677 [=================>............] - ETA: 4:17 - loss: 0.6763 - acc: 0.5793
3584/5677 [=================>............] - ETA: 4:09 - loss: 0.6760 - acc: 0.5787
3648/5677 [==================>...........] - ETA: 4:02 - loss: 0.6756 - acc: 0.5798
3712/5677 [==================>...........] - ETA: 3:55 - loss: 0.6752 - acc: 0.5805
3776/5677 [==================>...........] - ETA: 3:48 - loss: 0.6748 - acc: 0.5813
3840/5677 [===================>..........] - ETA: 3:40 - loss: 0.6752 - acc: 0.5810
3904/5677 [===================>..........] - ETA: 3:34 - loss: 0.6749 - acc: 0.5825
3968/5677 [===================>..........] - ETA: 3:26 - loss: 0.6741 - acc: 0.5837
4032/5677 [====================>.........] - ETA: 3:18 - loss: 0.6741 - acc: 0.5833
4096/5677 [====================>.........] - ETA: 3:11 - loss: 0.6740 - acc: 0.5825
4160/5677 [====================>.........] - ETA: 3:04 - loss: 0.6750 - acc: 0.5800
4224/5677 [=====================>........] - ETA: 2:56 - loss: 0.6756 - acc: 0.5798
4288/5677 [=====================>........] - ETA: 2:49 - loss: 0.6750 - acc: 0.5807
4352/5677 [=====================>........] - ETA: 2:42 - loss: 0.6746 - acc: 0.5816
4416/5677 [======================>.......] - ETA: 2:34 - loss: 0.6747 - acc: 0.5822
4480/5677 [======================>.......] - ETA: 2:26 - loss: 0.6751 - acc: 0.5806
4544/5677 [=======================>......] - ETA: 2:19 - loss: 0.6753 - acc: 0.5801
4608/5677 [=======================>......] - ETA: 2:11 - loss: 0.6744 - acc: 0.5814
4672/5677 [=======================>......] - ETA: 2:03 - loss: 0.6745 - acc: 0.5809
4736/5677 [========================>.....] - ETA: 1:56 - loss: 0.6739 - acc: 0.5828
4800/5677 [========================>.....] - ETA: 1:48 - loss: 0.6736 - acc: 0.5833
4864/5677 [========================>.....] - ETA: 1:40 - loss: 0.6743 - acc: 0.5820
4928/5677 [=========================>....] - ETA: 1:32 - loss: 0.6744 - acc: 0.5820
4992/5677 [=========================>....] - ETA: 1:24 - loss: 0.6744 - acc: 0.5815
5056/5677 [=========================>....] - ETA: 1:16 - loss: 0.6741 - acc: 0.5819
5120/5677 [==========================>...] - ETA: 1:09 - loss: 0.6735 - acc: 0.5834
5184/5677 [==========================>...] - ETA: 1:01 - loss: 0.6733 - acc: 0.5841
5248/5677 [==========================>...] - ETA: 53s - loss: 0.6732 - acc: 0.5844 
5312/5677 [===========================>..] - ETA: 45s - loss: 0.6735 - acc: 0.5841
5376/5677 [===========================>..] - ETA: 37s - loss: 0.6736 - acc: 0.5837
5440/5677 [===========================>..] - ETA: 29s - loss: 0.6743 - acc: 0.5825
5504/5677 [============================>.] - ETA: 21s - loss: 0.6741 - acc: 0.5828
5568/5677 [============================>.] - ETA: 13s - loss: 0.6744 - acc: 0.5826
5632/5677 [============================>.] - ETA: 5s - loss: 0.6745 - acc: 0.5826 
5677/5677 [==============================] - 742s 131ms/step - loss: 0.6750 - acc: 0.5823 - val_loss: 0.6939 - val_acc: 0.5610

Epoch 00010: val_acc did not improve from 0.58479
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f95d4453610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f95d4453610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f95d433cdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f95d433cdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d433c790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d433c790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95b87a4890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95b87a4890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93184e1fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93184e1fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b87768d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b87768d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95b865c210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95b865c210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b835bc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b835bc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95b86aeb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95b86aeb50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95b85583d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95b85583d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b84cb750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b84cb750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95b85e1e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95b85e1e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f919c64fe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f919c64fe50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95b8484150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95b8484150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95b8124490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95b8124490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b84d2a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b84d2a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95b8213050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95b8213050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b80bd910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b80bd910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f959468bad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f959468bad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f949c6e1750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f949c6e1750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b816d410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b816d410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9594680110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9594680110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f959465bd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f959465bd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f949c4b0510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f949c4b0510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f949c4a2a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f949c4a2a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c478ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c478ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f949c4b0c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f949c4b0c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c3e2ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f949c3e2ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f949c137f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f949c137f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f948078fa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f948078fa50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f948078b450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f948078b450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f949c363790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f949c363790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94806a8d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94806a8d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94805f2510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94805f2510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94804b4f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94804b4f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94804fce50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94804fce50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f949c157110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f949c157110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9480553c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9480553c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f948027ee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f948027ee10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9480226a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9480226a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f948027ed50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f948027ed50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f948027ec10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f948027ec10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93105b1e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93105b1e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9618212190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9618212190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f945c615810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f945c615810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b86a9650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95b86a9650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f948018d310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f948018d310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9100683e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9100683e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f945c44cf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f945c44cf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91006f8390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91006f8390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f945c540810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f945c540810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f945c37e610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f945c37e610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f945c37cb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f945c37cb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f945c1257d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f945c1257d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f944077e690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f944077e690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f945c3d07d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f945c3d07d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9100683890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9100683890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94406d2690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94406d2690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9440579510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9440579510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94404efc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f94404efc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94407cce50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94407cce50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f945c09cc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f945c09cc90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94407cf310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94407cf310>>: AttributeError: module 'gast' has no attribute 'Str'
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 5:50
 128/1578 [=>............................] - ETA: 3:19
 192/1578 [==>...........................] - ETA: 2:26
 256/1578 [===>..........................] - ETA: 1:59
 320/1578 [=====>........................] - ETA: 1:41
 384/1578 [======>.......................] - ETA: 1:29
 448/1578 [=======>......................] - ETA: 1:19
 512/1578 [========>.....................] - ETA: 1:11
 576/1578 [=========>....................] - ETA: 1:05
 640/1578 [===========>..................] - ETA: 59s 
 704/1578 [============>.................] - ETA: 53s
 768/1578 [=============>................] - ETA: 48s
 832/1578 [==============>...............] - ETA: 44s
 896/1578 [================>.............] - ETA: 40s
 960/1578 [=================>............] - ETA: 36s
1024/1578 [==================>...........] - ETA: 32s
1088/1578 [===================>..........] - ETA: 28s
1152/1578 [====================>.........] - ETA: 24s
1216/1578 [======================>.......] - ETA: 20s
1280/1578 [=======================>......] - ETA: 16s
1344/1578 [========================>.....] - ETA: 12s
1408/1578 [=========================>....] - ETA: 9s 
1472/1578 [==========================>...] - ETA: 5s
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 83s 53ms/step
loss: 0.666223256231412
acc: 0.6096324461343473
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9100702990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9100702990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f91006889d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f91006889d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94043d8090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94043d8090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f940424c190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f940424c190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91006a9c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91006a9c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91006912d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91006912d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f940424cd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f940424cd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94043bca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94043bca90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95d41a3110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f95d41a3110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95d42b8cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f95d42b8cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d4255310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d4255310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95d4105ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f95d4105ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d41c4590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d41c4590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90b43cdbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90b43cdbd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91003b4310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91003b4310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9480228f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9480228f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f910055cb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f910055cb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d428fd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f95d428fd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91001457d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91001457d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91000a1fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f91000a1fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91001d7690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f91001d7690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91002b4b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91002b4b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90e472fbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90e472fbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91003b4b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f91003b4b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90e4585690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90e4585690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f910009e410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f910009e410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91003b8050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f91003b8050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9100130fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9100130fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90e4507290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90e4507290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90e421e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90e421e550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90e4333bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90e4333bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90e4507a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90e4507a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90e4130910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90e4130910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90e40bc750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90e40bc750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90c86b7090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90c86b7090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c87e0c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c87e0c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90e40cd7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90e40cd7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90e4531090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90e4531090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90c84af990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90c84af990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90c839f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90c839f610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c8478690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c8478690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90c867b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90c867b8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c834b110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c834b110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90c84ce250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90c84ce250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90c81ed650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90c81ed650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c806d3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c806d3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90c84cec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90c84cec50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c81f5d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c81f5d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90c806f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90c806f5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90b450e9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f90b450e9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90b46a0f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90b46a0f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90c80bc610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90c80bc610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90b4530950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90b4530950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90b438bc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f90b438bc90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9010391590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9010391590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90b44cd450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90b44cd450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90b4505750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f90b4505750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90b451fd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90b451fd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8ff07e56d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f8ff07e56d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8ff075df90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8ff075df90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8ff0688290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f8ff0688290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f901011dd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f901011dd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f901026e850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f901026e850>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 1:42:53 - loss: 0.7654 - acc: 0.4844
 128/5677 [..............................] - ETA: 59:38 - loss: 0.8437 - acc: 0.4766  
 192/5677 [>.............................] - ETA: 45:19 - loss: 0.8228 - acc: 0.4583
 256/5677 [>.............................] - ETA: 37:58 - loss: 0.7954 - acc: 0.4766
 320/5677 [>.............................] - ETA: 33:09 - loss: 0.7806 - acc: 0.4844
 384/5677 [=>............................] - ETA: 29:54 - loss: 0.7654 - acc: 0.4922
 448/5677 [=>............................] - ETA: 27:31 - loss: 0.7555 - acc: 0.5000
 512/5677 [=>............................] - ETA: 25:38 - loss: 0.7489 - acc: 0.5098
 576/5677 [==>...........................] - ETA: 23:59 - loss: 0.7448 - acc: 0.5122
 640/5677 [==>...........................] - ETA: 22:45 - loss: 0.7434 - acc: 0.5047
 704/5677 [==>...........................] - ETA: 21:36 - loss: 0.7398 - acc: 0.5128
 768/5677 [===>..........................] - ETA: 20:36 - loss: 0.7367 - acc: 0.5143
 832/5677 [===>..........................] - ETA: 19:49 - loss: 0.7370 - acc: 0.5072
 896/5677 [===>..........................] - ETA: 19:04 - loss: 0.7326 - acc: 0.5156
 960/5677 [====>.........................] - ETA: 18:23 - loss: 0.7310 - acc: 0.5135
1024/5677 [====>.........................] - ETA: 17:48 - loss: 0.7271 - acc: 0.5156
1088/5677 [====>.........................] - ETA: 17:14 - loss: 0.7234 - acc: 0.5175
1152/5677 [=====>........................] - ETA: 16:46 - loss: 0.7243 - acc: 0.5165
1216/5677 [=====>........................] - ETA: 16:20 - loss: 0.7257 - acc: 0.5140
1280/5677 [=====>........................] - ETA: 15:52 - loss: 0.7262 - acc: 0.5125
1344/5677 [======>.......................] - ETA: 15:29 - loss: 0.7285 - acc: 0.5104
1408/5677 [======>.......................] - ETA: 15:03 - loss: 0.7277 - acc: 0.5107
1472/5677 [======>.......................] - ETA: 14:45 - loss: 0.7246 - acc: 0.5129
1536/5677 [=======>......................] - ETA: 14:22 - loss: 0.7214 - acc: 0.5182
1600/5677 [=======>......................] - ETA: 14:02 - loss: 0.7244 - acc: 0.5181
1664/5677 [=======>......................] - ETA: 13:40 - loss: 0.7228 - acc: 0.5180
1728/5677 [========>.....................] - ETA: 13:18 - loss: 0.7209 - acc: 0.5179
1792/5677 [========>.....................] - ETA: 12:58 - loss: 0.7214 - acc: 0.5167
1856/5677 [========>.....................] - ETA: 12:37 - loss: 0.7227 - acc: 0.5156
1920/5677 [=========>....................] - ETA: 12:20 - loss: 0.7221 - acc: 0.5130
1984/5677 [=========>....................] - ETA: 12:05 - loss: 0.7222 - acc: 0.5111
2048/5677 [=========>....................] - ETA: 11:49 - loss: 0.7215 - acc: 0.5098
2112/5677 [==========>...................] - ETA: 11:32 - loss: 0.7210 - acc: 0.5104
2176/5677 [==========>...................] - ETA: 11:15 - loss: 0.7213 - acc: 0.5097
2240/5677 [==========>...................] - ETA: 11:00 - loss: 0.7209 - acc: 0.5098
2304/5677 [===========>..................] - ETA: 10:43 - loss: 0.7210 - acc: 0.5113
2368/5677 [===========>..................] - ETA: 10:28 - loss: 0.7217 - acc: 0.5093
2432/5677 [===========>..................] - ETA: 10:13 - loss: 0.7217 - acc: 0.5090
2496/5677 [============>.................] - ETA: 9:58 - loss: 0.7213 - acc: 0.5100 
2560/5677 [============>.................] - ETA: 9:43 - loss: 0.7214 - acc: 0.5094
2624/5677 [============>.................] - ETA: 9:28 - loss: 0.7215 - acc: 0.5091
2688/5677 [=============>................] - ETA: 9:13 - loss: 0.7222 - acc: 0.5071
2752/5677 [=============>................] - ETA: 8:59 - loss: 0.7218 - acc: 0.5065
2816/5677 [=============>................] - ETA: 8:46 - loss: 0.7218 - acc: 0.5067
2880/5677 [==============>...............] - ETA: 8:32 - loss: 0.7217 - acc: 0.5062
2944/5677 [==============>...............] - ETA: 8:18 - loss: 0.7212 - acc: 0.5075
3008/5677 [==============>...............] - ETA: 8:04 - loss: 0.7211 - acc: 0.5076
3072/5677 [===============>..............] - ETA: 7:51 - loss: 0.7218 - acc: 0.5059
3136/5677 [===============>..............] - ETA: 7:38 - loss: 0.7221 - acc: 0.5048
3200/5677 [===============>..............] - ETA: 7:26 - loss: 0.7216 - acc: 0.5062
3264/5677 [================>.............] - ETA: 7:13 - loss: 0.7213 - acc: 0.5049
3328/5677 [================>.............] - ETA: 7:00 - loss: 0.7200 - acc: 0.5063
3392/5677 [================>.............] - ETA: 6:47 - loss: 0.7204 - acc: 0.5068
3456/5677 [=================>............] - ETA: 6:34 - loss: 0.7196 - acc: 0.5078
3520/5677 [=================>............] - ETA: 6:22 - loss: 0.7194 - acc: 0.5082
3584/5677 [=================>............] - ETA: 6:10 - loss: 0.7188 - acc: 0.5084
3648/5677 [==================>...........] - ETA: 5:58 - loss: 0.7187 - acc: 0.5082
3712/5677 [==================>...........] - ETA: 5:46 - loss: 0.7187 - acc: 0.5092
3776/5677 [==================>...........] - ETA: 5:34 - loss: 0.7193 - acc: 0.5079
3840/5677 [===================>..........] - ETA: 5:22 - loss: 0.7185 - acc: 0.5094
3904/5677 [===================>..........] - ETA: 5:10 - loss: 0.7187 - acc: 0.5090
3968/5677 [===================>..........] - ETA: 4:59 - loss: 0.7187 - acc: 0.5096
4032/5677 [====================>.........] - ETA: 4:47 - loss: 0.7187 - acc: 0.5089
4096/5677 [====================>.........] - ETA: 4:35 - loss: 0.7180 - acc: 0.5100
4160/5677 [====================>.........] - ETA: 4:24 - loss: 0.7169 - acc: 0.5118
4224/5677 [=====================>........] - ETA: 4:12 - loss: 0.7169 - acc: 0.5107
4288/5677 [=====================>........] - ETA: 4:00 - loss: 0.7174 - acc: 0.5098
4352/5677 [=====================>........] - ETA: 3:49 - loss: 0.7173 - acc: 0.5099
4416/5677 [======================>.......] - ETA: 3:38 - loss: 0.7170 - acc: 0.5106
4480/5677 [======================>.......] - ETA: 3:26 - loss: 0.7175 - acc: 0.5103
4544/5677 [=======================>......] - ETA: 3:15 - loss: 0.7173 - acc: 0.5103
4608/5677 [=======================>......] - ETA: 3:04 - loss: 0.7169 - acc: 0.5113
4672/5677 [=======================>......] - ETA: 2:52 - loss: 0.7170 - acc: 0.5116
4736/5677 [========================>.....] - ETA: 2:41 - loss: 0.7165 - acc: 0.5116
4800/5677 [========================>.....] - ETA: 2:30 - loss: 0.7163 - acc: 0.5125
4864/5677 [========================>.....] - ETA: 2:18 - loss: 0.7159 - acc: 0.5138
4928/5677 [=========================>....] - ETA: 2:07 - loss: 0.7164 - acc: 0.5134
4992/5677 [=========================>....] - ETA: 1:56 - loss: 0.7168 - acc: 0.5128
5056/5677 [=========================>....] - ETA: 1:45 - loss: 0.7167 - acc: 0.5129
5120/5677 [==========================>...] - ETA: 1:34 - loss: 0.7166 - acc: 0.5129
5184/5677 [==========================>...] - ETA: 1:23 - loss: 0.7165 - acc: 0.5125
5248/5677 [==========================>...] - ETA: 1:12 - loss: 0.7165 - acc: 0.5118
5312/5677 [===========================>..] - ETA: 1:01 - loss: 0.7170 - acc: 0.5105
5376/5677 [===========================>..] - ETA: 50s - loss: 0.7165 - acc: 0.5123 
5440/5677 [===========================>..] - ETA: 40s - loss: 0.7162 - acc: 0.5125
5504/5677 [============================>.] - ETA: 29s - loss: 0.7158 - acc: 0.5134
5568/5677 [============================>.] - ETA: 18s - loss: 0.7152 - acc: 0.5144
5632/5677 [============================>.] - ETA: 7s - loss: 0.7146 - acc: 0.5149 
5677/5677 [==============================] - 1012s 178ms/step - loss: 0.7143 - acc: 0.5158 - val_loss: 0.6982 - val_acc: 0.5040

Epoch 00001: val_acc improved from -inf to 0.50396, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window10/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 13:57 - loss: 0.7533 - acc: 0.4844
 128/5677 [..............................] - ETA: 14:13 - loss: 0.7178 - acc: 0.5391
 192/5677 [>.............................] - ETA: 14:38 - loss: 0.7105 - acc: 0.5260
 256/5677 [>.............................] - ETA: 14:44 - loss: 0.7032 - acc: 0.5312
 320/5677 [>.............................] - ETA: 14:29 - loss: 0.6931 - acc: 0.5469
 384/5677 [=>............................] - ETA: 14:26 - loss: 0.6946 - acc: 0.5443
 448/5677 [=>............................] - ETA: 13:59 - loss: 0.6933 - acc: 0.5402
 512/5677 [=>............................] - ETA: 13:42 - loss: 0.6935 - acc: 0.5371
 576/5677 [==>...........................] - ETA: 13:39 - loss: 0.6908 - acc: 0.5399
 640/5677 [==>...........................] - ETA: 13:34 - loss: 0.6864 - acc: 0.5516
 704/5677 [==>...........................] - ETA: 13:30 - loss: 0.6871 - acc: 0.5469
 768/5677 [===>..........................] - ETA: 13:19 - loss: 0.6871 - acc: 0.5443
 832/5677 [===>..........................] - ETA: 13:13 - loss: 0.6886 - acc: 0.5361
 896/5677 [===>..........................] - ETA: 13:04 - loss: 0.6894 - acc: 0.5368
 960/5677 [====>.........................] - ETA: 12:45 - loss: 0.6874 - acc: 0.5385
1024/5677 [====>.........................] - ETA: 12:37 - loss: 0.6844 - acc: 0.5479
1088/5677 [====>.........................] - ETA: 12:25 - loss: 0.6851 - acc: 0.5450
1152/5677 [=====>........................] - ETA: 12:11 - loss: 0.6855 - acc: 0.5443
1216/5677 [=====>........................] - ETA: 12:00 - loss: 0.6888 - acc: 0.5411
1280/5677 [=====>........................] - ETA: 11:48 - loss: 0.6891 - acc: 0.5414
1344/5677 [======>.......................] - ETA: 11:38 - loss: 0.6878 - acc: 0.5461
1408/5677 [======>.......................] - ETA: 11:26 - loss: 0.6894 - acc: 0.5447
1472/5677 [======>.......................] - ETA: 11:16 - loss: 0.6879 - acc: 0.5503
1536/5677 [=======>......................] - ETA: 11:08 - loss: 0.6877 - acc: 0.5521
1600/5677 [=======>......................] - ETA: 10:55 - loss: 0.6867 - acc: 0.5544
1664/5677 [=======>......................] - ETA: 10:44 - loss: 0.6860 - acc: 0.5559
1728/5677 [========>.....................] - ETA: 10:36 - loss: 0.6854 - acc: 0.5561
1792/5677 [========>.....................] - ETA: 10:26 - loss: 0.6823 - acc: 0.5619
1856/5677 [========>.....................] - ETA: 10:15 - loss: 0.6821 - acc: 0.5625
1920/5677 [=========>....................] - ETA: 10:05 - loss: 0.6823 - acc: 0.5615
1984/5677 [=========>....................] - ETA: 9:55 - loss: 0.6831 - acc: 0.5595 
2048/5677 [=========>....................] - ETA: 9:44 - loss: 0.6840 - acc: 0.5562
2112/5677 [==========>...................] - ETA: 9:33 - loss: 0.6839 - acc: 0.5554
2176/5677 [==========>...................] - ETA: 9:23 - loss: 0.6845 - acc: 0.5551
2240/5677 [==========>...................] - ETA: 9:12 - loss: 0.6840 - acc: 0.5563
2304/5677 [===========>..................] - ETA: 9:01 - loss: 0.6845 - acc: 0.5543
2368/5677 [===========>..................] - ETA: 8:51 - loss: 0.6857 - acc: 0.5528
2432/5677 [===========>..................] - ETA: 8:42 - loss: 0.6864 - acc: 0.5514
2496/5677 [============>.................] - ETA: 8:32 - loss: 0.6868 - acc: 0.5513
2560/5677 [============>.................] - ETA: 8:22 - loss: 0.6866 - acc: 0.5523
2624/5677 [============>.................] - ETA: 8:11 - loss: 0.6874 - acc: 0.5507
2688/5677 [=============>................] - ETA: 8:00 - loss: 0.6874 - acc: 0.5499
2752/5677 [=============>................] - ETA: 7:50 - loss: 0.6864 - acc: 0.5527
2816/5677 [=============>................] - ETA: 7:41 - loss: 0.6869 - acc: 0.5508
2880/5677 [==============>...............] - ETA: 7:29 - loss: 0.6869 - acc: 0.5514
2944/5677 [==============>...............] - ETA: 7:20 - loss: 0.6870 - acc: 0.5523
3008/5677 [==============>...............] - ETA: 7:09 - loss: 0.6870 - acc: 0.5522
3072/5677 [===============>..............] - ETA: 7:00 - loss: 0.6887 - acc: 0.5488
3136/5677 [===============>..............] - ETA: 6:50 - loss: 0.6884 - acc: 0.5501
3200/5677 [===============>..............] - ETA: 6:41 - loss: 0.6891 - acc: 0.5503
3264/5677 [================>.............] - ETA: 6:31 - loss: 0.6890 - acc: 0.5509
3328/5677 [================>.............] - ETA: 6:20 - loss: 0.6900 - acc: 0.5499
3392/5677 [================>.............] - ETA: 6:09 - loss: 0.6891 - acc: 0.5504
3456/5677 [=================>............] - ETA: 5:58 - loss: 0.6891 - acc: 0.5498
3520/5677 [=================>............] - ETA: 5:48 - loss: 0.6891 - acc: 0.5489
3584/5677 [=================>............] - ETA: 5:37 - loss: 0.6889 - acc: 0.5494
3648/5677 [==================>...........] - ETA: 5:27 - loss: 0.6889 - acc: 0.5482
3712/5677 [==================>...........] - ETA: 5:16 - loss: 0.6896 - acc: 0.5471
3776/5677 [==================>...........] - ETA: 5:06 - loss: 0.6896 - acc: 0.5477
3840/5677 [===================>..........] - ETA: 4:56 - loss: 0.6903 - acc: 0.5466
3904/5677 [===================>..........] - ETA: 4:46 - loss: 0.6906 - acc: 0.5451
3968/5677 [===================>..........] - ETA: 4:35 - loss: 0.6902 - acc: 0.5456
4032/5677 [====================>.........] - ETA: 4:25 - loss: 0.6906 - acc: 0.5444
4096/5677 [====================>.........] - ETA: 4:15 - loss: 0.6904 - acc: 0.5449
4160/5677 [====================>.........] - ETA: 4:04 - loss: 0.6904 - acc: 0.5445
4224/5677 [=====================>........] - ETA: 3:54 - loss: 0.6901 - acc: 0.5464
4288/5677 [=====================>........] - ETA: 3:44 - loss: 0.6907 - acc: 0.5457
4352/5677 [=====================>........] - ETA: 3:34 - loss: 0.6900 - acc: 0.5473
4416/5677 [======================>.......] - ETA: 3:24 - loss: 0.6902 - acc: 0.5464
4480/5677 [======================>.......] - ETA: 3:13 - loss: 0.6898 - acc: 0.5475
4544/5677 [=======================>......] - ETA: 3:03 - loss: 0.6898 - acc: 0.5484
4608/5677 [=======================>......] - ETA: 2:54 - loss: 0.6896 - acc: 0.5482
4672/5677 [=======================>......] - ETA: 2:43 - loss: 0.6894 - acc: 0.5488
4736/5677 [========================>.....] - ETA: 2:33 - loss: 0.6892 - acc: 0.5494
4800/5677 [========================>.....] - ETA: 2:22 - loss: 0.6893 - acc: 0.5494
4864/5677 [========================>.....] - ETA: 2:12 - loss: 0.6894 - acc: 0.5487
4928/5677 [=========================>....] - ETA: 2:02 - loss: 0.6897 - acc: 0.5481
4992/5677 [=========================>....] - ETA: 1:51 - loss: 0.6895 - acc: 0.5491
5056/5677 [=========================>....] - ETA: 1:41 - loss: 0.6897 - acc: 0.5485
5120/5677 [==========================>...] - ETA: 1:30 - loss: 0.6893 - acc: 0.5484
5184/5677 [==========================>...] - ETA: 1:20 - loss: 0.6896 - acc: 0.5475
5248/5677 [==========================>...] - ETA: 1:10 - loss: 0.6899 - acc: 0.5471
5312/5677 [===========================>..] - ETA: 59s - loss: 0.6901 - acc: 0.5471 
5376/5677 [===========================>..] - ETA: 49s - loss: 0.6900 - acc: 0.5472
5440/5677 [===========================>..] - ETA: 38s - loss: 0.6906 - acc: 0.5461
5504/5677 [============================>.] - ETA: 28s - loss: 0.6908 - acc: 0.5456
5568/5677 [============================>.] - ETA: 17s - loss: 0.6910 - acc: 0.5449
5632/5677 [============================>.] - ETA: 7s - loss: 0.6910 - acc: 0.5451 
5677/5677 [==============================] - 969s 171ms/step - loss: 0.6910 - acc: 0.5441 - val_loss: 0.6715 - val_acc: 0.5975

Epoch 00002: val_acc improved from 0.50396 to 0.59746, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window10/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 14:19 - loss: 0.6829 - acc: 0.5781
 128/5677 [..............................] - ETA: 14:44 - loss: 0.6935 - acc: 0.5469
 192/5677 [>.............................] - ETA: 14:23 - loss: 0.6969 - acc: 0.5417
 256/5677 [>.............................] - ETA: 13:44 - loss: 0.6965 - acc: 0.5273
 320/5677 [>.............................] - ETA: 13:43 - loss: 0.6944 - acc: 0.5281
 384/5677 [=>............................] - ETA: 13:40 - loss: 0.6868 - acc: 0.5391
 448/5677 [=>............................] - ETA: 13:26 - loss: 0.6866 - acc: 0.5536
 512/5677 [=>............................] - ETA: 13:20 - loss: 0.6869 - acc: 0.5508
 576/5677 [==>...........................] - ETA: 13:12 - loss: 0.6849 - acc: 0.5625
 640/5677 [==>...........................] - ETA: 13:01 - loss: 0.6837 - acc: 0.5672
 704/5677 [==>...........................] - ETA: 12:54 - loss: 0.6801 - acc: 0.5710
 768/5677 [===>..........................] - ETA: 12:36 - loss: 0.6794 - acc: 0.5703
 832/5677 [===>..........................] - ETA: 12:28 - loss: 0.6761 - acc: 0.5757
 896/5677 [===>..........................] - ETA: 12:20 - loss: 0.6762 - acc: 0.5714
 960/5677 [====>.........................] - ETA: 12:07 - loss: 0.6768 - acc: 0.5667
1024/5677 [====>.........................] - ETA: 11:58 - loss: 0.6793 - acc: 0.5645
1088/5677 [====>.........................] - ETA: 11:41 - loss: 0.6793 - acc: 0.5680
1152/5677 [=====>........................] - ETA: 11:30 - loss: 0.6777 - acc: 0.5712
1216/5677 [=====>........................] - ETA: 11:20 - loss: 0.6784 - acc: 0.5748
1280/5677 [=====>........................] - ETA: 11:09 - loss: 0.6798 - acc: 0.5719
1344/5677 [======>.......................] - ETA: 10:57 - loss: 0.6820 - acc: 0.5722
1408/5677 [======>.......................] - ETA: 10:45 - loss: 0.6806 - acc: 0.5703
1472/5677 [======>.......................] - ETA: 10:34 - loss: 0.6775 - acc: 0.5740
1536/5677 [=======>......................] - ETA: 10:24 - loss: 0.6769 - acc: 0.5749
1600/5677 [=======>......................] - ETA: 10:15 - loss: 0.6784 - acc: 0.5731
1664/5677 [=======>......................] - ETA: 10:07 - loss: 0.6785 - acc: 0.5715
1728/5677 [========>.....................] - ETA: 9:56 - loss: 0.6803 - acc: 0.5660 
1792/5677 [========>.....................] - ETA: 9:45 - loss: 0.6811 - acc: 0.5636
1856/5677 [========>.....................] - ETA: 9:34 - loss: 0.6821 - acc: 0.5641
1920/5677 [=========>....................] - ETA: 9:23 - loss: 0.6816 - acc: 0.5656
1984/5677 [=========>....................] - ETA: 9:13 - loss: 0.6821 - acc: 0.5655
2048/5677 [=========>....................] - ETA: 9:03 - loss: 0.6824 - acc: 0.5645
2112/5677 [==========>...................] - ETA: 8:54 - loss: 0.6814 - acc: 0.5663
2176/5677 [==========>...................] - ETA: 8:44 - loss: 0.6828 - acc: 0.5643
2240/5677 [==========>...................] - ETA: 8:35 - loss: 0.6832 - acc: 0.5629
2304/5677 [===========>..................] - ETA: 8:28 - loss: 0.6827 - acc: 0.5647
2368/5677 [===========>..................] - ETA: 8:19 - loss: 0.6834 - acc: 0.5621
2432/5677 [===========>..................] - ETA: 8:11 - loss: 0.6833 - acc: 0.5629
2496/5677 [============>.................] - ETA: 8:02 - loss: 0.6842 - acc: 0.5593
2560/5677 [============>.................] - ETA: 7:53 - loss: 0.6840 - acc: 0.5598
2624/5677 [============>.................] - ETA: 7:43 - loss: 0.6840 - acc: 0.5610
2688/5677 [=============>................] - ETA: 7:33 - loss: 0.6854 - acc: 0.5569
2752/5677 [=============>................] - ETA: 7:22 - loss: 0.6861 - acc: 0.5552
2816/5677 [=============>................] - ETA: 7:13 - loss: 0.6862 - acc: 0.5526
2880/5677 [==============>...............] - ETA: 7:03 - loss: 0.6858 - acc: 0.5538
2944/5677 [==============>...............] - ETA: 6:53 - loss: 0.6853 - acc: 0.5540
3008/5677 [==============>...............] - ETA: 6:43 - loss: 0.6857 - acc: 0.5529
3072/5677 [===============>..............] - ETA: 6:34 - loss: 0.6850 - acc: 0.5547
3136/5677 [===============>..............] - ETA: 6:24 - loss: 0.6847 - acc: 0.5558
3200/5677 [===============>..............] - ETA: 6:15 - loss: 0.6851 - acc: 0.5544
3264/5677 [================>.............] - ETA: 6:05 - loss: 0.6847 - acc: 0.5548
3328/5677 [================>.............] - ETA: 5:55 - loss: 0.6841 - acc: 0.5556
3392/5677 [================>.............] - ETA: 5:45 - loss: 0.6835 - acc: 0.5563
3456/5677 [=================>............] - ETA: 5:35 - loss: 0.6840 - acc: 0.5556
3520/5677 [=================>............] - ETA: 5:25 - loss: 0.6839 - acc: 0.5557
3584/5677 [=================>............] - ETA: 5:16 - loss: 0.6833 - acc: 0.5564
3648/5677 [==================>...........] - ETA: 5:06 - loss: 0.6822 - acc: 0.5592
3712/5677 [==================>...........] - ETA: 4:56 - loss: 0.6820 - acc: 0.5587
3776/5677 [==================>...........] - ETA: 4:46 - loss: 0.6828 - acc: 0.5564
3840/5677 [===================>..........] - ETA: 4:37 - loss: 0.6820 - acc: 0.5570
3904/5677 [===================>..........] - ETA: 4:27 - loss: 0.6827 - acc: 0.5566
3968/5677 [===================>..........] - ETA: 4:17 - loss: 0.6826 - acc: 0.5570
4032/5677 [====================>.........] - ETA: 4:07 - loss: 0.6831 - acc: 0.5575
4096/5677 [====================>.........] - ETA: 3:58 - loss: 0.6835 - acc: 0.5554
4160/5677 [====================>.........] - ETA: 3:48 - loss: 0.6842 - acc: 0.5534
4224/5677 [=====================>........] - ETA: 3:38 - loss: 0.6835 - acc: 0.5545
4288/5677 [=====================>........] - ETA: 3:28 - loss: 0.6832 - acc: 0.5557
4352/5677 [=====================>........] - ETA: 3:18 - loss: 0.6831 - acc: 0.5563
4416/5677 [======================>.......] - ETA: 3:08 - loss: 0.6833 - acc: 0.5559
4480/5677 [======================>.......] - ETA: 2:59 - loss: 0.6831 - acc: 0.5571
4544/5677 [=======================>......] - ETA: 2:49 - loss: 0.6833 - acc: 0.5568
4608/5677 [=======================>......] - ETA: 2:39 - loss: 0.6830 - acc: 0.5571
4672/5677 [=======================>......] - ETA: 2:30 - loss: 0.6830 - acc: 0.5576
4736/5677 [========================>.....] - ETA: 2:20 - loss: 0.6826 - acc: 0.5581
4800/5677 [========================>.....] - ETA: 2:10 - loss: 0.6828 - acc: 0.5592
4864/5677 [========================>.....] - ETA: 2:01 - loss: 0.6831 - acc: 0.5582
4928/5677 [=========================>....] - ETA: 1:51 - loss: 0.6831 - acc: 0.5582
4992/5677 [=========================>....] - ETA: 1:41 - loss: 0.6826 - acc: 0.5591
5056/5677 [=========================>....] - ETA: 1:32 - loss: 0.6826 - acc: 0.5593
5120/5677 [==========================>...] - ETA: 1:22 - loss: 0.6828 - acc: 0.5592
5184/5677 [==========================>...] - ETA: 1:13 - loss: 0.6834 - acc: 0.5584
5248/5677 [==========================>...] - ETA: 1:03 - loss: 0.6834 - acc: 0.5575
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6838 - acc: 0.5578 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6842 - acc: 0.5564
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6844 - acc: 0.5555
5504/5677 [============================>.] - ETA: 25s - loss: 0.6845 - acc: 0.5558
5568/5677 [============================>.] - ETA: 16s - loss: 0.6845 - acc: 0.5562
5632/5677 [============================>.] - ETA: 6s - loss: 0.6841 - acc: 0.5568 
5677/5677 [==============================] - 870s 153ms/step - loss: 0.6840 - acc: 0.5577 - val_loss: 0.6811 - val_acc: 0.5658

Epoch 00003: val_acc did not improve from 0.59746
Epoch 4/10

  64/5677 [..............................] - ETA: 12:21 - loss: 0.6987 - acc: 0.5781
 128/5677 [..............................] - ETA: 12:22 - loss: 0.6894 - acc: 0.5391
 192/5677 [>.............................] - ETA: 12:03 - loss: 0.6796 - acc: 0.5677
 256/5677 [>.............................] - ETA: 12:01 - loss: 0.6846 - acc: 0.5508
 320/5677 [>.............................] - ETA: 11:51 - loss: 0.6831 - acc: 0.5531
 384/5677 [=>............................] - ETA: 11:45 - loss: 0.6794 - acc: 0.5599
 448/5677 [=>............................] - ETA: 11:45 - loss: 0.6798 - acc: 0.5536
 512/5677 [=>............................] - ETA: 11:35 - loss: 0.6766 - acc: 0.5625
 576/5677 [==>...........................] - ETA: 11:27 - loss: 0.6815 - acc: 0.5556
 640/5677 [==>...........................] - ETA: 11:18 - loss: 0.6762 - acc: 0.5703
 704/5677 [==>...........................] - ETA: 11:12 - loss: 0.6786 - acc: 0.5668
 768/5677 [===>..........................] - ETA: 11:03 - loss: 0.6796 - acc: 0.5651
 832/5677 [===>..........................] - ETA: 10:57 - loss: 0.6843 - acc: 0.5613
 896/5677 [===>..........................] - ETA: 10:48 - loss: 0.6799 - acc: 0.5681
 960/5677 [====>.........................] - ETA: 10:42 - loss: 0.6802 - acc: 0.5677
1024/5677 [====>.........................] - ETA: 10:33 - loss: 0.6824 - acc: 0.5674
1088/5677 [====>.........................] - ETA: 10:23 - loss: 0.6830 - acc: 0.5708
1152/5677 [=====>........................] - ETA: 10:11 - loss: 0.6819 - acc: 0.5703
1216/5677 [=====>........................] - ETA: 10:02 - loss: 0.6821 - acc: 0.5699
1280/5677 [=====>........................] - ETA: 9:55 - loss: 0.6816 - acc: 0.5703 
1344/5677 [======>.......................] - ETA: 9:46 - loss: 0.6829 - acc: 0.5685
1408/5677 [======>.......................] - ETA: 9:38 - loss: 0.6829 - acc: 0.5689
1472/5677 [======>.......................] - ETA: 9:29 - loss: 0.6825 - acc: 0.5673
1536/5677 [=======>......................] - ETA: 9:21 - loss: 0.6814 - acc: 0.5703
1600/5677 [=======>......................] - ETA: 9:13 - loss: 0.6815 - acc: 0.5737
1664/5677 [=======>......................] - ETA: 9:03 - loss: 0.6795 - acc: 0.5757
1728/5677 [========>.....................] - ETA: 8:54 - loss: 0.6805 - acc: 0.5723
1792/5677 [========>.....................] - ETA: 8:46 - loss: 0.6795 - acc: 0.5748
1856/5677 [========>.....................] - ETA: 8:37 - loss: 0.6802 - acc: 0.5733
1920/5677 [=========>....................] - ETA: 8:30 - loss: 0.6811 - acc: 0.5714
1984/5677 [=========>....................] - ETA: 8:20 - loss: 0.6817 - acc: 0.5685
2048/5677 [=========>....................] - ETA: 8:12 - loss: 0.6818 - acc: 0.5679
2112/5677 [==========>...................] - ETA: 8:03 - loss: 0.6815 - acc: 0.5672
2176/5677 [==========>...................] - ETA: 7:53 - loss: 0.6809 - acc: 0.5676
2240/5677 [==========>...................] - ETA: 7:45 - loss: 0.6812 - acc: 0.5665
2304/5677 [===========>..................] - ETA: 7:36 - loss: 0.6818 - acc: 0.5642
2368/5677 [===========>..................] - ETA: 7:28 - loss: 0.6817 - acc: 0.5638
2432/5677 [===========>..................] - ETA: 7:19 - loss: 0.6818 - acc: 0.5637
2496/5677 [============>.................] - ETA: 7:10 - loss: 0.6827 - acc: 0.5597
2560/5677 [============>.................] - ETA: 7:01 - loss: 0.6841 - acc: 0.5582
2624/5677 [============>.................] - ETA: 6:51 - loss: 0.6832 - acc: 0.5595
2688/5677 [=============>................] - ETA: 6:43 - loss: 0.6827 - acc: 0.5614
2752/5677 [=============>................] - ETA: 6:34 - loss: 0.6831 - acc: 0.5629
2816/5677 [=============>................] - ETA: 6:25 - loss: 0.6835 - acc: 0.5621
2880/5677 [==============>...............] - ETA: 6:16 - loss: 0.6840 - acc: 0.5618
2944/5677 [==============>...............] - ETA: 6:07 - loss: 0.6840 - acc: 0.5611
3008/5677 [==============>...............] - ETA: 5:59 - loss: 0.6845 - acc: 0.5598
3072/5677 [===============>..............] - ETA: 5:50 - loss: 0.6844 - acc: 0.5609
3136/5677 [===============>..............] - ETA: 5:42 - loss: 0.6849 - acc: 0.5590
3200/5677 [===============>..............] - ETA: 5:33 - loss: 0.6839 - acc: 0.5606
3264/5677 [================>.............] - ETA: 5:24 - loss: 0.6843 - acc: 0.5600
3328/5677 [================>.............] - ETA: 5:15 - loss: 0.6838 - acc: 0.5613
3392/5677 [================>.............] - ETA: 5:07 - loss: 0.6844 - acc: 0.5587
3456/5677 [=================>............] - ETA: 4:58 - loss: 0.6834 - acc: 0.5599
3520/5677 [=================>............] - ETA: 4:50 - loss: 0.6836 - acc: 0.5597
3584/5677 [=================>............] - ETA: 4:42 - loss: 0.6843 - acc: 0.5589
3648/5677 [==================>...........] - ETA: 4:33 - loss: 0.6834 - acc: 0.5614
3712/5677 [==================>...........] - ETA: 4:24 - loss: 0.6828 - acc: 0.5633
3776/5677 [==================>...........] - ETA: 4:15 - loss: 0.6833 - acc: 0.5612
3840/5677 [===================>..........] - ETA: 4:07 - loss: 0.6835 - acc: 0.5620
3904/5677 [===================>..........] - ETA: 3:58 - loss: 0.6833 - acc: 0.5635
3968/5677 [===================>..........] - ETA: 3:50 - loss: 0.6830 - acc: 0.5630
4032/5677 [====================>.........] - ETA: 3:41 - loss: 0.6833 - acc: 0.5623
4096/5677 [====================>.........] - ETA: 3:33 - loss: 0.6825 - acc: 0.5632
4160/5677 [====================>.........] - ETA: 3:24 - loss: 0.6821 - acc: 0.5642
4224/5677 [=====================>........] - ETA: 3:15 - loss: 0.6821 - acc: 0.5634
4288/5677 [=====================>........] - ETA: 3:06 - loss: 0.6825 - acc: 0.5632
4352/5677 [=====================>........] - ETA: 2:58 - loss: 0.6825 - acc: 0.5620
4416/5677 [======================>.......] - ETA: 2:49 - loss: 0.6823 - acc: 0.5625
4480/5677 [======================>.......] - ETA: 2:41 - loss: 0.6819 - acc: 0.5634
4544/5677 [=======================>......] - ETA: 2:32 - loss: 0.6816 - acc: 0.5636
4608/5677 [=======================>......] - ETA: 2:24 - loss: 0.6827 - acc: 0.5621
4672/5677 [=======================>......] - ETA: 2:15 - loss: 0.6833 - acc: 0.5614
4736/5677 [========================>.....] - ETA: 2:06 - loss: 0.6828 - acc: 0.5633
4800/5677 [========================>.....] - ETA: 1:58 - loss: 0.6827 - acc: 0.5625
4864/5677 [========================>.....] - ETA: 1:49 - loss: 0.6831 - acc: 0.5621
4928/5677 [=========================>....] - ETA: 1:40 - loss: 0.6828 - acc: 0.5629
4992/5677 [=========================>....] - ETA: 1:32 - loss: 0.6831 - acc: 0.5629
5056/5677 [=========================>....] - ETA: 1:23 - loss: 0.6822 - acc: 0.5653
5120/5677 [==========================>...] - ETA: 1:15 - loss: 0.6819 - acc: 0.5658
5184/5677 [==========================>...] - ETA: 1:06 - loss: 0.6816 - acc: 0.5652
5248/5677 [==========================>...] - ETA: 57s - loss: 0.6813 - acc: 0.5652 
5312/5677 [===========================>..] - ETA: 49s - loss: 0.6814 - acc: 0.5646
5376/5677 [===========================>..] - ETA: 40s - loss: 0.6812 - acc: 0.5647
5440/5677 [===========================>..] - ETA: 32s - loss: 0.6812 - acc: 0.5649
5504/5677 [============================>.] - ETA: 23s - loss: 0.6814 - acc: 0.5643
5568/5677 [============================>.] - ETA: 14s - loss: 0.6819 - acc: 0.5643
5632/5677 [============================>.] - ETA: 6s - loss: 0.6821 - acc: 0.5645 
5677/5677 [==============================] - 803s 141ms/step - loss: 0.6820 - acc: 0.5649 - val_loss: 0.6798 - val_acc: 0.5689

Epoch 00004: val_acc did not improve from 0.59746
Epoch 5/10

  64/5677 [..............................] - ETA: 12:19 - loss: 0.6357 - acc: 0.6875
 128/5677 [..............................] - ETA: 12:27 - loss: 0.6372 - acc: 0.6562
 192/5677 [>.............................] - ETA: 12:06 - loss: 0.6526 - acc: 0.6094
 256/5677 [>.............................] - ETA: 12:06 - loss: 0.6684 - acc: 0.5742
 320/5677 [>.............................] - ETA: 12:08 - loss: 0.6629 - acc: 0.5844
 384/5677 [=>............................] - ETA: 11:53 - loss: 0.6650 - acc: 0.5911
 448/5677 [=>............................] - ETA: 11:46 - loss: 0.6705 - acc: 0.5804
 512/5677 [=>............................] - ETA: 11:36 - loss: 0.6732 - acc: 0.5742
 576/5677 [==>...........................] - ETA: 11:24 - loss: 0.6700 - acc: 0.5781
 640/5677 [==>...........................] - ETA: 11:12 - loss: 0.6720 - acc: 0.5734
 704/5677 [==>...........................] - ETA: 11:03 - loss: 0.6702 - acc: 0.5767
 768/5677 [===>..........................] - ETA: 10:54 - loss: 0.6706 - acc: 0.5794
 832/5677 [===>..........................] - ETA: 10:45 - loss: 0.6670 - acc: 0.5877
 896/5677 [===>..........................] - ETA: 10:41 - loss: 0.6685 - acc: 0.5871
 960/5677 [====>.........................] - ETA: 10:33 - loss: 0.6691 - acc: 0.5885
1024/5677 [====>.........................] - ETA: 10:23 - loss: 0.6664 - acc: 0.5908
1088/5677 [====>.........................] - ETA: 10:16 - loss: 0.6664 - acc: 0.5901
1152/5677 [=====>........................] - ETA: 10:08 - loss: 0.6680 - acc: 0.5868
1216/5677 [=====>........................] - ETA: 9:59 - loss: 0.6680 - acc: 0.5872 
1280/5677 [=====>........................] - ETA: 9:51 - loss: 0.6680 - acc: 0.5867
1344/5677 [======>.......................] - ETA: 9:41 - loss: 0.6686 - acc: 0.5871
1408/5677 [======>.......................] - ETA: 9:32 - loss: 0.6681 - acc: 0.5888
1472/5677 [======>.......................] - ETA: 9:24 - loss: 0.6678 - acc: 0.5897
1536/5677 [=======>......................] - ETA: 9:14 - loss: 0.6681 - acc: 0.5892
1600/5677 [=======>......................] - ETA: 9:05 - loss: 0.6685 - acc: 0.5887
1664/5677 [=======>......................] - ETA: 8:57 - loss: 0.6669 - acc: 0.5895
1728/5677 [========>.....................] - ETA: 8:49 - loss: 0.6672 - acc: 0.5880
1792/5677 [========>.....................] - ETA: 8:40 - loss: 0.6680 - acc: 0.5859
1856/5677 [========>.....................] - ETA: 8:32 - loss: 0.6681 - acc: 0.5867
1920/5677 [=========>....................] - ETA: 8:24 - loss: 0.6680 - acc: 0.5880
1984/5677 [=========>....................] - ETA: 8:16 - loss: 0.6676 - acc: 0.5887
2048/5677 [=========>....................] - ETA: 8:10 - loss: 0.6662 - acc: 0.5903
2112/5677 [==========>...................] - ETA: 8:00 - loss: 0.6649 - acc: 0.5919
2176/5677 [==========>...................] - ETA: 7:52 - loss: 0.6660 - acc: 0.5896
2240/5677 [==========>...................] - ETA: 7:44 - loss: 0.6653 - acc: 0.5902
2304/5677 [===========>..................] - ETA: 7:34 - loss: 0.6671 - acc: 0.5877
2368/5677 [===========>..................] - ETA: 7:26 - loss: 0.6673 - acc: 0.5866
2432/5677 [===========>..................] - ETA: 7:19 - loss: 0.6675 - acc: 0.5872
2496/5677 [============>.................] - ETA: 7:09 - loss: 0.6674 - acc: 0.5877
2560/5677 [============>.................] - ETA: 7:01 - loss: 0.6676 - acc: 0.5871
2624/5677 [============>.................] - ETA: 6:52 - loss: 0.6677 - acc: 0.5865
2688/5677 [=============>................] - ETA: 6:43 - loss: 0.6679 - acc: 0.5867
2752/5677 [=============>................] - ETA: 6:35 - loss: 0.6669 - acc: 0.5876
2816/5677 [=============>................] - ETA: 6:26 - loss: 0.6670 - acc: 0.5859
2880/5677 [==============>...............] - ETA: 6:17 - loss: 0.6675 - acc: 0.5854
2944/5677 [==============>...............] - ETA: 6:09 - loss: 0.6676 - acc: 0.5856
3008/5677 [==============>...............] - ETA: 6:00 - loss: 0.6676 - acc: 0.5854
3072/5677 [===============>..............] - ETA: 5:52 - loss: 0.6667 - acc: 0.5882
3136/5677 [===============>..............] - ETA: 5:42 - loss: 0.6679 - acc: 0.5864
3200/5677 [===============>..............] - ETA: 5:33 - loss: 0.6676 - acc: 0.5872
3264/5677 [================>.............] - ETA: 5:25 - loss: 0.6673 - acc: 0.5879
3328/5677 [================>.............] - ETA: 5:16 - loss: 0.6668 - acc: 0.5886
3392/5677 [================>.............] - ETA: 5:08 - loss: 0.6671 - acc: 0.5876
3456/5677 [=================>............] - ETA: 4:59 - loss: 0.6671 - acc: 0.5883
3520/5677 [=================>............] - ETA: 4:51 - loss: 0.6672 - acc: 0.5869
3584/5677 [=================>............] - ETA: 4:42 - loss: 0.6670 - acc: 0.5876
3648/5677 [==================>...........] - ETA: 4:33 - loss: 0.6678 - acc: 0.5863
3712/5677 [==================>...........] - ETA: 4:24 - loss: 0.6688 - acc: 0.5859
3776/5677 [==================>...........] - ETA: 4:16 - loss: 0.6699 - acc: 0.5850
3840/5677 [===================>..........] - ETA: 4:08 - loss: 0.6702 - acc: 0.5841
3904/5677 [===================>..........] - ETA: 3:59 - loss: 0.6700 - acc: 0.5850
3968/5677 [===================>..........] - ETA: 3:51 - loss: 0.6698 - acc: 0.5852
4032/5677 [====================>.........] - ETA: 3:42 - loss: 0.6700 - acc: 0.5843
4096/5677 [====================>.........] - ETA: 3:34 - loss: 0.6705 - acc: 0.5835
4160/5677 [====================>.........] - ETA: 3:26 - loss: 0.6700 - acc: 0.5853
4224/5677 [=====================>........] - ETA: 3:17 - loss: 0.6707 - acc: 0.5838
4288/5677 [=====================>........] - ETA: 3:09 - loss: 0.6711 - acc: 0.5830
4352/5677 [=====================>........] - ETA: 3:01 - loss: 0.6713 - acc: 0.5830
4416/5677 [======================>.......] - ETA: 2:52 - loss: 0.6717 - acc: 0.5829
4480/5677 [======================>.......] - ETA: 2:44 - loss: 0.6718 - acc: 0.5828
4544/5677 [=======================>......] - ETA: 2:35 - loss: 0.6718 - acc: 0.5825
4608/5677 [=======================>......] - ETA: 2:27 - loss: 0.6719 - acc: 0.5820
4672/5677 [=======================>......] - ETA: 2:18 - loss: 0.6720 - acc: 0.5818
4736/5677 [========================>.....] - ETA: 2:09 - loss: 0.6717 - acc: 0.5826
4800/5677 [========================>.....] - ETA: 2:00 - loss: 0.6719 - acc: 0.5821
4864/5677 [========================>.....] - ETA: 1:52 - loss: 0.6725 - acc: 0.5810
4928/5677 [=========================>....] - ETA: 1:43 - loss: 0.6723 - acc: 0.5822
4992/5677 [=========================>....] - ETA: 1:34 - loss: 0.6726 - acc: 0.5817
5056/5677 [=========================>....] - ETA: 1:25 - loss: 0.6731 - acc: 0.5803
5120/5677 [==========================>...] - ETA: 1:16 - loss: 0.6729 - acc: 0.5803
5184/5677 [==========================>...] - ETA: 1:08 - loss: 0.6725 - acc: 0.5810
5248/5677 [==========================>...] - ETA: 59s - loss: 0.6725 - acc: 0.5804 
5312/5677 [===========================>..] - ETA: 50s - loss: 0.6728 - acc: 0.5802
5376/5677 [===========================>..] - ETA: 41s - loss: 0.6729 - acc: 0.5798
5440/5677 [===========================>..] - ETA: 32s - loss: 0.6739 - acc: 0.5774
5504/5677 [============================>.] - ETA: 23s - loss: 0.6738 - acc: 0.5779
5568/5677 [============================>.] - ETA: 15s - loss: 0.6742 - acc: 0.5774
5632/5677 [============================>.] - ETA: 6s - loss: 0.6748 - acc: 0.5760 
5677/5677 [==============================] - 822s 145ms/step - loss: 0.6749 - acc: 0.5764 - val_loss: 0.6653 - val_acc: 0.5975

Epoch 00005: val_acc improved from 0.59746 to 0.59746, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window10/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 14:40 - loss: 0.7175 - acc: 0.5469
 128/5677 [..............................] - ETA: 13:25 - loss: 0.7137 - acc: 0.5391
 192/5677 [>.............................] - ETA: 14:04 - loss: 0.6945 - acc: 0.5625
 256/5677 [>.............................] - ETA: 13:46 - loss: 0.6828 - acc: 0.5781
 320/5677 [>.............................] - ETA: 14:05 - loss: 0.6809 - acc: 0.5938
 384/5677 [=>............................] - ETA: 13:54 - loss: 0.6821 - acc: 0.5859
 448/5677 [=>............................] - ETA: 13:39 - loss: 0.6907 - acc: 0.5603
 512/5677 [=>............................] - ETA: 13:27 - loss: 0.6830 - acc: 0.5703
 576/5677 [==>...........................] - ETA: 13:12 - loss: 0.6858 - acc: 0.5608
 640/5677 [==>...........................] - ETA: 13:03 - loss: 0.6877 - acc: 0.5625
 704/5677 [==>...........................] - ETA: 12:48 - loss: 0.6866 - acc: 0.5653
 768/5677 [===>..........................] - ETA: 12:35 - loss: 0.6836 - acc: 0.5677
 832/5677 [===>..........................] - ETA: 12:22 - loss: 0.6792 - acc: 0.5733
 896/5677 [===>..........................] - ETA: 12:09 - loss: 0.6817 - acc: 0.5692
 960/5677 [====>.........................] - ETA: 12:00 - loss: 0.6805 - acc: 0.5708
1024/5677 [====>.........................] - ETA: 11:47 - loss: 0.6792 - acc: 0.5703
1088/5677 [====>.........................] - ETA: 11:35 - loss: 0.6767 - acc: 0.5735
1152/5677 [=====>........................] - ETA: 11:23 - loss: 0.6778 - acc: 0.5738
1216/5677 [=====>........................] - ETA: 11:09 - loss: 0.6785 - acc: 0.5732
1280/5677 [=====>........................] - ETA: 10:59 - loss: 0.6764 - acc: 0.5789
1344/5677 [======>.......................] - ETA: 10:52 - loss: 0.6752 - acc: 0.5796
1408/5677 [======>.......................] - ETA: 10:41 - loss: 0.6733 - acc: 0.5817
1472/5677 [======>.......................] - ETA: 10:32 - loss: 0.6734 - acc: 0.5829
1536/5677 [=======>......................] - ETA: 10:20 - loss: 0.6744 - acc: 0.5833
1600/5677 [=======>......................] - ETA: 10:13 - loss: 0.6750 - acc: 0.5819
1664/5677 [=======>......................] - ETA: 10:05 - loss: 0.6760 - acc: 0.5787
1728/5677 [========>.....................] - ETA: 9:54 - loss: 0.6743 - acc: 0.5828 
1792/5677 [========>.....................] - ETA: 9:43 - loss: 0.6749 - acc: 0.5815
1856/5677 [========>.....................] - ETA: 9:35 - loss: 0.6745 - acc: 0.5841
1920/5677 [=========>....................] - ETA: 9:25 - loss: 0.6744 - acc: 0.5833
1984/5677 [=========>....................] - ETA: 9:16 - loss: 0.6738 - acc: 0.5867
2048/5677 [=========>....................] - ETA: 9:07 - loss: 0.6732 - acc: 0.5874
2112/5677 [==========>...................] - ETA: 8:57 - loss: 0.6726 - acc: 0.5890
2176/5677 [==========>...................] - ETA: 8:46 - loss: 0.6735 - acc: 0.5882
2240/5677 [==========>...................] - ETA: 8:36 - loss: 0.6750 - acc: 0.5862
2304/5677 [===========>..................] - ETA: 8:28 - loss: 0.6743 - acc: 0.5868
2368/5677 [===========>..................] - ETA: 8:18 - loss: 0.6738 - acc: 0.5883
2432/5677 [===========>..................] - ETA: 8:07 - loss: 0.6733 - acc: 0.5888
2496/5677 [============>.................] - ETA: 7:56 - loss: 0.6744 - acc: 0.5873
2560/5677 [============>.................] - ETA: 7:46 - loss: 0.6729 - acc: 0.5895
2624/5677 [============>.................] - ETA: 7:35 - loss: 0.6738 - acc: 0.5884
2688/5677 [=============>................] - ETA: 7:26 - loss: 0.6749 - acc: 0.5859
2752/5677 [=============>................] - ETA: 7:16 - loss: 0.6746 - acc: 0.5854
2816/5677 [=============>................] - ETA: 7:06 - loss: 0.6751 - acc: 0.5838
2880/5677 [==============>...............] - ETA: 6:57 - loss: 0.6751 - acc: 0.5840
2944/5677 [==============>...............] - ETA: 6:48 - loss: 0.6757 - acc: 0.5832
3008/5677 [==============>...............] - ETA: 6:39 - loss: 0.6757 - acc: 0.5834
3072/5677 [===============>..............] - ETA: 6:29 - loss: 0.6758 - acc: 0.5850
3136/5677 [===============>..............] - ETA: 6:19 - loss: 0.6760 - acc: 0.5858
3200/5677 [===============>..............] - ETA: 6:09 - loss: 0.6755 - acc: 0.5863
3264/5677 [================>.............] - ETA: 6:00 - loss: 0.6750 - acc: 0.5870
3328/5677 [================>.............] - ETA: 5:50 - loss: 0.6752 - acc: 0.5871
3392/5677 [================>.............] - ETA: 5:41 - loss: 0.6750 - acc: 0.5870
3456/5677 [=================>............] - ETA: 5:31 - loss: 0.6745 - acc: 0.5883
3520/5677 [=================>............] - ETA: 5:22 - loss: 0.6749 - acc: 0.5881
3584/5677 [=================>............] - ETA: 5:13 - loss: 0.6744 - acc: 0.5890
3648/5677 [==================>...........] - ETA: 5:03 - loss: 0.6743 - acc: 0.5894
3712/5677 [==================>...........] - ETA: 4:54 - loss: 0.6736 - acc: 0.5900
3776/5677 [==================>...........] - ETA: 4:45 - loss: 0.6740 - acc: 0.5885
3840/5677 [===================>..........] - ETA: 4:36 - loss: 0.6735 - acc: 0.5888
3904/5677 [===================>..........] - ETA: 4:27 - loss: 0.6738 - acc: 0.5886
3968/5677 [===================>..........] - ETA: 4:18 - loss: 0.6733 - acc: 0.5885
4032/5677 [====================>.........] - ETA: 4:08 - loss: 0.6750 - acc: 0.5868
4096/5677 [====================>.........] - ETA: 3:59 - loss: 0.6742 - acc: 0.5881
4160/5677 [====================>.........] - ETA: 3:50 - loss: 0.6746 - acc: 0.5880
4224/5677 [=====================>........] - ETA: 3:40 - loss: 0.6751 - acc: 0.5869
4288/5677 [=====================>........] - ETA: 3:30 - loss: 0.6749 - acc: 0.5879
4352/5677 [=====================>........] - ETA: 3:21 - loss: 0.6751 - acc: 0.5873
4416/5677 [======================>.......] - ETA: 3:11 - loss: 0.6751 - acc: 0.5879
4480/5677 [======================>.......] - ETA: 3:01 - loss: 0.6752 - acc: 0.5875
4544/5677 [=======================>......] - ETA: 2:52 - loss: 0.6759 - acc: 0.5860
4608/5677 [=======================>......] - ETA: 2:42 - loss: 0.6760 - acc: 0.5857
4672/5677 [=======================>......] - ETA: 2:33 - loss: 0.6756 - acc: 0.5869
4736/5677 [========================>.....] - ETA: 2:23 - loss: 0.6759 - acc: 0.5861
4800/5677 [========================>.....] - ETA: 2:14 - loss: 0.6761 - acc: 0.5852
4864/5677 [========================>.....] - ETA: 2:04 - loss: 0.6764 - acc: 0.5845
4928/5677 [=========================>....] - ETA: 1:54 - loss: 0.6761 - acc: 0.5854
4992/5677 [=========================>....] - ETA: 1:44 - loss: 0.6756 - acc: 0.5869
5056/5677 [=========================>....] - ETA: 1:35 - loss: 0.6755 - acc: 0.5854
5120/5677 [==========================>...] - ETA: 1:25 - loss: 0.6754 - acc: 0.5861
5184/5677 [==========================>...] - ETA: 1:15 - loss: 0.6756 - acc: 0.5856
5248/5677 [==========================>...] - ETA: 1:05 - loss: 0.6759 - acc: 0.5846
5312/5677 [===========================>..] - ETA: 56s - loss: 0.6760 - acc: 0.5849 
5376/5677 [===========================>..] - ETA: 46s - loss: 0.6762 - acc: 0.5837
5440/5677 [===========================>..] - ETA: 36s - loss: 0.6765 - acc: 0.5831
5504/5677 [============================>.] - ETA: 26s - loss: 0.6762 - acc: 0.5836
5568/5677 [============================>.] - ETA: 16s - loss: 0.6760 - acc: 0.5841
5632/5677 [============================>.] - ETA: 6s - loss: 0.6756 - acc: 0.5840 
5677/5677 [==============================] - 907s 160ms/step - loss: 0.6755 - acc: 0.5836 - val_loss: 0.7221 - val_acc: 0.4992

Epoch 00006: val_acc did not improve from 0.59746
Epoch 7/10

  64/5677 [..............................] - ETA: 16:02 - loss: 0.7235 - acc: 0.5000
 128/5677 [..............................] - ETA: 15:29 - loss: 0.7096 - acc: 0.5312
 192/5677 [>.............................] - ETA: 14:57 - loss: 0.7025 - acc: 0.5573
 256/5677 [>.............................] - ETA: 14:33 - loss: 0.7013 - acc: 0.5586
 320/5677 [>.............................] - ETA: 14:09 - loss: 0.6935 - acc: 0.5687
 384/5677 [=>............................] - ETA: 13:47 - loss: 0.6957 - acc: 0.5521
 448/5677 [=>............................] - ETA: 13:39 - loss: 0.6914 - acc: 0.5603
 512/5677 [=>............................] - ETA: 13:32 - loss: 0.6885 - acc: 0.5605
 576/5677 [==>...........................] - ETA: 13:26 - loss: 0.6904 - acc: 0.5538
 640/5677 [==>...........................] - ETA: 13:16 - loss: 0.6891 - acc: 0.5547
 704/5677 [==>...........................] - ETA: 13:10 - loss: 0.6909 - acc: 0.5526
 768/5677 [===>..........................] - ETA: 12:55 - loss: 0.6885 - acc: 0.5547
 832/5677 [===>..........................] - ETA: 12:44 - loss: 0.6861 - acc: 0.5601
 896/5677 [===>..........................] - ETA: 12:34 - loss: 0.6858 - acc: 0.5603
 960/5677 [====>.........................] - ETA: 12:30 - loss: 0.6833 - acc: 0.5563
1024/5677 [====>.........................] - ETA: 12:21 - loss: 0.6853 - acc: 0.5566
1088/5677 [====>.........................] - ETA: 12:06 - loss: 0.6863 - acc: 0.5570
1152/5677 [=====>........................] - ETA: 11:58 - loss: 0.6846 - acc: 0.5642
1216/5677 [=====>........................] - ETA: 11:48 - loss: 0.6835 - acc: 0.5641
1280/5677 [=====>........................] - ETA: 11:36 - loss: 0.6825 - acc: 0.5672
1344/5677 [======>.......................] - ETA: 11:26 - loss: 0.6816 - acc: 0.5685
1408/5677 [======>.......................] - ETA: 11:18 - loss: 0.6813 - acc: 0.5703
1472/5677 [======>.......................] - ETA: 11:08 - loss: 0.6821 - acc: 0.5700
1536/5677 [=======>......................] - ETA: 10:55 - loss: 0.6814 - acc: 0.5723
1600/5677 [=======>......................] - ETA: 10:46 - loss: 0.6836 - acc: 0.5675
1664/5677 [=======>......................] - ETA: 10:32 - loss: 0.6845 - acc: 0.5655
1728/5677 [========>.....................] - ETA: 10:22 - loss: 0.6847 - acc: 0.5660
1792/5677 [========>.....................] - ETA: 10:14 - loss: 0.6840 - acc: 0.5658
1856/5677 [========>.....................] - ETA: 10:02 - loss: 0.6828 - acc: 0.5695
1920/5677 [=========>....................] - ETA: 9:54 - loss: 0.6838 - acc: 0.5667 
1984/5677 [=========>....................] - ETA: 9:44 - loss: 0.6848 - acc: 0.5640
2048/5677 [=========>....................] - ETA: 9:33 - loss: 0.6847 - acc: 0.5649
2112/5677 [==========>...................] - ETA: 9:23 - loss: 0.6843 - acc: 0.5658
2176/5677 [==========>...................] - ETA: 9:11 - loss: 0.6833 - acc: 0.5680
2240/5677 [==========>...................] - ETA: 9:00 - loss: 0.6827 - acc: 0.5683
2304/5677 [===========>..................] - ETA: 8:49 - loss: 0.6827 - acc: 0.5668
2368/5677 [===========>..................] - ETA: 8:39 - loss: 0.6827 - acc: 0.5655
2432/5677 [===========>..................] - ETA: 8:28 - loss: 0.6831 - acc: 0.5629
2496/5677 [============>.................] - ETA: 8:19 - loss: 0.6830 - acc: 0.5637
2560/5677 [============>.................] - ETA: 8:08 - loss: 0.6829 - acc: 0.5648
2624/5677 [============>.................] - ETA: 7:57 - loss: 0.6823 - acc: 0.5652
2688/5677 [=============>................] - ETA: 7:47 - loss: 0.6819 - acc: 0.5655
2752/5677 [=============>................] - ETA: 7:38 - loss: 0.6811 - acc: 0.5672
2816/5677 [=============>................] - ETA: 7:28 - loss: 0.6809 - acc: 0.5668
2880/5677 [==============>...............] - ETA: 7:18 - loss: 0.6806 - acc: 0.5691
2944/5677 [==============>...............] - ETA: 7:08 - loss: 0.6802 - acc: 0.5700
3008/5677 [==============>...............] - ETA: 6:57 - loss: 0.6797 - acc: 0.5711
3072/5677 [===============>..............] - ETA: 6:46 - loss: 0.6800 - acc: 0.5706
3136/5677 [===============>..............] - ETA: 6:36 - loss: 0.6797 - acc: 0.5717
3200/5677 [===============>..............] - ETA: 6:27 - loss: 0.6795 - acc: 0.5713
3264/5677 [================>.............] - ETA: 6:17 - loss: 0.6794 - acc: 0.5705
3328/5677 [================>.............] - ETA: 6:08 - loss: 0.6790 - acc: 0.5712
3392/5677 [================>.............] - ETA: 5:57 - loss: 0.6792 - acc: 0.5696
3456/5677 [=================>............] - ETA: 5:47 - loss: 0.6792 - acc: 0.5692
3520/5677 [=================>............] - ETA: 5:37 - loss: 0.6794 - acc: 0.5682
3584/5677 [=================>............] - ETA: 5:26 - loss: 0.6778 - acc: 0.5706
3648/5677 [==================>...........] - ETA: 5:16 - loss: 0.6776 - acc: 0.5699
3712/5677 [==================>...........] - ETA: 5:06 - loss: 0.6773 - acc: 0.5700
3776/5677 [==================>...........] - ETA: 4:56 - loss: 0.6761 - acc: 0.5726
3840/5677 [===================>..........] - ETA: 4:46 - loss: 0.6764 - acc: 0.5721
3904/5677 [===================>..........] - ETA: 4:37 - loss: 0.6771 - acc: 0.5712
3968/5677 [===================>..........] - ETA: 4:27 - loss: 0.6774 - acc: 0.5706
4032/5677 [====================>.........] - ETA: 4:17 - loss: 0.6782 - acc: 0.5704
4096/5677 [====================>.........] - ETA: 4:07 - loss: 0.6782 - acc: 0.5696
4160/5677 [====================>.........] - ETA: 3:57 - loss: 0.6782 - acc: 0.5697
4224/5677 [=====================>........] - ETA: 3:47 - loss: 0.6780 - acc: 0.5691
4288/5677 [=====================>........] - ETA: 3:37 - loss: 0.6782 - acc: 0.5688
4352/5677 [=====================>........] - ETA: 3:27 - loss: 0.6772 - acc: 0.5708
4416/5677 [======================>.......] - ETA: 3:17 - loss: 0.6768 - acc: 0.5720
4480/5677 [======================>.......] - ETA: 3:07 - loss: 0.6769 - acc: 0.5721
4544/5677 [=======================>......] - ETA: 2:57 - loss: 0.6770 - acc: 0.5717
4608/5677 [=======================>......] - ETA: 2:47 - loss: 0.6768 - acc: 0.5718
4672/5677 [=======================>......] - ETA: 2:37 - loss: 0.6772 - acc: 0.5715
4736/5677 [========================>.....] - ETA: 2:26 - loss: 0.6767 - acc: 0.5726
4800/5677 [========================>.....] - ETA: 2:16 - loss: 0.6760 - acc: 0.5731
4864/5677 [========================>.....] - ETA: 2:06 - loss: 0.6758 - acc: 0.5736
4928/5677 [=========================>....] - ETA: 1:56 - loss: 0.6752 - acc: 0.5747
4992/5677 [=========================>....] - ETA: 1:46 - loss: 0.6750 - acc: 0.5745
5056/5677 [=========================>....] - ETA: 1:36 - loss: 0.6748 - acc: 0.5752
5120/5677 [==========================>...] - ETA: 1:26 - loss: 0.6749 - acc: 0.5754
5184/5677 [==========================>...] - ETA: 1:16 - loss: 0.6743 - acc: 0.5768
5248/5677 [==========================>...] - ETA: 1:06 - loss: 0.6742 - acc: 0.5768
5312/5677 [===========================>..] - ETA: 56s - loss: 0.6740 - acc: 0.5781 
5376/5677 [===========================>..] - ETA: 46s - loss: 0.6741 - acc: 0.5774
5440/5677 [===========================>..] - ETA: 36s - loss: 0.6745 - acc: 0.5765
5504/5677 [============================>.] - ETA: 26s - loss: 0.6745 - acc: 0.5767
5568/5677 [============================>.] - ETA: 16s - loss: 0.6745 - acc: 0.5770
5632/5677 [============================>.] - ETA: 6s - loss: 0.6739 - acc: 0.5785 
5677/5677 [==============================] - 912s 161ms/step - loss: 0.6736 - acc: 0.5787 - val_loss: 0.6893 - val_acc: 0.5689

Epoch 00007: val_acc did not improve from 0.59746
Epoch 8/10

  64/5677 [..............................] - ETA: 14:54 - loss: 0.6826 - acc: 0.5469
 128/5677 [..............................] - ETA: 14:43 - loss: 0.6544 - acc: 0.6094
 192/5677 [>.............................] - ETA: 13:46 - loss: 0.6579 - acc: 0.5990
 256/5677 [>.............................] - ETA: 13:54 - loss: 0.6647 - acc: 0.5781
 320/5677 [>.............................] - ETA: 13:34 - loss: 0.6601 - acc: 0.5906
 384/5677 [=>............................] - ETA: 13:03 - loss: 0.6600 - acc: 0.5859
 448/5677 [=>............................] - ETA: 12:48 - loss: 0.6561 - acc: 0.5893
 512/5677 [=>............................] - ETA: 12:38 - loss: 0.6648 - acc: 0.5820
 576/5677 [==>...........................] - ETA: 12:20 - loss: 0.6645 - acc: 0.5816
 640/5677 [==>...........................] - ETA: 12:18 - loss: 0.6641 - acc: 0.5797
 704/5677 [==>...........................] - ETA: 12:05 - loss: 0.6624 - acc: 0.5895
 768/5677 [===>..........................] - ETA: 11:50 - loss: 0.6613 - acc: 0.5951
 832/5677 [===>..........................] - ETA: 11:44 - loss: 0.6653 - acc: 0.5853
 896/5677 [===>..........................] - ETA: 11:35 - loss: 0.6661 - acc: 0.5826
 960/5677 [====>.........................] - ETA: 11:26 - loss: 0.6670 - acc: 0.5802
1024/5677 [====>.........................] - ETA: 11:16 - loss: 0.6658 - acc: 0.5830
1088/5677 [====>.........................] - ETA: 11:08 - loss: 0.6669 - acc: 0.5827
1152/5677 [=====>........................] - ETA: 11:00 - loss: 0.6670 - acc: 0.5833
1216/5677 [=====>........................] - ETA: 10:46 - loss: 0.6676 - acc: 0.5822
1280/5677 [=====>........................] - ETA: 10:34 - loss: 0.6678 - acc: 0.5797
1344/5677 [======>.......................] - ETA: 10:20 - loss: 0.6670 - acc: 0.5789
1408/5677 [======>.......................] - ETA: 10:08 - loss: 0.6660 - acc: 0.5831
1472/5677 [======>.......................] - ETA: 9:58 - loss: 0.6681 - acc: 0.5802 
1536/5677 [=======>......................] - ETA: 9:50 - loss: 0.6676 - acc: 0.5846
1600/5677 [=======>......................] - ETA: 9:36 - loss: 0.6684 - acc: 0.5831
1664/5677 [=======>......................] - ETA: 9:25 - loss: 0.6677 - acc: 0.5835
1728/5677 [========>.....................] - ETA: 9:14 - loss: 0.6690 - acc: 0.5816
1792/5677 [========>.....................] - ETA: 9:03 - loss: 0.6690 - acc: 0.5804
1856/5677 [========>.....................] - ETA: 8:55 - loss: 0.6674 - acc: 0.5824
1920/5677 [=========>....................] - ETA: 8:44 - loss: 0.6678 - acc: 0.5807
1984/5677 [=========>....................] - ETA: 8:33 - loss: 0.6682 - acc: 0.5801
2048/5677 [=========>....................] - ETA: 8:22 - loss: 0.6670 - acc: 0.5830
2112/5677 [==========>...................] - ETA: 8:11 - loss: 0.6675 - acc: 0.5819
2176/5677 [==========>...................] - ETA: 8:02 - loss: 0.6679 - acc: 0.5818
2240/5677 [==========>...................] - ETA: 7:54 - loss: 0.6667 - acc: 0.5826
2304/5677 [===========>..................] - ETA: 7:45 - loss: 0.6671 - acc: 0.5825
2368/5677 [===========>..................] - ETA: 7:35 - loss: 0.6664 - acc: 0.5840
2432/5677 [===========>..................] - ETA: 7:25 - loss: 0.6660 - acc: 0.5855
2496/5677 [============>.................] - ETA: 7:15 - loss: 0.6649 - acc: 0.5865
2560/5677 [============>.................] - ETA: 7:06 - loss: 0.6643 - acc: 0.5879
2624/5677 [============>.................] - ETA: 6:57 - loss: 0.6638 - acc: 0.5899
2688/5677 [=============>................] - ETA: 6:49 - loss: 0.6631 - acc: 0.5915
2752/5677 [=============>................] - ETA: 6:40 - loss: 0.6646 - acc: 0.5898
2816/5677 [=============>................] - ETA: 6:32 - loss: 0.6650 - acc: 0.5888
2880/5677 [==============>...............] - ETA: 6:24 - loss: 0.6647 - acc: 0.5889
2944/5677 [==============>...............] - ETA: 6:16 - loss: 0.6642 - acc: 0.5900
3008/5677 [==============>...............] - ETA: 6:07 - loss: 0.6641 - acc: 0.5911
3072/5677 [===============>..............] - ETA: 5:58 - loss: 0.6638 - acc: 0.5915
3136/5677 [===============>..............] - ETA: 5:49 - loss: 0.6643 - acc: 0.5912
3200/5677 [===============>..............] - ETA: 5:39 - loss: 0.6639 - acc: 0.5928
3264/5677 [================>.............] - ETA: 5:31 - loss: 0.6646 - acc: 0.5919
3328/5677 [================>.............] - ETA: 5:22 - loss: 0.6653 - acc: 0.5916
3392/5677 [================>.............] - ETA: 5:13 - loss: 0.6650 - acc: 0.5926
3456/5677 [=================>............] - ETA: 5:04 - loss: 0.6652 - acc: 0.5940
3520/5677 [=================>............] - ETA: 4:56 - loss: 0.6639 - acc: 0.5960
3584/5677 [=================>............] - ETA: 4:48 - loss: 0.6639 - acc: 0.5963
3648/5677 [==================>...........] - ETA: 4:40 - loss: 0.6644 - acc: 0.5951
3712/5677 [==================>...........] - ETA: 4:31 - loss: 0.6649 - acc: 0.5940
3776/5677 [==================>...........] - ETA: 4:22 - loss: 0.6644 - acc: 0.5948
3840/5677 [===================>..........] - ETA: 4:13 - loss: 0.6658 - acc: 0.5924
3904/5677 [===================>..........] - ETA: 4:04 - loss: 0.6652 - acc: 0.5930
3968/5677 [===================>..........] - ETA: 3:56 - loss: 0.6650 - acc: 0.5940
4032/5677 [====================>.........] - ETA: 3:47 - loss: 0.6649 - acc: 0.5942
4096/5677 [====================>.........] - ETA: 3:38 - loss: 0.6653 - acc: 0.5935
4160/5677 [====================>.........] - ETA: 3:29 - loss: 0.6656 - acc: 0.5925
4224/5677 [=====================>........] - ETA: 3:20 - loss: 0.6651 - acc: 0.5940
4288/5677 [=====================>........] - ETA: 3:12 - loss: 0.6651 - acc: 0.5938
4352/5677 [=====================>........] - ETA: 3:03 - loss: 0.6658 - acc: 0.5921
4416/5677 [======================>.......] - ETA: 2:54 - loss: 0.6664 - acc: 0.5913
4480/5677 [======================>.......] - ETA: 2:46 - loss: 0.6672 - acc: 0.5888
4544/5677 [=======================>......] - ETA: 2:37 - loss: 0.6675 - acc: 0.5874
4608/5677 [=======================>......] - ETA: 2:29 - loss: 0.6680 - acc: 0.5862
4672/5677 [=======================>......] - ETA: 2:20 - loss: 0.6678 - acc: 0.5863
4736/5677 [========================>.....] - ETA: 2:11 - loss: 0.6676 - acc: 0.5870
4800/5677 [========================>.....] - ETA: 2:02 - loss: 0.6681 - acc: 0.5863
4864/5677 [========================>.....] - ETA: 1:53 - loss: 0.6680 - acc: 0.5872
4928/5677 [=========================>....] - ETA: 1:45 - loss: 0.6677 - acc: 0.5879
4992/5677 [=========================>....] - ETA: 1:36 - loss: 0.6675 - acc: 0.5883
5056/5677 [=========================>....] - ETA: 1:27 - loss: 0.6681 - acc: 0.5872
5120/5677 [==========================>...] - ETA: 1:18 - loss: 0.6678 - acc: 0.5883
5184/5677 [==========================>...] - ETA: 1:09 - loss: 0.6678 - acc: 0.5874
5248/5677 [==========================>...] - ETA: 1:00 - loss: 0.6679 - acc: 0.5880
5312/5677 [===========================>..] - ETA: 51s - loss: 0.6685 - acc: 0.5862 
5376/5677 [===========================>..] - ETA: 42s - loss: 0.6685 - acc: 0.5861
5440/5677 [===========================>..] - ETA: 33s - loss: 0.6685 - acc: 0.5864
5504/5677 [============================>.] - ETA: 24s - loss: 0.6684 - acc: 0.5863
5568/5677 [============================>.] - ETA: 15s - loss: 0.6690 - acc: 0.5855
5632/5677 [============================>.] - ETA: 6s - loss: 0.6690 - acc: 0.5861 
5677/5677 [==============================] - 842s 148ms/step - loss: 0.6689 - acc: 0.5868 - val_loss: 0.6717 - val_acc: 0.5769

Epoch 00008: val_acc did not improve from 0.59746
Epoch 9/10

  64/5677 [..............................] - ETA: 15:46 - loss: 0.6851 - acc: 0.5625
 128/5677 [..............................] - ETA: 14:32 - loss: 0.6749 - acc: 0.5469
 192/5677 [>.............................] - ETA: 14:12 - loss: 0.6682 - acc: 0.5729
 256/5677 [>.............................] - ETA: 14:13 - loss: 0.6743 - acc: 0.5625
 320/5677 [>.............................] - ETA: 14:03 - loss: 0.6760 - acc: 0.5625
 384/5677 [=>............................] - ETA: 13:38 - loss: 0.6743 - acc: 0.5677
 448/5677 [=>............................] - ETA: 13:27 - loss: 0.6716 - acc: 0.5692
 512/5677 [=>............................] - ETA: 13:07 - loss: 0.6765 - acc: 0.5586
 576/5677 [==>...........................] - ETA: 12:47 - loss: 0.6724 - acc: 0.5625
 640/5677 [==>...........................] - ETA: 12:31 - loss: 0.6701 - acc: 0.5672
 704/5677 [==>...........................] - ETA: 12:18 - loss: 0.6744 - acc: 0.5568
 768/5677 [===>..........................] - ETA: 12:08 - loss: 0.6754 - acc: 0.5547
 832/5677 [===>..........................] - ETA: 11:56 - loss: 0.6771 - acc: 0.5505
 896/5677 [===>..........................] - ETA: 11:48 - loss: 0.6778 - acc: 0.5491
 960/5677 [====>.........................] - ETA: 11:45 - loss: 0.6767 - acc: 0.5469
1024/5677 [====>.........................] - ETA: 11:36 - loss: 0.6763 - acc: 0.5459
1088/5677 [====>.........................] - ETA: 11:22 - loss: 0.6760 - acc: 0.5533
1152/5677 [=====>........................] - ETA: 11:17 - loss: 0.6748 - acc: 0.5556
1216/5677 [=====>........................] - ETA: 11:10 - loss: 0.6737 - acc: 0.5609
1280/5677 [=====>........................] - ETA: 11:02 - loss: 0.6747 - acc: 0.5641
1344/5677 [======>.......................] - ETA: 10:55 - loss: 0.6752 - acc: 0.5662
1408/5677 [======>.......................] - ETA: 10:45 - loss: 0.6768 - acc: 0.5653
1472/5677 [======>.......................] - ETA: 10:41 - loss: 0.6764 - acc: 0.5686
1536/5677 [=======>......................] - ETA: 10:32 - loss: 0.6754 - acc: 0.5703
1600/5677 [=======>......................] - ETA: 10:24 - loss: 0.6772 - acc: 0.5663
1664/5677 [=======>......................] - ETA: 10:13 - loss: 0.6759 - acc: 0.5697
1728/5677 [========>.....................] - ETA: 10:03 - loss: 0.6776 - acc: 0.5683
1792/5677 [========>.....................] - ETA: 9:53 - loss: 0.6762 - acc: 0.5720 
1856/5677 [========>.....................] - ETA: 9:43 - loss: 0.6761 - acc: 0.5738
1920/5677 [=========>....................] - ETA: 9:34 - loss: 0.6756 - acc: 0.5755
1984/5677 [=========>....................] - ETA: 9:26 - loss: 0.6752 - acc: 0.5761
2048/5677 [=========>....................] - ETA: 9:16 - loss: 0.6743 - acc: 0.5786
2112/5677 [==========>...................] - ETA: 9:06 - loss: 0.6742 - acc: 0.5795
2176/5677 [==========>...................] - ETA: 8:58 - loss: 0.6749 - acc: 0.5786
2240/5677 [==========>...................] - ETA: 8:49 - loss: 0.6746 - acc: 0.5777
2304/5677 [===========>..................] - ETA: 8:40 - loss: 0.6744 - acc: 0.5786
2368/5677 [===========>..................] - ETA: 8:30 - loss: 0.6739 - acc: 0.5802
2432/5677 [===========>..................] - ETA: 8:21 - loss: 0.6725 - acc: 0.5822
2496/5677 [============>.................] - ETA: 8:10 - loss: 0.6718 - acc: 0.5813
2560/5677 [============>.................] - ETA: 8:01 - loss: 0.6712 - acc: 0.5836
2624/5677 [============>.................] - ETA: 7:51 - loss: 0.6718 - acc: 0.5831
2688/5677 [=============>................] - ETA: 7:41 - loss: 0.6716 - acc: 0.5848
2752/5677 [=============>................] - ETA: 7:30 - loss: 0.6722 - acc: 0.5843
2816/5677 [=============>................] - ETA: 7:21 - loss: 0.6724 - acc: 0.5842
2880/5677 [==============>...............] - ETA: 7:11 - loss: 0.6721 - acc: 0.5840
2944/5677 [==============>...............] - ETA: 7:00 - loss: 0.6713 - acc: 0.5853
3008/5677 [==============>...............] - ETA: 6:51 - loss: 0.6713 - acc: 0.5854
3072/5677 [===============>..............] - ETA: 6:42 - loss: 0.6704 - acc: 0.5872
3136/5677 [===============>..............] - ETA: 6:32 - loss: 0.6699 - acc: 0.5893
3200/5677 [===============>..............] - ETA: 6:22 - loss: 0.6689 - acc: 0.5913
3264/5677 [================>.............] - ETA: 6:12 - loss: 0.6698 - acc: 0.5901
3328/5677 [================>.............] - ETA: 6:02 - loss: 0.6710 - acc: 0.5886
3392/5677 [================>.............] - ETA: 5:52 - loss: 0.6710 - acc: 0.5887
3456/5677 [=================>............] - ETA: 5:42 - loss: 0.6711 - acc: 0.5894
3520/5677 [=================>............] - ETA: 5:33 - loss: 0.6711 - acc: 0.5895
3584/5677 [=================>............] - ETA: 5:23 - loss: 0.6719 - acc: 0.5882
3648/5677 [==================>...........] - ETA: 5:13 - loss: 0.6719 - acc: 0.5885
3712/5677 [==================>...........] - ETA: 5:03 - loss: 0.6714 - acc: 0.5886
3776/5677 [==================>...........] - ETA: 4:53 - loss: 0.6712 - acc: 0.5892
3840/5677 [===================>..........] - ETA: 4:43 - loss: 0.6715 - acc: 0.5891
3904/5677 [===================>..........] - ETA: 4:33 - loss: 0.6729 - acc: 0.5873
3968/5677 [===================>..........] - ETA: 4:23 - loss: 0.6736 - acc: 0.5864
4032/5677 [====================>.........] - ETA: 4:13 - loss: 0.6735 - acc: 0.5866
4096/5677 [====================>.........] - ETA: 4:03 - loss: 0.6738 - acc: 0.5864
4160/5677 [====================>.........] - ETA: 3:53 - loss: 0.6728 - acc: 0.5875
4224/5677 [=====================>........] - ETA: 3:44 - loss: 0.6724 - acc: 0.5883
4288/5677 [=====================>........] - ETA: 3:33 - loss: 0.6729 - acc: 0.5875
4352/5677 [=====================>........] - ETA: 3:24 - loss: 0.6724 - acc: 0.5880
4416/5677 [======================>.......] - ETA: 3:14 - loss: 0.6728 - acc: 0.5865
4480/5677 [======================>.......] - ETA: 3:04 - loss: 0.6729 - acc: 0.5877
4544/5677 [=======================>......] - ETA: 2:54 - loss: 0.6733 - acc: 0.5867
4608/5677 [=======================>......] - ETA: 2:44 - loss: 0.6734 - acc: 0.5864
4672/5677 [=======================>......] - ETA: 2:34 - loss: 0.6731 - acc: 0.5869
4736/5677 [========================>.....] - ETA: 2:25 - loss: 0.6726 - acc: 0.5878
4800/5677 [========================>.....] - ETA: 2:15 - loss: 0.6729 - acc: 0.5869
4864/5677 [========================>.....] - ETA: 2:05 - loss: 0.6731 - acc: 0.5868
4928/5677 [=========================>....] - ETA: 1:55 - loss: 0.6735 - acc: 0.5858
4992/5677 [=========================>....] - ETA: 1:45 - loss: 0.6731 - acc: 0.5861
5056/5677 [=========================>....] - ETA: 1:35 - loss: 0.6729 - acc: 0.5868
5120/5677 [==========================>...] - ETA: 1:25 - loss: 0.6729 - acc: 0.5873
5184/5677 [==========================>...] - ETA: 1:16 - loss: 0.6728 - acc: 0.5872
5248/5677 [==========================>...] - ETA: 1:06 - loss: 0.6720 - acc: 0.5884
5312/5677 [===========================>..] - ETA: 56s - loss: 0.6722 - acc: 0.5881 
5376/5677 [===========================>..] - ETA: 46s - loss: 0.6720 - acc: 0.5891
5440/5677 [===========================>..] - ETA: 36s - loss: 0.6720 - acc: 0.5897
5504/5677 [============================>.] - ETA: 26s - loss: 0.6726 - acc: 0.5887
5568/5677 [============================>.] - ETA: 16s - loss: 0.6723 - acc: 0.5898
5632/5677 [============================>.] - ETA: 6s - loss: 0.6725 - acc: 0.5890 
5677/5677 [==============================] - 916s 161ms/step - loss: 0.6720 - acc: 0.5896 - val_loss: 0.6824 - val_acc: 0.5753

Epoch 00009: val_acc did not improve from 0.59746
Epoch 10/10

  64/5677 [..............................] - ETA: 16:20 - loss: 0.6422 - acc: 0.6406
 128/5677 [..............................] - ETA: 15:28 - loss: 0.6900 - acc: 0.5781
 192/5677 [>.............................] - ETA: 14:53 - loss: 0.6772 - acc: 0.5938
 256/5677 [>.............................] - ETA: 14:23 - loss: 0.6805 - acc: 0.5859
 320/5677 [>.............................] - ETA: 14:21 - loss: 0.6791 - acc: 0.5750
 384/5677 [=>............................] - ETA: 14:15 - loss: 0.6838 - acc: 0.5677
 448/5677 [=>............................] - ETA: 14:07 - loss: 0.6833 - acc: 0.5603
 512/5677 [=>............................] - ETA: 13:55 - loss: 0.6805 - acc: 0.5586
 576/5677 [==>...........................] - ETA: 14:02 - loss: 0.6841 - acc: 0.5538
 640/5677 [==>...........................] - ETA: 14:02 - loss: 0.6828 - acc: 0.5563
 704/5677 [==>...........................] - ETA: 13:52 - loss: 0.6809 - acc: 0.5668
 768/5677 [===>..........................] - ETA: 13:41 - loss: 0.6792 - acc: 0.5651
 832/5677 [===>..........................] - ETA: 13:26 - loss: 0.6815 - acc: 0.5625
 896/5677 [===>..........................] - ETA: 13:08 - loss: 0.6819 - acc: 0.5636
 960/5677 [====>.........................] - ETA: 12:59 - loss: 0.6821 - acc: 0.5635
1024/5677 [====>.........................] - ETA: 12:47 - loss: 0.6822 - acc: 0.5605
1088/5677 [====>.........................] - ETA: 12:33 - loss: 0.6811 - acc: 0.5607
1152/5677 [=====>........................] - ETA: 12:21 - loss: 0.6800 - acc: 0.5616
1216/5677 [=====>........................] - ETA: 12:09 - loss: 0.6812 - acc: 0.5633
1280/5677 [=====>........................] - ETA: 11:55 - loss: 0.6811 - acc: 0.5680
1344/5677 [======>.......................] - ETA: 11:38 - loss: 0.6794 - acc: 0.5699
1408/5677 [======>.......................] - ETA: 11:24 - loss: 0.6799 - acc: 0.5689
1472/5677 [======>.......................] - ETA: 11:09 - loss: 0.6781 - acc: 0.5713
1536/5677 [=======>......................] - ETA: 10:55 - loss: 0.6766 - acc: 0.5736
1600/5677 [=======>......................] - ETA: 10:42 - loss: 0.6761 - acc: 0.5731
1664/5677 [=======>......................] - ETA: 10:30 - loss: 0.6756 - acc: 0.5751
1728/5677 [========>.....................] - ETA: 10:18 - loss: 0.6766 - acc: 0.5718
1792/5677 [========>.....................] - ETA: 10:07 - loss: 0.6739 - acc: 0.5787
1856/5677 [========>.....................] - ETA: 9:58 - loss: 0.6725 - acc: 0.5814 
1920/5677 [=========>....................] - ETA: 9:47 - loss: 0.6728 - acc: 0.5802
1984/5677 [=========>....................] - ETA: 9:35 - loss: 0.6721 - acc: 0.5817
2048/5677 [=========>....................] - ETA: 9:24 - loss: 0.6722 - acc: 0.5830
2112/5677 [==========>...................] - ETA: 9:15 - loss: 0.6724 - acc: 0.5824
2176/5677 [==========>...................] - ETA: 9:06 - loss: 0.6726 - acc: 0.5813
2240/5677 [==========>...................] - ETA: 8:58 - loss: 0.6721 - acc: 0.5795
2304/5677 [===========>..................] - ETA: 8:47 - loss: 0.6717 - acc: 0.5799
2368/5677 [===========>..................] - ETA: 8:36 - loss: 0.6723 - acc: 0.5777
2432/5677 [===========>..................] - ETA: 8:24 - loss: 0.6741 - acc: 0.5748
2496/5677 [============>.................] - ETA: 8:14 - loss: 0.6743 - acc: 0.5725
2560/5677 [============>.................] - ETA: 8:06 - loss: 0.6741 - acc: 0.5738
2624/5677 [============>.................] - ETA: 7:55 - loss: 0.6737 - acc: 0.5739
2688/5677 [=============>................] - ETA: 7:45 - loss: 0.6754 - acc: 0.5722
2752/5677 [=============>................] - ETA: 7:34 - loss: 0.6759 - acc: 0.5698
2816/5677 [=============>................] - ETA: 7:23 - loss: 0.6749 - acc: 0.5714
2880/5677 [==============>...............] - ETA: 7:13 - loss: 0.6745 - acc: 0.5726
2944/5677 [==============>...............] - ETA: 7:03 - loss: 0.6746 - acc: 0.5730
3008/5677 [==============>...............] - ETA: 6:52 - loss: 0.6739 - acc: 0.5755
3072/5677 [===============>..............] - ETA: 6:41 - loss: 0.6738 - acc: 0.5758
3136/5677 [===============>..............] - ETA: 6:30 - loss: 0.6735 - acc: 0.5765
3200/5677 [===============>..............] - ETA: 6:21 - loss: 0.6734 - acc: 0.5775
3264/5677 [================>.............] - ETA: 6:10 - loss: 0.6732 - acc: 0.5772
3328/5677 [================>.............] - ETA: 6:00 - loss: 0.6725 - acc: 0.5784
3392/5677 [================>.............] - ETA: 5:50 - loss: 0.6723 - acc: 0.5793
3456/5677 [=================>............] - ETA: 5:40 - loss: 0.6725 - acc: 0.5796
3520/5677 [=================>............] - ETA: 5:30 - loss: 0.6722 - acc: 0.5801
3584/5677 [=================>............] - ETA: 5:20 - loss: 0.6726 - acc: 0.5790
3648/5677 [==================>...........] - ETA: 5:09 - loss: 0.6719 - acc: 0.5806
3712/5677 [==================>...........] - ETA: 4:59 - loss: 0.6716 - acc: 0.5822
3776/5677 [==================>...........] - ETA: 4:49 - loss: 0.6711 - acc: 0.5826
3840/5677 [===================>..........] - ETA: 4:39 - loss: 0.6714 - acc: 0.5828
3904/5677 [===================>..........] - ETA: 4:30 - loss: 0.6702 - acc: 0.5840
3968/5677 [===================>..........] - ETA: 4:20 - loss: 0.6700 - acc: 0.5842
4032/5677 [====================>.........] - ETA: 4:10 - loss: 0.6699 - acc: 0.5861
4096/5677 [====================>.........] - ETA: 4:00 - loss: 0.6697 - acc: 0.5859
4160/5677 [====================>.........] - ETA: 3:51 - loss: 0.6694 - acc: 0.5853
4224/5677 [=====================>........] - ETA: 3:41 - loss: 0.6678 - acc: 0.5874
4288/5677 [=====================>........] - ETA: 3:31 - loss: 0.6679 - acc: 0.5872
4352/5677 [=====================>........] - ETA: 3:21 - loss: 0.6672 - acc: 0.5885
4416/5677 [======================>.......] - ETA: 3:11 - loss: 0.6678 - acc: 0.5885
4480/5677 [======================>.......] - ETA: 3:01 - loss: 0.6681 - acc: 0.5884
4544/5677 [=======================>......] - ETA: 2:51 - loss: 0.6688 - acc: 0.5876
4608/5677 [=======================>......] - ETA: 2:42 - loss: 0.6691 - acc: 0.5879
4672/5677 [=======================>......] - ETA: 2:32 - loss: 0.6695 - acc: 0.5873
4736/5677 [========================>.....] - ETA: 2:22 - loss: 0.6709 - acc: 0.5857
4800/5677 [========================>.....] - ETA: 2:12 - loss: 0.6707 - acc: 0.5860
4864/5677 [========================>.....] - ETA: 2:02 - loss: 0.6708 - acc: 0.5857
4928/5677 [=========================>....] - ETA: 1:53 - loss: 0.6705 - acc: 0.5864
4992/5677 [=========================>....] - ETA: 1:43 - loss: 0.6705 - acc: 0.5859
5056/5677 [=========================>....] - ETA: 1:33 - loss: 0.6703 - acc: 0.5862
5120/5677 [==========================>...] - ETA: 1:23 - loss: 0.6702 - acc: 0.5861
5184/5677 [==========================>...] - ETA: 1:14 - loss: 0.6705 - acc: 0.5851
5248/5677 [==========================>...] - ETA: 1:04 - loss: 0.6707 - acc: 0.5844
5312/5677 [===========================>..] - ETA: 54s - loss: 0.6701 - acc: 0.5857 
5376/5677 [===========================>..] - ETA: 45s - loss: 0.6695 - acc: 0.5869
5440/5677 [===========================>..] - ETA: 35s - loss: 0.6694 - acc: 0.5868
5504/5677 [============================>.] - ETA: 25s - loss: 0.6692 - acc: 0.5868
5568/5677 [============================>.] - ETA: 16s - loss: 0.6688 - acc: 0.5880
5632/5677 [============================>.] - ETA: 6s - loss: 0.6687 - acc: 0.5881 
5677/5677 [==============================] - 884s 156ms/step - loss: 0.6684 - acc: 0.5885 - val_loss: 0.6958 - val_acc: 0.5515

Epoch 00010: val_acc did not improve from 0.59746
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f94044a7c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f94044a7c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9404459ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9404459ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94042f8c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f94042f8c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94044595d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f94044595d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9404294950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9404294950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93dc725150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93dc725150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9404459310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9404459310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93dc74b510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93dc74b510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93dc5dadd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93dc5dadd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93dc486a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93dc486a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f901049dbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f901049dbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93dc4bd0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93dc4bd0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93dc4c3910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93dc4c3910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93dc36ee90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93dc36ee90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f3c11e7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f8f3c11e7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93dc2c6c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93dc2c6c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92e4118050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92e4118050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c84053d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f90c84053d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93dc1d0410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f93dc1d0410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93982a8a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93982a8a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93dc1e1dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f93dc1e1dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93dc2cb110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93dc2cb110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c0713090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c0713090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92c07d4690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92c07d4690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93982e4110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f93982e4110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c05d9790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c05d9790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93982a8bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f93982a8bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c0589490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c0589490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92c039ec10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92c039ec10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92c03dba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92c03dba50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c0672ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c0672ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92c0661ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92c0661ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c05f5e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c05f5e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92c0094b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92c0094b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92c03d7250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92c03d7250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c007fa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92c007fa10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92c0094c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92c0094c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92b07edc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92b07edc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92b0715290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92b0715290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92b0765850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92b0765850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92b062fc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92b062fc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92c00cfb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92c00cfb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92b04fb290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92b04fb290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92b025d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92b025d710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92b019e410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92b019e410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92b0207bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92b0207bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92b0523e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92b0523e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92a0797890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92a0797890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92a0699710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92a0699710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92a053e510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92a053e510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92a06bcad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92a06bcad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92a077b850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92a077b850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92b025df10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92b025df10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92a034aa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92a034aa50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92a060e390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92a060e390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f939837abd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f939837abd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92a034aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92a034aed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92a03d8f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92a03d8f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92a0128c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f92a0128c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92a00b1550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f92a00b1550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92a02627d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92a02627d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92a0128bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f92a0128bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92986c0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f92986c0e10>>: AttributeError: module 'gast' has no attribute 'Str'
02window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 10:22
 128/1578 [=>............................] - ETA: 5:33 
 192/1578 [==>...........................] - ETA: 3:55
 256/1578 [===>..........................] - ETA: 3:06
 320/1578 [=====>........................] - ETA: 2:35
 384/1578 [======>.......................] - ETA: 2:12
 448/1578 [=======>......................] - ETA: 1:53
 512/1578 [========>.....................] - ETA: 1:40
 576/1578 [=========>....................] - ETA: 1:29
 640/1578 [===========>..................] - ETA: 1:19
 704/1578 [============>.................] - ETA: 1:11
 768/1578 [=============>................] - ETA: 1:03
 832/1578 [==============>...............] - ETA: 57s 
 896/1578 [================>.............] - ETA: 51s
 960/1578 [=================>............] - ETA: 45s
1024/1578 [==================>...........] - ETA: 39s
1088/1578 [===================>..........] - ETA: 34s
1152/1578 [====================>.........] - ETA: 29s
1216/1578 [======================>.......] - ETA: 24s
1280/1578 [=======================>......] - ETA: 19s
1344/1578 [========================>.....] - ETA: 15s
1408/1578 [=========================>....] - ETA: 10s
1472/1578 [==========================>...] - ETA: 6s 
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 99s 63ms/step
loss: 0.6638437745235719
acc: 0.6121673003046836
