nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 3154
样本个数 6308
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa5a36bc410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa5a36bc410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa622c02d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa622c02d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa64cfc9f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa64cfc9f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa622d11790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa622d11790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5a35dbc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5a35dbc10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622ca72d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622ca72d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622d11e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622d11e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622cf6e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622cf6e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5a368e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5a368e550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5a3494610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5a3494610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a344fa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a344fa10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5a3451910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5a3451910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a31aba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a31aba50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5a30e2f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5a30e2f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5a3183d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5a3183d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59af0b350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59af0b350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5a30e2e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5a30e2e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59aeafd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59aeafd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa59ae2ae10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa59ae2ae10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5a30ef9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5a30ef9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59ae19550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59ae19550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa59ae1f550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa59ae1f550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a2fdc310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a2fdc310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa59ab02c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa59ab02c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa59aa029d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa59aa029d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59ad67c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59ad67c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa59ab02590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa59ab02590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59aa10190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59aa10190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa59aa02ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa59aa02ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa59266edd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa59266edd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59a811310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa59a811310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa59aa02690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa59aa02690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5926a3f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5926a3f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa59245b7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa59245b7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5924c7a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5924c7a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622cd5850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622cd5850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa59245b590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa59245b590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622c9d790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622c9d790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5921c6c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5921c6c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa59218add0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa59218add0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591f1af50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591f1af50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5921c6b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5921c6b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5920ea090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5920ea090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5920d4bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5920d4bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa591eb4950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa591eb4950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591e67b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591e67b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa592094dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa592094dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591d99810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591d99810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa591c516d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa591c516d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa591bbd890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa591bbd890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591d756d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591d756d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa591dedbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa591dedbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591a29990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591a29990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa591943110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa591943110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa59181a510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa59181a510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591a91950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591a91950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa591ab21d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa591ab21d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa589732510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa589732510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa58972a110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa58972a110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5894ed790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5894ed790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5894715d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5894715d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5897cbe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5897cbe50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5893d9d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5893d9d90>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-28 22:30:34.644635: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-28 22:30:34.766648: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-28 22:30:34.927024: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557f06628dc0 executing computations on platform Host. Devices:
2022-11-28 22:30:34.927143: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-28 22:30:36.271341: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 19:01 - loss: 0.7471 - acc: 0.5000
 128/5677 [..............................] - ETA: 14:21 - loss: 0.8676 - acc: 0.4688
 192/5677 [>.............................] - ETA: 12:13 - loss: 0.8608 - acc: 0.4375
 256/5677 [>.............................] - ETA: 11:32 - loss: 0.8349 - acc: 0.4609
 320/5677 [>.............................] - ETA: 10:33 - loss: 0.8195 - acc: 0.4750
 384/5677 [=>............................] - ETA: 9:48 - loss: 0.8036 - acc: 0.4844 
 448/5677 [=>............................] - ETA: 9:10 - loss: 0.7914 - acc: 0.4978
 512/5677 [=>............................] - ETA: 8:43 - loss: 0.7759 - acc: 0.5117
 576/5677 [==>...........................] - ETA: 8:18 - loss: 0.7611 - acc: 0.5243
 640/5677 [==>...........................] - ETA: 7:57 - loss: 0.7617 - acc: 0.5188
 704/5677 [==>...........................] - ETA: 7:47 - loss: 0.7626 - acc: 0.5170
 768/5677 [===>..........................] - ETA: 7:50 - loss: 0.7611 - acc: 0.5143
 832/5677 [===>..........................] - ETA: 7:41 - loss: 0.7569 - acc: 0.5156
 896/5677 [===>..........................] - ETA: 7:29 - loss: 0.7559 - acc: 0.5089
 960/5677 [====>.........................] - ETA: 7:16 - loss: 0.7550 - acc: 0.5073
1024/5677 [====>.........................] - ETA: 7:06 - loss: 0.7530 - acc: 0.5088
1088/5677 [====>.........................] - ETA: 6:53 - loss: 0.7515 - acc: 0.5046
1152/5677 [=====>........................] - ETA: 6:43 - loss: 0.7500 - acc: 0.5052
1216/5677 [=====>........................] - ETA: 6:38 - loss: 0.7475 - acc: 0.5058
1280/5677 [=====>........................] - ETA: 6:33 - loss: 0.7461 - acc: 0.5094
1344/5677 [======>.......................] - ETA: 6:24 - loss: 0.7444 - acc: 0.5089
1408/5677 [======>.......................] - ETA: 6:18 - loss: 0.7440 - acc: 0.5092
1472/5677 [======>.......................] - ETA: 6:10 - loss: 0.7416 - acc: 0.5129
1536/5677 [=======>......................] - ETA: 6:04 - loss: 0.7395 - acc: 0.5150
1600/5677 [=======>......................] - ETA: 5:57 - loss: 0.7392 - acc: 0.5162
1664/5677 [=======>......................] - ETA: 5:49 - loss: 0.7373 - acc: 0.5198
1728/5677 [========>.....................] - ETA: 5:42 - loss: 0.7374 - acc: 0.5179
1792/5677 [========>.....................] - ETA: 5:34 - loss: 0.7343 - acc: 0.5190
1856/5677 [========>.....................] - ETA: 5:27 - loss: 0.7332 - acc: 0.5205
1920/5677 [=========>....................] - ETA: 5:20 - loss: 0.7321 - acc: 0.5203
1984/5677 [=========>....................] - ETA: 5:14 - loss: 0.7316 - acc: 0.5197
2048/5677 [=========>....................] - ETA: 5:07 - loss: 0.7305 - acc: 0.5205
2112/5677 [==========>...................] - ETA: 5:00 - loss: 0.7308 - acc: 0.5185
2176/5677 [==========>...................] - ETA: 4:53 - loss: 0.7311 - acc: 0.5170
2240/5677 [==========>...................] - ETA: 4:47 - loss: 0.7296 - acc: 0.5174
2304/5677 [===========>..................] - ETA: 4:41 - loss: 0.7266 - acc: 0.5213
2368/5677 [===========>..................] - ETA: 4:36 - loss: 0.7258 - acc: 0.5236
2432/5677 [===========>..................] - ETA: 4:29 - loss: 0.7252 - acc: 0.5251
2496/5677 [============>.................] - ETA: 4:24 - loss: 0.7255 - acc: 0.5236
2560/5677 [============>.................] - ETA: 4:18 - loss: 0.7255 - acc: 0.5246
2624/5677 [============>.................] - ETA: 4:12 - loss: 0.7268 - acc: 0.5236
2688/5677 [=============>................] - ETA: 4:06 - loss: 0.7254 - acc: 0.5246
2752/5677 [=============>................] - ETA: 4:03 - loss: 0.7244 - acc: 0.5243
2816/5677 [=============>................] - ETA: 3:57 - loss: 0.7254 - acc: 0.5220
2880/5677 [==============>...............] - ETA: 3:52 - loss: 0.7239 - acc: 0.5243
2944/5677 [==============>...............] - ETA: 3:46 - loss: 0.7228 - acc: 0.5255
3008/5677 [==============>...............] - ETA: 3:41 - loss: 0.7238 - acc: 0.5239
3072/5677 [===============>..............] - ETA: 3:38 - loss: 0.7250 - acc: 0.5228
3136/5677 [===============>..............] - ETA: 3:32 - loss: 0.7235 - acc: 0.5255
3200/5677 [===============>..............] - ETA: 3:26 - loss: 0.7235 - acc: 0.5262
3264/5677 [================>.............] - ETA: 3:20 - loss: 0.7238 - acc: 0.5257
3328/5677 [================>.............] - ETA: 3:14 - loss: 0.7224 - acc: 0.5273
3392/5677 [================>.............] - ETA: 3:08 - loss: 0.7217 - acc: 0.5286
3456/5677 [=================>............] - ETA: 3:02 - loss: 0.7226 - acc: 0.5298
3520/5677 [=================>............] - ETA: 2:56 - loss: 0.7219 - acc: 0.5307
3584/5677 [=================>............] - ETA: 2:51 - loss: 0.7213 - acc: 0.5324
3648/5677 [==================>...........] - ETA: 2:45 - loss: 0.7220 - acc: 0.5307
3712/5677 [==================>...........] - ETA: 2:40 - loss: 0.7219 - acc: 0.5310
3776/5677 [==================>...........] - ETA: 2:34 - loss: 0.7207 - acc: 0.5334
3840/5677 [===================>..........] - ETA: 2:29 - loss: 0.7196 - acc: 0.5344
3904/5677 [===================>..........] - ETA: 2:23 - loss: 0.7192 - acc: 0.5353
3968/5677 [===================>..........] - ETA: 2:17 - loss: 0.7185 - acc: 0.5360
4032/5677 [====================>.........] - ETA: 2:12 - loss: 0.7191 - acc: 0.5355
4096/5677 [====================>.........] - ETA: 2:07 - loss: 0.7191 - acc: 0.5354
4160/5677 [====================>.........] - ETA: 2:02 - loss: 0.7187 - acc: 0.5353
4224/5677 [=====================>........] - ETA: 1:57 - loss: 0.7181 - acc: 0.5357
4288/5677 [=====================>........] - ETA: 1:51 - loss: 0.7180 - acc: 0.5359
4352/5677 [=====================>........] - ETA: 1:46 - loss: 0.7183 - acc: 0.5345
4416/5677 [======================>.......] - ETA: 1:41 - loss: 0.7180 - acc: 0.5349
4480/5677 [======================>.......] - ETA: 1:35 - loss: 0.7179 - acc: 0.5348
4544/5677 [=======================>......] - ETA: 1:30 - loss: 0.7180 - acc: 0.5352
4608/5677 [=======================>......] - ETA: 1:25 - loss: 0.7177 - acc: 0.5365
4672/5677 [=======================>......] - ETA: 1:20 - loss: 0.7175 - acc: 0.5368
4736/5677 [========================>.....] - ETA: 1:14 - loss: 0.7168 - acc: 0.5380
4800/5677 [========================>.....] - ETA: 1:09 - loss: 0.7169 - acc: 0.5375
4864/5677 [========================>.....] - ETA: 1:04 - loss: 0.7166 - acc: 0.5380
4928/5677 [=========================>....] - ETA: 59s - loss: 0.7160 - acc: 0.5396 
4992/5677 [=========================>....] - ETA: 54s - loss: 0.7158 - acc: 0.5395
5056/5677 [=========================>....] - ETA: 49s - loss: 0.7152 - acc: 0.5394
5120/5677 [==========================>...] - ETA: 43s - loss: 0.7151 - acc: 0.5391
5184/5677 [==========================>...] - ETA: 38s - loss: 0.7151 - acc: 0.5388
5248/5677 [==========================>...] - ETA: 33s - loss: 0.7139 - acc: 0.5402
5312/5677 [===========================>..] - ETA: 28s - loss: 0.7136 - acc: 0.5403
5376/5677 [===========================>..] - ETA: 23s - loss: 0.7137 - acc: 0.5398
5440/5677 [===========================>..] - ETA: 18s - loss: 0.7133 - acc: 0.5397
5504/5677 [============================>.] - ETA: 13s - loss: 0.7136 - acc: 0.5400
5568/5677 [============================>.] - ETA: 8s - loss: 0.7136 - acc: 0.5393 
5632/5677 [============================>.] - ETA: 3s - loss: 0.7141 - acc: 0.5385
5677/5677 [==============================] - 460s 81ms/step - loss: 0.7140 - acc: 0.5383 - val_loss: 0.7212 - val_acc: 0.4786

Epoch 00001: val_acc improved from -inf to 0.47861, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window11/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 6:13 - loss: 0.6977 - acc: 0.5156
 128/5677 [..............................] - ETA: 6:28 - loss: 0.6656 - acc: 0.6094
 192/5677 [>.............................] - ETA: 6:20 - loss: 0.6469 - acc: 0.6510
 256/5677 [>.............................] - ETA: 6:06 - loss: 0.6566 - acc: 0.6406
 320/5677 [>.............................] - ETA: 6:00 - loss: 0.6616 - acc: 0.6312
 384/5677 [=>............................] - ETA: 5:57 - loss: 0.6699 - acc: 0.6146
 448/5677 [=>............................] - ETA: 5:55 - loss: 0.6671 - acc: 0.6094
 512/5677 [=>............................] - ETA: 6:02 - loss: 0.6648 - acc: 0.6133
 576/5677 [==>...........................] - ETA: 6:00 - loss: 0.6684 - acc: 0.6094
 640/5677 [==>...........................] - ETA: 5:56 - loss: 0.6679 - acc: 0.6047
 704/5677 [==>...........................] - ETA: 5:53 - loss: 0.6671 - acc: 0.6094
 768/5677 [===>..........................] - ETA: 5:50 - loss: 0.6692 - acc: 0.6081
 832/5677 [===>..........................] - ETA: 5:44 - loss: 0.6715 - acc: 0.6022
 896/5677 [===>..........................] - ETA: 5:40 - loss: 0.6710 - acc: 0.6016
 960/5677 [====>.........................] - ETA: 5:35 - loss: 0.6736 - acc: 0.6000
1024/5677 [====>.........................] - ETA: 5:28 - loss: 0.6764 - acc: 0.5938
1088/5677 [====>.........................] - ETA: 5:26 - loss: 0.6789 - acc: 0.5919
1152/5677 [=====>........................] - ETA: 5:20 - loss: 0.6800 - acc: 0.5903
1216/5677 [=====>........................] - ETA: 5:16 - loss: 0.6852 - acc: 0.5798
1280/5677 [=====>........................] - ETA: 5:11 - loss: 0.6815 - acc: 0.5844
1344/5677 [======>.......................] - ETA: 5:07 - loss: 0.6837 - acc: 0.5789
1408/5677 [======>.......................] - ETA: 5:02 - loss: 0.6832 - acc: 0.5767
1472/5677 [======>.......................] - ETA: 4:58 - loss: 0.6795 - acc: 0.5822
1536/5677 [=======>......................] - ETA: 4:53 - loss: 0.6799 - acc: 0.5807
1600/5677 [=======>......................] - ETA: 4:50 - loss: 0.6790 - acc: 0.5806
1664/5677 [=======>......................] - ETA: 4:44 - loss: 0.6792 - acc: 0.5799
1728/5677 [========>.....................] - ETA: 4:39 - loss: 0.6796 - acc: 0.5775
1792/5677 [========>.....................] - ETA: 4:35 - loss: 0.6786 - acc: 0.5804
1856/5677 [========>.....................] - ETA: 4:30 - loss: 0.6779 - acc: 0.5819
1920/5677 [=========>....................] - ETA: 4:26 - loss: 0.6783 - acc: 0.5807
1984/5677 [=========>....................] - ETA: 4:21 - loss: 0.6799 - acc: 0.5781
2048/5677 [=========>....................] - ETA: 4:16 - loss: 0.6812 - acc: 0.5771
2112/5677 [==========>...................] - ETA: 4:12 - loss: 0.6812 - acc: 0.5772
2176/5677 [==========>...................] - ETA: 4:07 - loss: 0.6808 - acc: 0.5772
2240/5677 [==========>...................] - ETA: 4:02 - loss: 0.6802 - acc: 0.5772
2304/5677 [===========>..................] - ETA: 3:58 - loss: 0.6809 - acc: 0.5768
2368/5677 [===========>..................] - ETA: 3:53 - loss: 0.6801 - acc: 0.5794
2432/5677 [===========>..................] - ETA: 3:48 - loss: 0.6786 - acc: 0.5810
2496/5677 [============>.................] - ETA: 3:44 - loss: 0.6781 - acc: 0.5829
2560/5677 [============>.................] - ETA: 3:39 - loss: 0.6774 - acc: 0.5852
2624/5677 [============>.................] - ETA: 3:34 - loss: 0.6768 - acc: 0.5850
2688/5677 [=============>................] - ETA: 3:29 - loss: 0.6755 - acc: 0.5871
2752/5677 [=============>................] - ETA: 3:25 - loss: 0.6741 - acc: 0.5879
2816/5677 [=============>................] - ETA: 3:20 - loss: 0.6744 - acc: 0.5881
2880/5677 [==============>...............] - ETA: 3:16 - loss: 0.6749 - acc: 0.5889
2944/5677 [==============>...............] - ETA: 3:12 - loss: 0.6751 - acc: 0.5887
3008/5677 [==============>...............] - ETA: 3:07 - loss: 0.6750 - acc: 0.5891
3072/5677 [===============>..............] - ETA: 3:03 - loss: 0.6766 - acc: 0.5859
3136/5677 [===============>..............] - ETA: 2:58 - loss: 0.6773 - acc: 0.5855
3200/5677 [===============>..............] - ETA: 2:54 - loss: 0.6769 - acc: 0.5863
3264/5677 [================>.............] - ETA: 2:50 - loss: 0.6771 - acc: 0.5864
3328/5677 [================>.............] - ETA: 2:45 - loss: 0.6766 - acc: 0.5877
3392/5677 [================>.............] - ETA: 2:40 - loss: 0.6768 - acc: 0.5867
3456/5677 [=================>............] - ETA: 2:36 - loss: 0.6774 - acc: 0.5848
3520/5677 [=================>............] - ETA: 2:31 - loss: 0.6771 - acc: 0.5849
3584/5677 [=================>............] - ETA: 2:27 - loss: 0.6778 - acc: 0.5840
3648/5677 [==================>...........] - ETA: 2:22 - loss: 0.6776 - acc: 0.5844
3712/5677 [==================>...........] - ETA: 2:18 - loss: 0.6778 - acc: 0.5835
3776/5677 [==================>...........] - ETA: 2:13 - loss: 0.6774 - acc: 0.5840
3840/5677 [===================>..........] - ETA: 2:09 - loss: 0.6781 - acc: 0.5831
3904/5677 [===================>..........] - ETA: 2:04 - loss: 0.6787 - acc: 0.5822
3968/5677 [===================>..........] - ETA: 1:59 - loss: 0.6785 - acc: 0.5827
4032/5677 [====================>.........] - ETA: 1:55 - loss: 0.6781 - acc: 0.5826
4096/5677 [====================>.........] - ETA: 1:50 - loss: 0.6784 - acc: 0.5825
4160/5677 [====================>.........] - ETA: 1:46 - loss: 0.6781 - acc: 0.5834
4224/5677 [=====================>........] - ETA: 1:41 - loss: 0.6781 - acc: 0.5829
4288/5677 [=====================>........] - ETA: 1:37 - loss: 0.6780 - acc: 0.5830
4352/5677 [=====================>........] - ETA: 1:32 - loss: 0.6778 - acc: 0.5834
4416/5677 [======================>.......] - ETA: 1:28 - loss: 0.6779 - acc: 0.5833
4480/5677 [======================>.......] - ETA: 1:23 - loss: 0.6773 - acc: 0.5844
4544/5677 [=======================>......] - ETA: 1:19 - loss: 0.6774 - acc: 0.5847
4608/5677 [=======================>......] - ETA: 1:14 - loss: 0.6767 - acc: 0.5855
4672/5677 [=======================>......] - ETA: 1:10 - loss: 0.6768 - acc: 0.5856
4736/5677 [========================>.....] - ETA: 1:05 - loss: 0.6768 - acc: 0.5857
4800/5677 [========================>.....] - ETA: 1:01 - loss: 0.6767 - acc: 0.5858
4864/5677 [========================>.....] - ETA: 56s - loss: 0.6769 - acc: 0.5857 
4928/5677 [=========================>....] - ETA: 52s - loss: 0.6764 - acc: 0.5858
4992/5677 [=========================>....] - ETA: 47s - loss: 0.6758 - acc: 0.5869
5056/5677 [=========================>....] - ETA: 43s - loss: 0.6754 - acc: 0.5876
5120/5677 [==========================>...] - ETA: 38s - loss: 0.6750 - acc: 0.5875
5184/5677 [==========================>...] - ETA: 34s - loss: 0.6749 - acc: 0.5866
5248/5677 [==========================>...] - ETA: 29s - loss: 0.6755 - acc: 0.5852
5312/5677 [===========================>..] - ETA: 25s - loss: 0.6752 - acc: 0.5857
5376/5677 [===========================>..] - ETA: 21s - loss: 0.6764 - acc: 0.5841
5440/5677 [===========================>..] - ETA: 16s - loss: 0.6766 - acc: 0.5835
5504/5677 [============================>.] - ETA: 12s - loss: 0.6761 - acc: 0.5839
5568/5677 [============================>.] - ETA: 7s - loss: 0.6760 - acc: 0.5842 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6754 - acc: 0.5854
5677/5677 [==============================] - 421s 74ms/step - loss: 0.6755 - acc: 0.5859 - val_loss: 0.6559 - val_acc: 0.5990

Epoch 00002: val_acc improved from 0.47861 to 0.59905, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window11/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 8:38 - loss: 0.6394 - acc: 0.6719
 128/5677 [..............................] - ETA: 8:16 - loss: 0.6256 - acc: 0.6875
 192/5677 [>.............................] - ETA: 7:55 - loss: 0.6321 - acc: 0.6667
 256/5677 [>.............................] - ETA: 7:53 - loss: 0.6463 - acc: 0.6406
 320/5677 [>.............................] - ETA: 8:02 - loss: 0.6481 - acc: 0.6312
 384/5677 [=>............................] - ETA: 7:57 - loss: 0.6616 - acc: 0.6094
 448/5677 [=>............................] - ETA: 7:48 - loss: 0.6556 - acc: 0.6116
 512/5677 [=>............................] - ETA: 7:48 - loss: 0.6539 - acc: 0.6133
 576/5677 [==>...........................] - ETA: 7:44 - loss: 0.6561 - acc: 0.6181
 640/5677 [==>...........................] - ETA: 7:47 - loss: 0.6618 - acc: 0.6062
 704/5677 [==>...........................] - ETA: 7:37 - loss: 0.6595 - acc: 0.6023
 768/5677 [===>..........................] - ETA: 7:35 - loss: 0.6614 - acc: 0.6016
 832/5677 [===>..........................] - ETA: 7:28 - loss: 0.6624 - acc: 0.5998
 896/5677 [===>..........................] - ETA: 7:18 - loss: 0.6602 - acc: 0.6071
 960/5677 [====>.........................] - ETA: 7:14 - loss: 0.6612 - acc: 0.6094
1024/5677 [====>.........................] - ETA: 7:11 - loss: 0.6602 - acc: 0.6123
1088/5677 [====>.........................] - ETA: 7:03 - loss: 0.6642 - acc: 0.6075
1152/5677 [=====>........................] - ETA: 6:55 - loss: 0.6652 - acc: 0.6042
1216/5677 [=====>........................] - ETA: 6:52 - loss: 0.6661 - acc: 0.6012
1280/5677 [=====>........................] - ETA: 6:46 - loss: 0.6660 - acc: 0.5992
1344/5677 [======>.......................] - ETA: 6:37 - loss: 0.6675 - acc: 0.5960
1408/5677 [======>.......................] - ETA: 6:30 - loss: 0.6689 - acc: 0.5930
1472/5677 [======>.......................] - ETA: 6:27 - loss: 0.6691 - acc: 0.5931
1536/5677 [=======>......................] - ETA: 6:21 - loss: 0.6681 - acc: 0.5970
1600/5677 [=======>......................] - ETA: 6:15 - loss: 0.6691 - acc: 0.5944
1664/5677 [=======>......................] - ETA: 6:08 - loss: 0.6711 - acc: 0.5925
1728/5677 [========>.....................] - ETA: 6:00 - loss: 0.6706 - acc: 0.5926
1792/5677 [========>.....................] - ETA: 5:54 - loss: 0.6698 - acc: 0.5938
1856/5677 [========>.....................] - ETA: 5:50 - loss: 0.6710 - acc: 0.5938
1920/5677 [=========>....................] - ETA: 5:44 - loss: 0.6724 - acc: 0.5911
1984/5677 [=========>....................] - ETA: 5:38 - loss: 0.6691 - acc: 0.5953
2048/5677 [=========>....................] - ETA: 5:31 - loss: 0.6676 - acc: 0.5972
2112/5677 [==========>...................] - ETA: 5:25 - loss: 0.6681 - acc: 0.5971
2176/5677 [==========>...................] - ETA: 5:18 - loss: 0.6684 - acc: 0.5960
2240/5677 [==========>...................] - ETA: 5:13 - loss: 0.6685 - acc: 0.5969
2304/5677 [===========>..................] - ETA: 5:08 - loss: 0.6693 - acc: 0.5964
2368/5677 [===========>..................] - ETA: 5:03 - loss: 0.6709 - acc: 0.5942
2432/5677 [===========>..................] - ETA: 4:57 - loss: 0.6694 - acc: 0.5950
2496/5677 [============>.................] - ETA: 4:51 - loss: 0.6692 - acc: 0.5950
2560/5677 [============>.................] - ETA: 4:45 - loss: 0.6691 - acc: 0.5957
2624/5677 [============>.................] - ETA: 4:38 - loss: 0.6685 - acc: 0.5968
2688/5677 [=============>................] - ETA: 4:32 - loss: 0.6689 - acc: 0.5967
2752/5677 [=============>................] - ETA: 4:28 - loss: 0.6692 - acc: 0.5967
2816/5677 [=============>................] - ETA: 4:22 - loss: 0.6695 - acc: 0.5962
2880/5677 [==============>...............] - ETA: 4:17 - loss: 0.6692 - acc: 0.5969
2944/5677 [==============>...............] - ETA: 4:11 - loss: 0.6693 - acc: 0.5975
3008/5677 [==============>...............] - ETA: 4:05 - loss: 0.6697 - acc: 0.5984
3072/5677 [===============>..............] - ETA: 3:58 - loss: 0.6700 - acc: 0.5980
3136/5677 [===============>..............] - ETA: 3:52 - loss: 0.6696 - acc: 0.5969
3200/5677 [===============>..............] - ETA: 3:46 - loss: 0.6687 - acc: 0.5984
3264/5677 [================>.............] - ETA: 3:40 - loss: 0.6692 - acc: 0.5987
3328/5677 [================>.............] - ETA: 3:35 - loss: 0.6682 - acc: 0.5998
3392/5677 [================>.............] - ETA: 3:29 - loss: 0.6680 - acc: 0.5994
3456/5677 [=================>............] - ETA: 3:24 - loss: 0.6675 - acc: 0.5998
3520/5677 [=================>............] - ETA: 3:18 - loss: 0.6680 - acc: 0.5997
3584/5677 [=================>............] - ETA: 3:12 - loss: 0.6680 - acc: 0.5999
3648/5677 [==================>...........] - ETA: 3:06 - loss: 0.6681 - acc: 0.5987
3712/5677 [==================>...........] - ETA: 3:00 - loss: 0.6694 - acc: 0.5962
3776/5677 [==================>...........] - ETA: 2:54 - loss: 0.6689 - acc: 0.5959
3840/5677 [===================>..........] - ETA: 2:48 - loss: 0.6686 - acc: 0.5966
3904/5677 [===================>..........] - ETA: 2:42 - loss: 0.6687 - acc: 0.5971
3968/5677 [===================>..........] - ETA: 2:36 - loss: 0.6691 - acc: 0.5968
4032/5677 [====================>.........] - ETA: 2:30 - loss: 0.6694 - acc: 0.5950
4096/5677 [====================>.........] - ETA: 2:25 - loss: 0.6687 - acc: 0.5957
4160/5677 [====================>.........] - ETA: 2:19 - loss: 0.6682 - acc: 0.5962
4224/5677 [=====================>........] - ETA: 2:13 - loss: 0.6677 - acc: 0.5978
4288/5677 [=====================>........] - ETA: 2:07 - loss: 0.6674 - acc: 0.5977
4352/5677 [=====================>........] - ETA: 2:01 - loss: 0.6671 - acc: 0.5970
4416/5677 [======================>.......] - ETA: 1:55 - loss: 0.6672 - acc: 0.5962
4480/5677 [======================>.......] - ETA: 1:49 - loss: 0.6670 - acc: 0.5971
4544/5677 [=======================>......] - ETA: 1:43 - loss: 0.6668 - acc: 0.5971
4608/5677 [=======================>......] - ETA: 1:37 - loss: 0.6663 - acc: 0.5974
4672/5677 [=======================>......] - ETA: 1:32 - loss: 0.6663 - acc: 0.5970
4736/5677 [========================>.....] - ETA: 1:26 - loss: 0.6663 - acc: 0.5965
4800/5677 [========================>.....] - ETA: 1:20 - loss: 0.6661 - acc: 0.5965
4864/5677 [========================>.....] - ETA: 1:14 - loss: 0.6672 - acc: 0.5948
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.6668 - acc: 0.5950
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6669 - acc: 0.5942
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6670 - acc: 0.5951 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6665 - acc: 0.5955
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6670 - acc: 0.5943
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6662 - acc: 0.5953
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6658 - acc: 0.5960
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6658 - acc: 0.5964
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6656 - acc: 0.5965
5504/5677 [============================>.] - ETA: 16s - loss: 0.6657 - acc: 0.5963
5568/5677 [============================>.] - ETA: 10s - loss: 0.6662 - acc: 0.5959
5632/5677 [============================>.] - ETA: 4s - loss: 0.6656 - acc: 0.5969 
5677/5677 [==============================] - 550s 97ms/step - loss: 0.6661 - acc: 0.5956 - val_loss: 0.7223 - val_acc: 0.5214

Epoch 00003: val_acc did not improve from 0.59905
Epoch 4/10

  64/5677 [..............................] - ETA: 9:06 - loss: 0.7230 - acc: 0.5156
 128/5677 [..............................] - ETA: 8:47 - loss: 0.7293 - acc: 0.5312
 192/5677 [>.............................] - ETA: 8:54 - loss: 0.7185 - acc: 0.5469
 256/5677 [>.............................] - ETA: 8:47 - loss: 0.6959 - acc: 0.5625
 320/5677 [>.............................] - ETA: 8:23 - loss: 0.6927 - acc: 0.5594
 384/5677 [=>............................] - ETA: 8:09 - loss: 0.6810 - acc: 0.5729
 448/5677 [=>............................] - ETA: 7:58 - loss: 0.6788 - acc: 0.5737
 512/5677 [=>............................] - ETA: 7:50 - loss: 0.6812 - acc: 0.5762
 576/5677 [==>...........................] - ETA: 7:41 - loss: 0.6786 - acc: 0.5799
 640/5677 [==>...........................] - ETA: 7:33 - loss: 0.6789 - acc: 0.5781
 704/5677 [==>...........................] - ETA: 7:36 - loss: 0.6790 - acc: 0.5824
 768/5677 [===>..........................] - ETA: 7:35 - loss: 0.6754 - acc: 0.5846
 832/5677 [===>..........................] - ETA: 7:27 - loss: 0.6736 - acc: 0.5865
 896/5677 [===>..........................] - ETA: 7:21 - loss: 0.6689 - acc: 0.5915
 960/5677 [====>.........................] - ETA: 7:11 - loss: 0.6677 - acc: 0.5927
1024/5677 [====>.........................] - ETA: 7:04 - loss: 0.6692 - acc: 0.5938
1088/5677 [====>.........................] - ETA: 6:55 - loss: 0.6675 - acc: 0.5983
1152/5677 [=====>........................] - ETA: 6:48 - loss: 0.6653 - acc: 0.5981
1216/5677 [=====>........................] - ETA: 6:43 - loss: 0.6677 - acc: 0.5929
1280/5677 [=====>........................] - ETA: 6:39 - loss: 0.6694 - acc: 0.5945
1344/5677 [======>.......................] - ETA: 6:32 - loss: 0.6711 - acc: 0.5923
1408/5677 [======>.......................] - ETA: 6:27 - loss: 0.6667 - acc: 0.5973
1472/5677 [======>.......................] - ETA: 6:20 - loss: 0.6680 - acc: 0.5965
1536/5677 [=======>......................] - ETA: 6:14 - loss: 0.6687 - acc: 0.5944
1600/5677 [=======>......................] - ETA: 6:08 - loss: 0.6689 - acc: 0.5944
1664/5677 [=======>......................] - ETA: 6:01 - loss: 0.6681 - acc: 0.5956
1728/5677 [========>.....................] - ETA: 5:54 - loss: 0.6690 - acc: 0.5972
1792/5677 [========>.....................] - ETA: 5:47 - loss: 0.6688 - acc: 0.5954
1856/5677 [========>.....................] - ETA: 5:42 - loss: 0.6689 - acc: 0.5943
1920/5677 [=========>....................] - ETA: 5:37 - loss: 0.6672 - acc: 0.5964
1984/5677 [=========>....................] - ETA: 5:32 - loss: 0.6685 - acc: 0.5938
2048/5677 [=========>....................] - ETA: 5:27 - loss: 0.6685 - acc: 0.5933
2112/5677 [==========>...................] - ETA: 5:22 - loss: 0.6664 - acc: 0.5942
2176/5677 [==========>...................] - ETA: 5:17 - loss: 0.6659 - acc: 0.5960
2240/5677 [==========>...................] - ETA: 5:10 - loss: 0.6651 - acc: 0.5951
2304/5677 [===========>..................] - ETA: 5:04 - loss: 0.6633 - acc: 0.5981
2368/5677 [===========>..................] - ETA: 4:58 - loss: 0.6641 - acc: 0.5967
2432/5677 [===========>..................] - ETA: 4:54 - loss: 0.6657 - acc: 0.5942
2496/5677 [============>.................] - ETA: 4:49 - loss: 0.6647 - acc: 0.5962
2560/5677 [============>.................] - ETA: 4:43 - loss: 0.6658 - acc: 0.5945
2624/5677 [============>.................] - ETA: 4:38 - loss: 0.6651 - acc: 0.5968
2688/5677 [=============>................] - ETA: 4:32 - loss: 0.6658 - acc: 0.5964
2752/5677 [=============>................] - ETA: 4:27 - loss: 0.6646 - acc: 0.5988
2816/5677 [=============>................] - ETA: 4:21 - loss: 0.6633 - acc: 0.6009
2880/5677 [==============>...............] - ETA: 4:16 - loss: 0.6625 - acc: 0.6021
2944/5677 [==============>...............] - ETA: 4:10 - loss: 0.6616 - acc: 0.6029
3008/5677 [==============>...............] - ETA: 4:04 - loss: 0.6606 - acc: 0.6037
3072/5677 [===============>..............] - ETA: 3:59 - loss: 0.6602 - acc: 0.6051
3136/5677 [===============>..............] - ETA: 3:53 - loss: 0.6617 - acc: 0.6033
3200/5677 [===============>..............] - ETA: 3:48 - loss: 0.6616 - acc: 0.6041
3264/5677 [================>.............] - ETA: 3:43 - loss: 0.6616 - acc: 0.6036
3328/5677 [================>.............] - ETA: 3:37 - loss: 0.6607 - acc: 0.6043
3392/5677 [================>.............] - ETA: 3:31 - loss: 0.6618 - acc: 0.6044
3456/5677 [=================>............] - ETA: 3:26 - loss: 0.6615 - acc: 0.6053
3520/5677 [=================>............] - ETA: 3:20 - loss: 0.6615 - acc: 0.6051
3584/5677 [=================>............] - ETA: 3:14 - loss: 0.6617 - acc: 0.6049
3648/5677 [==================>...........] - ETA: 3:08 - loss: 0.6611 - acc: 0.6064
3712/5677 [==================>...........] - ETA: 3:02 - loss: 0.6614 - acc: 0.6067
3776/5677 [==================>...........] - ETA: 2:56 - loss: 0.6612 - acc: 0.6067
3840/5677 [===================>..........] - ETA: 2:51 - loss: 0.6614 - acc: 0.6073
3904/5677 [===================>..........] - ETA: 2:45 - loss: 0.6598 - acc: 0.6094
3968/5677 [===================>..........] - ETA: 2:39 - loss: 0.6597 - acc: 0.6089
4032/5677 [====================>.........] - ETA: 2:33 - loss: 0.6590 - acc: 0.6091
4096/5677 [====================>.........] - ETA: 2:27 - loss: 0.6583 - acc: 0.6101
4160/5677 [====================>.........] - ETA: 2:21 - loss: 0.6594 - acc: 0.6089
4224/5677 [=====================>........] - ETA: 2:15 - loss: 0.6583 - acc: 0.6096
4288/5677 [=====================>........] - ETA: 2:09 - loss: 0.6580 - acc: 0.6096
4352/5677 [=====================>........] - ETA: 2:03 - loss: 0.6589 - acc: 0.6087
4416/5677 [======================>.......] - ETA: 1:57 - loss: 0.6577 - acc: 0.6098
4480/5677 [======================>.......] - ETA: 1:51 - loss: 0.6590 - acc: 0.6092
4544/5677 [=======================>......] - ETA: 1:45 - loss: 0.6595 - acc: 0.6089
4608/5677 [=======================>......] - ETA: 1:39 - loss: 0.6600 - acc: 0.6083
4672/5677 [=======================>......] - ETA: 1:33 - loss: 0.6598 - acc: 0.6085
4736/5677 [========================>.....] - ETA: 1:27 - loss: 0.6592 - acc: 0.6092
4800/5677 [========================>.....] - ETA: 1:21 - loss: 0.6591 - acc: 0.6094
4864/5677 [========================>.....] - ETA: 1:15 - loss: 0.6589 - acc: 0.6098
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.6593 - acc: 0.6088
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6597 - acc: 0.6082
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6603 - acc: 0.6074 
5120/5677 [==========================>...] - ETA: 52s - loss: 0.6599 - acc: 0.6074
5184/5677 [==========================>...] - ETA: 46s - loss: 0.6590 - acc: 0.6084
5248/5677 [==========================>...] - ETA: 40s - loss: 0.6596 - acc: 0.6075
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6603 - acc: 0.6064
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6597 - acc: 0.6071
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6605 - acc: 0.6051
5504/5677 [============================>.] - ETA: 16s - loss: 0.6601 - acc: 0.6065
5568/5677 [============================>.] - ETA: 10s - loss: 0.6604 - acc: 0.6058
5632/5677 [============================>.] - ETA: 4s - loss: 0.6605 - acc: 0.6056 
5677/5677 [==============================] - 553s 97ms/step - loss: 0.6603 - acc: 0.6054 - val_loss: 0.6338 - val_acc: 0.6292

Epoch 00004: val_acc improved from 0.59905 to 0.62916, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window11/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 5/10

  64/5677 [..............................] - ETA: 9:18 - loss: 0.6375 - acc: 0.6094
 128/5677 [..............................] - ETA: 8:28 - loss: 0.6623 - acc: 0.5859
 192/5677 [>.............................] - ETA: 8:47 - loss: 0.6501 - acc: 0.6094
 256/5677 [>.............................] - ETA: 8:37 - loss: 0.6487 - acc: 0.6094
 320/5677 [>.............................] - ETA: 8:28 - loss: 0.6423 - acc: 0.6125
 384/5677 [=>............................] - ETA: 8:22 - loss: 0.6545 - acc: 0.5964
 448/5677 [=>............................] - ETA: 8:15 - loss: 0.6488 - acc: 0.6004
 512/5677 [=>............................] - ETA: 8:03 - loss: 0.6457 - acc: 0.6172
 576/5677 [==>...........................] - ETA: 8:00 - loss: 0.6461 - acc: 0.6198
 640/5677 [==>...........................] - ETA: 7:54 - loss: 0.6472 - acc: 0.6234
 704/5677 [==>...........................] - ETA: 7:47 - loss: 0.6443 - acc: 0.6207
 768/5677 [===>..........................] - ETA: 7:41 - loss: 0.6462 - acc: 0.6250
 832/5677 [===>..........................] - ETA: 7:36 - loss: 0.6510 - acc: 0.6214
 896/5677 [===>..........................] - ETA: 7:31 - loss: 0.6534 - acc: 0.6194
 960/5677 [====>.........................] - ETA: 7:25 - loss: 0.6570 - acc: 0.6135
1024/5677 [====>.........................] - ETA: 7:34 - loss: 0.6537 - acc: 0.6162
1088/5677 [====>.........................] - ETA: 7:35 - loss: 0.6514 - acc: 0.6176
1152/5677 [=====>........................] - ETA: 7:26 - loss: 0.6507 - acc: 0.6163
1216/5677 [=====>........................] - ETA: 7:17 - loss: 0.6500 - acc: 0.6151
1280/5677 [=====>........................] - ETA: 7:08 - loss: 0.6526 - acc: 0.6117
1344/5677 [======>.......................] - ETA: 7:00 - loss: 0.6529 - acc: 0.6124
1408/5677 [======>.......................] - ETA: 6:51 - loss: 0.6514 - acc: 0.6108
1472/5677 [======>.......................] - ETA: 6:43 - loss: 0.6507 - acc: 0.6101
1536/5677 [=======>......................] - ETA: 6:35 - loss: 0.6499 - acc: 0.6113
1600/5677 [=======>......................] - ETA: 6:27 - loss: 0.6516 - acc: 0.6088
1664/5677 [=======>......................] - ETA: 6:19 - loss: 0.6504 - acc: 0.6106
1728/5677 [========>.....................] - ETA: 6:11 - loss: 0.6522 - acc: 0.6105
1792/5677 [========>.....................] - ETA: 6:04 - loss: 0.6527 - acc: 0.6094
1856/5677 [========>.....................] - ETA: 5:56 - loss: 0.6547 - acc: 0.6067
1920/5677 [=========>....................] - ETA: 5:50 - loss: 0.6541 - acc: 0.6078
1984/5677 [=========>....................] - ETA: 5:43 - loss: 0.6533 - acc: 0.6053
2048/5677 [=========>....................] - ETA: 5:36 - loss: 0.6528 - acc: 0.6069
2112/5677 [==========>...................] - ETA: 5:29 - loss: 0.6537 - acc: 0.6042
2176/5677 [==========>...................] - ETA: 5:22 - loss: 0.6551 - acc: 0.6011
2240/5677 [==========>...................] - ETA: 5:16 - loss: 0.6542 - acc: 0.6031
2304/5677 [===========>..................] - ETA: 5:10 - loss: 0.6526 - acc: 0.6072
2368/5677 [===========>..................] - ETA: 5:03 - loss: 0.6516 - acc: 0.6085
2432/5677 [===========>..................] - ETA: 4:56 - loss: 0.6527 - acc: 0.6073
2496/5677 [============>.................] - ETA: 4:50 - loss: 0.6530 - acc: 0.6078
2560/5677 [============>.................] - ETA: 4:44 - loss: 0.6526 - acc: 0.6070
2624/5677 [============>.................] - ETA: 4:37 - loss: 0.6540 - acc: 0.6048
2688/5677 [=============>................] - ETA: 4:31 - loss: 0.6536 - acc: 0.6053
2752/5677 [=============>................] - ETA: 4:25 - loss: 0.6532 - acc: 0.6061
2816/5677 [=============>................] - ETA: 4:19 - loss: 0.6535 - acc: 0.6069
2880/5677 [==============>...............] - ETA: 4:12 - loss: 0.6526 - acc: 0.6073
2944/5677 [==============>...............] - ETA: 4:07 - loss: 0.6517 - acc: 0.6084
3008/5677 [==============>...............] - ETA: 4:00 - loss: 0.6529 - acc: 0.6070
3072/5677 [===============>..............] - ETA: 3:54 - loss: 0.6534 - acc: 0.6058
3136/5677 [===============>..............] - ETA: 3:48 - loss: 0.6530 - acc: 0.6071
3200/5677 [===============>..............] - ETA: 3:42 - loss: 0.6530 - acc: 0.6078
3264/5677 [================>.............] - ETA: 3:35 - loss: 0.6542 - acc: 0.6072
3328/5677 [================>.............] - ETA: 3:29 - loss: 0.6552 - acc: 0.6055
3392/5677 [================>.............] - ETA: 3:24 - loss: 0.6553 - acc: 0.6055
3456/5677 [=================>............] - ETA: 3:18 - loss: 0.6544 - acc: 0.6068
3520/5677 [=================>............] - ETA: 3:12 - loss: 0.6544 - acc: 0.6062
3584/5677 [=================>............] - ETA: 3:06 - loss: 0.6540 - acc: 0.6066
3648/5677 [==================>...........] - ETA: 3:00 - loss: 0.6537 - acc: 0.6061
3712/5677 [==================>...........] - ETA: 2:54 - loss: 0.6529 - acc: 0.6075
3776/5677 [==================>...........] - ETA: 2:49 - loss: 0.6536 - acc: 0.6057
3840/5677 [===================>..........] - ETA: 2:43 - loss: 0.6541 - acc: 0.6039
3904/5677 [===================>..........] - ETA: 2:37 - loss: 0.6537 - acc: 0.6037
3968/5677 [===================>..........] - ETA: 2:31 - loss: 0.6533 - acc: 0.6051
4032/5677 [====================>.........] - ETA: 2:25 - loss: 0.6538 - acc: 0.6054
4096/5677 [====================>.........] - ETA: 2:19 - loss: 0.6540 - acc: 0.6055
4160/5677 [====================>.........] - ETA: 2:14 - loss: 0.6540 - acc: 0.6062
4224/5677 [=====================>........] - ETA: 2:08 - loss: 0.6549 - acc: 0.6051
4288/5677 [=====================>........] - ETA: 2:02 - loss: 0.6563 - acc: 0.6042
4352/5677 [=====================>........] - ETA: 1:56 - loss: 0.6556 - acc: 0.6055
4416/5677 [======================>.......] - ETA: 1:51 - loss: 0.6557 - acc: 0.6055
4480/5677 [======================>.......] - ETA: 1:45 - loss: 0.6556 - acc: 0.6065
4544/5677 [=======================>......] - ETA: 1:39 - loss: 0.6550 - acc: 0.6083
4608/5677 [=======================>......] - ETA: 1:33 - loss: 0.6548 - acc: 0.6074
4672/5677 [=======================>......] - ETA: 1:28 - loss: 0.6546 - acc: 0.6074
4736/5677 [========================>.....] - ETA: 1:22 - loss: 0.6536 - acc: 0.6090
4800/5677 [========================>.....] - ETA: 1:16 - loss: 0.6536 - acc: 0.6096
4864/5677 [========================>.....] - ETA: 1:11 - loss: 0.6536 - acc: 0.6098
4928/5677 [=========================>....] - ETA: 1:05 - loss: 0.6526 - acc: 0.6108
4992/5677 [=========================>....] - ETA: 59s - loss: 0.6532 - acc: 0.6108 
5056/5677 [=========================>....] - ETA: 54s - loss: 0.6527 - acc: 0.6116
5120/5677 [==========================>...] - ETA: 48s - loss: 0.6531 - acc: 0.6115
5184/5677 [==========================>...] - ETA: 43s - loss: 0.6529 - acc: 0.6125
5248/5677 [==========================>...] - ETA: 37s - loss: 0.6531 - acc: 0.6117
5312/5677 [===========================>..] - ETA: 31s - loss: 0.6535 - acc: 0.6114
5376/5677 [===========================>..] - ETA: 26s - loss: 0.6527 - acc: 0.6124
5440/5677 [===========================>..] - ETA: 20s - loss: 0.6535 - acc: 0.6119
5504/5677 [============================>.] - ETA: 15s - loss: 0.6532 - acc: 0.6130
5568/5677 [============================>.] - ETA: 9s - loss: 0.6528 - acc: 0.6139 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6527 - acc: 0.6138
5677/5677 [==============================] - 518s 91ms/step - loss: 0.6526 - acc: 0.6141 - val_loss: 0.6380 - val_acc: 0.6371

Epoch 00005: val_acc improved from 0.62916 to 0.63708, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window11/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 9:51 - loss: 0.5825 - acc: 0.7500
 128/5677 [..............................] - ETA: 9:10 - loss: 0.5926 - acc: 0.7266
 192/5677 [>.............................] - ETA: 8:47 - loss: 0.5747 - acc: 0.7240
 256/5677 [>.............................] - ETA: 8:21 - loss: 0.5855 - acc: 0.6992
 320/5677 [>.............................] - ETA: 8:11 - loss: 0.5995 - acc: 0.6813
 384/5677 [=>............................] - ETA: 8:10 - loss: 0.6046 - acc: 0.6849
 448/5677 [=>............................] - ETA: 8:06 - loss: 0.6050 - acc: 0.6830
 512/5677 [=>............................] - ETA: 7:57 - loss: 0.6102 - acc: 0.6738
 576/5677 [==>...........................] - ETA: 7:47 - loss: 0.6065 - acc: 0.6701
 640/5677 [==>...........................] - ETA: 7:38 - loss: 0.6056 - acc: 0.6734
 704/5677 [==>...........................] - ETA: 7:39 - loss: 0.6128 - acc: 0.6676
 768/5677 [===>..........................] - ETA: 7:38 - loss: 0.6235 - acc: 0.6549
 832/5677 [===>..........................] - ETA: 7:34 - loss: 0.6277 - acc: 0.6514
 896/5677 [===>..........................] - ETA: 7:34 - loss: 0.6341 - acc: 0.6440
 960/5677 [====>.........................] - ETA: 7:28 - loss: 0.6316 - acc: 0.6448
1024/5677 [====>.........................] - ETA: 7:19 - loss: 0.6380 - acc: 0.6387
1088/5677 [====>.........................] - ETA: 7:10 - loss: 0.6367 - acc: 0.6415
1152/5677 [=====>........................] - ETA: 7:03 - loss: 0.6380 - acc: 0.6380
1216/5677 [=====>........................] - ETA: 6:57 - loss: 0.6390 - acc: 0.6357
1280/5677 [=====>........................] - ETA: 6:51 - loss: 0.6395 - acc: 0.6359
1344/5677 [======>.......................] - ETA: 6:45 - loss: 0.6421 - acc: 0.6302
1408/5677 [======>.......................] - ETA: 6:39 - loss: 0.6459 - acc: 0.6264
1472/5677 [======>.......................] - ETA: 6:34 - loss: 0.6465 - acc: 0.6257
1536/5677 [=======>......................] - ETA: 6:27 - loss: 0.6470 - acc: 0.6250
1600/5677 [=======>......................] - ETA: 6:20 - loss: 0.6471 - acc: 0.6225
1664/5677 [=======>......................] - ETA: 6:13 - loss: 0.6456 - acc: 0.6238
1728/5677 [========>.....................] - ETA: 6:05 - loss: 0.6460 - acc: 0.6238
1792/5677 [========>.....................] - ETA: 5:58 - loss: 0.6475 - acc: 0.6222
1856/5677 [========>.....................] - ETA: 5:55 - loss: 0.6481 - acc: 0.6207
1920/5677 [=========>....................] - ETA: 5:50 - loss: 0.6489 - acc: 0.6188
1984/5677 [=========>....................] - ETA: 5:45 - loss: 0.6505 - acc: 0.6149
2048/5677 [=========>....................] - ETA: 5:39 - loss: 0.6514 - acc: 0.6133
2112/5677 [==========>...................] - ETA: 5:33 - loss: 0.6515 - acc: 0.6127
2176/5677 [==========>...................] - ETA: 5:28 - loss: 0.6517 - acc: 0.6121
2240/5677 [==========>...................] - ETA: 5:22 - loss: 0.6521 - acc: 0.6121
2304/5677 [===========>..................] - ETA: 5:17 - loss: 0.6525 - acc: 0.6115
2368/5677 [===========>..................] - ETA: 5:10 - loss: 0.6526 - acc: 0.6111
2432/5677 [===========>..................] - ETA: 5:03 - loss: 0.6541 - acc: 0.6077
2496/5677 [============>.................] - ETA: 4:58 - loss: 0.6533 - acc: 0.6082
2560/5677 [============>.................] - ETA: 4:52 - loss: 0.6536 - acc: 0.6070
2624/5677 [============>.................] - ETA: 4:46 - loss: 0.6531 - acc: 0.6067
2688/5677 [=============>................] - ETA: 4:41 - loss: 0.6544 - acc: 0.6034
2752/5677 [=============>................] - ETA: 4:34 - loss: 0.6558 - acc: 0.6025
2816/5677 [=============>................] - ETA: 4:28 - loss: 0.6561 - acc: 0.6001
2880/5677 [==============>...............] - ETA: 4:22 - loss: 0.6571 - acc: 0.5997
2944/5677 [==============>...............] - ETA: 4:16 - loss: 0.6564 - acc: 0.6012
3008/5677 [==============>...............] - ETA: 4:09 - loss: 0.6563 - acc: 0.6017
3072/5677 [===============>..............] - ETA: 4:03 - loss: 0.6558 - acc: 0.6029
3136/5677 [===============>..............] - ETA: 3:57 - loss: 0.6558 - acc: 0.6040
3200/5677 [===============>..............] - ETA: 3:51 - loss: 0.6562 - acc: 0.6025
3264/5677 [================>.............] - ETA: 3:45 - loss: 0.6565 - acc: 0.6011
3328/5677 [================>.............] - ETA: 3:39 - loss: 0.6565 - acc: 0.6010
3392/5677 [================>.............] - ETA: 3:33 - loss: 0.6566 - acc: 0.6008
3456/5677 [=================>............] - ETA: 3:27 - loss: 0.6572 - acc: 0.5998
3520/5677 [=================>............] - ETA: 3:21 - loss: 0.6578 - acc: 0.5983
3584/5677 [=================>............] - ETA: 3:15 - loss: 0.6575 - acc: 0.5982
3648/5677 [==================>...........] - ETA: 3:09 - loss: 0.6573 - acc: 0.5979
3712/5677 [==================>...........] - ETA: 3:03 - loss: 0.6560 - acc: 0.6005
3776/5677 [==================>...........] - ETA: 2:57 - loss: 0.6574 - acc: 0.5977
3840/5677 [===================>..........] - ETA: 2:51 - loss: 0.6577 - acc: 0.5969
3904/5677 [===================>..........] - ETA: 2:45 - loss: 0.6580 - acc: 0.5971
3968/5677 [===================>..........] - ETA: 2:39 - loss: 0.6577 - acc: 0.5978
4032/5677 [====================>.........] - ETA: 2:33 - loss: 0.6575 - acc: 0.5980
4096/5677 [====================>.........] - ETA: 2:28 - loss: 0.6578 - acc: 0.5974
4160/5677 [====================>.........] - ETA: 2:22 - loss: 0.6574 - acc: 0.5986
4224/5677 [=====================>........] - ETA: 2:16 - loss: 0.6559 - acc: 0.6004
4288/5677 [=====================>........] - ETA: 2:10 - loss: 0.6556 - acc: 0.6005
4352/5677 [=====================>........] - ETA: 2:03 - loss: 0.6543 - acc: 0.6032
4416/5677 [======================>.......] - ETA: 1:57 - loss: 0.6531 - acc: 0.6037
4480/5677 [======================>.......] - ETA: 1:51 - loss: 0.6531 - acc: 0.6031
4544/5677 [=======================>......] - ETA: 1:45 - loss: 0.6530 - acc: 0.6026
4608/5677 [=======================>......] - ETA: 1:39 - loss: 0.6527 - acc: 0.6024
4672/5677 [=======================>......] - ETA: 1:33 - loss: 0.6530 - acc: 0.6012
4736/5677 [========================>.....] - ETA: 1:27 - loss: 0.6518 - acc: 0.6030
4800/5677 [========================>.....] - ETA: 1:21 - loss: 0.6515 - acc: 0.6033
4864/5677 [========================>.....] - ETA: 1:15 - loss: 0.6517 - acc: 0.6030
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.6516 - acc: 0.6039
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6515 - acc: 0.6042
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6509 - acc: 0.6046 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6502 - acc: 0.6053
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6507 - acc: 0.6049
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6502 - acc: 0.6056
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6505 - acc: 0.6054
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6494 - acc: 0.6068
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6497 - acc: 0.6062
5504/5677 [============================>.] - ETA: 15s - loss: 0.6492 - acc: 0.6072
5568/5677 [============================>.] - ETA: 9s - loss: 0.6500 - acc: 0.6061 
5632/5677 [============================>.] - ETA: 4s - loss: 0.6498 - acc: 0.6060
5677/5677 [==============================] - 536s 94ms/step - loss: 0.6498 - acc: 0.6061 - val_loss: 0.6228 - val_acc: 0.6323

Epoch 00006: val_acc did not improve from 0.63708
Epoch 7/10

  64/5677 [..............................] - ETA: 7:13 - loss: 0.6161 - acc: 0.6562
 128/5677 [..............................] - ETA: 7:37 - loss: 0.6166 - acc: 0.6406
 192/5677 [>.............................] - ETA: 7:32 - loss: 0.6427 - acc: 0.6146
 256/5677 [>.............................] - ETA: 7:36 - loss: 0.6408 - acc: 0.6289
 320/5677 [>.............................] - ETA: 7:40 - loss: 0.6318 - acc: 0.6531
 384/5677 [=>............................] - ETA: 7:38 - loss: 0.6344 - acc: 0.6432
 448/5677 [=>............................] - ETA: 7:38 - loss: 0.6352 - acc: 0.6362
 512/5677 [=>............................] - ETA: 7:37 - loss: 0.6346 - acc: 0.6328
 576/5677 [==>...........................] - ETA: 7:28 - loss: 0.6293 - acc: 0.6476
 640/5677 [==>...........................] - ETA: 7:23 - loss: 0.6291 - acc: 0.6500
 704/5677 [==>...........................] - ETA: 7:20 - loss: 0.6281 - acc: 0.6463
 768/5677 [===>..........................] - ETA: 7:15 - loss: 0.6350 - acc: 0.6419
 832/5677 [===>..........................] - ETA: 7:12 - loss: 0.6350 - acc: 0.6406
 896/5677 [===>..........................] - ETA: 7:08 - loss: 0.6347 - acc: 0.6406
 960/5677 [====>.........................] - ETA: 7:04 - loss: 0.6385 - acc: 0.6344
1024/5677 [====>.........................] - ETA: 6:59 - loss: 0.6401 - acc: 0.6309
1088/5677 [====>.........................] - ETA: 6:52 - loss: 0.6379 - acc: 0.6351
1152/5677 [=====>........................] - ETA: 6:47 - loss: 0.6384 - acc: 0.6293
1216/5677 [=====>........................] - ETA: 6:44 - loss: 0.6358 - acc: 0.6324
1280/5677 [=====>........................] - ETA: 6:39 - loss: 0.6389 - acc: 0.6258
1344/5677 [======>.......................] - ETA: 6:33 - loss: 0.6386 - acc: 0.6272
1408/5677 [======>.......................] - ETA: 6:25 - loss: 0.6367 - acc: 0.6286
1472/5677 [======>.......................] - ETA: 6:20 - loss: 0.6353 - acc: 0.6311
1536/5677 [=======>......................] - ETA: 6:13 - loss: 0.6349 - acc: 0.6322
1600/5677 [=======>......................] - ETA: 6:06 - loss: 0.6371 - acc: 0.6300
1664/5677 [=======>......................] - ETA: 5:59 - loss: 0.6357 - acc: 0.6334
1728/5677 [========>.....................] - ETA: 5:52 - loss: 0.6352 - acc: 0.6343
1792/5677 [========>.....................] - ETA: 5:46 - loss: 0.6381 - acc: 0.6317
1856/5677 [========>.....................] - ETA: 5:40 - loss: 0.6389 - acc: 0.6304
1920/5677 [=========>....................] - ETA: 5:33 - loss: 0.6388 - acc: 0.6318
1984/5677 [=========>....................] - ETA: 5:27 - loss: 0.6378 - acc: 0.6321
2048/5677 [=========>....................] - ETA: 5:22 - loss: 0.6395 - acc: 0.6313
2112/5677 [==========>...................] - ETA: 5:16 - loss: 0.6411 - acc: 0.6288
2176/5677 [==========>...................] - ETA: 5:11 - loss: 0.6410 - acc: 0.6282
2240/5677 [==========>...................] - ETA: 5:05 - loss: 0.6406 - acc: 0.6290
2304/5677 [===========>..................] - ETA: 4:59 - loss: 0.6421 - acc: 0.6259
2368/5677 [===========>..................] - ETA: 4:53 - loss: 0.6435 - acc: 0.6263
2432/5677 [===========>..................] - ETA: 4:47 - loss: 0.6445 - acc: 0.6242
2496/5677 [============>.................] - ETA: 4:41 - loss: 0.6449 - acc: 0.6242
2560/5677 [============>.................] - ETA: 4:35 - loss: 0.6437 - acc: 0.6250
2624/5677 [============>.................] - ETA: 4:29 - loss: 0.6432 - acc: 0.6261
2688/5677 [=============>................] - ETA: 4:23 - loss: 0.6427 - acc: 0.6265
2752/5677 [=============>................] - ETA: 4:17 - loss: 0.6435 - acc: 0.6246
2816/5677 [=============>................] - ETA: 4:11 - loss: 0.6432 - acc: 0.6257
2880/5677 [==============>...............] - ETA: 4:05 - loss: 0.6444 - acc: 0.6240
2944/5677 [==============>...............] - ETA: 4:00 - loss: 0.6434 - acc: 0.6243
3008/5677 [==============>...............] - ETA: 3:54 - loss: 0.6449 - acc: 0.6217
3072/5677 [===============>..............] - ETA: 3:48 - loss: 0.6450 - acc: 0.6211
3136/5677 [===============>..............] - ETA: 3:42 - loss: 0.6453 - acc: 0.6205
3200/5677 [===============>..............] - ETA: 3:37 - loss: 0.6448 - acc: 0.6209
3264/5677 [================>.............] - ETA: 3:31 - loss: 0.6447 - acc: 0.6216
3328/5677 [================>.............] - ETA: 3:25 - loss: 0.6448 - acc: 0.6217
3392/5677 [================>.............] - ETA: 3:20 - loss: 0.6452 - acc: 0.6223
3456/5677 [=================>............] - ETA: 3:15 - loss: 0.6456 - acc: 0.6218
3520/5677 [=================>............] - ETA: 3:09 - loss: 0.6462 - acc: 0.6216
3584/5677 [=================>............] - ETA: 3:04 - loss: 0.6455 - acc: 0.6228
3648/5677 [==================>...........] - ETA: 2:58 - loss: 0.6448 - acc: 0.6236
3712/5677 [==================>...........] - ETA: 2:53 - loss: 0.6447 - acc: 0.6239
3776/5677 [==================>...........] - ETA: 2:48 - loss: 0.6466 - acc: 0.6213
3840/5677 [===================>..........] - ETA: 2:42 - loss: 0.6472 - acc: 0.6216
3904/5677 [===================>..........] - ETA: 2:36 - loss: 0.6469 - acc: 0.6224
3968/5677 [===================>..........] - ETA: 2:31 - loss: 0.6466 - acc: 0.6227
4032/5677 [====================>.........] - ETA: 2:25 - loss: 0.6450 - acc: 0.6252
4096/5677 [====================>.........] - ETA: 2:20 - loss: 0.6464 - acc: 0.6230
4160/5677 [====================>.........] - ETA: 2:14 - loss: 0.6469 - acc: 0.6228
4224/5677 [=====================>........] - ETA: 2:08 - loss: 0.6475 - acc: 0.6219
4288/5677 [=====================>........] - ETA: 2:03 - loss: 0.6487 - acc: 0.6206
4352/5677 [=====================>........] - ETA: 1:57 - loss: 0.6483 - acc: 0.6220
4416/5677 [======================>.......] - ETA: 1:52 - loss: 0.6480 - acc: 0.6218
4480/5677 [======================>.......] - ETA: 1:46 - loss: 0.6477 - acc: 0.6221
4544/5677 [=======================>......] - ETA: 1:40 - loss: 0.6476 - acc: 0.6230
4608/5677 [=======================>......] - ETA: 1:34 - loss: 0.6481 - acc: 0.6222
4672/5677 [=======================>......] - ETA: 1:29 - loss: 0.6484 - acc: 0.6218
4736/5677 [========================>.....] - ETA: 1:23 - loss: 0.6488 - acc: 0.6210
4800/5677 [========================>.....] - ETA: 1:18 - loss: 0.6489 - acc: 0.6210
4864/5677 [========================>.....] - ETA: 1:12 - loss: 0.6491 - acc: 0.6207
4928/5677 [=========================>....] - ETA: 1:06 - loss: 0.6489 - acc: 0.6197
4992/5677 [=========================>....] - ETA: 1:00 - loss: 0.6485 - acc: 0.6204
5056/5677 [=========================>....] - ETA: 55s - loss: 0.6485 - acc: 0.6205 
5120/5677 [==========================>...] - ETA: 49s - loss: 0.6491 - acc: 0.6186
5184/5677 [==========================>...] - ETA: 44s - loss: 0.6492 - acc: 0.6181
5248/5677 [==========================>...] - ETA: 38s - loss: 0.6495 - acc: 0.6179
5312/5677 [===========================>..] - ETA: 32s - loss: 0.6494 - acc: 0.6178
5376/5677 [===========================>..] - ETA: 26s - loss: 0.6489 - acc: 0.6185
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6492 - acc: 0.6175
5504/5677 [============================>.] - ETA: 15s - loss: 0.6486 - acc: 0.6183
5568/5677 [============================>.] - ETA: 9s - loss: 0.6490 - acc: 0.6182 
5632/5677 [============================>.] - ETA: 4s - loss: 0.6494 - acc: 0.6172
5677/5677 [==============================] - 528s 93ms/step - loss: 0.6495 - acc: 0.6167 - val_loss: 0.6351 - val_acc: 0.6181

Epoch 00007: val_acc did not improve from 0.63708
Epoch 8/10

  64/5677 [..............................] - ETA: 8:05 - loss: 0.6428 - acc: 0.6406
 128/5677 [..............................] - ETA: 7:49 - loss: 0.6221 - acc: 0.6719
 192/5677 [>.............................] - ETA: 7:32 - loss: 0.6262 - acc: 0.6562
 256/5677 [>.............................] - ETA: 7:35 - loss: 0.6380 - acc: 0.6406
 320/5677 [>.............................] - ETA: 7:52 - loss: 0.6374 - acc: 0.6344
 384/5677 [=>............................] - ETA: 7:55 - loss: 0.6476 - acc: 0.6276
 448/5677 [=>............................] - ETA: 7:52 - loss: 0.6505 - acc: 0.6250
 512/5677 [=>............................] - ETA: 7:56 - loss: 0.6495 - acc: 0.6250
 576/5677 [==>...........................] - ETA: 7:43 - loss: 0.6501 - acc: 0.6267
 640/5677 [==>...........................] - ETA: 7:37 - loss: 0.6510 - acc: 0.6281
 704/5677 [==>...........................] - ETA: 7:27 - loss: 0.6473 - acc: 0.6293
 768/5677 [===>..........................] - ETA: 7:21 - loss: 0.6465 - acc: 0.6263
 832/5677 [===>..........................] - ETA: 7:24 - loss: 0.6442 - acc: 0.6286
 896/5677 [===>..........................] - ETA: 7:25 - loss: 0.6451 - acc: 0.6283
 960/5677 [====>.........................] - ETA: 7:20 - loss: 0.6425 - acc: 0.6344
1024/5677 [====>.........................] - ETA: 7:13 - loss: 0.6410 - acc: 0.6309
1088/5677 [====>.........................] - ETA: 7:09 - loss: 0.6418 - acc: 0.6278
1152/5677 [=====>........................] - ETA: 7:02 - loss: 0.6415 - acc: 0.6293
1216/5677 [=====>........................] - ETA: 6:58 - loss: 0.6403 - acc: 0.6299
1280/5677 [=====>........................] - ETA: 6:51 - loss: 0.6396 - acc: 0.6328
1344/5677 [======>.......................] - ETA: 6:44 - loss: 0.6382 - acc: 0.6354
1408/5677 [======>.......................] - ETA: 6:38 - loss: 0.6390 - acc: 0.6328
1472/5677 [======>.......................] - ETA: 6:33 - loss: 0.6404 - acc: 0.6318
1536/5677 [=======>......................] - ETA: 6:29 - loss: 0.6423 - acc: 0.6309
1600/5677 [=======>......................] - ETA: 6:23 - loss: 0.6412 - acc: 0.6312
1664/5677 [=======>......................] - ETA: 6:17 - loss: 0.6409 - acc: 0.6298
1728/5677 [========>.....................] - ETA: 6:10 - loss: 0.6436 - acc: 0.6267
1792/5677 [========>.....................] - ETA: 6:04 - loss: 0.6435 - acc: 0.6283
1856/5677 [========>.....................] - ETA: 5:59 - loss: 0.6430 - acc: 0.6288
1920/5677 [=========>....................] - ETA: 5:54 - loss: 0.6423 - acc: 0.6297
1984/5677 [=========>....................] - ETA: 5:47 - loss: 0.6417 - acc: 0.6300
2048/5677 [=========>....................] - ETA: 5:41 - loss: 0.6393 - acc: 0.6333
2112/5677 [==========>...................] - ETA: 5:35 - loss: 0.6386 - acc: 0.6364
2176/5677 [==========>...................] - ETA: 5:28 - loss: 0.6397 - acc: 0.6360
2240/5677 [==========>...................] - ETA: 5:22 - loss: 0.6385 - acc: 0.6366
2304/5677 [===========>..................] - ETA: 5:16 - loss: 0.6396 - acc: 0.6350
2368/5677 [===========>..................] - ETA: 5:10 - loss: 0.6389 - acc: 0.6368
2432/5677 [===========>..................] - ETA: 5:05 - loss: 0.6398 - acc: 0.6373
2496/5677 [============>.................] - ETA: 4:59 - loss: 0.6408 - acc: 0.6358
2560/5677 [============>.................] - ETA: 4:53 - loss: 0.6410 - acc: 0.6352
2624/5677 [============>.................] - ETA: 4:47 - loss: 0.6422 - acc: 0.6326
2688/5677 [=============>................] - ETA: 4:41 - loss: 0.6420 - acc: 0.6332
2752/5677 [=============>................] - ETA: 4:35 - loss: 0.6432 - acc: 0.6301
2816/5677 [=============>................] - ETA: 4:29 - loss: 0.6429 - acc: 0.6300
2880/5677 [==============>...............] - ETA: 4:23 - loss: 0.6437 - acc: 0.6281
2944/5677 [==============>...............] - ETA: 4:17 - loss: 0.6431 - acc: 0.6287
3008/5677 [==============>...............] - ETA: 4:11 - loss: 0.6451 - acc: 0.6253
3072/5677 [===============>..............] - ETA: 4:05 - loss: 0.6443 - acc: 0.6263
3136/5677 [===============>..............] - ETA: 3:59 - loss: 0.6440 - acc: 0.6266
3200/5677 [===============>..............] - ETA: 3:53 - loss: 0.6443 - acc: 0.6275
3264/5677 [================>.............] - ETA: 3:47 - loss: 0.6443 - acc: 0.6278
3328/5677 [================>.............] - ETA: 3:41 - loss: 0.6439 - acc: 0.6274
3392/5677 [================>.............] - ETA: 3:34 - loss: 0.6435 - acc: 0.6288
3456/5677 [=================>............] - ETA: 3:28 - loss: 0.6429 - acc: 0.6302
3520/5677 [=================>............] - ETA: 3:22 - loss: 0.6430 - acc: 0.6307
3584/5677 [=================>............] - ETA: 3:16 - loss: 0.6442 - acc: 0.6295
3648/5677 [==================>...........] - ETA: 3:10 - loss: 0.6444 - acc: 0.6294
3712/5677 [==================>...........] - ETA: 3:04 - loss: 0.6446 - acc: 0.6296
3776/5677 [==================>...........] - ETA: 2:57 - loss: 0.6437 - acc: 0.6300
3840/5677 [===================>..........] - ETA: 2:51 - loss: 0.6435 - acc: 0.6299
3904/5677 [===================>..........] - ETA: 2:45 - loss: 0.6439 - acc: 0.6301
3968/5677 [===================>..........] - ETA: 2:39 - loss: 0.6443 - acc: 0.6295
4032/5677 [====================>.........] - ETA: 2:33 - loss: 0.6455 - acc: 0.6280
4096/5677 [====================>.........] - ETA: 2:27 - loss: 0.6455 - acc: 0.6287
4160/5677 [====================>.........] - ETA: 2:21 - loss: 0.6442 - acc: 0.6298
4224/5677 [=====================>........] - ETA: 2:15 - loss: 0.6444 - acc: 0.6295
4288/5677 [=====================>........] - ETA: 2:09 - loss: 0.6438 - acc: 0.6299
4352/5677 [=====================>........] - ETA: 2:03 - loss: 0.6431 - acc: 0.6312
4416/5677 [======================>.......] - ETA: 1:57 - loss: 0.6430 - acc: 0.6307
4480/5677 [======================>.......] - ETA: 1:51 - loss: 0.6434 - acc: 0.6297
4544/5677 [=======================>......] - ETA: 1:45 - loss: 0.6439 - acc: 0.6290
4608/5677 [=======================>......] - ETA: 1:39 - loss: 0.6442 - acc: 0.6296
4672/5677 [=======================>......] - ETA: 1:33 - loss: 0.6443 - acc: 0.6293
4736/5677 [========================>.....] - ETA: 1:27 - loss: 0.6451 - acc: 0.6282
4800/5677 [========================>.....] - ETA: 1:21 - loss: 0.6445 - acc: 0.6285
4864/5677 [========================>.....] - ETA: 1:15 - loss: 0.6456 - acc: 0.6277
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.6452 - acc: 0.6280
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6452 - acc: 0.6276
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6451 - acc: 0.6274 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6450 - acc: 0.6275
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6449 - acc: 0.6279
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6452 - acc: 0.6282
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6447 - acc: 0.6295
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6452 - acc: 0.6291
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6452 - acc: 0.6292
5504/5677 [============================>.] - ETA: 15s - loss: 0.6450 - acc: 0.6286
5568/5677 [============================>.] - ETA: 9s - loss: 0.6447 - acc: 0.6282 
5632/5677 [============================>.] - ETA: 4s - loss: 0.6445 - acc: 0.6287
5677/5677 [==============================] - 535s 94ms/step - loss: 0.6445 - acc: 0.6283 - val_loss: 0.6285 - val_acc: 0.6339

Epoch 00008: val_acc did not improve from 0.63708
Epoch 9/10

  64/5677 [..............................] - ETA: 8:51 - loss: 0.5839 - acc: 0.6719
 128/5677 [..............................] - ETA: 8:27 - loss: 0.6340 - acc: 0.6641
 192/5677 [>.............................] - ETA: 8:10 - loss: 0.6198 - acc: 0.6823
 256/5677 [>.............................] - ETA: 7:55 - loss: 0.6257 - acc: 0.6875
 320/5677 [>.............................] - ETA: 7:41 - loss: 0.6273 - acc: 0.6719
 384/5677 [=>............................] - ETA: 7:29 - loss: 0.6337 - acc: 0.6510
 448/5677 [=>............................] - ETA: 7:19 - loss: 0.6343 - acc: 0.6607
 512/5677 [=>............................] - ETA: 7:08 - loss: 0.6495 - acc: 0.6406
 576/5677 [==>...........................] - ETA: 7:03 - loss: 0.6403 - acc: 0.6545
 640/5677 [==>...........................] - ETA: 6:55 - loss: 0.6401 - acc: 0.6516
 704/5677 [==>...........................] - ETA: 6:47 - loss: 0.6365 - acc: 0.6548
 768/5677 [===>..........................] - ETA: 6:39 - loss: 0.6351 - acc: 0.6589
 832/5677 [===>..........................] - ETA: 6:33 - loss: 0.6317 - acc: 0.6635
 896/5677 [===>..........................] - ETA: 6:27 - loss: 0.6359 - acc: 0.6574
 960/5677 [====>.........................] - ETA: 6:21 - loss: 0.6339 - acc: 0.6604
1024/5677 [====>.........................] - ETA: 6:16 - loss: 0.6396 - acc: 0.6514
1088/5677 [====>.........................] - ETA: 6:08 - loss: 0.6418 - acc: 0.6489
1152/5677 [=====>........................] - ETA: 6:03 - loss: 0.6405 - acc: 0.6528
1216/5677 [=====>........................] - ETA: 5:57 - loss: 0.6386 - acc: 0.6546
1280/5677 [=====>........................] - ETA: 5:55 - loss: 0.6382 - acc: 0.6562
1344/5677 [======>.......................] - ETA: 5:49 - loss: 0.6397 - acc: 0.6540
1408/5677 [======>.......................] - ETA: 5:44 - loss: 0.6377 - acc: 0.6555
1472/5677 [======>.......................] - ETA: 5:39 - loss: 0.6384 - acc: 0.6529
1536/5677 [=======>......................] - ETA: 5:35 - loss: 0.6381 - acc: 0.6504
1600/5677 [=======>......................] - ETA: 5:30 - loss: 0.6374 - acc: 0.6481
1664/5677 [=======>......................] - ETA: 5:24 - loss: 0.6384 - acc: 0.6454
1728/5677 [========>.....................] - ETA: 5:19 - loss: 0.6376 - acc: 0.6453
1792/5677 [========>.....................] - ETA: 5:12 - loss: 0.6352 - acc: 0.6468
1856/5677 [========>.....................] - ETA: 5:07 - loss: 0.6373 - acc: 0.6433
1920/5677 [=========>....................] - ETA: 5:01 - loss: 0.6374 - acc: 0.6422
1984/5677 [=========>....................] - ETA: 4:56 - loss: 0.6393 - acc: 0.6391
2048/5677 [=========>....................] - ETA: 4:50 - loss: 0.6401 - acc: 0.6387
2112/5677 [==========>...................] - ETA: 4:45 - loss: 0.6420 - acc: 0.6373
2176/5677 [==========>...................] - ETA: 4:40 - loss: 0.6421 - acc: 0.6356
2240/5677 [==========>...................] - ETA: 4:35 - loss: 0.6417 - acc: 0.6357
2304/5677 [===========>..................] - ETA: 4:31 - loss: 0.6428 - acc: 0.6337
2368/5677 [===========>..................] - ETA: 4:27 - loss: 0.6436 - acc: 0.6305
2432/5677 [===========>..................] - ETA: 4:23 - loss: 0.6454 - acc: 0.6295
2496/5677 [============>.................] - ETA: 4:18 - loss: 0.6458 - acc: 0.6298
2560/5677 [============>.................] - ETA: 4:14 - loss: 0.6466 - acc: 0.6266
2624/5677 [============>.................] - ETA: 4:09 - loss: 0.6467 - acc: 0.6250
2688/5677 [=============>................] - ETA: 4:03 - loss: 0.6456 - acc: 0.6254
2752/5677 [=============>................] - ETA: 3:58 - loss: 0.6457 - acc: 0.6250
2816/5677 [=============>................] - ETA: 3:54 - loss: 0.6471 - acc: 0.6236
2880/5677 [==============>...............] - ETA: 3:49 - loss: 0.6481 - acc: 0.6212
2944/5677 [==============>...............] - ETA: 3:45 - loss: 0.6482 - acc: 0.6216
3008/5677 [==============>...............] - ETA: 3:40 - loss: 0.6482 - acc: 0.6220
3072/5677 [===============>..............] - ETA: 3:35 - loss: 0.6481 - acc: 0.6204
3136/5677 [===============>..............] - ETA: 3:30 - loss: 0.6475 - acc: 0.6228
3200/5677 [===============>..............] - ETA: 3:25 - loss: 0.6491 - acc: 0.6206
3264/5677 [================>.............] - ETA: 3:21 - loss: 0.6497 - acc: 0.6198
3328/5677 [================>.............] - ETA: 3:15 - loss: 0.6485 - acc: 0.6205
3392/5677 [================>.............] - ETA: 3:10 - loss: 0.6490 - acc: 0.6194
3456/5677 [=================>............] - ETA: 3:05 - loss: 0.6485 - acc: 0.6215
3520/5677 [=================>............] - ETA: 3:00 - loss: 0.6487 - acc: 0.6207
3584/5677 [=================>............] - ETA: 2:55 - loss: 0.6480 - acc: 0.6217
3648/5677 [==================>...........] - ETA: 2:50 - loss: 0.6479 - acc: 0.6206
3712/5677 [==================>...........] - ETA: 2:45 - loss: 0.6482 - acc: 0.6207
3776/5677 [==================>...........] - ETA: 2:39 - loss: 0.6488 - acc: 0.6205
3840/5677 [===================>..........] - ETA: 2:34 - loss: 0.6483 - acc: 0.6203
3904/5677 [===================>..........] - ETA: 2:29 - loss: 0.6486 - acc: 0.6183
3968/5677 [===================>..........] - ETA: 2:23 - loss: 0.6481 - acc: 0.6190
4032/5677 [====================>.........] - ETA: 2:18 - loss: 0.6472 - acc: 0.6205
4096/5677 [====================>.........] - ETA: 2:12 - loss: 0.6464 - acc: 0.6213
4160/5677 [====================>.........] - ETA: 2:07 - loss: 0.6457 - acc: 0.6224
4224/5677 [=====================>........] - ETA: 2:02 - loss: 0.6445 - acc: 0.6245
4288/5677 [=====================>........] - ETA: 1:56 - loss: 0.6431 - acc: 0.6259
4352/5677 [=====================>........] - ETA: 1:51 - loss: 0.6422 - acc: 0.6275
4416/5677 [======================>.......] - ETA: 1:46 - loss: 0.6413 - acc: 0.6284
4480/5677 [======================>.......] - ETA: 1:40 - loss: 0.6420 - acc: 0.6277
4544/5677 [=======================>......] - ETA: 1:35 - loss: 0.6417 - acc: 0.6276
4608/5677 [=======================>......] - ETA: 1:30 - loss: 0.6415 - acc: 0.6276
4672/5677 [=======================>......] - ETA: 1:24 - loss: 0.6414 - acc: 0.6282
4736/5677 [========================>.....] - ETA: 1:19 - loss: 0.6411 - acc: 0.6282
4800/5677 [========================>.....] - ETA: 1:13 - loss: 0.6406 - acc: 0.6273
4864/5677 [========================>.....] - ETA: 1:08 - loss: 0.6402 - acc: 0.6285
4928/5677 [=========================>....] - ETA: 1:03 - loss: 0.6402 - acc: 0.6293
4992/5677 [=========================>....] - ETA: 57s - loss: 0.6393 - acc: 0.6298 
5056/5677 [=========================>....] - ETA: 52s - loss: 0.6396 - acc: 0.6288
5120/5677 [==========================>...] - ETA: 46s - loss: 0.6396 - acc: 0.6287
5184/5677 [==========================>...] - ETA: 41s - loss: 0.6398 - acc: 0.6273
5248/5677 [==========================>...] - ETA: 36s - loss: 0.6401 - acc: 0.6265
5312/5677 [===========================>..] - ETA: 30s - loss: 0.6405 - acc: 0.6258
5376/5677 [===========================>..] - ETA: 25s - loss: 0.6403 - acc: 0.6261
5440/5677 [===========================>..] - ETA: 20s - loss: 0.6393 - acc: 0.6268
5504/5677 [============================>.] - ETA: 14s - loss: 0.6402 - acc: 0.6263
5568/5677 [============================>.] - ETA: 9s - loss: 0.6405 - acc: 0.6259 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6401 - acc: 0.6262
5677/5677 [==============================] - 506s 89ms/step - loss: 0.6402 - acc: 0.6260 - val_loss: 0.6313 - val_acc: 0.6355

Epoch 00009: val_acc did not improve from 0.63708
Epoch 10/10

  64/5677 [..............................] - ETA: 9:08 - loss: 0.6869 - acc: 0.6719
 128/5677 [..............................] - ETA: 9:11 - loss: 0.6459 - acc: 0.6719
 192/5677 [>.............................] - ETA: 9:02 - loss: 0.6371 - acc: 0.6823
 256/5677 [>.............................] - ETA: 9:01 - loss: 0.6403 - acc: 0.6523
 320/5677 [>.............................] - ETA: 8:42 - loss: 0.6354 - acc: 0.6625
 384/5677 [=>............................] - ETA: 8:37 - loss: 0.6382 - acc: 0.6641
 448/5677 [=>............................] - ETA: 8:25 - loss: 0.6374 - acc: 0.6518
 512/5677 [=>............................] - ETA: 8:21 - loss: 0.6439 - acc: 0.6504
 576/5677 [==>...........................] - ETA: 8:13 - loss: 0.6429 - acc: 0.6476
 640/5677 [==>...........................] - ETA: 8:06 - loss: 0.6392 - acc: 0.6484
 704/5677 [==>...........................] - ETA: 7:56 - loss: 0.6394 - acc: 0.6449
 768/5677 [===>..........................] - ETA: 7:53 - loss: 0.6400 - acc: 0.6432
 832/5677 [===>..........................] - ETA: 7:54 - loss: 0.6369 - acc: 0.6430
 896/5677 [===>..........................] - ETA: 7:48 - loss: 0.6370 - acc: 0.6451
 960/5677 [====>.........................] - ETA: 7:44 - loss: 0.6407 - acc: 0.6375
1024/5677 [====>.........................] - ETA: 7:38 - loss: 0.6443 - acc: 0.6318
1088/5677 [====>.........................] - ETA: 7:38 - loss: 0.6439 - acc: 0.6314
1152/5677 [=====>........................] - ETA: 7:32 - loss: 0.6456 - acc: 0.6267
1216/5677 [=====>........................] - ETA: 7:26 - loss: 0.6444 - acc: 0.6258
1280/5677 [=====>........................] - ETA: 7:21 - loss: 0.6450 - acc: 0.6258
1344/5677 [======>.......................] - ETA: 7:12 - loss: 0.6442 - acc: 0.6265
1408/5677 [======>.......................] - ETA: 7:09 - loss: 0.6448 - acc: 0.6278
1472/5677 [======>.......................] - ETA: 7:05 - loss: 0.6447 - acc: 0.6250
1536/5677 [=======>......................] - ETA: 6:58 - loss: 0.6442 - acc: 0.6243
1600/5677 [=======>......................] - ETA: 6:50 - loss: 0.6427 - acc: 0.6250
1664/5677 [=======>......................] - ETA: 6:46 - loss: 0.6436 - acc: 0.6256
1728/5677 [========>.....................] - ETA: 6:41 - loss: 0.6439 - acc: 0.6262
1792/5677 [========>.....................] - ETA: 6:33 - loss: 0.6428 - acc: 0.6295
1856/5677 [========>.....................] - ETA: 6:26 - loss: 0.6418 - acc: 0.6298
1920/5677 [=========>....................] - ETA: 6:19 - loss: 0.6412 - acc: 0.6307
1984/5677 [=========>....................] - ETA: 6:16 - loss: 0.6408 - acc: 0.6316
2048/5677 [=========>....................] - ETA: 6:11 - loss: 0.6401 - acc: 0.6323
2112/5677 [==========>...................] - ETA: 6:04 - loss: 0.6406 - acc: 0.6297
2176/5677 [==========>...................] - ETA: 5:57 - loss: 0.6396 - acc: 0.6319
2240/5677 [==========>...................] - ETA: 5:49 - loss: 0.6378 - acc: 0.6339
2304/5677 [===========>..................] - ETA: 5:42 - loss: 0.6379 - acc: 0.6354
2368/5677 [===========>..................] - ETA: 5:37 - loss: 0.6386 - acc: 0.6334
2432/5677 [===========>..................] - ETA: 5:32 - loss: 0.6389 - acc: 0.6324
2496/5677 [============>.................] - ETA: 5:26 - loss: 0.6398 - acc: 0.6314
2560/5677 [============>.................] - ETA: 5:21 - loss: 0.6415 - acc: 0.6301
2624/5677 [============>.................] - ETA: 5:14 - loss: 0.6402 - acc: 0.6319
2688/5677 [=============>................] - ETA: 5:07 - loss: 0.6403 - acc: 0.6317
2752/5677 [=============>................] - ETA: 5:01 - loss: 0.6400 - acc: 0.6330
2816/5677 [=============>................] - ETA: 4:54 - loss: 0.6399 - acc: 0.6332
2880/5677 [==============>...............] - ETA: 4:49 - loss: 0.6390 - acc: 0.6340
2944/5677 [==============>...............] - ETA: 4:44 - loss: 0.6368 - acc: 0.6359
3008/5677 [==============>...............] - ETA: 4:39 - loss: 0.6346 - acc: 0.6376
3072/5677 [===============>..............] - ETA: 4:33 - loss: 0.6357 - acc: 0.6367
3136/5677 [===============>..............] - ETA: 4:26 - loss: 0.6344 - acc: 0.6378
3200/5677 [===============>..............] - ETA: 4:19 - loss: 0.6359 - acc: 0.6375
3264/5677 [================>.............] - ETA: 4:12 - loss: 0.6352 - acc: 0.6382
3328/5677 [================>.............] - ETA: 4:06 - loss: 0.6368 - acc: 0.6358
3392/5677 [================>.............] - ETA: 4:01 - loss: 0.6366 - acc: 0.6350
3456/5677 [=================>............] - ETA: 3:55 - loss: 0.6364 - acc: 0.6345
3520/5677 [=================>............] - ETA: 3:49 - loss: 0.6362 - acc: 0.6358
3584/5677 [=================>............] - ETA: 3:42 - loss: 0.6357 - acc: 0.6362
3648/5677 [==================>...........] - ETA: 3:35 - loss: 0.6364 - acc: 0.6351
3712/5677 [==================>...........] - ETA: 3:29 - loss: 0.6357 - acc: 0.6363
3776/5677 [==================>...........] - ETA: 3:22 - loss: 0.6348 - acc: 0.6382
3840/5677 [===================>..........] - ETA: 3:15 - loss: 0.6342 - acc: 0.6393
3904/5677 [===================>..........] - ETA: 3:09 - loss: 0.6355 - acc: 0.6373
3968/5677 [===================>..........] - ETA: 3:03 - loss: 0.6360 - acc: 0.6361
4032/5677 [====================>.........] - ETA: 2:56 - loss: 0.6369 - acc: 0.6349
4096/5677 [====================>.........] - ETA: 2:50 - loss: 0.6378 - acc: 0.6340
4160/5677 [====================>.........] - ETA: 2:43 - loss: 0.6370 - acc: 0.6344
4224/5677 [=====================>........] - ETA: 2:36 - loss: 0.6374 - acc: 0.6335
4288/5677 [=====================>........] - ETA: 2:29 - loss: 0.6374 - acc: 0.6341
4352/5677 [=====================>........] - ETA: 2:22 - loss: 0.6369 - acc: 0.6349
4416/5677 [======================>.......] - ETA: 2:15 - loss: 0.6373 - acc: 0.6336
4480/5677 [======================>.......] - ETA: 2:09 - loss: 0.6380 - acc: 0.6335
4544/5677 [=======================>......] - ETA: 2:02 - loss: 0.6372 - acc: 0.6340
4608/5677 [=======================>......] - ETA: 1:55 - loss: 0.6380 - acc: 0.6332
4672/5677 [=======================>......] - ETA: 1:49 - loss: 0.6375 - acc: 0.6348
4736/5677 [========================>.....] - ETA: 1:42 - loss: 0.6370 - acc: 0.6353
4800/5677 [========================>.....] - ETA: 1:35 - loss: 0.6374 - acc: 0.6350
4864/5677 [========================>.....] - ETA: 1:28 - loss: 0.6374 - acc: 0.6351
4928/5677 [=========================>....] - ETA: 1:22 - loss: 0.6364 - acc: 0.6368
4992/5677 [=========================>....] - ETA: 1:15 - loss: 0.6370 - acc: 0.6364
5056/5677 [=========================>....] - ETA: 1:08 - loss: 0.6370 - acc: 0.6365
5120/5677 [==========================>...] - ETA: 1:01 - loss: 0.6367 - acc: 0.6367
5184/5677 [==========================>...] - ETA: 54s - loss: 0.6360 - acc: 0.6372 
5248/5677 [==========================>...] - ETA: 47s - loss: 0.6367 - acc: 0.6355
5312/5677 [===========================>..] - ETA: 40s - loss: 0.6363 - acc: 0.6355
5376/5677 [===========================>..] - ETA: 33s - loss: 0.6369 - acc: 0.6347
5440/5677 [===========================>..] - ETA: 26s - loss: 0.6367 - acc: 0.6344
5504/5677 [============================>.] - ETA: 19s - loss: 0.6373 - acc: 0.6337
5568/5677 [============================>.] - ETA: 12s - loss: 0.6364 - acc: 0.6351
5632/5677 [============================>.] - ETA: 5s - loss: 0.6365 - acc: 0.6351 
5677/5677 [==============================] - 657s 116ms/step - loss: 0.6364 - acc: 0.6354 - val_loss: 0.6391 - val_acc: 0.6450

Epoch 00010: val_acc improved from 0.63708 to 0.64501, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window11/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa62b17c350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa62b17c350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa622f45550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa622f45550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622bb7110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622bb7110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa622d28550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa622d28550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa622c9c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa622c9c390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62297f490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62297f490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622bba090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622bba090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a3784b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a3784b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa622a4e5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa622a4e5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa622a1cdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa622a1cdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6229d0790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6229d0790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622a4ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622a4ec50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622ac17d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622ac17d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa58925de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa58925de90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa62259de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa62259de90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62266c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62266c710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6227139d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6227139d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6228c4450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6228c4450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa622399150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa622399150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa622584fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa622584fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6225f32d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6225f32d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622597110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622597110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62226f250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62226f250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa622079510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa622079510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa62202c350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa62202c350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62218ac90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62218ac90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622079310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622079310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa611f48650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa611f48650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa62257cf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa62257cf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa611d30510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa611d30510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa611f84f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa611f84f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622092b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622092b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa611be9cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa611be9cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa611af17d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa611af17d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa611915fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa611915fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622d285d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622d285d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa611d1a0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa611d1a0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa611813ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa611813ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa611beb950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa611beb950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa611948790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa611948790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa609709110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa609709110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6119d7cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6119d7cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa609677e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa609677e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa6096039d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa6096039d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60932f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60932f5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6093ae990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6093ae990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6095eb8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6095eb8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6093aecd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6093aecd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60932f950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60932f950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa6093ac850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa6093ac850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6090fb750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6090fb750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa609067210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa609067210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60902a410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60902a410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa600f77110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa600f77110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa600cfe990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa600cfe990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa600f74c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa600f74c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa05066bf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa05066bf50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa600b6ce10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa600b6ce10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa600de3ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa600de3ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa600c67410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa600c67410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa600c9c610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa600c9c610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa600a6d810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa600a6d810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa600c9c090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa600c9c090>>: AttributeError: module 'gast' has no attribute 'Str'
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 2:27
 128/1578 [=>............................] - ETA: 1:36
 192/1578 [==>...........................] - ETA: 1:18
 256/1578 [===>..........................] - ETA: 1:08
 320/1578 [=====>........................] - ETA: 1:00
 384/1578 [======>.......................] - ETA: 54s 
 448/1578 [=======>......................] - ETA: 49s
 512/1578 [========>.....................] - ETA: 45s
 576/1578 [=========>....................] - ETA: 41s
 640/1578 [===========>..................] - ETA: 38s
 704/1578 [============>.................] - ETA: 34s
 768/1578 [=============>................] - ETA: 31s
 832/1578 [==============>...............] - ETA: 28s
 896/1578 [================>.............] - ETA: 25s
 960/1578 [=================>............] - ETA: 23s
1024/1578 [==================>...........] - ETA: 20s
1088/1578 [===================>..........] - ETA: 17s
1152/1578 [====================>.........] - ETA: 15s
1216/1578 [======================>.......] - ETA: 13s
1280/1578 [=======================>......] - ETA: 10s
1344/1578 [========================>.....] - ETA: 8s 
1408/1578 [=========================>....] - ETA: 5s
1472/1578 [==========================>...] - ETA: 3s
1536/1578 [============================>.] - ETA: 1s
1578/1578 [==============================] - 55s 35ms/step
loss: 0.6379508416915575
acc: 0.6261089991858402
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa0506f1e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa0506f1e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa05066b190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa05066b190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622bf6110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622bf6110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fb02a7410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fb02a7410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5cd796a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5cd796a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5ef8e5b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5ef8e5b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622df02d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa622df02d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622d37910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622d37910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa62b04cd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa62b04cd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa622eb8a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa622eb8a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62b01c990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62b01c990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa62b04ce90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa62b04ce90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62b0fcb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa62b0fcb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa05055a110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa05055a110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa0504c3c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa0504c3c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591b0ee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa591b0ee10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa050606d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa050606d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6093c4f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6093c4f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa0500c7650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa0500c7650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa0502ba8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa0502ba8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa050604a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa050604a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa0107e2ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa0107e2ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa01068b2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa01068b2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa0500c7c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa0500c7c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa0105a0550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa0105a0550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa01051b0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa01051b0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa050163f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa050163f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa01048de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa01048de90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa0105ed450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa0105ed450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa010230050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa010230050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa01005cc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa01005cc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa0105ed090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa0105ed090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa0102b7f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa0102b7f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fe0768310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fe0768310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9fe06699d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9fe06699d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa010230cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa010230cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fe0768750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fe0768750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa010492590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa010492590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fe0408ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fe0408ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9fe03cc350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9fe03cc350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fe03c20d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fe03c20d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fe0408350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fe0408350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fe01c4610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fe01c4610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fe00e6b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fe00e6b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9fb07b5590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9fb07b5590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa05042d110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa05042d110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fe019cd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fe019cd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fb077b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fb077b1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fe0127550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fe0127550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9fb0785f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9fb0785f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fb06b7890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fb06b7890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fe0058610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fe0058610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fb02ff510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fb02ff510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fe01b9910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9fe01b9910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e8403de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e8403de10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e84194910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e84194910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fb05a14d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fb05a14d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e841f7410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e841f7410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e3c6bfe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e3c6bfe50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e3c5dda50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e3c5dda50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e3c5625d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e3c5625d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e3c6bf950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e3c6bf950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e3c642ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e3c642ad0>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 38:11 - loss: 0.8667 - acc: 0.4062
 128/5677 [..............................] - ETA: 22:54 - loss: 0.7668 - acc: 0.4922
 192/5677 [>.............................] - ETA: 17:53 - loss: 0.7666 - acc: 0.5052
 256/5677 [>.............................] - ETA: 15:10 - loss: 0.7574 - acc: 0.5039
 320/5677 [>.............................] - ETA: 13:30 - loss: 0.7586 - acc: 0.4969
 384/5677 [=>............................] - ETA: 12:27 - loss: 0.7589 - acc: 0.5026
 448/5677 [=>............................] - ETA: 11:42 - loss: 0.7550 - acc: 0.5089
 512/5677 [=>............................] - ETA: 11:07 - loss: 0.7571 - acc: 0.5020
 576/5677 [==>...........................] - ETA: 10:41 - loss: 0.7559 - acc: 0.4965
 640/5677 [==>...........................] - ETA: 10:09 - loss: 0.7513 - acc: 0.4969
 704/5677 [==>...........................] - ETA: 9:47 - loss: 0.7519 - acc: 0.4943 
 768/5677 [===>..........................] - ETA: 9:27 - loss: 0.7497 - acc: 0.5000
 832/5677 [===>..........................] - ETA: 9:08 - loss: 0.7524 - acc: 0.4928
 896/5677 [===>..........................] - ETA: 8:55 - loss: 0.7496 - acc: 0.4922
 960/5677 [====>.........................] - ETA: 8:41 - loss: 0.7465 - acc: 0.4948
1024/5677 [====>.........................] - ETA: 8:26 - loss: 0.7490 - acc: 0.4951
1088/5677 [====>.........................] - ETA: 8:14 - loss: 0.7490 - acc: 0.4954
1152/5677 [=====>........................] - ETA: 8:04 - loss: 0.7469 - acc: 0.4957
1216/5677 [=====>........................] - ETA: 7:53 - loss: 0.7466 - acc: 0.4942
1280/5677 [=====>........................] - ETA: 7:43 - loss: 0.7507 - acc: 0.4922
1344/5677 [======>.......................] - ETA: 7:34 - loss: 0.7493 - acc: 0.4948
1408/5677 [======>.......................] - ETA: 7:22 - loss: 0.7466 - acc: 0.4986
1472/5677 [======>.......................] - ETA: 7:12 - loss: 0.7420 - acc: 0.5054
1536/5677 [=======>......................] - ETA: 7:03 - loss: 0.7419 - acc: 0.5026
1600/5677 [=======>......................] - ETA: 6:54 - loss: 0.7423 - acc: 0.5031
1664/5677 [=======>......................] - ETA: 6:46 - loss: 0.7413 - acc: 0.5018
1728/5677 [========>.....................] - ETA: 6:39 - loss: 0.7404 - acc: 0.5012
1792/5677 [========>.....................] - ETA: 6:30 - loss: 0.7388 - acc: 0.5045
1856/5677 [========>.....................] - ETA: 6:22 - loss: 0.7384 - acc: 0.5059
1920/5677 [=========>....................] - ETA: 6:13 - loss: 0.7357 - acc: 0.5083
1984/5677 [=========>....................] - ETA: 6:06 - loss: 0.7337 - acc: 0.5116
2048/5677 [=========>....................] - ETA: 5:58 - loss: 0.7345 - acc: 0.5088
2112/5677 [==========>...................] - ETA: 5:50 - loss: 0.7330 - acc: 0.5099
2176/5677 [==========>...................] - ETA: 5:42 - loss: 0.7321 - acc: 0.5110
2240/5677 [==========>...................] - ETA: 5:35 - loss: 0.7300 - acc: 0.5125
2304/5677 [===========>..................] - ETA: 5:28 - loss: 0.7300 - acc: 0.5087
2368/5677 [===========>..................] - ETA: 5:20 - loss: 0.7292 - acc: 0.5093
2432/5677 [===========>..................] - ETA: 5:13 - loss: 0.7291 - acc: 0.5099
2496/5677 [============>.................] - ETA: 5:06 - loss: 0.7294 - acc: 0.5092
2560/5677 [============>.................] - ETA: 4:59 - loss: 0.7283 - acc: 0.5098
2624/5677 [============>.................] - ETA: 4:52 - loss: 0.7272 - acc: 0.5103
2688/5677 [=============>................] - ETA: 4:46 - loss: 0.7257 - acc: 0.5130
2752/5677 [=============>................] - ETA: 4:39 - loss: 0.7248 - acc: 0.5142
2816/5677 [=============>................] - ETA: 4:33 - loss: 0.7246 - acc: 0.5124
2880/5677 [==============>...............] - ETA: 4:26 - loss: 0.7241 - acc: 0.5125
2944/5677 [==============>...............] - ETA: 4:20 - loss: 0.7238 - acc: 0.5139
3008/5677 [==============>...............] - ETA: 4:14 - loss: 0.7232 - acc: 0.5126
3072/5677 [===============>..............] - ETA: 4:07 - loss: 0.7226 - acc: 0.5127
3136/5677 [===============>..............] - ETA: 4:00 - loss: 0.7218 - acc: 0.5134
3200/5677 [===============>..............] - ETA: 3:54 - loss: 0.7206 - acc: 0.5144
3264/5677 [================>.............] - ETA: 3:48 - loss: 0.7205 - acc: 0.5132
3328/5677 [================>.............] - ETA: 3:42 - loss: 0.7201 - acc: 0.5129
3392/5677 [================>.............] - ETA: 3:35 - loss: 0.7201 - acc: 0.5118
3456/5677 [=================>............] - ETA: 3:29 - loss: 0.7193 - acc: 0.5136
3520/5677 [=================>............] - ETA: 3:23 - loss: 0.7182 - acc: 0.5151
3584/5677 [=================>............] - ETA: 3:16 - loss: 0.7174 - acc: 0.5156
3648/5677 [==================>...........] - ETA: 3:10 - loss: 0.7169 - acc: 0.5162
3712/5677 [==================>...........] - ETA: 3:04 - loss: 0.7172 - acc: 0.5164
3776/5677 [==================>...........] - ETA: 2:58 - loss: 0.7171 - acc: 0.5169
3840/5677 [===================>..........] - ETA: 2:52 - loss: 0.7171 - acc: 0.5167
3904/5677 [===================>..........] - ETA: 2:46 - loss: 0.7173 - acc: 0.5164
3968/5677 [===================>..........] - ETA: 2:39 - loss: 0.7174 - acc: 0.5171
4032/5677 [====================>.........] - ETA: 2:33 - loss: 0.7169 - acc: 0.5179
4096/5677 [====================>.........] - ETA: 2:27 - loss: 0.7170 - acc: 0.5168
4160/5677 [====================>.........] - ETA: 2:21 - loss: 0.7166 - acc: 0.5175
4224/5677 [=====================>........] - ETA: 2:15 - loss: 0.7164 - acc: 0.5175
4288/5677 [=====================>........] - ETA: 2:09 - loss: 0.7158 - acc: 0.5187
4352/5677 [=====================>........] - ETA: 2:03 - loss: 0.7157 - acc: 0.5186
4416/5677 [======================>.......] - ETA: 1:57 - loss: 0.7152 - acc: 0.5190
4480/5677 [======================>.......] - ETA: 1:51 - loss: 0.7156 - acc: 0.5174
4544/5677 [=======================>......] - ETA: 1:45 - loss: 0.7154 - acc: 0.5176
4608/5677 [=======================>......] - ETA: 1:39 - loss: 0.7151 - acc: 0.5178
4672/5677 [=======================>......] - ETA: 1:33 - loss: 0.7148 - acc: 0.5182
4736/5677 [========================>.....] - ETA: 1:27 - loss: 0.7143 - acc: 0.5177
4800/5677 [========================>.....] - ETA: 1:21 - loss: 0.7145 - acc: 0.5179
4864/5677 [========================>.....] - ETA: 1:15 - loss: 0.7139 - acc: 0.5189
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.7136 - acc: 0.5195
4992/5677 [=========================>....] - ETA: 1:04 - loss: 0.7133 - acc: 0.5202
5056/5677 [=========================>....] - ETA: 58s - loss: 0.7129 - acc: 0.5204 
5120/5677 [==========================>...] - ETA: 52s - loss: 0.7135 - acc: 0.5189
5184/5677 [==========================>...] - ETA: 46s - loss: 0.7132 - acc: 0.5185
5248/5677 [==========================>...] - ETA: 40s - loss: 0.7130 - acc: 0.5181
5312/5677 [===========================>..] - ETA: 34s - loss: 0.7135 - acc: 0.5169
5376/5677 [===========================>..] - ETA: 28s - loss: 0.7130 - acc: 0.5173
5440/5677 [===========================>..] - ETA: 22s - loss: 0.7124 - acc: 0.5175
5504/5677 [============================>.] - ETA: 16s - loss: 0.7122 - acc: 0.5184
5568/5677 [============================>.] - ETA: 10s - loss: 0.7116 - acc: 0.5189
5632/5677 [============================>.] - ETA: 4s - loss: 0.7112 - acc: 0.5197 
5677/5677 [==============================] - 558s 98ms/step - loss: 0.7111 - acc: 0.5200 - val_loss: 0.6842 - val_acc: 0.5420

Epoch 00001: val_acc improved from -inf to 0.54200, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window12/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 8:24 - loss: 0.7127 - acc: 0.4844
 128/5677 [..............................] - ETA: 9:18 - loss: 0.7091 - acc: 0.4844
 192/5677 [>.............................] - ETA: 9:35 - loss: 0.7193 - acc: 0.4896
 256/5677 [>.............................] - ETA: 9:22 - loss: 0.7084 - acc: 0.5117
 320/5677 [>.............................] - ETA: 9:11 - loss: 0.7021 - acc: 0.5219
 384/5677 [=>............................] - ETA: 8:49 - loss: 0.7027 - acc: 0.5182
 448/5677 [=>............................] - ETA: 8:33 - loss: 0.7010 - acc: 0.5223
 512/5677 [=>............................] - ETA: 8:22 - loss: 0.7014 - acc: 0.5234
 576/5677 [==>...........................] - ETA: 8:11 - loss: 0.6979 - acc: 0.5399
 640/5677 [==>...........................] - ETA: 8:11 - loss: 0.6999 - acc: 0.5359
 704/5677 [==>...........................] - ETA: 8:08 - loss: 0.7033 - acc: 0.5312
 768/5677 [===>..........................] - ETA: 8:04 - loss: 0.6975 - acc: 0.5456
 832/5677 [===>..........................] - ETA: 8:00 - loss: 0.6942 - acc: 0.5541
 896/5677 [===>..........................] - ETA: 7:50 - loss: 0.6943 - acc: 0.5525
 960/5677 [====>.........................] - ETA: 7:39 - loss: 0.6950 - acc: 0.5510
1024/5677 [====>.........................] - ETA: 7:32 - loss: 0.6945 - acc: 0.5488
1088/5677 [====>.........................] - ETA: 7:24 - loss: 0.6959 - acc: 0.5450
1152/5677 [=====>........................] - ETA: 7:18 - loss: 0.6988 - acc: 0.5382
1216/5677 [=====>........................] - ETA: 7:16 - loss: 0.6977 - acc: 0.5387
1280/5677 [=====>........................] - ETA: 7:11 - loss: 0.6976 - acc: 0.5391
1344/5677 [======>.......................] - ETA: 7:06 - loss: 0.6991 - acc: 0.5342
1408/5677 [======>.......................] - ETA: 6:59 - loss: 0.6987 - acc: 0.5412
1472/5677 [======>.......................] - ETA: 6:50 - loss: 0.6964 - acc: 0.5442
1536/5677 [=======>......................] - ETA: 6:42 - loss: 0.6976 - acc: 0.5436
1600/5677 [=======>......................] - ETA: 6:34 - loss: 0.6972 - acc: 0.5469
1664/5677 [=======>......................] - ETA: 6:25 - loss: 0.6982 - acc: 0.5445
1728/5677 [========>.....................] - ETA: 6:18 - loss: 0.6977 - acc: 0.5440
1792/5677 [========>.....................] - ETA: 6:14 - loss: 0.6988 - acc: 0.5419
1856/5677 [========>.....................] - ETA: 6:09 - loss: 0.6991 - acc: 0.5409
1920/5677 [=========>....................] - ETA: 6:05 - loss: 0.6987 - acc: 0.5427
1984/5677 [=========>....................] - ETA: 5:58 - loss: 0.6982 - acc: 0.5444
2048/5677 [=========>....................] - ETA: 5:52 - loss: 0.6974 - acc: 0.5464
2112/5677 [==========>...................] - ETA: 5:47 - loss: 0.6980 - acc: 0.5440
2176/5677 [==========>...................] - ETA: 5:41 - loss: 0.6984 - acc: 0.5418
2240/5677 [==========>...................] - ETA: 5:33 - loss: 0.6974 - acc: 0.5437
2304/5677 [===========>..................] - ETA: 5:26 - loss: 0.6978 - acc: 0.5434
2368/5677 [===========>..................] - ETA: 5:19 - loss: 0.6969 - acc: 0.5435
2432/5677 [===========>..................] - ETA: 5:12 - loss: 0.6964 - acc: 0.5448
2496/5677 [============>.................] - ETA: 5:05 - loss: 0.6961 - acc: 0.5445
2560/5677 [============>.................] - ETA: 5:00 - loss: 0.6945 - acc: 0.5473
2624/5677 [============>.................] - ETA: 4:55 - loss: 0.6942 - acc: 0.5484
2688/5677 [=============>................] - ETA: 4:49 - loss: 0.6935 - acc: 0.5487
2752/5677 [=============>................] - ETA: 4:43 - loss: 0.6938 - acc: 0.5483
2816/5677 [=============>................] - ETA: 4:36 - loss: 0.6941 - acc: 0.5483
2880/5677 [==============>...............] - ETA: 4:30 - loss: 0.6947 - acc: 0.5472
2944/5677 [==============>...............] - ETA: 4:24 - loss: 0.6947 - acc: 0.5479
3008/5677 [==============>...............] - ETA: 4:17 - loss: 0.6943 - acc: 0.5482
3072/5677 [===============>..............] - ETA: 4:11 - loss: 0.6941 - acc: 0.5495
3136/5677 [===============>..............] - ETA: 4:04 - loss: 0.6938 - acc: 0.5501
3200/5677 [===============>..............] - ETA: 3:58 - loss: 0.6939 - acc: 0.5497
3264/5677 [================>.............] - ETA: 3:51 - loss: 0.6948 - acc: 0.5475
3328/5677 [================>.............] - ETA: 3:46 - loss: 0.6940 - acc: 0.5484
3392/5677 [================>.............] - ETA: 3:40 - loss: 0.6934 - acc: 0.5486
3456/5677 [=================>............] - ETA: 3:34 - loss: 0.6937 - acc: 0.5483
3520/5677 [=================>............] - ETA: 3:28 - loss: 0.6940 - acc: 0.5486
3584/5677 [=================>............] - ETA: 3:22 - loss: 0.6947 - acc: 0.5466
3648/5677 [==================>...........] - ETA: 3:17 - loss: 0.6948 - acc: 0.5466
3712/5677 [==================>...........] - ETA: 3:11 - loss: 0.6953 - acc: 0.5450
3776/5677 [==================>...........] - ETA: 3:05 - loss: 0.6951 - acc: 0.5461
3840/5677 [===================>..........] - ETA: 2:59 - loss: 0.6948 - acc: 0.5466
3904/5677 [===================>..........] - ETA: 2:53 - loss: 0.6950 - acc: 0.5461
3968/5677 [===================>..........] - ETA: 2:47 - loss: 0.6950 - acc: 0.5459
4032/5677 [====================>.........] - ETA: 2:41 - loss: 0.6942 - acc: 0.5469
4096/5677 [====================>.........] - ETA: 2:35 - loss: 0.6942 - acc: 0.5474
4160/5677 [====================>.........] - ETA: 2:29 - loss: 0.6953 - acc: 0.5469
4224/5677 [=====================>........] - ETA: 2:23 - loss: 0.6947 - acc: 0.5469
4288/5677 [=====================>........] - ETA: 2:17 - loss: 0.6947 - acc: 0.5473
4352/5677 [=====================>........] - ETA: 2:11 - loss: 0.6947 - acc: 0.5476
4416/5677 [======================>.......] - ETA: 2:04 - loss: 0.6947 - acc: 0.5476
4480/5677 [======================>.......] - ETA: 1:58 - loss: 0.6945 - acc: 0.5487
4544/5677 [=======================>......] - ETA: 1:52 - loss: 0.6943 - acc: 0.5489
4608/5677 [=======================>......] - ETA: 1:46 - loss: 0.6947 - acc: 0.5480
4672/5677 [=======================>......] - ETA: 1:40 - loss: 0.6951 - acc: 0.5469
4736/5677 [========================>.....] - ETA: 1:33 - loss: 0.6952 - acc: 0.5471
4800/5677 [========================>.....] - ETA: 1:27 - loss: 0.6949 - acc: 0.5475
4864/5677 [========================>.....] - ETA: 1:21 - loss: 0.6952 - acc: 0.5469
4928/5677 [=========================>....] - ETA: 1:14 - loss: 0.6950 - acc: 0.5469
4992/5677 [=========================>....] - ETA: 1:08 - loss: 0.6955 - acc: 0.5457
5056/5677 [=========================>....] - ETA: 1:01 - loss: 0.6953 - acc: 0.5455
5120/5677 [==========================>...] - ETA: 55s - loss: 0.6952 - acc: 0.5465 
5184/5677 [==========================>...] - ETA: 48s - loss: 0.6955 - acc: 0.5461
5248/5677 [==========================>...] - ETA: 42s - loss: 0.6949 - acc: 0.5473
5312/5677 [===========================>..] - ETA: 36s - loss: 0.6949 - acc: 0.5476
5376/5677 [===========================>..] - ETA: 29s - loss: 0.6948 - acc: 0.5474
5440/5677 [===========================>..] - ETA: 23s - loss: 0.6946 - acc: 0.5476
5504/5677 [============================>.] - ETA: 17s - loss: 0.6949 - acc: 0.5463
5568/5677 [============================>.] - ETA: 10s - loss: 0.6953 - acc: 0.5460
5632/5677 [============================>.] - ETA: 4s - loss: 0.6954 - acc: 0.5462 
5677/5677 [==============================] - 584s 103ms/step - loss: 0.6953 - acc: 0.5462 - val_loss: 0.6769 - val_acc: 0.5610

Epoch 00002: val_acc improved from 0.54200 to 0.56101, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window12/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 9:41 - loss: 0.6494 - acc: 0.7188
 128/5677 [..............................] - ETA: 9:37 - loss: 0.6735 - acc: 0.6719
 192/5677 [>.............................] - ETA: 9:23 - loss: 0.6810 - acc: 0.6302
 256/5677 [>.............................] - ETA: 9:18 - loss: 0.6693 - acc: 0.6211
 320/5677 [>.............................] - ETA: 9:05 - loss: 0.6683 - acc: 0.6094
 384/5677 [=>............................] - ETA: 8:47 - loss: 0.6717 - acc: 0.6042
 448/5677 [=>............................] - ETA: 8:31 - loss: 0.6793 - acc: 0.5826
 512/5677 [=>............................] - ETA: 8:16 - loss: 0.6809 - acc: 0.5781
 576/5677 [==>...........................] - ETA: 8:05 - loss: 0.6857 - acc: 0.5642
 640/5677 [==>...........................] - ETA: 8:08 - loss: 0.6889 - acc: 0.5578
 704/5677 [==>...........................] - ETA: 8:11 - loss: 0.6888 - acc: 0.5497
 768/5677 [===>..........................] - ETA: 8:08 - loss: 0.6883 - acc: 0.5469
 832/5677 [===>..........................] - ETA: 8:02 - loss: 0.6913 - acc: 0.5397
 896/5677 [===>..........................] - ETA: 7:55 - loss: 0.6914 - acc: 0.5391
 960/5677 [====>.........................] - ETA: 7:45 - loss: 0.6899 - acc: 0.5406
1024/5677 [====>.........................] - ETA: 7:36 - loss: 0.6885 - acc: 0.5439
1088/5677 [====>.........................] - ETA: 7:26 - loss: 0.6880 - acc: 0.5460
1152/5677 [=====>........................] - ETA: 7:17 - loss: 0.6888 - acc: 0.5469
1216/5677 [=====>........................] - ETA: 7:09 - loss: 0.6886 - acc: 0.5493
1280/5677 [=====>........................] - ETA: 7:00 - loss: 0.6880 - acc: 0.5531
1344/5677 [======>.......................] - ETA: 6:57 - loss: 0.6883 - acc: 0.5536
1408/5677 [======>.......................] - ETA: 6:54 - loss: 0.6884 - acc: 0.5540
1472/5677 [======>.......................] - ETA: 6:50 - loss: 0.6884 - acc: 0.5543
1536/5677 [=======>......................] - ETA: 6:44 - loss: 0.6887 - acc: 0.5553
1600/5677 [=======>......................] - ETA: 6:40 - loss: 0.6876 - acc: 0.5556
1664/5677 [=======>......................] - ETA: 6:32 - loss: 0.6878 - acc: 0.5535
1728/5677 [========>.....................] - ETA: 6:25 - loss: 0.6863 - acc: 0.5544
1792/5677 [========>.....................] - ETA: 6:18 - loss: 0.6865 - acc: 0.5547
1856/5677 [========>.....................] - ETA: 6:10 - loss: 0.6864 - acc: 0.5550
1920/5677 [=========>....................] - ETA: 6:03 - loss: 0.6866 - acc: 0.5536
1984/5677 [=========>....................] - ETA: 5:58 - loss: 0.6855 - acc: 0.5565
2048/5677 [=========>....................] - ETA: 5:54 - loss: 0.6852 - acc: 0.5571
2112/5677 [==========>...................] - ETA: 5:48 - loss: 0.6852 - acc: 0.5559
2176/5677 [==========>...................] - ETA: 5:42 - loss: 0.6857 - acc: 0.5556
2240/5677 [==========>...................] - ETA: 5:36 - loss: 0.6852 - acc: 0.5567
2304/5677 [===========>..................] - ETA: 5:29 - loss: 0.6851 - acc: 0.5573
2368/5677 [===========>..................] - ETA: 5:22 - loss: 0.6862 - acc: 0.5553
2432/5677 [===========>..................] - ETA: 5:14 - loss: 0.6859 - acc: 0.5547
2496/5677 [============>.................] - ETA: 5:07 - loss: 0.6856 - acc: 0.5553
2560/5677 [============>.................] - ETA: 4:59 - loss: 0.6858 - acc: 0.5555
2624/5677 [============>.................] - ETA: 4:52 - loss: 0.6857 - acc: 0.5568
2688/5677 [=============>................] - ETA: 4:48 - loss: 0.6860 - acc: 0.5551
2752/5677 [=============>................] - ETA: 4:43 - loss: 0.6863 - acc: 0.5560
2816/5677 [=============>................] - ETA: 4:36 - loss: 0.6864 - acc: 0.5550
2880/5677 [==============>...............] - ETA: 4:30 - loss: 0.6850 - acc: 0.5583
2944/5677 [==============>...............] - ETA: 4:24 - loss: 0.6855 - acc: 0.5567
3008/5677 [==============>...............] - ETA: 4:17 - loss: 0.6852 - acc: 0.5578
3072/5677 [===============>..............] - ETA: 4:10 - loss: 0.6847 - acc: 0.5586
3136/5677 [===============>..............] - ETA: 4:04 - loss: 0.6838 - acc: 0.5606
3200/5677 [===============>..............] - ETA: 3:57 - loss: 0.6837 - acc: 0.5597
3264/5677 [================>.............] - ETA: 3:50 - loss: 0.6831 - acc: 0.5604
3328/5677 [================>.............] - ETA: 3:44 - loss: 0.6824 - acc: 0.5610
3392/5677 [================>.............] - ETA: 3:38 - loss: 0.6823 - acc: 0.5619
3456/5677 [=================>............] - ETA: 3:32 - loss: 0.6822 - acc: 0.5619
3520/5677 [=================>............] - ETA: 3:26 - loss: 0.6826 - acc: 0.5622
3584/5677 [=================>............] - ETA: 3:20 - loss: 0.6823 - acc: 0.5619
3648/5677 [==================>...........] - ETA: 3:14 - loss: 0.6820 - acc: 0.5641
3712/5677 [==================>...........] - ETA: 3:08 - loss: 0.6829 - acc: 0.5625
3776/5677 [==================>...........] - ETA: 3:02 - loss: 0.6826 - acc: 0.5620
3840/5677 [===================>..........] - ETA: 2:56 - loss: 0.6823 - acc: 0.5633
3904/5677 [===================>..........] - ETA: 2:50 - loss: 0.6825 - acc: 0.5633
3968/5677 [===================>..........] - ETA: 2:44 - loss: 0.6818 - acc: 0.5635
4032/5677 [====================>.........] - ETA: 2:38 - loss: 0.6816 - acc: 0.5637
4096/5677 [====================>.........] - ETA: 2:31 - loss: 0.6822 - acc: 0.5635
4160/5677 [====================>.........] - ETA: 2:26 - loss: 0.6827 - acc: 0.5623
4224/5677 [=====================>........] - ETA: 2:20 - loss: 0.6831 - acc: 0.5616
4288/5677 [=====================>........] - ETA: 2:14 - loss: 0.6830 - acc: 0.5623
4352/5677 [=====================>........] - ETA: 2:08 - loss: 0.6823 - acc: 0.5641
4416/5677 [======================>.......] - ETA: 2:02 - loss: 0.6832 - acc: 0.5623
4480/5677 [======================>.......] - ETA: 1:56 - loss: 0.6829 - acc: 0.5627
4544/5677 [=======================>......] - ETA: 1:49 - loss: 0.6823 - acc: 0.5640
4608/5677 [=======================>......] - ETA: 1:43 - loss: 0.6824 - acc: 0.5640
4672/5677 [=======================>......] - ETA: 1:37 - loss: 0.6829 - acc: 0.5629
4736/5677 [========================>.....] - ETA: 1:31 - loss: 0.6831 - acc: 0.5623
4800/5677 [========================>.....] - ETA: 1:25 - loss: 0.6830 - acc: 0.5629
4864/5677 [========================>.....] - ETA: 1:19 - loss: 0.6839 - acc: 0.5613
4928/5677 [=========================>....] - ETA: 1:13 - loss: 0.6833 - acc: 0.5623
4992/5677 [=========================>....] - ETA: 1:07 - loss: 0.6831 - acc: 0.5623
5056/5677 [=========================>....] - ETA: 1:01 - loss: 0.6830 - acc: 0.5623
5120/5677 [==========================>...] - ETA: 54s - loss: 0.6827 - acc: 0.5633 
5184/5677 [==========================>...] - ETA: 48s - loss: 0.6830 - acc: 0.5621
5248/5677 [==========================>...] - ETA: 42s - loss: 0.6834 - acc: 0.5614
5312/5677 [===========================>..] - ETA: 35s - loss: 0.6836 - acc: 0.5610
5376/5677 [===========================>..] - ETA: 29s - loss: 0.6834 - acc: 0.5619
5440/5677 [===========================>..] - ETA: 23s - loss: 0.6833 - acc: 0.5627
5504/5677 [============================>.] - ETA: 17s - loss: 0.6837 - acc: 0.5620
5568/5677 [============================>.] - ETA: 10s - loss: 0.6840 - acc: 0.5609
5632/5677 [============================>.] - ETA: 4s - loss: 0.6839 - acc: 0.5616 
5677/5677 [==============================] - 589s 104ms/step - loss: 0.6842 - acc: 0.5609 - val_loss: 0.6755 - val_acc: 0.5594

Epoch 00003: val_acc did not improve from 0.56101
Epoch 4/10

  64/5677 [..............................] - ETA: 9:14 - loss: 0.6975 - acc: 0.5156
 128/5677 [..............................] - ETA: 9:27 - loss: 0.7032 - acc: 0.5078
 192/5677 [>.............................] - ETA: 9:14 - loss: 0.7124 - acc: 0.5156
 256/5677 [>.............................] - ETA: 8:50 - loss: 0.7090 - acc: 0.5156
 320/5677 [>.............................] - ETA: 8:30 - loss: 0.7028 - acc: 0.5344
 384/5677 [=>............................] - ETA: 8:13 - loss: 0.7036 - acc: 0.5365
 448/5677 [=>............................] - ETA: 8:01 - loss: 0.6980 - acc: 0.5379
 512/5677 [=>............................] - ETA: 7:49 - loss: 0.6886 - acc: 0.5527
 576/5677 [==>...........................] - ETA: 7:41 - loss: 0.6913 - acc: 0.5486
 640/5677 [==>...........................] - ETA: 7:41 - loss: 0.6935 - acc: 0.5391
 704/5677 [==>...........................] - ETA: 7:37 - loss: 0.6897 - acc: 0.5497
 768/5677 [===>..........................] - ETA: 7:31 - loss: 0.6865 - acc: 0.5599
 832/5677 [===>..........................] - ETA: 7:24 - loss: 0.6857 - acc: 0.5565
 896/5677 [===>..........................] - ETA: 7:20 - loss: 0.6869 - acc: 0.5547
 960/5677 [====>.........................] - ETA: 7:10 - loss: 0.6865 - acc: 0.5583
1024/5677 [====>.........................] - ETA: 7:04 - loss: 0.6853 - acc: 0.5635
1088/5677 [====>.........................] - ETA: 7:00 - loss: 0.6842 - acc: 0.5653
1152/5677 [=====>........................] - ETA: 7:00 - loss: 0.6851 - acc: 0.5668
1216/5677 [=====>........................] - ETA: 6:58 - loss: 0.6863 - acc: 0.5625
1280/5677 [=====>........................] - ETA: 6:53 - loss: 0.6863 - acc: 0.5633
1344/5677 [======>.......................] - ETA: 6:47 - loss: 0.6860 - acc: 0.5662
1408/5677 [======>.......................] - ETA: 6:42 - loss: 0.6844 - acc: 0.5682
1472/5677 [======>.......................] - ETA: 6:34 - loss: 0.6850 - acc: 0.5659
1536/5677 [=======>......................] - ETA: 6:28 - loss: 0.6849 - acc: 0.5658
1600/5677 [=======>......................] - ETA: 6:22 - loss: 0.6838 - acc: 0.5675
1664/5677 [=======>......................] - ETA: 6:16 - loss: 0.6836 - acc: 0.5679
1728/5677 [========>.....................] - ETA: 6:10 - loss: 0.6828 - acc: 0.5683
1792/5677 [========>.....................] - ETA: 6:05 - loss: 0.6835 - acc: 0.5675
1856/5677 [========>.....................] - ETA: 6:02 - loss: 0.6838 - acc: 0.5652
1920/5677 [=========>....................] - ETA: 5:58 - loss: 0.6844 - acc: 0.5641
1984/5677 [=========>....................] - ETA: 5:53 - loss: 0.6838 - acc: 0.5640
2048/5677 [=========>....................] - ETA: 5:47 - loss: 0.6835 - acc: 0.5635
2112/5677 [==========>...................] - ETA: 5:41 - loss: 0.6834 - acc: 0.5649
2176/5677 [==========>...................] - ETA: 5:35 - loss: 0.6830 - acc: 0.5653
2240/5677 [==========>...................] - ETA: 5:29 - loss: 0.6840 - acc: 0.5634
2304/5677 [===========>..................] - ETA: 5:21 - loss: 0.6837 - acc: 0.5642
2368/5677 [===========>..................] - ETA: 5:14 - loss: 0.6830 - acc: 0.5650
2432/5677 [===========>..................] - ETA: 5:09 - loss: 0.6833 - acc: 0.5650
2496/5677 [============>.................] - ETA: 5:04 - loss: 0.6828 - acc: 0.5653
2560/5677 [============>.................] - ETA: 4:59 - loss: 0.6814 - acc: 0.5680
2624/5677 [============>.................] - ETA: 4:53 - loss: 0.6807 - acc: 0.5686
2688/5677 [=============>................] - ETA: 4:47 - loss: 0.6806 - acc: 0.5670
2752/5677 [=============>................] - ETA: 4:41 - loss: 0.6808 - acc: 0.5661
2816/5677 [=============>................] - ETA: 4:34 - loss: 0.6805 - acc: 0.5668
2880/5677 [==============>...............] - ETA: 4:28 - loss: 0.6799 - acc: 0.5674
2944/5677 [==============>...............] - ETA: 4:22 - loss: 0.6786 - acc: 0.5683
3008/5677 [==============>...............] - ETA: 4:16 - loss: 0.6780 - acc: 0.5701
3072/5677 [===============>..............] - ETA: 4:09 - loss: 0.6784 - acc: 0.5713
3136/5677 [===============>..............] - ETA: 4:03 - loss: 0.6769 - acc: 0.5740
3200/5677 [===============>..............] - ETA: 3:58 - loss: 0.6775 - acc: 0.5722
3264/5677 [================>.............] - ETA: 3:52 - loss: 0.6775 - acc: 0.5717
3328/5677 [================>.............] - ETA: 3:46 - loss: 0.6776 - acc: 0.5718
3392/5677 [================>.............] - ETA: 3:40 - loss: 0.6783 - acc: 0.5710
3456/5677 [=================>............] - ETA: 3:34 - loss: 0.6785 - acc: 0.5715
3520/5677 [=================>............] - ETA: 3:28 - loss: 0.6781 - acc: 0.5730
3584/5677 [=================>............] - ETA: 3:21 - loss: 0.6785 - acc: 0.5734
3648/5677 [==================>...........] - ETA: 3:15 - loss: 0.6783 - acc: 0.5732
3712/5677 [==================>...........] - ETA: 3:08 - loss: 0.6783 - acc: 0.5725
3776/5677 [==================>...........] - ETA: 3:02 - loss: 0.6785 - acc: 0.5710
3840/5677 [===================>..........] - ETA: 2:57 - loss: 0.6786 - acc: 0.5708
3904/5677 [===================>..........] - ETA: 2:51 - loss: 0.6782 - acc: 0.5715
3968/5677 [===================>..........] - ETA: 2:45 - loss: 0.6782 - acc: 0.5718
4032/5677 [====================>.........] - ETA: 2:39 - loss: 0.6783 - acc: 0.5717
4096/5677 [====================>.........] - ETA: 2:32 - loss: 0.6778 - acc: 0.5732
4160/5677 [====================>.........] - ETA: 2:26 - loss: 0.6780 - acc: 0.5728
4224/5677 [=====================>........] - ETA: 2:20 - loss: 0.6781 - acc: 0.5727
4288/5677 [=====================>........] - ETA: 2:13 - loss: 0.6787 - acc: 0.5718
4352/5677 [=====================>........] - ETA: 2:07 - loss: 0.6786 - acc: 0.5738
4416/5677 [======================>.......] - ETA: 2:01 - loss: 0.6789 - acc: 0.5731
4480/5677 [======================>.......] - ETA: 1:55 - loss: 0.6789 - acc: 0.5728
4544/5677 [=======================>......] - ETA: 1:49 - loss: 0.6786 - acc: 0.5731
4608/5677 [=======================>......] - ETA: 1:43 - loss: 0.6783 - acc: 0.5740
4672/5677 [=======================>......] - ETA: 1:37 - loss: 0.6774 - acc: 0.5771
4736/5677 [========================>.....] - ETA: 1:31 - loss: 0.6779 - acc: 0.5752
4800/5677 [========================>.....] - ETA: 1:25 - loss: 0.6777 - acc: 0.5748
4864/5677 [========================>.....] - ETA: 1:18 - loss: 0.6773 - acc: 0.5755
4928/5677 [=========================>....] - ETA: 1:12 - loss: 0.6773 - acc: 0.5753
4992/5677 [=========================>....] - ETA: 1:06 - loss: 0.6773 - acc: 0.5749
5056/5677 [=========================>....] - ETA: 1:00 - loss: 0.6768 - acc: 0.5758
5120/5677 [==========================>...] - ETA: 54s - loss: 0.6770 - acc: 0.5744 
5184/5677 [==========================>...] - ETA: 47s - loss: 0.6773 - acc: 0.5745
5248/5677 [==========================>...] - ETA: 41s - loss: 0.6775 - acc: 0.5747
5312/5677 [===========================>..] - ETA: 35s - loss: 0.6773 - acc: 0.5747
5376/5677 [===========================>..] - ETA: 29s - loss: 0.6775 - acc: 0.5744
5440/5677 [===========================>..] - ETA: 23s - loss: 0.6776 - acc: 0.5741
5504/5677 [============================>.] - ETA: 16s - loss: 0.6775 - acc: 0.5745
5568/5677 [============================>.] - ETA: 10s - loss: 0.6777 - acc: 0.5745
5632/5677 [============================>.] - ETA: 4s - loss: 0.6773 - acc: 0.5748 
5677/5677 [==============================] - 580s 102ms/step - loss: 0.6772 - acc: 0.5755 - val_loss: 0.6845 - val_acc: 0.5483

Epoch 00004: val_acc did not improve from 0.56101
Epoch 5/10

  64/5677 [..............................] - ETA: 8:39 - loss: 0.7012 - acc: 0.5000
 128/5677 [..............................] - ETA: 9:32 - loss: 0.7024 - acc: 0.5000
 192/5677 [>.............................] - ETA: 9:18 - loss: 0.6877 - acc: 0.5312
 256/5677 [>.............................] - ETA: 9:21 - loss: 0.6855 - acc: 0.5352
 320/5677 [>.............................] - ETA: 9:18 - loss: 0.6785 - acc: 0.5625
 384/5677 [=>............................] - ETA: 9:07 - loss: 0.6877 - acc: 0.5469
 448/5677 [=>............................] - ETA: 8:59 - loss: 0.6853 - acc: 0.5558
 512/5677 [=>............................] - ETA: 8:51 - loss: 0.6797 - acc: 0.5625
 576/5677 [==>...........................] - ETA: 8:46 - loss: 0.6763 - acc: 0.5781
 640/5677 [==>...........................] - ETA: 8:44 - loss: 0.6744 - acc: 0.5750
 704/5677 [==>...........................] - ETA: 8:34 - loss: 0.6765 - acc: 0.5682
 768/5677 [===>..........................] - ETA: 8:27 - loss: 0.6800 - acc: 0.5690
 832/5677 [===>..........................] - ETA: 8:24 - loss: 0.6804 - acc: 0.5649
 896/5677 [===>..........................] - ETA: 8:14 - loss: 0.6823 - acc: 0.5614
 960/5677 [====>.........................] - ETA: 8:08 - loss: 0.6808 - acc: 0.5604
1024/5677 [====>.........................] - ETA: 8:00 - loss: 0.6819 - acc: 0.5566
1088/5677 [====>.........................] - ETA: 7:53 - loss: 0.6794 - acc: 0.5625
1152/5677 [=====>........................] - ETA: 7:48 - loss: 0.6814 - acc: 0.5564
1216/5677 [=====>........................] - ETA: 7:42 - loss: 0.6830 - acc: 0.5526
1280/5677 [=====>........................] - ETA: 7:35 - loss: 0.6837 - acc: 0.5531
1344/5677 [======>.......................] - ETA: 7:28 - loss: 0.6818 - acc: 0.5573
1408/5677 [======>.......................] - ETA: 7:19 - loss: 0.6821 - acc: 0.5554
1472/5677 [======>.......................] - ETA: 7:12 - loss: 0.6819 - acc: 0.5577
1536/5677 [=======>......................] - ETA: 7:05 - loss: 0.6810 - acc: 0.5599
1600/5677 [=======>......................] - ETA: 6:56 - loss: 0.6801 - acc: 0.5619
1664/5677 [=======>......................] - ETA: 6:49 - loss: 0.6799 - acc: 0.5631
1728/5677 [========>.....................] - ETA: 6:42 - loss: 0.6798 - acc: 0.5637
1792/5677 [========>.....................] - ETA: 6:36 - loss: 0.6802 - acc: 0.5631
1856/5677 [========>.....................] - ETA: 6:28 - loss: 0.6809 - acc: 0.5620
1920/5677 [=========>....................] - ETA: 6:21 - loss: 0.6806 - acc: 0.5630
1984/5677 [=========>....................] - ETA: 6:13 - loss: 0.6792 - acc: 0.5660
2048/5677 [=========>....................] - ETA: 6:07 - loss: 0.6803 - acc: 0.5640
2112/5677 [==========>...................] - ETA: 5:59 - loss: 0.6788 - acc: 0.5668
2176/5677 [==========>...................] - ETA: 5:53 - loss: 0.6778 - acc: 0.5689
2240/5677 [==========>...................] - ETA: 5:46 - loss: 0.6770 - acc: 0.5701
2304/5677 [===========>..................] - ETA: 5:38 - loss: 0.6772 - acc: 0.5690
2368/5677 [===========>..................] - ETA: 5:32 - loss: 0.6776 - acc: 0.5688
2432/5677 [===========>..................] - ETA: 5:25 - loss: 0.6775 - acc: 0.5687
2496/5677 [============>.................] - ETA: 5:18 - loss: 0.6776 - acc: 0.5693
2560/5677 [============>.................] - ETA: 5:11 - loss: 0.6768 - acc: 0.5707
2624/5677 [============>.................] - ETA: 5:05 - loss: 0.6767 - acc: 0.5728
2688/5677 [=============>................] - ETA: 4:58 - loss: 0.6760 - acc: 0.5733
2752/5677 [=============>................] - ETA: 4:51 - loss: 0.6766 - acc: 0.5719
2816/5677 [=============>................] - ETA: 4:45 - loss: 0.6755 - acc: 0.5742
2880/5677 [==============>...............] - ETA: 4:38 - loss: 0.6751 - acc: 0.5736
2944/5677 [==============>...............] - ETA: 4:32 - loss: 0.6752 - acc: 0.5724
3008/5677 [==============>...............] - ETA: 4:25 - loss: 0.6756 - acc: 0.5711
3072/5677 [===============>..............] - ETA: 4:18 - loss: 0.6771 - acc: 0.5700
3136/5677 [===============>..............] - ETA: 4:12 - loss: 0.6777 - acc: 0.5702
3200/5677 [===============>..............] - ETA: 4:05 - loss: 0.6781 - acc: 0.5703
3264/5677 [================>.............] - ETA: 3:59 - loss: 0.6788 - acc: 0.5702
3328/5677 [================>.............] - ETA: 3:52 - loss: 0.6792 - acc: 0.5691
3392/5677 [================>.............] - ETA: 3:46 - loss: 0.6786 - acc: 0.5702
3456/5677 [=================>............] - ETA: 3:39 - loss: 0.6789 - acc: 0.5700
3520/5677 [=================>............] - ETA: 3:32 - loss: 0.6784 - acc: 0.5702
3584/5677 [=================>............] - ETA: 3:26 - loss: 0.6777 - acc: 0.5711
3648/5677 [==================>...........] - ETA: 3:18 - loss: 0.6774 - acc: 0.5726
3712/5677 [==================>...........] - ETA: 3:12 - loss: 0.6783 - acc: 0.5714
3776/5677 [==================>...........] - ETA: 3:05 - loss: 0.6771 - acc: 0.5739
3840/5677 [===================>..........] - ETA: 2:58 - loss: 0.6778 - acc: 0.5734
3904/5677 [===================>..........] - ETA: 2:52 - loss: 0.6781 - acc: 0.5722
3968/5677 [===================>..........] - ETA: 2:45 - loss: 0.6778 - acc: 0.5731
4032/5677 [====================>.........] - ETA: 2:38 - loss: 0.6775 - acc: 0.5742
4096/5677 [====================>.........] - ETA: 2:32 - loss: 0.6764 - acc: 0.5769
4160/5677 [====================>.........] - ETA: 2:26 - loss: 0.6769 - acc: 0.5760
4224/5677 [=====================>........] - ETA: 2:20 - loss: 0.6767 - acc: 0.5760
4288/5677 [=====================>........] - ETA: 2:14 - loss: 0.6768 - acc: 0.5758
4352/5677 [=====================>........] - ETA: 2:07 - loss: 0.6762 - acc: 0.5763
4416/5677 [======================>.......] - ETA: 2:01 - loss: 0.6752 - acc: 0.5777
4480/5677 [======================>.......] - ETA: 1:54 - loss: 0.6753 - acc: 0.5775
4544/5677 [=======================>......] - ETA: 1:48 - loss: 0.6749 - acc: 0.5788
4608/5677 [=======================>......] - ETA: 1:42 - loss: 0.6753 - acc: 0.5783
4672/5677 [=======================>......] - ETA: 1:35 - loss: 0.6756 - acc: 0.5783
4736/5677 [========================>.....] - ETA: 1:29 - loss: 0.6750 - acc: 0.5790
4800/5677 [========================>.....] - ETA: 1:23 - loss: 0.6754 - acc: 0.5785
4864/5677 [========================>.....] - ETA: 1:17 - loss: 0.6745 - acc: 0.5792
4928/5677 [=========================>....] - ETA: 1:11 - loss: 0.6748 - acc: 0.5787
4992/5677 [=========================>....] - ETA: 1:05 - loss: 0.6744 - acc: 0.5799
5056/5677 [=========================>....] - ETA: 59s - loss: 0.6741 - acc: 0.5793 
5120/5677 [==========================>...] - ETA: 52s - loss: 0.6746 - acc: 0.5781
5184/5677 [==========================>...] - ETA: 46s - loss: 0.6744 - acc: 0.5789
5248/5677 [==========================>...] - ETA: 40s - loss: 0.6745 - acc: 0.5783
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6740 - acc: 0.5794
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6732 - acc: 0.5807
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6744 - acc: 0.5796
5504/5677 [============================>.] - ETA: 16s - loss: 0.6741 - acc: 0.5807
5568/5677 [============================>.] - ETA: 10s - loss: 0.6752 - acc: 0.5799
5632/5677 [============================>.] - ETA: 4s - loss: 0.6749 - acc: 0.5803 
5677/5677 [==============================] - 555s 98ms/step - loss: 0.6747 - acc: 0.5809 - val_loss: 0.6761 - val_acc: 0.5753

Epoch 00005: val_acc improved from 0.56101 to 0.57528, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window12/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 7:51 - loss: 0.6562 - acc: 0.6250
 128/5677 [..............................] - ETA: 8:21 - loss: 0.6688 - acc: 0.6250
 192/5677 [>.............................] - ETA: 8:21 - loss: 0.6740 - acc: 0.5990
 256/5677 [>.............................] - ETA: 8:14 - loss: 0.6686 - acc: 0.5977
 320/5677 [>.............................] - ETA: 8:16 - loss: 0.6678 - acc: 0.6000
 384/5677 [=>............................] - ETA: 8:14 - loss: 0.6656 - acc: 0.6068
 448/5677 [=>............................] - ETA: 8:06 - loss: 0.6714 - acc: 0.5982
 512/5677 [=>............................] - ETA: 7:55 - loss: 0.6708 - acc: 0.5938
 576/5677 [==>...........................] - ETA: 7:50 - loss: 0.6746 - acc: 0.5868
 640/5677 [==>...........................] - ETA: 7:42 - loss: 0.6712 - acc: 0.5906
 704/5677 [==>...........................] - ETA: 7:30 - loss: 0.6735 - acc: 0.5795
 768/5677 [===>..........................] - ETA: 7:19 - loss: 0.6690 - acc: 0.5872
 832/5677 [===>..........................] - ETA: 7:10 - loss: 0.6679 - acc: 0.5865
 896/5677 [===>..........................] - ETA: 7:11 - loss: 0.6689 - acc: 0.5848
 960/5677 [====>.........................] - ETA: 7:06 - loss: 0.6677 - acc: 0.5875
1024/5677 [====>.........................] - ETA: 7:02 - loss: 0.6677 - acc: 0.5879
1088/5677 [====>.........................] - ETA: 6:55 - loss: 0.6667 - acc: 0.5928
1152/5677 [=====>........................] - ETA: 6:51 - loss: 0.6670 - acc: 0.5955
1216/5677 [=====>........................] - ETA: 6:45 - loss: 0.6681 - acc: 0.5913
1280/5677 [=====>........................] - ETA: 6:39 - loss: 0.6673 - acc: 0.5914
1344/5677 [======>.......................] - ETA: 6:35 - loss: 0.6679 - acc: 0.5915
1408/5677 [======>.......................] - ETA: 6:32 - loss: 0.6685 - acc: 0.5895
1472/5677 [======>.......................] - ETA: 6:27 - loss: 0.6666 - acc: 0.5938
1536/5677 [=======>......................] - ETA: 6:26 - loss: 0.6661 - acc: 0.5951
1600/5677 [=======>......................] - ETA: 6:23 - loss: 0.6644 - acc: 0.5956
1664/5677 [=======>......................] - ETA: 6:19 - loss: 0.6634 - acc: 0.5956
1728/5677 [========>.....................] - ETA: 6:15 - loss: 0.6629 - acc: 0.5961
1792/5677 [========>.....................] - ETA: 6:11 - loss: 0.6636 - acc: 0.5954
1856/5677 [========>.....................] - ETA: 6:04 - loss: 0.6629 - acc: 0.5948
1920/5677 [=========>....................] - ETA: 5:58 - loss: 0.6653 - acc: 0.5932
1984/5677 [=========>....................] - ETA: 5:53 - loss: 0.6654 - acc: 0.5917
2048/5677 [=========>....................] - ETA: 5:46 - loss: 0.6637 - acc: 0.5928
2112/5677 [==========>...................] - ETA: 5:40 - loss: 0.6625 - acc: 0.5952
2176/5677 [==========>...................] - ETA: 5:35 - loss: 0.6621 - acc: 0.5947
2240/5677 [==========>...................] - ETA: 5:30 - loss: 0.6617 - acc: 0.5946
2304/5677 [===========>..................] - ETA: 5:26 - loss: 0.6628 - acc: 0.5938
2368/5677 [===========>..................] - ETA: 5:21 - loss: 0.6625 - acc: 0.5946
2432/5677 [===========>..................] - ETA: 5:16 - loss: 0.6630 - acc: 0.5938
2496/5677 [============>.................] - ETA: 5:10 - loss: 0.6644 - acc: 0.5905
2560/5677 [============>.................] - ETA: 5:04 - loss: 0.6648 - acc: 0.5902
2624/5677 [============>.................] - ETA: 4:57 - loss: 0.6657 - acc: 0.5892
2688/5677 [=============>................] - ETA: 4:51 - loss: 0.6646 - acc: 0.5923
2752/5677 [=============>................] - ETA: 4:45 - loss: 0.6665 - acc: 0.5905
2816/5677 [=============>................] - ETA: 4:39 - loss: 0.6665 - acc: 0.5902
2880/5677 [==============>...............] - ETA: 4:33 - loss: 0.6658 - acc: 0.5924
2944/5677 [==============>...............] - ETA: 4:28 - loss: 0.6654 - acc: 0.5934
3008/5677 [==============>...............] - ETA: 4:22 - loss: 0.6648 - acc: 0.5941
3072/5677 [===============>..............] - ETA: 4:16 - loss: 0.6654 - acc: 0.5941
3136/5677 [===============>..............] - ETA: 4:10 - loss: 0.6663 - acc: 0.5931
3200/5677 [===============>..............] - ETA: 4:05 - loss: 0.6662 - acc: 0.5931
3264/5677 [================>.............] - ETA: 3:59 - loss: 0.6659 - acc: 0.5934
3328/5677 [================>.............] - ETA: 3:53 - loss: 0.6653 - acc: 0.5944
3392/5677 [================>.............] - ETA: 3:47 - loss: 0.6660 - acc: 0.5938
3456/5677 [=================>............] - ETA: 3:41 - loss: 0.6668 - acc: 0.5929
3520/5677 [=================>............] - ETA: 3:35 - loss: 0.6665 - acc: 0.5935
3584/5677 [=================>............] - ETA: 3:30 - loss: 0.6664 - acc: 0.5943
3648/5677 [==================>...........] - ETA: 3:24 - loss: 0.6665 - acc: 0.5948
3712/5677 [==================>...........] - ETA: 3:18 - loss: 0.6666 - acc: 0.5948
3776/5677 [==================>...........] - ETA: 3:12 - loss: 0.6664 - acc: 0.5948
3840/5677 [===================>..........] - ETA: 3:05 - loss: 0.6665 - acc: 0.5948
3904/5677 [===================>..........] - ETA: 2:59 - loss: 0.6669 - acc: 0.5948
3968/5677 [===================>..........] - ETA: 2:53 - loss: 0.6666 - acc: 0.5955
4032/5677 [====================>.........] - ETA: 2:47 - loss: 0.6670 - acc: 0.5947
4096/5677 [====================>.........] - ETA: 2:40 - loss: 0.6672 - acc: 0.5930
4160/5677 [====================>.........] - ETA: 2:34 - loss: 0.6666 - acc: 0.5933
4224/5677 [=====================>........] - ETA: 2:28 - loss: 0.6661 - acc: 0.5942
4288/5677 [=====================>........] - ETA: 2:22 - loss: 0.6665 - acc: 0.5931
4352/5677 [=====================>........] - ETA: 2:15 - loss: 0.6662 - acc: 0.5951
4416/5677 [======================>.......] - ETA: 2:09 - loss: 0.6669 - acc: 0.5942
4480/5677 [======================>.......] - ETA: 2:02 - loss: 0.6670 - acc: 0.5931
4544/5677 [=======================>......] - ETA: 1:56 - loss: 0.6675 - acc: 0.5922
4608/5677 [=======================>......] - ETA: 1:50 - loss: 0.6677 - acc: 0.5922
4672/5677 [=======================>......] - ETA: 1:43 - loss: 0.6679 - acc: 0.5916
4736/5677 [========================>.....] - ETA: 1:37 - loss: 0.6686 - acc: 0.5910
4800/5677 [========================>.....] - ETA: 1:30 - loss: 0.6689 - acc: 0.5904
4864/5677 [========================>.....] - ETA: 1:24 - loss: 0.6690 - acc: 0.5909
4928/5677 [=========================>....] - ETA: 1:17 - loss: 0.6692 - acc: 0.5903
4992/5677 [=========================>....] - ETA: 1:11 - loss: 0.6694 - acc: 0.5903
5056/5677 [=========================>....] - ETA: 1:04 - loss: 0.6691 - acc: 0.5906
5120/5677 [==========================>...] - ETA: 57s - loss: 0.6694 - acc: 0.5896 
5184/5677 [==========================>...] - ETA: 51s - loss: 0.6692 - acc: 0.5901
5248/5677 [==========================>...] - ETA: 44s - loss: 0.6697 - acc: 0.5896
5312/5677 [===========================>..] - ETA: 37s - loss: 0.6699 - acc: 0.5898
5376/5677 [===========================>..] - ETA: 31s - loss: 0.6695 - acc: 0.5910
5440/5677 [===========================>..] - ETA: 24s - loss: 0.6696 - acc: 0.5910
5504/5677 [============================>.] - ETA: 18s - loss: 0.6695 - acc: 0.5912
5568/5677 [============================>.] - ETA: 11s - loss: 0.6699 - acc: 0.5900
5632/5677 [============================>.] - ETA: 4s - loss: 0.6703 - acc: 0.5891 
5677/5677 [==============================] - 618s 109ms/step - loss: 0.6706 - acc: 0.5887 - val_loss: 0.7012 - val_acc: 0.5515

Epoch 00006: val_acc did not improve from 0.57528
Epoch 7/10

  64/5677 [..............................] - ETA: 10:07 - loss: 0.6617 - acc: 0.6250
 128/5677 [..............................] - ETA: 9:51 - loss: 0.6466 - acc: 0.6094 
 192/5677 [>.............................] - ETA: 9:36 - loss: 0.6450 - acc: 0.6146
 256/5677 [>.............................] - ETA: 9:42 - loss: 0.6598 - acc: 0.5898
 320/5677 [>.............................] - ETA: 9:28 - loss: 0.6605 - acc: 0.5844
 384/5677 [=>............................] - ETA: 9:21 - loss: 0.6625 - acc: 0.5859
 448/5677 [=>............................] - ETA: 9:17 - loss: 0.6624 - acc: 0.5982
 512/5677 [=>............................] - ETA: 9:08 - loss: 0.6617 - acc: 0.5938
 576/5677 [==>...........................] - ETA: 9:02 - loss: 0.6683 - acc: 0.5851
 640/5677 [==>...........................] - ETA: 8:50 - loss: 0.6707 - acc: 0.5797
 704/5677 [==>...........................] - ETA: 8:39 - loss: 0.6681 - acc: 0.5838
 768/5677 [===>..........................] - ETA: 8:34 - loss: 0.6694 - acc: 0.5846
 832/5677 [===>..........................] - ETA: 8:31 - loss: 0.6659 - acc: 0.5925
 896/5677 [===>..........................] - ETA: 8:23 - loss: 0.6636 - acc: 0.5949
 960/5677 [====>.........................] - ETA: 8:16 - loss: 0.6642 - acc: 0.5927
1024/5677 [====>.........................] - ETA: 8:12 - loss: 0.6651 - acc: 0.5898
1088/5677 [====>.........................] - ETA: 8:08 - loss: 0.6698 - acc: 0.5827
1152/5677 [=====>........................] - ETA: 8:03 - loss: 0.6714 - acc: 0.5790
1216/5677 [=====>........................] - ETA: 7:59 - loss: 0.6728 - acc: 0.5765
1280/5677 [=====>........................] - ETA: 7:53 - loss: 0.6714 - acc: 0.5781
1344/5677 [======>.......................] - ETA: 7:46 - loss: 0.6731 - acc: 0.5744
1408/5677 [======>.......................] - ETA: 7:39 - loss: 0.6728 - acc: 0.5767
1472/5677 [======>.......................] - ETA: 7:31 - loss: 0.6727 - acc: 0.5768
1536/5677 [=======>......................] - ETA: 7:25 - loss: 0.6720 - acc: 0.5788
1600/5677 [=======>......................] - ETA: 7:18 - loss: 0.6740 - acc: 0.5763
1664/5677 [=======>......................] - ETA: 7:12 - loss: 0.6742 - acc: 0.5769
1728/5677 [========>.....................] - ETA: 7:05 - loss: 0.6755 - acc: 0.5735
1792/5677 [========>.....................] - ETA: 6:59 - loss: 0.6757 - acc: 0.5737
1856/5677 [========>.....................] - ETA: 6:52 - loss: 0.6747 - acc: 0.5781
1920/5677 [=========>....................] - ETA: 6:45 - loss: 0.6756 - acc: 0.5766
1984/5677 [=========>....................] - ETA: 6:38 - loss: 0.6744 - acc: 0.5806
2048/5677 [=========>....................] - ETA: 6:32 - loss: 0.6749 - acc: 0.5801
2112/5677 [==========>...................] - ETA: 6:24 - loss: 0.6736 - acc: 0.5848
2176/5677 [==========>...................] - ETA: 6:18 - loss: 0.6743 - acc: 0.5813
2240/5677 [==========>...................] - ETA: 6:12 - loss: 0.6746 - acc: 0.5826
2304/5677 [===========>..................] - ETA: 6:04 - loss: 0.6730 - acc: 0.5864
2368/5677 [===========>..................] - ETA: 5:57 - loss: 0.6738 - acc: 0.5836
2432/5677 [===========>..................] - ETA: 5:49 - loss: 0.6725 - acc: 0.5843
2496/5677 [============>.................] - ETA: 5:41 - loss: 0.6721 - acc: 0.5857
2560/5677 [============>.................] - ETA: 5:33 - loss: 0.6710 - acc: 0.5871
2624/5677 [============>.................] - ETA: 5:26 - loss: 0.6712 - acc: 0.5865
2688/5677 [=============>................] - ETA: 5:19 - loss: 0.6711 - acc: 0.5867
2752/5677 [=============>................] - ETA: 5:13 - loss: 0.6708 - acc: 0.5883
2816/5677 [=============>................] - ETA: 5:06 - loss: 0.6710 - acc: 0.5866
2880/5677 [==============>...............] - ETA: 4:59 - loss: 0.6700 - acc: 0.5889
2944/5677 [==============>...............] - ETA: 4:52 - loss: 0.6713 - acc: 0.5859
3008/5677 [==============>...............] - ETA: 4:45 - loss: 0.6710 - acc: 0.5854
3072/5677 [===============>..............] - ETA: 4:38 - loss: 0.6701 - acc: 0.5872
3136/5677 [===============>..............] - ETA: 4:31 - loss: 0.6701 - acc: 0.5867
3200/5677 [===============>..............] - ETA: 4:24 - loss: 0.6701 - acc: 0.5866
3264/5677 [================>.............] - ETA: 4:18 - loss: 0.6701 - acc: 0.5870
3328/5677 [================>.............] - ETA: 4:11 - loss: 0.6700 - acc: 0.5871
3392/5677 [================>.............] - ETA: 4:04 - loss: 0.6700 - acc: 0.5858
3456/5677 [=================>............] - ETA: 3:58 - loss: 0.6688 - acc: 0.5883
3520/5677 [=================>............] - ETA: 3:50 - loss: 0.6683 - acc: 0.5889
3584/5677 [=================>............] - ETA: 3:44 - loss: 0.6685 - acc: 0.5882
3648/5677 [==================>...........] - ETA: 3:37 - loss: 0.6679 - acc: 0.5896
3712/5677 [==================>...........] - ETA: 3:30 - loss: 0.6681 - acc: 0.5900
3776/5677 [==================>...........] - ETA: 3:23 - loss: 0.6680 - acc: 0.5906
3840/5677 [===================>..........] - ETA: 3:16 - loss: 0.6680 - acc: 0.5901
3904/5677 [===================>..........] - ETA: 3:09 - loss: 0.6691 - acc: 0.5881
3968/5677 [===================>..........] - ETA: 3:03 - loss: 0.6694 - acc: 0.5877
4032/5677 [====================>.........] - ETA: 2:56 - loss: 0.6694 - acc: 0.5880
4096/5677 [====================>.........] - ETA: 2:49 - loss: 0.6690 - acc: 0.5889
4160/5677 [====================>.........] - ETA: 2:42 - loss: 0.6682 - acc: 0.5899
4224/5677 [=====================>........] - ETA: 2:36 - loss: 0.6675 - acc: 0.5914
4288/5677 [=====================>........] - ETA: 2:29 - loss: 0.6677 - acc: 0.5910
4352/5677 [=====================>........] - ETA: 2:22 - loss: 0.6684 - acc: 0.5905
4416/5677 [======================>.......] - ETA: 2:15 - loss: 0.6684 - acc: 0.5908
4480/5677 [======================>.......] - ETA: 2:08 - loss: 0.6680 - acc: 0.5917
4544/5677 [=======================>......] - ETA: 2:01 - loss: 0.6684 - acc: 0.5911
4608/5677 [=======================>......] - ETA: 1:54 - loss: 0.6683 - acc: 0.5920
4672/5677 [=======================>......] - ETA: 1:47 - loss: 0.6680 - acc: 0.5927
4736/5677 [========================>.....] - ETA: 1:41 - loss: 0.6674 - acc: 0.5933
4800/5677 [========================>.....] - ETA: 1:34 - loss: 0.6673 - acc: 0.5931
4864/5677 [========================>.....] - ETA: 1:27 - loss: 0.6685 - acc: 0.5915
4928/5677 [=========================>....] - ETA: 1:20 - loss: 0.6680 - acc: 0.5925
4992/5677 [=========================>....] - ETA: 1:13 - loss: 0.6681 - acc: 0.5921
5056/5677 [=========================>....] - ETA: 1:06 - loss: 0.6681 - acc: 0.5920
5120/5677 [==========================>...] - ETA: 1:00 - loss: 0.6678 - acc: 0.5924
5184/5677 [==========================>...] - ETA: 53s - loss: 0.6679 - acc: 0.5930 
5248/5677 [==========================>...] - ETA: 46s - loss: 0.6683 - acc: 0.5920
5312/5677 [===========================>..] - ETA: 39s - loss: 0.6682 - acc: 0.5917
5376/5677 [===========================>..] - ETA: 32s - loss: 0.6680 - acc: 0.5924
5440/5677 [===========================>..] - ETA: 25s - loss: 0.6680 - acc: 0.5930
5504/5677 [============================>.] - ETA: 18s - loss: 0.6681 - acc: 0.5925
5568/5677 [============================>.] - ETA: 11s - loss: 0.6682 - acc: 0.5929
5632/5677 [============================>.] - ETA: 4s - loss: 0.6683 - acc: 0.5929 
5677/5677 [==============================] - 641s 113ms/step - loss: 0.6685 - acc: 0.5929 - val_loss: 0.6703 - val_acc: 0.5769

Epoch 00007: val_acc improved from 0.57528 to 0.57686, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window12/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 8/10

  64/5677 [..............................] - ETA: 10:15 - loss: 0.6512 - acc: 0.5312
 128/5677 [..............................] - ETA: 10:36 - loss: 0.6536 - acc: 0.5547
 192/5677 [>.............................] - ETA: 10:16 - loss: 0.6609 - acc: 0.5625
 256/5677 [>.............................] - ETA: 10:04 - loss: 0.6625 - acc: 0.5508
 320/5677 [>.............................] - ETA: 9:58 - loss: 0.6545 - acc: 0.5719 
 384/5677 [=>............................] - ETA: 9:42 - loss: 0.6579 - acc: 0.5807
 448/5677 [=>............................] - ETA: 9:28 - loss: 0.6652 - acc: 0.5692
 512/5677 [=>............................] - ETA: 9:20 - loss: 0.6692 - acc: 0.5762
 576/5677 [==>...........................] - ETA: 9:14 - loss: 0.6722 - acc: 0.5781
 640/5677 [==>...........................] - ETA: 9:01 - loss: 0.6694 - acc: 0.5844
 704/5677 [==>...........................] - ETA: 8:54 - loss: 0.6734 - acc: 0.5753
 768/5677 [===>..........................] - ETA: 8:43 - loss: 0.6748 - acc: 0.5729
 832/5677 [===>..........................] - ETA: 8:37 - loss: 0.6686 - acc: 0.5853
 896/5677 [===>..........................] - ETA: 8:27 - loss: 0.6723 - acc: 0.5781
 960/5677 [====>.........................] - ETA: 8:19 - loss: 0.6736 - acc: 0.5729
1024/5677 [====>.........................] - ETA: 8:16 - loss: 0.6771 - acc: 0.5693
1088/5677 [====>.........................] - ETA: 8:10 - loss: 0.6767 - acc: 0.5662
1152/5677 [=====>........................] - ETA: 8:04 - loss: 0.6748 - acc: 0.5720
1216/5677 [=====>........................] - ETA: 7:57 - loss: 0.6739 - acc: 0.5748
1280/5677 [=====>........................] - ETA: 7:53 - loss: 0.6718 - acc: 0.5813
1344/5677 [======>.......................] - ETA: 7:48 - loss: 0.6711 - acc: 0.5841
1408/5677 [======>.......................] - ETA: 7:44 - loss: 0.6718 - acc: 0.5824
1472/5677 [======>.......................] - ETA: 7:37 - loss: 0.6713 - acc: 0.5836
1536/5677 [=======>......................] - ETA: 7:31 - loss: 0.6706 - acc: 0.5840
1600/5677 [=======>......................] - ETA: 7:26 - loss: 0.6702 - acc: 0.5844
1664/5677 [=======>......................] - ETA: 7:20 - loss: 0.6706 - acc: 0.5823
1728/5677 [========>.....................] - ETA: 7:14 - loss: 0.6710 - acc: 0.5828
1792/5677 [========>.....................] - ETA: 7:06 - loss: 0.6729 - acc: 0.5787
1856/5677 [========>.....................] - ETA: 7:00 - loss: 0.6727 - acc: 0.5803
1920/5677 [=========>....................] - ETA: 6:53 - loss: 0.6715 - acc: 0.5844
1984/5677 [=========>....................] - ETA: 6:47 - loss: 0.6710 - acc: 0.5852
2048/5677 [=========>....................] - ETA: 6:41 - loss: 0.6696 - acc: 0.5869
2112/5677 [==========>...................] - ETA: 6:35 - loss: 0.6707 - acc: 0.5866
2176/5677 [==========>...................] - ETA: 6:28 - loss: 0.6709 - acc: 0.5873
2240/5677 [==========>...................] - ETA: 6:22 - loss: 0.6708 - acc: 0.5866
2304/5677 [===========>..................] - ETA: 6:15 - loss: 0.6725 - acc: 0.5851
2368/5677 [===========>..................] - ETA: 6:09 - loss: 0.6736 - acc: 0.5823
2432/5677 [===========>..................] - ETA: 6:02 - loss: 0.6741 - acc: 0.5826
2496/5677 [============>.................] - ETA: 5:55 - loss: 0.6743 - acc: 0.5813
2560/5677 [============>.................] - ETA: 5:49 - loss: 0.6743 - acc: 0.5813
2624/5677 [============>.................] - ETA: 5:42 - loss: 0.6735 - acc: 0.5823
2688/5677 [=============>................] - ETA: 5:35 - loss: 0.6726 - acc: 0.5841
2752/5677 [=============>................] - ETA: 5:28 - loss: 0.6721 - acc: 0.5828
2816/5677 [=============>................] - ETA: 5:21 - loss: 0.6720 - acc: 0.5824
2880/5677 [==============>...............] - ETA: 5:14 - loss: 0.6709 - acc: 0.5844
2944/5677 [==============>...............] - ETA: 5:07 - loss: 0.6708 - acc: 0.5846
3008/5677 [==============>...............] - ETA: 5:00 - loss: 0.6706 - acc: 0.5854
3072/5677 [===============>..............] - ETA: 4:53 - loss: 0.6700 - acc: 0.5853
3136/5677 [===============>..............] - ETA: 4:46 - loss: 0.6687 - acc: 0.5886
3200/5677 [===============>..............] - ETA: 4:39 - loss: 0.6684 - acc: 0.5894
3264/5677 [================>.............] - ETA: 4:31 - loss: 0.6688 - acc: 0.5888
3328/5677 [================>.............] - ETA: 4:24 - loss: 0.6684 - acc: 0.5883
3392/5677 [================>.............] - ETA: 4:17 - loss: 0.6685 - acc: 0.5879
3456/5677 [=================>............] - ETA: 4:10 - loss: 0.6678 - acc: 0.5897
3520/5677 [=================>............] - ETA: 4:02 - loss: 0.6682 - acc: 0.5903
3584/5677 [=================>............] - ETA: 3:55 - loss: 0.6682 - acc: 0.5918
3648/5677 [==================>...........] - ETA: 3:48 - loss: 0.6680 - acc: 0.5918
3712/5677 [==================>...........] - ETA: 3:41 - loss: 0.6678 - acc: 0.5919
3776/5677 [==================>...........] - ETA: 3:34 - loss: 0.6682 - acc: 0.5922
3840/5677 [===================>..........] - ETA: 3:27 - loss: 0.6674 - acc: 0.5935
3904/5677 [===================>..........] - ETA: 3:20 - loss: 0.6666 - acc: 0.5943
3968/5677 [===================>..........] - ETA: 3:13 - loss: 0.6662 - acc: 0.5950
4032/5677 [====================>.........] - ETA: 3:06 - loss: 0.6659 - acc: 0.5955
4096/5677 [====================>.........] - ETA: 2:59 - loss: 0.6652 - acc: 0.5974
4160/5677 [====================>.........] - ETA: 2:52 - loss: 0.6654 - acc: 0.5978
4224/5677 [=====================>........] - ETA: 2:45 - loss: 0.6661 - acc: 0.5966
4288/5677 [=====================>........] - ETA: 2:38 - loss: 0.6669 - acc: 0.5954
4352/5677 [=====================>........] - ETA: 2:31 - loss: 0.6663 - acc: 0.5963
4416/5677 [======================>.......] - ETA: 2:24 - loss: 0.6664 - acc: 0.5965
4480/5677 [======================>.......] - ETA: 2:17 - loss: 0.6673 - acc: 0.5946
4544/5677 [=======================>......] - ETA: 2:10 - loss: 0.6673 - acc: 0.5949
4608/5677 [=======================>......] - ETA: 2:03 - loss: 0.6672 - acc: 0.5948
4672/5677 [=======================>......] - ETA: 1:55 - loss: 0.6674 - acc: 0.5955
4736/5677 [========================>.....] - ETA: 1:48 - loss: 0.6674 - acc: 0.5959
4800/5677 [========================>.....] - ETA: 1:41 - loss: 0.6676 - acc: 0.5962
4864/5677 [========================>.....] - ETA: 1:34 - loss: 0.6680 - acc: 0.5960
4928/5677 [=========================>....] - ETA: 1:26 - loss: 0.6673 - acc: 0.5970
4992/5677 [=========================>....] - ETA: 1:19 - loss: 0.6670 - acc: 0.5976
5056/5677 [=========================>....] - ETA: 1:12 - loss: 0.6671 - acc: 0.5973
5120/5677 [==========================>...] - ETA: 1:04 - loss: 0.6671 - acc: 0.5975
5184/5677 [==========================>...] - ETA: 57s - loss: 0.6679 - acc: 0.5957 
5248/5677 [==========================>...] - ETA: 50s - loss: 0.6686 - acc: 0.5943
5312/5677 [===========================>..] - ETA: 42s - loss: 0.6680 - acc: 0.5949
5376/5677 [===========================>..] - ETA: 35s - loss: 0.6683 - acc: 0.5943
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6685 - acc: 0.5941
5504/5677 [============================>.] - ETA: 20s - loss: 0.6687 - acc: 0.5938
5568/5677 [============================>.] - ETA: 12s - loss: 0.6685 - acc: 0.5938
5632/5677 [============================>.] - ETA: 5s - loss: 0.6686 - acc: 0.5939 
5677/5677 [==============================] - 694s 122ms/step - loss: 0.6690 - acc: 0.5933 - val_loss: 0.6775 - val_acc: 0.5705

Epoch 00008: val_acc did not improve from 0.57686
Epoch 9/10

  64/5677 [..............................] - ETA: 11:08 - loss: 0.6737 - acc: 0.5625
 128/5677 [..............................] - ETA: 11:01 - loss: 0.6775 - acc: 0.5625
 192/5677 [>.............................] - ETA: 10:57 - loss: 0.6743 - acc: 0.5677
 256/5677 [>.............................] - ETA: 10:50 - loss: 0.6674 - acc: 0.5859
 320/5677 [>.............................] - ETA: 10:51 - loss: 0.6668 - acc: 0.6031
 384/5677 [=>............................] - ETA: 10:42 - loss: 0.6702 - acc: 0.5964
 448/5677 [=>............................] - ETA: 10:39 - loss: 0.6742 - acc: 0.5871
 512/5677 [=>............................] - ETA: 10:30 - loss: 0.6745 - acc: 0.5859
 576/5677 [==>...........................] - ETA: 10:25 - loss: 0.6756 - acc: 0.5764
 640/5677 [==>...........................] - ETA: 10:17 - loss: 0.6705 - acc: 0.5906
 704/5677 [==>...........................] - ETA: 10:08 - loss: 0.6709 - acc: 0.5852
 768/5677 [===>..........................] - ETA: 9:59 - loss: 0.6739 - acc: 0.5833 
 832/5677 [===>..........................] - ETA: 9:49 - loss: 0.6755 - acc: 0.5805
 896/5677 [===>..........................] - ETA: 9:45 - loss: 0.6785 - acc: 0.5737
 960/5677 [====>.........................] - ETA: 9:36 - loss: 0.6786 - acc: 0.5729
1024/5677 [====>.........................] - ETA: 9:28 - loss: 0.6788 - acc: 0.5684
1088/5677 [====>.........................] - ETA: 9:18 - loss: 0.6799 - acc: 0.5671
1152/5677 [=====>........................] - ETA: 9:10 - loss: 0.6782 - acc: 0.5686
1216/5677 [=====>........................] - ETA: 9:02 - loss: 0.6770 - acc: 0.5740
1280/5677 [=====>........................] - ETA: 8:54 - loss: 0.6752 - acc: 0.5742
1344/5677 [======>.......................] - ETA: 8:46 - loss: 0.6736 - acc: 0.5766
1408/5677 [======>.......................] - ETA: 8:37 - loss: 0.6739 - acc: 0.5781
1472/5677 [======>.......................] - ETA: 8:30 - loss: 0.6746 - acc: 0.5781
1536/5677 [=======>......................] - ETA: 8:22 - loss: 0.6732 - acc: 0.5820
1600/5677 [=======>......................] - ETA: 8:15 - loss: 0.6719 - acc: 0.5856
1664/5677 [=======>......................] - ETA: 8:07 - loss: 0.6719 - acc: 0.5865
1728/5677 [========>.....................] - ETA: 7:57 - loss: 0.6723 - acc: 0.5839
1792/5677 [========>.....................] - ETA: 7:51 - loss: 0.6717 - acc: 0.5859
1856/5677 [========>.....................] - ETA: 7:44 - loss: 0.6714 - acc: 0.5873
1920/5677 [=========>....................] - ETA: 7:37 - loss: 0.6723 - acc: 0.5849
1984/5677 [=========>....................] - ETA: 7:28 - loss: 0.6729 - acc: 0.5847
2048/5677 [=========>....................] - ETA: 7:21 - loss: 0.6732 - acc: 0.5835
2112/5677 [==========>...................] - ETA: 7:14 - loss: 0.6719 - acc: 0.5843
2176/5677 [==========>...................] - ETA: 7:06 - loss: 0.6717 - acc: 0.5859
2240/5677 [==========>...................] - ETA: 6:59 - loss: 0.6723 - acc: 0.5848
2304/5677 [===========>..................] - ETA: 6:51 - loss: 0.6731 - acc: 0.5838
2368/5677 [===========>..................] - ETA: 6:43 - loss: 0.6729 - acc: 0.5840
2432/5677 [===========>..................] - ETA: 6:35 - loss: 0.6731 - acc: 0.5831
2496/5677 [============>.................] - ETA: 6:28 - loss: 0.6720 - acc: 0.5849
2560/5677 [============>.................] - ETA: 6:20 - loss: 0.6712 - acc: 0.5844
2624/5677 [============>.................] - ETA: 6:12 - loss: 0.6703 - acc: 0.5869
2688/5677 [=============>................] - ETA: 6:04 - loss: 0.6698 - acc: 0.5871
2752/5677 [=============>................] - ETA: 5:57 - loss: 0.6701 - acc: 0.5861
2816/5677 [=============>................] - ETA: 5:49 - loss: 0.6706 - acc: 0.5863
2880/5677 [==============>...............] - ETA: 5:41 - loss: 0.6694 - acc: 0.5882
2944/5677 [==============>...............] - ETA: 5:33 - loss: 0.6696 - acc: 0.5866
3008/5677 [==============>...............] - ETA: 5:25 - loss: 0.6705 - acc: 0.5861
3072/5677 [===============>..............] - ETA: 5:16 - loss: 0.6695 - acc: 0.5876
3136/5677 [===============>..............] - ETA: 5:08 - loss: 0.6693 - acc: 0.5886
3200/5677 [===============>..............] - ETA: 5:00 - loss: 0.6699 - acc: 0.5878
3264/5677 [================>.............] - ETA: 4:52 - loss: 0.6697 - acc: 0.5879
3328/5677 [================>.............] - ETA: 4:44 - loss: 0.6688 - acc: 0.5898
3392/5677 [================>.............] - ETA: 4:36 - loss: 0.6686 - acc: 0.5896
3456/5677 [=================>............] - ETA: 4:28 - loss: 0.6679 - acc: 0.5909
3520/5677 [=================>............] - ETA: 4:20 - loss: 0.6691 - acc: 0.5895
3584/5677 [=================>............] - ETA: 4:12 - loss: 0.6684 - acc: 0.5926
3648/5677 [==================>...........] - ETA: 4:04 - loss: 0.6673 - acc: 0.5943
3712/5677 [==================>...........] - ETA: 3:56 - loss: 0.6667 - acc: 0.5946
3776/5677 [==================>...........] - ETA: 3:48 - loss: 0.6667 - acc: 0.5940
3840/5677 [===================>..........] - ETA: 3:40 - loss: 0.6674 - acc: 0.5927
3904/5677 [===================>..........] - ETA: 3:33 - loss: 0.6677 - acc: 0.5925
3968/5677 [===================>..........] - ETA: 3:25 - loss: 0.6672 - acc: 0.5925
4032/5677 [====================>.........] - ETA: 3:17 - loss: 0.6673 - acc: 0.5915
4096/5677 [====================>.........] - ETA: 3:09 - loss: 0.6667 - acc: 0.5923
4160/5677 [====================>.........] - ETA: 3:01 - loss: 0.6677 - acc: 0.5906
4224/5677 [=====================>........] - ETA: 2:53 - loss: 0.6679 - acc: 0.5909
4288/5677 [=====================>........] - ETA: 2:46 - loss: 0.6679 - acc: 0.5917
4352/5677 [=====================>........] - ETA: 2:38 - loss: 0.6679 - acc: 0.5912
4416/5677 [======================>.......] - ETA: 2:30 - loss: 0.6677 - acc: 0.5910
4480/5677 [======================>.......] - ETA: 2:22 - loss: 0.6683 - acc: 0.5897
4544/5677 [=======================>......] - ETA: 2:14 - loss: 0.6681 - acc: 0.5904
4608/5677 [=======================>......] - ETA: 2:07 - loss: 0.6683 - acc: 0.5909
4672/5677 [=======================>......] - ETA: 1:59 - loss: 0.6679 - acc: 0.5912
4736/5677 [========================>.....] - ETA: 1:51 - loss: 0.6685 - acc: 0.5897
4800/5677 [========================>.....] - ETA: 1:44 - loss: 0.6675 - acc: 0.5904
4864/5677 [========================>.....] - ETA: 1:36 - loss: 0.6683 - acc: 0.5890
4928/5677 [=========================>....] - ETA: 1:28 - loss: 0.6683 - acc: 0.5893
4992/5677 [=========================>....] - ETA: 1:21 - loss: 0.6684 - acc: 0.5895
5056/5677 [=========================>....] - ETA: 1:13 - loss: 0.6686 - acc: 0.5892
5120/5677 [==========================>...] - ETA: 1:05 - loss: 0.6689 - acc: 0.5893
5184/5677 [==========================>...] - ETA: 58s - loss: 0.6690 - acc: 0.5891 
5248/5677 [==========================>...] - ETA: 50s - loss: 0.6686 - acc: 0.5894
5312/5677 [===========================>..] - ETA: 43s - loss: 0.6684 - acc: 0.5892
5376/5677 [===========================>..] - ETA: 35s - loss: 0.6683 - acc: 0.5893
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6691 - acc: 0.5884
5504/5677 [============================>.] - ETA: 20s - loss: 0.6691 - acc: 0.5883
5568/5677 [============================>.] - ETA: 12s - loss: 0.6694 - acc: 0.5884
5632/5677 [============================>.] - ETA: 5s - loss: 0.6696 - acc: 0.5881 
5677/5677 [==============================] - 691s 122ms/step - loss: 0.6690 - acc: 0.5890 - val_loss: 0.6846 - val_acc: 0.5610

Epoch 00009: val_acc did not improve from 0.57686
Epoch 10/10

  64/5677 [..............................] - ETA: 10:33 - loss: 0.7078 - acc: 0.5625
 128/5677 [..............................] - ETA: 10:50 - loss: 0.6740 - acc: 0.6328
 192/5677 [>.............................] - ETA: 10:29 - loss: 0.6712 - acc: 0.6094
 256/5677 [>.............................] - ETA: 10:17 - loss: 0.6671 - acc: 0.6172
 320/5677 [>.............................] - ETA: 10:12 - loss: 0.6772 - acc: 0.6031
 384/5677 [=>............................] - ETA: 10:00 - loss: 0.6702 - acc: 0.6172
 448/5677 [=>............................] - ETA: 10:00 - loss: 0.6763 - acc: 0.6094
 512/5677 [=>............................] - ETA: 9:48 - loss: 0.6641 - acc: 0.6309 
 576/5677 [==>...........................] - ETA: 9:42 - loss: 0.6698 - acc: 0.6128
 640/5677 [==>...........................] - ETA: 9:32 - loss: 0.6701 - acc: 0.6172
 704/5677 [==>...........................] - ETA: 9:27 - loss: 0.6662 - acc: 0.6179
 768/5677 [===>..........................] - ETA: 9:20 - loss: 0.6662 - acc: 0.6159
 832/5677 [===>..........................] - ETA: 9:13 - loss: 0.6682 - acc: 0.6118
 896/5677 [===>..........................] - ETA: 9:04 - loss: 0.6739 - acc: 0.6004
 960/5677 [====>.........................] - ETA: 8:56 - loss: 0.6740 - acc: 0.5990
1024/5677 [====>.........................] - ETA: 8:46 - loss: 0.6719 - acc: 0.6016
1088/5677 [====>.........................] - ETA: 8:36 - loss: 0.6709 - acc: 0.5993
1152/5677 [=====>........................] - ETA: 8:30 - loss: 0.6687 - acc: 0.6007
1216/5677 [=====>........................] - ETA: 8:21 - loss: 0.6704 - acc: 0.5970
1280/5677 [=====>........................] - ETA: 8:13 - loss: 0.6706 - acc: 0.6000
1344/5677 [======>.......................] - ETA: 8:05 - loss: 0.6704 - acc: 0.6012
1408/5677 [======>.......................] - ETA: 7:57 - loss: 0.6693 - acc: 0.6016
1472/5677 [======>.......................] - ETA: 7:49 - loss: 0.6697 - acc: 0.6005
1536/5677 [=======>......................] - ETA: 7:42 - loss: 0.6690 - acc: 0.6016
1600/5677 [=======>......................] - ETA: 7:35 - loss: 0.6679 - acc: 0.6000
1664/5677 [=======>......................] - ETA: 7:27 - loss: 0.6679 - acc: 0.6004
1728/5677 [========>.....................] - ETA: 7:21 - loss: 0.6670 - acc: 0.6013
1792/5677 [========>.....................] - ETA: 7:13 - loss: 0.6667 - acc: 0.6027
1856/5677 [========>.....................] - ETA: 7:05 - loss: 0.6673 - acc: 0.6024
1920/5677 [=========>....................] - ETA: 6:58 - loss: 0.6678 - acc: 0.6005
1984/5677 [=========>....................] - ETA: 6:51 - loss: 0.6689 - acc: 0.5948
2048/5677 [=========>....................] - ETA: 6:44 - loss: 0.6690 - acc: 0.5938
2112/5677 [==========>...................] - ETA: 6:37 - loss: 0.6687 - acc: 0.5942
2176/5677 [==========>...................] - ETA: 6:30 - loss: 0.6694 - acc: 0.5928
2240/5677 [==========>...................] - ETA: 6:23 - loss: 0.6695 - acc: 0.5920
2304/5677 [===========>..................] - ETA: 6:16 - loss: 0.6692 - acc: 0.5938
2368/5677 [===========>..................] - ETA: 6:08 - loss: 0.6692 - acc: 0.5933
2432/5677 [===========>..................] - ETA: 6:01 - loss: 0.6693 - acc: 0.5929
2496/5677 [============>.................] - ETA: 5:54 - loss: 0.6690 - acc: 0.5942
2560/5677 [============>.................] - ETA: 5:47 - loss: 0.6692 - acc: 0.5934
2624/5677 [============>.................] - ETA: 5:40 - loss: 0.6677 - acc: 0.5960
2688/5677 [=============>................] - ETA: 5:33 - loss: 0.6690 - acc: 0.5938
2752/5677 [=============>................] - ETA: 5:26 - loss: 0.6701 - acc: 0.5927
2816/5677 [=============>................] - ETA: 5:18 - loss: 0.6693 - acc: 0.5941
2880/5677 [==============>...............] - ETA: 5:11 - loss: 0.6684 - acc: 0.5962
2944/5677 [==============>...............] - ETA: 5:04 - loss: 0.6675 - acc: 0.5975
3008/5677 [==============>...............] - ETA: 4:57 - loss: 0.6668 - acc: 0.5987
3072/5677 [===============>..............] - ETA: 4:50 - loss: 0.6672 - acc: 0.5990
3136/5677 [===============>..............] - ETA: 4:43 - loss: 0.6674 - acc: 0.5998
3200/5677 [===============>..............] - ETA: 4:36 - loss: 0.6667 - acc: 0.6016
3264/5677 [================>.............] - ETA: 4:29 - loss: 0.6667 - acc: 0.6023
3328/5677 [================>.............] - ETA: 4:21 - loss: 0.6672 - acc: 0.6016
3392/5677 [================>.............] - ETA: 4:14 - loss: 0.6672 - acc: 0.6008
3456/5677 [=================>............] - ETA: 4:08 - loss: 0.6670 - acc: 0.6013
3520/5677 [=================>............] - ETA: 4:00 - loss: 0.6664 - acc: 0.6028
3584/5677 [=================>............] - ETA: 3:53 - loss: 0.6662 - acc: 0.6030
3648/5677 [==================>...........] - ETA: 3:46 - loss: 0.6655 - acc: 0.6042
3712/5677 [==================>...........] - ETA: 3:39 - loss: 0.6660 - acc: 0.6037
3776/5677 [==================>...........] - ETA: 3:32 - loss: 0.6665 - acc: 0.6030
3840/5677 [===================>..........] - ETA: 3:25 - loss: 0.6659 - acc: 0.6039
3904/5677 [===================>..........] - ETA: 3:18 - loss: 0.6666 - acc: 0.6022
3968/5677 [===================>..........] - ETA: 3:11 - loss: 0.6660 - acc: 0.6028
4032/5677 [====================>.........] - ETA: 3:04 - loss: 0.6657 - acc: 0.6042
4096/5677 [====================>.........] - ETA: 2:56 - loss: 0.6647 - acc: 0.6064
4160/5677 [====================>.........] - ETA: 2:49 - loss: 0.6637 - acc: 0.6079
4224/5677 [=====================>........] - ETA: 2:42 - loss: 0.6638 - acc: 0.6082
4288/5677 [=====================>........] - ETA: 2:35 - loss: 0.6631 - acc: 0.6096
4352/5677 [=====================>........] - ETA: 2:28 - loss: 0.6639 - acc: 0.6080
4416/5677 [======================>.......] - ETA: 2:20 - loss: 0.6641 - acc: 0.6076
4480/5677 [======================>.......] - ETA: 2:13 - loss: 0.6635 - acc: 0.6087
4544/5677 [=======================>......] - ETA: 2:06 - loss: 0.6644 - acc: 0.6065
4608/5677 [=======================>......] - ETA: 1:59 - loss: 0.6643 - acc: 0.6074
4672/5677 [=======================>......] - ETA: 1:52 - loss: 0.6638 - acc: 0.6083
4736/5677 [========================>.....] - ETA: 1:45 - loss: 0.6638 - acc: 0.6083
4800/5677 [========================>.....] - ETA: 1:38 - loss: 0.6640 - acc: 0.6077
4864/5677 [========================>.....] - ETA: 1:30 - loss: 0.6646 - acc: 0.6071
4928/5677 [=========================>....] - ETA: 1:23 - loss: 0.6661 - acc: 0.6053
4992/5677 [=========================>....] - ETA: 1:16 - loss: 0.6656 - acc: 0.6064
5056/5677 [=========================>....] - ETA: 1:09 - loss: 0.6659 - acc: 0.6064
5120/5677 [==========================>...] - ETA: 1:02 - loss: 0.6655 - acc: 0.6074
5184/5677 [==========================>...] - ETA: 55s - loss: 0.6660 - acc: 0.6069 
5248/5677 [==========================>...] - ETA: 47s - loss: 0.6663 - acc: 0.6059
5312/5677 [===========================>..] - ETA: 40s - loss: 0.6667 - acc: 0.6054
5376/5677 [===========================>..] - ETA: 33s - loss: 0.6671 - acc: 0.6051
5440/5677 [===========================>..] - ETA: 26s - loss: 0.6676 - acc: 0.6044
5504/5677 [============================>.] - ETA: 19s - loss: 0.6677 - acc: 0.6041
5568/5677 [============================>.] - ETA: 12s - loss: 0.6675 - acc: 0.6045
5632/5677 [============================>.] - ETA: 5s - loss: 0.6674 - acc: 0.6046 
5677/5677 [==============================] - 659s 116ms/step - loss: 0.6671 - acc: 0.6051 - val_loss: 0.6674 - val_acc: 0.5959

Epoch 00010: val_acc improved from 0.57686 to 0.59588, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window12/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa5ef9b6ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa5ef9b6ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa622f45c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa622f45c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622bf6090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622bf6090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5cd470f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5cd470f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5cd4a0790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5cd4a0790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd3ff290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd3ff290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5cd470e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5cd470e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bcf49310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bcf49310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5cd48cf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5cd48cf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5cd2b4f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5cd2b4f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd4a0a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd4a0a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5cd2a93d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5cd2a93d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd380ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd380ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e84376190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e84376190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d90747ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d90747ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd0b9a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd0b9a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e84376650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e84376650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d90771ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d90771ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5bcfc5950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5bcfc5950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5bcc38110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5bcc38110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bcb4fd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bcb4fd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5cd043c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5cd043c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bcc7bad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bcc7bad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5bc9fb810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5bc9fb810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5bc9c4ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5bc9c4ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bca46ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bca46ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5bc9fbc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5bc9fbc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bc915210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bc915210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5b4702510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5b4702510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5b45f2d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5b45f2d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b46f8090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b46f8090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5b47022d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5b47022d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b460a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b460a1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5b43e5dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5b43e5dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5b4275c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5b4275c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b47317d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b47317d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5b43ddb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5b43ddb10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b43217d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b43217d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5b40c3790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5b40c3790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5b4062190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5b4062190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b41c7c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b41c7c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5b40c3ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5b40c3ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b4106510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5b4106510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5abd87c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5abd87c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5abd57a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5abd57a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5abfb7ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5abfb7ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5b3fe15d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5b3fe15d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5abdac950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5abdac950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5abcf2c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5abcf2c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5aba725d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa5aba725d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5ab98c3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5ab98c3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5abcf2050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5abcf2050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5aba94510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5aba94510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5aba16b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5aba16b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa218111b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa218111b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bc96c310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5bc96c310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5ab80d610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5ab80d610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a3717450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5a3717450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa2181e5990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa2181e5990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9fb0077e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9fb0077e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fb006d150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fb006d150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fb020b5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fb020b5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fa077c610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9fa077c610>>: AttributeError: module 'gast' has no attribute 'Str'
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 3:38
 128/1578 [=>............................] - ETA: 2:17
 192/1578 [==>...........................] - ETA: 1:47
 256/1578 [===>..........................] - ETA: 1:29
 320/1578 [=====>........................] - ETA: 1:18
 384/1578 [======>.......................] - ETA: 1:14
 448/1578 [=======>......................] - ETA: 1:07
 512/1578 [========>.....................] - ETA: 1:01
 576/1578 [=========>....................] - ETA: 55s 
 640/1578 [===========>..................] - ETA: 50s
 704/1578 [============>.................] - ETA: 45s
 768/1578 [=============>................] - ETA: 42s
 832/1578 [==============>...............] - ETA: 38s
 896/1578 [================>.............] - ETA: 34s
 960/1578 [=================>............] - ETA: 31s
1024/1578 [==================>...........] - ETA: 27s
1088/1578 [===================>..........] - ETA: 23s
1152/1578 [====================>.........] - ETA: 20s
1216/1578 [======================>.......] - ETA: 17s
1280/1578 [=======================>......] - ETA: 14s
1344/1578 [========================>.....] - ETA: 11s
1408/1578 [=========================>....] - ETA: 8s 
1472/1578 [==========================>...] - ETA: 5s
1536/1578 [============================>.] - ETA: 1s
1578/1578 [==============================] - 75s 48ms/step
loss: 0.6567195864684682
acc: 0.610899873710555
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9de87bd250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9de87bd250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9d906fe190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9d906fe190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622c069d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622c069d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f3c711a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f3c711a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3c72c5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3c72c5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd608050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd608050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f3c711290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f3c711290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd613750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd613750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5ef95fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5ef95fe10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d5c394a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d5c394a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa600e9b210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa600e9b210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5ef95f250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa5ef95f250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d5c38ab10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d5c38ab10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5cd5fcb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa5cd5fcb50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d90506150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d90506150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d90522550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d90522550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fa0414590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9fa0414590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d9055df50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d9055df50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d904b8e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d904b8e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d901df810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d901df810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d901c7490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d901c7490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d90225690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d90225690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d706d1450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d706d1450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d706a1dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d706a1dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d706cc590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d706cc590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d705fbe50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d705fbe50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d90560950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d90560950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d70552e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d70552e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d705a6d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d705a6d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d70328810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d70328810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d702b6590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d702b6590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d705a6fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d705a6fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d70342bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d70342bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d7006e810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d7006e810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d686ddf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d686ddf10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d68714650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d68714650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d90145610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d90145610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d685bfc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d685bfc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d6878cdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d6878cdd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d6837fe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d6837fe90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d68434750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d68434750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d6878c850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d6878c850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d684a65d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d684a65d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d68730d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d68730d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d6815c550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d6815c550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d68214d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d68214d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d68730e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d68730e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d68216ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d68216ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d5c695650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d5c695650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d5c647410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d5c647410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d680c5a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d680c5a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d681d1e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d681d1e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d5c647050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d5c647050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d5c453510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d5c453510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d5c465f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d5c465f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d680da250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d680da250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf03b5190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf03b5190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf014d810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf014d810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9cf01f8b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9cf01f8b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ce87a5350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ce87a5350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf03d5190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf03d5190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d5c4509d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d5c4509d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf02c3d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf02c3d50>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 1:20:34 - loss: 0.7447 - acc: 0.5000
 128/5677 [..............................] - ETA: 45:19 - loss: 0.7043 - acc: 0.5703  
 192/5677 [>.............................] - ETA: 33:23 - loss: 0.7518 - acc: 0.4948
 256/5677 [>.............................] - ETA: 27:41 - loss: 0.7419 - acc: 0.4805
 320/5677 [>.............................] - ETA: 24:17 - loss: 0.7225 - acc: 0.5219
 384/5677 [=>............................] - ETA: 22:04 - loss: 0.7295 - acc: 0.5182
 448/5677 [=>............................] - ETA: 20:22 - loss: 0.7421 - acc: 0.5022
 512/5677 [=>............................] - ETA: 18:53 - loss: 0.7382 - acc: 0.5039
 576/5677 [==>...........................] - ETA: 17:51 - loss: 0.7287 - acc: 0.5156
 640/5677 [==>...........................] - ETA: 16:50 - loss: 0.7365 - acc: 0.5094
 704/5677 [==>...........................] - ETA: 16:00 - loss: 0.7322 - acc: 0.5114
 768/5677 [===>..........................] - ETA: 15:14 - loss: 0.7337 - acc: 0.5104
 832/5677 [===>..........................] - ETA: 14:34 - loss: 0.7309 - acc: 0.5108
 896/5677 [===>..........................] - ETA: 14:00 - loss: 0.7285 - acc: 0.5156
 960/5677 [====>.........................] - ETA: 13:28 - loss: 0.7305 - acc: 0.5104
1024/5677 [====>.........................] - ETA: 12:59 - loss: 0.7315 - acc: 0.5088
1088/5677 [====>.........................] - ETA: 12:33 - loss: 0.7347 - acc: 0.5046
1152/5677 [=====>........................] - ETA: 12:10 - loss: 0.7337 - acc: 0.5069
1216/5677 [=====>........................] - ETA: 11:50 - loss: 0.7325 - acc: 0.5107
1280/5677 [=====>........................] - ETA: 11:28 - loss: 0.7307 - acc: 0.5141
1344/5677 [======>.......................] - ETA: 11:09 - loss: 0.7287 - acc: 0.5141
1408/5677 [======>.......................] - ETA: 10:53 - loss: 0.7275 - acc: 0.5156
1472/5677 [======>.......................] - ETA: 10:36 - loss: 0.7252 - acc: 0.5190
1536/5677 [=======>......................] - ETA: 10:18 - loss: 0.7276 - acc: 0.5124
1600/5677 [=======>......................] - ETA: 10:02 - loss: 0.7266 - acc: 0.5138
1664/5677 [=======>......................] - ETA: 9:46 - loss: 0.7275 - acc: 0.5126 
1728/5677 [========>.....................] - ETA: 9:32 - loss: 0.7284 - acc: 0.5104
1792/5677 [========>.....................] - ETA: 9:18 - loss: 0.7291 - acc: 0.5084
1856/5677 [========>.....................] - ETA: 9:05 - loss: 0.7275 - acc: 0.5097
1920/5677 [=========>....................] - ETA: 8:52 - loss: 0.7260 - acc: 0.5109
1984/5677 [=========>....................] - ETA: 8:39 - loss: 0.7237 - acc: 0.5146
2048/5677 [=========>....................] - ETA: 8:27 - loss: 0.7232 - acc: 0.5142
2112/5677 [==========>...................] - ETA: 8:15 - loss: 0.7238 - acc: 0.5133
2176/5677 [==========>...................] - ETA: 8:04 - loss: 0.7247 - acc: 0.5133
2240/5677 [==========>...................] - ETA: 7:54 - loss: 0.7235 - acc: 0.5134
2304/5677 [===========>..................] - ETA: 7:44 - loss: 0.7224 - acc: 0.5139
2368/5677 [===========>..................] - ETA: 7:34 - loss: 0.7239 - acc: 0.5097
2432/5677 [===========>..................] - ETA: 7:24 - loss: 0.7238 - acc: 0.5107
2496/5677 [============>.................] - ETA: 7:13 - loss: 0.7235 - acc: 0.5100
2560/5677 [============>.................] - ETA: 7:03 - loss: 0.7232 - acc: 0.5105
2624/5677 [============>.................] - ETA: 6:52 - loss: 0.7219 - acc: 0.5141
2688/5677 [=============>................] - ETA: 6:42 - loss: 0.7209 - acc: 0.5160
2752/5677 [=============>................] - ETA: 6:32 - loss: 0.7213 - acc: 0.5138
2816/5677 [=============>................] - ETA: 6:22 - loss: 0.7222 - acc: 0.5117
2880/5677 [==============>...............] - ETA: 6:12 - loss: 0.7224 - acc: 0.5097
2944/5677 [==============>...............] - ETA: 6:02 - loss: 0.7225 - acc: 0.5082
3008/5677 [==============>...............] - ETA: 5:53 - loss: 0.7214 - acc: 0.5106
3072/5677 [===============>..............] - ETA: 5:43 - loss: 0.7213 - acc: 0.5107
3136/5677 [===============>..............] - ETA: 5:33 - loss: 0.7208 - acc: 0.5115
3200/5677 [===============>..............] - ETA: 5:24 - loss: 0.7208 - acc: 0.5119
3264/5677 [================>.............] - ETA: 5:16 - loss: 0.7211 - acc: 0.5110
3328/5677 [================>.............] - ETA: 5:07 - loss: 0.7211 - acc: 0.5111
3392/5677 [================>.............] - ETA: 4:58 - loss: 0.7211 - acc: 0.5124
3456/5677 [=================>............] - ETA: 4:49 - loss: 0.7213 - acc: 0.5119
3520/5677 [=================>............] - ETA: 4:40 - loss: 0.7207 - acc: 0.5122
3584/5677 [=================>............] - ETA: 4:32 - loss: 0.7201 - acc: 0.5120
3648/5677 [==================>...........] - ETA: 4:23 - loss: 0.7192 - acc: 0.5129
3712/5677 [==================>...........] - ETA: 4:14 - loss: 0.7186 - acc: 0.5132
3776/5677 [==================>...........] - ETA: 4:05 - loss: 0.7183 - acc: 0.5132
3840/5677 [===================>..........] - ETA: 3:56 - loss: 0.7176 - acc: 0.5135
3904/5677 [===================>..........] - ETA: 3:48 - loss: 0.7176 - acc: 0.5138
3968/5677 [===================>..........] - ETA: 3:39 - loss: 0.7180 - acc: 0.5116
4032/5677 [====================>.........] - ETA: 3:31 - loss: 0.7184 - acc: 0.5087
4096/5677 [====================>.........] - ETA: 3:22 - loss: 0.7179 - acc: 0.5098
4160/5677 [====================>.........] - ETA: 3:14 - loss: 0.7175 - acc: 0.5108
4224/5677 [=====================>........] - ETA: 3:05 - loss: 0.7177 - acc: 0.5109
4288/5677 [=====================>........] - ETA: 2:57 - loss: 0.7174 - acc: 0.5107
4352/5677 [=====================>........] - ETA: 2:49 - loss: 0.7177 - acc: 0.5106
4416/5677 [======================>.......] - ETA: 2:40 - loss: 0.7179 - acc: 0.5091
4480/5677 [======================>.......] - ETA: 2:32 - loss: 0.7178 - acc: 0.5094
4544/5677 [=======================>......] - ETA: 2:24 - loss: 0.7168 - acc: 0.5101
4608/5677 [=======================>......] - ETA: 2:15 - loss: 0.7166 - acc: 0.5104
4672/5677 [=======================>......] - ETA: 2:07 - loss: 0.7167 - acc: 0.5098
4736/5677 [========================>.....] - ETA: 1:59 - loss: 0.7161 - acc: 0.5108
4800/5677 [========================>.....] - ETA: 1:51 - loss: 0.7159 - acc: 0.5112
4864/5677 [========================>.....] - ETA: 1:42 - loss: 0.7154 - acc: 0.5115
4928/5677 [=========================>....] - ETA: 1:34 - loss: 0.7150 - acc: 0.5132
4992/5677 [=========================>....] - ETA: 1:26 - loss: 0.7141 - acc: 0.5146
5056/5677 [=========================>....] - ETA: 1:18 - loss: 0.7137 - acc: 0.5154
5120/5677 [==========================>...] - ETA: 1:10 - loss: 0.7139 - acc: 0.5148
5184/5677 [==========================>...] - ETA: 1:02 - loss: 0.7143 - acc: 0.5139
5248/5677 [==========================>...] - ETA: 53s - loss: 0.7146 - acc: 0.5128 
5312/5677 [===========================>..] - ETA: 45s - loss: 0.7141 - acc: 0.5139
5376/5677 [===========================>..] - ETA: 37s - loss: 0.7140 - acc: 0.5147
5440/5677 [===========================>..] - ETA: 29s - loss: 0.7138 - acc: 0.5143
5504/5677 [============================>.] - ETA: 21s - loss: 0.7136 - acc: 0.5147
5568/5677 [============================>.] - ETA: 13s - loss: 0.7136 - acc: 0.5140
5632/5677 [============================>.] - ETA: 5s - loss: 0.7135 - acc: 0.5140 
5677/5677 [==============================] - 748s 132ms/step - loss: 0.7135 - acc: 0.5138 - val_loss: 0.6847 - val_acc: 0.5705

Epoch 00001: val_acc improved from -inf to 0.57052, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window13/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 13:59 - loss: 0.7085 - acc: 0.5156
 128/5677 [..............................] - ETA: 12:15 - loss: 0.7069 - acc: 0.5312
 192/5677 [>.............................] - ETA: 11:52 - loss: 0.7129 - acc: 0.5260
 256/5677 [>.............................] - ETA: 11:39 - loss: 0.7148 - acc: 0.5078
 320/5677 [>.............................] - ETA: 11:35 - loss: 0.7149 - acc: 0.5000
 384/5677 [=>............................] - ETA: 11:20 - loss: 0.7149 - acc: 0.4922
 448/5677 [=>............................] - ETA: 11:05 - loss: 0.7116 - acc: 0.5000
 512/5677 [=>............................] - ETA: 11:05 - loss: 0.7091 - acc: 0.5078
 576/5677 [==>...........................] - ETA: 11:03 - loss: 0.7044 - acc: 0.5243
 640/5677 [==>...........................] - ETA: 10:59 - loss: 0.7032 - acc: 0.5312
 704/5677 [==>...........................] - ETA: 10:40 - loss: 0.7047 - acc: 0.5256
 768/5677 [===>..........................] - ETA: 10:28 - loss: 0.7037 - acc: 0.5247
 832/5677 [===>..........................] - ETA: 10:12 - loss: 0.7042 - acc: 0.5240
 896/5677 [===>..........................] - ETA: 10:05 - loss: 0.7085 - acc: 0.5112
 960/5677 [====>.........................] - ETA: 10:05 - loss: 0.7091 - acc: 0.5115
1024/5677 [====>.........................] - ETA: 9:54 - loss: 0.7097 - acc: 0.5127 
1088/5677 [====>.........................] - ETA: 9:44 - loss: 0.7081 - acc: 0.5119
1152/5677 [=====>........................] - ETA: 9:30 - loss: 0.7076 - acc: 0.5139
1216/5677 [=====>........................] - ETA: 9:22 - loss: 0.7096 - acc: 0.5066
1280/5677 [=====>........................] - ETA: 9:12 - loss: 0.7089 - acc: 0.5047
1344/5677 [======>.......................] - ETA: 9:03 - loss: 0.7075 - acc: 0.5060
1408/5677 [======>.......................] - ETA: 8:51 - loss: 0.7058 - acc: 0.5114
1472/5677 [======>.......................] - ETA: 8:41 - loss: 0.7035 - acc: 0.5177
1536/5677 [=======>......................] - ETA: 8:32 - loss: 0.7021 - acc: 0.5215
1600/5677 [=======>......................] - ETA: 8:25 - loss: 0.7002 - acc: 0.5250
1664/5677 [=======>......................] - ETA: 8:20 - loss: 0.7011 - acc: 0.5216
1728/5677 [========>.....................] - ETA: 8:15 - loss: 0.7001 - acc: 0.5220
1792/5677 [========>.....................] - ETA: 8:07 - loss: 0.6996 - acc: 0.5229
1856/5677 [========>.....................] - ETA: 7:55 - loss: 0.6993 - acc: 0.5226
1920/5677 [=========>....................] - ETA: 7:47 - loss: 0.7010 - acc: 0.5188
1984/5677 [=========>....................] - ETA: 7:37 - loss: 0.7000 - acc: 0.5202
2048/5677 [=========>....................] - ETA: 7:30 - loss: 0.6999 - acc: 0.5186
2112/5677 [==========>...................] - ETA: 7:25 - loss: 0.7002 - acc: 0.5170
2176/5677 [==========>...................] - ETA: 7:17 - loss: 0.6983 - acc: 0.5221
2240/5677 [==========>...................] - ETA: 7:09 - loss: 0.6991 - acc: 0.5205
2304/5677 [===========>..................] - ETA: 7:01 - loss: 0.6988 - acc: 0.5213
2368/5677 [===========>..................] - ETA: 6:51 - loss: 0.6980 - acc: 0.5232
2432/5677 [===========>..................] - ETA: 6:42 - loss: 0.6981 - acc: 0.5226
2496/5677 [============>.................] - ETA: 6:34 - loss: 0.6982 - acc: 0.5236
2560/5677 [============>.................] - ETA: 6:26 - loss: 0.6987 - acc: 0.5250
2624/5677 [============>.................] - ETA: 6:17 - loss: 0.6990 - acc: 0.5232
2688/5677 [=============>................] - ETA: 6:10 - loss: 0.6991 - acc: 0.5234
2752/5677 [=============>................] - ETA: 6:03 - loss: 0.6982 - acc: 0.5243
2816/5677 [=============>................] - ETA: 5:54 - loss: 0.6986 - acc: 0.5231
2880/5677 [==============>...............] - ETA: 5:47 - loss: 0.6979 - acc: 0.5233
2944/5677 [==============>...............] - ETA: 5:38 - loss: 0.6974 - acc: 0.5241
3008/5677 [==============>...............] - ETA: 5:29 - loss: 0.6971 - acc: 0.5249
3072/5677 [===============>..............] - ETA: 5:21 - loss: 0.6975 - acc: 0.5238
3136/5677 [===============>..............] - ETA: 5:13 - loss: 0.6978 - acc: 0.5226
3200/5677 [===============>..............] - ETA: 5:04 - loss: 0.6979 - acc: 0.5234
3264/5677 [================>.............] - ETA: 4:56 - loss: 0.6981 - acc: 0.5233
3328/5677 [================>.............] - ETA: 4:49 - loss: 0.6981 - acc: 0.5234
3392/5677 [================>.............] - ETA: 4:42 - loss: 0.6991 - acc: 0.5227
3456/5677 [=================>............] - ETA: 4:34 - loss: 0.7000 - acc: 0.5220
3520/5677 [=================>............] - ETA: 4:26 - loss: 0.6994 - acc: 0.5230
3584/5677 [=================>............] - ETA: 4:19 - loss: 0.6996 - acc: 0.5212
3648/5677 [==================>...........] - ETA: 4:11 - loss: 0.6999 - acc: 0.5203
3712/5677 [==================>...........] - ETA: 4:02 - loss: 0.6999 - acc: 0.5202
3776/5677 [==================>...........] - ETA: 3:54 - loss: 0.6999 - acc: 0.5193
3840/5677 [===================>..........] - ETA: 3:46 - loss: 0.6993 - acc: 0.5203
3904/5677 [===================>..........] - ETA: 3:38 - loss: 0.6993 - acc: 0.5202
3968/5677 [===================>..........] - ETA: 3:30 - loss: 0.6991 - acc: 0.5212
4032/5677 [====================>.........] - ETA: 3:23 - loss: 0.6991 - acc: 0.5213
4096/5677 [====================>.........] - ETA: 3:15 - loss: 0.6991 - acc: 0.5210
4160/5677 [====================>.........] - ETA: 3:07 - loss: 0.6996 - acc: 0.5209
4224/5677 [=====================>........] - ETA: 3:00 - loss: 0.6995 - acc: 0.5206
4288/5677 [=====================>........] - ETA: 2:52 - loss: 0.6994 - acc: 0.5212
4352/5677 [=====================>........] - ETA: 2:44 - loss: 0.6990 - acc: 0.5216
4416/5677 [======================>.......] - ETA: 2:36 - loss: 0.6991 - acc: 0.5211
4480/5677 [======================>.......] - ETA: 2:29 - loss: 0.6995 - acc: 0.5212
4544/5677 [=======================>......] - ETA: 2:21 - loss: 0.6991 - acc: 0.5211
4608/5677 [=======================>......] - ETA: 2:13 - loss: 0.6994 - acc: 0.5213
4672/5677 [=======================>......] - ETA: 2:05 - loss: 0.6993 - acc: 0.5216
4736/5677 [========================>.....] - ETA: 1:57 - loss: 0.6994 - acc: 0.5215
4800/5677 [========================>.....] - ETA: 1:49 - loss: 0.6996 - acc: 0.5206
4864/5677 [========================>.....] - ETA: 1:41 - loss: 0.6994 - acc: 0.5210
4928/5677 [=========================>....] - ETA: 1:33 - loss: 0.6994 - acc: 0.5215
4992/5677 [=========================>....] - ETA: 1:25 - loss: 0.6992 - acc: 0.5222
5056/5677 [=========================>....] - ETA: 1:17 - loss: 0.6992 - acc: 0.5227
5120/5677 [==========================>...] - ETA: 1:09 - loss: 0.6991 - acc: 0.5221
5184/5677 [==========================>...] - ETA: 1:01 - loss: 0.6989 - acc: 0.5222
5248/5677 [==========================>...] - ETA: 53s - loss: 0.6990 - acc: 0.5223 
5312/5677 [===========================>..] - ETA: 45s - loss: 0.6985 - acc: 0.5226
5376/5677 [===========================>..] - ETA: 37s - loss: 0.6987 - acc: 0.5223
5440/5677 [===========================>..] - ETA: 29s - loss: 0.6985 - acc: 0.5221
5504/5677 [============================>.] - ETA: 21s - loss: 0.6983 - acc: 0.5229
5568/5677 [============================>.] - ETA: 13s - loss: 0.6977 - acc: 0.5246
5632/5677 [============================>.] - ETA: 5s - loss: 0.6976 - acc: 0.5247 
5677/5677 [==============================] - 737s 130ms/step - loss: 0.6977 - acc: 0.5240 - val_loss: 0.6861 - val_acc: 0.5277

Epoch 00002: val_acc did not improve from 0.57052
Epoch 3/10

  64/5677 [..............................] - ETA: 11:52 - loss: 0.6673 - acc: 0.6406
 128/5677 [..............................] - ETA: 11:09 - loss: 0.6631 - acc: 0.6484
 192/5677 [>.............................] - ETA: 11:04 - loss: 0.6658 - acc: 0.6198
 256/5677 [>.............................] - ETA: 10:51 - loss: 0.6859 - acc: 0.5781
 320/5677 [>.............................] - ETA: 10:36 - loss: 0.6853 - acc: 0.5719
 384/5677 [=>............................] - ETA: 10:27 - loss: 0.6889 - acc: 0.5703
 448/5677 [=>............................] - ETA: 10:20 - loss: 0.6814 - acc: 0.5804
 512/5677 [=>............................] - ETA: 10:07 - loss: 0.6810 - acc: 0.5898
 576/5677 [==>...........................] - ETA: 10:07 - loss: 0.6890 - acc: 0.5781
 640/5677 [==>...........................] - ETA: 10:07 - loss: 0.6890 - acc: 0.5766
 704/5677 [==>...........................] - ETA: 10:04 - loss: 0.6892 - acc: 0.5696
 768/5677 [===>..........................] - ETA: 10:01 - loss: 0.6871 - acc: 0.5716
 832/5677 [===>..........................] - ETA: 10:01 - loss: 0.6879 - acc: 0.5673
 896/5677 [===>..........................] - ETA: 9:55 - loss: 0.6880 - acc: 0.5670 
 960/5677 [====>.........................] - ETA: 9:46 - loss: 0.6895 - acc: 0.5646
1024/5677 [====>.........................] - ETA: 9:41 - loss: 0.6883 - acc: 0.5693
1088/5677 [====>.........................] - ETA: 9:37 - loss: 0.6882 - acc: 0.5708
1152/5677 [=====>........................] - ETA: 9:32 - loss: 0.6874 - acc: 0.5720
1216/5677 [=====>........................] - ETA: 9:28 - loss: 0.6882 - acc: 0.5715
1280/5677 [=====>........................] - ETA: 9:21 - loss: 0.6866 - acc: 0.5758
1344/5677 [======>.......................] - ETA: 9:14 - loss: 0.6848 - acc: 0.5766
1408/5677 [======>.......................] - ETA: 9:05 - loss: 0.6843 - acc: 0.5724
1472/5677 [======>.......................] - ETA: 8:56 - loss: 0.6864 - acc: 0.5666
1536/5677 [=======>......................] - ETA: 8:52 - loss: 0.6871 - acc: 0.5658
1600/5677 [=======>......................] - ETA: 8:43 - loss: 0.6891 - acc: 0.5613
1664/5677 [=======>......................] - ETA: 8:39 - loss: 0.6895 - acc: 0.5613
1728/5677 [========>.....................] - ETA: 8:34 - loss: 0.6901 - acc: 0.5625
1792/5677 [========>.....................] - ETA: 8:27 - loss: 0.6907 - acc: 0.5608
1856/5677 [========>.....................] - ETA: 8:19 - loss: 0.6928 - acc: 0.5571
1920/5677 [=========>....................] - ETA: 8:12 - loss: 0.6921 - acc: 0.5563
1984/5677 [=========>....................] - ETA: 8:03 - loss: 0.6919 - acc: 0.5549
2048/5677 [=========>....................] - ETA: 7:54 - loss: 0.6913 - acc: 0.5581
2112/5677 [==========>...................] - ETA: 7:47 - loss: 0.6923 - acc: 0.5568
2176/5677 [==========>...................] - ETA: 7:39 - loss: 0.6943 - acc: 0.5524
2240/5677 [==========>...................] - ETA: 7:31 - loss: 0.6948 - acc: 0.5513
2304/5677 [===========>..................] - ETA: 7:22 - loss: 0.6935 - acc: 0.5534
2368/5677 [===========>..................] - ETA: 7:14 - loss: 0.6921 - acc: 0.5553
2432/5677 [===========>..................] - ETA: 7:06 - loss: 0.6932 - acc: 0.5543
2496/5677 [============>.................] - ETA: 6:58 - loss: 0.6930 - acc: 0.5545
2560/5677 [============>.................] - ETA: 6:49 - loss: 0.6915 - acc: 0.5570
2624/5677 [============>.................] - ETA: 6:40 - loss: 0.6908 - acc: 0.5583
2688/5677 [=============>................] - ETA: 6:32 - loss: 0.6908 - acc: 0.5588
2752/5677 [=============>................] - ETA: 6:23 - loss: 0.6905 - acc: 0.5589
2816/5677 [=============>................] - ETA: 6:14 - loss: 0.6900 - acc: 0.5593
2880/5677 [==============>...............] - ETA: 6:06 - loss: 0.6902 - acc: 0.5597
2944/5677 [==============>...............] - ETA: 5:57 - loss: 0.6909 - acc: 0.5594
3008/5677 [==============>...............] - ETA: 5:48 - loss: 0.6915 - acc: 0.5595
3072/5677 [===============>..............] - ETA: 5:40 - loss: 0.6915 - acc: 0.5576
3136/5677 [===============>..............] - ETA: 5:31 - loss: 0.6912 - acc: 0.5568
3200/5677 [===============>..............] - ETA: 5:23 - loss: 0.6915 - acc: 0.5563
3264/5677 [================>.............] - ETA: 5:14 - loss: 0.6916 - acc: 0.5555
3328/5677 [================>.............] - ETA: 5:06 - loss: 0.6914 - acc: 0.5568
3392/5677 [================>.............] - ETA: 4:57 - loss: 0.6917 - acc: 0.5563
3456/5677 [=================>............] - ETA: 4:49 - loss: 0.6917 - acc: 0.5556
3520/5677 [=================>............] - ETA: 4:41 - loss: 0.6917 - acc: 0.5565
3584/5677 [=================>............] - ETA: 4:32 - loss: 0.6919 - acc: 0.5552
3648/5677 [==================>...........] - ETA: 4:24 - loss: 0.6922 - acc: 0.5559
3712/5677 [==================>...........] - ETA: 4:16 - loss: 0.6922 - acc: 0.5552
3776/5677 [==================>...........] - ETA: 4:08 - loss: 0.6919 - acc: 0.5553
3840/5677 [===================>..........] - ETA: 3:59 - loss: 0.6911 - acc: 0.5576
3904/5677 [===================>..........] - ETA: 3:51 - loss: 0.6910 - acc: 0.5574
3968/5677 [===================>..........] - ETA: 3:43 - loss: 0.6914 - acc: 0.5570
4032/5677 [====================>.........] - ETA: 3:34 - loss: 0.6916 - acc: 0.5563
4096/5677 [====================>.........] - ETA: 3:26 - loss: 0.6913 - acc: 0.5579
4160/5677 [====================>.........] - ETA: 3:17 - loss: 0.6911 - acc: 0.5582
4224/5677 [=====================>........] - ETA: 3:09 - loss: 0.6916 - acc: 0.5573
4288/5677 [=====================>........] - ETA: 3:00 - loss: 0.6914 - acc: 0.5562
4352/5677 [=====================>........] - ETA: 2:52 - loss: 0.6909 - acc: 0.5565
4416/5677 [======================>.......] - ETA: 2:44 - loss: 0.6917 - acc: 0.5566
4480/5677 [======================>.......] - ETA: 2:35 - loss: 0.6917 - acc: 0.5569
4544/5677 [=======================>......] - ETA: 2:27 - loss: 0.6921 - acc: 0.5561
4608/5677 [=======================>......] - ETA: 2:19 - loss: 0.6919 - acc: 0.5556
4672/5677 [=======================>......] - ETA: 2:10 - loss: 0.6923 - acc: 0.5544
4736/5677 [========================>.....] - ETA: 2:02 - loss: 0.6923 - acc: 0.5536
4800/5677 [========================>.....] - ETA: 1:54 - loss: 0.6926 - acc: 0.5527
4864/5677 [========================>.....] - ETA: 1:45 - loss: 0.6923 - acc: 0.5526
4928/5677 [=========================>....] - ETA: 1:37 - loss: 0.6921 - acc: 0.5530
4992/5677 [=========================>....] - ETA: 1:28 - loss: 0.6921 - acc: 0.5531
5056/5677 [=========================>....] - ETA: 1:20 - loss: 0.6922 - acc: 0.5524
5120/5677 [==========================>...] - ETA: 1:12 - loss: 0.6921 - acc: 0.5529
5184/5677 [==========================>...] - ETA: 1:03 - loss: 0.6919 - acc: 0.5529
5248/5677 [==========================>...] - ETA: 55s - loss: 0.6919 - acc: 0.5524 
5312/5677 [===========================>..] - ETA: 47s - loss: 0.6922 - acc: 0.5505
5376/5677 [===========================>..] - ETA: 38s - loss: 0.6924 - acc: 0.5499
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6923 - acc: 0.5494
5504/5677 [============================>.] - ETA: 22s - loss: 0.6925 - acc: 0.5489
5568/5677 [============================>.] - ETA: 14s - loss: 0.6926 - acc: 0.5489
5632/5677 [============================>.] - ETA: 5s - loss: 0.6921 - acc: 0.5501 
5677/5677 [==============================] - 764s 135ms/step - loss: 0.6919 - acc: 0.5499 - val_loss: 0.7057 - val_acc: 0.5103

Epoch 00003: val_acc did not improve from 0.57052
Epoch 4/10

  64/5677 [..............................] - ETA: 11:45 - loss: 0.6592 - acc: 0.6094
 128/5677 [..............................] - ETA: 12:07 - loss: 0.6757 - acc: 0.5625
 192/5677 [>.............................] - ETA: 12:00 - loss: 0.6759 - acc: 0.5833
 256/5677 [>.............................] - ETA: 11:44 - loss: 0.6795 - acc: 0.5625
 320/5677 [>.............................] - ETA: 11:39 - loss: 0.6858 - acc: 0.5656
 384/5677 [=>............................] - ETA: 11:31 - loss: 0.6905 - acc: 0.5573
 448/5677 [=>............................] - ETA: 11:25 - loss: 0.6847 - acc: 0.5692
 512/5677 [=>............................] - ETA: 11:22 - loss: 0.6844 - acc: 0.5703
 576/5677 [==>...........................] - ETA: 11:18 - loss: 0.6890 - acc: 0.5590
 640/5677 [==>...........................] - ETA: 11:15 - loss: 0.6881 - acc: 0.5625
 704/5677 [==>...........................] - ETA: 11:07 - loss: 0.6881 - acc: 0.5653
 768/5677 [===>..........................] - ETA: 10:55 - loss: 0.6861 - acc: 0.5612
 832/5677 [===>..........................] - ETA: 10:44 - loss: 0.6862 - acc: 0.5613
 896/5677 [===>..........................] - ETA: 10:41 - loss: 0.6862 - acc: 0.5580
 960/5677 [====>.........................] - ETA: 10:33 - loss: 0.6886 - acc: 0.5531
1024/5677 [====>.........................] - ETA: 10:28 - loss: 0.6893 - acc: 0.5498
1088/5677 [====>.........................] - ETA: 10:15 - loss: 0.6922 - acc: 0.5478
1152/5677 [=====>........................] - ETA: 10:08 - loss: 0.6923 - acc: 0.5486
1216/5677 [=====>........................] - ETA: 9:57 - loss: 0.6937 - acc: 0.5436 
1280/5677 [=====>........................] - ETA: 9:46 - loss: 0.6905 - acc: 0.5508
1344/5677 [======>.......................] - ETA: 9:39 - loss: 0.6917 - acc: 0.5491
1408/5677 [======>.......................] - ETA: 9:29 - loss: 0.6930 - acc: 0.5455
1472/5677 [======>.......................] - ETA: 9:20 - loss: 0.6933 - acc: 0.5462
1536/5677 [=======>......................] - ETA: 9:12 - loss: 0.6915 - acc: 0.5521
1600/5677 [=======>......................] - ETA: 9:05 - loss: 0.6910 - acc: 0.5519
1664/5677 [=======>......................] - ETA: 8:57 - loss: 0.6912 - acc: 0.5511
1728/5677 [========>.....................] - ETA: 8:46 - loss: 0.6915 - acc: 0.5509
1792/5677 [========>.....................] - ETA: 8:37 - loss: 0.6902 - acc: 0.5508
1856/5677 [========>.....................] - ETA: 8:27 - loss: 0.6890 - acc: 0.5555
1920/5677 [=========>....................] - ETA: 8:21 - loss: 0.6887 - acc: 0.5568
1984/5677 [=========>....................] - ETA: 8:14 - loss: 0.6886 - acc: 0.5565
2048/5677 [=========>....................] - ETA: 8:06 - loss: 0.6885 - acc: 0.5562
2112/5677 [==========>...................] - ETA: 7:58 - loss: 0.6887 - acc: 0.5559
2176/5677 [==========>...................] - ETA: 7:51 - loss: 0.6897 - acc: 0.5538
2240/5677 [==========>...................] - ETA: 7:43 - loss: 0.6902 - acc: 0.5527
2304/5677 [===========>..................] - ETA: 7:36 - loss: 0.6893 - acc: 0.5538
2368/5677 [===========>..................] - ETA: 7:27 - loss: 0.6894 - acc: 0.5528
2432/5677 [===========>..................] - ETA: 7:17 - loss: 0.6894 - acc: 0.5514
2496/5677 [============>.................] - ETA: 7:07 - loss: 0.6899 - acc: 0.5489
2560/5677 [============>.................] - ETA: 6:56 - loss: 0.6893 - acc: 0.5523
2624/5677 [============>.................] - ETA: 6:50 - loss: 0.6895 - acc: 0.5518
2688/5677 [=============>................] - ETA: 6:42 - loss: 0.6894 - acc: 0.5502
2752/5677 [=============>................] - ETA: 6:34 - loss: 0.6887 - acc: 0.5523
2816/5677 [=============>................] - ETA: 6:24 - loss: 0.6887 - acc: 0.5526
2880/5677 [==============>...............] - ETA: 6:15 - loss: 0.6888 - acc: 0.5528
2944/5677 [==============>...............] - ETA: 6:06 - loss: 0.6901 - acc: 0.5493
3008/5677 [==============>...............] - ETA: 5:59 - loss: 0.6898 - acc: 0.5505
3072/5677 [===============>..............] - ETA: 5:51 - loss: 0.6908 - acc: 0.5482
3136/5677 [===============>..............] - ETA: 5:43 - loss: 0.6907 - acc: 0.5485
3200/5677 [===============>..............] - ETA: 5:33 - loss: 0.6904 - acc: 0.5478
3264/5677 [================>.............] - ETA: 5:24 - loss: 0.6906 - acc: 0.5478
3328/5677 [================>.............] - ETA: 5:16 - loss: 0.6914 - acc: 0.5454
3392/5677 [================>.............] - ETA: 5:07 - loss: 0.6919 - acc: 0.5433
3456/5677 [=================>............] - ETA: 4:58 - loss: 0.6938 - acc: 0.5408
3520/5677 [=================>............] - ETA: 4:50 - loss: 0.6946 - acc: 0.5403
3584/5677 [=================>............] - ETA: 4:42 - loss: 0.6948 - acc: 0.5396
3648/5677 [==================>...........] - ETA: 4:33 - loss: 0.6942 - acc: 0.5408
3712/5677 [==================>...........] - ETA: 4:24 - loss: 0.6940 - acc: 0.5415
3776/5677 [==================>...........] - ETA: 4:15 - loss: 0.6935 - acc: 0.5434
3840/5677 [===================>..........] - ETA: 4:06 - loss: 0.6933 - acc: 0.5440
3904/5677 [===================>..........] - ETA: 3:57 - loss: 0.6925 - acc: 0.5461
3968/5677 [===================>..........] - ETA: 3:49 - loss: 0.6924 - acc: 0.5461
4032/5677 [====================>.........] - ETA: 3:40 - loss: 0.6922 - acc: 0.5464
4096/5677 [====================>.........] - ETA: 3:32 - loss: 0.6924 - acc: 0.5464
4160/5677 [====================>.........] - ETA: 3:23 - loss: 0.6926 - acc: 0.5450
4224/5677 [=====================>........] - ETA: 3:15 - loss: 0.6925 - acc: 0.5452
4288/5677 [=====================>........] - ETA: 3:06 - loss: 0.6927 - acc: 0.5450
4352/5677 [=====================>........] - ETA: 2:58 - loss: 0.6926 - acc: 0.5446
4416/5677 [======================>.......] - ETA: 2:49 - loss: 0.6929 - acc: 0.5437
4480/5677 [======================>.......] - ETA: 2:40 - loss: 0.6930 - acc: 0.5433
4544/5677 [=======================>......] - ETA: 2:31 - loss: 0.6930 - acc: 0.5429
4608/5677 [=======================>......] - ETA: 2:23 - loss: 0.6929 - acc: 0.5438
4672/5677 [=======================>......] - ETA: 2:14 - loss: 0.6930 - acc: 0.5437
4736/5677 [========================>.....] - ETA: 2:05 - loss: 0.6928 - acc: 0.5448
4800/5677 [========================>.....] - ETA: 1:57 - loss: 0.6931 - acc: 0.5437
4864/5677 [========================>.....] - ETA: 1:49 - loss: 0.6931 - acc: 0.5432
4928/5677 [=========================>....] - ETA: 1:40 - loss: 0.6930 - acc: 0.5444
4992/5677 [=========================>....] - ETA: 1:32 - loss: 0.6927 - acc: 0.5447
5056/5677 [=========================>....] - ETA: 1:23 - loss: 0.6928 - acc: 0.5445
5120/5677 [==========================>...] - ETA: 1:14 - loss: 0.6924 - acc: 0.5449
5184/5677 [==========================>...] - ETA: 1:06 - loss: 0.6922 - acc: 0.5455
5248/5677 [==========================>...] - ETA: 57s - loss: 0.6918 - acc: 0.5455 
5312/5677 [===========================>..] - ETA: 49s - loss: 0.6922 - acc: 0.5452
5376/5677 [===========================>..] - ETA: 40s - loss: 0.6923 - acc: 0.5450
5440/5677 [===========================>..] - ETA: 31s - loss: 0.6923 - acc: 0.5449
5504/5677 [============================>.] - ETA: 23s - loss: 0.6923 - acc: 0.5445
5568/5677 [============================>.] - ETA: 14s - loss: 0.6926 - acc: 0.5433
5632/5677 [============================>.] - ETA: 6s - loss: 0.6923 - acc: 0.5435 
5677/5677 [==============================] - 795s 140ms/step - loss: 0.6922 - acc: 0.5441 - val_loss: 0.6743 - val_acc: 0.5784

Epoch 00004: val_acc improved from 0.57052 to 0.57845, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window13/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 5/10

  64/5677 [..............................] - ETA: 12:55 - loss: 0.6667 - acc: 0.6094
 128/5677 [..............................] - ETA: 12:10 - loss: 0.6749 - acc: 0.6094
 192/5677 [>.............................] - ETA: 12:03 - loss: 0.6726 - acc: 0.6302
 256/5677 [>.............................] - ETA: 12:18 - loss: 0.6804 - acc: 0.5938
 320/5677 [>.............................] - ETA: 12:14 - loss: 0.6810 - acc: 0.5906
 384/5677 [=>............................] - ETA: 11:57 - loss: 0.6800 - acc: 0.5911
 448/5677 [=>............................] - ETA: 11:43 - loss: 0.6801 - acc: 0.5915
 512/5677 [=>............................] - ETA: 11:43 - loss: 0.6779 - acc: 0.5879
 576/5677 [==>...........................] - ETA: 11:38 - loss: 0.6767 - acc: 0.5868
 640/5677 [==>...........................] - ETA: 11:33 - loss: 0.6778 - acc: 0.5781
 704/5677 [==>...........................] - ETA: 11:23 - loss: 0.6802 - acc: 0.5710
 768/5677 [===>..........................] - ETA: 11:12 - loss: 0.6808 - acc: 0.5742
 832/5677 [===>..........................] - ETA: 11:02 - loss: 0.6811 - acc: 0.5721
 896/5677 [===>..........................] - ETA: 10:53 - loss: 0.6829 - acc: 0.5681
 960/5677 [====>.........................] - ETA: 10:48 - loss: 0.6811 - acc: 0.5729
1024/5677 [====>.........................] - ETA: 10:37 - loss: 0.6810 - acc: 0.5742
1088/5677 [====>.........................] - ETA: 10:27 - loss: 0.6819 - acc: 0.5772
1152/5677 [=====>........................] - ETA: 10:15 - loss: 0.6828 - acc: 0.5781
1216/5677 [=====>........................] - ETA: 10:01 - loss: 0.6838 - acc: 0.5781
1280/5677 [=====>........................] - ETA: 9:49 - loss: 0.6834 - acc: 0.5766 
1344/5677 [======>.......................] - ETA: 9:36 - loss: 0.6821 - acc: 0.5774
1408/5677 [======>.......................] - ETA: 9:28 - loss: 0.6821 - acc: 0.5767
1472/5677 [======>.......................] - ETA: 9:18 - loss: 0.6812 - acc: 0.5781
1536/5677 [=======>......................] - ETA: 9:06 - loss: 0.6819 - acc: 0.5807
1600/5677 [=======>......................] - ETA: 8:57 - loss: 0.6820 - acc: 0.5787
1664/5677 [=======>......................] - ETA: 8:47 - loss: 0.6815 - acc: 0.5793
1728/5677 [========>.....................] - ETA: 8:36 - loss: 0.6803 - acc: 0.5816
1792/5677 [========>.....................] - ETA: 8:26 - loss: 0.6796 - acc: 0.5815
1856/5677 [========>.....................] - ETA: 8:14 - loss: 0.6789 - acc: 0.5830
1920/5677 [=========>....................] - ETA: 8:04 - loss: 0.6790 - acc: 0.5823
1984/5677 [=========>....................] - ETA: 7:55 - loss: 0.6784 - acc: 0.5832
2048/5677 [=========>....................] - ETA: 7:46 - loss: 0.6786 - acc: 0.5840
2112/5677 [==========>...................] - ETA: 7:38 - loss: 0.6784 - acc: 0.5838
2176/5677 [==========>...................] - ETA: 7:28 - loss: 0.6791 - acc: 0.5827
2240/5677 [==========>...................] - ETA: 7:19 - loss: 0.6799 - acc: 0.5795
2304/5677 [===========>..................] - ETA: 7:10 - loss: 0.6799 - acc: 0.5807
2368/5677 [===========>..................] - ETA: 7:01 - loss: 0.6788 - acc: 0.5811
2432/5677 [===========>..................] - ETA: 6:52 - loss: 0.6778 - acc: 0.5826
2496/5677 [============>.................] - ETA: 6:44 - loss: 0.6787 - acc: 0.5793
2560/5677 [============>.................] - ETA: 6:35 - loss: 0.6782 - acc: 0.5789
2624/5677 [============>.................] - ETA: 6:27 - loss: 0.6793 - acc: 0.5766
2688/5677 [=============>................] - ETA: 6:18 - loss: 0.6786 - acc: 0.5785
2752/5677 [=============>................] - ETA: 6:09 - loss: 0.6790 - acc: 0.5778
2816/5677 [=============>................] - ETA: 6:01 - loss: 0.6790 - acc: 0.5774
2880/5677 [==============>...............] - ETA: 5:53 - loss: 0.6794 - acc: 0.5753
2944/5677 [==============>...............] - ETA: 5:45 - loss: 0.6798 - acc: 0.5744
3008/5677 [==============>...............] - ETA: 5:38 - loss: 0.6813 - acc: 0.5711
3072/5677 [===============>..............] - ETA: 5:30 - loss: 0.6815 - acc: 0.5697
3136/5677 [===============>..............] - ETA: 5:20 - loss: 0.6809 - acc: 0.5711
3200/5677 [===============>..............] - ETA: 5:12 - loss: 0.6810 - acc: 0.5719
3264/5677 [================>.............] - ETA: 5:04 - loss: 0.6814 - acc: 0.5714
3328/5677 [================>.............] - ETA: 4:56 - loss: 0.6808 - acc: 0.5721
3392/5677 [================>.............] - ETA: 4:47 - loss: 0.6810 - acc: 0.5710
3456/5677 [=================>............] - ETA: 4:39 - loss: 0.6805 - acc: 0.5720
3520/5677 [=================>............] - ETA: 4:31 - loss: 0.6806 - acc: 0.5713
3584/5677 [=================>............] - ETA: 4:22 - loss: 0.6799 - acc: 0.5725
3648/5677 [==================>...........] - ETA: 4:14 - loss: 0.6788 - acc: 0.5743
3712/5677 [==================>...........] - ETA: 4:06 - loss: 0.6790 - acc: 0.5735
3776/5677 [==================>...........] - ETA: 3:57 - loss: 0.6790 - acc: 0.5728
3840/5677 [===================>..........] - ETA: 3:50 - loss: 0.6793 - acc: 0.5729
3904/5677 [===================>..........] - ETA: 3:41 - loss: 0.6797 - acc: 0.5717
3968/5677 [===================>..........] - ETA: 3:33 - loss: 0.6800 - acc: 0.5698
4032/5677 [====================>.........] - ETA: 3:25 - loss: 0.6797 - acc: 0.5697
4096/5677 [====================>.........] - ETA: 3:17 - loss: 0.6799 - acc: 0.5693
4160/5677 [====================>.........] - ETA: 3:09 - loss: 0.6800 - acc: 0.5697
4224/5677 [=====================>........] - ETA: 3:01 - loss: 0.6802 - acc: 0.5687
4288/5677 [=====================>........] - ETA: 2:53 - loss: 0.6801 - acc: 0.5690
4352/5677 [=====================>........] - ETA: 2:45 - loss: 0.6799 - acc: 0.5699
4416/5677 [======================>.......] - ETA: 2:37 - loss: 0.6797 - acc: 0.5697
4480/5677 [======================>.......] - ETA: 2:29 - loss: 0.6798 - acc: 0.5703
4544/5677 [=======================>......] - ETA: 2:21 - loss: 0.6798 - acc: 0.5704
4608/5677 [=======================>......] - ETA: 2:13 - loss: 0.6800 - acc: 0.5699
4672/5677 [=======================>......] - ETA: 2:05 - loss: 0.6802 - acc: 0.5683
4736/5677 [========================>.....] - ETA: 1:56 - loss: 0.6800 - acc: 0.5688
4800/5677 [========================>.....] - ETA: 1:48 - loss: 0.6800 - acc: 0.5690
4864/5677 [========================>.....] - ETA: 1:40 - loss: 0.6802 - acc: 0.5687
4928/5677 [=========================>....] - ETA: 1:32 - loss: 0.6804 - acc: 0.5684
4992/5677 [=========================>....] - ETA: 1:25 - loss: 0.6802 - acc: 0.5689
5056/5677 [=========================>....] - ETA: 1:17 - loss: 0.6797 - acc: 0.5702
5120/5677 [==========================>...] - ETA: 1:09 - loss: 0.6796 - acc: 0.5709
5184/5677 [==========================>...] - ETA: 1:01 - loss: 0.6796 - acc: 0.5700
5248/5677 [==========================>...] - ETA: 53s - loss: 0.6796 - acc: 0.5699 
5312/5677 [===========================>..] - ETA: 45s - loss: 0.6797 - acc: 0.5697
5376/5677 [===========================>..] - ETA: 37s - loss: 0.6795 - acc: 0.5701
5440/5677 [===========================>..] - ETA: 29s - loss: 0.6797 - acc: 0.5695
5504/5677 [============================>.] - ETA: 21s - loss: 0.6793 - acc: 0.5699
5568/5677 [============================>.] - ETA: 13s - loss: 0.6794 - acc: 0.5706
5632/5677 [============================>.] - ETA: 5s - loss: 0.6790 - acc: 0.5717 
5677/5677 [==============================] - 735s 129ms/step - loss: 0.6789 - acc: 0.5723 - val_loss: 0.6984 - val_acc: 0.5436

Epoch 00005: val_acc did not improve from 0.57845
Epoch 6/10

  64/5677 [..............................] - ETA: 13:08 - loss: 0.7042 - acc: 0.5000
 128/5677 [..............................] - ETA: 13:09 - loss: 0.7208 - acc: 0.4922
 192/5677 [>.............................] - ETA: 12:11 - loss: 0.7024 - acc: 0.5260
 256/5677 [>.............................] - ETA: 11:43 - loss: 0.6964 - acc: 0.5391
 320/5677 [>.............................] - ETA: 11:14 - loss: 0.6970 - acc: 0.5406
 384/5677 [=>............................] - ETA: 11:00 - loss: 0.7044 - acc: 0.5182
 448/5677 [=>............................] - ETA: 10:40 - loss: 0.7060 - acc: 0.5156
 512/5677 [=>............................] - ETA: 10:30 - loss: 0.7075 - acc: 0.5117
 576/5677 [==>...........................] - ETA: 10:21 - loss: 0.7036 - acc: 0.5191
 640/5677 [==>...........................] - ETA: 10:13 - loss: 0.7024 - acc: 0.5219
 704/5677 [==>...........................] - ETA: 10:04 - loss: 0.6992 - acc: 0.5298
 768/5677 [===>..........................] - ETA: 9:51 - loss: 0.6992 - acc: 0.5247 
 832/5677 [===>..........................] - ETA: 9:42 - loss: 0.7003 - acc: 0.5276
 896/5677 [===>..........................] - ETA: 9:33 - loss: 0.6986 - acc: 0.5312
 960/5677 [====>.........................] - ETA: 9:22 - loss: 0.6955 - acc: 0.5365
1024/5677 [====>.........................] - ETA: 9:11 - loss: 0.6940 - acc: 0.5420
1088/5677 [====>.........................] - ETA: 9:02 - loss: 0.6935 - acc: 0.5404
1152/5677 [=====>........................] - ETA: 8:50 - loss: 0.6918 - acc: 0.5477
1216/5677 [=====>........................] - ETA: 8:41 - loss: 0.6922 - acc: 0.5461
1280/5677 [=====>........................] - ETA: 8:32 - loss: 0.6912 - acc: 0.5461
1344/5677 [======>.......................] - ETA: 8:22 - loss: 0.6897 - acc: 0.5469
1408/5677 [======>.......................] - ETA: 8:12 - loss: 0.6880 - acc: 0.5518
1472/5677 [======>.......................] - ETA: 8:04 - loss: 0.6873 - acc: 0.5577
1536/5677 [=======>......................] - ETA: 7:55 - loss: 0.6855 - acc: 0.5625
1600/5677 [=======>......................] - ETA: 7:47 - loss: 0.6874 - acc: 0.5563
1664/5677 [=======>......................] - ETA: 7:37 - loss: 0.6872 - acc: 0.5577
1728/5677 [========>.....................] - ETA: 7:29 - loss: 0.6859 - acc: 0.5579
1792/5677 [========>.....................] - ETA: 7:23 - loss: 0.6841 - acc: 0.5631
1856/5677 [========>.....................] - ETA: 7:17 - loss: 0.6845 - acc: 0.5625
1920/5677 [=========>....................] - ETA: 7:10 - loss: 0.6839 - acc: 0.5630
1984/5677 [=========>....................] - ETA: 7:02 - loss: 0.6846 - acc: 0.5625
2048/5677 [=========>....................] - ETA: 6:56 - loss: 0.6831 - acc: 0.5649
2112/5677 [==========>...................] - ETA: 6:48 - loss: 0.6823 - acc: 0.5663
2176/5677 [==========>...................] - ETA: 6:40 - loss: 0.6836 - acc: 0.5657
2240/5677 [==========>...................] - ETA: 6:34 - loss: 0.6849 - acc: 0.5647
2304/5677 [===========>..................] - ETA: 6:27 - loss: 0.6851 - acc: 0.5651
2368/5677 [===========>..................] - ETA: 6:20 - loss: 0.6850 - acc: 0.5629
2432/5677 [===========>..................] - ETA: 6:14 - loss: 0.6848 - acc: 0.5633
2496/5677 [============>.................] - ETA: 6:05 - loss: 0.6844 - acc: 0.5637
2560/5677 [============>.................] - ETA: 5:58 - loss: 0.6837 - acc: 0.5652
2624/5677 [============>.................] - ETA: 5:52 - loss: 0.6831 - acc: 0.5682
2688/5677 [=============>................] - ETA: 5:44 - loss: 0.6835 - acc: 0.5666
2752/5677 [=============>................] - ETA: 5:37 - loss: 0.6836 - acc: 0.5669
2816/5677 [=============>................] - ETA: 5:30 - loss: 0.6845 - acc: 0.5664
2880/5677 [==============>...............] - ETA: 5:25 - loss: 0.6829 - acc: 0.5687
2944/5677 [==============>...............] - ETA: 5:16 - loss: 0.6823 - acc: 0.5696
3008/5677 [==============>...............] - ETA: 5:08 - loss: 0.6821 - acc: 0.5705
3072/5677 [===============>..............] - ETA: 5:00 - loss: 0.6817 - acc: 0.5706
3136/5677 [===============>..............] - ETA: 4:53 - loss: 0.6818 - acc: 0.5698
3200/5677 [===============>..............] - ETA: 4:47 - loss: 0.6821 - acc: 0.5694
3264/5677 [================>.............] - ETA: 4:40 - loss: 0.6810 - acc: 0.5711
3328/5677 [================>.............] - ETA: 4:33 - loss: 0.6810 - acc: 0.5706
3392/5677 [================>.............] - ETA: 4:25 - loss: 0.6810 - acc: 0.5713
3456/5677 [=================>............] - ETA: 4:17 - loss: 0.6807 - acc: 0.5706
3520/5677 [=================>............] - ETA: 4:10 - loss: 0.6805 - acc: 0.5705
3584/5677 [=================>............] - ETA: 4:03 - loss: 0.6808 - acc: 0.5700
3648/5677 [==================>...........] - ETA: 3:56 - loss: 0.6805 - acc: 0.5707
3712/5677 [==================>...........] - ETA: 3:50 - loss: 0.6801 - acc: 0.5703
3776/5677 [==================>...........] - ETA: 3:42 - loss: 0.6803 - acc: 0.5699
3840/5677 [===================>..........] - ETA: 3:34 - loss: 0.6807 - acc: 0.5682
3904/5677 [===================>..........] - ETA: 3:26 - loss: 0.6799 - acc: 0.5694
3968/5677 [===================>..........] - ETA: 3:18 - loss: 0.6801 - acc: 0.5693
4032/5677 [====================>.........] - ETA: 3:11 - loss: 0.6806 - acc: 0.5697
4096/5677 [====================>.........] - ETA: 3:04 - loss: 0.6800 - acc: 0.5715
4160/5677 [====================>.........] - ETA: 2:57 - loss: 0.6796 - acc: 0.5724
4224/5677 [=====================>........] - ETA: 2:50 - loss: 0.6803 - acc: 0.5717
4288/5677 [=====================>........] - ETA: 2:43 - loss: 0.6803 - acc: 0.5721
4352/5677 [=====================>........] - ETA: 2:35 - loss: 0.6804 - acc: 0.5719
4416/5677 [======================>.......] - ETA: 2:28 - loss: 0.6805 - acc: 0.5722
4480/5677 [======================>.......] - ETA: 2:20 - loss: 0.6802 - acc: 0.5734
4544/5677 [=======================>......] - ETA: 2:13 - loss: 0.6798 - acc: 0.5742
4608/5677 [=======================>......] - ETA: 2:06 - loss: 0.6803 - acc: 0.5723
4672/5677 [=======================>......] - ETA: 1:59 - loss: 0.6803 - acc: 0.5717
4736/5677 [========================>.....] - ETA: 1:52 - loss: 0.6799 - acc: 0.5724
4800/5677 [========================>.....] - ETA: 1:44 - loss: 0.6792 - acc: 0.5744
4864/5677 [========================>.....] - ETA: 1:36 - loss: 0.6794 - acc: 0.5746
4928/5677 [=========================>....] - ETA: 1:29 - loss: 0.6795 - acc: 0.5745
4992/5677 [=========================>....] - ETA: 1:21 - loss: 0.6799 - acc: 0.5727
5056/5677 [=========================>....] - ETA: 1:14 - loss: 0.6800 - acc: 0.5720
5120/5677 [==========================>...] - ETA: 1:06 - loss: 0.6803 - acc: 0.5713
5184/5677 [==========================>...] - ETA: 59s - loss: 0.6799 - acc: 0.5718 
5248/5677 [==========================>...] - ETA: 51s - loss: 0.6797 - acc: 0.5722
5312/5677 [===========================>..] - ETA: 44s - loss: 0.6794 - acc: 0.5730
5376/5677 [===========================>..] - ETA: 36s - loss: 0.6798 - acc: 0.5720
5440/5677 [===========================>..] - ETA: 28s - loss: 0.6804 - acc: 0.5708
5504/5677 [============================>.] - ETA: 21s - loss: 0.6802 - acc: 0.5707
5568/5677 [============================>.] - ETA: 13s - loss: 0.6801 - acc: 0.5715
5632/5677 [============================>.] - ETA: 5s - loss: 0.6800 - acc: 0.5716 
5677/5677 [==============================] - 731s 129ms/step - loss: 0.6802 - acc: 0.5707 - val_loss: 0.6876 - val_acc: 0.5261

Epoch 00006: val_acc did not improve from 0.57845
Epoch 7/10

  64/5677 [..............................] - ETA: 12:11 - loss: 0.6468 - acc: 0.6719
 128/5677 [..............................] - ETA: 12:59 - loss: 0.6847 - acc: 0.5781
 192/5677 [>.............................] - ETA: 12:58 - loss: 0.6747 - acc: 0.6094
 256/5677 [>.............................] - ETA: 12:45 - loss: 0.6659 - acc: 0.6133
 320/5677 [>.............................] - ETA: 12:53 - loss: 0.6648 - acc: 0.6250
 384/5677 [=>............................] - ETA: 12:54 - loss: 0.6682 - acc: 0.6094
 448/5677 [=>............................] - ETA: 12:43 - loss: 0.6651 - acc: 0.6116
 512/5677 [=>............................] - ETA: 12:35 - loss: 0.6633 - acc: 0.6113
 576/5677 [==>...........................] - ETA: 12:30 - loss: 0.6621 - acc: 0.6128
 640/5677 [==>...........................] - ETA: 12:14 - loss: 0.6629 - acc: 0.6094
 704/5677 [==>...........................] - ETA: 12:00 - loss: 0.6628 - acc: 0.6108
 768/5677 [===>..........................] - ETA: 11:49 - loss: 0.6620 - acc: 0.6159
 832/5677 [===>..........................] - ETA: 11:35 - loss: 0.6622 - acc: 0.6178
 896/5677 [===>..........................] - ETA: 11:22 - loss: 0.6641 - acc: 0.6138
 960/5677 [====>.........................] - ETA: 11:09 - loss: 0.6652 - acc: 0.6115
1024/5677 [====>.........................] - ETA: 10:58 - loss: 0.6650 - acc: 0.6074
1088/5677 [====>.........................] - ETA: 10:45 - loss: 0.6643 - acc: 0.6085
1152/5677 [=====>........................] - ETA: 10:31 - loss: 0.6665 - acc: 0.6024
1216/5677 [=====>........................] - ETA: 10:23 - loss: 0.6648 - acc: 0.6036
1280/5677 [=====>........................] - ETA: 10:12 - loss: 0.6673 - acc: 0.5992
1344/5677 [======>.......................] - ETA: 10:05 - loss: 0.6687 - acc: 0.6004
1408/5677 [======>.......................] - ETA: 9:54 - loss: 0.6686 - acc: 0.6001 
1472/5677 [======>.......................] - ETA: 9:42 - loss: 0.6703 - acc: 0.5965
1536/5677 [=======>......................] - ETA: 9:32 - loss: 0.6708 - acc: 0.5957
1600/5677 [=======>......................] - ETA: 9:25 - loss: 0.6715 - acc: 0.5931
1664/5677 [=======>......................] - ETA: 9:15 - loss: 0.6714 - acc: 0.5925
1728/5677 [========>.....................] - ETA: 9:03 - loss: 0.6710 - acc: 0.5920
1792/5677 [========>.....................] - ETA: 8:53 - loss: 0.6725 - acc: 0.5910
1856/5677 [========>.....................] - ETA: 8:43 - loss: 0.6724 - acc: 0.5932
1920/5677 [=========>....................] - ETA: 8:35 - loss: 0.6714 - acc: 0.5953
1984/5677 [=========>....................] - ETA: 8:26 - loss: 0.6707 - acc: 0.5953
2048/5677 [=========>....................] - ETA: 8:17 - loss: 0.6711 - acc: 0.5952
2112/5677 [==========>...................] - ETA: 8:08 - loss: 0.6713 - acc: 0.5952
2176/5677 [==========>...................] - ETA: 7:59 - loss: 0.6711 - acc: 0.5928
2240/5677 [==========>...................] - ETA: 7:49 - loss: 0.6725 - acc: 0.5893
2304/5677 [===========>..................] - ETA: 7:40 - loss: 0.6716 - acc: 0.5916
2368/5677 [===========>..................] - ETA: 7:31 - loss: 0.6717 - acc: 0.5899
2432/5677 [===========>..................] - ETA: 7:21 - loss: 0.6731 - acc: 0.5884
2496/5677 [============>.................] - ETA: 7:13 - loss: 0.6734 - acc: 0.5897
2560/5677 [============>.................] - ETA: 7:03 - loss: 0.6730 - acc: 0.5891
2624/5677 [============>.................] - ETA: 6:54 - loss: 0.6733 - acc: 0.5880
2688/5677 [=============>................] - ETA: 6:44 - loss: 0.6736 - acc: 0.5874
2752/5677 [=============>................] - ETA: 6:35 - loss: 0.6737 - acc: 0.5865
2816/5677 [=============>................] - ETA: 6:26 - loss: 0.6737 - acc: 0.5859
2880/5677 [==============>...............] - ETA: 6:16 - loss: 0.6729 - acc: 0.5878
2944/5677 [==============>...............] - ETA: 6:07 - loss: 0.6740 - acc: 0.5856
3008/5677 [==============>...............] - ETA: 5:57 - loss: 0.6746 - acc: 0.5841
3072/5677 [===============>..............] - ETA: 5:48 - loss: 0.6743 - acc: 0.5850
3136/5677 [===============>..............] - ETA: 5:39 - loss: 0.6747 - acc: 0.5832
3200/5677 [===============>..............] - ETA: 5:30 - loss: 0.6754 - acc: 0.5819
3264/5677 [================>.............] - ETA: 5:21 - loss: 0.6762 - acc: 0.5800
3328/5677 [================>.............] - ETA: 5:12 - loss: 0.6763 - acc: 0.5787
3392/5677 [================>.............] - ETA: 5:03 - loss: 0.6768 - acc: 0.5781
3456/5677 [=================>............] - ETA: 4:54 - loss: 0.6770 - acc: 0.5784
3520/5677 [=================>............] - ETA: 4:45 - loss: 0.6774 - acc: 0.5778
3584/5677 [=================>............] - ETA: 4:37 - loss: 0.6769 - acc: 0.5798
3648/5677 [==================>...........] - ETA: 4:29 - loss: 0.6768 - acc: 0.5800
3712/5677 [==================>...........] - ETA: 4:20 - loss: 0.6768 - acc: 0.5800
3776/5677 [==================>...........] - ETA: 4:11 - loss: 0.6759 - acc: 0.5824
3840/5677 [===================>..........] - ETA: 4:02 - loss: 0.6761 - acc: 0.5813
3904/5677 [===================>..........] - ETA: 3:53 - loss: 0.6757 - acc: 0.5820
3968/5677 [===================>..........] - ETA: 3:44 - loss: 0.6758 - acc: 0.5827
4032/5677 [====================>.........] - ETA: 3:36 - loss: 0.6756 - acc: 0.5826
4096/5677 [====================>.........] - ETA: 3:27 - loss: 0.6758 - acc: 0.5835
4160/5677 [====================>.........] - ETA: 3:18 - loss: 0.6755 - acc: 0.5841
4224/5677 [=====================>........] - ETA: 3:09 - loss: 0.6756 - acc: 0.5840
4288/5677 [=====================>........] - ETA: 3:01 - loss: 0.6762 - acc: 0.5821
4352/5677 [=====================>........] - ETA: 2:52 - loss: 0.6757 - acc: 0.5839
4416/5677 [======================>.......] - ETA: 2:44 - loss: 0.6761 - acc: 0.5831
4480/5677 [======================>.......] - ETA: 2:35 - loss: 0.6760 - acc: 0.5824
4544/5677 [=======================>......] - ETA: 2:27 - loss: 0.6761 - acc: 0.5821
4608/5677 [=======================>......] - ETA: 2:18 - loss: 0.6760 - acc: 0.5822
4672/5677 [=======================>......] - ETA: 2:10 - loss: 0.6764 - acc: 0.5818
4736/5677 [========================>.....] - ETA: 2:02 - loss: 0.6765 - acc: 0.5809
4800/5677 [========================>.....] - ETA: 1:53 - loss: 0.6765 - acc: 0.5806
4864/5677 [========================>.....] - ETA: 1:45 - loss: 0.6761 - acc: 0.5812
4928/5677 [=========================>....] - ETA: 1:36 - loss: 0.6767 - acc: 0.5804
4992/5677 [=========================>....] - ETA: 1:28 - loss: 0.6766 - acc: 0.5801
5056/5677 [=========================>....] - ETA: 1:19 - loss: 0.6769 - acc: 0.5793
5120/5677 [==========================>...] - ETA: 1:11 - loss: 0.6769 - acc: 0.5791
5184/5677 [==========================>...] - ETA: 1:03 - loss: 0.6771 - acc: 0.5785
5248/5677 [==========================>...] - ETA: 55s - loss: 0.6776 - acc: 0.5774 
5312/5677 [===========================>..] - ETA: 46s - loss: 0.6777 - acc: 0.5759
5376/5677 [===========================>..] - ETA: 38s - loss: 0.6776 - acc: 0.5761
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6775 - acc: 0.5761
5504/5677 [============================>.] - ETA: 22s - loss: 0.6771 - acc: 0.5767
5568/5677 [============================>.] - ETA: 13s - loss: 0.6772 - acc: 0.5769
5632/5677 [============================>.] - ETA: 5s - loss: 0.6775 - acc: 0.5769 
5677/5677 [==============================] - 751s 132ms/step - loss: 0.6776 - acc: 0.5769 - val_loss: 0.6727 - val_acc: 0.5975

Epoch 00007: val_acc improved from 0.57845 to 0.59746, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window13/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 8/10

  64/5677 [..............................] - ETA: 10:40 - loss: 0.7041 - acc: 0.6250
 128/5677 [..............................] - ETA: 9:45 - loss: 0.6922 - acc: 0.6172 
 192/5677 [>.............................] - ETA: 9:53 - loss: 0.6892 - acc: 0.5938
 256/5677 [>.............................] - ETA: 9:46 - loss: 0.6785 - acc: 0.6055
 320/5677 [>.............................] - ETA: 9:43 - loss: 0.6705 - acc: 0.6094
 384/5677 [=>............................] - ETA: 9:46 - loss: 0.6731 - acc: 0.5885
 448/5677 [=>............................] - ETA: 9:50 - loss: 0.6679 - acc: 0.5960
 512/5677 [=>............................] - ETA: 9:38 - loss: 0.6677 - acc: 0.6055
 576/5677 [==>...........................] - ETA: 9:29 - loss: 0.6751 - acc: 0.5885
 640/5677 [==>...........................] - ETA: 9:17 - loss: 0.6788 - acc: 0.5859
 704/5677 [==>...........................] - ETA: 9:13 - loss: 0.6813 - acc: 0.5781
 768/5677 [===>..........................] - ETA: 9:03 - loss: 0.6800 - acc: 0.5859
 832/5677 [===>..........................] - ETA: 8:56 - loss: 0.6816 - acc: 0.5769
 896/5677 [===>..........................] - ETA: 8:50 - loss: 0.6784 - acc: 0.5837
 960/5677 [====>.........................] - ETA: 8:43 - loss: 0.6779 - acc: 0.5833
1024/5677 [====>.........................] - ETA: 8:35 - loss: 0.6786 - acc: 0.5811
1088/5677 [====>.........................] - ETA: 8:28 - loss: 0.6789 - acc: 0.5846
1152/5677 [=====>........................] - ETA: 8:20 - loss: 0.6794 - acc: 0.5825
1216/5677 [=====>........................] - ETA: 8:12 - loss: 0.6789 - acc: 0.5839
1280/5677 [=====>........................] - ETA: 8:05 - loss: 0.6797 - acc: 0.5820
1344/5677 [======>.......................] - ETA: 8:03 - loss: 0.6789 - acc: 0.5826
1408/5677 [======>.......................] - ETA: 7:56 - loss: 0.6786 - acc: 0.5845
1472/5677 [======>.......................] - ETA: 7:51 - loss: 0.6782 - acc: 0.5849
1536/5677 [=======>......................] - ETA: 7:44 - loss: 0.6789 - acc: 0.5840
1600/5677 [=======>......................] - ETA: 7:37 - loss: 0.6785 - acc: 0.5850
1664/5677 [=======>......................] - ETA: 7:31 - loss: 0.6787 - acc: 0.5823
1728/5677 [========>.....................] - ETA: 7:23 - loss: 0.6771 - acc: 0.5845
1792/5677 [========>.....................] - ETA: 7:18 - loss: 0.6768 - acc: 0.5843
1856/5677 [========>.....................] - ETA: 7:19 - loss: 0.6763 - acc: 0.5857
1920/5677 [=========>....................] - ETA: 7:11 - loss: 0.6774 - acc: 0.5833
1984/5677 [=========>....................] - ETA: 7:05 - loss: 0.6786 - acc: 0.5796
2048/5677 [=========>....................] - ETA: 6:58 - loss: 0.6782 - acc: 0.5771
2112/5677 [==========>...................] - ETA: 6:51 - loss: 0.6767 - acc: 0.5791
2176/5677 [==========>...................] - ETA: 6:45 - loss: 0.6774 - acc: 0.5790
2240/5677 [==========>...................] - ETA: 6:39 - loss: 0.6781 - acc: 0.5786
2304/5677 [===========>..................] - ETA: 6:32 - loss: 0.6779 - acc: 0.5781
2368/5677 [===========>..................] - ETA: 6:24 - loss: 0.6792 - acc: 0.5764
2432/5677 [===========>..................] - ETA: 6:15 - loss: 0.6787 - acc: 0.5773
2496/5677 [============>.................] - ETA: 6:10 - loss: 0.6800 - acc: 0.5745
2560/5677 [============>.................] - ETA: 6:03 - loss: 0.6800 - acc: 0.5723
2624/5677 [============>.................] - ETA: 5:55 - loss: 0.6797 - acc: 0.5732
2688/5677 [=============>................] - ETA: 5:48 - loss: 0.6785 - acc: 0.5751
2752/5677 [=============>................] - ETA: 5:42 - loss: 0.6784 - acc: 0.5759
2816/5677 [=============>................] - ETA: 5:34 - loss: 0.6771 - acc: 0.5781
2880/5677 [==============>...............] - ETA: 5:25 - loss: 0.6764 - acc: 0.5788
2944/5677 [==============>...............] - ETA: 5:17 - loss: 0.6768 - acc: 0.5785
3008/5677 [==============>...............] - ETA: 5:08 - loss: 0.6766 - acc: 0.5791
3072/5677 [===============>..............] - ETA: 5:01 - loss: 0.6760 - acc: 0.5791
3136/5677 [===============>..............] - ETA: 4:54 - loss: 0.6766 - acc: 0.5778
3200/5677 [===============>..............] - ETA: 4:46 - loss: 0.6754 - acc: 0.5806
3264/5677 [================>.............] - ETA: 4:38 - loss: 0.6746 - acc: 0.5821
3328/5677 [================>.............] - ETA: 4:30 - loss: 0.6743 - acc: 0.5841
3392/5677 [================>.............] - ETA: 4:23 - loss: 0.6736 - acc: 0.5861
3456/5677 [=================>............] - ETA: 4:16 - loss: 0.6735 - acc: 0.5859
3520/5677 [=================>............] - ETA: 4:08 - loss: 0.6730 - acc: 0.5878
3584/5677 [=================>............] - ETA: 4:00 - loss: 0.6742 - acc: 0.5859
3648/5677 [==================>...........] - ETA: 3:53 - loss: 0.6742 - acc: 0.5863
3712/5677 [==================>...........] - ETA: 3:45 - loss: 0.6732 - acc: 0.5884
3776/5677 [==================>...........] - ETA: 3:38 - loss: 0.6740 - acc: 0.5877
3840/5677 [===================>..........] - ETA: 3:31 - loss: 0.6737 - acc: 0.5883
3904/5677 [===================>..........] - ETA: 3:23 - loss: 0.6740 - acc: 0.5881
3968/5677 [===================>..........] - ETA: 3:16 - loss: 0.6751 - acc: 0.5872
4032/5677 [====================>.........] - ETA: 3:09 - loss: 0.6766 - acc: 0.5846
4096/5677 [====================>.........] - ETA: 3:01 - loss: 0.6761 - acc: 0.5859
4160/5677 [====================>.........] - ETA: 2:54 - loss: 0.6758 - acc: 0.5853
4224/5677 [=====================>........] - ETA: 2:47 - loss: 0.6759 - acc: 0.5855
4288/5677 [=====================>........] - ETA: 2:40 - loss: 0.6756 - acc: 0.5861
4352/5677 [=====================>........] - ETA: 2:32 - loss: 0.6757 - acc: 0.5859
4416/5677 [======================>.......] - ETA: 2:25 - loss: 0.6754 - acc: 0.5867
4480/5677 [======================>.......] - ETA: 2:17 - loss: 0.6766 - acc: 0.5855
4544/5677 [=======================>......] - ETA: 2:09 - loss: 0.6772 - acc: 0.5843
4608/5677 [=======================>......] - ETA: 2:02 - loss: 0.6776 - acc: 0.5829
4672/5677 [=======================>......] - ETA: 1:54 - loss: 0.6785 - acc: 0.5811
4736/5677 [========================>.....] - ETA: 1:47 - loss: 0.6786 - acc: 0.5800
4800/5677 [========================>.....] - ETA: 1:40 - loss: 0.6782 - acc: 0.5804
4864/5677 [========================>.....] - ETA: 1:33 - loss: 0.6787 - acc: 0.5794
4928/5677 [=========================>....] - ETA: 1:25 - loss: 0.6787 - acc: 0.5795
4992/5677 [=========================>....] - ETA: 1:18 - loss: 0.6784 - acc: 0.5809
5056/5677 [=========================>....] - ETA: 1:11 - loss: 0.6785 - acc: 0.5811
5120/5677 [==========================>...] - ETA: 1:03 - loss: 0.6788 - acc: 0.5807
5184/5677 [==========================>...] - ETA: 56s - loss: 0.6785 - acc: 0.5812 
5248/5677 [==========================>...] - ETA: 48s - loss: 0.6783 - acc: 0.5806
5312/5677 [===========================>..] - ETA: 41s - loss: 0.6786 - acc: 0.5802
5376/5677 [===========================>..] - ETA: 34s - loss: 0.6790 - acc: 0.5800
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6793 - acc: 0.5792
5504/5677 [============================>.] - ETA: 19s - loss: 0.6790 - acc: 0.5794
5568/5677 [============================>.] - ETA: 12s - loss: 0.6784 - acc: 0.5803
5632/5677 [============================>.] - ETA: 5s - loss: 0.6785 - acc: 0.5806 
5677/5677 [==============================] - 670s 118ms/step - loss: 0.6786 - acc: 0.5804 - val_loss: 0.6712 - val_acc: 0.6038

Epoch 00008: val_acc improved from 0.59746 to 0.60380, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window13/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 9/10

  64/5677 [..............................] - ETA: 10:51 - loss: 0.7230 - acc: 0.4219
 128/5677 [..............................] - ETA: 10:31 - loss: 0.7141 - acc: 0.4844
 192/5677 [>.............................] - ETA: 10:27 - loss: 0.7179 - acc: 0.4635
 256/5677 [>.............................] - ETA: 10:22 - loss: 0.7048 - acc: 0.5039
 320/5677 [>.............................] - ETA: 10:21 - loss: 0.7004 - acc: 0.5156
 384/5677 [=>............................] - ETA: 9:48 - loss: 0.6947 - acc: 0.5391 
 448/5677 [=>............................] - ETA: 9:38 - loss: 0.6964 - acc: 0.5357
 512/5677 [=>............................] - ETA: 9:26 - loss: 0.6911 - acc: 0.5449
 576/5677 [==>...........................] - ETA: 9:05 - loss: 0.6897 - acc: 0.5503
 640/5677 [==>...........................] - ETA: 8:52 - loss: 0.6892 - acc: 0.5500
 704/5677 [==>...........................] - ETA: 8:49 - loss: 0.6905 - acc: 0.5440
 768/5677 [===>..........................] - ETA: 8:43 - loss: 0.6900 - acc: 0.5482
 832/5677 [===>..........................] - ETA: 8:38 - loss: 0.6878 - acc: 0.5505
 896/5677 [===>..........................] - ETA: 8:29 - loss: 0.6870 - acc: 0.5525
 960/5677 [====>.........................] - ETA: 8:23 - loss: 0.6858 - acc: 0.5563
1024/5677 [====>.........................] - ETA: 8:17 - loss: 0.6840 - acc: 0.5576
1088/5677 [====>.........................] - ETA: 8:09 - loss: 0.6829 - acc: 0.5597
1152/5677 [=====>........................] - ETA: 7:59 - loss: 0.6838 - acc: 0.5599
1216/5677 [=====>........................] - ETA: 7:50 - loss: 0.6829 - acc: 0.5650
1280/5677 [=====>........................] - ETA: 7:41 - loss: 0.6811 - acc: 0.5703
1344/5677 [======>.......................] - ETA: 7:32 - loss: 0.6801 - acc: 0.5714
1408/5677 [======>.......................] - ETA: 7:26 - loss: 0.6812 - acc: 0.5696
1472/5677 [======>.......................] - ETA: 7:18 - loss: 0.6806 - acc: 0.5700
1536/5677 [=======>......................] - ETA: 7:15 - loss: 0.6828 - acc: 0.5651
1600/5677 [=======>......................] - ETA: 7:08 - loss: 0.6829 - acc: 0.5637
1664/5677 [=======>......................] - ETA: 7:03 - loss: 0.6823 - acc: 0.5655
1728/5677 [========>.....................] - ETA: 6:57 - loss: 0.6816 - acc: 0.5654
1792/5677 [========>.....................] - ETA: 6:51 - loss: 0.6799 - acc: 0.5692
1856/5677 [========>.....................] - ETA: 6:44 - loss: 0.6790 - acc: 0.5711
1920/5677 [=========>....................] - ETA: 6:38 - loss: 0.6790 - acc: 0.5708
1984/5677 [=========>....................] - ETA: 6:32 - loss: 0.6770 - acc: 0.5756
2048/5677 [=========>....................] - ETA: 6:24 - loss: 0.6765 - acc: 0.5747
2112/5677 [==========>...................] - ETA: 6:17 - loss: 0.6749 - acc: 0.5791
2176/5677 [==========>...................] - ETA: 6:09 - loss: 0.6745 - acc: 0.5786
2240/5677 [==========>...................] - ETA: 6:01 - loss: 0.6745 - acc: 0.5781
2304/5677 [===========>..................] - ETA: 5:55 - loss: 0.6748 - acc: 0.5777
2368/5677 [===========>..................] - ETA: 5:48 - loss: 0.6740 - acc: 0.5777
2432/5677 [===========>..................] - ETA: 5:42 - loss: 0.6753 - acc: 0.5748
2496/5677 [============>.................] - ETA: 5:35 - loss: 0.6747 - acc: 0.5761
2560/5677 [============>.................] - ETA: 5:28 - loss: 0.6752 - acc: 0.5754
2624/5677 [============>.................] - ETA: 5:21 - loss: 0.6742 - acc: 0.5774
2688/5677 [=============>................] - ETA: 5:13 - loss: 0.6754 - acc: 0.5774
2752/5677 [=============>................] - ETA: 5:07 - loss: 0.6759 - acc: 0.5763
2816/5677 [=============>................] - ETA: 5:00 - loss: 0.6759 - acc: 0.5760
2880/5677 [==============>...............] - ETA: 4:54 - loss: 0.6753 - acc: 0.5774
2944/5677 [==============>...............] - ETA: 4:47 - loss: 0.6750 - acc: 0.5781
3008/5677 [==============>...............] - ETA: 4:41 - loss: 0.6746 - acc: 0.5801
3072/5677 [===============>..............] - ETA: 4:34 - loss: 0.6755 - acc: 0.5794
3136/5677 [===============>..............] - ETA: 4:28 - loss: 0.6744 - acc: 0.5804
3200/5677 [===============>..............] - ETA: 4:21 - loss: 0.6743 - acc: 0.5813
3264/5677 [================>.............] - ETA: 4:15 - loss: 0.6733 - acc: 0.5830
3328/5677 [================>.............] - ETA: 4:09 - loss: 0.6733 - acc: 0.5832
3392/5677 [================>.............] - ETA: 4:03 - loss: 0.6732 - acc: 0.5834
3456/5677 [=================>............] - ETA: 3:56 - loss: 0.6745 - acc: 0.5813
3520/5677 [=================>............] - ETA: 3:49 - loss: 0.6747 - acc: 0.5821
3584/5677 [=================>............] - ETA: 3:42 - loss: 0.6741 - acc: 0.5834
3648/5677 [==================>...........] - ETA: 3:36 - loss: 0.6749 - acc: 0.5825
3712/5677 [==================>...........] - ETA: 3:29 - loss: 0.6746 - acc: 0.5832
3776/5677 [==================>...........] - ETA: 3:22 - loss: 0.6745 - acc: 0.5826
3840/5677 [===================>..........] - ETA: 3:14 - loss: 0.6746 - acc: 0.5818
3904/5677 [===================>..........] - ETA: 3:07 - loss: 0.6746 - acc: 0.5817
3968/5677 [===================>..........] - ETA: 3:00 - loss: 0.6750 - acc: 0.5817
4032/5677 [====================>.........] - ETA: 2:53 - loss: 0.6754 - acc: 0.5818
4096/5677 [====================>.........] - ETA: 2:46 - loss: 0.6763 - acc: 0.5803
4160/5677 [====================>.........] - ETA: 2:40 - loss: 0.6759 - acc: 0.5808
4224/5677 [=====================>........] - ETA: 2:33 - loss: 0.6755 - acc: 0.5817
4288/5677 [=====================>........] - ETA: 2:26 - loss: 0.6753 - acc: 0.5816
4352/5677 [=====================>........] - ETA: 2:20 - loss: 0.6754 - acc: 0.5825
4416/5677 [======================>.......] - ETA: 2:13 - loss: 0.6754 - acc: 0.5829
4480/5677 [======================>.......] - ETA: 2:06 - loss: 0.6758 - acc: 0.5810
4544/5677 [=======================>......] - ETA: 1:59 - loss: 0.6755 - acc: 0.5821
4608/5677 [=======================>......] - ETA: 1:52 - loss: 0.6755 - acc: 0.5820
4672/5677 [=======================>......] - ETA: 1:46 - loss: 0.6756 - acc: 0.5813
4736/5677 [========================>.....] - ETA: 1:39 - loss: 0.6757 - acc: 0.5815
4800/5677 [========================>.....] - ETA: 1:32 - loss: 0.6756 - acc: 0.5819
4864/5677 [========================>.....] - ETA: 1:25 - loss: 0.6759 - acc: 0.5814
4928/5677 [=========================>....] - ETA: 1:19 - loss: 0.6758 - acc: 0.5818
4992/5677 [=========================>....] - ETA: 1:12 - loss: 0.6756 - acc: 0.5823
5056/5677 [=========================>....] - ETA: 1:05 - loss: 0.6750 - acc: 0.5835
5120/5677 [==========================>...] - ETA: 58s - loss: 0.6747 - acc: 0.5842 
5184/5677 [==========================>...] - ETA: 52s - loss: 0.6741 - acc: 0.5853
5248/5677 [==========================>...] - ETA: 45s - loss: 0.6738 - acc: 0.5848
5312/5677 [===========================>..] - ETA: 38s - loss: 0.6737 - acc: 0.5857
5376/5677 [===========================>..] - ETA: 31s - loss: 0.6733 - acc: 0.5865
5440/5677 [===========================>..] - ETA: 25s - loss: 0.6736 - acc: 0.5857
5504/5677 [============================>.] - ETA: 18s - loss: 0.6742 - acc: 0.5848
5568/5677 [============================>.] - ETA: 11s - loss: 0.6747 - acc: 0.5842
5632/5677 [============================>.] - ETA: 4s - loss: 0.6751 - acc: 0.5835 
5677/5677 [==============================] - 628s 111ms/step - loss: 0.6747 - acc: 0.5845 - val_loss: 0.6988 - val_acc: 0.5261

Epoch 00009: val_acc did not improve from 0.60380
Epoch 10/10

  64/5677 [..............................] - ETA: 10:06 - loss: 0.6789 - acc: 0.5938
 128/5677 [..............................] - ETA: 10:54 - loss: 0.6701 - acc: 0.6094
 192/5677 [>.............................] - ETA: 9:54 - loss: 0.6736 - acc: 0.6146 
 256/5677 [>.............................] - ETA: 9:35 - loss: 0.6639 - acc: 0.6172
 320/5677 [>.............................] - ETA: 9:16 - loss: 0.6646 - acc: 0.6062
 384/5677 [=>............................] - ETA: 9:13 - loss: 0.6725 - acc: 0.5990
 448/5677 [=>............................] - ETA: 9:20 - loss: 0.6755 - acc: 0.5915
 512/5677 [=>............................] - ETA: 9:14 - loss: 0.6806 - acc: 0.5742
 576/5677 [==>...........................] - ETA: 9:11 - loss: 0.6821 - acc: 0.5677
 640/5677 [==>...........................] - ETA: 9:08 - loss: 0.6831 - acc: 0.5703
 704/5677 [==>...........................] - ETA: 9:13 - loss: 0.6838 - acc: 0.5724
 768/5677 [===>..........................] - ETA: 9:13 - loss: 0.6810 - acc: 0.5794
 832/5677 [===>..........................] - ETA: 9:09 - loss: 0.6783 - acc: 0.5817
 896/5677 [===>..........................] - ETA: 9:02 - loss: 0.6779 - acc: 0.5859
 960/5677 [====>.........................] - ETA: 8:59 - loss: 0.6792 - acc: 0.5802
1024/5677 [====>.........................] - ETA: 8:54 - loss: 0.6793 - acc: 0.5781
1088/5677 [====>.........................] - ETA: 8:43 - loss: 0.6782 - acc: 0.5772
1152/5677 [=====>........................] - ETA: 8:37 - loss: 0.6781 - acc: 0.5816
1216/5677 [=====>........................] - ETA: 8:29 - loss: 0.6796 - acc: 0.5781
1280/5677 [=====>........................] - ETA: 8:24 - loss: 0.6786 - acc: 0.5805
1344/5677 [======>.......................] - ETA: 8:20 - loss: 0.6787 - acc: 0.5781
1408/5677 [======>.......................] - ETA: 8:14 - loss: 0.6788 - acc: 0.5767
1472/5677 [======>.......................] - ETA: 8:06 - loss: 0.6778 - acc: 0.5802
1536/5677 [=======>......................] - ETA: 8:01 - loss: 0.6780 - acc: 0.5794
1600/5677 [=======>......................] - ETA: 7:54 - loss: 0.6762 - acc: 0.5837
1664/5677 [=======>......................] - ETA: 7:46 - loss: 0.6751 - acc: 0.5859
1728/5677 [========>.....................] - ETA: 7:37 - loss: 0.6751 - acc: 0.5856
1792/5677 [========>.....................] - ETA: 7:29 - loss: 0.6748 - acc: 0.5865
1856/5677 [========>.....................] - ETA: 7:21 - loss: 0.6746 - acc: 0.5884
1920/5677 [=========>....................] - ETA: 7:15 - loss: 0.6758 - acc: 0.5854
1984/5677 [=========>....................] - ETA: 7:07 - loss: 0.6774 - acc: 0.5832
2048/5677 [=========>....................] - ETA: 7:00 - loss: 0.6767 - acc: 0.5864
2112/5677 [==========>...................] - ETA: 6:53 - loss: 0.6772 - acc: 0.5833
2176/5677 [==========>...................] - ETA: 6:45 - loss: 0.6766 - acc: 0.5841
2240/5677 [==========>...................] - ETA: 6:38 - loss: 0.6762 - acc: 0.5844
2304/5677 [===========>..................] - ETA: 6:32 - loss: 0.6758 - acc: 0.5859
2368/5677 [===========>..................] - ETA: 6:25 - loss: 0.6757 - acc: 0.5849
2432/5677 [===========>..................] - ETA: 6:18 - loss: 0.6758 - acc: 0.5855
2496/5677 [============>.................] - ETA: 6:11 - loss: 0.6751 - acc: 0.5861
2560/5677 [============>.................] - ETA: 6:04 - loss: 0.6755 - acc: 0.5863
2624/5677 [============>.................] - ETA: 5:56 - loss: 0.6747 - acc: 0.5873
2688/5677 [=============>................] - ETA: 5:47 - loss: 0.6742 - acc: 0.5882
2752/5677 [=============>................] - ETA: 5:39 - loss: 0.6736 - acc: 0.5901
2816/5677 [=============>................] - ETA: 5:31 - loss: 0.6735 - acc: 0.5909
2880/5677 [==============>...............] - ETA: 5:25 - loss: 0.6738 - acc: 0.5903
2944/5677 [==============>...............] - ETA: 5:17 - loss: 0.6737 - acc: 0.5897
3008/5677 [==============>...............] - ETA: 5:09 - loss: 0.6730 - acc: 0.5904
3072/5677 [===============>..............] - ETA: 5:01 - loss: 0.6737 - acc: 0.5885
3136/5677 [===============>..............] - ETA: 4:54 - loss: 0.6735 - acc: 0.5890
3200/5677 [===============>..............] - ETA: 4:46 - loss: 0.6748 - acc: 0.5866
3264/5677 [================>.............] - ETA: 4:38 - loss: 0.6746 - acc: 0.5867
3328/5677 [================>.............] - ETA: 4:30 - loss: 0.6745 - acc: 0.5865
3392/5677 [================>.............] - ETA: 4:23 - loss: 0.6739 - acc: 0.5881
3456/5677 [=================>............] - ETA: 4:16 - loss: 0.6748 - acc: 0.5856
3520/5677 [=================>............] - ETA: 4:08 - loss: 0.6754 - acc: 0.5849
3584/5677 [=================>............] - ETA: 4:00 - loss: 0.6757 - acc: 0.5843
3648/5677 [==================>...........] - ETA: 3:52 - loss: 0.6755 - acc: 0.5842
3712/5677 [==================>...........] - ETA: 3:45 - loss: 0.6754 - acc: 0.5846
3776/5677 [==================>...........] - ETA: 3:37 - loss: 0.6748 - acc: 0.5853
3840/5677 [===================>..........] - ETA: 3:29 - loss: 0.6750 - acc: 0.5841
3904/5677 [===================>..........] - ETA: 3:22 - loss: 0.6761 - acc: 0.5830
3968/5677 [===================>..........] - ETA: 3:14 - loss: 0.6757 - acc: 0.5847
4032/5677 [====================>.........] - ETA: 3:08 - loss: 0.6750 - acc: 0.5853
4096/5677 [====================>.........] - ETA: 3:00 - loss: 0.6750 - acc: 0.5859
4160/5677 [====================>.........] - ETA: 2:53 - loss: 0.6751 - acc: 0.5858
4224/5677 [=====================>........] - ETA: 2:46 - loss: 0.6749 - acc: 0.5845
4288/5677 [=====================>........] - ETA: 2:39 - loss: 0.6745 - acc: 0.5844
4352/5677 [=====================>........] - ETA: 2:32 - loss: 0.6748 - acc: 0.5834
4416/5677 [======================>.......] - ETA: 2:24 - loss: 0.6750 - acc: 0.5831
4480/5677 [======================>.......] - ETA: 2:17 - loss: 0.6748 - acc: 0.5830
4544/5677 [=======================>......] - ETA: 2:10 - loss: 0.6739 - acc: 0.5849
4608/5677 [=======================>......] - ETA: 2:03 - loss: 0.6736 - acc: 0.5864
4672/5677 [=======================>......] - ETA: 1:56 - loss: 0.6739 - acc: 0.5860
4736/5677 [========================>.....] - ETA: 1:48 - loss: 0.6736 - acc: 0.5870
4800/5677 [========================>.....] - ETA: 1:41 - loss: 0.6728 - acc: 0.5890
4864/5677 [========================>.....] - ETA: 1:33 - loss: 0.6732 - acc: 0.5886
4928/5677 [=========================>....] - ETA: 1:26 - loss: 0.6728 - acc: 0.5895
4992/5677 [=========================>....] - ETA: 1:19 - loss: 0.6725 - acc: 0.5911
5056/5677 [=========================>....] - ETA: 1:11 - loss: 0.6732 - acc: 0.5904
5120/5677 [==========================>...] - ETA: 1:04 - loss: 0.6728 - acc: 0.5908
5184/5677 [==========================>...] - ETA: 56s - loss: 0.6722 - acc: 0.5912 
5248/5677 [==========================>...] - ETA: 49s - loss: 0.6723 - acc: 0.5913
5312/5677 [===========================>..] - ETA: 42s - loss: 0.6720 - acc: 0.5917
5376/5677 [===========================>..] - ETA: 34s - loss: 0.6723 - acc: 0.5910
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6721 - acc: 0.5912
5504/5677 [============================>.] - ETA: 19s - loss: 0.6726 - acc: 0.5899
5568/5677 [============================>.] - ETA: 12s - loss: 0.6724 - acc: 0.5903
5632/5677 [============================>.] - ETA: 5s - loss: 0.6724 - acc: 0.5902 
5677/5677 [==============================] - 681s 120ms/step - loss: 0.6721 - acc: 0.5901 - val_loss: 0.6837 - val_acc: 0.5483

Epoch 00010: val_acc did not improve from 0.60380
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9f6c1edf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9f6c1edf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9f6c189c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9f6c189c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622ef2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa622ef2390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f3c447a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f3c447a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3c4e9690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3c4e9690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b7059cf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b7059cf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f3c447110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f3c447110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c7f0610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c7f0610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f2467a7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f2467a7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3c30be90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3c30be90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c38bb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c38bb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f3c426750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f3c426750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c2b5f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c2b5f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f247f6710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f247f6710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f24665f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f24665f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c312f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c312f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f3c12b890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f3c12b890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f24765a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f24765a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f2443c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f2443c1d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f246e3750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f246e3750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f2446b810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f2446b810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f2433f190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f2433f190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f243ca690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f243ca690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f2415f690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f2415f690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f1c7dead0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f1c7dead0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f242644d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f242644d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f24374090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f24374090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f2407fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f2407fc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f24061c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f24061c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f24110110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f24110110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c386190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c386190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f24061390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f24061390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c7de310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c7de310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f1c28a2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f1c28a2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f1c1a1a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f1c1a1a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c2d1cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c2d1cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f1c28aad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f1c28aad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c1e4b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c1e4b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f147748d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f147748d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f1474e150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f1474e150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f14727850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f14727850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f14774710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f14774710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c3eef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c3eef50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f14470310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f14470310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f1432f4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f1432f4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f14452dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f14452dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f14640610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f14640610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f14498350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f14498350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f14178610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f14178610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f0478a610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f0478a610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f14126c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f14126c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f14178310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f14178310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f04603f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f04603f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f046207d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f046207d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f044fde50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f044fde50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f04502a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f04502a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f047bd1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f047bd1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f04603090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f04603090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f04682a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f04682a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f0427e890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f0427e890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f042423d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f042423d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f0447b890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f0447b890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f04162fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f04162fd0>>: AttributeError: module 'gast' has no attribute 'Str'
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 4:26
 128/1578 [=>............................] - ETA: 2:45
 192/1578 [==>...........................] - ETA: 2:07
 256/1578 [===>..........................] - ETA: 1:45
 320/1578 [=====>........................] - ETA: 1:31
 384/1578 [======>.......................] - ETA: 1:21
 448/1578 [=======>......................] - ETA: 1:14
 512/1578 [========>.....................] - ETA: 1:09
 576/1578 [=========>....................] - ETA: 1:03
 640/1578 [===========>..................] - ETA: 58s 
 704/1578 [============>.................] - ETA: 53s
 768/1578 [=============>................] - ETA: 48s
 832/1578 [==============>...............] - ETA: 44s
 896/1578 [================>.............] - ETA: 40s
 960/1578 [=================>............] - ETA: 36s
1024/1578 [==================>...........] - ETA: 32s
1088/1578 [===================>..........] - ETA: 28s
1152/1578 [====================>.........] - ETA: 24s
1216/1578 [======================>.......] - ETA: 20s
1280/1578 [=======================>......] - ETA: 16s
1344/1578 [========================>.....] - ETA: 13s
1408/1578 [=========================>....] - ETA: 9s 
1472/1578 [==========================>...] - ETA: 5s
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 88s 56ms/step
loss: 0.677651544260888
acc: 0.5652724968692044
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9bb81f0e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9bb81f0e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9bb81aaed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9bb81aaed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd7b2bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa5cd7b2bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9eac65b090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9eac65b090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3c5fce50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3c5fce50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f6c10b550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f6c10b550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9eac65b110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9eac65b110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c50c550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c50c550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f6c061dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9f6c061dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3c679490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9f3c679490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c257650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f1c257650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f6c061ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9f6c061ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ba07eadd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ba07eadd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ba06d09d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ba06d09d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ba05e9d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ba05e9d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bb810a350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9bb810a350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ba06d02d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ba06d02d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ba04e1cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ba04e1cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ba042cf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ba042cf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e3c279c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e3c279c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ba03cfa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ba03cfa90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ba075a2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ba075a2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ba0184fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ba0184fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ba0231f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ba0231f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ba02b9cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ba02b9cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ba004d450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9ba004d450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ba01f6150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ba01f6150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b907ed190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b907ed190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b905d9650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b905d9650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b905b8290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b905b8290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b9057f210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b9057f210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b906ab410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b906ab410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b904a0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b904a0fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b90485790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b90485790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b902b7bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b902b7bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b902f1190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b902f1190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b901b39d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b901b39d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b904a4c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b904a4c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b806be910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b806be910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b900513d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b900513d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f6c171750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f6c171750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b80747750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b80747750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b806fba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b806fba50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b80747410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b80747410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b8060bcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b8060bcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b803c8850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b803c8850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b806ff510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b806ff510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b803da210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b803da210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b800be550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b800be550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b80068490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b80068490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b802ee2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b802ee2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b800bea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b800bea90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b80327a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b80327a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b705f1cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b705f1cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9af44b6c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9af44b6c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b800613d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b800613d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b705f1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b705f1c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9af43a88d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9af43a88d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9af4239c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9af4239c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9af419b0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9af419b0d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9af44ad250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9af44ad250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9af435ffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9af435ffd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9af437ce50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9af437ce50>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 50:50 - loss: 0.7210 - acc: 0.5469
 128/5677 [..............................] - ETA: 30:55 - loss: 0.7895 - acc: 0.5234
 192/5677 [>.............................] - ETA: 24:38 - loss: 0.7690 - acc: 0.5365
 256/5677 [>.............................] - ETA: 21:26 - loss: 0.7648 - acc: 0.5156
 320/5677 [>.............................] - ETA: 19:28 - loss: 0.7627 - acc: 0.4969
 384/5677 [=>............................] - ETA: 17:54 - loss: 0.7591 - acc: 0.5078
 448/5677 [=>............................] - ETA: 16:54 - loss: 0.7542 - acc: 0.5112
 512/5677 [=>............................] - ETA: 15:59 - loss: 0.7454 - acc: 0.5254
 576/5677 [==>...........................] - ETA: 15:15 - loss: 0.7441 - acc: 0.5243
 640/5677 [==>...........................] - ETA: 14:42 - loss: 0.7450 - acc: 0.5219
 704/5677 [==>...........................] - ETA: 14:14 - loss: 0.7420 - acc: 0.5227
 768/5677 [===>..........................] - ETA: 13:47 - loss: 0.7393 - acc: 0.5182
 832/5677 [===>..........................] - ETA: 13:22 - loss: 0.7384 - acc: 0.5120
 896/5677 [===>..........................] - ETA: 13:00 - loss: 0.7466 - acc: 0.5033
 960/5677 [====>.........................] - ETA: 12:36 - loss: 0.7493 - acc: 0.4979
1024/5677 [====>.........................] - ETA: 12:16 - loss: 0.7465 - acc: 0.5000
1088/5677 [====>.........................] - ETA: 12:02 - loss: 0.7447 - acc: 0.5018
1152/5677 [=====>........................] - ETA: 11:43 - loss: 0.7477 - acc: 0.4957
1216/5677 [=====>........................] - ETA: 11:29 - loss: 0.7457 - acc: 0.5000
1280/5677 [=====>........................] - ETA: 11:12 - loss: 0.7438 - acc: 0.5039
1344/5677 [======>.......................] - ETA: 11:01 - loss: 0.7415 - acc: 0.5074
1408/5677 [======>.......................] - ETA: 10:47 - loss: 0.7398 - acc: 0.5078
1472/5677 [======>.......................] - ETA: 10:34 - loss: 0.7376 - acc: 0.5109
1536/5677 [=======>......................] - ETA: 10:20 - loss: 0.7369 - acc: 0.5124
1600/5677 [=======>......................] - ETA: 10:08 - loss: 0.7363 - acc: 0.5138
1664/5677 [=======>......................] - ETA: 9:56 - loss: 0.7359 - acc: 0.5126 
1728/5677 [========>.....................] - ETA: 9:46 - loss: 0.7338 - acc: 0.5133
1792/5677 [========>.....................] - ETA: 9:34 - loss: 0.7329 - acc: 0.5151
1856/5677 [========>.....................] - ETA: 9:23 - loss: 0.7324 - acc: 0.5129
1920/5677 [=========>....................] - ETA: 9:13 - loss: 0.7309 - acc: 0.5151
1984/5677 [=========>....................] - ETA: 9:02 - loss: 0.7297 - acc: 0.5151
2048/5677 [=========>....................] - ETA: 8:50 - loss: 0.7292 - acc: 0.5176
2112/5677 [==========>...................] - ETA: 8:38 - loss: 0.7289 - acc: 0.5161
2176/5677 [==========>...................] - ETA: 8:28 - loss: 0.7274 - acc: 0.5165
2240/5677 [==========>...................] - ETA: 8:17 - loss: 0.7269 - acc: 0.5156
2304/5677 [===========>..................] - ETA: 8:06 - loss: 0.7261 - acc: 0.5156
2368/5677 [===========>..................] - ETA: 7:55 - loss: 0.7238 - acc: 0.5186
2432/5677 [===========>..................] - ETA: 7:46 - loss: 0.7222 - acc: 0.5218
2496/5677 [============>.................] - ETA: 7:35 - loss: 0.7217 - acc: 0.5208
2560/5677 [============>.................] - ETA: 7:25 - loss: 0.7199 - acc: 0.5227
2624/5677 [============>.................] - ETA: 7:15 - loss: 0.7204 - acc: 0.5213
2688/5677 [=============>................] - ETA: 7:05 - loss: 0.7203 - acc: 0.5205
2752/5677 [=============>................] - ETA: 6:56 - loss: 0.7208 - acc: 0.5196
2816/5677 [=============>................] - ETA: 6:47 - loss: 0.7193 - acc: 0.5213
2880/5677 [==============>...............] - ETA: 6:37 - loss: 0.7184 - acc: 0.5226
2944/5677 [==============>...............] - ETA: 6:28 - loss: 0.7184 - acc: 0.5221
3008/5677 [==============>...............] - ETA: 6:18 - loss: 0.7180 - acc: 0.5206
3072/5677 [===============>..............] - ETA: 6:08 - loss: 0.7168 - acc: 0.5221
3136/5677 [===============>..............] - ETA: 5:58 - loss: 0.7169 - acc: 0.5207
3200/5677 [===============>..............] - ETA: 5:49 - loss: 0.7170 - acc: 0.5212
3264/5677 [================>.............] - ETA: 5:39 - loss: 0.7157 - acc: 0.5230
3328/5677 [================>.............] - ETA: 5:29 - loss: 0.7149 - acc: 0.5252
3392/5677 [================>.............] - ETA: 5:20 - loss: 0.7144 - acc: 0.5259
3456/5677 [=================>............] - ETA: 5:11 - loss: 0.7155 - acc: 0.5240
3520/5677 [=================>............] - ETA: 5:02 - loss: 0.7155 - acc: 0.5241
3584/5677 [=================>............] - ETA: 4:52 - loss: 0.7162 - acc: 0.5243
3648/5677 [==================>...........] - ETA: 4:43 - loss: 0.7162 - acc: 0.5249
3712/5677 [==================>...........] - ETA: 4:34 - loss: 0.7172 - acc: 0.5234
3776/5677 [==================>...........] - ETA: 4:24 - loss: 0.7166 - acc: 0.5238
3840/5677 [===================>..........] - ETA: 4:16 - loss: 0.7166 - acc: 0.5229
3904/5677 [===================>..........] - ETA: 4:07 - loss: 0.7160 - acc: 0.5246
3968/5677 [===================>..........] - ETA: 3:58 - loss: 0.7157 - acc: 0.5252
4032/5677 [====================>.........] - ETA: 3:49 - loss: 0.7151 - acc: 0.5265
4096/5677 [====================>.........] - ETA: 3:40 - loss: 0.7156 - acc: 0.5251
4160/5677 [====================>.........] - ETA: 3:31 - loss: 0.7154 - acc: 0.5248
4224/5677 [=====================>........] - ETA: 3:23 - loss: 0.7149 - acc: 0.5253
4288/5677 [=====================>........] - ETA: 3:14 - loss: 0.7148 - acc: 0.5257
4352/5677 [=====================>........] - ETA: 3:05 - loss: 0.7149 - acc: 0.5253
4416/5677 [======================>.......] - ETA: 2:56 - loss: 0.7139 - acc: 0.5272
4480/5677 [======================>.......] - ETA: 2:48 - loss: 0.7136 - acc: 0.5272
4544/5677 [=======================>......] - ETA: 2:39 - loss: 0.7131 - acc: 0.5286
4608/5677 [=======================>......] - ETA: 2:30 - loss: 0.7128 - acc: 0.5293
4672/5677 [=======================>......] - ETA: 2:21 - loss: 0.7123 - acc: 0.5291
4736/5677 [========================>.....] - ETA: 2:12 - loss: 0.7122 - acc: 0.5291
4800/5677 [========================>.....] - ETA: 2:03 - loss: 0.7121 - acc: 0.5294
4864/5677 [========================>.....] - ETA: 1:54 - loss: 0.7118 - acc: 0.5296
4928/5677 [=========================>....] - ETA: 1:45 - loss: 0.7118 - acc: 0.5298
4992/5677 [=========================>....] - ETA: 1:36 - loss: 0.7116 - acc: 0.5300
5056/5677 [=========================>....] - ETA: 1:27 - loss: 0.7121 - acc: 0.5281
5120/5677 [==========================>...] - ETA: 1:18 - loss: 0.7120 - acc: 0.5277
5184/5677 [==========================>...] - ETA: 1:09 - loss: 0.7120 - acc: 0.5276
5248/5677 [==========================>...] - ETA: 1:00 - loss: 0.7119 - acc: 0.5274
5312/5677 [===========================>..] - ETA: 51s - loss: 0.7123 - acc: 0.5273 
5376/5677 [===========================>..] - ETA: 42s - loss: 0.7119 - acc: 0.5275
5440/5677 [===========================>..] - ETA: 33s - loss: 0.7120 - acc: 0.5268
5504/5677 [============================>.] - ETA: 24s - loss: 0.7112 - acc: 0.5282
5568/5677 [============================>.] - ETA: 15s - loss: 0.7111 - acc: 0.5278
5632/5677 [============================>.] - ETA: 6s - loss: 0.7111 - acc: 0.5270 
5677/5677 [==============================] - 838s 148ms/step - loss: 0.7110 - acc: 0.5281 - val_loss: 0.6931 - val_acc: 0.5452

Epoch 00001: val_acc improved from -inf to 0.54517, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window14/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 12:38 - loss: 0.6634 - acc: 0.5938
 128/5677 [..............................] - ETA: 12:44 - loss: 0.6676 - acc: 0.6172
 192/5677 [>.............................] - ETA: 12:58 - loss: 0.6824 - acc: 0.5781
 256/5677 [>.............................] - ETA: 13:08 - loss: 0.6915 - acc: 0.5547
 320/5677 [>.............................] - ETA: 12:59 - loss: 0.6910 - acc: 0.5531
 384/5677 [=>............................] - ETA: 12:49 - loss: 0.6862 - acc: 0.5703
 448/5677 [=>............................] - ETA: 12:43 - loss: 0.6923 - acc: 0.5536
 512/5677 [=>............................] - ETA: 12:20 - loss: 0.6916 - acc: 0.5527
 576/5677 [==>...........................] - ETA: 12:02 - loss: 0.6882 - acc: 0.5573
 640/5677 [==>...........................] - ETA: 11:47 - loss: 0.6880 - acc: 0.5484
 704/5677 [==>...........................] - ETA: 11:35 - loss: 0.6889 - acc: 0.5526
 768/5677 [===>..........................] - ETA: 11:26 - loss: 0.6913 - acc: 0.5430
 832/5677 [===>..........................] - ETA: 11:28 - loss: 0.6910 - acc: 0.5445
 896/5677 [===>..........................] - ETA: 11:27 - loss: 0.6935 - acc: 0.5413
 960/5677 [====>.........................] - ETA: 11:22 - loss: 0.6922 - acc: 0.5490
1024/5677 [====>.........................] - ETA: 11:10 - loss: 0.6947 - acc: 0.5410
1088/5677 [====>.........................] - ETA: 10:57 - loss: 0.6983 - acc: 0.5294
1152/5677 [=====>........................] - ETA: 10:47 - loss: 0.6976 - acc: 0.5304
1216/5677 [=====>........................] - ETA: 10:36 - loss: 0.6969 - acc: 0.5321
1280/5677 [=====>........................] - ETA: 10:32 - loss: 0.6981 - acc: 0.5312
1344/5677 [======>.......................] - ETA: 10:24 - loss: 0.6993 - acc: 0.5327
1408/5677 [======>.......................] - ETA: 10:16 - loss: 0.6984 - acc: 0.5376
1472/5677 [======>.......................] - ETA: 10:10 - loss: 0.6990 - acc: 0.5380
1536/5677 [=======>......................] - ETA: 10:03 - loss: 0.6994 - acc: 0.5352
1600/5677 [=======>......................] - ETA: 9:54 - loss: 0.6992 - acc: 0.5344 
1664/5677 [=======>......................] - ETA: 9:46 - loss: 0.6996 - acc: 0.5319
1728/5677 [========>.....................] - ETA: 9:37 - loss: 0.6999 - acc: 0.5278
1792/5677 [========>.....................] - ETA: 9:29 - loss: 0.6990 - acc: 0.5290
1856/5677 [========>.....................] - ETA: 9:19 - loss: 0.6977 - acc: 0.5307
1920/5677 [=========>....................] - ETA: 9:12 - loss: 0.6975 - acc: 0.5302
1984/5677 [=========>....................] - ETA: 9:06 - loss: 0.6981 - acc: 0.5292
2048/5677 [=========>....................] - ETA: 8:56 - loss: 0.6989 - acc: 0.5273
2112/5677 [==========>...................] - ETA: 8:47 - loss: 0.6993 - acc: 0.5275
2176/5677 [==========>...................] - ETA: 8:36 - loss: 0.6996 - acc: 0.5294
2240/5677 [==========>...................] - ETA: 8:25 - loss: 0.6998 - acc: 0.5272
2304/5677 [===========>..................] - ETA: 8:16 - loss: 0.6996 - acc: 0.5282
2368/5677 [===========>..................] - ETA: 8:06 - loss: 0.6985 - acc: 0.5304
2432/5677 [===========>..................] - ETA: 7:55 - loss: 0.6985 - acc: 0.5312
2496/5677 [============>.................] - ETA: 7:45 - loss: 0.6982 - acc: 0.5321
2560/5677 [============>.................] - ETA: 7:35 - loss: 0.6989 - acc: 0.5316
2624/5677 [============>.................] - ETA: 7:26 - loss: 0.6982 - acc: 0.5324
2688/5677 [=============>................] - ETA: 7:16 - loss: 0.6984 - acc: 0.5320
2752/5677 [=============>................] - ETA: 7:05 - loss: 0.6984 - acc: 0.5302
2816/5677 [=============>................] - ETA: 6:55 - loss: 0.6975 - acc: 0.5330
2880/5677 [==============>...............] - ETA: 6:45 - loss: 0.6973 - acc: 0.5330
2944/5677 [==============>...............] - ETA: 6:35 - loss: 0.6965 - acc: 0.5350
3008/5677 [==============>...............] - ETA: 6:25 - loss: 0.6973 - acc: 0.5336
3072/5677 [===============>..............] - ETA: 6:15 - loss: 0.6972 - acc: 0.5326
3136/5677 [===============>..............] - ETA: 6:06 - loss: 0.6972 - acc: 0.5319
3200/5677 [===============>..............] - ETA: 5:57 - loss: 0.6970 - acc: 0.5319
3264/5677 [================>.............] - ETA: 5:47 - loss: 0.6973 - acc: 0.5312
3328/5677 [================>.............] - ETA: 5:37 - loss: 0.6969 - acc: 0.5316
3392/5677 [================>.............] - ETA: 5:27 - loss: 0.6972 - acc: 0.5312
3456/5677 [=================>............] - ETA: 5:16 - loss: 0.6971 - acc: 0.5312
3520/5677 [=================>............] - ETA: 5:07 - loss: 0.6966 - acc: 0.5330
3584/5677 [=================>............] - ETA: 4:58 - loss: 0.6972 - acc: 0.5301
3648/5677 [==================>...........] - ETA: 4:49 - loss: 0.6965 - acc: 0.5321
3712/5677 [==================>...........] - ETA: 4:39 - loss: 0.6965 - acc: 0.5310
3776/5677 [==================>...........] - ETA: 4:30 - loss: 0.6965 - acc: 0.5315
3840/5677 [===================>..........] - ETA: 4:21 - loss: 0.6971 - acc: 0.5299
3904/5677 [===================>..........] - ETA: 4:12 - loss: 0.6969 - acc: 0.5295
3968/5677 [===================>..........] - ETA: 4:02 - loss: 0.6966 - acc: 0.5297
4032/5677 [====================>.........] - ETA: 3:52 - loss: 0.6966 - acc: 0.5298
4096/5677 [====================>.........] - ETA: 3:43 - loss: 0.6965 - acc: 0.5298
4160/5677 [====================>.........] - ETA: 3:34 - loss: 0.6962 - acc: 0.5308
4224/5677 [=====================>........] - ETA: 3:25 - loss: 0.6968 - acc: 0.5305
4288/5677 [=====================>........] - ETA: 3:16 - loss: 0.6962 - acc: 0.5324
4352/5677 [=====================>........] - ETA: 3:07 - loss: 0.6956 - acc: 0.5338
4416/5677 [======================>.......] - ETA: 2:58 - loss: 0.6957 - acc: 0.5337
4480/5677 [======================>.......] - ETA: 2:49 - loss: 0.6956 - acc: 0.5330
4544/5677 [=======================>......] - ETA: 2:40 - loss: 0.6954 - acc: 0.5328
4608/5677 [=======================>......] - ETA: 2:31 - loss: 0.6956 - acc: 0.5328
4672/5677 [=======================>......] - ETA: 2:21 - loss: 0.6954 - acc: 0.5325
4736/5677 [========================>.....] - ETA: 2:12 - loss: 0.6954 - acc: 0.5334
4800/5677 [========================>.....] - ETA: 2:03 - loss: 0.6954 - acc: 0.5335
4864/5677 [========================>.....] - ETA: 1:54 - loss: 0.6955 - acc: 0.5339
4928/5677 [=========================>....] - ETA: 1:45 - loss: 0.6956 - acc: 0.5331
4992/5677 [=========================>....] - ETA: 1:36 - loss: 0.6951 - acc: 0.5345
5056/5677 [=========================>....] - ETA: 1:27 - loss: 0.6953 - acc: 0.5342
5120/5677 [==========================>...] - ETA: 1:18 - loss: 0.6953 - acc: 0.5350
5184/5677 [==========================>...] - ETA: 1:09 - loss: 0.6955 - acc: 0.5351
5248/5677 [==========================>...] - ETA: 1:00 - loss: 0.6949 - acc: 0.5364
5312/5677 [===========================>..] - ETA: 51s - loss: 0.6950 - acc: 0.5365 
5376/5677 [===========================>..] - ETA: 42s - loss: 0.6948 - acc: 0.5374
5440/5677 [===========================>..] - ETA: 33s - loss: 0.6947 - acc: 0.5375
5504/5677 [============================>.] - ETA: 24s - loss: 0.6951 - acc: 0.5372
5568/5677 [============================>.] - ETA: 15s - loss: 0.6950 - acc: 0.5370
5632/5677 [============================>.] - ETA: 6s - loss: 0.6949 - acc: 0.5369 
5677/5677 [==============================] - 831s 146ms/step - loss: 0.6949 - acc: 0.5364 - val_loss: 0.6875 - val_acc: 0.5309

Epoch 00002: val_acc did not improve from 0.54517
Epoch 3/10

  64/5677 [..............................] - ETA: 13:57 - loss: 0.6660 - acc: 0.5781
 128/5677 [..............................] - ETA: 13:18 - loss: 0.6877 - acc: 0.5156
 192/5677 [>.............................] - ETA: 12:59 - loss: 0.6896 - acc: 0.5312
 256/5677 [>.............................] - ETA: 12:53 - loss: 0.6932 - acc: 0.5156
 320/5677 [>.............................] - ETA: 12:50 - loss: 0.6890 - acc: 0.5188
 384/5677 [=>............................] - ETA: 12:37 - loss: 0.6867 - acc: 0.5260
 448/5677 [=>............................] - ETA: 12:24 - loss: 0.6836 - acc: 0.5357
 512/5677 [=>............................] - ETA: 12:22 - loss: 0.6858 - acc: 0.5352
 576/5677 [==>...........................] - ETA: 12:24 - loss: 0.6873 - acc: 0.5382
 640/5677 [==>...........................] - ETA: 12:11 - loss: 0.6849 - acc: 0.5437
 704/5677 [==>...........................] - ETA: 11:57 - loss: 0.6889 - acc: 0.5341
 768/5677 [===>..........................] - ETA: 11:49 - loss: 0.6894 - acc: 0.5339
 832/5677 [===>..........................] - ETA: 11:36 - loss: 0.6883 - acc: 0.5397
 896/5677 [===>..........................] - ETA: 11:20 - loss: 0.6865 - acc: 0.5413
 960/5677 [====>.........................] - ETA: 11:11 - loss: 0.6865 - acc: 0.5417
1024/5677 [====>.........................] - ETA: 11:01 - loss: 0.6867 - acc: 0.5420
1088/5677 [====>.........................] - ETA: 10:47 - loss: 0.6847 - acc: 0.5496
1152/5677 [=====>........................] - ETA: 10:36 - loss: 0.6850 - acc: 0.5495
1216/5677 [=====>........................] - ETA: 10:27 - loss: 0.6864 - acc: 0.5493
1280/5677 [=====>........................] - ETA: 10:19 - loss: 0.6870 - acc: 0.5469
1344/5677 [======>.......................] - ETA: 10:07 - loss: 0.6863 - acc: 0.5461
1408/5677 [======>.......................] - ETA: 9:58 - loss: 0.6861 - acc: 0.5483 
1472/5677 [======>.......................] - ETA: 9:44 - loss: 0.6871 - acc: 0.5462
1536/5677 [=======>......................] - ETA: 9:30 - loss: 0.6878 - acc: 0.5475
1600/5677 [=======>......................] - ETA: 9:19 - loss: 0.6891 - acc: 0.5444
1664/5677 [=======>......................] - ETA: 9:08 - loss: 0.6891 - acc: 0.5451
1728/5677 [========>.....................] - ETA: 8:57 - loss: 0.6888 - acc: 0.5457
1792/5677 [========>.....................] - ETA: 8:47 - loss: 0.6886 - acc: 0.5474
1856/5677 [========>.....................] - ETA: 8:38 - loss: 0.6875 - acc: 0.5480
1920/5677 [=========>....................] - ETA: 8:27 - loss: 0.6876 - acc: 0.5469
1984/5677 [=========>....................] - ETA: 8:18 - loss: 0.6875 - acc: 0.5474
2048/5677 [=========>....................] - ETA: 8:10 - loss: 0.6865 - acc: 0.5493
2112/5677 [==========>...................] - ETA: 8:01 - loss: 0.6865 - acc: 0.5488
2176/5677 [==========>...................] - ETA: 7:53 - loss: 0.6843 - acc: 0.5533
2240/5677 [==========>...................] - ETA: 7:45 - loss: 0.6866 - acc: 0.5504
2304/5677 [===========>..................] - ETA: 7:35 - loss: 0.6868 - acc: 0.5499
2368/5677 [===========>..................] - ETA: 7:27 - loss: 0.6881 - acc: 0.5477
2432/5677 [===========>..................] - ETA: 7:18 - loss: 0.6874 - acc: 0.5481
2496/5677 [============>.................] - ETA: 7:09 - loss: 0.6874 - acc: 0.5481
2560/5677 [============>.................] - ETA: 6:59 - loss: 0.6873 - acc: 0.5492
2624/5677 [============>.................] - ETA: 6:50 - loss: 0.6872 - acc: 0.5503
2688/5677 [=============>................] - ETA: 6:41 - loss: 0.6884 - acc: 0.5465
2752/5677 [=============>................] - ETA: 6:31 - loss: 0.6888 - acc: 0.5472
2816/5677 [=============>................] - ETA: 6:23 - loss: 0.6885 - acc: 0.5479
2880/5677 [==============>...............] - ETA: 6:14 - loss: 0.6891 - acc: 0.5479
2944/5677 [==============>...............] - ETA: 6:05 - loss: 0.6897 - acc: 0.5472
3008/5677 [==============>...............] - ETA: 5:56 - loss: 0.6898 - acc: 0.5472
3072/5677 [===============>..............] - ETA: 5:47 - loss: 0.6890 - acc: 0.5495
3136/5677 [===============>..............] - ETA: 5:38 - loss: 0.6887 - acc: 0.5513
3200/5677 [===============>..............] - ETA: 5:29 - loss: 0.6890 - acc: 0.5522
3264/5677 [================>.............] - ETA: 5:21 - loss: 0.6893 - acc: 0.5536
3328/5677 [================>.............] - ETA: 5:11 - loss: 0.6887 - acc: 0.5553
3392/5677 [================>.............] - ETA: 5:02 - loss: 0.6886 - acc: 0.5569
3456/5677 [=================>............] - ETA: 4:54 - loss: 0.6887 - acc: 0.5564
3520/5677 [=================>............] - ETA: 4:45 - loss: 0.6890 - acc: 0.5565
3584/5677 [=================>............] - ETA: 4:37 - loss: 0.6888 - acc: 0.5558
3648/5677 [==================>...........] - ETA: 4:28 - loss: 0.6889 - acc: 0.5548
3712/5677 [==================>...........] - ETA: 4:19 - loss: 0.6885 - acc: 0.5550
3776/5677 [==================>...........] - ETA: 4:12 - loss: 0.6886 - acc: 0.5538
3840/5677 [===================>..........] - ETA: 4:03 - loss: 0.6888 - acc: 0.5534
3904/5677 [===================>..........] - ETA: 3:55 - loss: 0.6890 - acc: 0.5530
3968/5677 [===================>..........] - ETA: 3:46 - loss: 0.6889 - acc: 0.5534
4032/5677 [====================>.........] - ETA: 3:38 - loss: 0.6889 - acc: 0.5533
4096/5677 [====================>.........] - ETA: 3:29 - loss: 0.6895 - acc: 0.5505
4160/5677 [====================>.........] - ETA: 3:20 - loss: 0.6896 - acc: 0.5510
4224/5677 [=====================>........] - ETA: 3:12 - loss: 0.6898 - acc: 0.5509
4288/5677 [=====================>........] - ETA: 3:03 - loss: 0.6901 - acc: 0.5497
4352/5677 [=====================>........] - ETA: 2:55 - loss: 0.6896 - acc: 0.5503
4416/5677 [======================>.......] - ETA: 2:46 - loss: 0.6897 - acc: 0.5503
4480/5677 [======================>.......] - ETA: 2:38 - loss: 0.6897 - acc: 0.5504
4544/5677 [=======================>......] - ETA: 2:29 - loss: 0.6893 - acc: 0.5517
4608/5677 [=======================>......] - ETA: 2:21 - loss: 0.6893 - acc: 0.5501
4672/5677 [=======================>......] - ETA: 2:13 - loss: 0.6892 - acc: 0.5499
4736/5677 [========================>.....] - ETA: 2:04 - loss: 0.6890 - acc: 0.5498
4800/5677 [========================>.....] - ETA: 1:56 - loss: 0.6896 - acc: 0.5490
4864/5677 [========================>.....] - ETA: 1:47 - loss: 0.6894 - acc: 0.5485
4928/5677 [=========================>....] - ETA: 1:39 - loss: 0.6896 - acc: 0.5475
4992/5677 [=========================>....] - ETA: 1:30 - loss: 0.6897 - acc: 0.5487
5056/5677 [=========================>....] - ETA: 1:22 - loss: 0.6900 - acc: 0.5479
5120/5677 [==========================>...] - ETA: 1:13 - loss: 0.6904 - acc: 0.5477
5184/5677 [==========================>...] - ETA: 1:05 - loss: 0.6906 - acc: 0.5473
5248/5677 [==========================>...] - ETA: 56s - loss: 0.6902 - acc: 0.5484 
5312/5677 [===========================>..] - ETA: 48s - loss: 0.6904 - acc: 0.5478
5376/5677 [===========================>..] - ETA: 39s - loss: 0.6904 - acc: 0.5472
5440/5677 [===========================>..] - ETA: 31s - loss: 0.6903 - acc: 0.5472
5504/5677 [============================>.] - ETA: 22s - loss: 0.6902 - acc: 0.5480
5568/5677 [============================>.] - ETA: 14s - loss: 0.6898 - acc: 0.5485
5632/5677 [============================>.] - ETA: 5s - loss: 0.6896 - acc: 0.5487 
5677/5677 [==============================] - 777s 137ms/step - loss: 0.6896 - acc: 0.5487 - val_loss: 0.6755 - val_acc: 0.5832

Epoch 00003: val_acc improved from 0.54517 to 0.58320, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window14/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 4/10

  64/5677 [..............................] - ETA: 13:14 - loss: 0.7106 - acc: 0.5312
 128/5677 [..............................] - ETA: 12:59 - loss: 0.6826 - acc: 0.5938
 192/5677 [>.............................] - ETA: 12:07 - loss: 0.6955 - acc: 0.5521
 256/5677 [>.............................] - ETA: 11:39 - loss: 0.6874 - acc: 0.5664
 320/5677 [>.............................] - ETA: 11:38 - loss: 0.6866 - acc: 0.5594
 384/5677 [=>............................] - ETA: 11:21 - loss: 0.6874 - acc: 0.5521
 448/5677 [=>............................] - ETA: 10:56 - loss: 0.6863 - acc: 0.5603
 512/5677 [=>............................] - ETA: 10:46 - loss: 0.6872 - acc: 0.5664
 576/5677 [==>...........................] - ETA: 10:41 - loss: 0.6858 - acc: 0.5660
 640/5677 [==>...........................] - ETA: 10:36 - loss: 0.6859 - acc: 0.5703
 704/5677 [==>...........................] - ETA: 10:39 - loss: 0.6847 - acc: 0.5739
 768/5677 [===>..........................] - ETA: 10:37 - loss: 0.6865 - acc: 0.5664
 832/5677 [===>..........................] - ETA: 10:31 - loss: 0.6853 - acc: 0.5673
 896/5677 [===>..........................] - ETA: 10:29 - loss: 0.6843 - acc: 0.5692
 960/5677 [====>.........................] - ETA: 10:27 - loss: 0.6844 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 10:23 - loss: 0.6850 - acc: 0.5674
1088/5677 [====>.........................] - ETA: 10:11 - loss: 0.6836 - acc: 0.5653
1152/5677 [=====>........................] - ETA: 10:04 - loss: 0.6842 - acc: 0.5616
1216/5677 [=====>........................] - ETA: 9:56 - loss: 0.6839 - acc: 0.5650 
1280/5677 [=====>........................] - ETA: 9:45 - loss: 0.6856 - acc: 0.5633
1344/5677 [======>.......................] - ETA: 9:38 - loss: 0.6871 - acc: 0.5603
1408/5677 [======>.......................] - ETA: 9:27 - loss: 0.6873 - acc: 0.5575
1472/5677 [======>.......................] - ETA: 9:15 - loss: 0.6892 - acc: 0.5543
1536/5677 [=======>......................] - ETA: 9:08 - loss: 0.6909 - acc: 0.5501
1600/5677 [=======>......................] - ETA: 8:59 - loss: 0.6914 - acc: 0.5537
1664/5677 [=======>......................] - ETA: 8:51 - loss: 0.6918 - acc: 0.5529
1728/5677 [========>.....................] - ETA: 8:41 - loss: 0.6902 - acc: 0.5538
1792/5677 [========>.....................] - ETA: 8:30 - loss: 0.6898 - acc: 0.5547
1856/5677 [========>.....................] - ETA: 8:23 - loss: 0.6902 - acc: 0.5555
1920/5677 [=========>....................] - ETA: 8:14 - loss: 0.6891 - acc: 0.5578
1984/5677 [=========>....................] - ETA: 8:04 - loss: 0.6883 - acc: 0.5600
2048/5677 [=========>....................] - ETA: 7:55 - loss: 0.6875 - acc: 0.5625
2112/5677 [==========>...................] - ETA: 7:45 - loss: 0.6868 - acc: 0.5634
2176/5677 [==========>...................] - ETA: 7:37 - loss: 0.6874 - acc: 0.5625
2240/5677 [==========>...................] - ETA: 7:27 - loss: 0.6875 - acc: 0.5625
2304/5677 [===========>..................] - ETA: 7:18 - loss: 0.6884 - acc: 0.5616
2368/5677 [===========>..................] - ETA: 7:09 - loss: 0.6883 - acc: 0.5617
2432/5677 [===========>..................] - ETA: 7:00 - loss: 0.6895 - acc: 0.5592
2496/5677 [============>.................] - ETA: 6:51 - loss: 0.6900 - acc: 0.5581
2560/5677 [============>.................] - ETA: 6:43 - loss: 0.6915 - acc: 0.5547
2624/5677 [============>.................] - ETA: 6:34 - loss: 0.6912 - acc: 0.5556
2688/5677 [=============>................] - ETA: 6:25 - loss: 0.6910 - acc: 0.5569
2752/5677 [=============>................] - ETA: 6:16 - loss: 0.6904 - acc: 0.5581
2816/5677 [=============>................] - ETA: 6:08 - loss: 0.6902 - acc: 0.5593
2880/5677 [==============>...............] - ETA: 6:00 - loss: 0.6897 - acc: 0.5608
2944/5677 [==============>...............] - ETA: 5:52 - loss: 0.6889 - acc: 0.5622
3008/5677 [==============>...............] - ETA: 5:44 - loss: 0.6893 - acc: 0.5602
3072/5677 [===============>..............] - ETA: 5:36 - loss: 0.6887 - acc: 0.5622
3136/5677 [===============>..............] - ETA: 5:27 - loss: 0.6890 - acc: 0.5619
3200/5677 [===============>..............] - ETA: 5:20 - loss: 0.6890 - acc: 0.5625
3264/5677 [================>.............] - ETA: 5:11 - loss: 0.6893 - acc: 0.5625
3328/5677 [================>.............] - ETA: 5:03 - loss: 0.6894 - acc: 0.5613
3392/5677 [================>.............] - ETA: 4:55 - loss: 0.6898 - acc: 0.5598
3456/5677 [=================>............] - ETA: 4:47 - loss: 0.6899 - acc: 0.5593
3520/5677 [=================>............] - ETA: 4:38 - loss: 0.6904 - acc: 0.5580
3584/5677 [=================>............] - ETA: 4:30 - loss: 0.6903 - acc: 0.5575
3648/5677 [==================>...........] - ETA: 4:22 - loss: 0.6900 - acc: 0.5576
3712/5677 [==================>...........] - ETA: 4:14 - loss: 0.6898 - acc: 0.5571
3776/5677 [==================>...........] - ETA: 4:05 - loss: 0.6898 - acc: 0.5564
3840/5677 [===================>..........] - ETA: 3:57 - loss: 0.6894 - acc: 0.5586
3904/5677 [===================>..........] - ETA: 3:49 - loss: 0.6888 - acc: 0.5605
3968/5677 [===================>..........] - ETA: 3:41 - loss: 0.6890 - acc: 0.5597
4032/5677 [====================>.........] - ETA: 3:33 - loss: 0.6897 - acc: 0.5585
4096/5677 [====================>.........] - ETA: 3:25 - loss: 0.6899 - acc: 0.5588
4160/5677 [====================>.........] - ETA: 3:17 - loss: 0.6894 - acc: 0.5591
4224/5677 [=====================>........] - ETA: 3:09 - loss: 0.6898 - acc: 0.5580
4288/5677 [=====================>........] - ETA: 3:01 - loss: 0.6899 - acc: 0.5581
4352/5677 [=====================>........] - ETA: 2:52 - loss: 0.6892 - acc: 0.5593
4416/5677 [======================>.......] - ETA: 2:44 - loss: 0.6892 - acc: 0.5587
4480/5677 [======================>.......] - ETA: 2:36 - loss: 0.6882 - acc: 0.5618
4544/5677 [=======================>......] - ETA: 2:28 - loss: 0.6883 - acc: 0.5621
4608/5677 [=======================>......] - ETA: 2:19 - loss: 0.6886 - acc: 0.5614
4672/5677 [=======================>......] - ETA: 2:11 - loss: 0.6887 - acc: 0.5616
4736/5677 [========================>.....] - ETA: 2:03 - loss: 0.6891 - acc: 0.5604
4800/5677 [========================>.....] - ETA: 1:54 - loss: 0.6894 - acc: 0.5594
4864/5677 [========================>.....] - ETA: 1:46 - loss: 0.6896 - acc: 0.5590
4928/5677 [=========================>....] - ETA: 1:37 - loss: 0.6895 - acc: 0.5582
4992/5677 [=========================>....] - ETA: 1:29 - loss: 0.6893 - acc: 0.5585
5056/5677 [=========================>....] - ETA: 1:21 - loss: 0.6884 - acc: 0.5599
5120/5677 [==========================>...] - ETA: 1:12 - loss: 0.6881 - acc: 0.5615
5184/5677 [==========================>...] - ETA: 1:04 - loss: 0.6880 - acc: 0.5613
5248/5677 [==========================>...] - ETA: 56s - loss: 0.6877 - acc: 0.5619 
5312/5677 [===========================>..] - ETA: 47s - loss: 0.6878 - acc: 0.5619
5376/5677 [===========================>..] - ETA: 39s - loss: 0.6878 - acc: 0.5616
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6879 - acc: 0.5614
5504/5677 [============================>.] - ETA: 22s - loss: 0.6879 - acc: 0.5610
5568/5677 [============================>.] - ETA: 14s - loss: 0.6880 - acc: 0.5605
5632/5677 [============================>.] - ETA: 5s - loss: 0.6882 - acc: 0.5598 
5677/5677 [==============================] - 764s 135ms/step - loss: 0.6880 - acc: 0.5607 - val_loss: 0.6776 - val_acc: 0.5737

Epoch 00004: val_acc did not improve from 0.58320
Epoch 5/10

  64/5677 [..............................] - ETA: 11:46 - loss: 0.6857 - acc: 0.5938
 128/5677 [..............................] - ETA: 11:33 - loss: 0.6706 - acc: 0.6016
 192/5677 [>.............................] - ETA: 11:33 - loss: 0.6672 - acc: 0.6354
 256/5677 [>.............................] - ETA: 11:14 - loss: 0.6620 - acc: 0.6406
 320/5677 [>.............................] - ETA: 10:59 - loss: 0.6655 - acc: 0.6250
 384/5677 [=>............................] - ETA: 10:55 - loss: 0.6694 - acc: 0.6068
 448/5677 [=>............................] - ETA: 10:53 - loss: 0.6643 - acc: 0.6094
 512/5677 [=>............................] - ETA: 10:29 - loss: 0.6692 - acc: 0.6074
 576/5677 [==>...........................] - ETA: 10:20 - loss: 0.6678 - acc: 0.6059
 640/5677 [==>...........................] - ETA: 10:12 - loss: 0.6702 - acc: 0.6016
 704/5677 [==>...........................] - ETA: 10:03 - loss: 0.6741 - acc: 0.5909
 768/5677 [===>..........................] - ETA: 9:55 - loss: 0.6761 - acc: 0.5859 
 832/5677 [===>..........................] - ETA: 9:48 - loss: 0.6786 - acc: 0.5829
 896/5677 [===>..........................] - ETA: 9:35 - loss: 0.6814 - acc: 0.5826
 960/5677 [====>.........................] - ETA: 9:27 - loss: 0.6795 - acc: 0.5875
1024/5677 [====>.........................] - ETA: 9:19 - loss: 0.6804 - acc: 0.5830
1088/5677 [====>.........................] - ETA: 9:12 - loss: 0.6796 - acc: 0.5836
1152/5677 [=====>........................] - ETA: 9:02 - loss: 0.6780 - acc: 0.5859
1216/5677 [=====>........................] - ETA: 8:53 - loss: 0.6782 - acc: 0.5847
1280/5677 [=====>........................] - ETA: 8:44 - loss: 0.6765 - acc: 0.5836
1344/5677 [======>.......................] - ETA: 8:40 - loss: 0.6778 - acc: 0.5848
1408/5677 [======>.......................] - ETA: 8:33 - loss: 0.6790 - acc: 0.5831
1472/5677 [======>.......................] - ETA: 8:29 - loss: 0.6781 - acc: 0.5842
1536/5677 [=======>......................] - ETA: 8:22 - loss: 0.6791 - acc: 0.5820
1600/5677 [=======>......................] - ETA: 8:15 - loss: 0.6794 - acc: 0.5813
1664/5677 [=======>......................] - ETA: 8:07 - loss: 0.6786 - acc: 0.5835
1728/5677 [========>.....................] - ETA: 7:59 - loss: 0.6791 - acc: 0.5828
1792/5677 [========>.....................] - ETA: 7:50 - loss: 0.6799 - acc: 0.5820
1856/5677 [========>.....................] - ETA: 7:40 - loss: 0.6799 - acc: 0.5819
1920/5677 [=========>....................] - ETA: 7:32 - loss: 0.6795 - acc: 0.5828
1984/5677 [=========>....................] - ETA: 7:26 - loss: 0.6794 - acc: 0.5801
2048/5677 [=========>....................] - ETA: 7:20 - loss: 0.6792 - acc: 0.5801
2112/5677 [==========>...................] - ETA: 7:13 - loss: 0.6785 - acc: 0.5814
2176/5677 [==========>...................] - ETA: 7:07 - loss: 0.6782 - acc: 0.5818
2240/5677 [==========>...................] - ETA: 7:00 - loss: 0.6776 - acc: 0.5839
2304/5677 [===========>..................] - ETA: 6:53 - loss: 0.6765 - acc: 0.5851
2368/5677 [===========>..................] - ETA: 6:47 - loss: 0.6775 - acc: 0.5840
2432/5677 [===========>..................] - ETA: 6:40 - loss: 0.6774 - acc: 0.5847
2496/5677 [============>.................] - ETA: 6:33 - loss: 0.6780 - acc: 0.5833
2560/5677 [============>.................] - ETA: 6:26 - loss: 0.6778 - acc: 0.5840
2624/5677 [============>.................] - ETA: 6:18 - loss: 0.6769 - acc: 0.5827
2688/5677 [=============>................] - ETA: 6:10 - loss: 0.6776 - acc: 0.5826
2752/5677 [=============>................] - ETA: 6:02 - loss: 0.6772 - acc: 0.5843
2816/5677 [=============>................] - ETA: 5:55 - loss: 0.6772 - acc: 0.5827
2880/5677 [==============>...............] - ETA: 5:46 - loss: 0.6766 - acc: 0.5837
2944/5677 [==============>...............] - ETA: 5:38 - loss: 0.6763 - acc: 0.5836
3008/5677 [==============>...............] - ETA: 5:30 - loss: 0.6762 - acc: 0.5828
3072/5677 [===============>..............] - ETA: 5:22 - loss: 0.6764 - acc: 0.5837
3136/5677 [===============>..............] - ETA: 5:13 - loss: 0.6762 - acc: 0.5845
3200/5677 [===============>..............] - ETA: 5:05 - loss: 0.6757 - acc: 0.5856
3264/5677 [================>.............] - ETA: 4:57 - loss: 0.6771 - acc: 0.5833
3328/5677 [================>.............] - ETA: 4:50 - loss: 0.6764 - acc: 0.5832
3392/5677 [================>.............] - ETA: 4:41 - loss: 0.6768 - acc: 0.5828
3456/5677 [=================>............] - ETA: 4:33 - loss: 0.6778 - acc: 0.5810
3520/5677 [=================>............] - ETA: 4:25 - loss: 0.6771 - acc: 0.5813
3584/5677 [=================>............] - ETA: 4:17 - loss: 0.6776 - acc: 0.5812
3648/5677 [==================>...........] - ETA: 4:09 - loss: 0.6780 - acc: 0.5809
3712/5677 [==================>...........] - ETA: 4:02 - loss: 0.6786 - acc: 0.5792
3776/5677 [==================>...........] - ETA: 3:54 - loss: 0.6792 - acc: 0.5787
3840/5677 [===================>..........] - ETA: 3:47 - loss: 0.6801 - acc: 0.5758
3904/5677 [===================>..........] - ETA: 3:39 - loss: 0.6804 - acc: 0.5751
3968/5677 [===================>..........] - ETA: 3:32 - loss: 0.6810 - acc: 0.5736
4032/5677 [====================>.........] - ETA: 3:24 - loss: 0.6818 - acc: 0.5722
4096/5677 [====================>.........] - ETA: 3:17 - loss: 0.6815 - acc: 0.5728
4160/5677 [====================>.........] - ETA: 3:09 - loss: 0.6815 - acc: 0.5726
4224/5677 [=====================>........] - ETA: 3:01 - loss: 0.6815 - acc: 0.5724
4288/5677 [=====================>........] - ETA: 2:54 - loss: 0.6819 - acc: 0.5709
4352/5677 [=====================>........] - ETA: 2:46 - loss: 0.6816 - acc: 0.5710
4416/5677 [======================>.......] - ETA: 2:38 - loss: 0.6820 - acc: 0.5702
4480/5677 [======================>.......] - ETA: 2:30 - loss: 0.6820 - acc: 0.5701
4544/5677 [=======================>......] - ETA: 2:22 - loss: 0.6823 - acc: 0.5693
4608/5677 [=======================>......] - ETA: 2:14 - loss: 0.6825 - acc: 0.5692
4672/5677 [=======================>......] - ETA: 2:06 - loss: 0.6822 - acc: 0.5700
4736/5677 [========================>.....] - ETA: 1:58 - loss: 0.6826 - acc: 0.5695
4800/5677 [========================>.....] - ETA: 1:50 - loss: 0.6828 - acc: 0.5681
4864/5677 [========================>.....] - ETA: 1:42 - loss: 0.6827 - acc: 0.5687
4928/5677 [=========================>....] - ETA: 1:34 - loss: 0.6828 - acc: 0.5682
4992/5677 [=========================>....] - ETA: 1:26 - loss: 0.6831 - acc: 0.5673
5056/5677 [=========================>....] - ETA: 1:18 - loss: 0.6832 - acc: 0.5672
5120/5677 [==========================>...] - ETA: 1:10 - loss: 0.6829 - acc: 0.5678
5184/5677 [==========================>...] - ETA: 1:02 - loss: 0.6830 - acc: 0.5683
5248/5677 [==========================>...] - ETA: 54s - loss: 0.6827 - acc: 0.5686 
5312/5677 [===========================>..] - ETA: 46s - loss: 0.6826 - acc: 0.5687
5376/5677 [===========================>..] - ETA: 38s - loss: 0.6828 - acc: 0.5675
5440/5677 [===========================>..] - ETA: 29s - loss: 0.6829 - acc: 0.5675
5504/5677 [============================>.] - ETA: 21s - loss: 0.6827 - acc: 0.5681
5568/5677 [============================>.] - ETA: 13s - loss: 0.6824 - acc: 0.5693
5632/5677 [============================>.] - ETA: 5s - loss: 0.6826 - acc: 0.5680 
5677/5677 [==============================] - 748s 132ms/step - loss: 0.6829 - acc: 0.5676 - val_loss: 0.6714 - val_acc: 0.5943

Epoch 00005: val_acc improved from 0.58320 to 0.59429, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window14/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 12:12 - loss: 0.6790 - acc: 0.5781
 128/5677 [..............................] - ETA: 11:17 - loss: 0.6612 - acc: 0.5703
 192/5677 [>.............................] - ETA: 11:04 - loss: 0.6737 - acc: 0.5573
 256/5677 [>.............................] - ETA: 11:16 - loss: 0.6736 - acc: 0.5586
 320/5677 [>.............................] - ETA: 11:13 - loss: 0.6802 - acc: 0.5406
 384/5677 [=>............................] - ETA: 11:15 - loss: 0.6803 - acc: 0.5365
 448/5677 [=>............................] - ETA: 11:14 - loss: 0.6774 - acc: 0.5491
 512/5677 [=>............................] - ETA: 11:08 - loss: 0.6809 - acc: 0.5312
 576/5677 [==>...........................] - ETA: 11:01 - loss: 0.6804 - acc: 0.5312
 640/5677 [==>...........................] - ETA: 10:51 - loss: 0.6828 - acc: 0.5312
 704/5677 [==>...........................] - ETA: 10:50 - loss: 0.6810 - acc: 0.5398
 768/5677 [===>..........................] - ETA: 10:47 - loss: 0.6794 - acc: 0.5404
 832/5677 [===>..........................] - ETA: 10:43 - loss: 0.6789 - acc: 0.5421
 896/5677 [===>..........................] - ETA: 10:34 - loss: 0.6772 - acc: 0.5513
 960/5677 [====>.........................] - ETA: 10:28 - loss: 0.6773 - acc: 0.5521
1024/5677 [====>.........................] - ETA: 10:20 - loss: 0.6789 - acc: 0.5479
1088/5677 [====>.........................] - ETA: 10:13 - loss: 0.6790 - acc: 0.5515
1152/5677 [=====>........................] - ETA: 10:04 - loss: 0.6815 - acc: 0.5477
1216/5677 [=====>........................] - ETA: 9:55 - loss: 0.6815 - acc: 0.5493 
1280/5677 [=====>........................] - ETA: 9:49 - loss: 0.6815 - acc: 0.5500
1344/5677 [======>.......................] - ETA: 9:43 - loss: 0.6808 - acc: 0.5506
1408/5677 [======>.......................] - ETA: 9:37 - loss: 0.6818 - acc: 0.5476
1472/5677 [======>.......................] - ETA: 9:27 - loss: 0.6811 - acc: 0.5496
1536/5677 [=======>......................] - ETA: 9:19 - loss: 0.6829 - acc: 0.5462
1600/5677 [=======>......................] - ETA: 9:12 - loss: 0.6816 - acc: 0.5500
1664/5677 [=======>......................] - ETA: 9:05 - loss: 0.6818 - acc: 0.5511
1728/5677 [========>.....................] - ETA: 8:56 - loss: 0.6816 - acc: 0.5515
1792/5677 [========>.....................] - ETA: 8:48 - loss: 0.6809 - acc: 0.5541
1856/5677 [========>.....................] - ETA: 8:39 - loss: 0.6792 - acc: 0.5571
1920/5677 [=========>....................] - ETA: 8:31 - loss: 0.6799 - acc: 0.5573
1984/5677 [=========>....................] - ETA: 8:22 - loss: 0.6808 - acc: 0.5549
2048/5677 [=========>....................] - ETA: 8:12 - loss: 0.6810 - acc: 0.5557
2112/5677 [==========>...................] - ETA: 8:03 - loss: 0.6806 - acc: 0.5563
2176/5677 [==========>...................] - ETA: 7:54 - loss: 0.6804 - acc: 0.5593
2240/5677 [==========>...................] - ETA: 7:46 - loss: 0.6791 - acc: 0.5621
2304/5677 [===========>..................] - ETA: 7:37 - loss: 0.6780 - acc: 0.5647
2368/5677 [===========>..................] - ETA: 7:29 - loss: 0.6789 - acc: 0.5629
2432/5677 [===========>..................] - ETA: 7:20 - loss: 0.6789 - acc: 0.5629
2496/5677 [============>.................] - ETA: 7:11 - loss: 0.6789 - acc: 0.5641
2560/5677 [============>.................] - ETA: 7:04 - loss: 0.6784 - acc: 0.5660
2624/5677 [============>.................] - ETA: 6:55 - loss: 0.6798 - acc: 0.5644
2688/5677 [=============>................] - ETA: 6:46 - loss: 0.6811 - acc: 0.5636
2752/5677 [=============>................] - ETA: 6:39 - loss: 0.6805 - acc: 0.5636
2816/5677 [=============>................] - ETA: 6:30 - loss: 0.6807 - acc: 0.5636
2880/5677 [==============>...............] - ETA: 6:21 - loss: 0.6812 - acc: 0.5625
2944/5677 [==============>...............] - ETA: 6:12 - loss: 0.6810 - acc: 0.5645
3008/5677 [==============>...............] - ETA: 6:03 - loss: 0.6802 - acc: 0.5662
3072/5677 [===============>..............] - ETA: 5:54 - loss: 0.6802 - acc: 0.5661
3136/5677 [===============>..............] - ETA: 5:45 - loss: 0.6806 - acc: 0.5644
3200/5677 [===============>..............] - ETA: 5:37 - loss: 0.6812 - acc: 0.5634
3264/5677 [================>.............] - ETA: 5:29 - loss: 0.6817 - acc: 0.5622
3328/5677 [================>.............] - ETA: 5:20 - loss: 0.6811 - acc: 0.5640
3392/5677 [================>.............] - ETA: 5:11 - loss: 0.6810 - acc: 0.5640
3456/5677 [=================>............] - ETA: 5:02 - loss: 0.6813 - acc: 0.5631
3520/5677 [=================>............] - ETA: 4:53 - loss: 0.6815 - acc: 0.5639
3584/5677 [=================>............] - ETA: 4:45 - loss: 0.6806 - acc: 0.5653
3648/5677 [==================>...........] - ETA: 4:36 - loss: 0.6806 - acc: 0.5661
3712/5677 [==================>...........] - ETA: 4:27 - loss: 0.6810 - acc: 0.5649
3776/5677 [==================>...........] - ETA: 4:18 - loss: 0.6811 - acc: 0.5651
3840/5677 [===================>..........] - ETA: 4:09 - loss: 0.6813 - acc: 0.5648
3904/5677 [===================>..........] - ETA: 4:01 - loss: 0.6811 - acc: 0.5653
3968/5677 [===================>..........] - ETA: 3:52 - loss: 0.6817 - acc: 0.5633
4032/5677 [====================>.........] - ETA: 3:43 - loss: 0.6820 - acc: 0.5630
4096/5677 [====================>.........] - ETA: 3:34 - loss: 0.6819 - acc: 0.5625
4160/5677 [====================>.........] - ETA: 3:25 - loss: 0.6814 - acc: 0.5635
4224/5677 [=====================>........] - ETA: 3:16 - loss: 0.6818 - acc: 0.5623
4288/5677 [=====================>........] - ETA: 3:08 - loss: 0.6819 - acc: 0.5606
4352/5677 [=====================>........] - ETA: 2:59 - loss: 0.6819 - acc: 0.5611
4416/5677 [======================>.......] - ETA: 2:50 - loss: 0.6819 - acc: 0.5614
4480/5677 [======================>.......] - ETA: 2:41 - loss: 0.6821 - acc: 0.5609
4544/5677 [=======================>......] - ETA: 2:32 - loss: 0.6823 - acc: 0.5599
4608/5677 [=======================>......] - ETA: 2:23 - loss: 0.6826 - acc: 0.5601
4672/5677 [=======================>......] - ETA: 2:15 - loss: 0.6826 - acc: 0.5604
4736/5677 [========================>.....] - ETA: 2:06 - loss: 0.6827 - acc: 0.5593
4800/5677 [========================>.....] - ETA: 1:58 - loss: 0.6823 - acc: 0.5613
4864/5677 [========================>.....] - ETA: 1:49 - loss: 0.6817 - acc: 0.5627
4928/5677 [=========================>....] - ETA: 1:41 - loss: 0.6817 - acc: 0.5621
4992/5677 [=========================>....] - ETA: 1:32 - loss: 0.6816 - acc: 0.5633
5056/5677 [=========================>....] - ETA: 1:23 - loss: 0.6812 - acc: 0.5655
5120/5677 [==========================>...] - ETA: 1:15 - loss: 0.6810 - acc: 0.5656
5184/5677 [==========================>...] - ETA: 1:06 - loss: 0.6812 - acc: 0.5650
5248/5677 [==========================>...] - ETA: 57s - loss: 0.6811 - acc: 0.5652 
5312/5677 [===========================>..] - ETA: 49s - loss: 0.6810 - acc: 0.5659
5376/5677 [===========================>..] - ETA: 40s - loss: 0.6813 - acc: 0.5658
5440/5677 [===========================>..] - ETA: 32s - loss: 0.6816 - acc: 0.5647
5504/5677 [============================>.] - ETA: 23s - loss: 0.6820 - acc: 0.5636
5568/5677 [============================>.] - ETA: 14s - loss: 0.6818 - acc: 0.5639
5632/5677 [============================>.] - ETA: 6s - loss: 0.6818 - acc: 0.5645 
5677/5677 [==============================] - 798s 141ms/step - loss: 0.6818 - acc: 0.5644 - val_loss: 0.6850 - val_acc: 0.5594

Epoch 00006: val_acc did not improve from 0.59429
Epoch 7/10

  64/5677 [..............................] - ETA: 12:42 - loss: 0.6677 - acc: 0.6094
 128/5677 [..............................] - ETA: 12:11 - loss: 0.6926 - acc: 0.5391
 192/5677 [>.............................] - ETA: 11:59 - loss: 0.6884 - acc: 0.5417
 256/5677 [>.............................] - ETA: 11:29 - loss: 0.6749 - acc: 0.5703
 320/5677 [>.............................] - ETA: 11:34 - loss: 0.6622 - acc: 0.5969
 384/5677 [=>............................] - ETA: 11:28 - loss: 0.6660 - acc: 0.5964
 448/5677 [=>............................] - ETA: 11:08 - loss: 0.6743 - acc: 0.5781
 512/5677 [=>............................] - ETA: 10:56 - loss: 0.6757 - acc: 0.5801
 576/5677 [==>...........................] - ETA: 10:59 - loss: 0.6766 - acc: 0.5851
 640/5677 [==>...........................] - ETA: 10:49 - loss: 0.6785 - acc: 0.5828
 704/5677 [==>...........................] - ETA: 10:40 - loss: 0.6811 - acc: 0.5795
 768/5677 [===>..........................] - ETA: 10:39 - loss: 0.6780 - acc: 0.5846
 832/5677 [===>..........................] - ETA: 10:33 - loss: 0.6779 - acc: 0.5793
 896/5677 [===>..........................] - ETA: 10:32 - loss: 0.6791 - acc: 0.5770
 960/5677 [====>.........................] - ETA: 10:18 - loss: 0.6764 - acc: 0.5802
1024/5677 [====>.........................] - ETA: 10:12 - loss: 0.6756 - acc: 0.5781
1088/5677 [====>.........................] - ETA: 10:09 - loss: 0.6770 - acc: 0.5754
1152/5677 [=====>........................] - ETA: 10:02 - loss: 0.6780 - acc: 0.5755
1216/5677 [=====>........................] - ETA: 9:53 - loss: 0.6762 - acc: 0.5773 
1280/5677 [=====>........................] - ETA: 9:42 - loss: 0.6770 - acc: 0.5789
1344/5677 [======>.......................] - ETA: 9:35 - loss: 0.6773 - acc: 0.5804
1408/5677 [======>.......................] - ETA: 9:27 - loss: 0.6781 - acc: 0.5788
1472/5677 [======>.......................] - ETA: 9:20 - loss: 0.6783 - acc: 0.5788
1536/5677 [=======>......................] - ETA: 9:10 - loss: 0.6760 - acc: 0.5840
1600/5677 [=======>......................] - ETA: 9:02 - loss: 0.6760 - acc: 0.5825
1664/5677 [=======>......................] - ETA: 8:51 - loss: 0.6753 - acc: 0.5829
1728/5677 [========>.....................] - ETA: 8:40 - loss: 0.6735 - acc: 0.5880
1792/5677 [========>.....................] - ETA: 8:31 - loss: 0.6736 - acc: 0.5887
1856/5677 [========>.....................] - ETA: 8:21 - loss: 0.6721 - acc: 0.5921
1920/5677 [=========>....................] - ETA: 8:09 - loss: 0.6721 - acc: 0.5896
1984/5677 [=========>....................] - ETA: 8:00 - loss: 0.6733 - acc: 0.5872
2048/5677 [=========>....................] - ETA: 7:50 - loss: 0.6726 - acc: 0.5894
2112/5677 [==========>...................] - ETA: 7:40 - loss: 0.6731 - acc: 0.5900
2176/5677 [==========>...................] - ETA: 7:30 - loss: 0.6740 - acc: 0.5873
2240/5677 [==========>...................] - ETA: 7:22 - loss: 0.6744 - acc: 0.5862
2304/5677 [===========>..................] - ETA: 7:14 - loss: 0.6749 - acc: 0.5859
2368/5677 [===========>..................] - ETA: 7:05 - loss: 0.6735 - acc: 0.5878
2432/5677 [===========>..................] - ETA: 6:59 - loss: 0.6720 - acc: 0.5884
2496/5677 [============>.................] - ETA: 6:52 - loss: 0.6723 - acc: 0.5877
2560/5677 [============>.................] - ETA: 6:44 - loss: 0.6725 - acc: 0.5875
2624/5677 [============>.................] - ETA: 6:36 - loss: 0.6722 - acc: 0.5869
2688/5677 [=============>................] - ETA: 6:29 - loss: 0.6732 - acc: 0.5859
2752/5677 [=============>................] - ETA: 6:21 - loss: 0.6727 - acc: 0.5850
2816/5677 [=============>................] - ETA: 6:13 - loss: 0.6729 - acc: 0.5849
2880/5677 [==============>...............] - ETA: 6:04 - loss: 0.6723 - acc: 0.5865
2944/5677 [==============>...............] - ETA: 5:56 - loss: 0.6721 - acc: 0.5866
3008/5677 [==============>...............] - ETA: 5:47 - loss: 0.6720 - acc: 0.5868
3072/5677 [===============>..............] - ETA: 5:39 - loss: 0.6707 - acc: 0.5885
3136/5677 [===============>..............] - ETA: 5:31 - loss: 0.6709 - acc: 0.5877
3200/5677 [===============>..............] - ETA: 5:24 - loss: 0.6702 - acc: 0.5875
3264/5677 [================>.............] - ETA: 5:17 - loss: 0.6717 - acc: 0.5852
3328/5677 [================>.............] - ETA: 5:08 - loss: 0.6723 - acc: 0.5853
3392/5677 [================>.............] - ETA: 4:59 - loss: 0.6726 - acc: 0.5852
3456/5677 [=================>............] - ETA: 4:51 - loss: 0.6728 - acc: 0.5842
3520/5677 [=================>............] - ETA: 4:43 - loss: 0.6728 - acc: 0.5838
3584/5677 [=================>............] - ETA: 4:35 - loss: 0.6725 - acc: 0.5843
3648/5677 [==================>...........] - ETA: 4:26 - loss: 0.6726 - acc: 0.5831
3712/5677 [==================>...........] - ETA: 4:19 - loss: 0.6732 - acc: 0.5814
3776/5677 [==================>...........] - ETA: 4:11 - loss: 0.6731 - acc: 0.5821
3840/5677 [===================>..........] - ETA: 4:03 - loss: 0.6734 - acc: 0.5807
3904/5677 [===================>..........] - ETA: 3:55 - loss: 0.6733 - acc: 0.5812
3968/5677 [===================>..........] - ETA: 3:47 - loss: 0.6739 - acc: 0.5789
4032/5677 [====================>.........] - ETA: 3:40 - loss: 0.6743 - acc: 0.5776
4096/5677 [====================>.........] - ETA: 3:32 - loss: 0.6746 - acc: 0.5771
4160/5677 [====================>.........] - ETA: 3:23 - loss: 0.6757 - acc: 0.5755
4224/5677 [=====================>........] - ETA: 3:15 - loss: 0.6758 - acc: 0.5762
4288/5677 [=====================>........] - ETA: 3:07 - loss: 0.6758 - acc: 0.5760
4352/5677 [=====================>........] - ETA: 2:59 - loss: 0.6761 - acc: 0.5756
4416/5677 [======================>.......] - ETA: 2:51 - loss: 0.6763 - acc: 0.5745
4480/5677 [======================>.......] - ETA: 2:42 - loss: 0.6765 - acc: 0.5737
4544/5677 [=======================>......] - ETA: 2:34 - loss: 0.6765 - acc: 0.5742
4608/5677 [=======================>......] - ETA: 2:25 - loss: 0.6762 - acc: 0.5742
4672/5677 [=======================>......] - ETA: 2:17 - loss: 0.6763 - acc: 0.5745
4736/5677 [========================>.....] - ETA: 2:08 - loss: 0.6765 - acc: 0.5733
4800/5677 [========================>.....] - ETA: 1:59 - loss: 0.6766 - acc: 0.5727
4864/5677 [========================>.....] - ETA: 1:50 - loss: 0.6764 - acc: 0.5722
4928/5677 [=========================>....] - ETA: 1:42 - loss: 0.6764 - acc: 0.5714
4992/5677 [=========================>....] - ETA: 1:33 - loss: 0.6767 - acc: 0.5713
5056/5677 [=========================>....] - ETA: 1:24 - loss: 0.6766 - acc: 0.5716
5120/5677 [==========================>...] - ETA: 1:15 - loss: 0.6764 - acc: 0.5725
5184/5677 [==========================>...] - ETA: 1:07 - loss: 0.6756 - acc: 0.5747
5248/5677 [==========================>...] - ETA: 58s - loss: 0.6752 - acc: 0.5751 
5312/5677 [===========================>..] - ETA: 49s - loss: 0.6751 - acc: 0.5751
5376/5677 [===========================>..] - ETA: 41s - loss: 0.6753 - acc: 0.5755
5440/5677 [===========================>..] - ETA: 32s - loss: 0.6753 - acc: 0.5763
5504/5677 [============================>.] - ETA: 23s - loss: 0.6754 - acc: 0.5758
5568/5677 [============================>.] - ETA: 14s - loss: 0.6755 - acc: 0.5760
5632/5677 [============================>.] - ETA: 6s - loss: 0.6759 - acc: 0.5751 
5677/5677 [==============================] - 799s 141ms/step - loss: 0.6756 - acc: 0.5762 - val_loss: 0.6908 - val_acc: 0.5784

Epoch 00007: val_acc did not improve from 0.59429
Epoch 8/10

  64/5677 [..............................] - ETA: 12:30 - loss: 0.6726 - acc: 0.6094
 128/5677 [..............................] - ETA: 12:24 - loss: 0.6860 - acc: 0.5938
 192/5677 [>.............................] - ETA: 12:35 - loss: 0.6997 - acc: 0.5677
 256/5677 [>.............................] - ETA: 12:52 - loss: 0.7034 - acc: 0.5391
 320/5677 [>.............................] - ETA: 12:36 - loss: 0.6996 - acc: 0.5437
 384/5677 [=>............................] - ETA: 12:34 - loss: 0.7035 - acc: 0.5339
 448/5677 [=>............................] - ETA: 12:23 - loss: 0.7059 - acc: 0.5379
 512/5677 [=>............................] - ETA: 12:05 - loss: 0.6983 - acc: 0.5508
 576/5677 [==>...........................] - ETA: 11:48 - loss: 0.6981 - acc: 0.5538
 640/5677 [==>...........................] - ETA: 11:42 - loss: 0.7000 - acc: 0.5484
 704/5677 [==>...........................] - ETA: 11:34 - loss: 0.6952 - acc: 0.5540
 768/5677 [===>..........................] - ETA: 11:23 - loss: 0.6926 - acc: 0.5599
 832/5677 [===>..........................] - ETA: 11:13 - loss: 0.6933 - acc: 0.5613
 896/5677 [===>..........................] - ETA: 11:05 - loss: 0.6931 - acc: 0.5636
 960/5677 [====>.........................] - ETA: 10:56 - loss: 0.6901 - acc: 0.5677
1024/5677 [====>.........................] - ETA: 10:51 - loss: 0.6905 - acc: 0.5645
1088/5677 [====>.........................] - ETA: 10:39 - loss: 0.6919 - acc: 0.5597
1152/5677 [=====>........................] - ETA: 10:26 - loss: 0.6945 - acc: 0.5573
1216/5677 [=====>........................] - ETA: 10:19 - loss: 0.6930 - acc: 0.5592
1280/5677 [=====>........................] - ETA: 10:09 - loss: 0.6905 - acc: 0.5656
1344/5677 [======>.......................] - ETA: 9:59 - loss: 0.6903 - acc: 0.5655 
1408/5677 [======>.......................] - ETA: 9:48 - loss: 0.6889 - acc: 0.5682
1472/5677 [======>.......................] - ETA: 9:39 - loss: 0.6892 - acc: 0.5652
1536/5677 [=======>......................] - ETA: 9:32 - loss: 0.6885 - acc: 0.5664
1600/5677 [=======>......................] - ETA: 9:24 - loss: 0.6887 - acc: 0.5644
1664/5677 [=======>......................] - ETA: 9:15 - loss: 0.6884 - acc: 0.5649
1728/5677 [========>.....................] - ETA: 9:08 - loss: 0.6882 - acc: 0.5666
1792/5677 [========>.....................] - ETA: 8:58 - loss: 0.6897 - acc: 0.5631
1856/5677 [========>.....................] - ETA: 8:50 - loss: 0.6895 - acc: 0.5620
1920/5677 [=========>....................] - ETA: 8:43 - loss: 0.6906 - acc: 0.5578
1984/5677 [=========>....................] - ETA: 8:33 - loss: 0.6895 - acc: 0.5605
2048/5677 [=========>....................] - ETA: 8:22 - loss: 0.6878 - acc: 0.5620
2112/5677 [==========>...................] - ETA: 8:13 - loss: 0.6874 - acc: 0.5644
2176/5677 [==========>...................] - ETA: 8:05 - loss: 0.6876 - acc: 0.5648
2240/5677 [==========>...................] - ETA: 7:55 - loss: 0.6868 - acc: 0.5661
2304/5677 [===========>..................] - ETA: 7:47 - loss: 0.6874 - acc: 0.5634
2368/5677 [===========>..................] - ETA: 7:37 - loss: 0.6870 - acc: 0.5617
2432/5677 [===========>..................] - ETA: 7:27 - loss: 0.6866 - acc: 0.5621
2496/5677 [============>.................] - ETA: 7:18 - loss: 0.6863 - acc: 0.5637
2560/5677 [============>.................] - ETA: 7:09 - loss: 0.6857 - acc: 0.5645
2624/5677 [============>.................] - ETA: 7:01 - loss: 0.6852 - acc: 0.5644
2688/5677 [=============>................] - ETA: 6:53 - loss: 0.6840 - acc: 0.5677
2752/5677 [=============>................] - ETA: 6:46 - loss: 0.6835 - acc: 0.5676
2816/5677 [=============>................] - ETA: 6:38 - loss: 0.6828 - acc: 0.5689
2880/5677 [==============>...............] - ETA: 6:30 - loss: 0.6822 - acc: 0.5705
2944/5677 [==============>...............] - ETA: 6:23 - loss: 0.6817 - acc: 0.5700
3008/5677 [==============>...............] - ETA: 6:15 - loss: 0.6812 - acc: 0.5701
3072/5677 [===============>..............] - ETA: 6:07 - loss: 0.6809 - acc: 0.5703
3136/5677 [===============>..............] - ETA: 5:59 - loss: 0.6803 - acc: 0.5708
3200/5677 [===============>..............] - ETA: 5:51 - loss: 0.6804 - acc: 0.5709
3264/5677 [================>.............] - ETA: 5:42 - loss: 0.6801 - acc: 0.5708
3328/5677 [================>.............] - ETA: 5:34 - loss: 0.6791 - acc: 0.5718
3392/5677 [================>.............] - ETA: 5:26 - loss: 0.6790 - acc: 0.5713
3456/5677 [=================>............] - ETA: 5:17 - loss: 0.6793 - acc: 0.5720
3520/5677 [=================>............] - ETA: 5:09 - loss: 0.6786 - acc: 0.5733
3584/5677 [=================>............] - ETA: 5:00 - loss: 0.6791 - acc: 0.5731
3648/5677 [==================>...........] - ETA: 4:51 - loss: 0.6792 - acc: 0.5713
3712/5677 [==================>...........] - ETA: 4:43 - loss: 0.6786 - acc: 0.5719
3776/5677 [==================>...........] - ETA: 4:34 - loss: 0.6778 - acc: 0.5731
3840/5677 [===================>..........] - ETA: 4:25 - loss: 0.6777 - acc: 0.5732
3904/5677 [===================>..........] - ETA: 4:16 - loss: 0.6777 - acc: 0.5738
3968/5677 [===================>..........] - ETA: 4:07 - loss: 0.6776 - acc: 0.5731
4032/5677 [====================>.........] - ETA: 3:58 - loss: 0.6769 - acc: 0.5742
4096/5677 [====================>.........] - ETA: 3:49 - loss: 0.6772 - acc: 0.5740
4160/5677 [====================>.........] - ETA: 3:40 - loss: 0.6789 - acc: 0.5719
4224/5677 [=====================>........] - ETA: 3:31 - loss: 0.6794 - acc: 0.5717
4288/5677 [=====================>........] - ETA: 3:22 - loss: 0.6794 - acc: 0.5711
4352/5677 [=====================>........] - ETA: 3:13 - loss: 0.6791 - acc: 0.5717
4416/5677 [======================>.......] - ETA: 3:04 - loss: 0.6785 - acc: 0.5727
4480/5677 [======================>.......] - ETA: 2:55 - loss: 0.6781 - acc: 0.5737
4544/5677 [=======================>......] - ETA: 2:45 - loss: 0.6781 - acc: 0.5728
4608/5677 [=======================>......] - ETA: 2:36 - loss: 0.6781 - acc: 0.5729
4672/5677 [=======================>......] - ETA: 2:27 - loss: 0.6781 - acc: 0.5726
4736/5677 [========================>.....] - ETA: 2:18 - loss: 0.6781 - acc: 0.5720
4800/5677 [========================>.....] - ETA: 2:08 - loss: 0.6778 - acc: 0.5725
4864/5677 [========================>.....] - ETA: 1:59 - loss: 0.6779 - acc: 0.5718
4928/5677 [=========================>....] - ETA: 1:50 - loss: 0.6775 - acc: 0.5722
4992/5677 [=========================>....] - ETA: 1:40 - loss: 0.6770 - acc: 0.5741
5056/5677 [=========================>....] - ETA: 1:31 - loss: 0.6768 - acc: 0.5748
5120/5677 [==========================>...] - ETA: 1:21 - loss: 0.6766 - acc: 0.5758
5184/5677 [==========================>...] - ETA: 1:12 - loss: 0.6769 - acc: 0.5752
5248/5677 [==========================>...] - ETA: 1:03 - loss: 0.6769 - acc: 0.5751
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6770 - acc: 0.5751 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6770 - acc: 0.5755
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6772 - acc: 0.5748
5504/5677 [============================>.] - ETA: 25s - loss: 0.6768 - acc: 0.5754
5568/5677 [============================>.] - ETA: 15s - loss: 0.6773 - acc: 0.5747
5632/5677 [============================>.] - ETA: 6s - loss: 0.6773 - acc: 0.5742 
5677/5677 [==============================] - 860s 152ms/step - loss: 0.6773 - acc: 0.5742 - val_loss: 0.6770 - val_acc: 0.5880

Epoch 00008: val_acc did not improve from 0.59429
Epoch 9/10

  64/5677 [..............................] - ETA: 12:46 - loss: 0.6556 - acc: 0.6094
 128/5677 [..............................] - ETA: 12:26 - loss: 0.6511 - acc: 0.6484
 192/5677 [>.............................] - ETA: 12:39 - loss: 0.6568 - acc: 0.6510
 256/5677 [>.............................] - ETA: 12:31 - loss: 0.6614 - acc: 0.6367
 320/5677 [>.............................] - ETA: 12:30 - loss: 0.6658 - acc: 0.6250
 384/5677 [=>............................] - ETA: 12:24 - loss: 0.6691 - acc: 0.5990
 448/5677 [=>............................] - ETA: 12:22 - loss: 0.6623 - acc: 0.6161
 512/5677 [=>............................] - ETA: 12:19 - loss: 0.6620 - acc: 0.6113
 576/5677 [==>...........................] - ETA: 12:09 - loss: 0.6632 - acc: 0.6059
 640/5677 [==>...........................] - ETA: 11:58 - loss: 0.6662 - acc: 0.5938
 704/5677 [==>...........................] - ETA: 11:43 - loss: 0.6628 - acc: 0.6023
 768/5677 [===>..........................] - ETA: 11:36 - loss: 0.6678 - acc: 0.5924
 832/5677 [===>..........................] - ETA: 11:30 - loss: 0.6688 - acc: 0.5913
 896/5677 [===>..........................] - ETA: 11:25 - loss: 0.6706 - acc: 0.5893
 960/5677 [====>.........................] - ETA: 11:16 - loss: 0.6694 - acc: 0.5917
1024/5677 [====>.........................] - ETA: 11:07 - loss: 0.6730 - acc: 0.5869
1088/5677 [====>.........................] - ETA: 11:01 - loss: 0.6745 - acc: 0.5818
1152/5677 [=====>........................] - ETA: 10:55 - loss: 0.6726 - acc: 0.5825
1216/5677 [=====>........................] - ETA: 10:43 - loss: 0.6729 - acc: 0.5847
1280/5677 [=====>........................] - ETA: 10:36 - loss: 0.6726 - acc: 0.5852
1344/5677 [======>.......................] - ETA: 10:28 - loss: 0.6735 - acc: 0.5818
1408/5677 [======>.......................] - ETA: 10:19 - loss: 0.6731 - acc: 0.5831
1472/5677 [======>.......................] - ETA: 10:11 - loss: 0.6748 - acc: 0.5836
1536/5677 [=======>......................] - ETA: 10:06 - loss: 0.6743 - acc: 0.5833
1600/5677 [=======>......................] - ETA: 9:58 - loss: 0.6728 - acc: 0.5900 
1664/5677 [=======>......................] - ETA: 9:50 - loss: 0.6730 - acc: 0.5895
1728/5677 [========>.....................] - ETA: 9:42 - loss: 0.6739 - acc: 0.5880
1792/5677 [========>.....................] - ETA: 9:34 - loss: 0.6729 - acc: 0.5904
1856/5677 [========>.....................] - ETA: 9:26 - loss: 0.6737 - acc: 0.5884
1920/5677 [=========>....................] - ETA: 9:18 - loss: 0.6740 - acc: 0.5859
1984/5677 [=========>....................] - ETA: 9:12 - loss: 0.6749 - acc: 0.5847
2048/5677 [=========>....................] - ETA: 9:02 - loss: 0.6753 - acc: 0.5830
2112/5677 [==========>...................] - ETA: 8:52 - loss: 0.6745 - acc: 0.5838
2176/5677 [==========>...................] - ETA: 8:42 - loss: 0.6750 - acc: 0.5818
2240/5677 [==========>...................] - ETA: 8:33 - loss: 0.6768 - acc: 0.5790
2304/5677 [===========>..................] - ETA: 8:22 - loss: 0.6759 - acc: 0.5786
2368/5677 [===========>..................] - ETA: 8:13 - loss: 0.6752 - acc: 0.5798
2432/5677 [===========>..................] - ETA: 8:04 - loss: 0.6747 - acc: 0.5822
2496/5677 [============>.................] - ETA: 7:54 - loss: 0.6746 - acc: 0.5821
2560/5677 [============>.................] - ETA: 7:45 - loss: 0.6743 - acc: 0.5824
2624/5677 [============>.................] - ETA: 7:35 - loss: 0.6753 - acc: 0.5827
2688/5677 [=============>................] - ETA: 7:26 - loss: 0.6754 - acc: 0.5833
2752/5677 [=============>................] - ETA: 7:17 - loss: 0.6754 - acc: 0.5836
2816/5677 [=============>................] - ETA: 7:08 - loss: 0.6763 - acc: 0.5810
2880/5677 [==============>...............] - ETA: 6:58 - loss: 0.6772 - acc: 0.5795
2944/5677 [==============>...............] - ETA: 6:48 - loss: 0.6772 - acc: 0.5798
3008/5677 [==============>...............] - ETA: 6:40 - loss: 0.6762 - acc: 0.5824
3072/5677 [===============>..............] - ETA: 6:30 - loss: 0.6777 - acc: 0.5804
3136/5677 [===============>..............] - ETA: 6:22 - loss: 0.6772 - acc: 0.5807
3200/5677 [===============>..............] - ETA: 6:13 - loss: 0.6774 - acc: 0.5800
3264/5677 [================>.............] - ETA: 6:03 - loss: 0.6774 - acc: 0.5809
3328/5677 [================>.............] - ETA: 5:53 - loss: 0.6775 - acc: 0.5808
3392/5677 [================>.............] - ETA: 5:44 - loss: 0.6784 - acc: 0.5781
3456/5677 [=================>............] - ETA: 5:34 - loss: 0.6785 - acc: 0.5784
3520/5677 [=================>............] - ETA: 5:24 - loss: 0.6778 - acc: 0.5804
3584/5677 [=================>............] - ETA: 5:15 - loss: 0.6770 - acc: 0.5829
3648/5677 [==================>...........] - ETA: 5:05 - loss: 0.6770 - acc: 0.5839
3712/5677 [==================>...........] - ETA: 4:56 - loss: 0.6766 - acc: 0.5851
3776/5677 [==================>...........] - ETA: 4:47 - loss: 0.6763 - acc: 0.5866
3840/5677 [===================>..........] - ETA: 4:37 - loss: 0.6761 - acc: 0.5872
3904/5677 [===================>..........] - ETA: 4:28 - loss: 0.6753 - acc: 0.5886
3968/5677 [===================>..........] - ETA: 4:18 - loss: 0.6746 - acc: 0.5902
4032/5677 [====================>.........] - ETA: 4:08 - loss: 0.6739 - acc: 0.5913
4096/5677 [====================>.........] - ETA: 3:59 - loss: 0.6742 - acc: 0.5908
4160/5677 [====================>.........] - ETA: 3:49 - loss: 0.6740 - acc: 0.5911
4224/5677 [=====================>........] - ETA: 3:39 - loss: 0.6746 - acc: 0.5902
4288/5677 [=====================>........] - ETA: 3:29 - loss: 0.6753 - acc: 0.5891
4352/5677 [=====================>........] - ETA: 3:20 - loss: 0.6754 - acc: 0.5885
4416/5677 [======================>.......] - ETA: 3:10 - loss: 0.6748 - acc: 0.5890
4480/5677 [======================>.......] - ETA: 3:01 - loss: 0.6754 - acc: 0.5875
4544/5677 [=======================>......] - ETA: 2:51 - loss: 0.6759 - acc: 0.5871
4608/5677 [=======================>......] - ETA: 2:41 - loss: 0.6749 - acc: 0.5885
4672/5677 [=======================>......] - ETA: 2:32 - loss: 0.6749 - acc: 0.5888
4736/5677 [========================>.....] - ETA: 2:22 - loss: 0.6751 - acc: 0.5891
4800/5677 [========================>.....] - ETA: 2:12 - loss: 0.6750 - acc: 0.5896
4864/5677 [========================>.....] - ETA: 2:03 - loss: 0.6747 - acc: 0.5905
4928/5677 [=========================>....] - ETA: 1:53 - loss: 0.6742 - acc: 0.5909
4992/5677 [=========================>....] - ETA: 1:44 - loss: 0.6741 - acc: 0.5909
5056/5677 [=========================>....] - ETA: 1:34 - loss: 0.6741 - acc: 0.5904
5120/5677 [==========================>...] - ETA: 1:25 - loss: 0.6741 - acc: 0.5900
5184/5677 [==========================>...] - ETA: 1:15 - loss: 0.6748 - acc: 0.5882
5248/5677 [==========================>...] - ETA: 1:05 - loss: 0.6752 - acc: 0.5871
5312/5677 [===========================>..] - ETA: 55s - loss: 0.6751 - acc: 0.5873 
5376/5677 [===========================>..] - ETA: 46s - loss: 0.6756 - acc: 0.5871
5440/5677 [===========================>..] - ETA: 36s - loss: 0.6754 - acc: 0.5873
5504/5677 [============================>.] - ETA: 26s - loss: 0.6759 - acc: 0.5865
5568/5677 [============================>.] - ETA: 16s - loss: 0.6759 - acc: 0.5862
5632/5677 [============================>.] - ETA: 6s - loss: 0.6755 - acc: 0.5865 
5677/5677 [==============================] - 912s 161ms/step - loss: 0.6759 - acc: 0.5853 - val_loss: 0.6844 - val_acc: 0.5642

Epoch 00009: val_acc did not improve from 0.59429
Epoch 10/10

  64/5677 [..............................] - ETA: 15:13 - loss: 0.6716 - acc: 0.5469
 128/5677 [..............................] - ETA: 15:16 - loss: 0.6894 - acc: 0.5156
 192/5677 [>.............................] - ETA: 15:20 - loss: 0.6784 - acc: 0.5677
 256/5677 [>.............................] - ETA: 15:11 - loss: 0.6750 - acc: 0.5781
 320/5677 [>.............................] - ETA: 15:07 - loss: 0.6801 - acc: 0.5813
 384/5677 [=>............................] - ETA: 15:06 - loss: 0.6768 - acc: 0.5833
 448/5677 [=>............................] - ETA: 15:04 - loss: 0.6771 - acc: 0.5893
 512/5677 [=>............................] - ETA: 15:10 - loss: 0.6757 - acc: 0.5879
 576/5677 [==>...........................] - ETA: 15:07 - loss: 0.6751 - acc: 0.5920
 640/5677 [==>...........................] - ETA: 15:01 - loss: 0.6814 - acc: 0.5750
 704/5677 [==>...........................] - ETA: 14:54 - loss: 0.6810 - acc: 0.5739
 768/5677 [===>..........................] - ETA: 14:44 - loss: 0.6823 - acc: 0.5664
 832/5677 [===>..........................] - ETA: 14:29 - loss: 0.6830 - acc: 0.5649
 896/5677 [===>..........................] - ETA: 14:13 - loss: 0.6841 - acc: 0.5614
 960/5677 [====>.........................] - ETA: 13:58 - loss: 0.6813 - acc: 0.5656
1024/5677 [====>.........................] - ETA: 13:43 - loss: 0.6798 - acc: 0.5693
1088/5677 [====>.........................] - ETA: 13:29 - loss: 0.6780 - acc: 0.5763
1152/5677 [=====>........................] - ETA: 13:12 - loss: 0.6781 - acc: 0.5738
1216/5677 [=====>........................] - ETA: 12:57 - loss: 0.6781 - acc: 0.5740
1280/5677 [=====>........................] - ETA: 12:45 - loss: 0.6788 - acc: 0.5719
1344/5677 [======>.......................] - ETA: 12:31 - loss: 0.6789 - acc: 0.5714
1408/5677 [======>.......................] - ETA: 12:16 - loss: 0.6785 - acc: 0.5696
1472/5677 [======>.......................] - ETA: 12:03 - loss: 0.6771 - acc: 0.5713
1536/5677 [=======>......................] - ETA: 11:53 - loss: 0.6764 - acc: 0.5736
1600/5677 [=======>......................] - ETA: 11:43 - loss: 0.6761 - acc: 0.5744
1664/5677 [=======>......................] - ETA: 11:31 - loss: 0.6756 - acc: 0.5745
1728/5677 [========>.....................] - ETA: 11:18 - loss: 0.6745 - acc: 0.5758
1792/5677 [========>.....................] - ETA: 11:12 - loss: 0.6743 - acc: 0.5753
1856/5677 [========>.....................] - ETA: 11:05 - loss: 0.6740 - acc: 0.5749
1920/5677 [=========>....................] - ETA: 10:54 - loss: 0.6738 - acc: 0.5745
1984/5677 [=========>....................] - ETA: 10:41 - loss: 0.6736 - acc: 0.5746
2048/5677 [=========>....................] - ETA: 10:29 - loss: 0.6748 - acc: 0.5732
2112/5677 [==========>...................] - ETA: 10:17 - loss: 0.6741 - acc: 0.5743
2176/5677 [==========>...................] - ETA: 10:04 - loss: 0.6743 - acc: 0.5735
2240/5677 [==========>...................] - ETA: 9:49 - loss: 0.6732 - acc: 0.5768 
2304/5677 [===========>..................] - ETA: 9:39 - loss: 0.6720 - acc: 0.5807
2368/5677 [===========>..................] - ETA: 9:26 - loss: 0.6732 - acc: 0.5785
2432/5677 [===========>..................] - ETA: 9:15 - loss: 0.6738 - acc: 0.5798
2496/5677 [============>.................] - ETA: 9:04 - loss: 0.6743 - acc: 0.5797
2560/5677 [============>.................] - ETA: 8:52 - loss: 0.6760 - acc: 0.5781
2624/5677 [============>.................] - ETA: 8:41 - loss: 0.6754 - acc: 0.5796
2688/5677 [=============>................] - ETA: 8:30 - loss: 0.6760 - acc: 0.5781
2752/5677 [=============>................] - ETA: 8:19 - loss: 0.6755 - acc: 0.5799
2816/5677 [=============>................] - ETA: 8:08 - loss: 0.6747 - acc: 0.5827
2880/5677 [==============>...............] - ETA: 7:56 - loss: 0.6749 - acc: 0.5830
2944/5677 [==============>...............] - ETA: 7:45 - loss: 0.6757 - acc: 0.5808
3008/5677 [==============>...............] - ETA: 7:34 - loss: 0.6751 - acc: 0.5818
3072/5677 [===============>..............] - ETA: 7:23 - loss: 0.6742 - acc: 0.5833
3136/5677 [===============>..............] - ETA: 7:13 - loss: 0.6741 - acc: 0.5845
3200/5677 [===============>..............] - ETA: 7:02 - loss: 0.6743 - acc: 0.5834
3264/5677 [================>.............] - ETA: 6:49 - loss: 0.6742 - acc: 0.5852
3328/5677 [================>.............] - ETA: 6:38 - loss: 0.6749 - acc: 0.5856
3392/5677 [================>.............] - ETA: 6:26 - loss: 0.6749 - acc: 0.5855
3456/5677 [=================>............] - ETA: 6:15 - loss: 0.6753 - acc: 0.5854
3520/5677 [=================>............] - ETA: 6:03 - loss: 0.6744 - acc: 0.5869
3584/5677 [=================>............] - ETA: 5:52 - loss: 0.6740 - acc: 0.5882
3648/5677 [==================>...........] - ETA: 5:41 - loss: 0.6734 - acc: 0.5894
3712/5677 [==================>...........] - ETA: 5:30 - loss: 0.6728 - acc: 0.5902
3776/5677 [==================>...........] - ETA: 5:19 - loss: 0.6728 - acc: 0.5900
3840/5677 [===================>..........] - ETA: 5:08 - loss: 0.6733 - acc: 0.5883
3904/5677 [===================>..........] - ETA: 4:57 - loss: 0.6740 - acc: 0.5863
3968/5677 [===================>..........] - ETA: 4:47 - loss: 0.6746 - acc: 0.5852
4032/5677 [====================>.........] - ETA: 4:36 - loss: 0.6741 - acc: 0.5856
4096/5677 [====================>.........] - ETA: 4:25 - loss: 0.6745 - acc: 0.5859
4160/5677 [====================>.........] - ETA: 4:15 - loss: 0.6750 - acc: 0.5846
4224/5677 [=====================>........] - ETA: 4:04 - loss: 0.6751 - acc: 0.5831
4288/5677 [=====================>........] - ETA: 3:54 - loss: 0.6757 - acc: 0.5819
4352/5677 [=====================>........] - ETA: 3:43 - loss: 0.6753 - acc: 0.5818
4416/5677 [======================>.......] - ETA: 3:32 - loss: 0.6757 - acc: 0.5815
4480/5677 [======================>.......] - ETA: 3:21 - loss: 0.6757 - acc: 0.5819
4544/5677 [=======================>......] - ETA: 3:10 - loss: 0.6757 - acc: 0.5821
4608/5677 [=======================>......] - ETA: 2:59 - loss: 0.6759 - acc: 0.5812
4672/5677 [=======================>......] - ETA: 2:49 - loss: 0.6760 - acc: 0.5809
4736/5677 [========================>.....] - ETA: 2:38 - loss: 0.6757 - acc: 0.5817
4800/5677 [========================>.....] - ETA: 2:27 - loss: 0.6749 - acc: 0.5829
4864/5677 [========================>.....] - ETA: 2:16 - loss: 0.6749 - acc: 0.5835
4928/5677 [=========================>....] - ETA: 2:05 - loss: 0.6751 - acc: 0.5832
4992/5677 [=========================>....] - ETA: 1:55 - loss: 0.6751 - acc: 0.5827
5056/5677 [=========================>....] - ETA: 1:44 - loss: 0.6752 - acc: 0.5825
5120/5677 [==========================>...] - ETA: 1:33 - loss: 0.6757 - acc: 0.5814
5184/5677 [==========================>...] - ETA: 1:22 - loss: 0.6758 - acc: 0.5806
5248/5677 [==========================>...] - ETA: 1:12 - loss: 0.6758 - acc: 0.5808
5312/5677 [===========================>..] - ETA: 1:01 - loss: 0.6756 - acc: 0.5811
5376/5677 [===========================>..] - ETA: 50s - loss: 0.6752 - acc: 0.5818 
5440/5677 [===========================>..] - ETA: 39s - loss: 0.6754 - acc: 0.5816
5504/5677 [============================>.] - ETA: 29s - loss: 0.6757 - acc: 0.5810
5568/5677 [============================>.] - ETA: 18s - loss: 0.6758 - acc: 0.5803
5632/5677 [============================>.] - ETA: 7s - loss: 0.6756 - acc: 0.5806 
5677/5677 [==============================] - 988s 174ms/step - loss: 0.6752 - acc: 0.5815 - val_loss: 0.6696 - val_acc: 0.5911

Epoch 00010: val_acc did not improve from 0.59429
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9ebc14d490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9ebc14d490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9ebc05f3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9ebc05f3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9eac7d1810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9eac7d1810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9eac3d92d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9eac3d92d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9eac735d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9eac735d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d38146510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d38146510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9eac3d9610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9eac3d9610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9eac350fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9eac350fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9af474ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9af474ded0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9eac269e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9eac269e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9eac1ba710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9eac1ba710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9eac2a8c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9eac2a8c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e84743f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e84743f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9eac135bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9eac135bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e845f9b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e845f9b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e8464d790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e8464d790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9eac10a450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9eac10a450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e845da4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e845da4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e8431e510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9e8431e510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e84598590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e84598590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e842dfed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9e842dfed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e8431ebd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9e8431ebd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9eac1306d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9eac1306d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d5c0bbfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d5c0bbfd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e84386b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9e84386b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d5c153e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d5c153e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d5c1b2190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d5c1b2190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d547a1e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d547a1e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d54583090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d54583090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d5467aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d5467aed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d545091d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d545091d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d54799c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d54799c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d54345e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d54345e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d54441590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d54441590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d54208c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d54208c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d541bc8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d541bc8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d54441dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d54441dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d5421a790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d5421a790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d5423ee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d5423ee10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4c5c9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d4c5c9e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c610610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c610610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4c732190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4c732190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c619590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c619590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4c67d610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4c67d610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bb833b150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9bb833b150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c1be750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c1be750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4c3ef750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4c3ef750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c305710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c305710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4c211e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4c211e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d387a9990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d387a9990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c188f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c188f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4c0c4910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4c0c4910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c094ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c094ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4c420c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d4c420c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d38576f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d38576f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d383453d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d383453d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4c1527d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d4c1527d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d38321990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d38321990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d387f1190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d387f1190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d3839af10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9d3839af10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d38244c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d38244c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d385a4f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9d385a4f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d207d2190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d207d2190>>: AttributeError: module 'gast' has no attribute 'Str'
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 9:01
 128/1578 [=>............................] - ETA: 5:04
 192/1578 [==>...........................] - ETA: 3:44
 256/1578 [===>..........................] - ETA: 2:59
 320/1578 [=====>........................] - ETA: 2:37
 384/1578 [======>.......................] - ETA: 2:16
 448/1578 [=======>......................] - ETA: 2:00
 512/1578 [========>.....................] - ETA: 1:48
 576/1578 [=========>....................] - ETA: 1:37
 640/1578 [===========>..................] - ETA: 1:28
 704/1578 [============>.................] - ETA: 1:20
 768/1578 [=============>................] - ETA: 1:12
 832/1578 [==============>...............] - ETA: 1:06
 896/1578 [================>.............] - ETA: 59s 
 960/1578 [=================>............] - ETA: 52s
1024/1578 [==================>...........] - ETA: 46s
1088/1578 [===================>..........] - ETA: 40s
1152/1578 [====================>.........] - ETA: 34s
1216/1578 [======================>.......] - ETA: 29s
1280/1578 [=======================>......] - ETA: 24s
1344/1578 [========================>.....] - ETA: 18s
1408/1578 [=========================>....] - ETA: 13s
1472/1578 [==========================>...] - ETA: 8s 
1536/1578 [============================>.] - ETA: 3s
1578/1578 [==============================] - 125s 79ms/step
loss: 0.6776263159339722
acc: 0.5671736381957437
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f99202fa810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f99202fa810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f99202f9a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f99202f9a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c743290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9f3c743290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9cf87b3750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9cf87b3750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9cf8781f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9cf8781f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9eac590a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9eac590a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf879ddd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf879ddd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99202fe210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99202fe210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ebc048890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9ebc048890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9eac7a2d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9eac7a2d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c744850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d4c744850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ebc048410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9ebc048410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99202e4dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99202e4dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9eac6ea2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9eac6ea2d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99200adb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99200adb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9920121e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9920121e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9eac6c8210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9eac6c8210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99087a61d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99087a61d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9908591a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9908591a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99087b7f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99087b7f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f992004e7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f992004e7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f99085918d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f99085918d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99082c9ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99082c9ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f990825b690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f990825b690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99084a7d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99084a7d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9908311e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9908311e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f99082a2f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f99082a2f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99081435d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99081435d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f99081d8090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f99081d8090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99005d5350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99005d5350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9900615510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9900615510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9908075310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9908075310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9900497d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9900497d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f990060b4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f990060b4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99003994d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f99003994d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99006315d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99006315d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9900620690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9900620690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f990034b250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f990034b250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9900104e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9900104e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f98d8751ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f98d8751ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99000bd090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f99000bd090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9900104f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9900104f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f98d85c4c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f98d85c4c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f98d8551a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f98d8551a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f98d84c7710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f98d84c7710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f98d85f2e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f98d85f2e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f98d85511d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f98d85511d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f98d8478750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f98d8478750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f98d81fcd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f98d81fcd50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f98d815e590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f98d815e590>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f98d82e8b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f98d82e8b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f98d836f550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f98d836f550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f98d8110310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f98d8110310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97687b1c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f97687b1c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97686c1a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97686c1a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9768717d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9768717d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97687b1310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f97687b1310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97686b0110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97686b0110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9768667310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9768667310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97684eedd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f97684eedd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97681f9250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97681f9250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9768667250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9768667250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97683e8110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f97683e8110>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 1:33:55 - loss: 0.7929 - acc: 0.4531
 128/5677 [..............................] - ETA: 54:16 - loss: 0.7922 - acc: 0.5000  
 192/5677 [>.............................] - ETA: 40:32 - loss: 0.7728 - acc: 0.5156
 256/5677 [>.............................] - ETA: 33:31 - loss: 0.7696 - acc: 0.5039
 320/5677 [>.............................] - ETA: 29:32 - loss: 0.7712 - acc: 0.5000
 384/5677 [=>............................] - ETA: 26:58 - loss: 0.7770 - acc: 0.4870
 448/5677 [=>............................] - ETA: 25:03 - loss: 0.7671 - acc: 0.4911
 512/5677 [=>............................] - ETA: 23:13 - loss: 0.7610 - acc: 0.4980
 576/5677 [==>...........................] - ETA: 21:49 - loss: 0.7646 - acc: 0.5000
 640/5677 [==>...........................] - ETA: 20:37 - loss: 0.7594 - acc: 0.5047
 704/5677 [==>...........................] - ETA: 19:41 - loss: 0.7560 - acc: 0.5114
 768/5677 [===>..........................] - ETA: 18:55 - loss: 0.7510 - acc: 0.5182
 832/5677 [===>..........................] - ETA: 18:16 - loss: 0.7483 - acc: 0.5132
 896/5677 [===>..........................] - ETA: 17:35 - loss: 0.7508 - acc: 0.5056
 960/5677 [====>.........................] - ETA: 16:59 - loss: 0.7490 - acc: 0.5042
1024/5677 [====>.........................] - ETA: 16:28 - loss: 0.7452 - acc: 0.5020
1088/5677 [====>.........................] - ETA: 15:55 - loss: 0.7432 - acc: 0.5018
1152/5677 [=====>........................] - ETA: 15:32 - loss: 0.7410 - acc: 0.5069
1216/5677 [=====>........................] - ETA: 15:04 - loss: 0.7376 - acc: 0.5132
1280/5677 [=====>........................] - ETA: 14:40 - loss: 0.7349 - acc: 0.5148
1344/5677 [======>.......................] - ETA: 14:19 - loss: 0.7324 - acc: 0.5156
1408/5677 [======>.......................] - ETA: 13:52 - loss: 0.7323 - acc: 0.5178
1472/5677 [======>.......................] - ETA: 13:31 - loss: 0.7322 - acc: 0.5211
1536/5677 [=======>......................] - ETA: 13:10 - loss: 0.7319 - acc: 0.5221
1600/5677 [=======>......................] - ETA: 12:50 - loss: 0.7321 - acc: 0.5225
1664/5677 [=======>......................] - ETA: 12:32 - loss: 0.7299 - acc: 0.5246
1728/5677 [========>.....................] - ETA: 12:17 - loss: 0.7320 - acc: 0.5260
1792/5677 [========>.....................] - ETA: 12:02 - loss: 0.7325 - acc: 0.5251
1856/5677 [========>.....................] - ETA: 11:45 - loss: 0.7356 - acc: 0.5205
1920/5677 [=========>....................] - ETA: 11:30 - loss: 0.7350 - acc: 0.5198
1984/5677 [=========>....................] - ETA: 11:15 - loss: 0.7363 - acc: 0.5181
2048/5677 [=========>....................] - ETA: 10:58 - loss: 0.7357 - acc: 0.5200
2112/5677 [==========>...................] - ETA: 10:44 - loss: 0.7353 - acc: 0.5199
2176/5677 [==========>...................] - ETA: 10:29 - loss: 0.7362 - acc: 0.5165
2240/5677 [==========>...................] - ETA: 10:14 - loss: 0.7366 - acc: 0.5143
2304/5677 [===========>..................] - ETA: 9:59 - loss: 0.7355 - acc: 0.5169 
2368/5677 [===========>..................] - ETA: 9:45 - loss: 0.7356 - acc: 0.5144
2432/5677 [===========>..................] - ETA: 9:31 - loss: 0.7346 - acc: 0.5148
2496/5677 [============>.................] - ETA: 9:17 - loss: 0.7336 - acc: 0.5168
2560/5677 [============>.................] - ETA: 9:04 - loss: 0.7328 - acc: 0.5160
2624/5677 [============>.................] - ETA: 8:50 - loss: 0.7317 - acc: 0.5168
2688/5677 [=============>................] - ETA: 8:36 - loss: 0.7308 - acc: 0.5175
2752/5677 [=============>................] - ETA: 8:24 - loss: 0.7308 - acc: 0.5167
2816/5677 [=============>................] - ETA: 8:12 - loss: 0.7311 - acc: 0.5149
2880/5677 [==============>...............] - ETA: 7:59 - loss: 0.7311 - acc: 0.5135
2944/5677 [==============>...............] - ETA: 7:46 - loss: 0.7317 - acc: 0.5105
3008/5677 [==============>...............] - ETA: 7:33 - loss: 0.7314 - acc: 0.5106
3072/5677 [===============>..............] - ETA: 7:20 - loss: 0.7312 - acc: 0.5107
3136/5677 [===============>..............] - ETA: 7:08 - loss: 0.7316 - acc: 0.5089
3200/5677 [===============>..............] - ETA: 6:57 - loss: 0.7309 - acc: 0.5097
3264/5677 [================>.............] - ETA: 6:45 - loss: 0.7319 - acc: 0.5058
3328/5677 [================>.............] - ETA: 6:34 - loss: 0.7316 - acc: 0.5060
3392/5677 [================>.............] - ETA: 6:23 - loss: 0.7311 - acc: 0.5062
3456/5677 [=================>............] - ETA: 6:12 - loss: 0.7306 - acc: 0.5075
3520/5677 [=================>............] - ETA: 6:01 - loss: 0.7299 - acc: 0.5082
3584/5677 [=================>............] - ETA: 5:49 - loss: 0.7288 - acc: 0.5092
3648/5677 [==================>...........] - ETA: 5:38 - loss: 0.7281 - acc: 0.5104
3712/5677 [==================>...........] - ETA: 5:27 - loss: 0.7293 - acc: 0.5089
3776/5677 [==================>...........] - ETA: 5:16 - loss: 0.7291 - acc: 0.5087
3840/5677 [===================>..........] - ETA: 5:05 - loss: 0.7277 - acc: 0.5107
3904/5677 [===================>..........] - ETA: 4:54 - loss: 0.7279 - acc: 0.5108
3968/5677 [===================>..........] - ETA: 4:43 - loss: 0.7275 - acc: 0.5111
4032/5677 [====================>.........] - ETA: 4:32 - loss: 0.7270 - acc: 0.5112
4096/5677 [====================>.........] - ETA: 4:21 - loss: 0.7266 - acc: 0.5117
4160/5677 [====================>.........] - ETA: 4:10 - loss: 0.7265 - acc: 0.5108
4224/5677 [=====================>........] - ETA: 3:59 - loss: 0.7259 - acc: 0.5116
4288/5677 [=====================>........] - ETA: 3:49 - loss: 0.7262 - acc: 0.5110
4352/5677 [=====================>........] - ETA: 3:37 - loss: 0.7259 - acc: 0.5110
4416/5677 [======================>.......] - ETA: 3:27 - loss: 0.7265 - acc: 0.5100
4480/5677 [======================>.......] - ETA: 3:16 - loss: 0.7266 - acc: 0.5094
4544/5677 [=======================>......] - ETA: 3:05 - loss: 0.7260 - acc: 0.5092
4608/5677 [=======================>......] - ETA: 2:54 - loss: 0.7262 - acc: 0.5095
4672/5677 [=======================>......] - ETA: 2:44 - loss: 0.7259 - acc: 0.5096
4736/5677 [========================>.....] - ETA: 2:33 - loss: 0.7255 - acc: 0.5099
4800/5677 [========================>.....] - ETA: 2:23 - loss: 0.7254 - acc: 0.5090
4864/5677 [========================>.....] - ETA: 2:12 - loss: 0.7254 - acc: 0.5086
4928/5677 [=========================>....] - ETA: 2:01 - loss: 0.7252 - acc: 0.5085
4992/5677 [=========================>....] - ETA: 1:51 - loss: 0.7249 - acc: 0.5088
5056/5677 [=========================>....] - ETA: 1:40 - loss: 0.7249 - acc: 0.5091
5120/5677 [==========================>...] - ETA: 1:30 - loss: 0.7250 - acc: 0.5098
5184/5677 [==========================>...] - ETA: 1:20 - loss: 0.7247 - acc: 0.5095
5248/5677 [==========================>...] - ETA: 1:09 - loss: 0.7247 - acc: 0.5088
5312/5677 [===========================>..] - ETA: 59s - loss: 0.7243 - acc: 0.5100 
5376/5677 [===========================>..] - ETA: 48s - loss: 0.7244 - acc: 0.5089
5440/5677 [===========================>..] - ETA: 38s - loss: 0.7241 - acc: 0.5090
5504/5677 [============================>.] - ETA: 27s - loss: 0.7238 - acc: 0.5093
5568/5677 [============================>.] - ETA: 17s - loss: 0.7239 - acc: 0.5090
5632/5677 [============================>.] - ETA: 7s - loss: 0.7233 - acc: 0.5101 
5677/5677 [==============================] - 959s 169ms/step - loss: 0.7233 - acc: 0.5098 - val_loss: 0.6874 - val_acc: 0.5261

Epoch 00001: val_acc improved from -inf to 0.52615, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window15/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 15:43 - loss: 0.6470 - acc: 0.6562
 128/5677 [..............................] - ETA: 13:43 - loss: 0.6796 - acc: 0.5781
 192/5677 [>.............................] - ETA: 13:46 - loss: 0.6724 - acc: 0.5885
 256/5677 [>.............................] - ETA: 13:31 - loss: 0.6841 - acc: 0.5859
 320/5677 [>.............................] - ETA: 13:15 - loss: 0.6876 - acc: 0.5781
 384/5677 [=>............................] - ETA: 13:06 - loss: 0.6846 - acc: 0.5833
 448/5677 [=>............................] - ETA: 12:37 - loss: 0.6855 - acc: 0.5804
 512/5677 [=>............................] - ETA: 12:26 - loss: 0.6866 - acc: 0.5762
 576/5677 [==>...........................] - ETA: 12:39 - loss: 0.6909 - acc: 0.5642
 640/5677 [==>...........................] - ETA: 12:25 - loss: 0.6920 - acc: 0.5578
 704/5677 [==>...........................] - ETA: 12:15 - loss: 0.7010 - acc: 0.5440
 768/5677 [===>..........................] - ETA: 12:03 - loss: 0.7005 - acc: 0.5404
 832/5677 [===>..........................] - ETA: 11:58 - loss: 0.7015 - acc: 0.5361
 896/5677 [===>..........................] - ETA: 11:56 - loss: 0.7006 - acc: 0.5391
 960/5677 [====>.........................] - ETA: 11:46 - loss: 0.7006 - acc: 0.5354
1024/5677 [====>.........................] - ETA: 11:40 - loss: 0.7006 - acc: 0.5342
1088/5677 [====>.........................] - ETA: 11:30 - loss: 0.7015 - acc: 0.5303
1152/5677 [=====>........................] - ETA: 11:18 - loss: 0.7010 - acc: 0.5295
1216/5677 [=====>........................] - ETA: 11:13 - loss: 0.7012 - acc: 0.5280
1280/5677 [=====>........................] - ETA: 11:04 - loss: 0.7006 - acc: 0.5258
1344/5677 [======>.......................] - ETA: 10:59 - loss: 0.6994 - acc: 0.5305
1408/5677 [======>.......................] - ETA: 10:49 - loss: 0.7009 - acc: 0.5284
1472/5677 [======>.......................] - ETA: 10:40 - loss: 0.6997 - acc: 0.5299
1536/5677 [=======>......................] - ETA: 10:30 - loss: 0.6997 - acc: 0.5326
1600/5677 [=======>......................] - ETA: 10:22 - loss: 0.6983 - acc: 0.5350
1664/5677 [=======>......................] - ETA: 10:15 - loss: 0.6983 - acc: 0.5343
1728/5677 [========>.....................] - ETA: 10:05 - loss: 0.6985 - acc: 0.5289
1792/5677 [========>.....................] - ETA: 9:54 - loss: 0.6987 - acc: 0.5296 
1856/5677 [========>.....................] - ETA: 9:43 - loss: 0.6994 - acc: 0.5280
1920/5677 [=========>....................] - ETA: 9:34 - loss: 0.7000 - acc: 0.5271
1984/5677 [=========>....................] - ETA: 9:24 - loss: 0.7002 - acc: 0.5247
2048/5677 [=========>....................] - ETA: 9:14 - loss: 0.7004 - acc: 0.5249
2112/5677 [==========>...................] - ETA: 9:05 - loss: 0.7000 - acc: 0.5270
2176/5677 [==========>...................] - ETA: 8:54 - loss: 0.7006 - acc: 0.5257
2240/5677 [==========>...................] - ETA: 8:44 - loss: 0.7008 - acc: 0.5277
2304/5677 [===========>..................] - ETA: 8:34 - loss: 0.7007 - acc: 0.5278
2368/5677 [===========>..................] - ETA: 8:23 - loss: 0.7006 - acc: 0.5296
2432/5677 [===========>..................] - ETA: 8:12 - loss: 0.7003 - acc: 0.5304
2496/5677 [============>.................] - ETA: 8:02 - loss: 0.7006 - acc: 0.5276
2560/5677 [============>.................] - ETA: 7:53 - loss: 0.7008 - acc: 0.5254
2624/5677 [============>.................] - ETA: 7:43 - loss: 0.7004 - acc: 0.5255
2688/5677 [=============>................] - ETA: 7:33 - loss: 0.6999 - acc: 0.5260
2752/5677 [=============>................] - ETA: 7:23 - loss: 0.7001 - acc: 0.5262
2816/5677 [=============>................] - ETA: 7:14 - loss: 0.7003 - acc: 0.5245
2880/5677 [==============>...............] - ETA: 7:04 - loss: 0.6997 - acc: 0.5274
2944/5677 [==============>...............] - ETA: 6:54 - loss: 0.6993 - acc: 0.5272
3008/5677 [==============>...............] - ETA: 6:44 - loss: 0.6996 - acc: 0.5263
3072/5677 [===============>..............] - ETA: 6:34 - loss: 0.7002 - acc: 0.5244
3136/5677 [===============>..............] - ETA: 6:24 - loss: 0.7003 - acc: 0.5249
3200/5677 [===============>..............] - ETA: 6:13 - loss: 0.7005 - acc: 0.5241
3264/5677 [================>.............] - ETA: 6:04 - loss: 0.7004 - acc: 0.5245
3328/5677 [================>.............] - ETA: 5:54 - loss: 0.6995 - acc: 0.5264
3392/5677 [================>.............] - ETA: 5:44 - loss: 0.6990 - acc: 0.5277
3456/5677 [=================>............] - ETA: 5:35 - loss: 0.6992 - acc: 0.5284
3520/5677 [=================>............] - ETA: 5:25 - loss: 0.6987 - acc: 0.5284
3584/5677 [=================>............] - ETA: 5:15 - loss: 0.6986 - acc: 0.5287
3648/5677 [==================>...........] - ETA: 5:06 - loss: 0.6985 - acc: 0.5291
3712/5677 [==================>...........] - ETA: 4:56 - loss: 0.6982 - acc: 0.5302
3776/5677 [==================>...........] - ETA: 4:47 - loss: 0.6978 - acc: 0.5315
3840/5677 [===================>..........] - ETA: 4:38 - loss: 0.6977 - acc: 0.5320
3904/5677 [===================>..........] - ETA: 4:28 - loss: 0.6979 - acc: 0.5318
3968/5677 [===================>..........] - ETA: 4:19 - loss: 0.6977 - acc: 0.5312
4032/5677 [====================>.........] - ETA: 4:09 - loss: 0.6977 - acc: 0.5312
4096/5677 [====================>.........] - ETA: 3:59 - loss: 0.6976 - acc: 0.5315
4160/5677 [====================>.........] - ETA: 3:49 - loss: 0.6973 - acc: 0.5317
4224/5677 [=====================>........] - ETA: 3:40 - loss: 0.6970 - acc: 0.5322
4288/5677 [=====================>........] - ETA: 3:31 - loss: 0.6970 - acc: 0.5317
4352/5677 [=====================>........] - ETA: 3:21 - loss: 0.6970 - acc: 0.5326
4416/5677 [======================>.......] - ETA: 3:12 - loss: 0.6970 - acc: 0.5335
4480/5677 [======================>.......] - ETA: 3:02 - loss: 0.6968 - acc: 0.5335
4544/5677 [=======================>......] - ETA: 2:52 - loss: 0.6961 - acc: 0.5346
4608/5677 [=======================>......] - ETA: 2:42 - loss: 0.6958 - acc: 0.5349
4672/5677 [=======================>......] - ETA: 2:33 - loss: 0.6957 - acc: 0.5353
4736/5677 [========================>.....] - ETA: 2:23 - loss: 0.6953 - acc: 0.5363
4800/5677 [========================>.....] - ETA: 2:13 - loss: 0.6948 - acc: 0.5381
4864/5677 [========================>.....] - ETA: 2:03 - loss: 0.6948 - acc: 0.5378
4928/5677 [=========================>....] - ETA: 1:54 - loss: 0.6944 - acc: 0.5386
4992/5677 [=========================>....] - ETA: 1:44 - loss: 0.6951 - acc: 0.5361
5056/5677 [=========================>....] - ETA: 1:34 - loss: 0.6953 - acc: 0.5356
5120/5677 [==========================>...] - ETA: 1:24 - loss: 0.6957 - acc: 0.5350
5184/5677 [==========================>...] - ETA: 1:15 - loss: 0.6960 - acc: 0.5340
5248/5677 [==========================>...] - ETA: 1:05 - loss: 0.6964 - acc: 0.5322
5312/5677 [===========================>..] - ETA: 55s - loss: 0.6965 - acc: 0.5314 
5376/5677 [===========================>..] - ETA: 45s - loss: 0.6965 - acc: 0.5318
5440/5677 [===========================>..] - ETA: 36s - loss: 0.6966 - acc: 0.5316
5504/5677 [============================>.] - ETA: 26s - loss: 0.6967 - acc: 0.5318
5568/5677 [============================>.] - ETA: 16s - loss: 0.6965 - acc: 0.5323
5632/5677 [============================>.] - ETA: 6s - loss: 0.6964 - acc: 0.5332 
5677/5677 [==============================] - 898s 158ms/step - loss: 0.6965 - acc: 0.5329 - val_loss: 0.7002 - val_acc: 0.4739

Epoch 00002: val_acc did not improve from 0.52615
Epoch 3/10

  64/5677 [..............................] - ETA: 14:29 - loss: 0.6891 - acc: 0.5156
 128/5677 [..............................] - ETA: 14:28 - loss: 0.6911 - acc: 0.5000
 192/5677 [>.............................] - ETA: 14:10 - loss: 0.7006 - acc: 0.4948
 256/5677 [>.............................] - ETA: 14:21 - loss: 0.7005 - acc: 0.5039
 320/5677 [>.............................] - ETA: 14:07 - loss: 0.7051 - acc: 0.4906
 384/5677 [=>............................] - ETA: 13:53 - loss: 0.6946 - acc: 0.5208
 448/5677 [=>............................] - ETA: 13:41 - loss: 0.6948 - acc: 0.5223
 512/5677 [=>............................] - ETA: 13:32 - loss: 0.6906 - acc: 0.5312
 576/5677 [==>...........................] - ETA: 13:29 - loss: 0.6924 - acc: 0.5330
 640/5677 [==>...........................] - ETA: 13:21 - loss: 0.6923 - acc: 0.5359
 704/5677 [==>...........................] - ETA: 13:14 - loss: 0.6910 - acc: 0.5398
 768/5677 [===>..........................] - ETA: 13:08 - loss: 0.6903 - acc: 0.5417
 832/5677 [===>..........................] - ETA: 12:56 - loss: 0.6891 - acc: 0.5421
 896/5677 [===>..........................] - ETA: 12:49 - loss: 0.6882 - acc: 0.5435
 960/5677 [====>.........................] - ETA: 12:39 - loss: 0.6897 - acc: 0.5417
1024/5677 [====>.........................] - ETA: 12:27 - loss: 0.6916 - acc: 0.5391
1088/5677 [====>.........................] - ETA: 12:19 - loss: 0.6908 - acc: 0.5404
1152/5677 [=====>........................] - ETA: 12:08 - loss: 0.6907 - acc: 0.5391
1216/5677 [=====>........................] - ETA: 11:58 - loss: 0.6913 - acc: 0.5362
1280/5677 [=====>........................] - ETA: 11:48 - loss: 0.6905 - acc: 0.5344
1344/5677 [======>.......................] - ETA: 11:38 - loss: 0.6911 - acc: 0.5312
1408/5677 [======>.......................] - ETA: 11:28 - loss: 0.6909 - acc: 0.5334
1472/5677 [======>.......................] - ETA: 11:18 - loss: 0.6914 - acc: 0.5312
1536/5677 [=======>......................] - ETA: 11:08 - loss: 0.6915 - acc: 0.5312
1600/5677 [=======>......................] - ETA: 10:57 - loss: 0.6929 - acc: 0.5281
1664/5677 [=======>......................] - ETA: 10:46 - loss: 0.6923 - acc: 0.5276
1728/5677 [========>.....................] - ETA: 10:35 - loss: 0.6915 - acc: 0.5295
1792/5677 [========>.....................] - ETA: 10:23 - loss: 0.6912 - acc: 0.5296
1856/5677 [========>.....................] - ETA: 10:14 - loss: 0.6901 - acc: 0.5323
1920/5677 [=========>....................] - ETA: 10:05 - loss: 0.6903 - acc: 0.5328
1984/5677 [=========>....................] - ETA: 9:56 - loss: 0.6909 - acc: 0.5323 
2048/5677 [=========>....................] - ETA: 9:45 - loss: 0.6907 - acc: 0.5322
2112/5677 [==========>...................] - ETA: 9:37 - loss: 0.6923 - acc: 0.5308
2176/5677 [==========>...................] - ETA: 9:25 - loss: 0.6922 - acc: 0.5326
2240/5677 [==========>...................] - ETA: 9:15 - loss: 0.6929 - acc: 0.5295
2304/5677 [===========>..................] - ETA: 9:03 - loss: 0.6926 - acc: 0.5286
2368/5677 [===========>..................] - ETA: 8:53 - loss: 0.6929 - acc: 0.5287
2432/5677 [===========>..................] - ETA: 8:43 - loss: 0.6928 - acc: 0.5280
2496/5677 [============>.................] - ETA: 8:32 - loss: 0.6938 - acc: 0.5256
2560/5677 [============>.................] - ETA: 8:23 - loss: 0.6942 - acc: 0.5270
2624/5677 [============>.................] - ETA: 8:12 - loss: 0.6941 - acc: 0.5274
2688/5677 [=============>................] - ETA: 8:02 - loss: 0.6947 - acc: 0.5272
2752/5677 [=============>................] - ETA: 7:51 - loss: 0.6941 - acc: 0.5280
2816/5677 [=============>................] - ETA: 7:42 - loss: 0.6938 - acc: 0.5277
2880/5677 [==============>...............] - ETA: 7:32 - loss: 0.6943 - acc: 0.5267
2944/5677 [==============>...............] - ETA: 7:22 - loss: 0.6947 - acc: 0.5251
3008/5677 [==============>...............] - ETA: 7:12 - loss: 0.6954 - acc: 0.5243
3072/5677 [===============>..............] - ETA: 7:01 - loss: 0.6957 - acc: 0.5247
3136/5677 [===============>..............] - ETA: 6:50 - loss: 0.6961 - acc: 0.5242
3200/5677 [===============>..............] - ETA: 6:40 - loss: 0.6962 - acc: 0.5247
3264/5677 [================>.............] - ETA: 6:29 - loss: 0.6961 - acc: 0.5254
3328/5677 [================>.............] - ETA: 6:19 - loss: 0.6960 - acc: 0.5252
3392/5677 [================>.............] - ETA: 6:08 - loss: 0.6961 - acc: 0.5251
3456/5677 [=================>............] - ETA: 5:58 - loss: 0.6961 - acc: 0.5237
3520/5677 [=================>............] - ETA: 5:47 - loss: 0.6956 - acc: 0.5256
3584/5677 [=================>............] - ETA: 5:37 - loss: 0.6958 - acc: 0.5257
3648/5677 [==================>...........] - ETA: 5:26 - loss: 0.6957 - acc: 0.5258
3712/5677 [==================>...........] - ETA: 5:16 - loss: 0.6948 - acc: 0.5294
3776/5677 [==================>...........] - ETA: 5:05 - loss: 0.6945 - acc: 0.5302
3840/5677 [===================>..........] - ETA: 4:54 - loss: 0.6945 - acc: 0.5312
3904/5677 [===================>..........] - ETA: 4:45 - loss: 0.6943 - acc: 0.5312
3968/5677 [===================>..........] - ETA: 4:34 - loss: 0.6940 - acc: 0.5323
4032/5677 [====================>.........] - ETA: 4:25 - loss: 0.6948 - acc: 0.5310
4096/5677 [====================>.........] - ETA: 4:15 - loss: 0.6943 - acc: 0.5325
4160/5677 [====================>.........] - ETA: 4:05 - loss: 0.6945 - acc: 0.5312
4224/5677 [=====================>........] - ETA: 3:55 - loss: 0.6942 - acc: 0.5322
4288/5677 [=====================>........] - ETA: 3:45 - loss: 0.6940 - acc: 0.5319
4352/5677 [=====================>........] - ETA: 3:34 - loss: 0.6939 - acc: 0.5317
4416/5677 [======================>.......] - ETA: 3:24 - loss: 0.6940 - acc: 0.5322
4480/5677 [======================>.......] - ETA: 3:14 - loss: 0.6940 - acc: 0.5324
4544/5677 [=======================>......] - ETA: 3:04 - loss: 0.6933 - acc: 0.5343
4608/5677 [=======================>......] - ETA: 2:53 - loss: 0.6929 - acc: 0.5339
4672/5677 [=======================>......] - ETA: 2:43 - loss: 0.6926 - acc: 0.5345
4736/5677 [========================>.....] - ETA: 2:33 - loss: 0.6922 - acc: 0.5361
4800/5677 [========================>.....] - ETA: 2:23 - loss: 0.6920 - acc: 0.5371
4864/5677 [========================>.....] - ETA: 2:12 - loss: 0.6918 - acc: 0.5380
4928/5677 [=========================>....] - ETA: 2:02 - loss: 0.6913 - acc: 0.5390
4992/5677 [=========================>....] - ETA: 1:52 - loss: 0.6911 - acc: 0.5395
5056/5677 [=========================>....] - ETA: 1:41 - loss: 0.6915 - acc: 0.5388
5120/5677 [==========================>...] - ETA: 1:31 - loss: 0.6919 - acc: 0.5385
5184/5677 [==========================>...] - ETA: 1:20 - loss: 0.6917 - acc: 0.5388
5248/5677 [==========================>...] - ETA: 1:10 - loss: 0.6924 - acc: 0.5375
5312/5677 [===========================>..] - ETA: 59s - loss: 0.6926 - acc: 0.5373 
5376/5677 [===========================>..] - ETA: 49s - loss: 0.6927 - acc: 0.5366
5440/5677 [===========================>..] - ETA: 38s - loss: 0.6928 - acc: 0.5360
5504/5677 [============================>.] - ETA: 28s - loss: 0.6929 - acc: 0.5363
5568/5677 [============================>.] - ETA: 17s - loss: 0.6931 - acc: 0.5348
5632/5677 [============================>.] - ETA: 7s - loss: 0.6927 - acc: 0.5359 
5677/5677 [==============================] - 970s 171ms/step - loss: 0.6928 - acc: 0.5364 - val_loss: 0.6774 - val_acc: 0.5975

Epoch 00003: val_acc improved from 0.52615 to 0.59746, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window15/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 4/10

  64/5677 [..............................] - ETA: 16:52 - loss: 0.6689 - acc: 0.6094
 128/5677 [..............................] - ETA: 15:56 - loss: 0.6919 - acc: 0.5625
 192/5677 [>.............................] - ETA: 16:17 - loss: 0.6989 - acc: 0.5521
 256/5677 [>.............................] - ETA: 16:03 - loss: 0.6948 - acc: 0.5547
 320/5677 [>.............................] - ETA: 15:55 - loss: 0.6981 - acc: 0.5469
 384/5677 [=>............................] - ETA: 15:33 - loss: 0.7009 - acc: 0.5339
 448/5677 [=>............................] - ETA: 15:12 - loss: 0.6980 - acc: 0.5357
 512/5677 [=>............................] - ETA: 15:05 - loss: 0.6993 - acc: 0.5234
 576/5677 [==>...........................] - ETA: 14:46 - loss: 0.6995 - acc: 0.5260
 640/5677 [==>...........................] - ETA: 14:27 - loss: 0.6959 - acc: 0.5344
 704/5677 [==>...........................] - ETA: 14:16 - loss: 0.6939 - acc: 0.5440
 768/5677 [===>..........................] - ETA: 13:55 - loss: 0.6936 - acc: 0.5417
 832/5677 [===>..........................] - ETA: 13:40 - loss: 0.6930 - acc: 0.5433
 896/5677 [===>..........................] - ETA: 13:31 - loss: 0.6942 - acc: 0.5368
 960/5677 [====>.........................] - ETA: 13:23 - loss: 0.6935 - acc: 0.5385
1024/5677 [====>.........................] - ETA: 13:12 - loss: 0.6913 - acc: 0.5469
1088/5677 [====>.........................] - ETA: 13:02 - loss: 0.6927 - acc: 0.5432
1152/5677 [=====>........................] - ETA: 12:48 - loss: 0.6948 - acc: 0.5408
1216/5677 [=====>........................] - ETA: 12:34 - loss: 0.6945 - acc: 0.5395
1280/5677 [=====>........................] - ETA: 12:21 - loss: 0.6929 - acc: 0.5437
1344/5677 [======>.......................] - ETA: 12:09 - loss: 0.6920 - acc: 0.5454
1408/5677 [======>.......................] - ETA: 11:58 - loss: 0.6908 - acc: 0.5476
1472/5677 [======>.......................] - ETA: 11:49 - loss: 0.6901 - acc: 0.5516
1536/5677 [=======>......................] - ETA: 11:35 - loss: 0.6898 - acc: 0.5501
1600/5677 [=======>......................] - ETA: 11:24 - loss: 0.6904 - acc: 0.5494
1664/5677 [=======>......................] - ETA: 11:11 - loss: 0.6903 - acc: 0.5481
1728/5677 [========>.....................] - ETA: 10:59 - loss: 0.6907 - acc: 0.5469
1792/5677 [========>.....................] - ETA: 10:47 - loss: 0.6905 - acc: 0.5469
1856/5677 [========>.....................] - ETA: 10:34 - loss: 0.6904 - acc: 0.5469
1920/5677 [=========>....................] - ETA: 10:23 - loss: 0.6898 - acc: 0.5500
1984/5677 [=========>....................] - ETA: 10:10 - loss: 0.6892 - acc: 0.5504
2048/5677 [=========>....................] - ETA: 9:59 - loss: 0.6899 - acc: 0.5498 
2112/5677 [==========>...................] - ETA: 9:46 - loss: 0.6897 - acc: 0.5492
2176/5677 [==========>...................] - ETA: 9:37 - loss: 0.6898 - acc: 0.5492
2240/5677 [==========>...................] - ETA: 9:26 - loss: 0.6898 - acc: 0.5513
2304/5677 [===========>..................] - ETA: 9:14 - loss: 0.6901 - acc: 0.5499
2368/5677 [===========>..................] - ETA: 9:07 - loss: 0.6904 - acc: 0.5498
2432/5677 [===========>..................] - ETA: 8:56 - loss: 0.6904 - acc: 0.5498
2496/5677 [============>.................] - ETA: 8:46 - loss: 0.6892 - acc: 0.5533
2560/5677 [============>.................] - ETA: 8:34 - loss: 0.6877 - acc: 0.5570
2624/5677 [============>.................] - ETA: 8:22 - loss: 0.6876 - acc: 0.5564
2688/5677 [=============>................] - ETA: 8:11 - loss: 0.6876 - acc: 0.5562
2752/5677 [=============>................] - ETA: 8:00 - loss: 0.6879 - acc: 0.5567
2816/5677 [=============>................] - ETA: 7:49 - loss: 0.6880 - acc: 0.5568
2880/5677 [==============>...............] - ETA: 7:38 - loss: 0.6882 - acc: 0.5569
2944/5677 [==============>...............] - ETA: 7:26 - loss: 0.6878 - acc: 0.5567
3008/5677 [==============>...............] - ETA: 7:15 - loss: 0.6879 - acc: 0.5568
3072/5677 [===============>..............] - ETA: 7:04 - loss: 0.6878 - acc: 0.5563
3136/5677 [===============>..............] - ETA: 6:53 - loss: 0.6885 - acc: 0.5539
3200/5677 [===============>..............] - ETA: 6:42 - loss: 0.6889 - acc: 0.5534
3264/5677 [================>.............] - ETA: 6:31 - loss: 0.6891 - acc: 0.5530
3328/5677 [================>.............] - ETA: 6:20 - loss: 0.6894 - acc: 0.5529
3392/5677 [================>.............] - ETA: 6:10 - loss: 0.6890 - acc: 0.5525
3456/5677 [=================>............] - ETA: 6:00 - loss: 0.6885 - acc: 0.5538
3520/5677 [=================>............] - ETA: 5:49 - loss: 0.6888 - acc: 0.5534
3584/5677 [=================>............] - ETA: 5:38 - loss: 0.6894 - acc: 0.5516
3648/5677 [==================>...........] - ETA: 5:27 - loss: 0.6893 - acc: 0.5513
3712/5677 [==================>...........] - ETA: 5:17 - loss: 0.6890 - acc: 0.5520
3776/5677 [==================>...........] - ETA: 5:07 - loss: 0.6890 - acc: 0.5519
3840/5677 [===================>..........] - ETA: 4:56 - loss: 0.6891 - acc: 0.5505
3904/5677 [===================>..........] - ETA: 4:45 - loss: 0.6891 - acc: 0.5489
3968/5677 [===================>..........] - ETA: 4:34 - loss: 0.6884 - acc: 0.5512
4032/5677 [====================>.........] - ETA: 4:24 - loss: 0.6879 - acc: 0.5523
4096/5677 [====================>.........] - ETA: 4:13 - loss: 0.6883 - acc: 0.5510
4160/5677 [====================>.........] - ETA: 4:03 - loss: 0.6887 - acc: 0.5505
4224/5677 [=====================>........] - ETA: 3:52 - loss: 0.6889 - acc: 0.5492
4288/5677 [=====================>........] - ETA: 3:42 - loss: 0.6891 - acc: 0.5492
4352/5677 [=====================>........] - ETA: 3:31 - loss: 0.6892 - acc: 0.5494
4416/5677 [======================>.......] - ETA: 3:21 - loss: 0.6893 - acc: 0.5496
4480/5677 [======================>.......] - ETA: 3:11 - loss: 0.6892 - acc: 0.5509
4544/5677 [=======================>......] - ETA: 3:00 - loss: 0.6892 - acc: 0.5513
4608/5677 [=======================>......] - ETA: 2:50 - loss: 0.6898 - acc: 0.5497
4672/5677 [=======================>......] - ETA: 2:39 - loss: 0.6895 - acc: 0.5507
4736/5677 [========================>.....] - ETA: 2:29 - loss: 0.6891 - acc: 0.5519
4800/5677 [========================>.....] - ETA: 2:19 - loss: 0.6891 - acc: 0.5525
4864/5677 [========================>.....] - ETA: 2:09 - loss: 0.6889 - acc: 0.5526
4928/5677 [=========================>....] - ETA: 1:58 - loss: 0.6895 - acc: 0.5513
4992/5677 [=========================>....] - ETA: 1:48 - loss: 0.6893 - acc: 0.5515
5056/5677 [=========================>....] - ETA: 1:38 - loss: 0.6893 - acc: 0.5518
5120/5677 [==========================>...] - ETA: 1:28 - loss: 0.6893 - acc: 0.5512
5184/5677 [==========================>...] - ETA: 1:17 - loss: 0.6894 - acc: 0.5507
5248/5677 [==========================>...] - ETA: 1:07 - loss: 0.6892 - acc: 0.5520
5312/5677 [===========================>..] - ETA: 57s - loss: 0.6886 - acc: 0.5538 
5376/5677 [===========================>..] - ETA: 47s - loss: 0.6884 - acc: 0.5541
5440/5677 [===========================>..] - ETA: 37s - loss: 0.6882 - acc: 0.5553
5504/5677 [============================>.] - ETA: 27s - loss: 0.6881 - acc: 0.5556
5568/5677 [============================>.] - ETA: 17s - loss: 0.6877 - acc: 0.5566
5632/5677 [============================>.] - ETA: 7s - loss: 0.6877 - acc: 0.5570 
5677/5677 [==============================] - 919s 162ms/step - loss: 0.6881 - acc: 0.5563 - val_loss: 0.6889 - val_acc: 0.5563

Epoch 00004: val_acc did not improve from 0.59746
Epoch 5/10

  64/5677 [..............................] - ETA: 12:07 - loss: 0.7179 - acc: 0.4844
 128/5677 [..............................] - ETA: 12:17 - loss: 0.7041 - acc: 0.5234
 192/5677 [>.............................] - ETA: 12:17 - loss: 0.7068 - acc: 0.5052
 256/5677 [>.............................] - ETA: 12:17 - loss: 0.7042 - acc: 0.5117
 320/5677 [>.............................] - ETA: 12:39 - loss: 0.7009 - acc: 0.5312
 384/5677 [=>............................] - ETA: 12:20 - loss: 0.7037 - acc: 0.5365
 448/5677 [=>............................] - ETA: 12:11 - loss: 0.6992 - acc: 0.5491
 512/5677 [=>............................] - ETA: 11:58 - loss: 0.6940 - acc: 0.5625
 576/5677 [==>...........................] - ETA: 11:53 - loss: 0.6932 - acc: 0.5573
 640/5677 [==>...........................] - ETA: 11:42 - loss: 0.6894 - acc: 0.5687
 704/5677 [==>...........................] - ETA: 11:41 - loss: 0.6868 - acc: 0.5753
 768/5677 [===>..........................] - ETA: 11:34 - loss: 0.6849 - acc: 0.5820
 832/5677 [===>..........................] - ETA: 11:33 - loss: 0.6856 - acc: 0.5805
 896/5677 [===>..........................] - ETA: 11:18 - loss: 0.6864 - acc: 0.5759
 960/5677 [====>.........................] - ETA: 11:06 - loss: 0.6886 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 11:01 - loss: 0.6877 - acc: 0.5723
1088/5677 [====>.........................] - ETA: 10:47 - loss: 0.6893 - acc: 0.5680
1152/5677 [=====>........................] - ETA: 10:41 - loss: 0.6894 - acc: 0.5677
1216/5677 [=====>........................] - ETA: 10:35 - loss: 0.6885 - acc: 0.5699
1280/5677 [=====>........................] - ETA: 10:30 - loss: 0.6886 - acc: 0.5687
1344/5677 [======>.......................] - ETA: 10:21 - loss: 0.6883 - acc: 0.5662
1408/5677 [======>.......................] - ETA: 10:16 - loss: 0.6867 - acc: 0.5682
1472/5677 [======>.......................] - ETA: 10:08 - loss: 0.6847 - acc: 0.5747
1536/5677 [=======>......................] - ETA: 9:58 - loss: 0.6834 - acc: 0.5775 
1600/5677 [=======>......................] - ETA: 9:48 - loss: 0.6837 - acc: 0.5775
1664/5677 [=======>......................] - ETA: 9:39 - loss: 0.6847 - acc: 0.5751
1728/5677 [========>.....................] - ETA: 9:30 - loss: 0.6857 - acc: 0.5718
1792/5677 [========>.....................] - ETA: 9:22 - loss: 0.6863 - acc: 0.5720
1856/5677 [========>.....................] - ETA: 9:13 - loss: 0.6855 - acc: 0.5738
1920/5677 [=========>....................] - ETA: 9:05 - loss: 0.6857 - acc: 0.5729
1984/5677 [=========>....................] - ETA: 8:57 - loss: 0.6853 - acc: 0.5751
2048/5677 [=========>....................] - ETA: 8:46 - loss: 0.6845 - acc: 0.5781
2112/5677 [==========>...................] - ETA: 8:38 - loss: 0.6846 - acc: 0.5777
2176/5677 [==========>...................] - ETA: 8:28 - loss: 0.6845 - acc: 0.5767
2240/5677 [==========>...................] - ETA: 8:20 - loss: 0.6846 - acc: 0.5754
2304/5677 [===========>..................] - ETA: 8:11 - loss: 0.6847 - acc: 0.5747
2368/5677 [===========>..................] - ETA: 8:04 - loss: 0.6847 - acc: 0.5739
2432/5677 [===========>..................] - ETA: 7:53 - loss: 0.6847 - acc: 0.5740
2496/5677 [============>.................] - ETA: 7:44 - loss: 0.6842 - acc: 0.5729
2560/5677 [============>.................] - ETA: 7:36 - loss: 0.6841 - acc: 0.5742
2624/5677 [============>.................] - ETA: 7:26 - loss: 0.6843 - acc: 0.5755
2688/5677 [=============>................] - ETA: 7:17 - loss: 0.6844 - acc: 0.5763
2752/5677 [=============>................] - ETA: 7:07 - loss: 0.6847 - acc: 0.5763
2816/5677 [=============>................] - ETA: 6:58 - loss: 0.6855 - acc: 0.5728
2880/5677 [==============>...............] - ETA: 6:49 - loss: 0.6841 - acc: 0.5760
2944/5677 [==============>...............] - ETA: 6:41 - loss: 0.6844 - acc: 0.5744
3008/5677 [==============>...............] - ETA: 6:32 - loss: 0.6843 - acc: 0.5735
3072/5677 [===============>..............] - ETA: 6:23 - loss: 0.6831 - acc: 0.5758
3136/5677 [===============>..............] - ETA: 6:14 - loss: 0.6835 - acc: 0.5753
3200/5677 [===============>..............] - ETA: 6:06 - loss: 0.6833 - acc: 0.5753
3264/5677 [================>.............] - ETA: 5:57 - loss: 0.6830 - acc: 0.5738
3328/5677 [================>.............] - ETA: 5:47 - loss: 0.6835 - acc: 0.5730
3392/5677 [================>.............] - ETA: 5:37 - loss: 0.6836 - acc: 0.5725
3456/5677 [=================>............] - ETA: 5:28 - loss: 0.6842 - acc: 0.5715
3520/5677 [=================>............] - ETA: 5:19 - loss: 0.6848 - acc: 0.5693
3584/5677 [=================>............] - ETA: 5:09 - loss: 0.6848 - acc: 0.5689
3648/5677 [==================>...........] - ETA: 4:59 - loss: 0.6848 - acc: 0.5688
3712/5677 [==================>...........] - ETA: 4:50 - loss: 0.6843 - acc: 0.5698
3776/5677 [==================>...........] - ETA: 4:40 - loss: 0.6838 - acc: 0.5710
3840/5677 [===================>..........] - ETA: 4:30 - loss: 0.6840 - acc: 0.5708
3904/5677 [===================>..........] - ETA: 4:21 - loss: 0.6849 - acc: 0.5684
3968/5677 [===================>..........] - ETA: 4:11 - loss: 0.6845 - acc: 0.5683
4032/5677 [====================>.........] - ETA: 4:02 - loss: 0.6854 - acc: 0.5670
4096/5677 [====================>.........] - ETA: 3:52 - loss: 0.6862 - acc: 0.5667
4160/5677 [====================>.........] - ETA: 3:43 - loss: 0.6868 - acc: 0.5656
4224/5677 [=====================>........] - ETA: 3:34 - loss: 0.6868 - acc: 0.5644
4288/5677 [=====================>........] - ETA: 3:24 - loss: 0.6864 - acc: 0.5648
4352/5677 [=====================>........] - ETA: 3:15 - loss: 0.6855 - acc: 0.5666
4416/5677 [======================>.......] - ETA: 3:05 - loss: 0.6852 - acc: 0.5668
4480/5677 [======================>.......] - ETA: 2:56 - loss: 0.6852 - acc: 0.5663
4544/5677 [=======================>......] - ETA: 2:46 - loss: 0.6850 - acc: 0.5660
4608/5677 [=======================>......] - ETA: 2:37 - loss: 0.6851 - acc: 0.5662
4672/5677 [=======================>......] - ETA: 2:28 - loss: 0.6846 - acc: 0.5668
4736/5677 [========================>.....] - ETA: 2:18 - loss: 0.6850 - acc: 0.5659
4800/5677 [========================>.....] - ETA: 2:09 - loss: 0.6848 - acc: 0.5656
4864/5677 [========================>.....] - ETA: 1:59 - loss: 0.6848 - acc: 0.5660
4928/5677 [=========================>....] - ETA: 1:50 - loss: 0.6846 - acc: 0.5666
4992/5677 [=========================>....] - ETA: 1:40 - loss: 0.6852 - acc: 0.5661
5056/5677 [=========================>....] - ETA: 1:31 - loss: 0.6854 - acc: 0.5657
5120/5677 [==========================>...] - ETA: 1:22 - loss: 0.6853 - acc: 0.5658
5184/5677 [==========================>...] - ETA: 1:12 - loss: 0.6851 - acc: 0.5669
5248/5677 [==========================>...] - ETA: 1:03 - loss: 0.6850 - acc: 0.5663
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6852 - acc: 0.5663 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6849 - acc: 0.5666
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6846 - acc: 0.5664
5504/5677 [============================>.] - ETA: 25s - loss: 0.6845 - acc: 0.5669
5568/5677 [============================>.] - ETA: 16s - loss: 0.6845 - acc: 0.5672
5632/5677 [============================>.] - ETA: 6s - loss: 0.6842 - acc: 0.5678 
5677/5677 [==============================] - 866s 153ms/step - loss: 0.6844 - acc: 0.5677 - val_loss: 0.6747 - val_acc: 0.5864

Epoch 00005: val_acc did not improve from 0.59746
Epoch 6/10

  64/5677 [..............................] - ETA: 13:21 - loss: 0.6834 - acc: 0.5781
 128/5677 [..............................] - ETA: 12:57 - loss: 0.6759 - acc: 0.5547
 192/5677 [>.............................] - ETA: 12:48 - loss: 0.6717 - acc: 0.5417
 256/5677 [>.............................] - ETA: 12:46 - loss: 0.6689 - acc: 0.5625
 320/5677 [>.............................] - ETA: 12:45 - loss: 0.6750 - acc: 0.5563
 384/5677 [=>............................] - ETA: 12:35 - loss: 0.6871 - acc: 0.5365
 448/5677 [=>............................] - ETA: 12:27 - loss: 0.6832 - acc: 0.5491
 512/5677 [=>............................] - ETA: 12:19 - loss: 0.6842 - acc: 0.5508
 576/5677 [==>...........................] - ETA: 12:13 - loss: 0.6829 - acc: 0.5642
 640/5677 [==>...........................] - ETA: 12:05 - loss: 0.6817 - acc: 0.5687
 704/5677 [==>...........................] - ETA: 11:59 - loss: 0.6832 - acc: 0.5639
 768/5677 [===>..........................] - ETA: 11:51 - loss: 0.6825 - acc: 0.5625
 832/5677 [===>..........................] - ETA: 11:47 - loss: 0.6819 - acc: 0.5661
 896/5677 [===>..........................] - ETA: 11:38 - loss: 0.6792 - acc: 0.5714
 960/5677 [====>.........................] - ETA: 11:30 - loss: 0.6795 - acc: 0.5719
1024/5677 [====>.........................] - ETA: 11:19 - loss: 0.6784 - acc: 0.5742
1088/5677 [====>.........................] - ETA: 11:10 - loss: 0.6793 - acc: 0.5726
1152/5677 [=====>........................] - ETA: 10:58 - loss: 0.6773 - acc: 0.5781
1216/5677 [=====>........................] - ETA: 10:50 - loss: 0.6771 - acc: 0.5798
1280/5677 [=====>........................] - ETA: 10:41 - loss: 0.6796 - acc: 0.5742
1344/5677 [======>.......................] - ETA: 10:30 - loss: 0.6801 - acc: 0.5737
1408/5677 [======>.......................] - ETA: 10:19 - loss: 0.6806 - acc: 0.5746
1472/5677 [======>.......................] - ETA: 10:12 - loss: 0.6810 - acc: 0.5740
1536/5677 [=======>......................] - ETA: 10:00 - loss: 0.6801 - acc: 0.5736
1600/5677 [=======>......................] - ETA: 9:54 - loss: 0.6787 - acc: 0.5737 
1664/5677 [=======>......................] - ETA: 9:43 - loss: 0.6785 - acc: 0.5745
1728/5677 [========>.....................] - ETA: 9:35 - loss: 0.6788 - acc: 0.5735
1792/5677 [========>.....................] - ETA: 9:24 - loss: 0.6809 - acc: 0.5703
1856/5677 [========>.....................] - ETA: 9:17 - loss: 0.6794 - acc: 0.5711
1920/5677 [=========>....................] - ETA: 9:07 - loss: 0.6806 - acc: 0.5693
1984/5677 [=========>....................] - ETA: 8:58 - loss: 0.6803 - acc: 0.5685
2048/5677 [=========>....................] - ETA: 8:47 - loss: 0.6804 - acc: 0.5684
2112/5677 [==========>...................] - ETA: 8:37 - loss: 0.6803 - acc: 0.5668
2176/5677 [==========>...................] - ETA: 8:28 - loss: 0.6805 - acc: 0.5662
2240/5677 [==========>...................] - ETA: 8:17 - loss: 0.6796 - acc: 0.5683
2304/5677 [===========>..................] - ETA: 8:07 - loss: 0.6801 - acc: 0.5673
2368/5677 [===========>..................] - ETA: 7:58 - loss: 0.6794 - acc: 0.5684
2432/5677 [===========>..................] - ETA: 7:48 - loss: 0.6803 - acc: 0.5658
2496/5677 [============>.................] - ETA: 7:38 - loss: 0.6813 - acc: 0.5641
2560/5677 [============>.................] - ETA: 7:29 - loss: 0.6827 - acc: 0.5633
2624/5677 [============>.................] - ETA: 7:20 - loss: 0.6828 - acc: 0.5617
2688/5677 [=============>................] - ETA: 7:10 - loss: 0.6831 - acc: 0.5610
2752/5677 [=============>................] - ETA: 7:01 - loss: 0.6832 - acc: 0.5603
2816/5677 [=============>................] - ETA: 6:51 - loss: 0.6825 - acc: 0.5614
2880/5677 [==============>...............] - ETA: 6:41 - loss: 0.6824 - acc: 0.5618
2944/5677 [==============>...............] - ETA: 6:33 - loss: 0.6827 - acc: 0.5611
3008/5677 [==============>...............] - ETA: 6:25 - loss: 0.6830 - acc: 0.5612
3072/5677 [===============>..............] - ETA: 6:16 - loss: 0.6822 - acc: 0.5635
3136/5677 [===============>..............] - ETA: 6:07 - loss: 0.6828 - acc: 0.5612
3200/5677 [===============>..............] - ETA: 6:00 - loss: 0.6824 - acc: 0.5625
3264/5677 [================>.............] - ETA: 5:50 - loss: 0.6821 - acc: 0.5631
3328/5677 [================>.............] - ETA: 5:42 - loss: 0.6823 - acc: 0.5619
3392/5677 [================>.............] - ETA: 5:33 - loss: 0.6822 - acc: 0.5622
3456/5677 [=================>............] - ETA: 5:24 - loss: 0.6819 - acc: 0.5637
3520/5677 [=================>............] - ETA: 5:14 - loss: 0.6810 - acc: 0.5659
3584/5677 [=================>............] - ETA: 5:05 - loss: 0.6804 - acc: 0.5667
3648/5677 [==================>...........] - ETA: 4:57 - loss: 0.6796 - acc: 0.5677
3712/5677 [==================>...........] - ETA: 4:47 - loss: 0.6799 - acc: 0.5668
3776/5677 [==================>...........] - ETA: 4:38 - loss: 0.6800 - acc: 0.5670
3840/5677 [===================>..........] - ETA: 4:29 - loss: 0.6801 - acc: 0.5664
3904/5677 [===================>..........] - ETA: 4:20 - loss: 0.6799 - acc: 0.5674
3968/5677 [===================>..........] - ETA: 4:10 - loss: 0.6800 - acc: 0.5680
4032/5677 [====================>.........] - ETA: 4:01 - loss: 0.6805 - acc: 0.5670
4096/5677 [====================>.........] - ETA: 3:51 - loss: 0.6804 - acc: 0.5669
4160/5677 [====================>.........] - ETA: 3:42 - loss: 0.6809 - acc: 0.5659
4224/5677 [=====================>........] - ETA: 3:33 - loss: 0.6801 - acc: 0.5668
4288/5677 [=====================>........] - ETA: 3:24 - loss: 0.6802 - acc: 0.5667
4352/5677 [=====================>........] - ETA: 3:15 - loss: 0.6799 - acc: 0.5673
4416/5677 [======================>.......] - ETA: 3:05 - loss: 0.6800 - acc: 0.5682
4480/5677 [======================>.......] - ETA: 2:56 - loss: 0.6794 - acc: 0.5690
4544/5677 [=======================>......] - ETA: 2:47 - loss: 0.6797 - acc: 0.5684
4608/5677 [=======================>......] - ETA: 2:38 - loss: 0.6801 - acc: 0.5677
4672/5677 [=======================>......] - ETA: 2:28 - loss: 0.6804 - acc: 0.5679
4736/5677 [========================>.....] - ETA: 2:19 - loss: 0.6803 - acc: 0.5680
4800/5677 [========================>.....] - ETA: 2:09 - loss: 0.6801 - acc: 0.5687
4864/5677 [========================>.....] - ETA: 2:00 - loss: 0.6803 - acc: 0.5683
4928/5677 [=========================>....] - ETA: 1:50 - loss: 0.6805 - acc: 0.5684
4992/5677 [=========================>....] - ETA: 1:41 - loss: 0.6810 - acc: 0.5675
5056/5677 [=========================>....] - ETA: 1:31 - loss: 0.6811 - acc: 0.5674
5120/5677 [==========================>...] - ETA: 1:22 - loss: 0.6815 - acc: 0.5656
5184/5677 [==========================>...] - ETA: 1:12 - loss: 0.6815 - acc: 0.5656
5248/5677 [==========================>...] - ETA: 1:03 - loss: 0.6810 - acc: 0.5669
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6816 - acc: 0.5648 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6820 - acc: 0.5636
5440/5677 [===========================>..] - ETA: 35s - loss: 0.6823 - acc: 0.5627
5504/5677 [============================>.] - ETA: 25s - loss: 0.6821 - acc: 0.5634
5568/5677 [============================>.] - ETA: 16s - loss: 0.6820 - acc: 0.5630
5632/5677 [============================>.] - ETA: 6s - loss: 0.6817 - acc: 0.5637 
5677/5677 [==============================] - 873s 154ms/step - loss: 0.6820 - acc: 0.5635 - val_loss: 0.6779 - val_acc: 0.5436

Epoch 00006: val_acc did not improve from 0.59746
Epoch 7/10

  64/5677 [..............................] - ETA: 13:59 - loss: 0.7067 - acc: 0.4219
 128/5677 [..............................] - ETA: 13:27 - loss: 0.7069 - acc: 0.4453
 192/5677 [>.............................] - ETA: 13:17 - loss: 0.7002 - acc: 0.4844
 256/5677 [>.............................] - ETA: 13:23 - loss: 0.6977 - acc: 0.4766
 320/5677 [>.............................] - ETA: 13:20 - loss: 0.7015 - acc: 0.4781
 384/5677 [=>............................] - ETA: 13:04 - loss: 0.6970 - acc: 0.4974
 448/5677 [=>............................] - ETA: 12:54 - loss: 0.6962 - acc: 0.4955
 512/5677 [=>............................] - ETA: 12:39 - loss: 0.6919 - acc: 0.5098
 576/5677 [==>...........................] - ETA: 12:30 - loss: 0.6935 - acc: 0.5069
 640/5677 [==>...........................] - ETA: 12:31 - loss: 0.6958 - acc: 0.5000
 704/5677 [==>...........................] - ETA: 12:20 - loss: 0.6931 - acc: 0.5043
 768/5677 [===>..........................] - ETA: 12:09 - loss: 0.6949 - acc: 0.5013
 832/5677 [===>..........................] - ETA: 12:00 - loss: 0.6915 - acc: 0.5108
 896/5677 [===>..........................] - ETA: 11:53 - loss: 0.6917 - acc: 0.5112
 960/5677 [====>.........................] - ETA: 11:40 - loss: 0.6916 - acc: 0.5177
1024/5677 [====>.........................] - ETA: 11:29 - loss: 0.6892 - acc: 0.5195
1088/5677 [====>.........................] - ETA: 11:18 - loss: 0.6879 - acc: 0.5230
1152/5677 [=====>........................] - ETA: 11:04 - loss: 0.6881 - acc: 0.5252
1216/5677 [=====>........................] - ETA: 10:58 - loss: 0.6892 - acc: 0.5222
1280/5677 [=====>........................] - ETA: 10:47 - loss: 0.6876 - acc: 0.5289
1344/5677 [======>.......................] - ETA: 10:38 - loss: 0.6870 - acc: 0.5312
1408/5677 [======>.......................] - ETA: 10:28 - loss: 0.6871 - acc: 0.5320
1472/5677 [======>.......................] - ETA: 10:20 - loss: 0.6887 - acc: 0.5285
1536/5677 [=======>......................] - ETA: 10:10 - loss: 0.6873 - acc: 0.5332
1600/5677 [=======>......................] - ETA: 10:02 - loss: 0.6875 - acc: 0.5325
1664/5677 [=======>......................] - ETA: 9:56 - loss: 0.6868 - acc: 0.5379 
1728/5677 [========>.....................] - ETA: 9:49 - loss: 0.6862 - acc: 0.5411
1792/5677 [========>.....................] - ETA: 9:38 - loss: 0.6856 - acc: 0.5419
1856/5677 [========>.....................] - ETA: 9:28 - loss: 0.6850 - acc: 0.5453
1920/5677 [=========>....................] - ETA: 9:21 - loss: 0.6837 - acc: 0.5500
1984/5677 [=========>....................] - ETA: 9:12 - loss: 0.6821 - acc: 0.5524
2048/5677 [=========>....................] - ETA: 9:04 - loss: 0.6820 - acc: 0.5537
2112/5677 [==========>...................] - ETA: 8:57 - loss: 0.6813 - acc: 0.5549
2176/5677 [==========>...................] - ETA: 8:49 - loss: 0.6821 - acc: 0.5551
2240/5677 [==========>...................] - ETA: 8:39 - loss: 0.6828 - acc: 0.5536
2304/5677 [===========>..................] - ETA: 8:32 - loss: 0.6835 - acc: 0.5521
2368/5677 [===========>..................] - ETA: 8:24 - loss: 0.6842 - acc: 0.5519
2432/5677 [===========>..................] - ETA: 8:15 - loss: 0.6830 - acc: 0.5551
2496/5677 [============>.................] - ETA: 8:06 - loss: 0.6852 - acc: 0.5513
2560/5677 [============>.................] - ETA: 7:57 - loss: 0.6845 - acc: 0.5527
2624/5677 [============>.................] - ETA: 7:48 - loss: 0.6860 - acc: 0.5503
2688/5677 [=============>................] - ETA: 7:38 - loss: 0.6849 - acc: 0.5543
2752/5677 [=============>................] - ETA: 7:29 - loss: 0.6839 - acc: 0.5574
2816/5677 [=============>................] - ETA: 7:20 - loss: 0.6840 - acc: 0.5572
2880/5677 [==============>...............] - ETA: 7:11 - loss: 0.6836 - acc: 0.5576
2944/5677 [==============>...............] - ETA: 7:01 - loss: 0.6837 - acc: 0.5577
3008/5677 [==============>...............] - ETA: 6:52 - loss: 0.6842 - acc: 0.5572
3072/5677 [===============>..............] - ETA: 6:43 - loss: 0.6837 - acc: 0.5579
3136/5677 [===============>..............] - ETA: 6:33 - loss: 0.6839 - acc: 0.5577
3200/5677 [===============>..............] - ETA: 6:24 - loss: 0.6836 - acc: 0.5584
3264/5677 [================>.............] - ETA: 6:14 - loss: 0.6845 - acc: 0.5573
3328/5677 [================>.............] - ETA: 6:04 - loss: 0.6838 - acc: 0.5583
3392/5677 [================>.............] - ETA: 5:54 - loss: 0.6834 - acc: 0.5598
3456/5677 [=================>............] - ETA: 5:44 - loss: 0.6828 - acc: 0.5616
3520/5677 [=================>............] - ETA: 5:34 - loss: 0.6837 - acc: 0.5599
3584/5677 [=================>............] - ETA: 5:24 - loss: 0.6844 - acc: 0.5580
3648/5677 [==================>...........] - ETA: 5:14 - loss: 0.6835 - acc: 0.5598
3712/5677 [==================>...........] - ETA: 5:04 - loss: 0.6836 - acc: 0.5587
3776/5677 [==================>...........] - ETA: 4:55 - loss: 0.6830 - acc: 0.5609
3840/5677 [===================>..........] - ETA: 4:45 - loss: 0.6823 - acc: 0.5628
3904/5677 [===================>..........] - ETA: 4:35 - loss: 0.6835 - acc: 0.5605
3968/5677 [===================>..........] - ETA: 4:25 - loss: 0.6837 - acc: 0.5600
4032/5677 [====================>.........] - ETA: 4:15 - loss: 0.6839 - acc: 0.5593
4096/5677 [====================>.........] - ETA: 4:05 - loss: 0.6839 - acc: 0.5598
4160/5677 [====================>.........] - ETA: 3:55 - loss: 0.6841 - acc: 0.5596
4224/5677 [=====================>........] - ETA: 3:45 - loss: 0.6840 - acc: 0.5599
4288/5677 [=====================>........] - ETA: 3:35 - loss: 0.6834 - acc: 0.5627
4352/5677 [=====================>........] - ETA: 3:25 - loss: 0.6833 - acc: 0.5630
4416/5677 [======================>.......] - ETA: 3:15 - loss: 0.6835 - acc: 0.5630
4480/5677 [======================>.......] - ETA: 3:05 - loss: 0.6825 - acc: 0.5654
4544/5677 [=======================>......] - ETA: 2:55 - loss: 0.6828 - acc: 0.5638
4608/5677 [=======================>......] - ETA: 2:45 - loss: 0.6823 - acc: 0.5653
4672/5677 [=======================>......] - ETA: 2:35 - loss: 0.6826 - acc: 0.5640
4736/5677 [========================>.....] - ETA: 2:25 - loss: 0.6831 - acc: 0.5627
4800/5677 [========================>.....] - ETA: 2:15 - loss: 0.6832 - acc: 0.5623
4864/5677 [========================>.....] - ETA: 2:05 - loss: 0.6827 - acc: 0.5639
4928/5677 [=========================>....] - ETA: 1:56 - loss: 0.6823 - acc: 0.5659
4992/5677 [=========================>....] - ETA: 1:46 - loss: 0.6820 - acc: 0.5663
5056/5677 [=========================>....] - ETA: 1:36 - loss: 0.6819 - acc: 0.5663
5120/5677 [==========================>...] - ETA: 1:26 - loss: 0.6813 - acc: 0.5670
5184/5677 [==========================>...] - ETA: 1:16 - loss: 0.6808 - acc: 0.5689
5248/5677 [==========================>...] - ETA: 1:06 - loss: 0.6813 - acc: 0.5675
5312/5677 [===========================>..] - ETA: 56s - loss: 0.6815 - acc: 0.5670 
5376/5677 [===========================>..] - ETA: 46s - loss: 0.6813 - acc: 0.5668
5440/5677 [===========================>..] - ETA: 36s - loss: 0.6813 - acc: 0.5665
5504/5677 [============================>.] - ETA: 26s - loss: 0.6814 - acc: 0.5661
5568/5677 [============================>.] - ETA: 16s - loss: 0.6812 - acc: 0.5668
5632/5677 [============================>.] - ETA: 6s - loss: 0.6819 - acc: 0.5661 
5677/5677 [==============================] - 914s 161ms/step - loss: 0.6820 - acc: 0.5660 - val_loss: 0.6725 - val_acc: 0.5880

Epoch 00007: val_acc did not improve from 0.59746
Epoch 8/10

  64/5677 [..............................] - ETA: 15:42 - loss: 0.7013 - acc: 0.4844
 128/5677 [..............................] - ETA: 15:19 - loss: 0.6756 - acc: 0.5234
 192/5677 [>.............................] - ETA: 14:37 - loss: 0.6739 - acc: 0.5573
 256/5677 [>.............................] - ETA: 13:58 - loss: 0.6845 - acc: 0.5547
 320/5677 [>.............................] - ETA: 13:46 - loss: 0.6837 - acc: 0.5656
 384/5677 [=>............................] - ETA: 13:28 - loss: 0.6858 - acc: 0.5677
 448/5677 [=>............................] - ETA: 13:33 - loss: 0.6874 - acc: 0.5603
 512/5677 [=>............................] - ETA: 13:19 - loss: 0.6860 - acc: 0.5586
 576/5677 [==>...........................] - ETA: 13:07 - loss: 0.6860 - acc: 0.5556
 640/5677 [==>...........................] - ETA: 12:59 - loss: 0.6839 - acc: 0.5625
 704/5677 [==>...........................] - ETA: 12:51 - loss: 0.6812 - acc: 0.5696
 768/5677 [===>..........................] - ETA: 12:34 - loss: 0.6814 - acc: 0.5638
 832/5677 [===>..........................] - ETA: 12:32 - loss: 0.6816 - acc: 0.5613
 896/5677 [===>..........................] - ETA: 12:22 - loss: 0.6778 - acc: 0.5692
 960/5677 [====>.........................] - ETA: 12:06 - loss: 0.6771 - acc: 0.5698
1024/5677 [====>.........................] - ETA: 11:56 - loss: 0.6777 - acc: 0.5684
1088/5677 [====>.........................] - ETA: 11:48 - loss: 0.6795 - acc: 0.5616
1152/5677 [=====>........................] - ETA: 11:38 - loss: 0.6798 - acc: 0.5625
1216/5677 [=====>........................] - ETA: 11:27 - loss: 0.6798 - acc: 0.5650
1280/5677 [=====>........................] - ETA: 11:12 - loss: 0.6796 - acc: 0.5648
1344/5677 [======>.......................] - ETA: 11:03 - loss: 0.6790 - acc: 0.5640
1408/5677 [======>.......................] - ETA: 10:50 - loss: 0.6787 - acc: 0.5661
1472/5677 [======>.......................] - ETA: 10:40 - loss: 0.6790 - acc: 0.5639
1536/5677 [=======>......................] - ETA: 10:31 - loss: 0.6802 - acc: 0.5645
1600/5677 [=======>......................] - ETA: 10:21 - loss: 0.6810 - acc: 0.5625
1664/5677 [=======>......................] - ETA: 10:11 - loss: 0.6811 - acc: 0.5595
1728/5677 [========>.....................] - ETA: 9:57 - loss: 0.6813 - acc: 0.5602 
1792/5677 [========>.....................] - ETA: 9:47 - loss: 0.6808 - acc: 0.5614
1856/5677 [========>.....................] - ETA: 9:38 - loss: 0.6806 - acc: 0.5620
1920/5677 [=========>....................] - ETA: 9:29 - loss: 0.6808 - acc: 0.5620
1984/5677 [=========>....................] - ETA: 9:19 - loss: 0.6808 - acc: 0.5635
2048/5677 [=========>....................] - ETA: 9:09 - loss: 0.6811 - acc: 0.5654
2112/5677 [==========>...................] - ETA: 8:58 - loss: 0.6804 - acc: 0.5682
2176/5677 [==========>...................] - ETA: 8:50 - loss: 0.6802 - acc: 0.5685
2240/5677 [==========>...................] - ETA: 8:39 - loss: 0.6793 - acc: 0.5710
2304/5677 [===========>..................] - ETA: 8:30 - loss: 0.6793 - acc: 0.5716
2368/5677 [===========>..................] - ETA: 8:21 - loss: 0.6794 - acc: 0.5743
2432/5677 [===========>..................] - ETA: 8:12 - loss: 0.6788 - acc: 0.5757
2496/5677 [============>.................] - ETA: 7:59 - loss: 0.6781 - acc: 0.5765
2560/5677 [============>.................] - ETA: 7:48 - loss: 0.6782 - acc: 0.5766
2624/5677 [============>.................] - ETA: 7:37 - loss: 0.6776 - acc: 0.5774
2688/5677 [=============>................] - ETA: 7:26 - loss: 0.6773 - acc: 0.5774
2752/5677 [=============>................] - ETA: 7:17 - loss: 0.6777 - acc: 0.5759
2816/5677 [=============>................] - ETA: 7:08 - loss: 0.6766 - acc: 0.5788
2880/5677 [==============>...............] - ETA: 6:59 - loss: 0.6761 - acc: 0.5795
2944/5677 [==============>...............] - ETA: 6:49 - loss: 0.6744 - acc: 0.5829
3008/5677 [==============>...............] - ETA: 6:38 - loss: 0.6749 - acc: 0.5808
3072/5677 [===============>..............] - ETA: 6:27 - loss: 0.6749 - acc: 0.5804
3136/5677 [===============>..............] - ETA: 6:17 - loss: 0.6746 - acc: 0.5810
3200/5677 [===============>..............] - ETA: 6:08 - loss: 0.6735 - acc: 0.5819
3264/5677 [================>.............] - ETA: 5:57 - loss: 0.6744 - acc: 0.5809
3328/5677 [================>.............] - ETA: 5:48 - loss: 0.6743 - acc: 0.5811
3392/5677 [================>.............] - ETA: 5:39 - loss: 0.6741 - acc: 0.5820
3456/5677 [=================>............] - ETA: 5:30 - loss: 0.6745 - acc: 0.5813
3520/5677 [=================>............] - ETA: 5:21 - loss: 0.6738 - acc: 0.5832
3584/5677 [=================>............] - ETA: 5:11 - loss: 0.6738 - acc: 0.5845
3648/5677 [==================>...........] - ETA: 5:01 - loss: 0.6740 - acc: 0.5847
3712/5677 [==================>...........] - ETA: 4:51 - loss: 0.6742 - acc: 0.5841
3776/5677 [==================>...........] - ETA: 4:42 - loss: 0.6740 - acc: 0.5850
3840/5677 [===================>..........] - ETA: 4:32 - loss: 0.6743 - acc: 0.5849
3904/5677 [===================>..........] - ETA: 4:23 - loss: 0.6748 - acc: 0.5845
3968/5677 [===================>..........] - ETA: 4:13 - loss: 0.6749 - acc: 0.5829
4032/5677 [====================>.........] - ETA: 4:03 - loss: 0.6752 - acc: 0.5826
4096/5677 [====================>.........] - ETA: 3:53 - loss: 0.6756 - acc: 0.5811
4160/5677 [====================>.........] - ETA: 3:44 - loss: 0.6755 - acc: 0.5820
4224/5677 [=====================>........] - ETA: 3:34 - loss: 0.6757 - acc: 0.5824
4288/5677 [=====================>........] - ETA: 3:24 - loss: 0.6755 - acc: 0.5837
4352/5677 [=====================>........] - ETA: 3:15 - loss: 0.6755 - acc: 0.5832
4416/5677 [======================>.......] - ETA: 3:05 - loss: 0.6761 - acc: 0.5820
4480/5677 [======================>.......] - ETA: 2:56 - loss: 0.6761 - acc: 0.5824
4544/5677 [=======================>......] - ETA: 2:46 - loss: 0.6765 - acc: 0.5812
4608/5677 [=======================>......] - ETA: 2:37 - loss: 0.6767 - acc: 0.5812
4672/5677 [=======================>......] - ETA: 2:27 - loss: 0.6768 - acc: 0.5809
4736/5677 [========================>.....] - ETA: 2:18 - loss: 0.6774 - acc: 0.5792
4800/5677 [========================>.....] - ETA: 2:08 - loss: 0.6776 - acc: 0.5777
4864/5677 [========================>.....] - ETA: 1:59 - loss: 0.6780 - acc: 0.5771
4928/5677 [=========================>....] - ETA: 1:49 - loss: 0.6783 - acc: 0.5757
4992/5677 [=========================>....] - ETA: 1:40 - loss: 0.6783 - acc: 0.5751
5056/5677 [=========================>....] - ETA: 1:31 - loss: 0.6783 - acc: 0.5750
5120/5677 [==========================>...] - ETA: 1:21 - loss: 0.6784 - acc: 0.5744
5184/5677 [==========================>...] - ETA: 1:12 - loss: 0.6780 - acc: 0.5754
5248/5677 [==========================>...] - ETA: 1:02 - loss: 0.6777 - acc: 0.5770
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6780 - acc: 0.5761 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6778 - acc: 0.5765
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6779 - acc: 0.5756
5504/5677 [============================>.] - ETA: 25s - loss: 0.6776 - acc: 0.5767
5568/5677 [============================>.] - ETA: 15s - loss: 0.6777 - acc: 0.5763
5632/5677 [============================>.] - ETA: 6s - loss: 0.6779 - acc: 0.5760 
5677/5677 [==============================] - 860s 151ms/step - loss: 0.6780 - acc: 0.5762 - val_loss: 0.6744 - val_acc: 0.5959

Epoch 00008: val_acc did not improve from 0.59746
Epoch 9/10

  64/5677 [..............................] - ETA: 13:38 - loss: 0.6840 - acc: 0.5625
 128/5677 [..............................] - ETA: 13:26 - loss: 0.6824 - acc: 0.5391
 192/5677 [>.............................] - ETA: 13:14 - loss: 0.6762 - acc: 0.5781
 256/5677 [>.............................] - ETA: 12:48 - loss: 0.6782 - acc: 0.5742
 320/5677 [>.............................] - ETA: 12:51 - loss: 0.6829 - acc: 0.5594
 384/5677 [=>............................] - ETA: 12:33 - loss: 0.6859 - acc: 0.5443
 448/5677 [=>............................] - ETA: 12:13 - loss: 0.6924 - acc: 0.5335
 512/5677 [=>............................] - ETA: 12:15 - loss: 0.6997 - acc: 0.5234
 576/5677 [==>...........................] - ETA: 12:10 - loss: 0.6911 - acc: 0.5434
 640/5677 [==>...........................] - ETA: 12:05 - loss: 0.6913 - acc: 0.5391
 704/5677 [==>...........................] - ETA: 11:54 - loss: 0.6890 - acc: 0.5469
 768/5677 [===>..........................] - ETA: 11:42 - loss: 0.6913 - acc: 0.5404
 832/5677 [===>..........................] - ETA: 11:36 - loss: 0.6910 - acc: 0.5385
 896/5677 [===>..........................] - ETA: 11:29 - loss: 0.6908 - acc: 0.5413
 960/5677 [====>.........................] - ETA: 11:22 - loss: 0.6895 - acc: 0.5437
1024/5677 [====>.........................] - ETA: 11:18 - loss: 0.6884 - acc: 0.5449
1088/5677 [====>.........................] - ETA: 11:11 - loss: 0.6891 - acc: 0.5460
1152/5677 [=====>........................] - ETA: 11:02 - loss: 0.6872 - acc: 0.5503
1216/5677 [=====>........................] - ETA: 10:52 - loss: 0.6889 - acc: 0.5485
1280/5677 [=====>........................] - ETA: 10:46 - loss: 0.6897 - acc: 0.5437
1344/5677 [======>.......................] - ETA: 10:36 - loss: 0.6887 - acc: 0.5476
1408/5677 [======>.......................] - ETA: 10:28 - loss: 0.6878 - acc: 0.5526
1472/5677 [======>.......................] - ETA: 10:18 - loss: 0.6871 - acc: 0.5530
1536/5677 [=======>......................] - ETA: 10:09 - loss: 0.6871 - acc: 0.5534
1600/5677 [=======>......................] - ETA: 10:00 - loss: 0.6883 - acc: 0.5519
1664/5677 [=======>......................] - ETA: 9:52 - loss: 0.6876 - acc: 0.5535 
1728/5677 [========>.....................] - ETA: 9:41 - loss: 0.6867 - acc: 0.5556
1792/5677 [========>.....................] - ETA: 9:32 - loss: 0.6879 - acc: 0.5530
1856/5677 [========>.....................] - ETA: 9:22 - loss: 0.6873 - acc: 0.5523
1920/5677 [=========>....................] - ETA: 9:15 - loss: 0.6873 - acc: 0.5516
1984/5677 [=========>....................] - ETA: 9:05 - loss: 0.6870 - acc: 0.5524
2048/5677 [=========>....................] - ETA: 8:54 - loss: 0.6857 - acc: 0.5547
2112/5677 [==========>...................] - ETA: 8:44 - loss: 0.6852 - acc: 0.5540
2176/5677 [==========>...................] - ETA: 8:34 - loss: 0.6849 - acc: 0.5542
2240/5677 [==========>...................] - ETA: 8:25 - loss: 0.6859 - acc: 0.5527
2304/5677 [===========>..................] - ETA: 8:16 - loss: 0.6865 - acc: 0.5525
2368/5677 [===========>..................] - ETA: 8:05 - loss: 0.6871 - acc: 0.5503
2432/5677 [===========>..................] - ETA: 7:55 - loss: 0.6868 - acc: 0.5510
2496/5677 [============>.................] - ETA: 7:47 - loss: 0.6868 - acc: 0.5509
2560/5677 [============>.................] - ETA: 7:36 - loss: 0.6857 - acc: 0.5535
2624/5677 [============>.................] - ETA: 7:27 - loss: 0.6855 - acc: 0.5537
2688/5677 [=============>................] - ETA: 7:17 - loss: 0.6849 - acc: 0.5543
2752/5677 [=============>................] - ETA: 7:08 - loss: 0.6846 - acc: 0.5556
2816/5677 [=============>................] - ETA: 6:58 - loss: 0.6841 - acc: 0.5572
2880/5677 [==============>...............] - ETA: 6:49 - loss: 0.6835 - acc: 0.5601
2944/5677 [==============>...............] - ETA: 6:39 - loss: 0.6828 - acc: 0.5618
3008/5677 [==============>...............] - ETA: 6:31 - loss: 0.6827 - acc: 0.5628
3072/5677 [===============>..............] - ETA: 6:21 - loss: 0.6824 - acc: 0.5635
3136/5677 [===============>..............] - ETA: 6:11 - loss: 0.6821 - acc: 0.5644
3200/5677 [===============>..............] - ETA: 6:02 - loss: 0.6816 - acc: 0.5644
3264/5677 [================>.............] - ETA: 5:51 - loss: 0.6807 - acc: 0.5659
3328/5677 [================>.............] - ETA: 5:42 - loss: 0.6806 - acc: 0.5676
3392/5677 [================>.............] - ETA: 5:32 - loss: 0.6809 - acc: 0.5669
3456/5677 [=================>............] - ETA: 5:22 - loss: 0.6807 - acc: 0.5668
3520/5677 [=================>............] - ETA: 5:13 - loss: 0.6806 - acc: 0.5668
3584/5677 [=================>............] - ETA: 5:03 - loss: 0.6803 - acc: 0.5670
3648/5677 [==================>...........] - ETA: 4:54 - loss: 0.6796 - acc: 0.5674
3712/5677 [==================>...........] - ETA: 4:44 - loss: 0.6800 - acc: 0.5665
3776/5677 [==================>...........] - ETA: 4:34 - loss: 0.6792 - acc: 0.5678
3840/5677 [===================>..........] - ETA: 4:25 - loss: 0.6793 - acc: 0.5672
3904/5677 [===================>..........] - ETA: 4:16 - loss: 0.6789 - acc: 0.5681
3968/5677 [===================>..........] - ETA: 4:06 - loss: 0.6791 - acc: 0.5683
4032/5677 [====================>.........] - ETA: 3:57 - loss: 0.6793 - acc: 0.5667
4096/5677 [====================>.........] - ETA: 3:47 - loss: 0.6795 - acc: 0.5667
4160/5677 [====================>.........] - ETA: 3:38 - loss: 0.6797 - acc: 0.5668
4224/5677 [=====================>........] - ETA: 3:29 - loss: 0.6790 - acc: 0.5684
4288/5677 [=====================>........] - ETA: 3:19 - loss: 0.6796 - acc: 0.5676
4352/5677 [=====================>........] - ETA: 3:10 - loss: 0.6800 - acc: 0.5666
4416/5677 [======================>.......] - ETA: 3:01 - loss: 0.6800 - acc: 0.5663
4480/5677 [======================>.......] - ETA: 2:52 - loss: 0.6804 - acc: 0.5654
4544/5677 [=======================>......] - ETA: 2:42 - loss: 0.6801 - acc: 0.5665
4608/5677 [=======================>......] - ETA: 2:33 - loss: 0.6797 - acc: 0.5668
4672/5677 [=======================>......] - ETA: 2:23 - loss: 0.6794 - acc: 0.5676
4736/5677 [========================>.....] - ETA: 2:14 - loss: 0.6794 - acc: 0.5676
4800/5677 [========================>.....] - ETA: 2:05 - loss: 0.6793 - acc: 0.5683
4864/5677 [========================>.....] - ETA: 1:56 - loss: 0.6788 - acc: 0.5691
4928/5677 [=========================>....] - ETA: 1:47 - loss: 0.6790 - acc: 0.5686
4992/5677 [=========================>....] - ETA: 1:38 - loss: 0.6793 - acc: 0.5681
5056/5677 [=========================>....] - ETA: 1:28 - loss: 0.6789 - acc: 0.5688
5120/5677 [==========================>...] - ETA: 1:19 - loss: 0.6787 - acc: 0.5695
5184/5677 [==========================>...] - ETA: 1:10 - loss: 0.6790 - acc: 0.5691
5248/5677 [==========================>...] - ETA: 1:01 - loss: 0.6798 - acc: 0.5684
5312/5677 [===========================>..] - ETA: 51s - loss: 0.6798 - acc: 0.5681 
5376/5677 [===========================>..] - ETA: 42s - loss: 0.6799 - acc: 0.5681
5440/5677 [===========================>..] - ETA: 33s - loss: 0.6796 - acc: 0.5684
5504/5677 [============================>.] - ETA: 24s - loss: 0.6790 - acc: 0.5692
5568/5677 [============================>.] - ETA: 15s - loss: 0.6793 - acc: 0.5684
5632/5677 [============================>.] - ETA: 6s - loss: 0.6795 - acc: 0.5687 
5677/5677 [==============================] - 831s 146ms/step - loss: 0.6795 - acc: 0.5697 - val_loss: 0.6798 - val_acc: 0.5832

Epoch 00009: val_acc did not improve from 0.59746
Epoch 10/10

  64/5677 [..............................] - ETA: 12:21 - loss: 0.6249 - acc: 0.6719
 128/5677 [..............................] - ETA: 11:58 - loss: 0.6794 - acc: 0.5391
 192/5677 [>.............................] - ETA: 11:48 - loss: 0.6831 - acc: 0.5573
 256/5677 [>.............................] - ETA: 11:54 - loss: 0.6724 - acc: 0.5859
 320/5677 [>.............................] - ETA: 11:57 - loss: 0.6741 - acc: 0.5813
 384/5677 [=>............................] - ETA: 11:50 - loss: 0.6746 - acc: 0.5859
 448/5677 [=>............................] - ETA: 11:45 - loss: 0.6757 - acc: 0.5826
 512/5677 [=>............................] - ETA: 11:32 - loss: 0.6762 - acc: 0.5801
 576/5677 [==>...........................] - ETA: 11:18 - loss: 0.6750 - acc: 0.5764
 640/5677 [==>...........................] - ETA: 11:04 - loss: 0.6703 - acc: 0.5875
 704/5677 [==>...........................] - ETA: 10:55 - loss: 0.6689 - acc: 0.5895
 768/5677 [===>..........................] - ETA: 10:53 - loss: 0.6711 - acc: 0.5833
 832/5677 [===>..........................] - ETA: 10:45 - loss: 0.6695 - acc: 0.5853
 896/5677 [===>..........................] - ETA: 10:37 - loss: 0.6703 - acc: 0.5882
 960/5677 [====>.........................] - ETA: 10:24 - loss: 0.6700 - acc: 0.5896
1024/5677 [====>.........................] - ETA: 10:09 - loss: 0.6681 - acc: 0.5947
1088/5677 [====>.........................] - ETA: 9:55 - loss: 0.6664 - acc: 0.5965 
1152/5677 [=====>........................] - ETA: 9:46 - loss: 0.6635 - acc: 0.6033
1216/5677 [=====>........................] - ETA: 9:34 - loss: 0.6647 - acc: 0.6012
1280/5677 [=====>........................] - ETA: 9:27 - loss: 0.6648 - acc: 0.6000
1344/5677 [======>.......................] - ETA: 9:19 - loss: 0.6636 - acc: 0.6012
1408/5677 [======>.......................] - ETA: 9:13 - loss: 0.6662 - acc: 0.5966
1472/5677 [======>.......................] - ETA: 9:06 - loss: 0.6646 - acc: 0.5992
1536/5677 [=======>......................] - ETA: 8:55 - loss: 0.6649 - acc: 0.5983
1600/5677 [=======>......................] - ETA: 8:45 - loss: 0.6669 - acc: 0.5969
1664/5677 [=======>......................] - ETA: 8:37 - loss: 0.6670 - acc: 0.5992
1728/5677 [========>.....................] - ETA: 8:30 - loss: 0.6670 - acc: 0.5972
1792/5677 [========>.....................] - ETA: 8:21 - loss: 0.6667 - acc: 0.5993
1856/5677 [========>.....................] - ETA: 8:10 - loss: 0.6668 - acc: 0.6002
1920/5677 [=========>....................] - ETA: 8:01 - loss: 0.6674 - acc: 0.6005
1984/5677 [=========>....................] - ETA: 7:52 - loss: 0.6683 - acc: 0.5993
2048/5677 [=========>....................] - ETA: 7:44 - loss: 0.6699 - acc: 0.5967
2112/5677 [==========>...................] - ETA: 7:36 - loss: 0.6704 - acc: 0.5961
2176/5677 [==========>...................] - ETA: 7:26 - loss: 0.6699 - acc: 0.5960
2240/5677 [==========>...................] - ETA: 7:18 - loss: 0.6706 - acc: 0.5960
2304/5677 [===========>..................] - ETA: 7:09 - loss: 0.6714 - acc: 0.5938
2368/5677 [===========>..................] - ETA: 7:00 - loss: 0.6708 - acc: 0.5942
2432/5677 [===========>..................] - ETA: 6:53 - loss: 0.6711 - acc: 0.5933
2496/5677 [============>.................] - ETA: 6:45 - loss: 0.6718 - acc: 0.5913
2560/5677 [============>.................] - ETA: 6:36 - loss: 0.6718 - acc: 0.5914
2624/5677 [============>.................] - ETA: 6:27 - loss: 0.6712 - acc: 0.5934
2688/5677 [=============>................] - ETA: 6:20 - loss: 0.6719 - acc: 0.5915
2752/5677 [=============>................] - ETA: 6:12 - loss: 0.6715 - acc: 0.5930
2816/5677 [=============>................] - ETA: 6:04 - loss: 0.6715 - acc: 0.5930
2880/5677 [==============>...............] - ETA: 5:57 - loss: 0.6711 - acc: 0.5938
2944/5677 [==============>...............] - ETA: 5:50 - loss: 0.6712 - acc: 0.5944
3008/5677 [==============>...............] - ETA: 5:41 - loss: 0.6714 - acc: 0.5947
3072/5677 [===============>..............] - ETA: 5:33 - loss: 0.6720 - acc: 0.5924
3136/5677 [===============>..............] - ETA: 5:25 - loss: 0.6724 - acc: 0.5906
3200/5677 [===============>..............] - ETA: 5:16 - loss: 0.6728 - acc: 0.5903
3264/5677 [================>.............] - ETA: 5:08 - loss: 0.6735 - acc: 0.5882
3328/5677 [================>.............] - ETA: 4:59 - loss: 0.6730 - acc: 0.5874
3392/5677 [================>.............] - ETA: 4:51 - loss: 0.6730 - acc: 0.5876
3456/5677 [=================>............] - ETA: 4:43 - loss: 0.6725 - acc: 0.5891
3520/5677 [=================>............] - ETA: 4:35 - loss: 0.6728 - acc: 0.5881
3584/5677 [=================>............] - ETA: 4:26 - loss: 0.6737 - acc: 0.5873
3648/5677 [==================>...........] - ETA: 4:17 - loss: 0.6735 - acc: 0.5869
3712/5677 [==================>...........] - ETA: 4:10 - loss: 0.6738 - acc: 0.5865
3776/5677 [==================>...........] - ETA: 4:01 - loss: 0.6739 - acc: 0.5863
3840/5677 [===================>..........] - ETA: 3:53 - loss: 0.6737 - acc: 0.5867
3904/5677 [===================>..........] - ETA: 3:45 - loss: 0.6738 - acc: 0.5863
3968/5677 [===================>..........] - ETA: 3:37 - loss: 0.6744 - acc: 0.5839
4032/5677 [====================>.........] - ETA: 3:28 - loss: 0.6740 - acc: 0.5851
4096/5677 [====================>.........] - ETA: 3:20 - loss: 0.6747 - acc: 0.5840
4160/5677 [====================>.........] - ETA: 3:12 - loss: 0.6749 - acc: 0.5834
4224/5677 [=====================>........] - ETA: 3:04 - loss: 0.6754 - acc: 0.5824
4288/5677 [=====================>........] - ETA: 2:56 - loss: 0.6754 - acc: 0.5828
4352/5677 [=====================>........] - ETA: 2:48 - loss: 0.6754 - acc: 0.5839
4416/5677 [======================>.......] - ETA: 2:40 - loss: 0.6754 - acc: 0.5833
4480/5677 [======================>.......] - ETA: 2:32 - loss: 0.6756 - acc: 0.5819
4544/5677 [=======================>......] - ETA: 2:24 - loss: 0.6755 - acc: 0.5827
4608/5677 [=======================>......] - ETA: 2:15 - loss: 0.6755 - acc: 0.5827
4672/5677 [=======================>......] - ETA: 2:07 - loss: 0.6756 - acc: 0.5822
4736/5677 [========================>.....] - ETA: 2:00 - loss: 0.6758 - acc: 0.5821
4800/5677 [========================>.....] - ETA: 1:52 - loss: 0.6759 - acc: 0.5810
4864/5677 [========================>.....] - ETA: 1:44 - loss: 0.6757 - acc: 0.5814
4928/5677 [=========================>....] - ETA: 1:36 - loss: 0.6757 - acc: 0.5812
4992/5677 [=========================>....] - ETA: 1:27 - loss: 0.6755 - acc: 0.5823
5056/5677 [=========================>....] - ETA: 1:19 - loss: 0.6758 - acc: 0.5819
5120/5677 [==========================>...] - ETA: 1:11 - loss: 0.6757 - acc: 0.5818
5184/5677 [==========================>...] - ETA: 1:03 - loss: 0.6756 - acc: 0.5822
5248/5677 [==========================>...] - ETA: 55s - loss: 0.6756 - acc: 0.5816 
5312/5677 [===========================>..] - ETA: 46s - loss: 0.6762 - acc: 0.5802
5376/5677 [===========================>..] - ETA: 38s - loss: 0.6760 - acc: 0.5805
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6759 - acc: 0.5800
5504/5677 [============================>.] - ETA: 22s - loss: 0.6760 - acc: 0.5801
5568/5677 [============================>.] - ETA: 14s - loss: 0.6761 - acc: 0.5801
5632/5677 [============================>.] - ETA: 5s - loss: 0.6764 - acc: 0.5788 
5677/5677 [==============================] - 758s 134ms/step - loss: 0.6763 - acc: 0.5794 - val_loss: 0.6887 - val_acc: 0.5610

Epoch 00010: val_acc did not improve from 0.59746
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9d080bc1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f9d080bc1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9ebc05fad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f9ebc05fad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d906a91d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9d906a91d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9cf84338d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9cf84338d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9cf8316510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9cf8316510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf0611dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf0611dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf8433d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf8433d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9680165650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9680165650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9cf059e490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9cf059e490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9cf8296750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9cf8296750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf8247650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf8247650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf83a27d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf83a27d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f978c0feb50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f978c0feb50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9cf07906d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9cf07906d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9cf06d1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9cf06d1c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f992042ae10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f992042ae10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf057f5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf057f5d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9aec684f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9aec684f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d206f4350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9d206f4350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ce80b78d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9ce80b78d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c687dd2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c687dd2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf04a9650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9cf04a9650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c687941d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9c687941d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c686b9450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9c686b9450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b7029fe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b7029fe90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b7028c2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b7028c2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b703c4f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b703c4f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b7037b090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b7037b090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b70054510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b70054510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b286c9790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b286c9790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b287737d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b287737d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b700548d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b700548d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b287037d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b287037d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b2853ec10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b2853ec10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b283d4c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b283d4c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b7009c690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b7009c690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b287666d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b287666d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b284fe810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b284fe810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b281e6610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b281e6610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b283cef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b283cef50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf8375410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9cf8375410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b281e6750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b281e6750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b280f3090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b280f3090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b1fed3050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b1fed3050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b1fed62d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b1fed62d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1fc5c790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1fc5c790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b1fed3c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b1fed3c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b281c6bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b281c6bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b1fba6810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b1fba6810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b284100d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b284100d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1fb1f990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1fb1f990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b1fe1a050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b1fe1a050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1fa40390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1fa40390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b1f873290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b1f873290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b1c6e1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b1c6e1c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1c783a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1c783a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b1f900390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b1f900390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1c72dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1c72dfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b1c509710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f9b1c509710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b1c4fc9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f9b1c4fc9d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1c439190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1c439190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b1c7af450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f9b1c7af450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1c4fd2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f9b1c4fd2d0>>: AttributeError: module 'gast' has no attribute 'Str'
03window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 8:18
 128/1578 [=>............................] - ETA: 4:41
 192/1578 [==>...........................] - ETA: 3:28
 256/1578 [===>..........................] - ETA: 2:49
 320/1578 [=====>........................] - ETA: 2:23
 384/1578 [======>.......................] - ETA: 2:05
 448/1578 [=======>......................] - ETA: 1:52
 512/1578 [========>.....................] - ETA: 1:40
 576/1578 [=========>....................] - ETA: 1:30
 640/1578 [===========>..................] - ETA: 1:20
 704/1578 [============>.................] - ETA: 1:12
 768/1578 [=============>................] - ETA: 1:06
 832/1578 [==============>...............] - ETA: 59s 
 896/1578 [================>.............] - ETA: 53s
 960/1578 [=================>............] - ETA: 47s
1024/1578 [==================>...........] - ETA: 41s
1088/1578 [===================>..........] - ETA: 36s
1152/1578 [====================>.........] - ETA: 31s
1216/1578 [======================>.......] - ETA: 26s
1280/1578 [=======================>......] - ETA: 21s
1344/1578 [========================>.....] - ETA: 16s
1408/1578 [=========================>....] - ETA: 12s
1472/1578 [==========================>...] - ETA: 7s 
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 111s 70ms/step
loss: 0.679424331366487
acc: 0.5766793410512129
